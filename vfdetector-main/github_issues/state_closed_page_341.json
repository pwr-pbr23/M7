[{"number": 43898, "title": "TARGET command line option to the TFLM Makefile and the target-specific makefile have specific naming requirements", "body": "With #43896 we are going to enforce that `TARGET=blah` will only include `micro/tools/make/targets/blah_makefile.inc`.\r\n\r\nAt minimum this means that checks such as https://github.com/tensorflow/tensorflow/blob/abaca545db62fd30c6caad021d55cbcacbf01060/tensorflow/lite/micro/tools/make/targets/cortex_m_gcc_generic_makefile.inc#L2 will no longer be necessary.\r\n\r\n#43896 removes the checks for bluepill and apollo3evb.\r\n\r\nThis issue will remain open until the checks are removed for:\r\n * cortex_m_gcc_generic_makefile.inc\r\n * stm32f4_makefile.inc\r\n * hexagon_makefile.inc\r\n * xtensa_hifimini_makefile.inc\r\n\r\nBut the other targets might need updates beyond simply removing the no longer necessary if statements.\r\n\r\nTagging some of maintainers of targets that I am aware of: @yair-ehrenwald @dzakhar @JaccovG @mansnils ", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43898\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43898\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43898\">No</a>\n"]}, {"number": 43897, "title": "Custom Op Output Shape is Unknown in TF 1.15", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SLES 12.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.3\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): g++ 4.8 (for custom op)\r\n- CUDA/cuDNN version: 10.0 / 7.6\r\n- GPU model and memory: Tesla P100 / 12198MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCustom Op compiled against TF 1.15 has unknown output shape (`>>>print(repr(t.shape))  # TensorShape(None)`) in following conditions:\r\n- Input tensor has a known shape.\r\n- Custom Op ShapeFn has been implemented to have the same output shape as input shape.\r\n\r\nIn TF 1.11, the output shape is identical to the input shape as expected.\r\n\r\n**Describe the expected behavior**\r\nCustom Op output shape should be identical to input shape (`>>>print(repr(t.shape))  #TensorShape([6])`)\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nBased on the \"Zero Out\" example from [create an op](https://www.tensorflow.org/guide/create_op). The following should get it up and running:\r\n1. copy all files into the same directory\r\n2. `sh build.sh`\r\n3. `python zero_out_shape_test.py`\r\n\r\nIn TF 1.11 `as_list` will work and shape will be `TensorShape([6])`. In TF 1.15 `as_list` will fail (logs below) and shape will be `TensorShape(None)`.\r\n\r\n_zero_out.cc_\r\n```cpp\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n    for (int i = 1; i < N; i++) {\r\n      output_flat(i) = 0;\r\n    }\r\n\r\n    // Preserve the first input value if possible.\r\n    if (N > 0) output_flat(0) = input(0);\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\n_build.sh_\r\n```sh\r\nTF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))') )\r\nTF_LFLAGS=( $(python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))') )\r\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC ${TF_CFLAGS[@]} ${TF_LFLAGS[@]} -O2\r\n```\r\n\r\n_zero_out_shape_test.py_\r\n```py\r\nimport tensorflow as tf\r\n\r\n_input = tf.constant([1, 2, 3, 4, 5, 6])\r\nprint('input shape:', _input.shape.as_list())\r\n\r\nzero_out_module = tf.load_op_library('./zero_out.so')\r\n\r\nzo_op = zero_out_module.zero_out(_input)\r\nprint(repr(zo_op.get_shape()))\r\nprint(zo_op.get_shape())\r\nprint(zo_op.get_shape().as_list())\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nTF 1.15 logs\r\n```\r\ninput shape: [6]\r\nTensorShape(None)\r\n<unknown>\r\nTraceback (most recent call last):\r\n  File \"zero_out_shape_test.py\", line 11, in <module>\r\n    print(zo_op.get_shape().as_list())\r\n  File \"******/python3.5/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 1171, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\n\r\nTF 1.11 logs\r\n```\r\ninput shape: [6]\r\nTensorShape([Dimension(6)])\r\n(6,)\r\n[6]\r\n```\r\n", "comments": ["@iandreariley,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.3 and check if you are facing the same issue? Thanks!", "@amahendrakar I'm unable to reproduce in v2.3. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43897\">No</a>\n"]}, {"number": 43896, "title": "Enforce that the target makefile must be called TARGET_makefile.", "body": "This allows for better error reporting (b/170331334).\r\n\r\nFor example:\r\n  ```\r\n  make -f tensorflow/lite/micro/tools/make/Makefile microlite TARGET=foo\r\n  ```\r\n\r\nwill give the following error:\r\n  ```\r\n  tensorflow/lite/micro/tools/make/Makefile:359: tensorflow/lite/micro/tools/make/targets/foo_makefile.inc: No such file or directory\r\n  make: *** No rule to make target 'tensorflow/lite/micro/tools/make/targets/foo_makefile.inc'.  Stop.\r\n  ```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43895, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - Quantized Matmul op", "body": "DNN 0.x cleanup of MKL quantized matmul op:\r\n\r\n(1) Remove all DNN 0.x related code\r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43894, "title": "Error while building tensorflow from source, need it for C++ inference.", "body": "no such package '@icu//': java.io.IOException: **Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz**\r\n\r\n**System information**\r\n- Windows 10\r\n- TensorFlow installed from source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.7\r\n- Bazel version 0.21\r\n- GCC/Compiler version VS2015\r\n- CUDA/cuDNN version: 10, 7.4.1\r\n- GPU model and memory: Titan XP, 10Gb\r\n\r\n\r\n\r\n**While building tensorflow for windows based on tensoflow website directed steps I got this error below after configur.py was ran successfully**\r\n\r\nERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:149:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n(base) **c:\\>python tensorflow\\configure.py**\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: d6710a42-59b5-4bbb-9174-8bc67a08a8c7\r\nYou have bazel 0.21.0 installed.\r\nPlease specify the location of python. [Default is C:\\ProgramData\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  \\Users\\dsavaliya\\Repos\\Pytools\r\n  C:\\ProgramData\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [\\Users\\dsavaliya\\Repos\\Pytools]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n\r\n(base) c:\\>cd c:\\tensorflow\r\n(base) **c:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package**\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\tensorflow/.bazelrc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 27545a6e-c366-4264-9cb3-eea8d0762600\r\n**ERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:149:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'**\r\n**ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761**\r\nINFO: Elapsed time: 22.941s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (316 packages loaded, 11908 targets configured)\r\n    Fetching @icu; fetching 14s\r\n\r\n**Also tried GPU supported virion with command as below, but do the same error as above.** \r\n(base) **c:\\tensorflow>bazel build --config=monolithic --config=opt --config=cuda //tensorflow/tools/lib_package:libtensorflow**\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@devang1397 \r\nCurrently there is support for 2.x only, can you please upgrade to 2.x and let us know if you face any problem.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43894\">No</a>\n"]}, {"number": 43893, "title": "Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["**I'm getting this error while building tensorflow for windows,** \r\n\r\n(base) c:\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\tensorflow/.bazelrc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: 27545a6e-c366-4264-9cb3-eea8d0762600\r\nERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:149:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to C:/users/dsavaliya/_bazel_dsavaliya/xv6zejqw/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761\r\nINFO: Elapsed time: 22.941s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (316 packages loaded, 11908 targets configured)\r\n    Fetching @icu; fetching 14s\r\n\r\n**Configure did go as expected see below.** \r\n\r\n(base) c:\\>python tensorflow\\configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: d6710a42-59b5-4bbb-9174-8bc67a08a8c7\r\nYou have bazel 0.21.0 installed.\r\nPlease specify the location of python. [Default is C:\\ProgramData\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  \\Users\\dsavaliya\\Repos\\Pytools\r\n  C:\\ProgramData\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [\\Users\\dsavaliya\\Repos\\Pytools]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.", "I am a bit confused about why this is closed.\r\nI see a very similar problem when I do a yocto build.\r\n```\r\n| Analyzing: target //tensorflow/tools/pip_package:build_pip_package (339 packages loaded, 15635 targets configured)\r\n| ERROR: /workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/git/tensorflow/tools/pip_package/BUILD:149:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/bazel/output_base/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/tools/pip_package:licenses'\r\n| ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/bazel/output_base/external/icu/release-62-1.tar.gz: Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761\r\n| INFO: Elapsed time: 22.622s\r\n| INFO: 0 processes.\r\n| FAILED: Build did NOT complete successfully (339 packages loaded, 15635 targets configured)\r\n| FAILED: Build did NOT complete successfully (339 packages loaded, 15635 targets configured)\r\n| WARNING: /workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/temp/run.do_compile.142027:157 exit 1 from '/workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/bazel/bazel build -c opt --subcommands --explain=/workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/temp/explain.log --verbose_explanations --verbose_failures --verbose_failures //tensorflow/tools/pip_package:build_pip_package'\r\n| WARNING: Backtrace (BB generated script):\r\n|       #1: do_compile, /workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/temp/run.do_compile.142027, line 157\r\n|       #2: main, /workdir/build/container-x86-64-tensorflow-master/tmp/work/x86_64-linux/tensorflow-native/1.13.0-r0/temp/run.do_compile.142027, line 166\r\n| \r\n| Backtrace (metadata-relative locations):\r\n|       #1: do_compile, /workdir/sources/poky-master/../meta-tensorflow-master/recipes-framework/tensorflow/tensorflow-native_1.13.0.bb, line 15\r\nERROR: Task (/workdir/sources/poky-master/../meta-tensorflow-master/recipes-framework/tensorflow/tensorflow-native_1.13.0.bb:do_compile) failed with exit code '1'\r\nNOTE: Tasks Summary: Attempted 965 tasks of which 964 didn't need to be rerun and 1 failed.\r\nNOTE: The errors for this build are stored in /workdir/build/container-x86-64-tensorflow-master/tmp/log/error-report/error_report_20201026090200.txt\r\nYou can send the errors to a reports server by running:\r\n  send-error-report /workdir/build/container-x86-64-tensorflow-master/tmp/log/error-report/error_report_20201026090200.txt [-s server]\r\nNOTE: The contents of these logs will be posted in public if you use the above command with the default server. Please ensure you remove any identifying or proprietary information when prompted before sending.\r\nNOTE: Writing buildhistory\r\nNOTE: Writing buildhistory took: 1 seconds\r\n\r\nSummary: 1 task failed:\r\n  /workdir/sources/poky-master/../meta-tensorflow-master/recipes-framework/tensorflow/tensorflow-native_1.13.0.bb:do_compile\r\nSummary: There was 1 WARNING message shown.\r\nSummary: There was 1 ERROR message shown, returning a non-zero exit code.\r\npokyuser@f3b909198114:/workdir/build/container-x86-64-tensorflow-master/conf$ \r\n```\r\n\r\nWe do have multiple issues here:\r\n1) https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz <-- disappeared\r\n2) Checksum was 86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 <-- checksum changed\r\n\r\n```\r\nwget https://github.com/unicode-org/icu/archive/release-62-1.tar.gz\r\nsha256sum release-62-1.tar.gz\r\n86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181  release-62-1.tar.gz\r\n```\r\n\r\nSo I thought I could fix it like this:\r\n```\r\ndef repo():\r\n    third_party_http_archive(\r\n        name = \"icu\",\r\n        strip_prefix = \"icu-release-62-1\",\r\n        sha256 = \"86b85fbf1b251d7a658de86ce5a0c8f34151027cc60b01e1b76f167379acf181\",\r\n        urls = [\r\n            \"https://github.com/unicode-org/icu/archive/release-62-1.tar.gz\",\r\n        ],\r\n        build_file = \"//third_party/icu:BUILD.bazel\",\r\n        system_build_file = \"//third_party/icu:BUILD.system\",\r\n        patch_file = clean_dep(\"//third_party/icu:udata.patch\"),\r\n    )\r\n```\r\n\r\nBut I guess I need more changes somewhere else as well.\r\n\r\nPlease help!\r\n"]}, {"number": 43892, "title": "Model size reduction in set_weights()", "body": "I have used tf.Keras for training and then transferred weights to another model with the help of get_weights() and set_weights(). After saving the weights from the original to another model I have seen that there is the model size reduction of 3 times smaller. \r\nAny idea why this is happening. ", "comments": ["@OriAlpha,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43892\">No</a>\n"]}, {"number": 43890, "title": "pytorch sees my gpu, but tensorflow does not", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: no\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubunto 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: pip install tensorflow\r\n-   **TensorFlow version (use command below)**: 2.3.1\r\n-   **Python version**: 3.8.1\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: 11.1\r\n-   **GPU model and memory**: tesla p100\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\n\r\ni am not sure what is going on here. i set up a fresh gcloud instance, updated the nvidia drivers, downloaded anaconda, pytorch and tensorflow but tf can not seem to see the gpu.\r\n\r\nmy versions:\r\n\r\n\r\npytorch : '1.6.0'\r\ntorch.cuda.is_available() gives me True\r\n\r\n\r\ntf: 2.3.1.\r\ntf.config.list_physical_devices(\"GPU\") gives me []\r\n\r\n\r\nalso when listing all devices:\r\ntf.config.list_physical_devices()\r\n\r\n\r\ni get \r\n\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\r\n PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\r\n\r\n\r\nwhich means tf sees the gpu but does not use it", "comments": ["@PatschD \r\n\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source#gpu).Please, try with tested build configurations and let us know if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43889, "title": "Add oneDNN-aarch64 reference build option", "body": "Adds build option and supporting macros for oneDNN-reference builds on Aarch64.\r\nFor now, oneDNN will link to GCC's libgomp.so. See, related issues: https://github.com/tensorflow/tensorflow/issues/42858\r\nand https://github.com/tensorflow/tensorflow/pull/41232\r\n\r\nSigned-off-by: cfRod <crefeda.rodrigues@arm.com>", "comments": ["Thanks for the merge! I would also like to thank @agramesh1 for the support in the previous PR and issues.", "Adding a comment on usage:\r\n```\r\n bazel build  \\\r\n      --config=mkl_aarch64 \\\r\n      --copt=\"-mtune=native\" --copt=\"-march=armv8-a\" --copt=\"-moutline-atomics\" \\\r\n      --cxxopt=\"-mtune=native\" --cxxopt=\"-march=armv8-a\" --cxxopt=\"-moutline-atomics\" \\\r\n      --linkopt=\"-lm\" --linkopt=\"-fopenmp\" \\\r\n      --config=noaws --config=v2  --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" \\\r\n      //tensorflow/tools/pip_package:build_pip_package\r\n```", "> Thanks for the merge! I would also like to thank @agramesh1 for the support in the previous PR and issues.\r\n\r\n@cfRod thanks. FYI, we are making changes to the oneDNN build.  I just submitted a PR to remove the binary openMP and replace this with opensource version. https://github.com/tensorflow/tensorflow/pull/43586.  There is also an alternate (experimental) threading model in oneDNN, https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L165", "> > Thanks for the merge! I would also like to thank @agramesh1 for the support in the previous PR and issues.\r\n> \r\n> @cfRod thanks. FYI, we are making changes to the oneDNN build. I just submitted a PR to remove the binary openMP and replace this with opensource version. #43586. There is also an alternate (experimental) threading model in oneDNN, https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L165\r\n\r\n@agramesh1, Thanks for the link. I will have a look at it."]}, {"number": 43888, "title": "[XLA] When ptxas do not know about an SM, fallback to the driver.", "body": "Currently XLA always use ptxas. If a user have an old container, but a newer GPU, ptxas won't know its SM version.\r\nIn that case, instead of erroring, fallback to the driver to compile instead of PTXAS.\r\nIt won't have all optimization, but it will be working.", "comments": ["I amended the commit as it had one debug leftover."]}, {"number": 43887, "title": "Building on MacOS is broken for TFLite Micro", "body": "Latest head doesn't build on MacOS:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile test_micro_interpreter_test\r\n```\r\n\r\nException:\r\n```\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/micro_interpreter_test tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_interpreter_test.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm -framework Foundation -framework AudioToolbox\r\nld: unknown option: --fatal-warnings\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\ngmake: *** [tensorflow/lite/micro/tools/make/Makefile:460: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/micro_interpreter_test] Error 1\r\n```\r\n\r\nLooks like the build cleanup added this flag.\r\n\r\n", "comments": ["I would guess that the compiler version is what is causing the problem here? I don't have a way to test on Mac OS but we can likely add an if statement similar to the fix for #43336 "]}, {"number": 43886, "title": "Update Keras Conv2D layer docs to specify padding value", "body": "## URL(s) with the issue:\r\nhttps://keras.io/api/layers/convolution_layers/convolution2d/\r\n\r\n## Description of issue (what needs changing):\r\nSpecify what value is added as padding when padding argument is set to `same`. Is it zero values?\r\n\r\n", "comments": ["Yes in `padding = \"same\"` we add a n-pixel borders of zero to tell that don't reduce the dimension and have same dimension as input. ", "Can I work on this issue?\r\n@Saduf2019 ", "@marcospgp,\r\nThe reason information about `Padding` with Zeroes is not mentioned either in [Keras Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) or in [TF Keras Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) is because it is an **`Internal Operation on the Input`** of that `Layer` and the `Padded Zeroes` will not be visible to us both in `Input` or `Output Tensors`.\r\n\r\nHowever, information about `Padding` is added via [this PR](https://github.com/tensorflow/tensorflow/pull/40512).", "@marcospgp,\r\nCan you please respond to the above comment. Thanks! ", "@rmothukuru I don't see why it being an internal operation means the user doesn't need to know what's going on, especially since it alters the final result.\r\n\r\nIf that PR adds the information to the docs then great!\r\n\r\nCheers and thanks for your hard work and care!", "> @marcospgp,\r\n> The reason information about `Padding` with Zeroes is not mentioned either in [Keras Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/) or in [TF Keras Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) is because it is an **`Internal Operation on the Input`** of that `Layer` and the `Padded Zeroes` will not be visible to us both in `Input` or `Output Tensors`.\r\n> \r\n> However, information about `Padding` is added via [this PR](https://github.com/tensorflow/tensorflow/pull/40512).\r\n\r\n@marcospgp,\r\nI take back my comment that it is an internal operation. Yes, it impacts the result. Will update the documentation stating that **`padding = same` pads the Input Image Tensor with Zeros**. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 43885, "title": "Build Error with codes from TinyML Book", "body": "#40170 # System information\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.14.6\r\n-   **TensorFlow installed from (source or binary)**: 2.0.0\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:3.7\r\n-   **GCC/Compiler version (if compiling from source)**:4.3\r\n-   **Exact command to reproduce**: \"make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\"\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nEvery time I build using the command from book, encounter error below\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:34: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test] Error 1\r\n\r\n### Source code / logs\r\nJoonui-Mac-mini:tensorflow joonkim$ make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\r\ntensorflow/lite/micro/tools/make/Makefile:401: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:401: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp  \r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.12.0.tar.gz\" \"c62ffefb3d4548b127cca14ce047f16c\" tensorflow/lite/micro/tools/make/downloads/flatbuffers  \r\ndownloading http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.12.0.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip\" \"c720b1743360259ac45809a321f8f26c\" tensorflow/lite/micro/tools/make/downloads/ruy  \r\ndownloading https://github.com/google/ruy/archive/5bb02fbf90824c2eb6cd7418f766c593106a332b.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip\" \"55b85f76e2995153e660391d4a209ef1\" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip\" \"9b5b6d4677dd0a91b1bb992d1c4c0417\" tensorflow/lite/micro/tools/make/downloads/person_model_int8  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_06_23.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip\" \"1f4607b05ac45b8a6146fb883dbc2d7b\" tensorflow/lite/micro/tools/make/downloads/image_recognition_model  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\" \"c32a1d4ab5d03f1284b67883e8d87530\" tensorflow/lite/micro/tools/make/downloads/cifar10 patch_cifar10_dataset \r\ndownloading http://mirror.tensorflow.org/www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft \r\ndownloading http://mirror.tensorflow.org/github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/examples/hello_world/hello_world_test.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/hello_world_test.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/examples/hello_world/model.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/model.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/all_ops_resolver.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/all_ops_resolver.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/debug_log.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/debug_log.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/memory_helpers.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_helpers.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_allocator.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_error_reporter.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_error_reporter.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_interpreter.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_interpreter.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_profiler.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_profiler.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_string.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_string.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_time.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_time.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/micro_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_utils.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/recording_micro_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_micro_allocator.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/recording_simple_memory_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_simple_memory_allocator.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/simple_memory_allocator.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/simple_memory_allocator.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/test_helpers.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/test_helpers.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/activations.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/activations.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/add.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/add.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/arg_min_max.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/arg_min_max.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/ceil.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ceil.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/circular_buffer.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/circular_buffer.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/comparisons.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/comparisons.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/concatenation.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/concatenation.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/conv.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/conv.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/depthwise_conv.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/depthwise_conv.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/dequantize.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/dequantize.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/elementwise.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/elementwise.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/ethosu.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ethosu.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/floor.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/floor.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/fully_connected.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/hard_swish.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/hard_swish.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/kernel_runner.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_runner.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/kernel_util.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_util.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/l2norm.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/l2norm.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/logical.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logical.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/logistic.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logistic.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/maximum_minimum.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/maximum_minimum.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/mul.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/mul.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/neg.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/neg.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/pack.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pack.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/pad.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pad.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/pooling.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pooling.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/prelu.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/prelu.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/quantize.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/quantize.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/reduce.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reduce.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/reshape.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reshape.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/resize_nearest_neighbor.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/resize_nearest_neighbor.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/round.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/round.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/shape.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/shape.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/softmax.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/softmax.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/split.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/split_v.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split_v.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/strided_slice.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/strided_slice.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/sub.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/sub.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/svdf.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/svdf.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/tanh.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/tanh.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/unpack.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/unpack.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/greedy_memory_planner.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/memory_planner/linear_memory_planner.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/linear_memory_planner.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/testing/test_conv_model.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_conv_model.o\r\ngcc -std=c11 -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/c/common.c -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/c/common.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/error_reporter.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/error_reporter.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/flatbuffer_conversions.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/op_resolver.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/op_resolver.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/core/api/tensor_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/tensor_utils.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/internal/quantization_util.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/internal/quantization_util.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/kernels/kernel_util.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/kernel_util.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/testing/test_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_utils.o\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/schema/schema_utils.cc -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/schema/schema_utils.o\r\nar -r tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/all_ops_resolver.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/debug_log.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_helpers.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_error_reporter.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_interpreter.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_profiler.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_string.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_time.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/micro_utils.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_micro_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/recording_simple_memory_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/simple_memory_allocator.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/test_helpers.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/benchmarks/keyword_scrambled_model_data.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/activations.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/add.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/arg_min_max.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ceil.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/circular_buffer.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/comparisons.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/concatenation.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/conv.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/depthwise_conv.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/dequantize.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/elementwise.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/ethosu.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/floor.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/hard_swish.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_runner.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/kernel_util.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/l2norm.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logical.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/logistic.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/maximum_minimum.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/mul.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/neg.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pack.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pad.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/pooling.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/prelu.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/quantize.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reduce.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/reshape.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/resize_nearest_neighbor.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/round.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/shape.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/softmax.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/split_v.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/strided_slice.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/sub.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/svdf.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/tanh.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/kernels/unpack.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/greedy_memory_planner.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/memory_planner/linear_memory_planner.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_conv_model.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/c/common.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/error_reporter.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/flatbuffer_conversions.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/op_resolver.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/core/api/tensor_utils.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/internal/quantization_util.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/kernels/kernel_util.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/testing/test_utils.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/schema/schema_utils.o\r\nar: creating archive tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -DTF_LITE_DISABLE_X86_NEON -DTF_LITE_DISABLE_X86_NEON -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/hello_world_test.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/model.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm -framework Foundation -framework AudioToolbox\r\nld: unknown option: --fatal-warnings\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:34: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test] Error 1\r\n", "comments": ["One more build gives logs with warnings as below\r\n\"tensorflow/lite/micro/tools/make/Makefile:400: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:400: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\"\r\n\r\nIs there any problem in 'Makefile'? Can you share the one that is working without any bugs?\r\n![\uc2a4\ud06c\ub9b0\uc0f7 2020-10-09 \uc624\ud6c4 5 01 10](https://user-images.githubusercontent.com/72503160/95558366-15d76b00-0a51-11eb-9da6-75f1afcc947b.png)\r\n", "Hi @joongoes  I had the same trouble. This seems to be linked to tensorflow/tensorflow/issues/43887\r\n\r\nI have done a dirty fix here with https://github.com/EDGE22/tensorflow/commit/8b9a73c36fce2b4145419fe42df9f3c2274f09ee by just commenting out the `--fatal-warning` flag. \r\n\r\nI suspect this should be resolved soon with this [comment](https://github.com/tensorflow/tensorflow/issues/43887#issuecomment-705801453) and this [related issue](https://github.com/tensorflow/tensorflow/issues/43336)", "#44196 should fix the issue, and will close this bug. Please reopen if that does not work."]}, {"number": 43884, "title": "Update Keras 2D convolution docs", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43884) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@marcospgp  Can you please check @tanzhenyu's comments and keep us posted ? Thanks!\r\n", "@tanzhenyu @gbaned\r\n\r\n> If any dimension of your input can vary, you can specify it as None\r\n\r\nhttps://keras.io/getting_started/intro_to_keras_for_engineers/", "@tanzhenyu Can you please take a look on above comments from @marcospgp. Thanks!"]}, {"number": 43883, "title": "Embedding Projector - Unable to load different data set", "body": "We want to use dimension reduction and clustering app of tensorflow. However we are unable to load data set to http://projector.tensorflow.org/ \r\nIt gives us errors like Number of tensors (10000) do not match the number of lines in metadata (3492) or\r\nit does not give any error and does not load the file\r\n\r\n", "comments": ["@sinan-demirhan,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43883\">No</a>\n"]}, {"number": 43882, "title": "folding batchnorm into conv in per-tensor weights quantization", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version (or github SHA if from source): 2.3.1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nI took the keras qat tutorial and added a BatchNormalization layer in between the Conv2D and ReLU:\r\n\r\n```\r\nimport tempfile\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nmnist = keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.InputLayer(input_shape=(28, 28)),\r\n    keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n    keras.layers.Conv2D(filters=12, kernel_size=(3, 3)),\r\n    keras.layers.BatchNormalization(),\r\n    keras.layers.ReLU(),\r\n    keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n    keras.layers.Flatten(),\r\n    keras.layers.Dense(10)\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(\r\n    train_images,\r\n    train_labels,\r\n    epochs=1,\r\n    validation_split=0.1,\r\n)\r\nimport tensorflow_model_optimization as tfmot\r\nquantize_model = tfmot.quantization.keras.quantize_model\r\nq_aware_model = quantize_model(model)\r\nq_aware_model.compile(optimizer='adam',\r\n                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                      metrics=['accuracy'])\r\ntrain_images_subset = train_images[0:1000]  # out of 60000\r\ntrain_labels_subset = train_labels[0:1000]\r\nq_aware_model.fit(train_images_subset, train_labels_subset,\r\n                  batch_size=500, epochs=1, validation_split=0.1)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nquantized_tflite_model = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(quantized_tflite_model)\r\n```\r\nWhen I quantize the weights per-channel, I can see (using Netron) that the BatchNorm was folded into Conv2D (as I expect it to be). When I changed [Default8BitConvWeightsQuantizer](https://github.com/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantizers.py) to use per-tensor quantization (flipping per-axis flag and removing shape arguments in build), I saw that the BatchNorm was not folded.\r\nIs this the way it should be? Or is it an issue? \r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nWARNING:tensorflow:From ***/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-10-08 17:35:12.606042: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:From ***/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\n2020-10-08 17:35:13.543657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-08 17:35:13.544090: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-10-08 17:35:13.544145: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-10-08 17:35:13.601942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-08 17:35:13.602363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7313c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-08 17:35:13.602374: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-10-08 17:35:13.602525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-08 17:35:13.602870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-08 17:35:13.603313: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-08 17:35:13.603317: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-08 17:35:13.603326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-08 17:35:13.603330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-08 17:35:13.603333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-08 17:35:13.604761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-10-08 17:35:13.604769: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-10-08 17:35:13.604772: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-10-08 17:35:13.650284: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-10-08 17:35:13.650302: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2020-10-08 17:35:13.653368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-10-08 17:35:13.653747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-10-08 17:35:13.654132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-08 17:35:13.654135: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-10-08 17:35:13.654145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-08 17:35:13.654148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-08 17:35:13.654151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n\r\nProcess finished with exit code 0\r\n\r\n```\r\n\r\nThanks!", "comments": ["@reuvenperetz \r\nI ran the code shared on tf 2.3 and do not face any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/b9ea6d386228f2a83eb1bd136e3e8476/untitled427.ipynb).\r\nPlease provide with colab gist the error reported.", "@Saduf2019 How did you test this folding in colab?\n\nHe asked:\n\n> When I quantize the weights per-channel, I can see (using Netron) that the BatchNorm was folded into Conv2D (as I expect it to be). When I changed Default8BitConvWeightsQuantizer to use per-tensor quantization (flipping per-axis flag and removing shape arguments in build), I saw that the BatchNorm was not folded.\nIs this the way it should be? Or is it an issue?", "I used Netron to check it.\r\nGraph when I quantize it per-tensor:\r\n![photo_2020-10-10_14-55-59](https://user-images.githubusercontent.com/44209964/95655008-00c01200-0b0d-11eb-9d84-13e3d99be95d.jpg)\r\nGraph when I quantize it per-channel:\r\n![photo_2020-10-10_15-28-24](https://user-images.githubusercontent.com/44209964/95655044-47ae0780-0b0d-11eb-92fb-dd8612792b2a.jpg)\r\nBy the way, the batchnorm is folded in both cases when I don't use the MLIR converter (converter.experimental _new_ converter=False)\r\n", "@karimnosseir \r\n\r\nWith the focus on these layers in the model... Conv2d --> BatchNorm --> ReLu \r\n\r\nFor per-channel quantization: \r\nFor per-channel quantization + TOCO converter: \r\nFor per-tensor quantization (TF code modified by the user) + TOCO converter: \r\n- All 3 layers are combined into one layer.\r\n\r\nFor per-tensor quantization (TF code modified by the user): \r\n- All 3 layers are NOT combined into one layer.\r\n\r\nWe can ignore the usage of TOCO converter as that will be soon deprecated, however:\r\n1. Do we support per tensor quantized models? (instead of users modifying the TF code)\r\n2. If we do/don't, is the above behavior expected? (The 3 layers are NOT combined/folded into one unit)\r\n\r\n", "Actually the one with the old (deprecated) converter is wrong, and the one with the new converter (default) is correct.\r\nLet me try to explain,\r\n\r\nKeras by default sets dynamic batch size to true.\r\nThat means that the model input shape is [*,28,28] not [1,28,28].\r\nThe old(deprecated) converter used to ignore the dynamic batch and override this to 1 - which is wrong since this is not what the original model has - you can imagine how bad it will be when you try to resize the inputs at runtime.\r\n\r\nThe current converter instead handles the dynamic batch size correct, and the model generated can be resized at runtime correct.\r\nThat's why the sequence of \"Shape, StridedSlice, Pack\" wasn't constant folded, since the shape is dependent on the shape defined at runtime.\r\n\r\nIf you don't want dynamic batch in the final model, then set the model before converting to have a static shape.\r\nFor example,\r\n\r\ninput_name = model.input_names[0]\r\nindex = model.input_names.index(input_name)\r\nq_aware_model.inputs[index].set_shape([1, 28, 28])\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nquantized_tflite_model = converter.convert()\r\n\r\n\r\nHope that helps\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43882\">No</a>\n"]}, {"number": 43881, "title": "all_ops_resolver.h:4:10: fatal error: tensorflow/lite/micro/compatibility.h: No such file or directory", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution : Windows 10\r\n- Tensorflow version : 2.1.0\r\n- Target platform : Arduino Nano 33\r\n\r\n\r\nHeader are missing in hello_world sketches. So fails to build the deployment\r\n\r\nFollowing contains the headers I used.\r\n```\r\n\r\n#include \"tensorflow/lite/micro/all_ops_resolver.h\"\r\n#include \"tensorflow/lite/micro/examples/hello_world/model.h\"\r\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n#include \"tensorflow/lite/micro/testing/micro_test.h\"\r\n#include \"tensorflow/lite/schema/schema_generated.h\"\r\n#include \"tensorflow/lite/version.h\"\r\n```", "comments": ["Hi Alagiyawanna,\r\n\r\nI'm sorry that you hit this problem. This file seems to be in the github repo: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/compatibility.h\r\n\r\nCould you try sync to the latest head and see if it solves your problem?\r\n\r\nThanks,\r\nTiezhen", "Thank you @wangtz. Now its work fine. "]}, {"number": 43880, "title": "Fix channel first support for effieicentnet", "body": "Reshape was hardcoded to channel_last, now bn_axis is used", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43880) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 43879, "title": "Fix hardcoded channel axis in Reshape", "body": "Channel axis is hardcoded for channel_last, so model did not work with channel_first", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43879) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 43877, "title": "TFLite: Getting error \"Expected bias tensor to be a vector\" when trying to convert and quantize a model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): from pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nTensorFlow Lite fails to convert a FC4 model when quantization is activated.\r\nI am getting this error when trying to perform full integer quantization with mixed 16 bits activations and 8 bits weights ( tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8) and when trying to use 8 bit quantization method (tf.lite.OpsSet.TFLITE_BUILTINS_INT8).)\r\nI did several tests. First it is working if i convert a MobileNet V1 network, so the error depends on the network architecture.\r\nBy dichotomy, i have found the layer which causes the error. I guess the error is raised when there is a ADD (AddV2 in my case) operation right after a Conv2D operation. The converter try to merge Add operation as a bias in Conv2D. In my case, the tensor dimension of the constant data to be added is [1,1,1,64]. Maybe the converter is expecting [1,64] which could explain the error raised.\r\n\r\nPlease note that  8-bit quantization is working when using TensorFlow 2.1.0, so it looks like a regression introduced between 2.1.0 and 2.3.1.\r\n\r\n**Describe the expected behavior**\r\nI expect the converter to successfully convert the model with desired quantization.\r\n \r\n**Standalone code to reproduce the issue**\r\nHere is the code to reproduce the issue:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nimport os\r\n\r\ndef representative_dataset_gen():\r\n  ds = tfds.load(\"flic\",shuffle_files=True, split='train')\r\n  assert isinstance(ds, tf.data.Dataset)\r\n  print(ds)\r\n  num_calibration_steps = 10;\r\n  for _ in range(num_calibration_steps):\r\n    example = ds.take(1)\r\n    for i in example:\r\n        image = i[\"image\"]\r\n        name = i[\"moviename\"]\r\n        print(name)\r\n        print(\"type: \" + str(type(image)))\r\n        image = tf.image.resize(image,size=[256,256])\r\n        image = tf.expand_dims(image, axis=0)\r\n        yield [image]\r\n\r\n\r\n# Convert onnx model to tflite format\r\n\r\n# convert it first in tensorflow frozen graph\r\nif not os.path.exists(\"./convertedModels/tensorflow\"):\r\n    os.makedirs(\"./convertedModels/tensorflow\")\r\n\r\n#Then convert it in tflite format\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('pretrained/model_frozen.pb', #TensorFlow freezegraph .pb model file\r\n                                                      input_arrays=['FCN_1/AlexNet/mul'], # name of input arrays as defined in torch.onnx.export function before.\r\n                                                      output_arrays=['FCN_1/Sum'],  # name of output arrays defined in torch.onnx.export function before.\r\n                                                      input_shapes={ 'FCN_1/AlexNet/mul': [1,256,256,3]}\r\n                                                      )\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\nconverter.inference_input_type = tf.int16  # or tf.uint8\r\nconverter.inference_output_type = tf.int16  # or tf.uint8\r\n\r\n\r\nconverter.representative_dataset = tf.lite.RepresentativeDataset(\r\n    representative_dataset_gen)\r\n\r\ntf_lite_model = converter.convert()\r\n# save the converted model\r\nopen('fc4_quant.tflite', 'wb').write(tf_lite_model)\r\n\r\n```\r\nHere is the input model to convert:\r\n[model_frozen.zip](https://github.com/tensorflow/tensorflow/files/5347801/model_frozen.zip)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem.\r\nHere is the traces i got:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"convert_tflite.py\", line 61, in <module>\r\n    tf_lite_model = converter.convert()\r\n  File \"/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1970, in convert\r\n    return super(TFLiteConverter, self).convert()\r\n  File \"/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1339, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 452, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, activations_type)\r\n  File \"/home/arnaud/anaconda3/envs/FC4_CONV/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 98, in calibrate_and_quantize\r\n    np.dtype(activations_type.as_numpy_dtype()).num)\r\nRuntimeError: Expected bias tensor to be a vector.\r\n```\r\n\r\n", "comments": ["Can you share your model definition?", "@bhack  model is available here: https://github.com/yuanming-hu/fc4", "Can you extract a very minimal submodel to reproduce this? If not probably will require more bandwidth/time to investigate it on our side.", "@bhack  I am not familiar with the model but the interesting part is in build_model() method in squeeze_net.py.\r\nFor your information the problematic node is 'FCN_1/AlexNet/add'", "Can you share a full standalone example that we can just copy and run or a Colab (with the Model export code)?", "@bhack sorry for that i am not familiar with the network nor Colab", "I suppose that you have a minimal code to instantiate the net from the library and export It that It could simplu run and that you can share.\n\nIf not I suggest you to open a ticket in the net repository. ", "@bhack, i have finally created a colab of the project. Here it is: https://colab.research.google.com/drive/14X0DxaKSXmNBTYJC1gXnQ_SJrr9S7jkc?usp=sharing", "I don't see the convert code to reproduce your issue.", "@bhack. I thought providing only network inference was ok as i already provided code for conversion. I will add it on Colab.", "Also I see you are using `tensorflow==1.15.0` it will be very hard to you to get TF lite bug support here for an old version.", "@bhack the model is compatible with TF 1.15 but the converter i use is from 2.3.1. I have updated the Colab with generation of frozen graph and conversion in TensorFlow Lite format.", "In your colab I see: `AttributeError: module 'tensorflow' has no attribute 'contrib'`", "@bhack That's because you had to click on \"RESTART RUNTIME\" each time there is an upgrade of tensorflow via pip. I have changed the Colab so that the restart is done programmatically.", "I had already restarted the runtime there was something else. I will re-run now", "@ntreepoint \r\nIs this still an issue.", "@Saduf2019 , yes, this is still an issue.", "@ymodak \r\nI ran the code and colab crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/cf7e9fdf53892c37f30b64b8c551e9f7/untitled454.ipynb).", "It looks like the goal here is to run the colab to generate the saved model, and then run the TF Lite converter on that saved model to get the error. However, I find that I can't successfully run the colab here:\r\n\r\nin order to generate the saved model. The colab is odd because it has a cell for installing tf-nightly but then another cell for installing tf 1.15. If I install tf-nightly, then restart the run time, I get the error listed [above](https://github.com/tensorflow/tensorflow/issues/43877#issuecomment-708392873) about no module `contrib`. If I execute the cell that pip installs tf version 1.15, I get the following error instead:\r\n\r\n```\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-d34842144e55> in <module>()\r\n      8 import os\r\n      9 from tensorflow.python.tools import freeze_graph\r\n---> 10 import tensorflow_datasets as tfds\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/tf_compat.py in ensure_tf_install()\r\n     57   if tf_version < min_tf_version:\r\n     58     raise ImportError(\r\n---> 59         \"This version of TensorFlow Datasets requires TensorFlow \"\r\n     60         f\"version >= {MIN_TF_VERSION}; Detected an installation of version \"\r\n     61         f\"{tf.__version__}. Please upgrade TensorFlow to proceed.\"\r\n\r\nImportError: This version of TensorFlow Datasets requires TensorFlow version >= 2.1.0; Detected an installation of version 1.15.0. Please upgrade TensorFlow to proceed.\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n```\r\n\r\nIt's not clear to me how this colab is supposed to work. Are we generating the saved model with TF Nightly? Or with version 1.15?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43877\">No</a>\n"]}, {"number": 43876, "title": "OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.", "body": "import numpy as np \r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.models as models\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.optimizers as optimizers\r\nfrom tensorflow.keras.models import *\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.optimizers import *\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\nfrom tensorflow.keras import backend\r\n\r\n\r\ndef unet(pretrained_weights = None,input_size = (256,256,1)):\r\n    inputs = keras.Input(shape = input_size)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\r\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\r\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\r\n    drop4 = Dropout(0.5)(conv4)\r\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\r\n\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\r\n    drop5 = Dropout(0.5)(conv5)\r\n\r\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\r\n    merge6 = concatenate([drop4,up6], axis = 3)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\r\n\r\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\r\n    merge7 = concatenate([conv3,up7], axis = 3)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\r\n\r\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\r\n    merge8 = concatenate([conv2,up8], axis = 3)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\r\n\r\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\r\n    merge9 = concatenate([conv1,up9], axis = 3)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\r\n\r\n    model = Model(inputs = inputs, outputs = conv10)\r\n\r\n    def iou(y_pred, y_true):\r\n        y_pred = tf.cast((y_pred > 0), dtype=tf.float32)\r\n        i = tf.reduce_sum(y_true * y_pred)\r\n        u = tf.reduce_sum(y_true + y_pred)\r\n        return (i / u).item()if u != 0 else u.item()\r\n\r\n    ssim1 = tf.image.ssim(inputs, conv10, max_val=255, filter_size=11,filter_sigma=1.5, k1=0.01, k2=0.03)\r\n    \r\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'ssim1', metrics = ['accuracy',iou])\r\n    \r\n    model.summary()\r\n\r\n\r\n    if(pretrained_weights):\r\n    \tmodel.load_weights(pretrained_weights)\r\n\r\n    return model\r\n\r\nmodel = unet()", "comments": ["Can you reformat correctly the code block and very that we can copy, past and directly run your example?", "import numpy as np \r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.models as models\r\nimport tensorflow.keras.layers as layers\r\nimport tensorflow.keras.optimizers as optimizers\r\nfrom tensorflow.keras.models import *\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.optimizers import *\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\nfrom tensorflow.keras import backend\r\n", "def unet(pretrained_weights = None,input_size = (256,256,1)):\r\n    inputs = keras.Input(shape = input_size)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\r\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\r\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\r\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\r\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\r\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\r\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\r\n    drop4 = Dropout(0.5)(conv4)\r\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\r\n\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\r\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\r\n    drop5 = Dropout(0.5)(conv5)\r\n\r\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\r\n    merge6 = concatenate([drop4,up6], axis = 3)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\r\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\r\n\r\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\r\n    merge7 = concatenate([conv3,up7], axis = 3)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\r\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\r\n\r\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\r\n    merge8 = concatenate([conv2,up8], axis = 3)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\r\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\r\n\r\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\r\n    merge9 = concatenate([conv1,up9], axis = 3)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\r\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\r\n    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\r\n\r\n    model = Model(inputs = inputs, outputs = conv10)\r\n\r\n    def iou(y_pred, y_true):\r\n        y_pred = tf.cast((y_pred > 0), dtype=tf.float32)\r\n        i = tf.reduce_sum(y_true * y_pred)\r\n        u = tf.reduce_sum(y_true + y_pred)\r\n    ssim1 = tf.image.ssim(inputs, conv10, max_val=1.0, filter_size=11,filter_sigma=1.5, k1=0.01, k2=0.03)\r\n    \r\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'ssim1', metrics = ['accuracy',iou])\r\n    \r\n    model.summary()\r\n\r\n\r\n    if(pretrained_weights):\r\n    \tmodel.load_weights(pretrained_weights)\r\n\r\n    return model\r\n\r\nmodel = unet()\r\n\r\n\r\n@bhack", "@gupta35 \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using?\r\n\r\nRequest you to share colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\nI am running this code in google colab.", "Can you format correctly your code block or share your colab?", "https://colab.research.google.com/drive/1mhKTeMz0-NqTAXzP3_qyO4pdE-k5E7K4?usp=sharing\r\n\r\nyou can excess this code from the above link.", "@gupta35 \r\n\r\nI have tried in colab with TF nightly version(`2.4.0-dev20201007`) and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f9aa6e340ac337f78125f6df8aa670df/untitled438.ipynb).Please, verify once and close the issue.Thanks!", "not done"]}, {"number": 43875, "title": "Unable to load Cats vs Dogs dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo. I used the same code as is given in https://www.tensorflow.org/datasets/overview, but changed the dataset from mnist to cats_vs_dogs\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10.0.18362, 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo. Using a PC\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\nTensorflow 2.3.0\r\nTensorflow datasets 3.2.1\r\n- Python version:\r\nPython 3.5\r\n- Bazel version (if compiling from source):\r\nNA\r\n- GCC/Compiler version (if compiling from source):\r\nNA\r\n- CUDA/cuDNN version:\r\nCUDA driver version   : 10020\r\n- GPU model and memory:\r\nNVIDIA GeForce 940MX 4GB\r\n\r\n**Describe the current behavior**\r\nI am unable to download the cats_vs_dogs dataset through the tensorflow datasets.\r\n\r\n**Describe the expected behavior**\r\nI should be able to load a variable with the dataset.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nds = tfds.load('cats_vs_dogs', split = 'train', shuffle_files = True)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-10-45c82e970652> in <module>()\r\n----> 1 ds, train = tfds.load('cats_vs_dogs', split = 'train', shuffle_files = True) #download cats vs dogs file for this projectb\r\n\r\n~\\AppData\\Roaming\\Python\\Python35\\site-packages\\wrapt\\wrappers.py in __call__(self, *args, **kwargs)\r\n    565 \r\n    566         return self._self_wrapper(self.__wrapped__, self._self_instance,\r\n--> 567                 args, kwargs)\r\n    568 \r\n    569 class BoundFunctionWrapper(_FunctionWrapperBase):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     67     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     68     _check_required(fn, kwargs)\r\n---> 69     return fn(*args, **kwargs)\r\n     70 \r\n     71   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py in load(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\r\n    369   if download:\r\n    370     download_and_prepare_kwargs = download_and_prepare_kwargs or {}\r\n--> 371     dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n    372 \r\n    373   if as_dataset_kwargs is None:\r\n\r\n~\\AppData\\Roaming\\Python\\Python35\\site-packages\\wrapt\\wrappers.py in __call__(self, *args, **kwargs)\r\n    604 \r\n    605             return self._self_wrapper(self.__wrapped__, self._self_instance,\r\n--> 606                     args, kwargs)\r\n    607 \r\n    608         else:\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py in disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     67     _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     68     _check_required(fn, kwargs)\r\n---> 69     return fn(*args, **kwargs)\r\n     70 \r\n     71   return disallow_positional_args_dec(wrapped)  # pylint: disable=no-value-for-parameter\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py in download_and_prepare(self, download_dir, download_config)\r\n    374           self._download_and_prepare(\r\n    375               dl_manager=dl_manager,\r\n--> 376               download_config=download_config)\r\n    377 \r\n    378           # NOTE: If modifying the lines below to put additional information in\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py in _download_and_prepare(self, dl_manager, download_config)\r\n   1017     super(GeneratorBasedBuilder, self)._download_and_prepare(\r\n   1018         dl_manager=dl_manager,\r\n-> 1019         max_examples_per_split=download_config.max_examples_per_split,\r\n   1020     )\r\n   1021 \r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py in _download_and_prepare(self, dl_manager, **prepare_split_kwargs)\r\n    949 \r\n    950       # Prepare split will record examples associated to the split\r\n--> 951       self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    952 \r\n    953     # Update the info object with the splits.\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py in _prepare_split(self, split_generator, max_examples_per_split)\r\n   1032                                      hash_salt=split_generator.name)\r\n   1033     for key, record in utils.tqdm(generator, unit=\" examples\",\r\n-> 1034                                   total=split_info.num_examples, leave=False):\r\n   1035       example = self.info.features.encode_example(record)\r\n   1036       writer.write(key, example)\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tqdm\\notebook.py in __iter__(self, *args, **kwargs)\r\n    232     def __iter__(self, *args, **kwargs):\r\n    233         try:\r\n--> 234             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\r\n    235                 # return super(tqdm...) will not catch exception\r\n    236                 yield obj\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tqdm\\std.py in __iter__(self)\r\n   1163 \r\n   1164         try:\r\n-> 1165             for obj in iterable:\r\n   1166                 yield obj\r\n   1167                 # Update and possibly print the progressbar.\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\image_classification\\cats_vs_dogs.py in _generate_examples(self, archive)\r\n     87     \"\"\"Generate Cats vs Dogs images and labels given a directory path.\"\"\"\r\n     88     num_skipped = 0\r\n---> 89     for fname, fobj in archive:\r\n     90       res = _NAME_RE.match(fname)\r\n     91       if not res:  # README file, ...\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py in iter_zip(arch_f)\r\n    197     for member in z.infolist():\r\n    198       extract_file = z.open(member)\r\n--> 199       if member.is_dir():  # Filter directories  # pytype: disable=attribute-error\r\n    200         continue\r\n    201       path = _normpath(member.filename)\r\n\r\nAttributeError: 'ZipInfo' object has no attribute 'is_dir'\r\n", "comments": ["I cannot reproduce this. \r\nWe don't officially support anaconda here. Can you try again installing TF with the official installation guide?", "Its working fine for the mnist dataset. Its seems the problem is only with the cats vs dogs dataset. I'll do as you ask tonight and let you know.", "@TheNightBaron,\r\nI was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a1786e6ecb898dce2419f7a25721d219/43875.ipynb). \r\n\r\nCould you please check if you're facing the same issue in a Python virtual environment (i.e. outside Anaconda environment)? Thanks!", "I have python only on Anaconda. Do I need to install python separately for trying this?", "We don't directly support Anaconda and I cannot reproduce this in TF with the official install guide", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43874, "title": "remove error msg if >1 adapter can handle data (2)", "body": "This issue was touched upon in [1](https://github.com/tensorflow/tensorflow/issues/41328), [2](https://github.com/tensorflow/tensorflow/issues/33811) and [3](https://github.com/tensorflow/tensorflow/issues/33734). I believe that it should be handled differently:\r\n\r\nIf there is more than single DataAdapter which can handle certain data - great, just pick the best of them and return it from the function. It should not be the user problem if developers made adapters that can handle diverse types of data.\r\nIf some DataAdapters better fit than others. they can be prioritized by changing order in `ALL_ADAPTER_CLS`. Alternatively, adapters should be selected by type of data, with 1:1 clear match of data type and adapter.\r\n\r\nHere is an example code generating this error.\r\n```\r\nimport tensorflow as tf\r\n\r\nfeatures =  tf.random.normal(shape=(100, 1, 10))\r\nlabels = tf.random.normal((100,1,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((features,labels))\r\nds_iter = iter(dataset) # tensorflow.python.data.ops.iterator_ops.OwnedIterator\r\nfeatures.shape, labels.shape\r\n\r\nx = tf.keras.layers.Input(shape=[10])\r\ny_pred = tf.keras.layers.Dense(1, activation='sigmoid', name=\"L0\")(x)\r\nmodel = tf.keras.Model(x, y_pred)\r\nmodel.compile(optimizer='sgd', loss='mse',)\r\nmodel.fit(ds_iter, epochs=1)\r\n```\r\n\r\nThe code above triggers error `RuntimeError: Data adapters should be mutually exclusive for handling inputs`, however this is a good kind of problem to have. There are 2 adapters that claim ability to  handle such iterator: `CompositeTensorDataAdapter` and `GeneratorDataAdapter`.\r\n\r\nI suggest not to throw `RuntimeError: Data adapters should be mutually exclusive for handling inputs ` but instead just return `adapter_cls[0]`. The change requires deleting [5 lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L958-L96) in `tf.python.keras.engine.data_adapter.select_data_adapter`\r\n\r\n\r\nLater I ran tests with `tensorflow.python.data.ops.iterator_ops.OwnedIterator` forcing it to choose one of the DataAdapters:\r\n1) `GeneratorDataAdapter` works fine with it,\r\n\r\n2) `CompositeTensorDataAdapter` fails with error `AttributeError: 'IteratorSpec' object has no attribute '_to_batched_tensor_list'` [error stack here](https://pastebin.com/g1RxEhdc) Apparently some change is needed either in `CompositeTensorDataAdapter.can_handle()` or in the handling process itself.\r\n\r\nThis PR repeats [my previous closed PR](https://github.com/tensorflow/tensorflow/pull/43598) reviewed by @gbaned, now it separates 2 unrelated commits into separate PRs.", "comments": ["Thanks for sending the PR and reporting the issue.\r\n\r\nI think when we originally design the data adapter, we want each adapter to only handle certain use case, and not overlapping with each other, which is why we explicitly raise this error. Having ALL_ADAPTER_CLS serving as a priority list might not be a good idea, based on the original design for mutually exclusive, and it is not clear that which adapter is better in certain cases.\r\n\r\nIn this particular case, I guess the CompositeTensorDataAdapter might not be correct handler for it, and we need to check why it is selected (most likely due to the instance type check in https://github.com/tensorflow/tensorflow/blob/0e87b6606ef7789fedbd08c0bddc8dc591504d7e/tensorflow/python/keras/engine/data_adapter.py#L528).", "Will send a fix for the issue you facing very soon."]}, {"number": 43873, "title": "fix keras progbar output collision (2)", "body": "The suggested edit ensures that `keras.model.fit()` progbar output is complete before any output from custom callbacks appears on epoch end. Basically this edit puts the standard `ProgbarLogger` first in the list of callbacks. \r\n\r\n**Current behavior:**\r\nwhen training Model using Model.fit() with verbose=1 (for progress bar) and keras.callbacks.Callback() with on_epoch_end() function I observe that custom callback output happens BEFORE completion of the epoch progress output, like this:\r\n```\r\nEpoch 1/2\r\n 63/125 [==============>...............] - ETA: 0s - loss: 43.2549 - mean_absolute_error: 4.6003\r\nCALLBACK MESSAGE ON END EPOCH 0\r\n125/125 [==============================] - 0s 794us/step - loss: 37.6690 - mean_absolute_error: 4.6575\r\n\r\n```\r\n**Expected behavior:**\r\ncustom callback output should happen AFTER completion of the epoch progress output:\r\n```\r\nEpoch 1/2\r\n125/125 [==============================] - 0s 900us/step - loss: 36.9897 - mean_absolute_error: 4.8264\r\nCALLBACK MESSAGE ON END EPOCH 0\r\n```\r\nOriginally discussed at [tf github](https://github.com/tensorflow/tensorflow/issues/43184) with problem description and gist examples. Latest colab example is [here](https://colab.research.google.com/gist/poedator/3630b1cdec32ff6ef6ccdf6b63c4957a/custom_callback.ipynb).\r\n\r\nThis PR was attempted before(https://github.com/tensorflow/tensorflow/pull/43477) and was in process of review by @gbaned. Resubmitting separately from the other commit that caused conflict", "comments": []}, {"number": 43872, "title": "[Encountered unresolved custom op: ResizeNearestNeighbor] when running tflite model ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): 2.2.3\r\n\r\n**Run Code**\r\ninterpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n**Error**\r\nRuntimeError: Encountered unresolved custom op: ResizeNearestNeighbor.Node number 33 (ResizeNearestNeighbor) failed to prepare.\r\n\r\nI looked into many things but could not find the solution.\r\n\r\n", "comments": ["@sieme97 \r\nPlease refer to [this link](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/resize-nearest-neighbor) and let us know if it helps. [also #37003]\r\nPlease provide with simple stand alone indented code to replicate the issue or a colab gist with the error reported, also try on tf-nightly and let us know if the issue exists.\r\n", "I already looked into the issues section but could not get it resolved. I also tried on tf-nightly but no success. The model and code is from https://github.com/foamliu/Deep-Image-Matting\r\nThe model is converted to tflite successfully. Issue is with the running of model.", "    sess = tf.keras.backend.get_session()\r\n    converter = tf.contrib.lite.TocoConverter.from_session(sess,final.inputs,final.outputs)\r\n    converter.allow_custom_ops = True\r\n    converter.experimental_new_converter = True\r\n    converter=converter.convert()\r\n    interpreter=tf.contrib.lite.Interpreter(model_content=converter)\r\n    open('model.tflite', 'wb').write(converter)\r\n\r\nI am converting the model from the session as it has custom layer. For conversion, I am using tf 1.9.0 version. I also tried the conversion with tf 2.2.0 but could not convert it because of custom layer.\r\n\r\n", "@sieme97\r\nPlease share complete indented stand alone code such that we can replicate the issue faced or if possible share a colab gist with the error faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43871, "title": "Inconsistent keras.layers.normalization version", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Docker image python 3.8 (debian distro) & Google Colab\r\n- TensorFlow installed from pip\r\n- TensorFlow version 2.3.0\r\n- Python version: docker python 3.8.2 & Colab python 3.6.9\r\n\r\n**Describe the current behavior**\r\nWhen using `BatchNormalization `from different keras tree got a different result. In my case the model failed to converge.\r\n`tensorflow.keras` & `tensorflow.python.keras` both using the same version of keras but if we dump the `BatchNormalization `layer the version is different. \r\n\r\n**Describe the expected behavior**\r\nThe same version of keras, must use the same version of normalization.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport sys\r\n\r\nimport tensorflow\r\n\r\nfrom tensorflow import keras as k1\r\nfrom tensorflow.python import keras as k2\r\n\r\nfrom tensorflow.keras.layers import BatchNormalization as BN1\r\nfrom tensorflow.python.keras.layers import BatchNormalization as BN2\r\n\r\nprint('python version:', sys.version)\r\nprint('tf version:', tensorflow.__version__)\r\nprint('keras version:', k1.__version__)\r\nprint('python.keras version:', k2.__version__)\r\n\r\nbatch_normalization_layer_keras = BN1()\r\nprint('keras:', batch_normalization_layer_keras)\r\nbatch_normalization_layer_python_keras = BN2()\r\nprint('python.keras:', batch_normalization_layer_python_keras)\r\n```\r\n\r\n**Output**\r\n```\r\npython version: 3.6.9 (default, Jul 17 2020, 12:50:27) \r\n[GCC 8.4.0]\r\ntf version: 2.3.0\r\nkeras version: 2.4.0\r\npython.keras version: 2.4.0\r\nkeras: <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f2aba63f390>\r\npython.keras: <tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7f2aba63fb70>\r\n```", "comments": ["I have tried in colab with TF version 2.3,nightly version(`2.4.0-dev20201007`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0652ca81a568ec8ec31c0f7fb59da8b9/untitled434.ipynb).Thanks!", "@tiok-cek1 Anything under `tf.python.*` is private, intended for development only, rather than public use. Importing from tensorflow.python or any other modules (including import tensorflow_core...) is not supported, and can break unannounced. For full response on similar issue, you can check here. https://github.com/tensorflow/tensorflow/issues/33075", "I opened tf1 and tf2 documentation, both provide the same default parameters. No information about v1 and v2 normalization, only know to use v1 must set [_USE_V2_BEHAVIOR](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/layers/normalization.py#L147-L149) to False.\r\nI ran some code from internet, the optimization failed to converge because of this version changes (code can run without error but produce a different result).\r\n\r\nI think this issue is done, thanks @jvishnuvardhan for explaining the difference of `tf.python.keras` and `tf.keras`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43871\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43871\">No</a>\n"]}, {"number": 43870, "title": "Bump docker nightly cuda version to 11.1", "body": "As the title says. docker nightly-gpu uses cuda 11.0 instead of the latest 11.1, should we bump the version to 11.1?", "comments": ["/cc @angerson ", "I don't think TensorFlow officially support 11.1 yet -- I'd like to wait until our own packages build with 11.1 first.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43870\">No</a>\n"]}, {"number": 43869, "title": "No zero values inside callback functions", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jupyter macos\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nInside tensorflow callback function on_epoch_end. Trying to use 0 value inside a str function like this str(0)\r\n**Describe the expected behavior**\r\nShould be able to capture it. print(str(0.0) does not print anything\r\n**Standalone code to reproduce the issue**\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        print(str(0.0))", "comments": ["@mohamedzayan19 \r\nPlease share simple stand alone code to replicate issue reported or if possible share a colab gist with error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43869\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43869\">No</a>\n"]}, {"number": 43868, "title": "Refactor SparseApplyFtrl CPU kernel into class", "body": "This is in preparation for adding a GPU implementation.\r\n\r\n(This is the second split of https://github.com/tensorflow/tensorflow/pull/43299).\r\n\r\ncc @nluehr @sanjoy \r\n\r\nEdit: This PR makes no functional change except for the indexing error message (which is changed to match that of SparseApplyKerasMomentumOp which has the same code structure).", "comments": ["Description edited to describe NFC."]}, {"number": 43867, "title": "Patch kissfft to remove sys/types.h include from kiss_fft.h.", "body": "Fixes #43848 \r\n\r\nThe sys/types.h-include in kiss_fft.h causes issues when building micro_seech using ARM Mbed OS and ARM Compiler (armclang), since sys/types.h isn't available as it is in gcc.\r\n\r\nThe PR removes the #include <sys/types.h> from kiss_ff.h in the download_and_extract.sh script.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}]