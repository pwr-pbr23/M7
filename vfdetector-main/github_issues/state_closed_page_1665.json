[{"number": 2955, "title": "Readmea for tensorboard not on website", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Browser issue with readme tensorboard\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. not tensorproblem but website\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Go to https://www.tensorflow.org/versions/r0.9/how_tos/summaries_and_tensorboard/index.html and click on the readme tensorboard. Give you 404 not found error.\n### What have you tried?\n1. nothing, its on the website\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n![screen shot 2016-06-19 at 10 20 53 pm](https://cloud.githubusercontent.com/assets/1855278/16181716/2cfc7cd4-366c-11e6-92e2-ed67906c95eb.png)\n", "comments": ["Closing as duplicate of #2813\n"]}, {"number": 2954, "title": "tf.learn update", "body": "Update contrib/{learn,losses,layers,metrics,framework,slim} to reflect changes since r0.9 branch split.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Ignoring CLA because all commits are cherry-picks.\n", "Giving up. This is not worth the risk of breaking all sorts of other things.\n"]}, {"number": 2953, "title": "Fractional pooling", "body": "Is there or (will there be) a \"Fractional Pooling\" support in tensorflow?\n", "comments": ["Apparently we do have some Fractional pooling ops in development which should (hopefully) make it into the external tree shortly.  Will leave this open as a tracking bug.\n", "This should be coming out soon.\n", "Any word on this?\n", "Sorry for the delay. It is still making its way through code review, but really should be soon now!\n"]}, {"number": 2952, "title": "learn datasets not found", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nI have tensorflow 0.8.0 installed and when i import learn the datasets are not an option\n\n![screen shot 2016-06-19 at 3 00 22 pm](https://cloud.githubusercontent.com/assets/8854444/16180075/dbe3ff6c-362e-11e6-8f5a-4a7cf575f5cf.png)\n![screen shot 2016-06-19 at 3 02 10 pm](https://cloud.githubusercontent.com/assets/8854444/16180076/dbfd3fa4-362e-11e6-836d-d98c68400389.png)\n", "comments": ["0.8.0 doesn't contain (all of) tf.contrib.learn. Can you check with 0.9?\n", "works! thank you. I didn't think I missed that because when I prompted \"pip install tensorflow --upgrade\" it returned tf 0.8.0 was up to date. \n", "Because we're not in pypi, sadly.\nOn Mon, Jun 20, 2016 at 07:06 Mia Hunsicker notifications@github.com\nwrote:\n\n> works! thank you. I didn't think I missed that because when I prompted\n> \"pip install tensorflow --upgrade\" it returned tf 0.8.0 was up to date.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2952#issuecomment-227151786,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_YYU_CGN5lm_h7_uxFpUoVuzo80Dks5qNp5QgaJpZM4I5QyO\n> .\n"]}, {"number": 2951, "title": "Installation of Tensorflow on Mac OS X 10.11.5 wheel not supported?", "body": "Tried installing Tensorflow but wheel not supported on this platform. \n### Environment info\n\nOperating System: Mac OS X 10.11.5\n\nIf installed from binary pip package, provide:\n1. sudo -H pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n2. Traceback (most recent call last):\n   File \"<string>\", line 1, in <module>\n   ImportError: No module named tensorflow\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. sudo -H pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n### Logs or other output that would be helpful\n\ntensorflow-0.8.0-py2-none-any.whl is not a supported wheel on this platform.\n", "comments": ["Can you post the output of pip --version? It's possible that your pip is installed for python3.\n", "@martinwicke yep spot on that seems to be the issue. \n\n`pip 8.1.2 from /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (python 3.5)`\n\nI want to use Tensorflow on Python2. which version of pip is suitable for this?\n", "Just run \"easy_install-2.7 pip\" (or similar), that should make a pip2.7 and\nprobably also moves the pip symlink to the py2 version.\nOn Mon, Jun 20, 2016 at 12:32 cyriltw notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke yep spot on that seems to\n> be the issue.\n> \n> pip 8.1.2 from\n> /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages\n> (python 3.5)\n> \n> I want to use Tensorflow on Python2. which version of pip is suitable for\n> this?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2951#issuecomment-227244935,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_ZiYNx08fLTW6NOfth9v9jlcL40Eks5qNurEgaJpZM4I5NTm\n> .\n"]}, {"number": 2950, "title": "Error when running resnet.py", "body": "System: Mac OS Yosemite, Python 2.7.\n\n```\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-1-6138fced2cf1> in <module>()\n    150   # Train model and save summaries into logdir.\n    151   classifier.fit(\n--> 152       mnist.train.images, mnist.train.labels, logdir='models/resnet/')\n    153 \n    154   # Calculate accuracy.\n\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in fit(self, X, y, monitor, logdir)\n    225         if not self.continue_training or not self._initialized:\n    226             # Sets up model and trainer.\n--> 227             self._setup_training()\n    228             self._initialized = True\n    229         else:\n\n/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc in _setup_training(self)\n    146             # Create model's graph.\n    147             self._model_predictions, self._model_loss = self.model_fn(\n--> 148                 self._inp, self._out)\n    149 \n    150             # Set up a single operator to merge all the summaries\n\n<ipython-input-1-6138fced2cf1> in res_net(x, y, activation)\n    111       # shortcut connections that turn the network into its counterpart\n    112       # residual function (identity shortcut)\n--> 113       net = conv + net\n    114 \n    115       try:\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in binary_op_wrapper(x, y)\n    516       assert isinstance(x, ops.Tensor)\n    517       y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\n--> 518       return func(x, y, name=name)\n    519 \n    520   ops.Tensor._override_operator(\"__%s__\" % op_name, binary_op_wrapper)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc in add(x, y, name)\n     42     A `Tensor`. Has the same type as `x`.\n     43   \"\"\"\n---> 44   return _op_def_lib.apply_op(\"Add\", x=x, y=y, name=name)\n     45 \n     46 \n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    653         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    654                          input_types=input_types, attrs=attr_protos,\n--> 655                          op_def=op_def)\n    656         outputs = op.outputs\n    657         return _Restructure(ops.convert_n_to_tensor(outputs), output_structure)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2154                     original_op=self._default_original_op, op_def=op_def)\n   2155     if compute_shapes:\n-> 2156       set_shapes_for_outputs(ret)\n   2157     self._add_op(ret)\n   2158     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1610       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1611                          % op.type)\n-> 1612   shapes = shape_func(op)\n   1613   if len(op.outputs) != len(shapes):\n   1614     raise RuntimeError(\n\n/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in _BroadcastShape(op)\n   1388     else:\n   1389       raise ValueError(\"Incompatible shapes for broadcasting: %s and %s\"\n-> 1390                        % (shape_x, shape_y))\n   1391   return [tensor_shape.TensorShape(return_dims)]\n   1392 \n\nValueError: Incompatible shapes for broadcasting: (?, 14, 14, 128) and (?, 14, 14, 256)\n```\n", "comments": ["@ilblackdragon  could you please take a look at this?  \n", "@RafaelCosman what is the version of TF are you running?\n\nLet me try it out to see what's happening.\n", "@ilblackdragon There were some PRs regarding resnet.py. Seems like unnecessary indentation was introduced from some previous internal merges. \n", "@ilblackdragon Is this still an issue? / Should we close the bug for lack of activity?\n", "I think it was fixed, closing due to inactivity either way.\n"]}, {"number": 2949, "title": "Added camera example for Raspberry Pi", "body": "", "comments": []}, {"number": 2948, "title": "Automatically merge identical ops", "body": "When an op constructing function is called for multiple times on the same set of inputs, each of these calls adds a new op to the graph, which leads to extra computation. I understand that this could be avoided in most cases by reusing the output tensor. However, this can't be easily done in some situations, e.g. when using different optimizers for different parts of a neural network. Therefore, IMHO, it would be beneficial to have the framework automatically merge identical ops.\n", "comments": ["BTW PR #2747 has logic to detect when Python function call would create an op that's already in the graph and reuse that op if so\n", "This is really a general usage question and as such is better suited for StackOverflow. Please consider re-asking there and tag it with the `tensorflow` tag.\n\nTensorFlow graphs are extensively rewritten before execution, and one of the optimizations is common subexpression elimination which would remove duplicate code in cases where it does not affect the result.  So, in most cases this should not be a _performance_ issue.  \n\nIf you have a specific model where this is a performance problem, please consider capturing an execution Timeline using the RunOptions to session.Run() and reopen this issue.  \n\nSimilarly, if the graphdef is excessively large, please consider posting an example of the code idiom which is resulting in duplication.\n"]}, {"number": 2947, "title": "Added basic Raspberry Pi example", "body": "Also fixed a math error in the iOS examples image decoding.\n", "comments": ["I really hope it de-duped the image, we really don't want images checked into git. :(((\n", "I checked when I used it multiple times in other examples earlier, and it was deduped, so I think we're good. I understand why it's not a good idea in general though!\n"]}, {"number": 2946, "title": "Enable tf.square() for SparseTensor", "body": "Enabled `tf.square()` for `SparseTensor`. Added tests and verified locally. This partially addresses #1828.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 2945, "title": "Cast tf.shape() output to int32 for SparseTensor", "body": "`input.shape` gives a `Tensor` of type `int64`, where `input` is a `SparseTensor`. Cast the output of `tf.shape()` to `int32` for a `SparseTensor` to be consistent with the doc. Tested and verified locally. This completes the changes for #1968.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please.\n", "Jenkins, test this please.\n", "LGTM pending Jenkins.\n"]}, {"number": 2944, "title": "Errors while running Translate.py sample code with multiple GPUS", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Ubuntu\n\nInstalled version of CUDA and cuDNN: 7.5 and 7\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.8\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. running the translate.py example\n   2.Crashes while creating models after tokenizing the training data.\n\nThe sample code seems to run fine with single GPU of 4 GB memory. I am using AWS GPU instances. But it works fine for basic values, ie vocab size of 40000 and size of 512. I want to train on a vocab size of 500000 and size of 1024. This is the reason why I opted to go for 4 GPUs now. \n(In AWS terms, g2.2xlarge to g2.8xlarge). \n![g2-8xlarge-crash](https://cloud.githubusercontent.com/assets/5551707/16170060/c599bef8-3562-11e6-9723-4a38999c6a43.png)\n![nvidia-smi-details](https://cloud.githubusercontent.com/assets/5551707/16170062/d7a121fe-3562-11e6-928e-3270ebaa44b9.png)\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["@lukaszkaiser Can you give me any inputs?\n", "In order to understand what is happening here, you will at least need to provide the command lines you are using and the full output of the commands which are failing.  (e.g. There is not enough information in the two screenshots above)\n\n@lukaszkaiser  Are there any known issues with running this model multi-GPU?    \n", "I don't know of any issues. Could you provide a text log with full error message? (The screens don't seem to include it all, a text file with the whole error message would be more helpful.)\n", "[logs.txt](https://github.com/tensorflow/tensorflow/files/340736/logs.txt)\nSorry for reporting this issue, I feel this is not a bug but a problem related to my system memory.\nI tried running with varied size parameter. it runs for size 256 and 512 but for 1024 there is a crash. I am giving 22M sentences, and vocab size of 1M.  When I checked the logs for 1024, I saw that the crash is because of unavailability of memory. This is on a AWS instance with 4 GPU's of 4Gb each. I have attached the logs. \n"]}, {"number": 2943, "title": "Extracting features from mixed layer in inception-v3 ", "body": "Hi,\nFor my project I need to extract the features from the mixed layer in inception-v3. But, for doing so I need the name of various tensor layer.\nas far as I know \n`pool_1:0 correspond to (35X35X192) feature matrix`\n`pool_3:0 correspond to (2048,) feature matrix`\n`conv_1:0 correspond to ((147X147X32)) feature matrix`\n`conv_2:0 correspond to (147X147X64) feature matrix`\n`conv_3:0 correspond to (73X73X80) feature matrix`  \n`conv_4:0 correspond to (71X71X192) feature matrix`\n\nI am interested in final mixed layer [8X8X2048] `mixed_10` as mentioned in `model.txt` Can you please tell me how to extract this layer \n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 2942, "title": "Memory Leak in Queue", "body": "Version: nightly built for Python2+Linux+GPU about a week ago (perhaps earlier). Cannot test on latest binary due to #2939.\n\nThe following code:\n\n``` python\nimport tensorflow as tf\nimport numpy as np\nimport os\n\nif __name__ == '__main__':\n    input_vars = tf.placeholder(tf.float32, shape=(None, 60, 60, 4))\n    queue = tf.FIFOQueue(1000, [tf.float32], name='queue')\n    enqueue_op = queue.enqueue([input_vars])\n    output = tf.reduce_mean(queue.dequeue())\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n\n    with sess.as_default():\n        d = np.random.rand(16, 60, 60, 4)\n        for k in range(990):\n            enqueue_op.run(feed_dict={input_vars: d})\n        cmd = \"ps u \" + str(os.getpid()) + \" | tail -n 1 | awk '{print $6}'\"\n        while True:\n            print \"mem in KB: \"\n            os.system(cmd)\n            for k in range(300):\n                enqueue_op.run(feed_dict={input_vars: d})\n                output.eval()\n```\n\nquickly consumes 10GB main memory.\n\nTested on ArchLinux as well as CentOS7.\n", "comments": ["See same thing on today head\n", "After investigating this, it seems most likely that the cause is heap fragmentation, arising from the creation of a large number of NumPy arrays. The current `tf.Session` implementation is (inadvertantly) copying the incoming `feed_dict` values into new NumPy arrays on every step, and I've got a fix for that pending. This leads to a large amount of churn on the heap, and the default `malloc()` implementation doesn't appear to handle this well.\n\nOnce the fix is in, your example code should not leak, but we would recommend using [`tcmalloc`](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) for more realistic programs. Running the same code with `tcmalloc` enabled shows no leak at all with unmodified TensorFlow 0.9. An alternative workaround is to set the `malloc()` options to be more aggressive about `mmap()`-ing for large allocations. For example, setting following the environment variable also eliminated the leak for me:\n\n```\n$ MALLOC_MMAP_THRESHOLD_=100000 python ...\n```\n", "Fixed by f68cbf1\n"]}, {"number": 2941, "title": "Branch 125215862", "body": "", "comments": []}, {"number": 2940, "title": "Installing Tensorflow with GPU support on os x 10.11", "body": "After configuring and running \n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` \nI am getting this error during the build:\n\n`ERROR: /pathtotensorflow/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTraceback (most recent call last):\n  File \"/private/var/tmp/_bazel_xxx/2ba3b3e08313f86d16eb1de7b54bf064/tensorflow/bazel-out/host/bin/tensorflow/contrib/session_bundle/example/export_half_plus_two.runfiles/tensorflow/contrib/session_bundle/example/export_half_plus_two.py\", line 33, in <module>\n    from tensorflow.contrib.session_bundle import exporter\nImportError: No module named session_bundle\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build`\n### Environment info\n\nOperating System: os x el Capitan\n\nInstalled version of CUDA and cuDNN:  \n`-rwxr-xr-x  1 root  wheel  8280 Apr 12 23:02 /usr/local/cuda/lib/libcuda.dylib\nlrwxr-xr-x  1 root  wheel    45 Apr 12 23:03 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\nlrwxr-xr-x  1 root  wheel    50 Apr 12 23:03 /usr/local/cuda/lib/libcudart.7.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\nlrwxr-xr-x  1 root  wheel    46 Apr 12 23:03 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\nlrwxr-xr-x  1 root  wheel    49 Apr 12 23:03 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\nlrwxr-xr-x  1 root  admin    47 Jun 16 11:25 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudnn.5.dylib\nlrwxr-xr-x  1 root  admin    45 Jun 16 11:25 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudnn.dylib\nlrwxr-xr-x  1 root  admin    48 Jun 16 11:25 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudnn_static.a`\n", "comments": ["Hey guys\n\nI actually pass the `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` process.\nBut there is no wheel file in the `bazel-bin/tensorflow/tools/pip_package/` and \nthe `bazel-bin/tensorflow/tools/pip_package/build_pip_package` is not a directory.\n\nAnyone has any idear?\n(I'm also on OS X 10.11, RMBP 2012 with GeForce GT 650M)\n", "You have to run bazel-bin/tensorflow/tools/pip_package/build_pip_package.\nSee the installation instructions.\nOn Tue, Jun 21, 2016 at 06:28 Velkan.Xu notifications@github.com wrote:\n\n> Hey guys\n> \n> I actually pass the bazel build -c opt --config=cuda\n> //tensorflow/tools/pip_package:build_pip_package process.\n> But there is no wheel file in the bazel-bin/tensorflow/tools/pip_package/\n> and\n> the bazel-bin/tensorflow/tools/pip_package/build_pip_package is not a\n> directory.\n> \n> Anyone has any idear?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-227439086,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_Ym-8fCQzn1kP95IqZ1S5I7OZMl2ks5qN-b0gaJpZM4I4z4H\n> .\n", "Hi Martin:\n\nThanks to reply this, really.\n\nI run 'bazel-bin/tensorflow/tools/pip_package/build_pip_package' but got a\nerror 'error: invalid command 'bdist_wheel''\n\nI spent a day googled it and some said I need to install the wheel and\nsetuptools package which I already did. And this error still there.\nSome said maybe there was a conflict between the old version of setuptools\nand pip, but setuptools and pip on my macbook were both updated. And the error\nstill there.\n\nThere is a closed issue: https://github.com/tensorflow/tensorflow/issues/348\nThis guy andyyuan78 https://github.com/andyyuan78 said that he got this\nerror before and solved it by run 'pip2 install wheel'.\nI tried both 'pip install' and 'pip3 install' but it didn't work.\nI'm using Python3.5 and this is the only Python I have on my macbook.\n\nAny ideas?\n\nMartin Wicke notifications@github.com\u4e8e2016\u5e746\u670823\u65e5\u5468\u56db \u4e0a\u534811:32\u5199\u9053\uff1a\n\n> You have to run bazel-bin/tensorflow/tools/pip_package/build_pip_package.\n> See the installation instructions.\n> On Tue, Jun 21, 2016 at 06:28 Velkan.Xu notifications@github.com wrote:\n> \n> > Hey guys\n> > \n> > I actually pass the bazel build -c opt --config=cuda\n> > //tensorflow/tools/pip_package:build_pip_package process.\n> > But there is no wheel file in the bazel-bin/tensorflow/tools/pip_package/\n> > and\n> > the bazel-bin/tensorflow/tools/pip_package/build_pip_package is not a\n> > directory.\n> > \n> > Anyone has any idear?\n> > \n> > \u2014\n> > You are receiving this because you were assigned.\n> > \n> > Reply to this email directly, view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-227439086\n> > ,\n> > or mute the thread\n> > <\n> > https://github.com/notifications/unsubscribe/AAjO_Ym-8fCQzn1kP95IqZ1S5I7OZMl2ks5qN-b0gaJpZM4I4z4H\n> > \n> > .\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-227940782,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ADmW7B6IUAM_ftseeymPcaSblzZs3PIMks5qOf5IgaJpZM4I4z4H\n> .\n", "I ran into a similar issue with Python 2.7.11 on OS X 10.11.5 + MBP.\n\n$ ./configure\nPlease specify the location of python. [Default is /usr/local/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n\nSetting up Cuda include\nSetting up Cuda lib\nSetting up Cuda bin\nSetting up Cuda nvvm\nSetting up CUPTI include\nSetting up CUPTI lib64\nConfiguration finished\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n.......\n.......\n1 warning generated.\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so [for host]:\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nINFO: From Linking tensorflow/python/_pywrap_tensorflow.so:\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nERROR: /Users/peter_wang/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1: Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 245.\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 1826.337s, Critical Path: 1821.08s\n\nTo verify CUDA install:\n$ ./bin/x86_64/darwin/release/deviceQuery\n./bin/x86_64/darwin/release/deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\nDetected 1 CUDA Capable device(s)\n\nDevice 0: \"GeForce GT 650M\"\n  CUDA Driver Version / Runtime Version          7.5 / 7.5\n  CUDA Capability Major/Minor version number:    3.0\n  Total amount of global memory:                 512 MBytes (536543232 bytes)\n  ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\n  GPU Max Clock rate:                            405 MHz (0.41 GHz)\n  Memory Clock rate:                             2000 Mhz\n  Memory Bus Width:                              128-bit\n  L2 Cache Size:                                 262144 bytes\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n  Total amount of constant memory:               65536 bytes\n  Total amount of shared memory per block:       49152 bytes\n  Total number of registers available per block: 65536\n  Warp size:                                     32\n  Maximum number of threads per multiprocessor:  2048\n  Maximum number of threads per block:           1024\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n  Maximum memory pitch:                          2147483647 bytes\n  Texture alignment:                             512 bytes\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\n  Run time limit on kernels:                     Yes\n  Integrated GPU sharing Host Memory:            No\n  Support host page-locked memory mapping:       Yes\n  Alignment requirement for Surfaces:            Yes\n  Device has ECC support:                        Disabled\n  Device supports Unified Addressing (UVA):      Yes\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n  Compute Mode:\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime Version = 7.5, NumDevs = 1, Device0 = GeForce GT 650M\nResult = PASS\n", "Hi peter:\n\nI actually pass the build process.\n\nMaybe you want set a cuda capability during the tensorflow configuration?\nWhen it like:\n'Please specify a list of comma-separated Cuda compute capabilities you\nwant to build with.'\nEnter '3.0' for the gt 650m.\n\nAnd your error log said:\n'Use --verbose_failures to see the command lines of failed build steps.'\nMaybe you can get some details by doing that?\n\nAnd make sure your source code is updated. I got a wired error once by\nusing a old version of the source and solved it by clone from the master.\n\nOn Fri, Jun 24, 2016, 3:04 AM peterswang notifications@github.com wrote:\n\n> I ran into a similar issue with Python 2.7.11 on OS X 10.11.5 + MBP.\n> \n> $ ./configure\n> Please specify the location of python. [Default is /usr/local/bin/python]:\n> Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\n> No Google Cloud Platform support will be enabled for TensorFlow\n> Do you wish to build TensorFlow with GPU support? [y/N] y\n> GPU support will be enabled for TensorFlow\n> Please specify which gcc nvcc should use as the host compiler. [Default is\n> /usr/bin/gcc]:\n> Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave\n> empty to use system default]: 7.5\n> Please specify the location where CUDA 7.5 toolkit is installed. Refer to\n> README.md for more details. [Default is /usr/local/cuda]:\n> Please specify the Cudnn version you want to use. [Leave empty to use\n> system default]: 5\n> Please specify the location where cuDNN 5 library is installed. Refer to\n> README.md for more details. [Default is /usr/local/cuda]:\n> Please specify a list of comma-separated Cuda compute capabilities you\n> want to build with.\n> You can find the compute capability of your device at:\n> https://developer.nvidia.com/cuda-gpus.\n> Please note that each additional compute capability significantly\n> increases your build time and binary size.\n> \n> Setting up Cuda include\n> Setting up Cuda lib\n> Setting up Cuda bin\n> Setting up Cuda nvvm\n> Setting up CUPTI include\n> Setting up CUPTI lib64\n> Configuration finished\n> $ bazel build -c opt --config=cuda\n> //tensorflow/tools/pip_package:build_pip_package\n> .......\n> .......\n> 1 warning generated.\n> INFO: From Linking tensorflow/python/_pywrap_tensorflow.so [for host]:\n> clang: warning: argument unused during compilation: '-pthread'\n> ld: warning: option -noall_load is obsolete and being ignored\n> INFO: From Linking tensorflow/python/_pywrap_tensorflow.so:\n> clang: warning: argument unused during compilation: '-pthread'\n> ld: warning: option -noall_load is obsolete and being ignored\n> ERROR:\n> /Users/peter_wang/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:38:1:\n> Executing genrule //tensorflow/contrib/session_bundle/example:half_plus_two\n> failed: bash failed: error executing command /bin/bash -c ... (remaining 1\n> argument(s) skipped):\n> com.google.devtools.build.lib.shell.BadExitStatusException: Process exited\n> with status 245.\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA\n> library libcublas.7.5.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA\n> library libcudnn.5.dylib locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA\n> library libcufft.7.5.dylib locally\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> INFO: Elapsed time: 1826.337s, Critical Path: 1821.08s\n> \n> To verify CUDA install:\n> $ ./bin/x86_64/darwin/release/deviceQuery\n> ./bin/x86_64/darwin/release/deviceQuery Starting...\n> \n> CUDA Device Query (Runtime API) version (CUDART static linking)\n> \n> Detected 1 CUDA Capable device(s)\n> \n> Device 0: \"GeForce GT 650M\"\n> CUDA Driver Version / Runtime Version 7.5 / 7.5\n> CUDA Capability Major/Minor version number: 3.0\n> Total amount of global memory: 512 MBytes (536543232 bytes)\n> ( 2) Multiprocessors, (192) CUDA Cores/MP: 384 CUDA Cores\n> GPU Max Clock rate: 405 MHz (0.41 GHz)\n> Memory Clock rate: 2000 Mhz\n> Memory Bus Width: 128-bit\n> L2 Cache Size: 262144 bytes\n> Maximum Texture Dimension Size (x,y,z) 1D=(65536), 2D=(65536, 65536),\n> 3D=(4096, 4096, 4096)\n> Maximum Layered 1D Texture Size, (num) layers 1D=(16384), 2048 layers\n> Maximum Layered 2D Texture Size, (num) layers 2D=(16384, 16384), 2048\n> layers\n> Total amount of constant memory: 65536 bytes\n> Total amount of shared memory per block: 49152 bytes\n> Total number of registers available per block: 65536\n> Warp size: 32\n> Maximum number of threads per multiprocessor: 2048\n> Maximum number of threads per block: 1024\n> Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n> Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535)\n> Maximum memory pitch: 2147483647 bytes\n> Texture alignment: 512 bytes\n> Concurrent copy and kernel execution: Yes with 1 copy engine(s)\n> Run time limit on kernels: Yes\n> Integrated GPU sharing Host Memory: No\n> Support host page-locked memory mapping: Yes\n> Alignment requirement for Surfaces: Yes\n> Device has ECC support: Disabled\n> Device supports Unified Addressing (UVA): Yes\n> Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0\n> Compute Mode:\n> < Default (multiple host threads can use ::cudaSetDevice() with device\n> simultaneously) >\n> \n> deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 7.5, CUDA Runtime\n> Version = 7.5, NumDevs = 1, Device0 = GeForce GT 650M\n> Result = PASS\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-228151515,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ADmW7AoIow1wLqEj_Jk1rli5ifANu3Zlks5qOtjUgaJpZM4I4z4H\n> .\n", "Thx for your suggestions, Velkan. I followed the instructions under the Build from Source section of the TF [Download and Setup] page. Compute capability was specified to be '3.0', just that the copy-paste missed that line.\n\nThe Github TF page shows last successful build for Mac OS to be 14 days ago. I just tried building again with a fresh clone from the repo and it crashed at the same place. Actually, bazel apparently invoked Python, which crashed, seemingly while trying to access the CUDA library, perhaps for libcufftw.7.5.dylib?\n\nIf it may be of any help to Martin, below is part of call stack from the crash report:\n...\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\n\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000000\n\nVM Regions Near 0:\n--> \n    __TEXT                 000000010aec3000-000000010aec5000 [    8K] r-x/rwx SM=COW  /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\n\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   libsystem_c.dylib               0x00007fff8e610152 strlen + 18\n1   _pywrap_tensorflow.so           0x000000010d3e19a2 perftools::gputools::internal::DsoLoader::GetDsoHandle(tensorflow::StringPiece, void**, perftools::gputools::internal::DsoLoader::LoadKind) + 402\n2   _pywrap_tensorflow.so           0x000000010d3e2692 perftools::gputools::internal::DsoLoader::GetLibcudaDsoHandle(void**) + 258\n3   _pywrap_tensorflow.so           0x000000010d0d1f22 std::__1::__function::__func<tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*), std::__1::allocator<tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)>, tensorflow::Status (tensorflow::shape_inference::InferenceContext_)>::operator()(tensorflow::shape_inference::InferenceContext_&&) + 18\n4   _pywrap_tensorflow.so           0x000000010d3e2f7f perftools::gputools::internal::CachedDsoLoader::FetchHandleResult(std::__1::function<tensorflow::Status (void**)>) + 47\n5   _pywrap_tensorflow.so           0x000000010d3e34e0 perftools::gputools::internal::CachedDsoLoader::GetLibcudaDsoHandle() + 112\n6   _pywrap_tensorflow.so           0x000000010d464fe5 perftools::gputools::initialize_cuda_gpu_executor() + 37\n7   dyld                            0x00007fff60b5a10b ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 265\n8   dyld                            0x00007fff60b5a284 ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\n9   dyld                            0x00007fff60b568bd ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 305\n10  dyld                            0x00007fff60b56743 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 127\n11  dyld                            0x00007fff60b569b3 ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 75\n12  dyld                            0x00007fff60b4beb0 dyld::runInitializers(ImageLoader*) + 89\n13  dyld                            0x00007fff60b53308 dlopen + 555\n14  libdyld.dylib                   0x00007fff8d7ae79c dlopen + 59\n15  org.python.python               0x000000010af807d9 _PyImport_GetDynLoadFunc + 309\n...\n", "Just another datapoint. I'm getting the same python crash using \n  `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` \n\nAfter Python crashes for `bazel build` near the end, then \n  `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` \ngives the error \n  `error: invalid command 'bdist_wheel'` \n(Note: the directory `/tmp/tensorflow_pkg` does not exist. Probably because of the python crash)\n\nPossible useful config information:\n\n``` text\nSystem:\n  System Version:   OS X 10.10.5 (14F1808)\n  Kernel Version:   Darwin 14.5.0\n  Chipset Model:    NVIDIA GeForce GTX 760\n  Type: GPU\n  Bus:  PCIe\n  PCIe Lane Width:  x16\n  VRAM (Total): 4096 MB\n  Vendor:   NVIDIA (0x10de)\n  Device ID:    0x1187\n  Revision ID:  0x00a2\n  ROM Revision: preset 1.0.0\n\n\n> python --version\nPython 2.7.11\n\n> /Developer/NVIDIA/CUDA-7.5/bin/nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Thu_Sep_24_00:26:39_CDT_2015\nCuda compilation tools, release 7.5, V7.5.19\n\n>pip --version\npip 8.1.2 from /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (python 2.7)\n\n> bazel version\nBuild label: 0.2.3-homebrew\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue May 17 15:07:33 2016 (1463497653)\nBuild timestamp: 1463497653\nBuild timestamp as int: 1463497653\n```\n\nCrash stack\n\n``` text\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\n\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000000\n\nVM Regions Near 0:\n--> \n    __TEXT                 0000000100000000-0000000100001000 [    4K] r-x/rwx SM=COW  /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\n\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   libsystem_c.dylib               0x00007fff9666cf92 strlen + 18\n1   _pywrap_tensorflow.so           0x00000001058ff490 perftools::gputools::internal::DsoLoader::GetDsoHandle(tensorflow::StringPiece, void**, perftools::gputools::internal::DsoLoader::LoadKind) + 384\n2   _pywrap_tensorflow.so           0x000000010590000c perftools::gputools::internal::DsoLoader::GetLibcudaDsoHandle(void**) + 188\n3   _pywrap_tensorflow.so           0x0000000105901072 std::__1::__function::__func<tensorflow::Status (*)(void**), std::__1::allocator<tensorflow::Status (*)(void**)>, tensorflow::Status (void**)>::operator()(void**&&) + 18\n4   _pywrap_tensorflow.so           0x000000010590080f perftools::gputools::internal::CachedDsoLoader::FetchHandleResult(std::__1::function<tensorflow::Status (void**)>) + 47\n5   _pywrap_tensorflow.so           0x0000000105900d6f perftools::gputools::internal::CachedDsoLoader::GetLibcudaDsoHandle() + 111\n6   _pywrap_tensorflow.so           0x0000000105982b85 perftools::gputools::initialize_cuda_gpu_executor() + 37\n7   dyld                            0x00007fff63346d4b ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 265\n8   dyld                            0x00007fff63346ed8 ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\n9   dyld                            0x00007fff633438d1 ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 305\n10  dyld                            0x00007fff63343758 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 138\n11  dyld                            0x00007fff633439c9 ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 75\n12  dyld                            0x00007fff633390ca dyld::runInitializers(ImageLoader*) + 89\n13  dyld                            0x00007fff63340231 dlopen + 578\n14  libdyld.dylib                   0x00007fff9005c857 dlopen + 59\n15  org.python.python               0x00000001000fc86f _PyImport_GetDynLoadFunc + 303\n...\n```\n", "@peterswang btw, I have seen see this `strlen + 18` crash when \n1. DSO was is not in the library path\n2. I upgraded to El Capitan which enabled System Integrity Protection (this affects how DSOs get loaded)\n", "There's a bunch of separate issues on this thread. The half_plus_two issue should have been resolved since. @tomnielsen @Velkan as for the wheel problem -- I haven't seen that other than when there's a lingering old version of setuptools around. Can you try installing in a virtualenv? Especially on Mac that often helps.\n", "I also hit the same issue on Mac. I investigated a little bit.\n\nAs mentioned earlier in this thread, the crash happens because tensorflow couldn't load a cuda shared library. tensorflow shouldn't crash but it did because `LOG(INFO) <<  ... << getenv(\"LD_LIBRARY_PATH\")` doesn't support char pointer being nullptr. I enclosed the relevant code at the end of this message.\n\nAs for why tensorflow didn't find the cuda shared library, in my case, it was because ipython from anaconda doesn't inherit the environment variable from my bash shell. I switched to use python instead of ipython, tensorflow was able to load the cuda shared library.\n\nHopefully this finding will help others who have similar problem.\n\n`tensorflow/stream_executor/dso_loader.cc@93`\n\n```\n/* static */ port::Status DsoLoader::GetDsoHandle(port::StringPiece path,\n                                                  void** dso_handle,\n                                                  LoadKind load_kind) {\n\n  int dynload_flags =\n      RTLD_LAZY | (load_kind == LoadKind::kLocal ? RTLD_LOCAL : RTLD_GLOBAL);\n  string path_string = path.ToString();\n  *dso_handle = dlopen(path_string.c_str(), dynload_flags);\n  if (*dso_handle == nullptr) {\n    LOG(INFO) << \"Couldn't open CUDA library \" << path\n              << \". LD_LIBRARY_PATH: \" << getenv(\"LD_LIBRARY_PATH\");\n    return port::Status(\n        port::error::FAILED_PRECONDITION,\n        port::StrCat(\"could not dlopen DSO: \", path, \"; dlerror: \", dlerror()));\n  }\n  LOG(INFO) << \"successfully opened CUDA library \" << path\n            << (load_kind == LoadKind::kLocal ? \" locally\" : \" globally\");\n  return port::Status::OK();\n}\n```\n\n`probably common.cc in protobuf`\n\n```\nLogMessage& LogMessage::operator<<(const char* value) {\n  message_ += value;\n  return *this;\n}\n```\n\n`XCode c++ library string`\n\n```\n...\n\n_LIBCPP_INLINE_VISIBILITY basic_string& operator+=(const value_type* __s)         {return append(__s);}\n\n...\n\nbasic_string<_CharT, _Traits, _Allocator>::append(const value_type* __s)\n{\n    _LIBCPP_ASSERT(__s != nullptr, \"string::append received nullptr\");\n    return append(__s, traits_type::length(__s));\n}\n\n...\n\nstatic inline size_t length(const char_type* __s) {return strlen(__s);}\n```\n", "@Invisibility nice digging. I've seen similar looking crashes on Mac in other places with similar stack trace, now I know where to look for. BTW, you can set env var in anaconda directly using something like this\n\n```\nimport os\nos.environ[\"LD_LIBRARY_PATH\"]=....\nimport tensorflow\n```\n", "Thanks for the tip, @yaroslavvb. I tried, but it still crashed. It seems that for some reason `dlopen` in `DsoLoader::GetDsoHandle` didn't see the environment variable. I noticed that in my environment, ipython is a script with the following content:\n\n```\n#!/bin/bash <my anaconda virtual environment path>/bin/python.app\nif __name__ == '__main__':\n    import sys\n    import IPython\n\n    sys.exit(IPython.start_ipython())\n```\n\nMy workaround is to avoid using python.app, i.e. running the following command insteead of directly running ipython:\n\n```\npython <my anaconda virtual environment path>/bin/ipython\n```\n", "I'm also getting a segmentation fault in python.\n\n```\n$ python test.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\n[1]    34129 segmentation fault  python test.py\n```\n\ntest.py\n\n```\nimport tensorflow as tf\n\n# Creates a graph.\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\nc = tf.matmul(a, b)\n\n# Creates a session with log_device_placement set to True.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n\n# Runs the op.\nprint sess.run(c)\n```\n\nSystem Info:\n\n```\nProcess:               Python [34031]\nPath:                  /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\nIdentifier:            Python\nVersion:               2.7.11 (2.7.11)\nCode Type:             X86-64 (Native)\nParent Process:        zsh [33999]\nResponsible:           iTerm2 [508]\n\nOS Version:            Mac OS X 10.11.6 (15G31)\nKernel Version: Darwin 15.6.0\nGraphics: NVIDIA GeForce GTX 960, NVIDIA GeForce GTX 960, PCIe\n```\n\nStack Trace\n\n```\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\n\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000000\n\nVM Regions Near 0:\n--> \n    __TEXT                 000000010782c000-000000010782e000 [    8K] r-x/rwx SM=COW  /usr/local/Cellar/python/2.7.11/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\n\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   libsystem_c.dylib               0x00007fff8ce51132 strlen + 18\n1   _pywrap_tensorflow.so           0x000000010a02b912 perftools::gputools::internal::DsoLoader::GetDsoHandle(tensorflow::StringPiece, void**, perftools::gputools::internal::DsoLoader::LoadKind) + 402\n2   _pywrap_tensorflow.so           0x000000010a02c602 perftools::gputools::internal::DsoLoader::GetLibcudaDsoHandle(void**) + 258\n3   _pywrap_tensorflow.so           0x0000000109e44002 std::__1::__function::__func<tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*), std::__1::allocator<tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)>, tensorflow::Status (tensorflow::shape_inference::InferenceContext*)>::operator()(tensorflow::shape_inference::InferenceContext*&&) + 18\n4   _pywrap_tensorflow.so           0x000000010a02ceef perftools::gputools::internal::CachedDsoLoader::FetchHandleResult(std::__1::function<tensorflow::Status (void**)>) + 47\n5   _pywrap_tensorflow.so           0x000000010a02d450 perftools::gputools::internal::CachedDsoLoader::GetLibcudaDsoHandle() + 112\n6   _pywrap_tensorflow.so           0x000000010a0b0195 perftools::gputools::initialize_cuda_gpu_executor() + 37\n7   dyld                            0x00007fff6cb8710b ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 265\n8   dyld                            0x00007fff6cb87284 ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\n9   dyld                            0x00007fff6cb838bd ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 305\n10  dyld                            0x00007fff6cb83743 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 127\n11  dyld                            0x00007fff6cb839b3 ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 75\n12  dyld                            0x00007fff6cb78eb0 dyld::runInitializers(ImageLoader*) + 89\n13  dyld                            0x00007fff6cb80308 dlopen + 555\n14  libdyld.dylib                   0x00007fff960d779c dlopen + 59\n15  org.python.python               0x00000001078e7892 _PyImport_GetDynLoadFunc + 309\n...\n```\n\nedit: The issue occurs because the environment variable LD_LIBRARY_PATH is not set. I set it to the same as DYLD_LIBRARY_PATH which resolved the segmentation fault from a null path as discovered above in https://github.com/tensorflow/tensorflow/issues/2940#issuecomment-230186665 by @Invisibility .\n\nI then encountered this error: \n\n```\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcuda.1.dylib.\n```\n\nI created a symlink to in `/usr/local/cuda/lib` with `ln -s libcuda.dylib libcuda.1.dylib` and now I'm getting a different segmentation fault:\n\n```\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\n\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\nException Codes:       EXC_I386_GPFLT\n\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   _pywrap_tensorflow.so           0x000000010bb73e0b perftools::gputools::StreamExecutor::DeviceMemoryUsage(long long*, long long*) const + 11\n1   _pywrap_tensorflow.so           0x000000010b9587fe tensorflow::GPUMachineManager() + 382\n2   _pywrap_tensorflow.so           0x000000010b9564ce tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds(std::__1::vector<int, std::__1::allocator<int> >*) + 46\n3   _pywrap_tensorflow.so           0x000000010b9562f9 tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<tensorflow::Device*, std::__1::allocator<tensorflow::Device*> >*) + 345\n4   _pywrap_tensorflow.so           0x000000010bae3795 tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<tensorflow::Device*, std::__1::allocator<tensorflow::Device*> >*) + 245\n5   _pywrap_tensorflow.so           0x000000010b912c15 tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&) + 133\n6   _pywrap_tensorflow.so           0x000000010bb096c8 tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) + 184\n7   _pywrap_tensorflow.so           0x000000010bace8d1 TF_NewSession + 33\n8   _pywrap_tensorflow.so           0x000000010a4845b2 _wrap_TF_NewSession(_object*, _object*) + 162\n9   org.python.python               0x0000000109424f1b PyEval_EvalFrameEx + 27072\n...\n```\n", "Try Python3 instead of Python2\n", "I'm currently using a GTX 960 2GB at 4096x2160@60Hz. Based on  https://github.com/tensorflow/tensorflow/issues/2840, I wonder if all the VRAM is used up for the display which is why I'm getting a segmentation fault?  Also Chrome is using 49.18 GB / 64 GB system memory..\n\nI have a GTX 960 4GB that I can install and test tomorrow.\n", "Previously I was following these instructions https://gist.github.com/Mistobaan/dd32287eeb6859c6668d and built from `master`. Now I used https://gist.github.com/ageitgey/819a51afa4613649bd18 and built from tag `cuda_osx`  and it worked. Note: I had to run `git submodule update --init` prior to bazel build commands and also prior to creating the pip packages `cp -r bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/__main__/* bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/\n` from https://github.com/tensorflow/tensorflow/issues/2040#issuecomment-217833442 Notice the amount of free memory available!\n\n```\npython test.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] OS X does not support NUMA - returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.367\npciBusID 0000:04:00.0\nTotal memory: 2.00GiB\nFree memory: 176.32MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:841] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\nI tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\n\nMatMul: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] MatMul: /job:localhost/replica:0/task:0/gpu:0\nb: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] b: /job:localhost/replica:0/task:0/gpu:0\na: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] a: /job:localhost/replica:0/task:0/gpu:0\n[[ 22.  28.]\n [ 49.  64.]]\n```\n\nI also rebuilt this using `master` branch and it worked. I think the key is `TF_UNOFFICIAL_SETTING=1 ./configure`\n", "BTW, there's now a daily wheel with Mac GPU support:\n\n```\nexport url=http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.9.0-py3-none-any.whl\npip install --upgrade $url\n\n```\n", "There is a bug with loading libcuda.dylib - the default cuda install creates libcuda.dylib, but tensorflow tries to load libcuda.1.dylib .  This fails, resorting to using the LD_LIBRARY_PATH which if NULL crashes.  If you copy libcuda.dylib to libcuda.1.dylib it loads fine.\n", "Thanks @alexcolburn, copying libcuda.dylib to libcuda.1.dylib indeed fixed this issue on my Mac 10.10\n", "I confirmed comment of @xuanchien . So a simple solution. Worked on my MBP 2014 (nvidia 750M) with Mac OS 10.12\n", "Thanks. I applied the same trick and now my TF on python 2.7 on OSX 10.11.5 works. \n", "I'm closing this issue, I believe we have fixed the cuda version problem as well.", "The issues still persist in tensorflow 1.0.0 alpha:\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:116] Couldn't open CUDA library libcuda.1.dylib. LD_LIBRARY_PATH: :/usr/local/cuda/lib\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: Olegs-MacBook-Pro-4.local\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"\"\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1065] LD_LIBRARY_PATH: :/usr/local/cuda/lib\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1066] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.1.dylib; dlerror: dlopen(libcuda.1.dylib, 6): image not found\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.8.0.dylib locally\r\n1.0.0-alpha\r\n```\r\n\r\nalexcolburn advice to link libcuda.1.dylib to libcuda.dylib does help.\r\n", "Same problem after installing Tensorflow today. Version info:\r\n\r\n```\r\n$ python --version\r\nPython 2.7.13\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Sun_Oct_30_22:18:43_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.54\r\n$ gcc --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin15.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n\r\nProblem description:\r\n\r\n```\r\n$ python -c \"import tensorflow\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcuda.1.dylib. LD_LIBRARY_PATH: /Developer/NVIDIA/CUDA-8.0/lib\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcuda.dylib. LD_LIBRARY_PATH: /Developer/NVIDIA/CUDA-8.0/lib\r\n...\r\n```\r\n\r\nAfter creating a symlink:\r\n\r\n```\r\n$ sudo ln -s /usr/local/cuda/lib/libcuda.dylib /Developer/NVIDIA/CUDA-8.0/lib/libcuda.1.dylib\r\n$ python -c \"import tensorflow\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 75, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/node_def_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 22, in <module>\r\n    serialized_pb=_b('\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"m\\n\\x0eResourceHandle\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tB4\\n\\x18org.tensorflow.frameworkB\\x13ResourceHandleProtoP\\x01\\xf8\\x01\\x01\\x62\\x06proto3')\r\nTypeError: __init__() got an unexpected keyword argument 'syntax'\r\n```\r\n\r\nThe remaining error was due to protobuf not being installed correctly. I had previously run `brew uninstall protobuf`. Once I uninstalled it, Tensorflow works correctly:\r\n\r\n```\r\n$ brew uninstall protobuf\r\n$ python -c \"import tensorflow\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\n$ \r\n```", "This is still a common problem on OS X 10.11.5 / 10.11.6 as we have new team members following the instructions at https://www.tensorflow.org/install/install_mac\r\n\r\nThe solution is to create a link in /usr/local/cuda/lib with 'sudo ln -s /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib'", "@victorv Unfortunately, this problem still persists.\r\n\r\nI am getting \r\n```\r\n(tensorflow) systems-mbp:~ test$ python\r\nPython 3.5.2 |Anaconda 4.3.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nSegmentation fault: 11\r\n\r\n```", "Here's a successful 10.11.6 build. Mainly for my own documentation, it has been simplified and Just Works&trade; :\r\n\r\n## Pre-requisites: \r\nOS X 10.11.6\r\nhomebrew 1.1.8\r\nPython 2.7.11\r\npip 9.0.1\r\n\r\nDownload and install directly from nVIDIA:\r\n1. Install CUDA-8.0\r\n2. Install cuDNN-5.1\r\n     a. Copy files to `/Developer/NVIDIA/CUDA-8.0/{lib,include}`\r\n     b. Symlink `ln -s /Developer/NVIDIA/CUDA-8.0/lib/cudnn* /usr/local/cuda/lib/`\r\n     c. Create symlink for libcuda.1.dylib\r\n   `ln -s /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`\r\n\r\n### Test CUDA install\r\n\r\nCopy `/Developer/NVIDIA/CUDA-8.0/samples` to your working directory\r\ncd `$CUDA_SAMPLE_DIR/`\r\n     a. To make all the samples: `make -j<# of threads>`\r\n     b. To make only the deviceQueryDrv Utility, cd to `1_Utilities/deviceQueryDrv` and type `make -j<# of threads>`\r\n`$CUDA_SAMPLE_DIR/bin/x86_64/darwin/release/deviceQueryDrv`\r\n\r\nexpected output (for my GTX 960 4GB, yours will vary depending on your GPU):\r\n```\r\n\u00bb bin/x86_64/darwin/release/deviceQueryDrv\r\nbin/x86_64/darwin/release/deviceQueryDrv Starting...\r\n\r\nCUDA Device Query (Driver API) statically linked version\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GTX 960\"\r\n  CUDA Driver Version:                           8.0\r\n  CUDA Capability Major/Minor version number:    5.2\r\n  Total amount of global memory:                 4096 MBytes (4294770688 bytes)\r\n  ( 8) Multiprocessors, (128) CUDA Cores/MP:     1024 CUDA Cores\r\n  GPU Max Clock rate:                            1291 MHz (1.29 GHz)\r\n  Memory Clock rate:                             3505 Mhz\r\n  Memory Bus Width:                              128-bit\r\n  L2 Cache Size:                                 1048576 bytes\r\n  Max Texture Dimension Sizes                    1D=(65536) 2D=(65536, 65536) 3D=(4096, 4096, 4096)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size (x,y,z):    (2147483647, 65535, 65535)\r\n  Texture alignment:                             512 bytes\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Concurrent kernel execution:                   Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\nResult = PASS\r\n```\r\n\r\n## Other software installs\r\nYou might have to install `bazel`. \r\n\r\nI recommend `brew install bazel`. \r\n\r\n`brew` while not perfect, simplifies your life. It is a simple to use package manager for OS X with a variety of packages available and maintained.\r\n\r\n## Install Tensorflow\r\n\r\n3. Automagically install GPU version `pip install tensorflow-gpu`\r\n\r\n## Test\r\n\r\ntensorflow-test.py:\r\n```\r\nimport tensorflow as tf\r\n\r\n# Creates a graph.\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\n\r\n# Creates a session with log_device_placement set to True.\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\n# Runs the op.\r\nprint sess.run(c)\r\n```\r\n\r\nexpected output (the final result should be the same, the only differences being the GPU used and amount of free GPU RAM):\r\n```\r\n\u00bb python ./tensorflow-test.py\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 960\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.291\r\npciBusID 0000:04:00.0\r\nTotal memory: 4.00GiB\r\nFree memory: 921.46MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\r\nI tensorflow/core/common_runtime/direct_session.cc:255] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 960, pci bus id: 0000:04:00.0\r\n\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:827] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nb: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:827] b: (Const)/job:localhost/replica:0/task:0/gpu:0\r\na: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:827] a: (Const)/job:localhost/replica:0/task:0/gpu:0\r\n[[ 22.  28.]\r\n [ 49.  64.]]\r\n```\r\n\r\n## Enjoy your tensorflow install and make something insanely great. \r\n\r\n### Feedback\r\nIf this worked or more importantly, didnt work, let me know and I'll try to keep it updated.", "@hbfs please clarify specifics of:\r\n```\r\nInstall cuDNN-5.1\r\na. Copy files to /Developer/NVIDIA/CUDA-8.0/{lib,include}\r\nb. Symlink ln -s /Developer/NVIDIA/CUDA-8.0lib/cudnn* /usr/local/cuda/lib/\r\n```\r\n\r\nWhat do the curly brackets mean?\r\nWhy do you have a directory `CUDA-8.0lib`\r\n", "Had managed to build TF 0.12 by following [How to fix dyld: Library not loaded: @rpath/libcudart.7.5.dylib issue when you build tensorflow on your Mac](https://github.com/JimmyKon/tensorflow_build_issue_fix/tree/master).\r\n\r\nJust tried the building TF 1.0 from source using the same method and now the following error shows up after the patch in genrule-setup.sh:\r\nERROR: /Users/peter_wang/TF/tensorflow-1.0/tensorflow/python/BUILD:2279:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o ... (remaining 648 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nclang: warning: argument unused during compilation: '-pthread'\r\nld: file not found: -lcudart.8.0\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\nEnvironment: 2012 MBP with Nvidia 650M, OS X 10.12.3, Python 2.7.13. \r\n\r\nLinks in /usr/local/cuda/lib, as suggested by @victorv was set before, though this doesn't seem to be the same issue.\r\nlrwxr-xr-x  1 root  wheel     33 Feb 10 14:30 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n-rwxr-xr-x  1 root  wheel  13504 Jan 24 11:58 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x@ 1 root  wheel     45 Nov  3 11:40 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x@ 1 root  wheel     50 Nov  3 11:40 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x@ 1 root  wheel     46 Nov  3 11:40 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x@ 1 root  wheel     49 Nov  3 11:40 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\n\r\n\"pip install tensorflow-gpu\" does work. However, the standard build apparently doesn't include AVX & SSE, which I'd like to make use of as well.", "@esd100 Do you have more detail on your segfault?", "@victorv Tell me how to get more detail and I'll happily provide it.\r\n\r\nI am using:\r\nconda 4.3.11\r\nPython 3.5.2 :: Anaconda 4.3.0 (x86_64)\r\n\r\n=========================================\r\nNVIDIA CUDA PATH\r\nexport PATH=/Developer/NVIDIA/CUDA-8.0/bin${PATH:+:${PATH}}\r\nexport DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0/lib${DYLD_LIBRARY_PATH:+:${DYLD_LIBRARY_PATH}}\r\n\r\nFiles are installed in: \r\n/Developer/NVIDIA/CUDA-8.0/\r\n/Developer/NVIDIA/cuDNN\r\n\r\n=========================================\r\n\r\nCUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GT 750M\"\r\n  CUDA Driver Version / Runtime Version          8.0 / 8.0\r\n  CUDA Capability Major/Minor version number:    3.0\r\n  Total amount of global memory:                 2048 MBytes (2147024896 bytes)\r\n  ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores\r\n  GPU Max Clock rate:                            926 MHz (0.93 GHz)\r\n  Memory Clock rate:                             2508 Mhz\r\n  Memory Bus Width:                              128-bit\r\n  L2 Cache Size:                                 262144 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M\r\nResult = PASS\r\n\r\n=========================================\r\n\r\n$ system_profiler SPSoftwareDataType)\r\n\r\nSystem Version: macOS 10.12.2 (16C67)\r\nKernel Version: Darwin 16.3.0\r\nBoot Volume: Macintosh HD\r\nBoot Mode: Normal\r\nSecure Virtual Memory: Enabled\r\nSystem Integrity Protection: Enabled\r\n\r\n=========================================\r\n\r\n$ pkgutil --pkg-info=com.apple.pkg.CLTools_Executables\r\n\r\npackage-id: com.apple.pkg.CLTools_Executables\r\nversion: 8.2.0.0.1.1480973914\r\n\r\n=========================================\r\n$ gcc --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\n\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin16.3.0\r\nThread model: posix\r\n\r\nInstalledDir: \r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n=========================================", "@esd100 {} is for shell expansion (see 3.4.2 Brace expansion http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_04.html)\r\n\r\n8.0lib was a typo, missed a '/', thanks for pointing that out. Fixed.\r\n\r\n@peterswang good point. The pre-compiled version doesn't have all the optimization flags. For best performance for a specific system, build from source. (ubuntu vs gentoo philosophy)\r\n\r\nThe tensorflow guide on building from source is well documented: https://www.tensorflow.org/install/install_sources", "@esd100 Segmentation fault: 11 is usually followed by a OS X crash report. \r\n\r\nYou can find crash reports in your Console application -> User Diagnostic Reports\r\n\r\nThe segfault is usually a result of mixing your link libraries. \r\n\r\nYou can avoid lots of these issues by using the docker images.", "@victorv Thanks. Sorry for the delay.\r\n\r\nThis is the crash report after putting in the commands:\r\n```\r\nsource activate tensorflow\r\npython\r\nimport tensorflow as tf\r\n```\r\n```\r\nProcess:               python3.5 [25276]\r\nPath:                  /anaconda/*/python3.5\r\nIdentifier:            python3.5\r\nVersion:               ???\r\nCode Type:             X86-64 (Native)\r\nParent Process:        bash [25151]\r\nResponsible:           python3.5 [25276]\r\nUser ID:               501\r\n\r\nDate/Time:             2017-03-02 18:23:28.885 -0500\r\nOS Version:            Mac OS X 10.12.3 (16D32)\r\nReport Version:        12\r\nAnonymous UUID:        0F0D56D5-52EB-7CFC-CD5E-ED3D20E9E476\r\n\r\nSleep/Wake UUID:       6CAAD8B9-E727-4612-A71C-06E5466B2E86\r\n\r\nTime Awake Since Boot: 19000 seconds\r\nTime Since Wake:       4200 seconds\r\n\r\nSystem Integrity Protection: enabled\r\n\r\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\r\n\r\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\r\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000000\r\nException Note:        EXC_CORPSE_NOTIFY\r\n\r\nTermination Signal:    Segmentation fault: 11\r\nTermination Reason:    Namespace SIGNAL, Code 0xb\r\nTerminating Process:   exc handler [0]\r\n\r\nVM Regions Near 0:\r\n--> \r\n    __TEXT                 0000000100000000-0000000100001000 [    4K] r-x/rwx SM=COW  /anaconda/*/*.5\r\n\r\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\r\n0   libsystem_c.dylib             \t0x00007fffb545ab52 strlen + 18\r\n1   _pywrap_tensorflow.so         \t0x000000010537ad8a perftools::gputools::internal::DsoLoader::GetDsoHandle(tensorflow::StringPiece, void**, perftools::gputools::internal::DsoLoader::LoadKind) + 282\r\n2   _pywrap_tensorflow.so         \t0x000000010537b69c perftools::gputools::internal::DsoLoader::GetCudnnDsoHandle(void**) + 204\r\n3   _pywrap_tensorflow.so         \t0x000000010537ccb2 std::__1::__function::__func<tensorflow::Status (*)(void**), std::__1::allocator<tensorflow::Status (*)(void**)>, tensorflow::Status (void**)>::operator()(void**&&) + 18\r\n4   _pywrap_tensorflow.so         \t0x000000010537c45f perftools::gputools::internal::CachedDsoLoader::FetchHandleResult(std::__1::function<tensorflow::Status (void**)>) + 47\r\n5   _pywrap_tensorflow.so         \t0x000000010537c6ff perftools::gputools::internal::CachedDsoLoader::GetCudnnDsoHandle() + 111\r\n6   _pywrap_tensorflow.so         \t0x0000000105341e62 perftools::gputools::initialize_cudnn() + 242\r\n7   dyld                          \t0x00007fff6974e75f ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 385\r\n8   dyld                          \t0x00007fff6974e962 ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\r\n9   dyld                          \t0x00007fff6974a1ee ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 338\r\n10  dyld                          \t0x00007fff69749268 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 138\r\n11  dyld                          \t0x00007fff697492fd ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 75\r\n12  dyld                          \t0x00007fff6973e70d dyld::runInitializers(ImageLoader*) + 87\r\n13  dyld                          \t0x00007fff69746150 dlopen + 556\r\n14  libdyld.dylib                 \t0x00007fffb5421a3e dlopen + 59\r\n15  libpython3.5m.dylib           \t0x00000001000f9f17 _PyImport_FindSharedFuncptr + 311\r\n16  libpython3.5m.dylib           \t0x00000001000db506 _PyImport_LoadDynamicModuleWithSpec + 422\r\n17  libpython3.5m.dylib           \t0x00000001000db0c2 _imp_create_dynamic + 258\r\n18  libpython3.5m.dylib           \t0x000000010004ff38 PyCFunction_Call + 280\r\n19  libpython3.5m.dylib           \t0x00000001000bde4b PyEval_EvalFrameEx + 25355\r\n20  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n21  libpython3.5m.dylib           \t0x00000001000c19ae fast_function + 334\r\n22  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n23  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n24  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n25  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n26  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n27  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n28  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n29  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n30  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n31  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n32  libpython3.5m.dylib           \t0x00000001000c19ae fast_function + 334\r\n33  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n34  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n35  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n36  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n37  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n38  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n39  libpython3.5m.dylib           \t0x00000001000b7ac1 PyEval_EvalCode + 81\r\n40  libpython3.5m.dylib           \t0x00000001000b52ab builtin_exec + 555\r\n41  libpython3.5m.dylib           \t0x000000010004ff38 PyCFunction_Call + 280\r\n42  libpython3.5m.dylib           \t0x00000001000bde4b PyEval_EvalFrameEx + 25355\r\n43  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n44  libpython3.5m.dylib           \t0x00000001000c19ae fast_function + 334\r\n45  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n46  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n47  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n48  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n49  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n50  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n51  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n52  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n53  libpython3.5m.dylib           \t0x00000001000b7b1e PyEval_EvalCodeEx + 78\r\n54  libpython3.5m.dylib           \t0x000000010003430f function_call + 351\r\n55  libpython3.5m.dylib           \t0x000000010000fd73 PyObject_Call + 99\r\n56  libpython3.5m.dylib           \t0x0000000100010815 _PyObject_CallMethodIdObjArgs + 421\r\n57  libpython3.5m.dylib           \t0x00000001000da3af PyImport_ImportModuleLevelObject + 1951\r\n58  libpython3.5m.dylib           \t0x00000001000b4854 builtin___import__ + 132\r\n59  libpython3.5m.dylib           \t0x000000010004fe5c PyCFunction_Call + 60\r\n60  libpython3.5m.dylib           \t0x00000001000bde4b PyEval_EvalFrameEx + 25355\r\n61  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n62  libpython3.5m.dylib           \t0x00000001000c19ae fast_function + 334\r\n63  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n64  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n65  libpython3.5m.dylib           \t0x00000001000b7b1e PyEval_EvalCodeEx + 78\r\n66  libpython3.5m.dylib           \t0x000000010003430f function_call + 351\r\n67  libpython3.5m.dylib           \t0x000000010000fd73 PyObject_Call + 99\r\n68  libpython3.5m.dylib           \t0x0000000100010815 _PyObject_CallMethodIdObjArgs + 421\r\n69  libpython3.5m.dylib           \t0x00000001000da4f4 PyImport_ImportModuleLevelObject + 2276\r\n70  libpython3.5m.dylib           \t0x00000001000b4854 builtin___import__ + 132\r\n71  libpython3.5m.dylib           \t0x000000010004fe5c PyCFunction_Call + 60\r\n72  libpython3.5m.dylib           \t0x000000010000fd73 PyObject_Call + 99\r\n73  libpython3.5m.dylib           \t0x00000001000bbd2a PyEval_EvalFrameEx + 16874\r\n74  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n75  libpython3.5m.dylib           \t0x00000001000b7ac1 PyEval_EvalCode + 81\r\n76  libpython3.5m.dylib           \t0x00000001000b52ab builtin_exec + 555\r\n77  libpython3.5m.dylib           \t0x000000010004ff38 PyCFunction_Call + 280\r\n78  libpython3.5m.dylib           \t0x00000001000bde4b PyEval_EvalFrameEx + 25355\r\n79  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n80  libpython3.5m.dylib           \t0x00000001000c19ae fast_function + 334\r\n81  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n82  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n83  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n84  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n85  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n86  libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n87  libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n88  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n89  libpython3.5m.dylib           \t0x00000001000b7b1e PyEval_EvalCodeEx + 78\r\n90  libpython3.5m.dylib           \t0x000000010003430f function_call + 351\r\n91  libpython3.5m.dylib           \t0x000000010000fd73 PyObject_Call + 99\r\n92  libpython3.5m.dylib           \t0x0000000100010815 _PyObject_CallMethodIdObjArgs + 421\r\n93  libpython3.5m.dylib           \t0x00000001000da3af PyImport_ImportModuleLevelObject + 1951\r\n94  libpython3.5m.dylib           \t0x00000001000b4854 builtin___import__ + 132\r\n95  libpython3.5m.dylib           \t0x000000010004fe5c PyCFunction_Call + 60\r\n96  libpython3.5m.dylib           \t0x000000010000fd73 PyObject_Call + 99\r\n97  libpython3.5m.dylib           \t0x00000001000bbd2a PyEval_EvalFrameEx + 16874\r\n98  libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n99  libpython3.5m.dylib           \t0x00000001000b7ac1 PyEval_EvalCode + 81\r\n100 libpython3.5m.dylib           \t0x00000001000b52ab builtin_exec + 555\r\n101 libpython3.5m.dylib           \t0x000000010004ff38 PyCFunction_Call + 280\r\n102 libpython3.5m.dylib           \t0x00000001000bde4b PyEval_EvalFrameEx + 25355\r\n103 libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n104 libpython3.5m.dylib           \t0x00000001000c19ae fast_function + 334\r\n105 libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n106 libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n107 libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n108 libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n109 libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n110 libpython3.5m.dylib           \t0x00000001000c192f fast_function + 207\r\n111 libpython3.5m.dylib           \t0x00000001000bd434 PyEval_EvalFrameEx + 22772\r\n112 libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n113 libpython3.5m.dylib           \t0x00000001000b7b1e PyEval_EvalCodeEx + 78\r\n114 libpython3.5m.dylib           \t0x000000010003430f function_call + 351\r\n115 libpython3.5m.dylib           \t0x000000010000fd73 PyObject_Call + 99\r\n116 libpython3.5m.dylib           \t0x0000000100010815 _PyObject_CallMethodIdObjArgs + 421\r\n117 libpython3.5m.dylib           \t0x00000001000da3af PyImport_ImportModuleLevelObject + 1951\r\n118 libpython3.5m.dylib           \t0x00000001000b4854 builtin___import__ + 132\r\n119 libpython3.5m.dylib           \t0x000000010004fe5c PyCFunction_Call + 60\r\n120 libpython3.5m.dylib           \t0x000000010000fd73 PyObject_Call + 99\r\n121 libpython3.5m.dylib           \t0x00000001000bbd2a PyEval_EvalFrameEx + 16874\r\n122 libpython3.5m.dylib           \t0x00000001000c10c3 _PyEval_EvalCodeWithName + 1779\r\n123 libpython3.5m.dylib           \t0x00000001000b7ac1 PyEval_EvalCode + 81\r\n124 libpython3.5m.dylib           \t0x00000001000e642c PyRun_InteractiveOneObject + 588\r\n125 libpython3.5m.dylib           \t0x00000001000e5d5e PyRun_InteractiveLoopFlags + 206\r\n126 libpython3.5m.dylib           \t0x00000001000e5c5c PyRun_AnyFileExFlags + 60\r\n127 libpython3.5m.dylib           \t0x00000001000fcd47 Py_Main + 3591\r\n128 python                        \t0x0000000100000dc7 main + 215\r\n129 python                        \t0x0000000100000ce4 start + 52\r\n\r\nThread 0 crashed with X86 Thread State (64-bit):\r\n  rax: 0x0000000000000000  rbx: 0x0000000000000000  rcx: 0x0000000000000000  rdx: 0x0000000000000000\r\n  rdi: 0x0000000000000000  rsi: 0x0000000000000307  rbp: 0x00007fff5bff8e20  rsp: 0x00007fff5bff8e20\r\n   r8: 0x00000000fffffff1   r9: 0x0000000101202778  r10: 0x000000000000000f  r11: 0x0000000000000000\r\n  r12: 0x00007fff5bff9209  r13: 0xf7bee2bde4bf00e0  r14: 0x00007fff5bff9270  r15: 0x0000000000000010\r\n  rip: 0x00007fffb545ab52  rfl: 0x0000000000010246  cr2: 0x0000000000000000\r\n  \r\nLogical CPU:     6\r\nError Code:      0x00000004\r\nTrap Number:     14\r\n\r\n\r\nBinary Images:\r\n       0x100000000 -        0x100000ff7 +python (???) <A92DBDA7-44FF-35CF-8860-69BA977B417E> /anaconda/*/python\r\n       0x100003000 -        0x1001b5fff +libpython3.5m.dylib (3.5) <D24D7ED3-029D-3C55-8431-04B5A2E8639A> /anaconda/*/libpython3.5m.dylib\r\n       0x1003e6000 -        0x1003e7fff +_heapq.so (???) <01E9935D-050D-3732-BB3C-1F1A941DED4D> /anaconda/*/_heapq.so\r\n       0x1003eb000 -        0x1003edfff +readline.so (???) <2BBC4898-3D25-3E58-BED2-4CA01365FFB9> /anaconda/*/readline.so\r\n       0x1003f5000 -        0x1003f5ff7 +_opcode.so (???) <8F431933-E6A7-3318-81E4-C8157F1DA7F6> /anaconda/*/_opcode.so\r\n       0x1003f9000 -        0x1003f9fff +grp.so (???) <75A30066-2CB2-3F6E-ADAC-B249233CDB42> /anaconda/*/grp.so\r\n       0x1003fc000 -        0x1003fdfff +_random.so (???) <3326504F-74E4-33F3-8D0E-0468B6480468> /anaconda/*/_random.so\r\n       0x100731000 -        0x100762fff +libreadline.6.2.dylib (6.2) <8B0801EE-2449-E9D6-850B-4E7FB34BF7FB> /anaconda/*/libreadline.6.2.dylib\r\n       0x100778000 -        0x10078aff7 +_ctypes.so (???) <6784BCB6-23E1-3728-920D-D75D0AC4F34F> /anaconda/*/_ctypes.so\r\n       0x10079b000 -        0x10079ffff +_struct.so (???) <42353FCD-910E-319F-8BD5-F4A5BD9AAF2E> /anaconda/*/_struct.so\r\n       0x1007e8000 -        0x1007edff7 +math.so (???) <6301B65C-26E9-3CF7-9013-1239B15EF4D3> /anaconda/*/math.so\r\n       0x1007f4000 -        0x1007f7ff7 +_lzma.so (???) <45350179-45D9-3E0C-9034-A863F5978360> /anaconda/*/_lzma.so\r\n       0x101412000 -        0x1015a1fff +multiarray.cpython-35m-darwin.so (???) <C3082A47-3D27-3214-A3D3-346C8111EDE2> /anaconda/*/multiarray.cpython-35m-darwin.so\r\n       0x10169f000 -        0x1016abfff +_datetime.so (???) <2264E487-677B-34D3-A546-9745721617A1> /anaconda/*/_datetime.so\r\n       0x1016b6000 -        0x101789fff +umath.cpython-35m-darwin.so (???) <4F1ED886-054E-379D-96EE-5A522B1AD8FA> /anaconda/*/umath.cpython-35m-darwin.so\r\n       0x1017c2000 -        0x1017d1fff +_pickle.so (???) <76465AD0-A0A7-311E-BDAE-09A7768E8860> /anaconda/*/_pickle.so\r\n       0x1017dd000 -        0x1017f5ff7 +_bz2.so (???) <F29CB3C3-69BD-340E-AC38-BAA8C8F87D51> /anaconda/*/_bz2.so\r\n       0x1017fb000 -        0x1017fcff7 +lapack_lite.cpython-35m-darwin.so (???) <EAAAB223-4E4A-3E1D-B0CE-E8A639265D3C> /anaconda/*/lapack_lite.cpython-35m-darwin.so\r\n       0x102a80000 -        0x102ab4fff +liblzma.5.dylib (8.2) <DCF7E34A-AED3-3DB8-8DD8-F17522DAAE42> /anaconda/*/liblzma.5.dylib\r\n       0x102afb000 -        0x102afdff7 +_hashlib.so (???) <2A1F4AD5-F0A5-3636-B93A-87A1158EEDB7> /anaconda/*/_hashlib.so\r\n       0x102b02000 -        0x102b4cff7 +libssl.1.0.0.dylib (0) <9F42DCB5-702D-327C-84C1-AB64D6F942E1> /anaconda/*/libssl.1.0.0.dylib\r\n       0x102b69000 -        0x102cea227 +libcrypto.1.0.0.dylib (0) <F7749773-18B9-33DE-B4BB-A9D1BFB6D96E> /anaconda/*/libcrypto.1.0.0.dylib\r\n       0x102e20000 -        0x102e38fff +_umath_linalg.cpython-35m-darwin.so (???) <3DDEBEDD-5C80-339D-9770-6D62514FAA06> /anaconda/*/_umath_linalg.cpython-35m-darwin.so\r\n       0x102ec6000 -        0x102ecfff7 +fftpack_lite.cpython-35m-darwin.so (???) <58E02D71-415E-36AA-9D54-3F32E4786163> /anaconda/*/fftpack_lite.cpython-35m-darwin.so\r\n       0x103040000 -        0x1030eefff +mtrand.cpython-35m-darwin.so (???) <BE72B37C-5265-353B-AD9B-A44E9E57640D> /anaconda/*/mtrand.cpython-35m-darwin.so\r\n       0x10321c000 -        0x10637d767 +_pywrap_tensorflow.so (0) <CFBCCB99-C59C-389E-85E0-DC9B3AE13CA8> /anaconda/*/_pywrap_tensorflow.so\r\n       0x112a39000 -        0x112a74fff +libcudart.8.0.dylib (0) <6E55D67C-1A95-3C2B-B2C1-C3F5827A3F03> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\n       0x113000000 -        0x11333aff7 +libcublas.8.0.dylib (0) <373C3010-E15D-3563-B655-76532180F4CF> /Developer/NVIDIA/CUDA-8.0/lib/libcublas.8.0.dylib\r\n    0x7fff69739000 -     0x7fff69776267  dyld (421.2) <947FC440-80F9-32F7-A773-6FC418FE1AB7> /usr/lib/dyld\r\n    0x7fff9c9d9000 -     0x7fff9c9d9fff  com.apple.Accelerate (1.11 - Accelerate 1.11) <D700DBDF-69AE-37A2-B9C7-0961CF0B6841> /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate\r\n    0x7fff9c9f2000 -     0x7fff9cf0bfeb  com.apple.vImage (8.1 - ???) <6408805B-67E9-3874-8D32-0BB814CE5CDA> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vImage.framework/Versions/A/vImage\r\n    0x7fff9cf0c000 -     0x7fff9d07cff3  libBLAS.dylib (1185) <C7E42BBE-2337-3AEF-9C45-A2F2CB1A5B3E> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib\r\n    0x7fff9d07d000 -     0x7fff9d091ffb  libBNNS.dylib (14) <CFDEE88D-E002-347C-BC68-83099651585B> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBNNS.dylib\r\n    0x7fff9d092000 -     0x7fff9d488fef  libLAPACK.dylib (1185) <2E8201CB-9A41-3D65-853E-841917FCE77B> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib\r\n    0x7fff9d489000 -     0x7fff9d49ffff  libLinearAlgebra.dylib (1185) <8CC29DE1-A231-3D5E-B5F1-DCC309036FE0> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLinearAlgebra.dylib\r\n    0x7fff9d4a0000 -     0x7fff9d4a6fff  libQuadrature.dylib (3) <120F6228-A3D4-3184-89D7-785ADC2AC715> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libQuadrature.dylib\r\n    0x7fff9d4a7000 -     0x7fff9d4bbff7  libSparseBLAS.dylib (1185) <C35235B7-CFA6-39A7-BD6E-79F4D9CAFD36> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libSparseBLAS.dylib\r\n    0x7fff9d4bc000 -     0x7fff9d643fe7  libvDSP.dylib (600) <F59348AA-E1D3-3A27-8AB5-F546D38BFB76> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvDSP.dylib\r\n    0x7fff9d644000 -     0x7fff9d6f6ffb  libvMisc.dylib (600) <70D4B548-47EE-3C6B-A93B-3EA6B60701E0> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libvMisc.dylib\r\n    0x7fff9d6f7000 -     0x7fff9d6f7fff  com.apple.Accelerate.vecLib (3.11 - vecLib 3.11) <A395B521-8E54-30F2-B4FE-355D68900DAF> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/vecLib\r\n    0x7fff9fe45000 -     0x7fffa02dffff  com.apple.CoreFoundation (6.9 - 1348.28) <A40AA224-7A50-3989-95D0-5A228A0E2FAF> /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\r\n    0x7fffa1e27000 -     0x7fffa1ebcff7  com.apple.framework.IOKit (2.0.2 - 1324.30.13) <163BE7FA-B29A-348F-8B5F-E301F2E8C964> /System/Library/Frameworks/IOKit.framework/Versions/A/IOKit\r\n    0x7fffa634b000 -     0x7fffa666afff  com.apple.security (7.0 - 57740.31.2) <A47D7BAE-0591-3184-8E44-FB2EB08A19C2> /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n    0x7fffb3c42000 -     0x7fffb3c43ff3  libDiagnosticMessagesClient.dylib (102) <422911A4-E273-3E88-BFC4-DF6470E48242> /usr/lib/libDiagnosticMessagesClient.dylib\r\n    0x7fffb3e7b000 -     0x7fffb3e7bfff  libOpenScriptingUtil.dylib (172) <D025E180-BB3B-3FFA-98FC-B6835354D723> /usr/lib/libOpenScriptingUtil.dylib\r\n    0x7fffb3e81000 -     0x7fffb3e82ff3  libSystem.B.dylib (1238) <9CB018AF-54E9-300F-82BE-81FE553C9154> /usr/lib/libSystem.B.dylib\r\n    0x7fffb3f9a000 -     0x7fffb3f9aff3  libauto.dylib (187) <5BBF6A00-CC76-389D-84E7-CA88EDADE683> /usr/lib/libauto.dylib\r\n    0x7fffb3f9b000 -     0x7fffb3fabff3  libbsm.0.dylib (34) <20084796-B04D-3B35-A003-EA11459557A9> /usr/lib/libbsm.0.dylib\r\n    0x7fffb3fac000 -     0x7fffb3fbaff7  libbz2.1.0.dylib (38) <6FD3B63F-0F86-3A25-BD5B-E243F58792C9> /usr/lib/libbz2.1.0.dylib\r\n    0x7fffb3fbb000 -     0x7fffb4011ff7  libc++.1.dylib (307.4) <BEE86868-F831-384C-919E-2B286ACFE87C> /usr/lib/libc++.1.dylib\r\n    0x7fffb4012000 -     0x7fffb403cfff  libc++abi.dylib (307.2) <1CEF8ABB-7E6D-3C2F-8E0A-E7884478DD23> /usr/lib/libc++abi.dylib\r\n    0x7fffb4064000 -     0x7fffb4064ff7  libcoretls.dylib (121.31.1) <BCC32537-4831-3E9F-876E-8C9F4CF52FD3> /usr/lib/libcoretls.dylib\r\n    0x7fffb4065000 -     0x7fffb4066ff3  libcoretls_cfhelpers.dylib (121.31.1) <6F37C5AD-7999-3D31-A52F-7AEED935F32D> /usr/lib/libcoretls_cfhelpers.dylib\r\n    0x7fffb4471000 -     0x7fffb4471fff  libenergytrace.dylib (15) <A1B040A2-7977-3097-9ADF-34FF181EB970> /usr/lib/libenergytrace.dylib\r\n    0x7fffb457a000 -     0x7fffb479fffb  libicucore.A.dylib (57149.0.1) <6B5FDA93-AA88-318F-9608-C2A33D602EC7> /usr/lib/libicucore.A.dylib\r\n    0x7fffb47a7000 -     0x7fffb47c0ffb  liblzma.5.dylib (10) <44BD0279-99DD-36B5-8A6E-C11432E2098D> /usr/lib/liblzma.5.dylib\r\n    0x7fffb4a80000 -     0x7fffb4ab1ffb  libncurses.5.4.dylib (51.30.1) <A8C8F837-86A2-3EC2-B2D2-6E8A267847E4> /usr/lib/libncurses.5.4.dylib\r\n    0x7fffb4b2c000 -     0x7fffb4efcd97  libobjc.A.dylib (706) <F9AFE665-A3A2-3285-9495-19803A565861> /usr/lib/libobjc.A.dylib\r\n    0x7fffb4eff000 -     0x7fffb4f03fff  libpam.2.dylib (21.30.1) <71EB0D88-DE84-3C8D-A2C5-58AA282BC5BC> /usr/lib/libpam.2.dylib\r\n    0x7fffb4fbe000 -     0x7fffb5106fe3  libsqlite3.dylib (253) <B5BA5C96-AB13-34A0-8237-DD52A0181DFE> /usr/lib/libsqlite3.dylib\r\n    0x7fffb51fb000 -     0x7fffb5208fff  libxar.1.dylib (357) <58BFB84B-66FE-3299-AA3D-BBA178ADEE39> /usr/lib/libxar.1.dylib\r\n    0x7fffb520c000 -     0x7fffb52fbffb  libxml2.2.dylib (30.11) <E12AF929-0FA5-3214-840F-C81E6AC9F36E> /usr/lib/libxml2.2.dylib\r\n    0x7fffb5326000 -     0x7fffb5337ff3  libz.1.dylib (67) <46E3FFA2-4328-327A-8D34-A03E20BFFB8E> /usr/lib/libz.1.dylib\r\n    0x7fffb5346000 -     0x7fffb534aff7  libcache.dylib (79) <0C8092D3-600F-3ADD-A036-F225B6CDCA43> /usr/lib/system/libcache.dylib\r\n    0x7fffb534b000 -     0x7fffb5356ff7  libcommonCrypto.dylib (60092.30.2) <B16E29B6-EC8D-3A8F-9A89-DD9CF35F7C4B> /usr/lib/system/libcommonCrypto.dylib\r\n    0x7fffb5357000 -     0x7fffb535efff  libcompiler_rt.dylib (62) <E992E8D9-037C-3454-A366-A25E4D31D6BB> /usr/lib/system/libcompiler_rt.dylib\r\n    0x7fffb535f000 -     0x7fffb5367fff  libcopyfile.dylib (138) <64E285D9-5485-333B-AEE7-8B0C8FB9275F> /usr/lib/system/libcopyfile.dylib\r\n    0x7fffb5368000 -     0x7fffb53ebfdf  libcorecrypto.dylib (442.30.20) <2074B932-FD79-30A9-8E90-AF25C49F2AF1> /usr/lib/system/libcorecrypto.dylib\r\n    0x7fffb53ec000 -     0x7fffb541efff  libdispatch.dylib (703.30.5) <EA0CC14E-D559-3802-B4B2-0E8C7579AAC4> /usr/lib/system/libdispatch.dylib\r\n    0x7fffb541f000 -     0x7fffb5424ff3  libdyld.dylib (421.2) <6F506653-FFF6-3DB8-84F1-109AE3C52F32> /usr/lib/system/libdyld.dylib\r\n    0x7fffb5425000 -     0x7fffb5425ffb  libkeymgr.dylib (28) <1A318923-1200-3B06-B432-5007D82F195D> /usr/lib/system/libkeymgr.dylib\r\n    0x7fffb5426000 -     0x7fffb5432ffb  libkxld.dylib (3789.41.3) <87550136-9353-348B-9CD9-C342B48C5AAF> /usr/lib/system/libkxld.dylib\r\n    0x7fffb5433000 -     0x7fffb5433fff  liblaunch.dylib (972.30.7) <15FACC21-079A-3BDF-9AFB-4253EFDEB587> /usr/lib/system/liblaunch.dylib\r\n    0x7fffb5434000 -     0x7fffb5439fff  libmacho.dylib (894) <A2F38EC1-C37C-3B93-B0E4-36B07C177F8C> /usr/lib/system/libmacho.dylib\r\n    0x7fffb543a000 -     0x7fffb543cff3  libquarantine.dylib (85) <C1D7749F-5F5F-3BB9-BEFC-1F0B9DA941FD> /usr/lib/system/libquarantine.dylib\r\n    0x7fffb543d000 -     0x7fffb543effb  libremovefile.dylib (45) <CD42974E-BE0B-39FC-9BFC-8A7540A04DC6> /usr/lib/system/libremovefile.dylib\r\n    0x7fffb543f000 -     0x7fffb5457ff7  libsystem_asl.dylib (349.30.2) <EFAC72D7-CB13-3DF7-ADF3-EC6635C6F1EA> /usr/lib/system/libsystem_asl.dylib\r\n    0x7fffb5458000 -     0x7fffb5458ff7  libsystem_blocks.dylib (67) <B8C3701D-5A91-3D35-999D-2DC8D5393525> /usr/lib/system/libsystem_blocks.dylib\r\n    0x7fffb5459000 -     0x7fffb54e6fef  libsystem_c.dylib (1158.30.7) <2F881962-03CB-3B9D-A782-D98C1BBA4E3D> /usr/lib/system/libsystem_c.dylib\r\n    0x7fffb54e7000 -     0x7fffb54eaffb  libsystem_configuration.dylib (888.30.2) <4FE3983C-E4ED-3939-A578-03AD29C99788> /usr/lib/system/libsystem_configuration.dylib\r\n    0x7fffb54eb000 -     0x7fffb54eefff  libsystem_coreservices.dylib (41.4) <1A572B9E-0C47-320F-8C64-7990D0A5FB5A> /usr/lib/system/libsystem_coreservices.dylib\r\n    0x7fffb54ef000 -     0x7fffb5507ff3  libsystem_coretls.dylib (121.31.1) <4676F06D-274D-31BE-B61C-4D7A4AEF4858> /usr/lib/system/libsystem_coretls.dylib\r\n    0x7fffb5508000 -     0x7fffb550efff  libsystem_dnssd.dylib (765.30.11) <DC708D84-ED7D-3936-B996-A67C66B8DDAA> /usr/lib/system/libsystem_dnssd.dylib\r\n    0x7fffb550f000 -     0x7fffb5538ff7  libsystem_info.dylib (503.30.1) <9ED9121C-F111-3FAD-BC2F-C95DEE1C9362> /usr/lib/system/libsystem_info.dylib\r\n    0x7fffb5539000 -     0x7fffb555bff7  libsystem_kernel.dylib (3789.41.3) <B75B128C-7D7A-3318-91CD-82B5A69C5329> /usr/lib/system/libsystem_kernel.dylib\r\n    0x7fffb555c000 -     0x7fffb55a3fe7  libsystem_m.dylib (3121.4) <266DB92B-A86F-3691-80FB-1B26AD73CFF3> /usr/lib/system/libsystem_m.dylib\r\n    0x7fffb55a4000 -     0x7fffb55c2ff7  libsystem_malloc.dylib (116.30.3) <F40DEE3B-386A-3529-A3F7-98117ED55BF4> /usr/lib/system/libsystem_malloc.dylib\r\n    0x7fffb55c3000 -     0x7fffb561affb  libsystem_network.dylib (856.30.16) <4AE368E9-605D-379D-B04C-2AC7455B8250> /usr/lib/system/libsystem_network.dylib\r\n    0x7fffb561b000 -     0x7fffb5624ff3  libsystem_networkextension.dylib (563.30.15) <EB020B0C-7DF0-3EEF-8E3C-15DA3C01D687> /usr/lib/system/libsystem_networkextension.dylib\r\n    0x7fffb5625000 -     0x7fffb562eff3  libsystem_notify.dylib (165.20.1) <E7FD3A7C-DD07-36E2-9FA4-7561F9F114DA> /usr/lib/system/libsystem_notify.dylib\r\n    0x7fffb562f000 -     0x7fffb5637fe7  libsystem_platform.dylib (126.1.2) <3CA06D4E-C00A-36DE-AA65-3A390097D1F6> /usr/lib/system/libsystem_platform.dylib\r\n    0x7fffb5638000 -     0x7fffb5642ff7  libsystem_pthread.dylib (218.30.1) <C869ED7C-BE29-3532-8E69-3A8DA1447EDC> /usr/lib/system/libsystem_pthread.dylib\r\n    0x7fffb5643000 -     0x7fffb5646ff7  libsystem_sandbox.dylib (592.31.1) <7BBFDF96-293F-3DD9-B3A4-7C168280B441> /usr/lib/system/libsystem_sandbox.dylib\r\n    0x7fffb5647000 -     0x7fffb5648fff  libsystem_secinit.dylib (24) <5C1F1E47-0F7D-3E25-8DEB-D9DB1F902281> /usr/lib/system/libsystem_secinit.dylib\r\n    0x7fffb5649000 -     0x7fffb5650fff  libsystem_symptoms.dylib (532.30.6) <5D990CF5-B58F-39F7-B375-99B4EC62CFBD> /usr/lib/system/libsystem_symptoms.dylib\r\n    0x7fffb5651000 -     0x7fffb5671ff7  libsystem_trace.dylib (518.30.7) <6D34D1EA-2A3C-3D2D-803E-A666E6AEEE52> /usr/lib/system/libsystem_trace.dylib\r\n    0x7fffb5672000 -     0x7fffb5677ffb  libunwind.dylib (35.3) <9F7C2AD8-A9A7-3DE4-828D-B0F0F166AAA0> /usr/lib/system/libunwind.dylib\r\n    0x7fffb5678000 -     0x7fffb56a1ff7  libxpc.dylib (972.30.7) <65E41BB6-EBD5-3D93-B0BE-B190CEE4DD93> /usr/lib/system/libxpc.dylib\r\n\r\nExternal Modification Summary:\r\n  Calls made by other processes targeting this process:\r\n    task_for_pid: 0\r\n    thread_create: 0\r\n    thread_set_state: 0\r\n  Calls made by this process:\r\n    task_for_pid: 0\r\n    thread_create: 0\r\n    thread_set_state: 0\r\n  Calls made by all processes on this machine:\r\n    task_for_pid: 12969\r\n    thread_create: 0\r\n    thread_set_state: 0\r\n\r\nVM Region Summary:\r\nReadOnly portion of Libraries: Total=272.4M resident=0K(0%) swapped_out_or_unallocated=272.4M(100%)\r\nWritable regions: Total=60.4M written=0K(0%) resident=0K(0%) swapped_out=0K(0%) unallocated=60.4M(100%)\r\n \r\n                                VIRTUAL   REGION \r\nREGION TYPE                        SIZE    COUNT (non-coalesced) \r\n===========                     =======  ======= \r\nKernel Alloc Once                    8K        2 \r\nMALLOC                            43.4M       24 \r\nMALLOC guard page                   16K        4 \r\nMALLOC_LARGE (reserved)            512K        3         reserved VM address space (unallocated)\r\nVM_ALLOCATE                          8K        3 \r\n__DATA                            6396K      106 \r\n__LINKEDIT                       177.8M       31 \r\n__NV_CUDA                        169.4M        3 \r\n__TEXT                            94.6M       99 \r\n__UNICODE                          556K        2 \r\n__UNIXSTACK                       16.0M        2 \r\nshared memory                       12K        4 \r\n===========                     =======  ======= \r\nTOTAL                            508.6M      271 \r\nTOTAL, minus reserved VM space   508.1M      271 \r\n\r\n```", "Have you tried setting `LD_LIBRARY_PATH`?\r\nThere's also this [issue](https://github.com/tensorflow/tensorflow/pull/664#issuecomment-170134955) which tracked down similar \"strlen + 18\" to getenv `LD_LIBRARY_PATH` returning null\r\n\r\nI actually see\u00a0similar `strlen + 18` with probability 50% on latest MacOS when I create session in GPU TensorFlow with `CUDA_VISIBLE_DEVICES` set to empty.\r\n\r\nMost likely thing is that there's a piece of code which isn't checking for null pointers, which can happen non-deterministically.  For instance on MacOS, CFStringGetCStringPtr can return null non-deterministically. That output isn't checked for null and I fixed one such place in  https://github.com/tensorflow/tensorflow/pull/3448 but I see other return values that aren't checked for null, such as output of `CFDictionaryGetValue` in this [line](https://github.com/tensorflow/tensorflow/blob/73115538fc37dd8967b8531e04a7a1d42f6bada4/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L325)\r\n", "@esd100 it could be LD_LIBRARY_PATH as @yaroslavvb has noted or you could be missing a cuda.1.dylib link in your /usr/local/cuda/lib directory.\r\n\r\nlrwxr-xr-x   1 root  staff        33 Feb 16 12:33 libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n\r\nln -s libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib", "@victorv @yaroslavvb I have the cuda.1.dylib link.\r\n\r\nWould you recommend setting LD_LIBRARY_PATH in the bash profile? What do you set it to?\r\n\r\nIn my bash_profile, I have: \r\n```\r\n# NVIDIA CUDA PATH\r\nexport PATH=/Developer/NVIDIA/CUDA-8.0/bin${PATH:+:${PATH}}\r\nexport DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0/lib${DYLD_LIBRARY_PATH:+:${DYLD_LIBRARY_PATH}}\r\n```", "```\r\nexport DYLD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib\r\nexport LD_LIBRARY_PATH=$DYLD_LIBRARY_PATH\r\nexport PATH=$DYLD_LIBRARY_PATH:$PATH\r\n\r\n```", "@yaroslavvb \r\n\r\nThanks for the input commands.\r\n\r\nSome changes with that.\r\n\r\n```\r\nimport tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcudnn.5.dylib. LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib:/usr/local/cuda/lib:/Developer/NVIDIA/CUDA-8.0/lib:/opt/intel//compilers_and_libraries_2017.0.102/mac/daal/lib:/opt/intel//compilers_and_libraries_2017.0.102/mac/ipp/lib:/opt/intel//compilers_and_libraries_2017.0.102/mac/tbb/lib:/opt/intel//compilers_and_libraries_2017.0.102/mac/tbb/lib:/opt/intel//compilers_and_libraries_2017.0.102/mac/compiler/lib:/opt/intel//compilers_and_libraries_2017.0.102/mac/mkl/lib\r\nI tensorflow/stream_executor/cuda/cuda_dnn.cc:3517] Unable to load cuDNN DSO\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\n```", "I added\r\n```\r\nexport DYLD_LIBRARY_PATH=\"/Developer/NVIDIA/cuDNN/lib${DYLD_LIBRARY_PATH:+:${DYLD_LIBRARY_PATH}}\"\r\n```\r\n\r\nand now ...\r\n\r\n```\r\nimport tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\n```", "First trial run ...\r\n\r\n```\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:874] OS X does not support NUMA - returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GT 750M\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.9255\r\npciBusID 0000:01:00.0\r\nTotal memory: 2.00GiB\r\nFree memory: 1.85GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow!'\r\n\r\n```\r\n\r\nDo you know if this output means everything is running appropriately? I'm not sure why there are all the warning about the tensorflow compiling without the special instruction sets.  Any ideas?", "Everything is fine. The warnings just warn you that things could be faster. ", "@martinwicke thanks!", "@esd100 you can suppress info and warning log messages in the shell with `export TF_CPP_MIN_LOG_LEVEL=2` or in python with `os.environ['TF_CPP_MIN_LOG_LEVEL']=\"2\"`", "For me (MacBook Pro macOS Sierra + **CUDA-8.0**)\r\n```\r\nexport DYLD_LIBRARY_PATH=\"/Developer/NVIDIA/CUDA-8.0/lib${DYLD_LIBRARY_PATH:+:${DYLD_LIBRARY_PATH}}\"\r\n```\r\n\r\nThen\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\n```\r\n\r\nIt works... THX @yaroslavvb @esd100 "]}, {"number": 2939, "title": "Unable to download nightly binaries", "body": "All the links in README, such as [this one](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl)\ngive me \n\n```\nHTTP ERROR 404\n\nProblem accessing /view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl. Reason:\n\n    Not Found\nPowered by Jetty://\n```\n", "comments": ["Seems to be fixed.\n", "It failed again.\n"]}, {"number": 2938, "title": "dynamic_rnn after reshape seems impossible", "body": "I need to multiply a 3D tensor by a 2D weight matrix, then feed it to dynamic_rnn.\nBelow is the code.\n\n```\ninputSize = 1000\nembeddingSize = 100\n\nbatchX = tf.placeholder(tf.float32, [None, None, inputSize])\n#The maximum length of the sequences and the size of the mini-batch are dynamic\n#batchX is a time-major 3D tensor\n\nbatchXLenghts = tf.placeholder(tf.int32, [None,])\n#A list of integers indicating the length of each sequence in batchX\n\nmaxLength = tf.shape(batchX)[0]\nbatchSize = tf.shape(batchX)[1]\n\nwith tf.variable_scope('embedding') as scope:\n    W_emb = tf.get_variable('W_emb', [inputDimSize, embDimSize], initializer=tf.truncated_normal_initializer())\n    b_emb = tf.get_variable('b_emb', [embDimSize,], initializer=tf.constant_initializer())\n\nemb = tf.reshape(tf.matmul(tf.reshape(batchX, [maxLength*batchSize, inputDimSize]), W_emb) + b_emb, [maxLength, batchSize, embDimSize])\n#embedding step. There are two reshape operations because I am doing np.dot(3D, 2D)\n\ncell = GRUCell(embDimSize)\noutputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n\n# calculating logits, loss, etc.\n...\n...\n```\n\nWhen I run this code, I get the following error\n\n```\nTraceback (most recent call last):\n  File \"feedTestDynamicRnn.py\", line 127, in <module>\n    train(xFile=xFile, yFile=yFile)\n  File \"feedTestDynamicRnn.py\", line 98, in train\n    batchX, batchXLengths, batchY, logits, mean_loss = inference(options)\n  File \"feedTestDynamicRnn.py\", line 57, in inference\n    outputs, states = rnn.dynamic_rnn(cell, emb, sequence_length=batchXLengths, time_major=True, parallel_iterations=256, dtype='float32')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 580, in dynamic_rnn\n    swap_memory=swap_memory, sequence_length=sequence_length)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 630, in _dynamic_rnn_loop\n    \"Input size (depth of inputs) must be accessible via shape inference, \"\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\n```\n\nWhen I comment out the embedding step, and feed `batchX` directly to `dynamic_rnn`, there is no problem. So it seems that `tf.reshape()` loses shape information as told by [this post](http://stackoverflow.com/questions/35374958/reshape-tensor-using-placeholder-value). \n\nThe problem is, I cannot fix `maxLength` and `batchSize` to a static value, because they need to change. Also, I need to do embedding, so directly feeding `batchX` to the RNN is not an option. Is there a work around for this?\n", "comments": ["@mp2893 I've run in to a similar problem -- try calling `emb.set_shape([None, None, embDimSize])` to work around the issue.\n", "@jlowin Thank you!! This would have never ocurred to me. I tried `emb.set_shape([maxLength, batchSize, embDimSize])` but I got an error saying I cannot use 'Tensor' in the `set_shape` argument. Then I gave up hope. Thank you very much again!!\n", "@mp2893 glad I could be helpful! You might want to leave this issue open -- I believe it's a shape inference bug.\n", "@jlowin Re-opened the issue. Thanks!\n", "It may be possible that tf.reshape needs more aggressive shape inference code.  I'll take a look and see how much we can infer at graph construction time.\n", "Assigning to @mrry  who wrote the shape inference code; Derek, is something possible here?\n", "Is it possible that this just works now? As far as I can tell...\n\n``` python\nemb = tf.reshape(..., [maxLength, batchSize, embDimSize])\n```\n\n...with `maxLength` and `batchSize` as `tf.Tensor` objects, and `embDimSize` as a Python `int` should trigger auto-packing, and the packed new-shape argument should make it back out of `tensor_util.constant_value_as_shape()` (added in https://github.com/tensorflow/tensorflow/commit/706a5baa6e633ffbbcdf49f69e3ef88421001a76 on June 25th).\n", "I'm going to assume this is now fixed. Feel free to re-open the issue if you're still having problems.\n", "I am facing a similar problem with a tensor `features` coming from a shuffle_batch queue.\r\n```\r\n(Pdb) features.shape\r\nTensorShape([Dimension(5), Dimension(10)])\r\n(Pdb) features\r\n<tf.Tensor 'shuffle_batch:0' shape=(5, 10) dtype=int32>\r\n(Pdb) tf.shape(features)\r\n<tf.Tensor 'Shape_3:0' shape=(2,) dtype=int32>\r\n(Pdb) tf.shape(features)[0]\r\n<tf.Tensor 'strided_slice_1:0' shape=() dtype=int32>\r\n(Pdb) tf.shape(features)[0].shape\r\nTensorShape([])\r\n```\r\n\r\n`features.shape` shows the correct TensorShape but when using `tf.shape(features)[0]` the output is wrong or empty.", "I'm not 100% sure what the problem is here, but I think this is working as intended:\r\n\r\n* `features` is a 5 x 10 matrix (2-D tensor).\r\n* `tf.shape(features)` is a length-2 vector (1-D tensor), which will evaluate to `[5, 10]`.\r\n* `tf.shape(features)[0]` is a scalar (0-D tensor), which will evaluate to `5`.\r\n* `tf.shape(features)[0].shape` is the `tf.TensorShape` representing the (static) shape of `tf.shape(features)[0]`. It evaluates to `TensorShape([])`, which represents a scalar (0-D tensor => empty list of dimensions).", "I'm working with TF v1.2.0.  I have the original issue, where in reshaping the input for tf.nn.dynamic_rnn(), I can set the final dimension with an integer, but the first two must be placeholders. If I reshape with this mixture of tf.placeholders() and Python integers, I don't get the auto-packing: \r\n\r\n```\r\ntf.reshape(slim.flatten(inputMat),shape=[batch_size,trainLength,h_size]) \r\n\r\nValueError: Tried to convert 'shape' to a tensor and failed. Error: Shapes must be equal rank, but are 1 and 0\r\n\tFrom merging shape 1 with other shapes. for 'Reshape_6/packed' (op: 'Pack') with input shapes: [1], [1], [].\r\n```\r\n\r\nI tried converting the final dimension to a tf.constant() and using tf.stack() to get a shape tensor, but that produced the original error in this thread about shape inference. Here's the code using tf.stack():\r\n\r\n```\r\nn_inputs = 4\r\nh_size = 10\r\n\r\nrnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size,state_is_tuple=True)\r\n\r\nh_size_T = tf.constant(h_size,dtype=tf.int32,shape=[1])\r\nbatch_size = tf.placeholder(shape=[1],dtype=tf.int32)\r\ntrainLength = tf.placeholder(shape=[1],dtype=tf.int32)\r\n\r\nrawInput = tf.placeholder(shape=[None,n_inputs],\\\r\n        \t\t\t\t\t\t\t\t dtype=tf.float32)\r\ninputW = tf.Variable(tf.random_normal([n_inputs,h_size]))\r\ninputMat = tf.matmul(rawInput,inputW)\r\n\r\nshape_input = tf.reshape(tf.stack([batch_size,trainLength,h_size_T]),shape=[3])\r\ninputFlat = tf.reshape(slim.flatten(inputMat),shape=shape_input)\r\n\r\nstate_in = np.zeros([1,h_size])\r\nrnn,rnn_state = tf.nn.dynamic_rnn(inputs=inputFlat,cell=rnn_cell,\\\r\n                                  dtype=tf.float32,initial_state=state_in,\\\r\n                                  scope='main_rnn')\r\n\r\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\r\n```\r\nAny help would be really great. Thanks!", "(Please use Stack Overflow for questions like this in future!)\r\n\r\nThe problem stems from these definitions:\r\n\r\n```python\r\nbatch_size = tf.placeholder(shape=[1],dtype=tf.int32)\r\ntrainLength = tf.placeholder(shape=[1],dtype=tf.int32)\r\n```\r\n\r\nFor some reason, the `batch_size` and `trainLength` are defined as length-1 vectors, rather than scalars. `h_size` is a scalar. Auto-packing only works if all of the elements are the same shape. I'd recommend redefining `batch_size` and `trainLength` as scalars:\r\n\r\n```python\r\nbatch_size = tf.placeholder(shape=[], dtype=tf.int32)\r\ntrainLength = tf.placeholder(shape=[], dtype=tf.int32)\r\n```\r\n\r\n...though this will probably have knock-on effects in the rest of your code. Alternatively, you could change the failing statement as follows:\r\n\r\n```python\r\ntf.reshape(slim.flatten(inputMat), shape=[batch_size[0], trainLength[0], h_size]) \r\n```\r\n\r\n...which might cause less trouble in the rest of your program, but it wouldn't pass code review if I were the reviewer :).", "Many thanks @mrry \r\n\r\nIt works!", "@mrry I am not sure if this is the same issue or something I am missing, but I am running into this:\r\nhttps://stackoverflow.com/questions/46958414/tf-reshape-loses-shape-of-the-tensor\r\n(related: https://github.com/tensorflow/tensorflow/issues/12522)"]}, {"number": 2937, "title": "Tensorflow cudnn integration status", "body": "With this issue I would like to track the efforts in integrating the\ncudnn library within tensorflow.\n\nAs of June 17th 2016 doing a manual grep on the repository gives these\nfunctions as being mapped from cudnn to the stream executor,\n\nFrom chapter 4 of the cudnn User Guide version 5.0 (April 2016):\n- [x] cudnnGetVersion\n- [ ] cudnnGetErrorString\n- [x] cudnnCreate\n- [x] cudnnDestroy\n- [x] cudnnSetStream\n- [ ] cudnnGetStream\n- [x] cudnnCreateTensorDescriptor\n- [ ] cudnnSetTensor4dDescriptor\n- [ ] cudnnSetTensor4dDescriptorEx\n- [ ] cudnnGetTensor4dDescriptor\n- [x] cudnnSetTensorNdDescriptor\n- [x] cudnnDestroyTensorDescriptor\n- [x] cudnnTransformTensor\n- [x] cudnnAddTensor\n- [ ] cudnnOpTensor\n- [x] cudnnSetTensor\n- [ ] cudnnScaleTensor\n- [x] cudnnCreateFilterDescriptor\n- [ ] cudnnSetFilter4dDescriptor\n- [ ] cudnnGetFilter4dDescriptor\n- [ ] cudnnSetFilter4dDescriptor_v3 (versioned)\n- [ ] cudnnGetFilter4dDescriptor_v3 (versioned)\n- [ ] cudnnSetFilter4dDescriptor_v4 (versioned)\n- [ ] cudnnGetFilter4dDescriptor_v4 (versioned)\n- [x] cudnnSetFilterNdDescriptor\n- [ ] cudnnGetFilterNdDescriptor\n- [ ] cudnnGetFilterNdDescriptor_v3 (versioned)\n- [ ] cudnnGetFilterNdDescriptor_v3 (versioned)\n- [ ] cudnnGetFilterNdDescriptor_v4 (versioned)\n- [ ] cudnnGetFilterNdDescriptor_v4 (versioned)\n- [x] cudnnDestroyFilterDescriptor\n- [x] cudnnCreateConvolutionDescriptor\n- [ ] cudnnSetConvolution2dDescriptor\n- [ ] cudnnGetConvolution2dDescriptor\n- [ ] cudnnGetConvolution2dForwardOutputDim\n- [x] cudnnSetConvolutionNdDescriptor\n- [ ] cudnnGetConvolutionNdDescriptor\n- [x] cudnnGetConvolutionNdForwardOutputDim\n- [x] cudnnDestroyConvolutionDescriptor\n- [ ] cudnnFindConvolutionForwardAlgorithm\n- [ ] cudnnFindConvolutionForwardAlgorithmEx\n- [x] cudnnGetConvolutionForwardAlgorithm\n- [x] cudnnGetConvolutionForwardWorkspaceSize\n- [x] cudnnConvolutionForward\n- [ ] cudnnConvolutionBackwardBias\n- [ ] cudnnFindConvolutionBackwardFilterAlgorithm\n- [ ] cudnnFindConvolutionBackwardFilterAlgorithmEx\n- [x] cudnnGetConvolutionBackwardFilterAlgorithm\n- [x] cudnnGetConvolutionBackwardFilterWorkspaceSize\n- [x] cudnnConvolutionBackwardFilter\n- [ ] cudnnFindConvolutionBackwardDataAlgorithm\n- [ ] cudnnFindConvolutionBackwardDataAlgorithmEx\n- [x] cudnnGetConvolutionBackwardDataAlgorithm\n- [x] cudnnGetConvolutionBackwardDataWorkspaceSize\n- [x] cudnnConvolutionBackwardData\n- [ ] cudnnSoftmaxForward\n- [ ] cudnnSoftmaxBackward\n- [x] cudnnCreatePoolingDescriptor\n- [ ] cudnnSetPooling2dDescriptor\n- [ ] cudnnGetPooling2dDescriptor\n- [x] cudnnSetPoolingNdDescriptor\n- [ ] cudnnGetPoolingNdDescriptor\n- [ ] cudnnSetPooling2dDescriptor_v3 (versioned)\n- [ ] cudnnGetPooling2dDescriptor_v3 (versioned)\n- [ ] cudnnSetPoolingNdDescriptor_v3 (versioned)\n- [ ] cudnnGetPoolingNdDescriptor_v3 (versioned)\n- [ ] cudnnSetPooling2dDescriptor_v4 (versioned)\n- [ ] cudnnGetPooling2dDescriptor_v4 (versioned)\n- [ ] cudnnSetPoolingNdDescriptor_v4 (versioned)\n- [ ] cudnnGetPoolingNdDescriptor_v4 (versioned)\n- [x] cudnnDestroyPoolingDescriptor\n- [ ] cudnnGetPooling2dForwardOutputDim\n- [ ] cudnnGetPoolingNdForwardOutputDim\n- [x] cudnnPoolingForward\n- [x] cudnnPoolingBackward\n- [x] cudnnActivationForward\n- [ ] cudnnActivationBackward\n- [x] cudnnCreateActivationDescriptor\n- [x] cudnnSetActivationDescriptor\n- [x] cudnnGetActivationDescriptor\n- [x] cudnnDestroyActivationDescriptor\n- [ ] cudnnActivationForward_v3  (versioned)\n- [ ] cudnnActivationBackward_v3 (versioned)\n- [ ] cudnnActivationForward_v4  (versioned)\n- [ ] cudnnActivationBackward_v4 (versioned)\n- [ ] cudnnCreateLRNDescriptor\n- [ ] cudnnSetLRNDescriptor\n- [ ] cudnnGetLRNDescriptor\n- [ ] cudnnDestroyLRNDescriptor\n- [ ] cudnnLRNCrossChannelForward\n- [ ] cudnnLRNCrossChannelBackward\n- [ ] cudnnDivisiveNormalizationForward\n- [ ] cudnnDivisiveNormalizationBackward\n- [ ] cudnnBatchNormalizationForwardInference\n- [ ] cudnnBatchNormalizationForwardTraining\n- [ ] cudnnBatchNormalizationBackward\n- [ ] cudnnDeriveBNTensorDescriptor\n- [ ] cudnnCreateRNNDescriptor\n- [ ] cudnnDestroyRNNDescriptor\n- [ ] cudnnSetRNNDescriptor\n- [ ] cudnnGetRNNWorkspaceSize\n- [ ] cudnnGetRNNTrainingReserveSize\n- [ ] cudnnGetRNNParamsSize\n- [ ] cudnnGetRNNLinLayerMatrixParams\n- [ ] cudnnGetRNNLinLayerBiasParams\n- [ ] cudnnRNNForwardInference\n- [ ] cudnnRNNForwardTraining\n- [ ] cudnnRNNBackwardData\n- [ ] cudnnRNNBackwardWeights\n- [ ] cudnnCreateDropoutDescriptor\n- [ ] cudnnDestroyDropoutDescriptor\n- [ ] cudnnDropoutGetStatesSize\n- [ ] cudnnDropoutGetReserveSpaceSize\n- [ ] cudnnSetDropoutDescriptor\n- [ ] cudnnDropoutForward\n- [ ] cudnnDropoutBackward\n- [ ] cudnnCreateSpatialTransformerDescriptor\n- [ ] cudnnDestroySpatialTransformerDescriptor\n- [ ] cudnnSetSpatialTransformerNdDescriptor\n- [ ] cudnnSpatialTfGridGeneratorForward\n- [ ] cudnnSpatialTfGridGeneratorBackward\n- [ ] cudnnSpatialTfSamplerForward\n- [ ] cudnnSpatialTfSamplerBackward\n### Batch Normalization\n\nSeems @lukemetz was working on it but has stalled for a bit https://github.com/tensorflow/tensorflow/pull/1759\n### What is the plan for the RNN ?\n\nI know @wchan was working on a [cpu version](https://github.com/tensorflow/tensorflow/pull/2002). I was trying to get a stub at [using the cudnn version](https://github.com/Mistobaan/tensorflow/tree/feature/cudnn-rnn-lstm) but from the comments in that thread seemed like @zheng-xq was working on it internally.\nCan anyone comment on the status of these ops?\n### Other Questions\n1. Any reasons why the Softmax functions are not being used ?\n2. Would make sense to split the above list in chunks and create issues with contribution welcome, so external contributors can tackle them without duplicating internal work, or mark what google will be working on internally ?\n\nI hope this helps in organizing the work around the cudnn and inspire the community to contribute. I will try to keep this issue up to date.\n", "comments": ["In general, TensorFlow prefers its own implementation on GPU, if the performance is close enough. This is because we would have the source code for customization. Conv is a good example where it would take a lot of effort to replicate Cudnn's performance. \n\nThere is still an ongoing investigation how TensorFlow would expose a fused RNN/LSTM implementations. The current Cudnn interface does not provide enough customization to accommodate common variations. In the mean time, it is possible that we can add a wrapper op in TensorFlow/contrib around the current Cudnn implementation, before we can decide what is the best way for long-term support in the TensorFlow core. \n", "@zheng-xq How much of this policy is related to the blend effort of each framework to an API conformation on cudnn and how much to the control  of  kernels code from open source point of view?\n", "@bhack, in case of LSTM, the biggest concern is lack of customization. In general, having access to the source code gives us better ability us to tweak our code to better serve TensorFlow users. \n\nThat said, we won't hesitate to pick any good functionalities from any library including Cudnn, if there is evidence showing a sizeable benefit.\n", "Closing this tracking bug because it seems there is not a goal to map all cuDNN functions for reasons given by @zheng-xq. Please open separate issues to track specific missing cuDNN functionality.\n", "@michaelisard Can you give me an estimated ratio between not satisfied API design and the closed source nature of cudnn that not attracted full coverage? \n", "@zheng-xq can comment better than I can.\n", "You can close. I was only curious.\n", "I would be interested in having full support of the cuDNN library through the stream executor and all the goodies that comes with it. Expose all the endpoints in the cudnn layer and only few to the public layer. \n\n@zheng-xq  did the problem with cudnnRNN got solved at the end? When I tried the wrapping I wasn't sure where to store the temporary variables required by the cudnn library.\n", "@bhack, I don't think anyone sets a clear ratio. And it varies case by case for each op. For new ops, we firstly prefer a fast solution with source within reasonable amount of effort. If it takes a lot of effort to match the performance of Cudnn, or any other library, we would not hesitate to use that library. Overall, we go with the fastest solution. If the performance is about the same between our custom code and Cudnn, we prefer the one with the source.\n\n@Mistobaan, we are working on adding a Cudnn RNN layer to tf.contrib. We will announce in the release note when it is done. \n"]}, {"number": 2936, "title": "Fixed makefile problems with new files and zlib dependency", "body": "", "comments": []}, {"number": 2935, "title": "Exception when trying to use TensorFlowDNNClassifier with Scikit-Learn model_selection.cross_val_score", "body": "I am attempting to use cross_val_score with the TensorFlowDNNClassifier (I know it is deprecated) from Scikit-Learn. I used cross_val_score seamlessly with the original skflow wrapper which allowed me to use evolutionary computing to evolve the parameters of the TensorFlowDNNClassifier. \n\nIt appears that when a clone of the TensorFlowDNNClassifier is initialised for the number of folds specified in cross_val_score, the hidden_units argument is not included as the estimator.get_params() function within clone in sklearn/base.py does not retrieve the value set against _dnn_hidden_units. \n\nThe classifiers/regressors of Scikit-Learn take the approach of assigning the input parameters a default value which allows for a lot of flexibility. Instead of requiring the user to provide the structure of their DNN through the hidden_units parameter perhaps a default value should be provided so that the classifier can be cloned when the classifier is passed to cross_val_score.\n\nGiven that the objective of Skflow was to integrate tensorflow into the Scikit-learn pipeline it would be ideal to turn hidden_units into a key word argument.\n### Environment info\n\nOperating System:\nLinux Mint 17.2 Cinnamon 64 bit\n\nInstalled version of CUDA and cuDNN: \nNot relevant\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   pip3 install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp35-cp35m-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.9.0rc0\n\npython - 3.5.1\nscikit-learn - 0.18.dev0\ntensorflow - 0.9.0rc0\nscipy - 0.17.1\nnumpy - 1.11.0\n### Steps to reproduce\n1. Code provided\n\n```\nfrom sklearn import datasets, model_selection\nfrom tensorflow.contrib import learn\nimport traceback\n\nif __name__ == '__main__':\n    try:\n\n        iris = datasets.load_iris()\n\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n\n        kwargs = {\n            \"n_classes\":3,\n            \"optimizer\" : \"Adam\",\n            \"hidden_units\" : [10, 20, 10],\n            \"steps\": 250\n        }\n\n        classifier = learn.TensorFlowDNNClassifier(**kwargs)\n\n        scores = model_selection.cross_val_score(\n            classifier,\n            X_train,\n            y_train,\n            scoring=\"accuracy\"\n        )\n\n        print('Accuracy: {0:f}'.format(scores))\n\n    except:\n        print(traceback.format_exc())\n```\n### Logs or other output that would be helpful\n\n```\nTraceback (most recent call last):\n  File \"/home/thirdoctet/Development/projects/python/scikit-evolution/skevolve/tests/tensorflow_classification.py\", line 47, in <module>\n    scoring=\"accuracy\"\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\", line 167, in cross_val_score\n    for train, test in cv.split(X, y, labels))\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 800, in __call__\n    while self.dispatch_one_batch(iterator):\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 653, in dispatch_one_batch\n    tasks = BatchedCalls(itertools.islice(iterator, batch_size))\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 68, in __init__\n    self.items = list(iterator_slice)\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\", line 167, in <genexpr>\n    for train, test in cv.split(X, y, labels))\n  File \"/usr/local/lib/python3.5/site-packages/sklearn/base.py\", line 57, in clone\n    new_object = klass(**new_object_params)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.py\", line 452, in __init__\n    super(DeprecatedMixin, self).__init__(*args, **kwargs)\nTypeError: __init__() missing 1 required positional argument: 'hidden_units'\n```\n", "comments": ["@thirdOctet Thanks for reporting! I've added a temporary fix in #3137. Please use `DNNClassifier` as seen in the added test. \n", "At this moment compatibility with model selectors in scikit learn has been\ndisabled. We are implementing model selection in tf.learn itself\nOn Fri, Jul 1, 2016 at 6:37 PM ebrevdo notifications@github.com wrote:\n\n> Assigned #2935 https://github.com/tensorflow/tensorflow/issues/2935 to\n> @ilblackdragon https://github.com/ilblackdragon.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2935#event-711082685,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAKtfhsVzAVEnLJfY6aGP9YmW70OQ4LOks5qRcDTgaJpZM4I4n3U\n> .\n", "I see. Well thank you for the update and for investigating the issue @terrytangyuan. On the road map of tasks for tf.learn when might I expect the implementation of model selection to be complete? @ilblackdragon \n", "@thirdOctet it should work now\n"]}, {"number": 2934, "title": "Fix genrule Python execution problem with Python3.", "body": "Change: 124202095\n", "comments": []}, {"number": 2933, "title": "Make RNN api public.", "body": "Change: 124305799\n", "comments": ["@martinwicke can you review?\n"]}, {"number": 2932, "title": "Revert premature version bump in master branch", "body": "", "comments": ["@tensorflow-jenkins test this please.\n"]}, {"number": 2931, "title": "Use asarray to avoid copy - ~2x speedup for feed_dict", "body": "This starts to address #2919 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Jenkins, test this please.\n", "@nanddalal can you sign the CLA?\n", "Hi, sorry for the delay - I am still working on signing the CLA so I can contribute later on, but if you were already able to incorporate this change in a different PR, that would be great.\n", "It turns out I fixed this independently in another commit (which is will appear in the next push from internal) - thanks for noticing the issue though!\n"]}, {"number": 2930, "title": "Is contrib/quantization available through Python interface?", "body": "It works in C++ if I build label_image example but if I call through python, I got:\ntensorflow.python.framework.errors.NotFoundError: Op type not registered 'QuantizeV2'\n\nIs there any build file I need to update? I tested on RC0.9 release. Thanks. \n", "comments": ["In Python it's call  tf.contrib.quantization.python.quantize_v2.\n", "Please let me know if it works for you. Thanks.\n", "Thanks for the response. It happened when I load the pretrained and converted graph as\n`\n        with tf.gfile.FastGFile(model_path, 'rb') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n            _ = tf.import_graph_def(graph_def, name='')\n`\nand then run the session\n`\n            correct += np.sum(sesh.run(top_k_op,\n                                       feed_dict={input_node: images,\n                                                  label_node: labels}))\n`\nDoes it mean I have to update the the graph (.pb) file? Thanks.\n\nbtw, I am new to tensorflow. \n", "I found the solution from Pete Warden's blog. @petewarden Thanks. Pete provided solutions for both Python and C++ interface cases while I misread python part. \n"]}, {"number": 2929, "title": "IOS - No OpKernel was registered to support Op 'Conv2DBackpropInput' with these attrs", "body": "Hi everyone,\n\nWe are trying to load a very simple graph inside IOS that we generate and freeze with python.\nRight now, we have this following error:\n\n```\nE Running model failed: Invalid argument: \nNo OpKernel was registered to support Op 'Conv2DBackpropInput' with these attrs\n     [[Node: convt = Conv2DBackpropInput[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Shape, W/read, x)]]\n```\n\nHere is the python script generating the model:\n\n``` python\nfrom tools import freeze_graph\n\nimport tensorflow as tf\n\nprint('Init session')\nsess = tf.InteractiveSession()\n\nprint('Define vars and ops')\nx = tf.placeholder(tf.float32, shape=[None, 600, 600, 3], name=\"x\")\nW = tf.Variable(tf.truncated_normal([3, 3, 3, 3], stddev=0.1), name=\"W\")\nconvt = tf.nn.conv2d_transpose(x, W, tf.shape(x), strides=[1, 1, 1, 1], padding='SAME', name=\"convt\")\n\nprint('Init vars')\ninit = tf.initialize_all_variables()\nsess.run(init)\n\ninput_graph_name = \"input_graph.pb\"\n\nsaver = tf.train.Saver(var_list=None)\nsaver.save(sess, 'data/conv', global_step=None)\n\ngraph_def = sess.graph.as_graph_def()\ntf.train.write_graph(graph_def, \"data\", input_graph_name)\n\ninput_graph_path = 'data/' + input_graph_name\ninput_saver_def_path = \"\"\ninput_binary = False\ninput_checkpoint_path = 'data/conv'\noutput_node_names = \"convt\"\nrestore_op_name = \"save/restore_all\"\nfilename_tensor_name = \"save/Const:0\"\noutput_graph_path = \"data/frozen_convt.pb\"\nclear_devices = True\n\nfreeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n                          input_binary, input_checkpoint_path,\n                          output_node_names, restore_op_name,\n                          filename_tensor_name, output_graph_path,\n                          clear_devices, \"\")\n```\n\nHere is the Objective-C++ interesting part:\n\n``` C\n //  2. Load the network\n\n  NSString* network_path = FilePathForResourceName(@\"frozen_conv\", @\"pb\");\n  PortableReadFileToProto([network_path UTF8String], &tensorflow_graph);\n\n  LOG(INFO) << \"Creating session.\";\n  tensorflow::Status s = session->Create(tensorflow_graph);\n\n  if (!s.ok()) {\n    LOG(ERROR) << \"Could not create Tensorflow Graph: \" << s;\n  }\n\n\n  //  3. Run the network\n\n    std::string input_layer = \"x:0\";\n    std::string output_layer = \"convt:0\";\n\n    std::vector<tensorflow::Tensor> outputs;\n    tensorflow::Status run_status = session->Run({{input_layer, image_tensor}},\n                                                 {output_layer}, {}, &outputs);\n\n\n    if (!run_status.ok()) {\n      LOG(ERROR) << \"Running model failed: \" << run_status;\n    }\n```\n\nWe are using:\n- Python 3.5.1 for the script\n- The python tensorflow build is downloaded from `https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0rc0-py3-none-any.whl`\n- The compiled tensorflow for IOS is from the master branch 2 hour ago using the script `build_all_ios.sh`\n- We have **no** problem running the IOS example\n- We have **no** problem running a one standard convolutional graph frozen with custom python script\n- We checked that the file `tensorflow/core/ops/nn_ops.cc` is in the generated txt files (and this is the file containing the registering of the op 'Conv2DBackpropInput')\n\nI would be gratefull if anyone has an idea on why IOS seems to not be able to find the `Conv2DBackpropInput` Op ?\n", "comments": ["Sorry you're hitting problems, and thanks for the very clear report!\n\nBecause most mobile use cases are focused on inference, we don't support gradient ops like Conv2DBackpropInput. In this case though, the op is actually used under the hood by conv2d_transpose(), and so isn't removed when freeze_graph strips out training operations.\n\nAs a temporary solution, you should be able to add tensorflow/core/kernels/conv_grad_ops.cc to this file and re-run build_all_ios.sh:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_cc_files.txt#L77\n\nI'll work on getting a proper fix in.\n", "Never mind, the op is already listed in that file. I'll keep digging to see what's actually wrong here.\n", "I wasn't able to reproduce this locally, and on a deeper look I discovered that I'd fixed this in #2936 when I regenerated the file lists from the latest Bazel build. If you get top-of-tree, conv_grad_ops.cc is included and you should no longer see this problem.\n\nCan you confirm updating fixes this for you too?\n", "Hi Petewarden,\n\nThanks a lot, it is fixed! that was fast ;)\n", "It's me again, i've tried something else and found a new error, here is the piece of code:\n\n``` python\nW_1 = tf.Variable(tf.truncated_normal([3, 3, 3, 16], stddev=0.1), name=\"W_1\")\nb_1 = tf.constant(0.1, shape=[16], name=\"b_1\")\nconv_1 = tf.nn.conv2d(x, W_1, strides=[1, 2, 2, 1], padding='SAME', name=\"conv_1\")\n# PRelu\npos = tf.nn.relu(conv_1)\nneg = -0.1 * (conv_1 - tf.abs(conv_1)) * 0.5\nh_conv1 = pos + neg\n```\n\nhere is the error:\n`Running model failed: Invalid argument: No OpKernel was registered to support Op 'Abs' with these attrs\n     [[Node: Abs = Abs[T=DT_FLOAT](conv_1)]]`\n\nIt seems that the little part `tf.abs(conv_1))` can be frozen but not imported in the mobile\n", "I did a bit of digging into this, and the problem is that cwise_op_abs.cc is not included in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_cc_files.txt\n\nThis is generated from the Bazel build files, so I have a proper fix in review, but in the short term you should just be able to add tensorflow/core/kernels/cwise_op_abs.cc to that file locally, and re-run compile_ios_tensorflow.sh.\n", "Yes, thanks! (it's working)\n\nDo you know why some files could be missing from the bazel generated list ?\n", "It's an error on our end, we manually pick the ops that we want to include and it appears that abs was excluded, probably by accident. I've added it back in internally, and once that goes through I'll regenerate the tf_cc_files.txt.\n", "Given that the graph must be built in Python first, it might be useful to have a utility to parse a given graph definition and generate a list of the ops used in that graph. Then it would be possible to have a core lib to do the basics and a custom lib containing only the ops needed to run the graph on a mobile.\n", "We do actually have something like that running internally, to help us shrink the binary footprint. It relies on some features of our internal build system though, so we'll need to do a little work to get it into the public repo. It's something we're hoping to get done though.\n\nI'll close this bug for now, since it sounds like the original issue is resolved?\n", "Yes, it's resolved on my side. \n\nThe graph parser would be awesome for anybody who wants to avoid cherry picking deps. Looking forward to see it appear here!\n\nThanks again for your speed.\n", "I have followed the steps from Tensorflow For Poets (link : https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0 ) to train my own dataset using tensorflow, and getting very good prediction result.This provides me retrained.py, label_images.py, retrained_graph.pb, retrained_labels.txt files.So i have my own model graph and label file. \r\n\r\nAfter that i have downloaded Tensorflow Example Project for iOS from github. The project runs successfully in iOS platform. but when i am using my own retrained_graph.pb, retrained_labels.txt file on behalf of examples model and label file , i am getting error as  following - \r\n\r\nErrors : \r\n\r\nCould not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\r\n\r\nand ,\r\n\r\n Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\t [[Node: DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]\r\n\r\nCan you help me please ? I have been searching  a long time for it . @petewarden .\r\nthanks "]}, {"number": 2928, "title": "Zeros function not correctly determining shape.", "body": "I want to have dynamic batch sizes so I defined my input tensors as follows.\n\n`input_tensor = tf.placeholder(tf.float32, (None, TIME_STEPS, 128), 'input_tensor')`\n\nI then calculate the batch size.\n\n`batch_size = tf.shape(input_tensor)[0]`\n\nI can use this to determine the initial state for my RNNs as follows...\n\n```\ninitial_state = state = tf.zeros((batch_size, encoder_multi_rnn.state_size), tf.float32)\n...\nforward_outputs[t], state = encoder_multi_rnn(getTimeStep(input_tensor, t), state)\n```\n\nThis seems to work fine when I evaluate tf.shape(initial_state) in my sess.run() it prints what appears to be the correct value.  However, when do the following it prints (?, ?).  I think the second dimension should not be a ?\n\n```\n    state = tf.zeros((batch_size, encoder_multi_rnn.state_size), tf.float32)\n    print(state.get_shape())\n```\n\nThis is not causing an issue and my code compiles fine.  However this is causing an issue when I try to feed the output of the last time step of my sequence generating decoder back into the input of the MultiRNN.  On the first time step I feed it zeros since no output has yet to be produced.\n\n```\n    rnn_input = tf.reduce_sum(w * decoder_input, 1)\n    last_out = decoder_outputs[t - 1] if t else tf.zeros((batch_size, 128))\n    rnn_input = tf.concat(1, (rnn_input, last_out))\n```\n\nI get this error when concat is called.  ValueError: Linear expects shape[1] of arguments: [[None, None], [None, 1024]]\n\nWhen I print last_out.get_shape() I get (?, ?) again.  In this case I think it should be (?, 128) since the last dimension is clearly defined.  \n", "comments": ["Hi @chasep255. If it's a runtime decision, you should do\n\nlast_out = tf.cond(t > 0, lambda: decode_output[t-1], lambda: tf.zeros((batch_size, 128))\n\nCould you please give it a try and see if it works for you? If not, could you cut and paste the complete program? Thanks.\n", "It is not a runtime decision.  t is the time step.  Here is a more complete section.  I already fixed the issue by just defining a batch size so I don't have the original code.\n\nHere is what is looks like now.  See how t is just the loop variable.  \n\n```\nwith tf.variable_scope('decoder') as scope:\n    W_attn = tf.get_variable('W_attn', (TIME_STEPS, TIME_STEPS), tf.float32, tf.truncated_normal_initializer(1.0 / TIME_STEPS, 0.01))\n    W_sm = tf.get_variable('W_softmax', (decoder_gru_size, 128), tf.float32, tf.truncated_normal_initializer(0.0, 1 / np.sqrt(decoder_gru_size)))\n    b_sm = tf.get_variable('b_softmax', 128, tf.float32, tf.truncated_normal_initializer(0.0, 0.01))\n    decoder_outputs = [None] * TIME_STEPS\n    state = tf.zeros((BATCH_SIZE, decoder_multi_rnn.state_size), tf.float32)\n    for t in range(TIME_STEPS):\n        w = tf.reshape(W_attn[t, :], (1, TIME_STEPS, 1))\n        rnn_input = tf.reduce_sum(w * decoder_input, 1)\n        last = tf.cond(is_training, lambda: getTimeStep(expected_output, t - 1), lambda: decoder_outputs[t - 1]) if t else tf.zeros((BATCH_SIZE, 128))\n        rnn_input = tf.concat(1, (rnn_input, last))\n        rnn_output, state = decoder_multi_rnn(rnn_input, state)\n        decoder_outputs[t] = tf.nn.softmax(tf.matmul(rnn_output, W_sm) + b_sm)\n        scope.reuse_variables()\n    output_tensor = tf.concat(1, [tf.reshape(t, (-1, 1, 128)) for t in decoder_outputs])\n```\n", "Glad you managed to get it working.  I think your initial post has some confusion between the statically known shapes of ops/tensors in the graph, and the dynamically computed shapes of tensors evaluated when the graph is executed  (via session.run, feeding in placeholders) ?\n\ne.g. The input_tensor is a placeholder with partially specified shape.  We don't know until the input is fed what the actual batch size is (and it may vary from run to run).  You then take the _dynamic_ shape of the input (using a tf.shape op in the graph)  and slice off the first element.  \n\n```\ninput_tensor = tf.placeholder(tf.float32, (None, TIME_STEPS, 128), 'input_tensor')\nshape_of_input_tensor = tf.shape(input_tensor)\nbatch_size = shape_of_input_tensor[0]\n```\n\nIf we examine 'shape_of_input_tensor' we see that it is a vector whose value which cannot be computed until the input is fed into the graph.\n\n```\n>>> print shape_of_input_tensor\nTensor(\"Shape_3:0\", shape=(3,), dtype=int32)\n```\n\nNote that batch_size is actually the output of a dynamic 'Slice' operation \n\n```\n>>> print batch_size\nTensor(\"Squeeze_2:0\", shape=(), dtype=int32)\n>>> print batch_size.op                                                                                                                                                        \nname: \"Squeeze\"\nop: \"Squeeze\"\ninput: \"Slice\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_INT32\n  }\n}\nattr {\n  key: \"squeeze_dims\"\n  value {\n    list {\n      i: 0\n    }\n  }\n}\n```\n\nIn contrast, 'state.get_shape()' is a static property of the graph and so cannot know the batch size until the placeholder is fed at runtime.  See here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L329\n\nI couldn't reproduce further without know what 'encoder_multi_rnn.state_size' returns.  It clearly can't be a simple integer:\n\n```\n>>> STATE_SIZE=27\n>>> state = tf.zeros((batch_size, STATE_SIZE), tf.float32)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 622, in zeros\n    shape = ops.convert_to_tensor(shape, name=\"shape\")\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 566, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.py\", line 162, in constant\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 332, in make_tensor_proto\n    _AssertCompatible(values, dtype)\n  File \"/usr/local/google/home/pbar/tensorflow.virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 269, in _AssertCompatible\n    raise TypeError(\"List of Tensors when single Tensor expected\")\nTypeError: List of Tensors when single Tensor expected\n```\n\nNote that the args of tf.zeros are passed through [tf.convert_to_tensor](https://www.tensorflow.org/versions/r0.9/api_docs/python/framework.html#convert_to_tensor):\n\nWithout the entire original program, it's hard to follow why shape inference of last_shape (all the way through your encoder) was not working.  \n\nCould you provide a simple repro test case?  Failing that I propose we close this issue.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 2927, "title": "iOS example Retrained Stripped model memory usage causes app to crash without warning", "body": "### Environment info\n\nOperating System: iOS\n### Steps to reproduce\n1. create graph with image_retraining example\n2. use strip_unused tool on graph with input_node_names=Mul, output_node_names=final_result\n3. run iOS example with the retrained stripped graph\n4. 80% of the time the app crashes, apparently because it allocates too much memory\n### What have you tried?\n1. reducing the size of the image being processed\n### Logs or other output that would be helpful\n\nI /Users/matp/Documents/projects/mytrue/third-party/tensorflow3/tensorflow-0.9.0rc0/tensorflow/contrib/ios_examples/simple/RunModelViewController.mm:225] Session created.\nI /Users/matp/Documents/projects/mytrue/third-party/tensorflow3/tensorflow-0.9.0rc0/tensorflow/contrib/ios_examples/simple/RunModelViewController.mm:228] Graph created.\nI /Users/matp/Documents/projects/mytrue/third-party/tensorflow3/tensorflow-0.9.0rc0/tensorflow/contrib/ios_examples/simple/RunModelViewController.mm:235] Creating session.\nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n\n![image](https://cloud.githubusercontent.com/assets/4634115/16151795/dae3e988-346c-11e6-86ea-8ba085165887.png)\n", "comments": ["Sorry you're hitting problems! We actually have a few different ways to reduce memory usage. One of them is to run tensorflow/contrib/quantization/tools/quantize_graph with --mode=weights to shrink the size of the model on disk by quantizing the weights to eight bits. I'm also working on folding in the batch normalization ops into the weights, which will reduce memory pressure.\n\nWe also have memory-mapped constants, which you can try by running this script on your graph:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/util/convert_graphdef_memmapped_format.cc\n\nI haven't tried this myself yet on iOS, but with previous projects it was a great way to reduce memory problems since mapped files don't seem to count towards your overall usage, and are automatically swapped out when pressure is high.\n", "@mat-peterson have you tried Pete's suggestion and does it work? I'm still having problem using the quantized model (same as https://github.com/tensorflow/tensorflow/issues/3619).\n\n@petewarden i'm trying to understand why the Inception v1 can work with the iOS simple project on actual device (and Run Model many times) but a retrained and stripped model based on it simply crashes on device when Run Model? What's the difference there? Any insights would be appreciated! \n", "Oh I just realized one probably big difference: the inception v1 model is about 50+MB and my retrained model, which is based on inception v3, is close to 90MB. Maybe that's why. \n\nSee my updates in https://github.com/tensorflow/tensorflow/issues/3619 on how to resolve the memory issue using the quantized v1 model running on actual device.\n", "I'm hopeful that these issues are solved with the new approach documented here:\nhttps://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/\n\nI'm closing this bug, but please reopen if you're still seeing the same issue.\n", "Thanks @petewarden. For a relatively old approach but one that works for me in building an iOS app published in App Store, please take a look at my blog documenting the whole process of using a retrained, stripped and quantized inception v3 model at http://jeffxtang.github.io\n"]}, {"number": 2926, "title": "Any plans to support tf.map_fn/tf.fold{l,r} with multiple tensors?", "body": "I might be interested in potentially working on this too, but just want to make sure that this doesn't conflict with any planned work :)\n", "comments": ["Please feel free to send a PR.\n\nAdded @yuanbyu just in case.\n", "Some form of this is probably at HEAD.  Could you check if it meets your needs? If not, contributions are very welcome.\n", "Multiple tensors are now supported for map_fn and scan.\n", "Can you please give an example of how can map_fn be used for multiple tensors? Does it take a list of tensors and expects a function call on a list of 'unpacked' tensors? @ebrevdo \n", "See the doc.\n"]}]