[{"number": 9858, "title": "Function decode_raw ignoring parameter little_endian.", "body": "`little_endian` parameter in `decode_raw` takes no effect. I had a look at the C++ source code (`tensorflow/tensorflow/core/kernels/decode_raw_op.cc`), you are just using `reinterpret_cast` without any awareness of this parameter.", "comments": ["Would you be willing to send a PR?", "I hope PR #9876 could be good.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Fixed in #9876."]}, {"number": 9857, "title": "Is there a way to use tensorflow to load a few consecutive frames from a queue of many video frame folders?", "body": "### System information\r\n- **OS Platform and Distribution **: Linux fedora fc25.x86_64\r\n- **TensorFlow installed from (source or binary)**: from pip\r\n- **TensorFlow version (use command below)**: v1.0.0-rc2-15-g47bba63-dirty\r\n- **CUDA/cuDNN version**: cuda 8.0/cudnn 5\r\n- **GPU model and memory**: TITAN X (Pascal)\r\n### Describe the problem\r\nI have a file which lists video names. Frames of each video are saved in a separate folder. Now I want to use tensorflow to get a queue from list of video names and then load a subset of consecutive frames from a video folder every time. Is there a way to do that? \r\nI tried the following way but it does not work.\r\n\r\n### Source code / logs\r\ninput_queue is got using tf.train.slice_input_producer.\r\n```\r\ndef glob_frame_list(video_path):\r\n     frame_list = gfile.Glob(os.path.join(video_path[0]))\r\n     return frame_list\r\ndef read_video_sequence_from_disk(input_queue, seq_len, input_size):    \r\n    frame_list = tf.py_func(glob_frame_list, [input_queue], tf.string) # TensorShape(None)\r\n    frames = []\r\n    start_idx = tf.random_uniform([], minval=0, maxval=tf.shape(frame_list)[0]-seq_len, dtype=tf.int32) # TensorShape([])\r\n    # for i in tf.range(start_idx, start_idx+seq_len): # has a problem of 'tensor is not iterable'\r\n    def body(i):\r\n        img_contents = tf.read_file(frame_list[i]) \r\n        img = tf.image.decode_jpeg(img_contents, channels=3)        \r\n        frames.append(tf.expand_dims(img, axis=0))\r\n        return i+1    \r\n    i = start_idx\r\n    while_condition = lambda i: tf.less(i, start_idx+seq_len)\r\n    new_i = tf.while_loop(while_condition, body, [i])\r\n    frames = tf.concat(frames, axis=0)\r\n    return frames\r\n```\r\n   \r\nWhen I run \r\n`threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n`there is the following error.\r\n\"\r\nInternalError (see above for traceback): Failed to run py callback pyfunc_0: see error log.\r\n         [[Node: create_inputs/PyFunc = PyFunc[Tin=[DT_STRING], Tout=[DT_STRING], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](create_inputs/PyFunc/input_0)]]\r\n\"\r\nCan anyone help me on this?\r\nThanks a lot.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thanks. I have already posted it on stackoverflow. \r\nI think it would be good to have videosequence reader.", "@stephenjia  Did you get the issue resolved? I agree such feature would be really useful. I can't find a way to do it without previously creating TFRecords or using feed_dict"]}, {"number": 9856, "title": "Bazel Clean hangs ", "body": "Hi ,\r\n\r\nI am trying to build TensorFlow example for Android using Bazel but it hanged. So I tried to find the reason\r\nand after a while tried to clean the previous build with \"bazel clean\" and it hangs again the same way as it happened while building TensorFlow examples:\r\n\r\n$ bazel clean\r\n.................................................................................................................................................................................{hangs here}\r\n\r\nIt doesn't move beyond that and we get no other info. Is there any other way to debug this or log the \"bazel clean\" output which we can later use to debug the issue?\r\n\r\nWe are using:\r\ntensorflow: 1.1.0\r\nPython: 2.7.6\r\n\r\nWe tried to find bazel version using \"bazel version\" now this is also hanging:\r\n$ bazel version\r\n. {hangs here}\r\n\r\nLooks like its an issue with bazel. may be the installation is not correct. Can someone help?\r\n", "comments": ["Tried for bazel version after rebooting machine and following is the output:\r\n\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\n", "That looks like a bazel issue, could you report it there?\r\n\r\nPlease also report OS.\r\nCC @jart ", "Yes please report to Bazel. Also be sure to provide them with information about your OS, computer, etc."]}, {"number": 9855, "title": "Unexpected error at contrib.seq2seq's BeamSearchDecoder", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:  v1.1.0-rc2-773-g7fa0cf39f\r\n- **Bazel version (if compiling from source)**:  0.4.5\r\n- **CUDA/cuDNN version**:  None\r\n- **GPU model and memory**: CPU\r\n\r\n### Describe the problem\r\n\r\nI encountered an unexpected error at tf.contrib.seq2seq's BeamSearchDecoder, *when the beam width is smaller than number of vocabs*.\r\nThis is a part of `_beam_search_step` operation in `beam_search_decoder.py`:\r\n\r\n```\r\n  scores = _get_scores(\r\n      log_probs=total_probs,\r\n      sequence_lengths=new_prediction_lengths,\r\n      length_penalty_weight=length_penalty_weight)\r\n\r\n  time = ops.convert_to_tensor(time, name=\"time\")\r\n  # During the first time step we only consider the initial beam\r\n  scores_flat = control_flow_ops.cond(\r\n      time > 0,\r\n      lambda: array_ops.reshape(scores, [batch_size, -1]),\r\n      lambda: scores[:, 0])\r\n\r\n  # Pick the next beams according to the specified successors function\r\n  next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=beam_width)\r\n  next_beam_scores.set_shape([static_batch_size, beam_width])\r\n  word_indices.set_shape([static_batch_size, beam_width])\r\n```\r\n\r\nSince the shape of `scores` is `[batch_size, beam_width, vocab_size]`,  the shape of `scores_flat` is`[batch_size, vocab_size]` at time step 0. However, if `k < shape[-1]` in `nn.top_k` operation, it just throws `InvalidArgumentError: input must have at least k columns`. Thus the code just throws an error and dies.\r\nI think code should be modified to handle cases where `vocab_size ** n < beam_width`, or at least throw an appropriate error message when the input is `vocab_size < beam_width`.\r\n\r\n", "comments": ["Nice catch!  Seems like the `top_k` operation should be shielded with a `min`.  Would be up for submitting a small PR?", "OK, I fixed the issue and made PR #9875"]}, {"number": 9854, "title": "TensorBoard --logdir=c:\\foo support", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno.  I am working with the full example to get the tensorboard to work.  I changed the file directory for the logs into something on my system:\r\n\r\n  `train_writer = tf.summary.FileWriter('D:/logs_dt' + '/train', sess.graph)`\r\n  `test_writer = tf.summary.FileWriter('D:/logs_dt' + '/test')\r\n`\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindow 10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary with GPU\r\n- **TensorFlow version (use command below)**:\r\n1.1.0\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0\r\ncuDNN v5 for CUDA 8.0 (27 May 2016)\r\n- **GPU model and memory**:\r\nNVIDIA GTX 1070 4GB\r\n- **Exact command to reproduce**:\r\nfrom windows cmd: \r\n`tensorboard --logdir='D:\\logs_dt'`\r\n\r\nYou can collect some of this information using our environment capture script:\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/995339/tf_env.txt)\r\n\r\npython\r\n1.1.0\r\n\r\n### Describe the problem\r\nI'm not seeing anything on the tensorboard at all.  I can see where the log director(ies) is/are created by the tutorial script, and the subdirectories /test and /train are there with the event data present.  I point the tensorboard to the populated log directory with the following command,  but it cannot see the event files.  Nothing is present in tensorboard, and I'm redirected back to the tutorials.\r\n`tensorboard --logdir='D:\\logs_dt'`\r\n\r\n### Source code / logs\r\n[tensor_board_hello.zip](https://github.com/tensorflow/tensorflow/files/995362/tensor_board_hello.zip)\r\n[logs_dt.zip](https://github.com/tensorflow/tensorflow/files/995357/logs_dt.zip)\r\n", "comments": ["related to #9701?", "It looks like @IgorX2 had same issue in #8845.  I reproduced his error too with his data.   His thread references #7856, which has the following workaround posted by @mrry on 26 Feb\r\n\r\n>Unfortunately TensorBoard uses a colon as the separator between the optional \"run name\" and the path in the --logdir flag. That means that --logdir=E:\\tmp\\tensorflow\\mnist\\logs is interpreted as a run named E with path \\tmp\\tensorflow\\mnist\\logs, which explains why it's sensitive to the current working directory.\r\n>As a quick workaround, you could avoid this problem by always specifying an explicit run name as part of the --logdir flag, e.g.:\r\n\r\n>`tensorboard --logdir=training:E:\\tmp\\tensorflow\\mnist\\logs`\r\n\r\n>(Reassigning to @dandelionmane, who shows up on the blame for the relevant code.)\r\n\r\nThis workaround worked for me.  It seems to be a valid bug and the workaround seems valid.   I hope this helps others trying to get this running in windows.  At a minimum, perhaps put a note in the [tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard) for windows users to specify the explicit run name too?  \r\n\r\nVery exciting stuff you guys are working on!  Hate to see others get slowed down if a simple fix would work.  ", "@dandelionmane @jart ", "I'll write up a change soon to work around this issue. I'm not sure if a perfect solution is possible. It might be best if we just not allow single character run names. Or maybe only impose that behavior on Windows, or when backslashes are used.", "Thanks for reporting this. I've migrated it to our new repository at https://github.com/tensorflow/tensorboard/issues/51."]}, {"number": 9853, "title": "Multiprocessing for input pipeline ", "body": "I have asked this [question](http://stackoverflow.com/questions/43889941/enqueuing-a-tf-randomshufflequeue-from-multiple-processes-using-multiprocessing) on StackOverFlow but I also feel that it can also be seen as a feature request. \r\n\r\n------------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n   : YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    : Linux version 3.10.0-229.11.1.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) \r\n- **TensorFlow installed from (source or binary)**:\r\n    :  Installed from sources\r\n- **TensorFlow version (use command below)**:\r\n    :  v1.0.0-63-g5b4bb03-dirty' 1.0.1\r\n- **Bazel version (if compiling from source)**:\r\n    :  0.4.4\r\n- **CUDA/cuDNN version**:\r\n    :  CUDA 8.0 \r\n    :  CuDNN 5.1\r\n- **GPU model and memory**:\r\n    : NVidia Titan X (Maxwell)\r\n- **Exact command to reproduce**:\r\n    : It is a feature request or clarification\r\n\r\n\r\n### Describe the problem\r\n\r\n**Basic Objective**\r\nI have to train the OverFeat architecture for patch classification from scratch.\r\n\r\n**Problem scope**\r\nInput to the graph\r\n\r\n**Problem description**\r\nDuring the training of the OverFeat model, for each image 5 random crops and their mirrored versions (a total of 10 augmented images) are created and they are fed to the model.\r\n\r\nI previously used `feed_dict` to provide input to the model. However, it led to a very very slow  training and turned out to be impractical when handling 12 million input images (1.2 million of imagenet X 10).\r\nHence, I want to use native Tensorflow queues to provide input to the graph.\r\n\r\nI have used `multiprocessing` module of Python to create sharded TFRecord files of Imagenet dataset. \r\nNow, I would like to use `multiprocessing` to enqueue augmented images to a `tf.RandomShuffleQueue` from which the graph takes it input. \r\nI could have used `multithreading` but it is still slow. Seeing the speedup in creation of TFRecord files through `multiprocessing` as opposed to `multithreading`, I am convinced that using `multiprocessing` would increase the speed of preprocessing.\r\n\r\nThe problem is that, it is not clear to me that how do I use `multiprocessing` in TensorFlow. One code snippet is [here](https://github.com/WeiTang114/tf-image-classification/blob/ff790fcb1ab6062979688647bb078c2765ad0e7a/input.py)  , but it ends up using a `feed_dict` at the end of the pipeline which leads to one extra copy operation. \r\n\r\nSo, I want to launch like 20 processes using `multiprocessing`, each of which will process a range of shards and enqueue the augmented images and corresponding labels to a `tf.RandomShuffleQueue`\r\nIt would be very helpful if there is a feature for using multiprocessing with TensorFlow for input pipeline and data augmentation. A good guideline in that direction would be very helpful.\r\n\r\n\r\n### Source code / logs\r\nThe source code which is used for creating the TFRecord files is [here](https://gitlab.inria.fr/uujjwal/overfeat-tensorflow/blob/master/buildtfrecords.py)\r\n\r\nI also wrote a basic code to try out multiprocessing (to enqueue a queue from multiple processes) but it does not work.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom pathos.multiprocessing import Pool as mp\r\n\r\ndef f(g,i):\r\n    #g = tf.Graph()                                                                                                  \r\n    with g.as_default():\r\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\r\n                         shared_name=\"shared_q\", name=\"q\")\r\n        a = tf.constant(\"Hello\")\r\n        b =\tq.enqueue(a)\r\n        c = q.size()\r\n    with tf.Session(graph=g) as sess:\r\n        for n in range(i):\r\n            sess.run(b)\r\n            print(sess.run(c))\r\n    return\r\n\r\nif __name__ == \"__main__\":\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\n    p = mp(5)\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\r\n                         shared_name=\"shared_q\", name=\"q\")\r\n        qsz = q.size()\r\n\tp.starmap(f,[(g,1),(g,2)])\r\n        with tf.Session(graph=g) as sess:\r\n            for\ti in range(10):\r\n                print(sess.run(qsz))\r\n    \tos.unsetenv(\"CUDA_VISIBLE_DEVICES\")\r\n```\r\n\r\nThe output is \r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\r\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\r\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n1\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\r\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\r\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n1\r\n2\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA diagnostic information for host: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: nefgpu11\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:363] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  367.48  Sat Sep  3 18:21:08 PDT 2016\r\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \r\n\"\"\"\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: 367.48.0\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 367.48.0\r\nW tensorflow/compiler/xla/service/platform_util.cc:61] platform CUDA present but no visible devices found\r\nI tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 20 visible devices\r\nI tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\nI tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n0\r\n```\r\n\r\nAs can be seen in the subprocesses, the queue is being enqueued, but the size as shown in the main module is continuously zero.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Closing it is your discretionary power but I think that it is equally a feature request that for such an important bottleneck as input (which is critical for complex training pipelines), a clear and well documented feature for using multiprocessing is available, which is not. The input pipeline is the least explained part of TensorFlow. And moreover, such questions do not receive good attention on StackOverFlow and the present question is just another example of that. Thanks.", "If you want better input pipelines, follow https://github.com/tensorflow/tensorflow/issues/7951.", "Do you have solved the problem for input pipeline by multiprocess? @ghost", "I have modify your code ,but it still doesn't work.\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport pathos.multiprocessing as pmp\r\n\r\ndef f(graph, queue,i):\r\n    # g = tf.Graph()\r\n    with graph.as_default():\r\n        # q = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\r\n        #                  shared_name=\"shared_q\", name=\"q\")\r\n        a = tf.constant(\"Hello\")\r\n        b = queue.enqueue(a)\r\n        q_size = queue.size()\r\n    with tf.Session(graph=graph) as sess:\r\n        for n in range(i):\r\n            sess.run(b)\r\n            print(\"sub function:\", sess.run(q_size))\r\n    return\r\n\r\nif __name__ == \"__main__\":\r\n    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n    pool = pmp.Pool(2)\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        queue = tf.FIFOQueue(capacity=100, dtypes=[tf.string], shapes=[[]],\r\n                             shared_name=\"shared_q\", name=\"q\")\r\n        pool.starmap(f, [(graph,queue, 1), (graph,queue, 2)])\r\n        qsz = queue.size()\r\n        with tf.Session(graph=graph) as sess:\r\n            for i in range(3):\r\n                print(\"main:\",sess.run(qsz))\r\n        #os.unsetenv(\"CUDA_VISIBLE_DEVICES\")\r\n```\r\n\r\n"]}, {"number": 9852, "title": "Update Graph.java", "body": "Updating just a simple spelling error. I am new to this.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@tensorflow-jenkins test this please", "@asimshankar oops, didn't realize this is a minor change until assigned you as a reviewer. Sorry. I'll merge this PR as soon as @piyush121 signs the CLA. Thanks.", "I signed the CLA.", "Can one of the admins verify this patch?"]}, {"number": 9851, "title": "Issue with tf.nn.max_pool", "body": "I am a little confused about the implementation of tf.nn.max_pool() function. Below it's a toy example.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef weight_variable(shape, name):\r\n\tinitial = tf.truncated_normal(shape, stddev = 0.1)\r\n\treturn tf.Variable(initial, name = name)\r\n\r\n\r\nxe_ = tf.placeholder(tf.float32, shape = (None, 4, 20, 1))\r\nW_conv1 = weight_variable([4, 10, 1, 10], 'W_conv1')\r\n\r\nlayer0 = tf.nn.conv2d(xe_, W_conv1, strides = [1,1,1,1], padding='VALID')\r\nlayer1 = tf.nn.relu(layer0)\r\nlayer2 = tf.nn.max_pool(layer1, ksize = (1,1,5,1), strides = (1,1,5,1), padding='SAME')\r\n\r\n\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nnp.random.seed(0)\r\nsample  = np.random.rand(1,4,20,1)\r\nmat1 = layer1.eval(feed_dict = {xe_: sample}, session = sess)\r\nmat2 = layer2.eval(feed_dict = {xe_: sample}, session = sess)\r\n\r\nnp.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]\r\n```\r\nHere I just want to perform a convolution operation with ten `4*10` filters on a single `4*20` image with only one channel. After the convolution ,there is a ReLU activation layer, followed with a max pooling layer. For the max pooling layer, the window size is just `1*5` and the stride comes with the same size. `sample` is just a random image with desired size. `mat1` and `mat2` are the output after `sample` going through these designed layers, `conv2d+ReLU` and `conv2d+ReLU+max_pool` respectively. \r\n\r\nAm I supposed to get an all-zero array from the last line of the code `np.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]`. Please point out if I understand `tf.nn.max_pool()` in a wrong way. Great thanks.\r\n\r\n\r\n### Environment info\r\n- Ubuntu  14.04.5 LTS\r\n- Python 2.7.6\r\n- cuda 8, V8.0.44\r\n- cudnn 5.1.3\r\n- TensorFlow 1.0.1\r\n- GeForce GTX 1080\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I'm sorry to ask the question in the wrong place.\r\n\r\nIf anyone else has similar questions, please check the \"SAME\" configuration of `tf.nn.max_pool`. The function pads zeros **left** and **right** rather than **right only**.\r\n\r\n"]}, {"number": 9850, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 9849, "title": "Building failure on KNL", "body": "Build with\r\nCurrent master branch source code from github\r\n\r\n> ~/tensorflow$ ./configure \r\n> Please specify the location of python. [Default is /usr/bin/python]: \r\n> Found possible Python library paths:\r\n>   /usr/local/lib/python2.7/dist-packages\r\n>   /usr/lib/python2.7/dist-packages\r\n> Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n> Using python library path: /usr/local/lib/python2.7/dist-packages\r\n> Do you wish to build TensorFlow with MKL support? [y/N] y\r\n> MKL support will be enabled for TensorFlow\r\n> Do you wish to download MKL LIB from the web? [Y/n] \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n> Do you wish to use jemalloc as the malloc implementation? [Y/n] \r\n> jemalloc enabled\r\n> Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\n> No Google Cloud Platform support will be enabled for TensorFlow\r\n> Do you wish to build TensorFlow with Hadoop File System support? [y/N] \r\n> No Hadoop File System support will be enabled for TensorFlow\r\n> Do you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\n> No XLA support will be enabled for TensorFlow\r\n> Do you wish to build TensorFlow with VERBS support? [y/N] \r\n> No VERBS support will be enabled for TensorFlow\r\n> Do you wish to build TensorFlow with OpenCL support? [y/N] \r\n> No OpenCL support will be enabled for TensorFlow\r\n> Do you wish to build TensorFlow with CUDA support? [y/N] \r\n> No CUDA support will be enabled for TensorFlow\r\n> Warning: ignoring http_proxy in environment.\r\n> INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\n> Configuration finished\r\n\r\nCommand\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\nThe error only part of build log\r\n[Build Log.txt](https://github.com/tensorflow/tensorflow/files/994830/Build.Log.txt)\r\n\r\nThe Env collected by tf_env_collect.sh\r\n[Env.txt](https://github.com/tensorflow/tensorflow/files/994893/Env.txt)\r\n\r\n\r\nBazel\r\n> $ bazel version\r\n> Warning: ignoring http_proxy in environment.\r\n> Build label: 0.4.5\r\n> Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n> Build time: Thu Mar 16 12:19:38 2017 (1489666778)\r\n> Build timestamp: 1489666778\r\n> Build timestamp as int: 1489666778\r\n\r\nXeon Phi Platform(KNL)\r\nNo GPU\r\n\r\nOne thing to notice is that with my experiments,\r\nThe changes in\r\n[Fix TensorFlow compilation errors with KNL optimization flags](https://github.com/tensorflow/tensorflow/commit/51331365b60dd042ebc0849ea4e1ab6a396b60fd)\r\nfixed the building issue. Although I don't know whether it is functioning correctly.\r\n\r\nThen the code is rolled back in\r\n[FIxed merge issues ](https://github.com/tensorflow/tensorflow/commit/bbdd4a7f6508c32042dcff10025fd39aeba72cdc)\r\n\r\nCould you please look into this.\r\nThank you.", "comments": ["@benoitsteiner Do you know what would cause these Eigen errors:\r\n\r\n    In file included from tensorflow/core/kernels/sparse_matmul_op.cc:20:0:\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h: In function 'Packet Eigen::internal::pbroadcast_third(const Packet&) [with Packet = __vector(8) double]':\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h:258:46: error: cannot convert 'const Packet8d {aka const __vector(8) double}' to '__m512 {aka __vector(16) float}' for argument '1' to '__m128 _mm512_extractf32x4_ps(__m512, int)'\r\n       Packet2d a = _mm512_extractf32x4_ps(a_in, 1);\r\n                                                  ^\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h: In function 'Packet Eigen::internal::pbroadcast_fourth(const Packet&) [with Packet = __vector(8) double]':\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h:263:61: error: cannot convert 'const Packet8d {aka const __vector(8) double}' to '__m512 {aka __vector(16) float}' for argument '1' to '__m128 _mm512_extractf32x4_ps(__m512, int)'\r\n       Packet2d a = _mm_permute_pd(_mm512_extractf32x4_ps(a_in, 1), 3);\r\n                                                                 ^\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h: In function 'Eigen::internal::Packet16f Eigen::internal::pexpand_bf16_l(const Packet16f&)':\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h:420:77: error: cannot convert 'const Packet16f {aka const __vector(16) float}' to '__m512i {aka __vector(8) long long int}' for argument '1' to '__m256i _mm512_castsi512_si256(__m512i)'\r\n       return _mm512_slli_epi32(_mm512_cvtepu16_epi32(_mm512_castsi512_si256(from)),\r\n                                                                                 ^\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h: In function 'Eigen::internal::Packet16f Eigen::internal::pexpand_bf16_u(const Packet16f&)':\r\n    ./tensorflow/core/kernels/sparse_matmul_op.h:427:59: error: cannot convert 'const Packet16f {aka const __vector(16) float}' to '__m512d {aka __vector(8) double}' for argument '1' to '__m256d _mm512_extractf64x4_pd(__m512d, int)'\r\n           _mm512_cvtepu16_epi32(_mm512_extractf64x4_pd(from, 1)), 16);\r\n\r\n", "Hi,\r\nI noticed that this issue has been solved in\r\n[https://github.com/tensorflow/tensorflow/commit/23caaa5e42c87a189511438dcadc428b683cd028](https://github.com/tensorflow/tensorflow/commit/23caaa5e42c87a189511438dcadc428b683cd028)\r\nBut then I got this error\r\n[Build Log.txt](https://github.com/tensorflow/tensorflow/files/1020659/Build.Log.txt)\r\nShould I post another issue or just leave it here.\r\nI found a similar but not the same issue\r\n[https://github.com/tensorflow/tensorflow/issues/9697](https://github.com/tensorflow/tensorflow/issues/9697)\r\nThanks", "Hi,\r\nI am having exactly the same problem, did you find a solution yet?\r\nRegards\r\n", "If you are talking about the pmul issue,\r\nNothing changed yet", "@cYclonEK Did you get it running on the KNL? Also does your KNL have a GPU? Just asking because I'm trying to run the inception v3 model on my xeon phi KNL without GPU and it does not give a better outcome ~4images/sec.\r\n", "@cryptox31 Sorry to tell you that I still cannot successfully build TF on KNL with native optimization.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 9848, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 9847, "title": "TESTING, DO NOT MERGE.", "body": "", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 9846, "title": "Increase compatibility with pathlib", "body": "Pathlib (https://docs.python.org/3/library/pathlib.html) is a nice standard-library for dealing with paths, but it seems that TensorFlow currently does not always accept them. E.g. when creating a FileWriter like:\r\n\r\n    logdir = pathlib.Path('/my/path')\r\n    log = tf.summary.FileWriter(logdir, sess.graph)\r\n\r\nI get:\r\n\r\n    TypeError: Expected binary or unicode string, got PosixPath('/my/path')\r\n\r\nWhile it's easy to work around this, it would be nice if this could be supported out of the box.\r\n\r\nEDIT: this was using TF 1.1", "comments": ["@dandelionmane do we plan to consider this?  If so we can mark as contribution welcome. ", "I don't think we're going to support third party string types.", "Could you consider this feature again, please? ", "I've only now noticed that the issue is closed. Please notice that `pathlib` is a standard library, not a `third party string type`, and is the preferred way of handling file paths in Python3.", "Yeah, `pathlib` is standard library in Python 3.\r\n\r\nSimplest solution for this to keep compatibility with Python 2 is to simply wrap all paths with `str(mypath)` to convert `pathlib` paths to strings."]}, {"number": 9845, "title": "Fix typos in high performance models docs", "body": "", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?"]}, {"number": 9844, "title": "Fix misspells on c++ codes.", "body": "Fixed some misspells :)", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "Jenkins, test this please.\r\n", "Jenkins, test this please.\r\n"]}, {"number": 9843, "title": "Branch 155393864", "body": "", "comments": ["@wolffg fyi changes have been pushed out."]}, {"number": 9842, "title": "Dynamic Library Compilation Error", "body": "Hi,\r\n\r\nWhen I compile the dynamic library without the optimization flag, using the command `bazel build //tensorflow:libtensorflow.so`, everything works well (i.e., I am able to compile it and load it in the Java API). However, if I compile with some optimization flags, using the command `bazel build --config=opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse3 --copt=-msse4.1 --copt=-msse4.2 //tensorflow:libtensorflow.so`, the library compiles fine (i.e., no errors), but when I try to load it with the Java API, I get the following error:\r\n\r\n```txt\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGILL (0x4) at pc=0x000000012b7670a8, pid=62398, tid=7171\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_77-b03) (build 1.8.0_77-b03)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.77-b03 mixed mode bsd-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  [libtensorflow.so+0x1bb80a8]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto11TableStruct16InitDefaultsImplEv+0x38\r\n#\r\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n#\r\n# An error report file with more information is saved as:\r\n# /xxx/tensorflow_scala/hs_err_pid62398.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://bugreport.java.com/bugreport/crash.jsp\r\n# The crash happened outside the Java Virtual Machine in native code.\r\n# See problematic frame for where to report the bug.\r\n```\r\n\r\nThe relevant part of the error log stack trace is:\r\n\r\n```txt\r\nStack: [0x000070000606e000,0x000070000616e000],  sp=0x0000700006166ad0,  free space=994k\r\nNative frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\r\nC  [libtensorflow.so+0x1bb80a8]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto11TableStruct16InitDefaultsImplEv+0x38\r\nC  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\r\nC  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\r\nC  [libtensorflow.so+0x1bb8124]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto12InitDefaultsEv+0x44\r\nC  [libtensorflow.so+0x1bc9614]  _ZN10tensorflow55protobuf_tensorflow_2fcore_2fframework_2ftensor_2eproto11TableStruct16InitDefaultsImplEv+0x24\r\nC  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\r\nC  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\r\nC  [libtensorflow.so+0x1bc9724]  _ZN10tensorflow55protobuf_tensorflow_2fcore_2fframework_2ftensor_2eproto12InitDefaultsEv+0x44\r\nC  [libtensorflow.so+0x1b79447]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fattr_5fvalue_2eproto11TableStruct16InitDefaultsImplEv+0x27\r\nC  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\r\nC  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\r\nC  [libtensorflow.so+0x1b795e4]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fattr_5fvalue_2eproto12InitDefaultsEv+0x44\r\nC  [libtensorflow.so+0x1ba0354]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fkernel_5fdef_2eproto11TableStruct16InitDefaultsImplEv+0x24\r\nC  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\r\nC  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\r\nC  [libtensorflow.so+0x1ba0454]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fkernel_5fdef_2eproto12InitDefaultsEv+0x44\r\nC  [libtensorflow.so+0x1ba16af]  _ZN10tensorflow9KernelDefC2Ev+0x5f\r\nC  [libtensorflow.so+0x19793c8]  _ZN10tensorflow16KernelDefBuilderC2EPKc+0x28\r\nC  [libtensorflow.so+0x10e58e]  _GLOBAL__sub_I_batchtospace_op.cc+0x1e\r\nC  0x0000000112b39a1b\r\nC  0x0000000112b39c1e\r\nC  0x0000000112b354aa\r\nC  0x0000000112b35441\r\nC  0x0000000112b34524\r\nC  0x0000000112b345b9\r\nC  0x0000000112b297cd\r\nC  0x0000000112b313ec\r\nC  [libdyld.dylib+0x2832]  dlopen+0x3b\r\nV  [libjvm.dylib+0x4820f6]\r\nV  [libjvm.dylib+0x3456a8]\r\nC  [libjava.dylib+0x28b0]  Java_java_lang_ClassLoader_00024NativeLibrary_load+0x77\r\n```\r\n\r\nWhy is this happening and how could it be fixed?\r\n\r\n@alextp I think it may have to do with resources / resource-based variables given the error message, and I think you might be knowledgable with respect to that topic. I'm not sure if that's the issue though.\r\n\r\nThank you!\r\n\r\nP.S. Let me know if the complete error log file would help and I'll post it here. :)\r\n", "comments": ["This looks like something went wrong in the protocol buffer code somewhere.\nWhich is really weird because proto code should never trigger SIGILL. Are\nyou running on a machine which supports all instruction sets you're using\n(avx, avx2, fma, msse3, msse4.1, msse4.2)? M\n\nOn Thu, May 11, 2017 at 10:41 AM, Anthony Platanios <\nnotifications@github.com> wrote:\n\n> Hi,\n>\n> When I compile the dynamic library without the optimization flag, using\n> the command bazel build //tensorflow:libtensorflow.so, everything works\n> well (i.e., I am able to compile it and load it in the Java API). However,\n> if I compile with some optimization flags, using the command bazel build\n> --config=opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse3\n> --copt=-msse4.1 --copt=-msse4.2 //tensorflow:libtensorflow.so, the\n> library compiles fine (i.e., no errors), but when I try to load it with the\n> Java API, I get the following error:\n>\n> # A fatal error has been detected by the Java Runtime Environment:\n> #\n> #  SIGILL (0x4) at pc=0x000000012b7670a8, pid=62398, tid=7171\n> #\n> # JRE version: Java(TM) SE Runtime Environment (8.0_77-b03) (build 1.8.0_77-b03)\n> # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.77-b03 mixed mode bsd-amd64 compressed oops)\n> # Problematic frame:\n> # C  [libtensorflow.so+0x1bb80a8]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto11TableStruct16InitDefaultsImplEv+0x38\n> #\n> # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n> #\n> # An error report file with more information is saved as:\n> # /xxx/tensorflow_scala/hs_err_pid62398.log\n> #\n> # If you would like to submit a bug report, please visit:\n> #   http://bugreport.java.com/bugreport/crash.jsp\n> # The crash happened outside the Java Virtual Machine in native code.\n> # See problematic frame for where to report the bug.\n>\n> The relevant part of the error log stack trace is:\n>\n> Stack: [0x000070000606e000,0x000070000616e000],  sp=0x0000700006166ad0,  free space=994k\n> Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)\n> C  [libtensorflow.so+0x1bb80a8]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto11TableStruct16InitDefaultsImplEv+0x38\n> C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\n> C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\n> C  [libtensorflow.so+0x1bb8124]  _ZN10tensorflow66protobuf_tensorflow_2fcore_2fframework_2fresource_5fhandle_2eproto12InitDefaultsEv+0x44\n> C  [libtensorflow.so+0x1bc9614]  _ZN10tensorflow55protobuf_tensorflow_2fcore_2fframework_2ftensor_2eproto11TableStruct16InitDefaultsImplEv+0x24\n> C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\n> C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\n> C  [libtensorflow.so+0x1bc9724]  _ZN10tensorflow55protobuf_tensorflow_2fcore_2fframework_2ftensor_2eproto12InitDefaultsEv+0x44\n> C  [libtensorflow.so+0x1b79447]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fattr_5fvalue_2eproto11TableStruct16InitDefaultsImplEv+0x27\n> C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\n> C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\n> C  [libtensorflow.so+0x1b795e4]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fattr_5fvalue_2eproto12InitDefaultsEv+0x44\n> C  [libtensorflow.so+0x1ba0354]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fkernel_5fdef_2eproto11TableStruct16InitDefaultsImplEv+0x24\n> C  [libtensorflow.so+0x1d50941]  _ZN6google8protobuf8internal16FunctionClosure03RunEv+0x11\n> C  [libtensorflow.so+0x1d512eb]  _ZN6google8protobuf18GoogleOnceInitImplEPlPNS0_7ClosureE+0x3b\n> C  [libtensorflow.so+0x1ba0454]  _ZN10tensorflow61protobuf_tensorflow_2fcore_2fframework_2fkernel_5fdef_2eproto12InitDefaultsEv+0x44\n> C  [libtensorflow.so+0x1ba16af]  _ZN10tensorflow9KernelDefC2Ev+0x5f\n> C  [libtensorflow.so+0x19793c8]  _ZN10tensorflow16KernelDefBuilderC2EPKc+0x28\n> C  [libtensorflow.so+0x10e58e]  _GLOBAL__sub_I_batchtospace_op.cc+0x1e\n> C  0x0000000112b39a1b\n> C  0x0000000112b39c1e\n> C  0x0000000112b354aa\n> C  0x0000000112b35441\n> C  0x0000000112b34524\n> C  0x0000000112b345b9\n> C  0x0000000112b297cd\n> C  0x0000000112b313ec\n> C  [libdyld.dylib+0x2832]  dlopen+0x3b\n> V  [libjvm.dylib+0x4820f6]\n> V  [libjvm.dylib+0x3456a8]\n> C  [libjava.dylib+0x28b0]  Java_java_lang_ClassLoader_00024NativeLibrary_load+0x77\n>\n> Why is this happening and how could it be fixed?\n>\n> @alextp <https://github.com/alextp> I think it may have to do with\n> resources / resource-based variables given the error message, and I think\n> you might be knowledgable with respect to that topic. I'm not sure if\n> that's the issue though.\n>\n> Thank you!\n>\n> P.S. Let me know if the complete error log file would help and I'll post\n> it here. :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9842>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcGphbOXmBVsHadstEsYLyzbn8P7ks5r40gzgaJpZM4NYU4x>\n> .\n>\n\n\n\n-- \n - Alex\n", "I think I was but I will check this on another machine too. Shouldn't the compilation throw an error if any one of these instruction sets us not supported though? I remember that being the case with version 1.0, for example.", "If you explicitly pass flags to gcc you can tell it to compile to a machine\nother than the one you have. To use your local configuration you might need\n-march=native\n\nOn Thu, May 11, 2017 at 10:53 AM, Anthony Platanios <\nnotifications@github.com> wrote:\n\n> I think I was but I will check this on another machine too. Shouldn't the\n> compilation throw an error if any one of these instruction sets us not\n> supported though? I remember that being the case with version 1.0, for\n> example.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9842#issuecomment-300866878>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQEcc-DLv2jT-YcEKzO7mYzqn1Ycks5r40sqgaJpZM4NYU4x>\n> .\n>\n\n\n\n-- \n - Alex\n", "I see. Does `-march=native` automatically use all the instruction sets available in the architecture of the current machine? For example, if the CPU supports the AVX instruction set, will `-march=native` make the compiler use that?", "Yes, I believe.\n\nOn Thu, May 11, 2017 at 11:07 AM, Anthony Platanios <\nnotifications@github.com> wrote:\n\n> I see. Does -march=native automatically use all the instruction sets\n> available in the architecture of the current machine? For example, if the\n> CPU supports the AVX instruction set, will -march=native make the\n> compiler use that?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9842#issuecomment-300870921>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWwNUiKJHHh24IDCFnmLqCHIRyJiks5r4052gaJpZM4NYU4x>\n> .\n>\n\n\n\n-- \n - Alex\n", "Great! Thanks, that was quite informative because I haven't been able to find any documentation on these configuration flags. I'm currently recompiling with `-march=native`. I'll let you know if it resolves the issue once done.", "@alextp You were right! Compiling with `-march=native` resolves the issue. Thanks a lot! :)"]}, {"number": 9841, "title": "Unable to use two gmms in one classifier in tf gpu 1.1", "body": "I've tried to use tf.contrib.factorization.python.ops' gmm model\r\n\r\nI've watched the gmm_test.py as the doc about this model is not clear to use\r\n\r\nMy code is as below\r\n\r\n```\r\nclass DTW_GMM:\r\n    def __init__(self, cluster_num=3):\r\n        self.genuine_graph = tf.Graph()\r\n        self.forgery_graph = tf.Graph()\r\n        with self.genuine_graph.as_default():\r\n            self.genuine_gmm = gmm.GMM(cluster_num)\r\n        with self.forgery_graph.as_default():\r\n            self.forgery_gmm = gmm.GMM(cluster_num)\r\n\r\n    def compare(self, reference, target):\r\n        channel_dtw = []\r\n        for channel_index in range(len(reference)):\r\n            dis, _, _, _ = dtw(reference[channel_index], target[channel_index], dist=my_custom_norm)\r\n            channel_dtw.append(dis)\r\n        return channel_dtw\r\n\r\n    def input_fn(self, data):\r\n        def fn():\r\n            return data, None\r\n        return fn\r\n\r\n    def train_genuine(self, data, steps=10):\r\n        with self.genuine_graph.as_default():\r\n            self.genuine_gmm.fit(input_fn=self.input_fn(data), steps=steps)\r\n\r\n    def train_forgery(self, data, steps=10):\r\n        with self.forgery_graph.as_default():\r\n            self.forgery_gmm.fit(input_fn=self.input_fn(data), steps=steps)\r\n\r\n    def infer(self, data):\r\n        genuine_score = self.genuine_gmm.score(input_fn=self.input_fn(data), steps=1)\r\n        forgery_score = self.forgery_gmm.score(input_fn=self.input_fn(data), steps=1)\r\n        return genuine_score >= forgery_score\r\n\r\nbatch_size = 64\r\nloop = 100\r\n\r\n\r\ndef train():\r\n    model = DTW_GMM()\r\n    data = Data()\r\n\r\n    sess = tf.Session()\r\n    with sess.as_default():\r\n        sess.run(tf.global_variables_initializer())\r\n        for step in range(loop):\r\n            print('step: {}'.format(step))\r\n            genuine_data = []\r\n            forgery_data = []\r\n            for i in range(batch_size):\r\n                reference, target = data.get_genuine_pair()\r\n                genuine_data.append(model.compare(reference, target))\r\n                reference, target = data.get_fake_pair()\r\n                forgery_data.append(model.compare(reference, target))\r\n            genuine_data = tf.constant(genuine_data, dtype=np.float32)\r\n            forgery_data = tf.constant(forgery_data, dtype=np.float32)\r\n            model.train_genuine(genuine_data)\r\n            model.train_forgery(forgery_data)\r\n```\r\n\r\nEven I try to define graph for each of the two gmm, it still connot work with the error log:\r\n\r\n```\r\n  File \"DTW_GMM.py\", line 39, in train_genuine\r\n    self.genuine_gmm.fit(input_fn=self.input_fn(data), steps=steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 281, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 430, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 927, in _train_model\r\n    model_fn_ops = self._get_train_ops(features, labels)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1132, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1103, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm.py\", line 137, in _model_fn\r\n    self._params)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 498, in gmm\r\n    covariance_type, random_seed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 147, in __init__\r\n    self._create_variables(data, initial_means)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 173, in _create_variables\r\n    _init_clusters_random(data, self._num_classes, self._random_seed),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/factorization/python/ops/gmm_ops.py\", line 81, in _init_clusters_random\r\n    [check_ops.assert_less_equal(num_clusters, num_data)]):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3595, in control_dependencies\r\n    return get_default_graph().control_dependencies(control_inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3324, in control_dependencies\r\n    c = self.as_graph_element(c)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2414, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2498, in _as_graph_element_locked\r\n    raise ValueError(\"Operation %s is not an element of this graph.\" % obj)\r\nValueError: Operation name: \"assert_less_equal/Assert/Assert\"\r\nop: \"Assert\"\r\ninput: \"assert_less_equal/All\"\r\ninput: \"assert_less_equal/Assert/Assert/data_0\"\r\ninput: \"assert_less_equal/Assert/Assert/data_1\"\r\ninput: \"assert_less_equal/Assert/Assert/data_2\"\r\ninput: \"assert_less_equal/x\"\r\ninput: \"assert_less_equal/Assert/Assert/data_4\"\r\ninput: \"assert_less_equal/Assert/Assert/data_5\"\r\ninput: \"strided_slice_1\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    list {\r\n      type: DT_STRING\r\n      type: DT_STRING\r\n      type: DT_STRING\r\n      type: DT_INT32\r\n      type: DT_STRING\r\n      type: DT_STRING\r\n      type: DT_INT32\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"summarize\"\r\n  value {\r\n    i: 3\r\n  }\r\n}\r\n is not an element of this graph.\r\n```\r\n\r\nThe input is a constant tensor with shape of [batch_size, 4]", "comments": ["Error exists even I use only one gmm without graph. So how to use gmm model as gmm_test can work. I've take some hours read  the unit_test platform but it's too complex", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@agarwal-ashish Could you please help me? I've post a question in stack overflow [here](http://stackoverflow.com/questions/43929589/use-official-gmm-module-as-gmm-test-but-a-bug-reported-operation-name-assert)"]}, {"number": 9840, "title": "Add a note about how weights are initialized in Dense/dense to the docstrings", "body": "Hi,\r\nthis PR adds a note about the default weight initializer to the `tf.layers.Dense` and `tf.layers.dense` docstrings to clarify how the default weights are initialized as discussed in #9744 .", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "Thanks for this patch @rasbt, really helpful. I was looking for a response to this question for hours...", "@rasbt Could you add it here?\r\nhttps://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/state_ops.py#L52\r\n\r\nNot sure if it should be called xavier or glorot.", "I think it should be `glorot_uniform_initializer` but maybe @fchollet has a better suggestion. (I think xavier is more common in literature/on the internet but using the last name of the author is maybe more appropriate and keras uses it already as well)", "> I think it should be glorot_uniform_initializer\r\n\r\nYes\r\n\r\n> @rasbt Could you add it here?\r\n\r\nNo need, we have done this internally. The CL is pending submitting (has been in limbo for a few days). ", "Ping for @rasbt !", "Changed the phrase to \r\n\r\n> [...]weights are initialized using the default initializer used by `tf.get_variable` \r\n\r\nI think this might be the best solution in the long run; i.e., if the default initializer in `tf.get_variable` gets changed for some reason in future, we don't have to remember to update the documentation here as well.", "Sounds reasonable!  Thanks!\r\n\r\n@tensorflow-jenkins test this please", "I just noticed that I accidentally inserted an unnecessary whitespace after the colon. Let me fix that. Regarding the Cmake Windows test though, are those new and is the Failure expected (for now) or is there sth I can/should do about it?", "I think the cmake windows build is sort of broken, so I'm ignoring that, all the other test harnesses are passing I think :)", "@tensorflow-jenkins test this please"]}, {"number": 9839, "title": "Apply layer normalization to LSTMCell and class CoupledInputForgetGateLSTMCell #9600", "body": "I applied the layer normalization technique to LSTMCell and CoupledInputForgetGateLSTMCell.\r\n\r\n[Reference]\r\nhttps://arxiv.org/abs/1607.06450\r\n\"Layer Normalization\"\r\nJimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton\r\n\r\nThese codes need some code reviews.\r\n\r\nCan anybody help me? :)", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "core rnn cell code cannot use tf.contrib.layers.  can you do this without\nthat dependency?\n\nOn May 11, 2017 9:29 PM, \"Chris Hoyean Song\" <notifications@github.com>\nwrote:\n\n> @tensorflow-jenkins <https://github.com/tensorflow-jenkins> test this\n> please.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9839#issuecomment-300980889>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxhI53ERFI_ixUjiBhQ9GAa34pztks5r4-A_gaJpZM4NYFrL>\n> .\n>\n", "@ebrevdo Okay, I'll remove the dependency.\r\nThank you for the code review.", "@ebrevdo \r\nI removed the dependency by copying the required functions.\r\nI think I need some guidances for improving code reusability.", "@ebrevdo can you give some feedback on the question of \"reusability\"?", "@ebrevdo could you take a look at this?", "@chris-chris could you update and push again?", "@drpngx I updated and pushed my codes. :)", "Jenkins, test this please.\n\nOn Jun 27, 2017 3:53 AM, \"Chris Hoyean Song\" <notifications@github.com>\nwrote:\n\n> @drpngx <https://github.com/drpngx> I updated and pushed my codes. :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9839#issuecomment-311323454>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbaBXBHq9dMi0P1jrEGbIZ-1-gDMHks5sIN8_gaJpZM4NYFrL>\n> .\n>\n", "Unless @ebrevdo objects, I would recommend that this go in contrib first.", "Ping for @chris-chris ", "I'm OK with having these be subclasses or copies of LSTMCell / coupled, and sit in tf.contrib.rnn for now.", "OK, I'll move the into tf.contrib.rnn :) Thanks !", "@chris-chris let us know when you think you might have time to move it into contrib?", "@tensorflow-jenkins  test this please.", "@ebrevdo @rmlarsen \r\nAs you requested, I moved my codes into contrib. \r\nI'm currerntly testing my codes.\r\nIf you have any comment, please let me know.", "@tensorflow-jenkins test this please.", "@chris-chris thanks!\r\n@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@ebrevdo can you take another look, please?", "@ebrevdo any cycles to take a look?\r\n\r\n@chris-chris this has gone stale meanwhile, please rebase and push again?", "@drpngx rebase and pushed it :)", "Jenkins, test this please.", "@ebrevdo ping", "@ebrevdo I fixed the codes according to your comments. Also, I rebased it.", "@tensorflow-jenkins test this please.", "@golanpu @ebrevdo \r\nping :)", "@tensorflow-jenkins test this please.", "Jenkins, test this please.", "Test failure is an unrelated flake."]}, {"number": 9838, "title": "Importing tensorflow fails on travis-xenial because of glibc version", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:travis-ubuntu-16.04\r\n- **TensorFlow installed from (source or binary)**:pip (to get whatever is the latest version there)\r\n- **GPU model and memory**:travis default build target\r\n\r\nYou can collect some of this information using our environment capture script\r\n\r\n### Describe the problem\r\nSince the last few days, our utility which imports tensorflow has been failing automated builds on travis-ci. We are using a ubuntu-xenial target (16.04). This issue was NOT present a few days ago, but suddenly appeared causing travis builds to fail. I am assuming it is an issue with dependencies or version checks.\r\n\r\n### Source code / logs\r\n\r\nHere is the travis build history:\r\nhttps://travis-ci.org/autonomio/core-module\r\n\r\nHere is the .travis.yml:\r\nhttps://github.com/autonomio/core-module/blob/master/.travis.yml\r\n\r\nHere is the error we are getting:\r\n`Using TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"./test_script.py\", line 1, in <module>\r\n    from autonomio.commands import data, train, test\r\n  File \"/home/travis/build/m-anish/autonomio/autonomio/__init__.py\", line 3, in <module>\r\n    from train_new import kuubio\r\n  File \"/home/travis/build/m-anish/autonomio/autonomio/train_new.py\", line 7, in <module>\r\n    from transform_data import transform_data\r\n  File \"/home/travis/build/m-anish/autonomio/autonomio/transform_data.py\", line 6, in <module>\r\n    from y_transform import y_transform\r\n  File \"/home/travis/build/m-anish/autonomio/autonomio/y_transform.py\", line 2, in <module>\r\n    from keras.utils import np_utils\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import activations\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/activations.py\", line 4, in <module>\r\n    from . import backend as K\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/backend/__init__.py\", line 73, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.17' not found (required by /home/travis/virtualenv/python2.7.13/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n`\r\n\r\nHere is the full log:\r\nhttps://s3.amazonaws.com/archive.travis-ci.org/jobs/231031263/log.txt?X-Amz-Expires=30&X-Amz-Date=20170511T053131Z&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJRYRXRSVGNKPKO5A/20170511/us-east-1/s3/aws4_request&X-Amz-SignedHeaders=host&X-Amz-Signature=de398e3a80f8114605f1a334832b1dc024321d3c46dc8453384b53620c38ec37\r\n\r\n\r\n", "comments": ["I'm closing this since apparently, the correct version of ubuntu was not being selected by travis. \r\n\r\nSorry for the noise. "]}, {"number": 9836, "title": "Unable to run model in iOS: dtype() == expected_dtype (9 vs. 4)", "body": "### System information\r\n- **iOS emulator**\r\n- **TensorFlow version v1.1.0rc2**\r\n- compiler flag: -O3, `-D__ANDROID_TYPES_SLIM__` replaced by `-D__ANDROID_TYPES_FULL__` inside `tensorflow/contrib/makefile/Makefile` in order to fix a issue of missing kernel\r\n- modification made: `tensorflow/core/kernels/cwise_op_floor_mod.cc` added to `tensorflow/contrib/makefile/tf_op_files.txt b/tensorflow/contrib/makefile/tf_op_files.txt` in order to fix another issue of missing kernel\r\n\r\n### Describe the problem\r\n\r\nI am trying to run the [deeplab image segmentation](https://github.com/DrSleep/tensorflow-deeplab-resnet) on iOS.  I have freezed the model, which can then be run on a python shell.  But when I put it on to iOS, it crashes.  Please see the log from xcode below.  I am sure my build of tensorflow is working because I can run another model.\r\n\r\n### Source code / logs\r\n\r\nThe log comes from running a quantized model.  running a non-quantized version lead to the same problem.\r\n```\r\n2017-05-11 18:57:16.053474: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-11 18:57:16.053595: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-11 18:57:16.053717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-11 18:57:16.053830: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-11 18:57:16.053889: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-11 18:57:16.054 tf_ios_makefile_example[10355:45103643] Graph created.\r\n[libprotobuf INFO google/protobuf/io/coded_stream.cc:610] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\r\n[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 45491214\r\n2017-05-11 18:57:16.094 tf_ios_makefile_example[10355:45103643] Creating session.\r\n2017-05-11 18:57:17.110386: F tensorflow/core/framework/tensor.cc:487] Check failed: dtype() == expected_dtype (9 vs. 4)\r\n```\r\n\r\nThanks in advance.\r\n", "comments": ["@andrewharp could you take a look?  From \r\n```\r\n2017-05-11 18:57:17.110386: F tensorflow/core/framework/tensor.cc:487] Check failed: dtype() == expected_dtype (9 vs. 4)\r\n```\r\nit looks like a DT_INT64 and DT_UINT8 mismatch.  Global step?", "@petewarden Any ideas?", "My only guess is that one of the inputs being fed into the model is a different type on the C++ and iOS sides. Can you get a server version of your C++ code running to test that you're calling it correctly from there?", "**types.pb.h**\r\n`enum DataType {\r\n  DT_INVALID = 0,\r\n  DT_FLOAT = 1,\r\n  DT_DOUBLE = 2,\r\n  DT_INT32 = 3,\r\n  DT_UINT8 = 4,\r\n  DT_INT16 = 5,\r\n  DT_INT8 = 6,\r\n  DT_STRING = 7,\r\n  DT_COMPLEX64 = 8,\r\n  DT_INT64 = 9,`\r\n\r\n**it looks like a DT_INT64 and DT_UINT8 mismatch.**\r\n\r\n **like this code\uff1a**\r\n`\r\ntensorflow::Tensor x(tensorflow::DT_FLOAT, tensorflow::TensorShape({ 1, 39, 2 }));\r\n auto input = x.tensor<double, 3>();\r\n`\r\n**DT_FLOAT** mismatch **double**   <==>   1 vs. 2 \r\n\r\n**the console log:** \r\n`2017-11-10 15:22:10.168723: F tensorflow/core/framework/tensor.cc:487] Check failed: dtype() == expected_dtype (1 vs. 2)`", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 112 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 127 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 142 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 157 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closed due to inactivity and this issue don't bother me anymore.", "Have the same issue with the `NotEqual` op on iOS. I manually registered it for `float`, `int32` and `int64`\r\n\r\n```\r\n+REGISTER(BinaryOp, CPU, \"NotEqual\", functor::not_equal_to, float);\r\n+REGISTER(BinaryOp, CPU, \"NotEqual\", functor::not_equal_to, int32);\r\n+REGISTER(BinaryOp, CPU, \"NotEqual\", functor::not_equal_to, int64);\r\n```\r\n\r\n and this is what I get:\r\n\r\n    F tensorflow/core/framework/tensor.cc:622] Check failed: dtype() == expected_dtype (3 vs. 9)int64 expected, got int32"]}, {"number": 9835, "title": "JobDef omit key 0 when convert to string", "body": "I installed the package from the official binary through pip, I want save a custom ClusterDef and found the zero key is omitted from the JobDef, is this an expected behavior?\r\n```\r\nkingder@WEICHAO-NEW:/mnt/d$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 14.04.5 LTS\r\nRelease:        14.04\r\nCodename:       trusty\r\n\r\nkingder@WEICHAO-NEW:/mnt/d$ python\r\nPython 2.7.6 (default, Oct 26 2016, 20:30:19)\r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> jobdef = tf.train.JobDef()\r\n>>> jobdef.name='worker'\r\n>>> jobdef.tasks[0]=\"localhost:10000\"\r\n>>> jobdef.tasks[1]=\"localhost:10001\"\r\n>>> jobdef\r\nname: \"worker\"\r\ntasks {\r\n  value: \"localhost:10000\"\r\n}\r\ntasks {\r\n  key: 1\r\n  value: \"localhost:10001\"\r\n}\r\n\r\n>>> print(tf.GIT_VERSION, tf.VERSION)\r\n('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n>>>\r\n```", "comments": ["Yes.  Proto does not distinguish zeros from missing fields."]}, {"number": 9834, "title": "how to install tensorflow 0.12 or 0.9 version on windows now?", "body": "I have tried several ways to install tf based on docker, but the installation code can only install the latest tf version.......  ", "comments": ["You can find older versions of the website at: https://www.tensorflow.org/versions/\r\nand browse from there.\r\n\r\nFor example, installation instructions for 0.12 on Windows are at:\r\nhttps://www.tensorflow.org/versions/r0.12/get_started/os_setup#pip_installation_on_windows\r\n\r\nHope that helps.", "> You can find older versions of the website at: https://www.tensorflow.org/versions/\r\n> and browse from there.\r\n> \r\n> For example, installation instructions for 0.12 on Windows are at:\r\n> https://www.tensorflow.org/versions/r0.12/get_started/os_setup#pip_installation_on_windows\r\n> \r\n> Hope that helps.\r\n\r\nI did check the link that you provided but I din,t find tensorflow version<1.0. I need to download tensorflow version==0.12.1.", "sorry I cant find the link, and cant install tensorflow 0.12 on windows, can anyone suggest an approach or an alternative\r\n", "This version is no longer supported"]}, {"number": 9833, "title": "New seq2seq interface(basic_decoder) does not support sampled softmax?", "body": "basic_decoder init function has param output_layer which must be a type of layer like Dense.\r\nBut for using sampled softmax we need some code like below, as w_t and v is needed by tf.nn.sampled_softmax_loss \r\n      with tf.variable_scope('output_projection'):\r\n          self.w_t = melt.variable.get_weights_truncated('w',   \r\n                                               [vocab_size, num_units],   \r\n                                               stddev=FLAGS.weight_stddev)   \r\n          #weights  \r\n          self.w = tf.transpose(self.w_t)    \r\n          #biases  \r\n          self.v = melt.variable.get_weights_truncated('v',     \r\n                                              [vocab_size], \r\n                                              stddev=FLAGS.weight_stddev)   \r\nfor old seq2seq interface, we can just pass output_function like \r\n      def output_fn(output):  \r\n          return tf.nn.xw_plus_b(output, self.w, self.v )  \r\n\r\nHow can we do sampled softmax with seq2seq new internface ?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9832, "title": "BeamSearchDecoder not working", "body": "I intalled TensorFlow from 2017-05-10 nightly binaries. The TensorFlow version is v1.1.0-rc2-773-g7fa0cf3 1.1.0-rc2. I have CUDA 8.0 and a Tesla K20c with 4.5G memory. My OS is Linux Ubuntu 14.04.\r\n\r\nI tried to run the following code to test the latest BeamSearchDecoder:\r\n```\r\nlstm = rnn.OutputProjectionWrapper(\r\nrnn.LayerNormBasicLSTMCell(n_hidden, dropout_keep_prob=keep_prob), n_classes)\r\ninfer_decoder = BeamSearchDecoder(lstm, \r\nembedding=lambda tokens:tf.nn.embedding_lookup(embedding_matrix, tokens),\r\nstart_tokens=start_tokens, end_token=EOS, initial_state=encoder_state, beam_width=5)\r\ndecoder_outputs_infer, decoder_state_infer, decoder_seq_infer = dynamic_decode(infer_decoder)\r\n```\r\n\r\nBut I got :\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py\", line 458, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py\", line 458, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/util/compat.py\", line 65, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got None\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/anxf/test_beam.py\", line 127, in <module>\r\n    decoder_outputs_infer, decoder_state_infer, decoder_seq_infer = dynamic_decode(infer_decoder)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2705, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2534, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2484, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 234, in body\r\n    decoder_finished) = decoder.step(time, inputs, state)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 437, in step\r\n    length_penalty_weight=length_penalty_weight)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 516, in _beam_search_step\r\n    final_shape=[static_batch_size, beam_width])\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 638, in _tensor_gather_helper\r\n    output = array_ops.reshape(output, final_shape)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2548, in reshape\r\n    name=name)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 493, in apply_op\r\n    raise err\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 490, in apply_op\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 714, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py\", line 113, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/home/anxf/.local/lib/python3.4/site-packages/tensorflow/python/framework/tensor_util.py\", line 462, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [None, 5]. Consider casting elements to a supported type.\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nI wonder how I can fix this problem? Or does this mean BeamSearchDecoder is still testing and does not work right now?", "comments": ["I encountered exactly the same problem.\r\nHelp would be much appreciated", "I encountered the same issue, and I found out this error is gone *when the batch size of the input of BeamSearchDecoder is specified*. This error is due to the line 638 of `beam_search_decoder.py`,\r\n`  output = array_ops.reshape(output, final_shape)`.\r\nSince this `final_shape` parameter is `final_shape=[static_batch_size, beam_width]` and `  static_batch_size = tensor_util.constant_value(batch_size)`, `static_batch_size` is remained as `None` if the shape of the input batch size is not specified. Therefore, reshaping tensor as `[None, beam_width]` fails.", "as @shuuki4  mentioned, one way to solve this is to reshape as [-1, beam_width] when batch size is dynamic(None).", "Thanks for reporting this. I'll try to get a fix out on Monday.\n\nOn May 12, 2017 7:25 PM, \"allen\" <notifications@github.com> wrote:\n\n> as @shuuki4 <https://github.com/shuuki4> mentioned, one way to solve this\n> is to reshape as [-1, beam_width] when batch size is dyanmic(None).\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-301219688>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8jZICuw9wHLvY-zgpH3C-CpS9jJks5r5RScgaJpZM4NXsUR>\n> .\n>\n", "Hello, was wondering if there were there any updates on this? Also running into the same issue. Thanks!", "Fix is in the queue, but I don't think we're pushing any code over the long\nweekend.  Will likely show up by end of day Tuesday.\n\nOn May 27, 2017 3:10 AM, \"classicCoder16\" <notifications@github.com> wrote:\n\n> Hello, was wondering if there were there any updates on this? Also running\n> into the same issue. Thanks!\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-304442694>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim5on9_9pW8I2fcPQ4E2NSMC9g_e1ks5r9_aHgaJpZM4NXsUR>\n> .\n>\n", "Great, thanks! A couple of quick follow-up questions:\r\n\r\n- When using BeamSearchDecoder with `dynamic_decode`, it seems that we can't pass in `impute_finished=True` without obtaining an error -- is this expected?\r\n\r\n- Also, BeamSearchDecoder appears to expect `initial_state` to have a shape whose first dimension is `batch_size*beam_size`. However, it is not entirely clear to me how you would form this initial state, say, from the last hidden state of an encoder, which would have a first dimension of `batch_size.` Would you simply tile the encoded hidden state `beam_size` number of times? Or perhaps perform some type of interleaving?\r\n\r\nSome clarification on this would be much appreciated!", "I meet the same problem with @classicCoder16. I used a bi-direction rnn as encode,  right now I use following code to get the initial state.\r\n```python\r\ninitial_state = self.dec_cell.zero_state(dtype=tf.float32, batch_size = self.batch_size * self.beam_size)\r\nfw,bw = self.encode_state\r\nfw = tf.contrib.seq2seq.tile_batch(fw,multiplier=self.beam_size)\r\nbw = tf.contrib.seq2seq.tile_batch(bw,multiplier=self.beam_size)\r\nself.initial_state = initial_state.clone(cell_state=(fw,bw))\r\n```\r\n But when I ran the dynamic decode, I can't get correct beam search result. Maybe my code is not right. Has anyone tried the BeamSearchDecoder and got the correct result? Would you please share the code with me? My tensorFlow version is 1.2.0-rc0. Thanks!", "Agreed, would love to see a piece of example code that illustrates the proper way to use the BeamSearchDecoder.", "Struggling with the same issues as @xueyouluo and @classicCoder16. I have been working on resolving this problem for some time with no success and some sample code would be incredibly helpful! Thanks in advance", "@ebrevdo Thanks for your help thus far! I've been looking over the implementation of the tests in `beam_search_decoder_test.py` and they are quite helpful. However, I have a bit of a fundamental question: when using `dynamic_decode` with the BeamSearchDecoder, it seems that the `final_outputs` that is returned has an attribute `predicted_ids` whose shape is `(batch_size, T, beam_width)`, which are the final sequences found by beam search.\r\n\r\nHow then do we go from `final_outputs.predicted_ids` to the actual most probable sequence for each example in the batch, to therefore obtain something of size `(batch_size, T)`?\r\n\r\nThanks in advance!", "classicCoder: the BeamSearchDecoder uses tf.nn.top_k to update the beams at\neach iteration (top_k by default returns sorted values).  The final\ncomputation uses GatherTree to identify the true indices at each time\nstep.  This means that the 0th beam is the one with the highest score; and\nyou should be able to use predicted_ids[:, :, 0] to access it.\n\nOn Sun, Jun 4, 2017 at 2:49 PM, classicCoder16 <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Thanks for your help thus far! I've\n> been looking over the implementation of the tests in\n> beam_search_decoder_test.py and they are quite helpful. However, I have a\n> bit of a fundamental question: when using dynamic_decode with the\n> BeamSearchDecoder, it seems that the final_outputs that is returned has\n> an attribute predicted_ids whose shape is (batch_size, T, beam_width),\n> which are the final sequences found by beam search.\n>\n> How then do we go from final_outputs.predicted_ids to the actual most\n> probable sequence for each example in the batch, to therefore obtain\n> something of size (batch_size, T)?\n>\n> Thanks in advance!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-306069454>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8viN2KW4FrFKMd1WHS9PJPfU9Gcks5sAyZagaJpZM4NXsUR>\n> .\n>\n", "@ebrevdo I'm looking to incorporate BeamSearchDecoder into a project I'm working on as soon as possible. Have all the fixes been pushed/is there anything still in the pipeline to be done that I should be aware of when using the current state of the code? Thanks!", "@lmkostas  I think you can use the code of master branch to implement BeamSearchDecoder, and it works.  ", "Here's an example including beam search. The trick is to use the tile_batch method to replicate the input to the starting state.\r\n\r\nhttps://github.com/pplantinga/tensorflow-examples/blob/master/TensorFlow%201.2%20seq2seq%20example.ipynb", "I have run into the same problem with the dynamic input size, has this issue been solved? Is there any official workaround for this problem?", "This thread has many issues. Can you open a new one with code that replicates your error?  Using the latest tf nightly?", "I have the same problem with @classicCoder16:\r\n\r\n`When using BeamSearchDecoder with dynamic_decode, it seems that we can't pass in impute_finished=True without obtaining an error -- is this expected?`\r\n\r\nOf course, with `impute_finished=False`, the error is gone and the result seems to be reasonable.\r\nI'm using tf 1.2.1 on Mac OS X.", "Also the BeamSearchDecoder does not work with the multilayer cell wrapper, as the first dimension of the cell wrapper is number of layers. But BeamSearchDecoder expect initial_state to have batch_size*beam_size as the first dim.", "Steven, please open a new issue for this.  We'll keep track of it\nseparately.\n\nOn Fri, Sep 15, 2017 at 6:55 AM, Steven Ding <notifications@github.com>\nwrote:\n\n> Also the BeamSearchDecoder does not work with the multilayer cell wrapper,\n> as the first dimension of the cell wrapper is number of layers. But\n> BeamSearchDecoder expect initial_state to have batch_size*beam_size as the\n> first dim.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-329789609>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim39OugI3d4QoQa29xV6UiubDbm-pks5sioHVgaJpZM4NXsUR>\n> .\n>\n", "thank you ebrevdo for the quick reply. it is working fine now by\r\n* tile the initial state to beam_width\r\n* create a decode (tile whatever memory it uses)\r\n* create a zero_state\r\n*  zero_state.clone(..)\r\n", "This is pretty poor documentation on part of BeamSearchDecoder(as of Tensorflow v1.3).\r\n\r\nIt is very important to use tf.contrib.seq2seq.tile_batch and not tf.tile - but one wouldn't know that unless one stumbled upon this page. I used tf.tile about 2 months back in response to a cryptic error about not having created a initial-state of size = batch_size*beam_width. And I had been struggling to get my model accuracy to a reasonable value. Turns out it was because I was using tf.tile to tile my init-state-model. In my case the init-state is itself a neural-network that is conditioned by a sample-specific context. Therefore the initial-state is different for each input-sequence - not a straight zero-state. Hence it is very important in my case to line-up the init-state with the samples.\r\n\r\nBeamSearchDecoder folks - please document this.", "Added documentation and an example to `BeamSearchDecoder`,\n`AttentionWrapper.__init__`, and `AttentionWrapper.zero_state`.  Should\nshow up in a day or two.\n\nOn Fri, Sep 29, 2017 at 1:35 PM, Sumeet Singh <notifications@github.com>\nwrote:\n\n> This is pretty poor documentation on part of BeamSearchDecoder. Is it\n> really that hard to add one line of documentation (this is as of Tensorflow\n> v1.3)?\n> It is very important to use tf.contrib.seq2seq.tile_batch and not tf.tile\n> - but one wouldn't know that unless one stumbled upon this page. I used\n> tf.tile about 2 months back in response to a cryptic error about not having\n> created a zero state of size = batch_size*beam_width. And I had been\n> struggling to get my model accuracy to become reasonable. Turns out it was\n> because I was using tf.tile to tile my init-state-model. In my case the\n> init-state is itself a neural-network that is conditioned by a\n> sample-specific context. Therefore the initial-state is different for each\n> input-sequence - not a straight zero-state. Hence it is very important in\n> my case to line-up the init-state with the samples.\n>\n> BeamSearchDecoder folks - please document this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-333232331>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6AlG-lmxyb9FlL-6BFIHaLg0QE8ks5snVSCgaJpZM4NXsUR>\n> .\n>\n", "Thanks @ebrevdo ! I see the documentation in the master branch on github but the text hasn't yet shown up in the online documentation yet (for master branch). I hope it will show up soon. Just one additional point regarding the following text in the doc-string:\r\n`    - The initial state created with `zero_state` above contains a\r\n      `cell_state` value containing properly tiled final state from the\r\n      encoder.`\r\nIt is not always the case that the decoder-initial-state is equal to the encoder-final-state, hence it probably makes sense to add the following type of clarification: \r\n`Ensure that the decoder-initial-state is tiled to size batch_size*beam_width using @{tf.contrib.seq2seq.tile_batch} (NOT tf.tile).` If one were using zero-state to create the decoder-initial-state then the current doc-string is complete, however that's not true in every case.\r\nFor e.g. in my case for e.g. the encoder is a CNN - not an RNN - and the decoder-init-state is generated by a dedicated MLP (as in [this](https://arxiv.org/abs/1502.03044) paper) - not final state of an encoder.\r\n\r\nBest", "it seems that beam search not end with the symbol, because I have some the same sentence with beam search. I set the end_token, but it seems not work.\r\nI used this api:\r\n```\r\ninference_decoder = BeamSearchDecoder(\r\ncell=self.decoder_cell,\r\nembedding=embed_and_input_proj,\r\nstart_tokens=start_tokens,\r\nend_token=end_token,\r\ninitial_state=self.decoder_initial_state,\r\nbeam_width=self.beam_width,\r\noutput_layer=self.decoder_output_projection,\r\n)\r\n\r\n self.decoder_outputs_decode,_ ,_= (seq2seq.dynamic_decode\r\n(\r\n                    decoder=inference_decoder,\r\n                    output_time_major=self.time_major,\r\n                    maximum_iterations=max_decode_step,\r\n                    swap_memory=True,\r\n                    scope=decoder_scope ))\r\n```\r\nThe results are as follows:\r\nsentence1\uff1a.....eos unk unk unk unk unk\r\nsentence2\uff1a.....eos eos unk unk unk\r\nsentence3\uff1a.....eos eos unk unk unk\r\nsentence4\uff1a....eos eos unk unk unk\r\nsentence5 ....eos eos eos eos eos unk unk unk\r\nabove, sentence4 is same as sentence5. \r\n\r\nthis problem worried me, who can help me? thanks."]}, {"number": 9831, "title": "gpu_memory_fraction for tf.contrib.lean Estimators stopped working in tensorflow 1.1.0", "body": "I was updating to Tensorflow 1.1.0 to use the tf.estimator API and I found, that I couldn't get it to use less than the full GPU RAM for one process.\r\nI was trying to find out where the issue is and found, that also the tf.contrib.lean.Estimators had the same problem. \r\n\r\nHere a code snippet to reproduce the issue:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.contrib.learn import DNNClassifier\r\nimport os\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nfeature_columns = [tf.contrib.layers.real_valued_column(\"x\", dimension=mnist.train.images.shape[1])]\r\nmodel = DNNClassifier(hidden_units=[10,10],\r\n                      feature_columns=feature_columns,\r\n                      n_classes=10,\r\n                      config=tf.contrib.learn.RunConfig(gpu_memory_fraction=0.1))\r\n\r\ninput_fn_train = tf.contrib.learn.io.numpy_input_fn(\r\n                                x={\"x\":mnist.train.images.reshape([-1,28,28,1]).astype(np.float32)[:1000]},\r\n                                y=np.argmax(mnist.train.labels[:1000],1).astype(np.int32),\r\n                                batch_size=50,\r\n                                num_epochs=50,\r\n                                )\r\n\r\n\r\nmodel.fit(input_fn=input_fn_train)\r\n\r\n```\r\n\r\nWith tensorflow 1.0.1 it works as expected and only allocated 1/10 of the GPU RAM, but in tensorflow 1.1.0 it ignores this parameter. \r\n\r\nI also check with the code from the MNIST for ML beginners code  and used the GPUOptions when initializing the session:\r\n```\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```\r\n\r\nIt works as expected in both verions. \r\nSomehow this GPU options is not set correctly when using the tf.contib.learn.Estimator framework OR The gpu_memory is allocated before this call.\r\n\r\nI asked this before on the tensorflow discuss group and I was told that is is a bug.", "comments": ["This problem affects managed_session use generally, and not only through Estimator. The per_process_gpu_memory_fraction and the allow_growth settings have no effect.", "This should be fixed at the head. Could you please give it a try?", "Built from head (dbb78c7783c32f8f3dc86364dc10d7d51cf61277), this does not seem to work for me. \r\n\r\n\r\n```\r\n  session_config=tf.ConfigProto(\r\n          allow_soft_placement=True,\r\n          log_device_placement=False)\r\n  # session_config.gpu_options.allow_growth = True\r\n  session_config.gpu_options.per_process_gpu_memory_fraction = 0.40\r\n\r\n  saver = tf.train.Saver(\r\n      max_to_keep=50,\r\n      write_version=2 ,\r\n      sharded=True,)\r\n\r\n  init_op = tf.global_variables_initializer()\r\n  sv = tf.train.Supervisor(\r\n      saver=saver,\r\n      save_model_secs=model_params['checkpoint_interval'],\r\n      save_summaries_secs=model_params['report_interval'],\r\n      init_op=init_op,\r\n      logdir=model_params['checkpoint_dir'])\r\n\r\n  with sv.managed_session(config=session_config) as sess:\r\n\r\n```\r\n\r\n[1] TITAN X (Pascal) | 54'C,  22 % | 11672 / 12189 MB | asher(11117M)\r\n\r\nThe above worked previously, but please let me know if I've missed a change.\r\n", "Having the same issue here.\r\n\r\nAnd I notice that \"config_pb2.ConfigProto(allow_soft_placement=True)\" of \"monitored_session\" in lines 965-975 of estimator.py has no gpu memory config\r\nRevise the config to incorporate gpu memory fraction can manually control gpu memory usage again\r\n    ex. config.gpu_options.per_process_gpu_memory_fraction = 0.3", "Rebuilt from 7937fb34dbd9fd04dc878eefa6ccfe00d1262f2e and this behavior has changed. I'm not sure that it's quite correct - perhaps it's using the allow_growth option instead of per_process_gpu_memory_fraction?\r\n\r\n[0] TITAN X (Pascal) | 60'C,   2 % | 11685 / 12186 MB |\r\n[1] TITAN X (Pascal) | 62'C,  29 % | 12049 / 12189 MB | asher(457M)\r\n\r\nis the result of:\r\nsession_config.gpu_options.per_process_gpu_memory_fraction = 0.40\r\n\r\nWhich I would expect to reserve 40% of the available memory from one GPU (exposed via CUDA_VISIBLE_DEVICES).\r\n\r\n", "Has this issue been fixed? Seems like `gpu_memory_fraction` still has no effect in r1.2 and r1.3", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This is fixed in r1.4 using the tf.Estimator, eg.\r\n```\r\nconfig = tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))\r\ntrainingConfig = tf.estimator.RunConfig(session_config=config)\r\nestimator = tf.estimator.Estimator(config=trainingConfig, ...)\r\n```\r\nwill work as desired. I don't think it's expected to be fixed in the deprecated `tf.contrib.learn.Estimator`.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks for confirming, Urs.  "]}, {"number": 9830, "title": "There is no --logtostderror", "body": "There is not --logtostderror option. Remove them.\r\nAnd there should be no backslash in --transforms=...", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "Jenkins, test this please.\r\n"]}, {"number": 9829, "title": "Kernel Restarting The kernel appears to have died. It will restart automatically (Jupyter-Tensorflow)", "body": "I am facing a huge problem where the jupyter kernel keeps dying. This is my first experience of kernel ending on Jupyter. I'm using Tensorflow to fit a CNN model (the total input datasize is only 9.8MB)\r\n\r\nI did not have this issue in my previous run on the same code (however I did have errors where it said \"dst tensor is not initialized\". That was when I made the dataset really small to attempt to fit it. Could someone please kindly help ?\r\n\r\nThis is the code:\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nIMG_PX_SIZE = 50\r\nHM_SLICES = 20\r\n\r\nn_classes = 2\r\n\r\nx = tf.placeholder('float')\r\ny = tf.placeholder('float')\r\n\r\nkeep_rate = 0.8\r\nkeep_prob = tf.placeholder(tf.float32)\r\n\r\ndef conv3d(x, W):\r\n    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\r\n\r\ndef maxpool3d(x):\r\n    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\r\n\r\ndef convolutional_neural_network(x):\r\n    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\r\n               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\r\n               'W_fc':tf.Variable(tf.random_normal([62720  ,1024])),\r\n               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\r\n\r\n    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\r\n               'b_conv2':tf.Variable(tf.random_normal([64])),\r\n               'b_fc':tf.Variable(tf.random_normal([1024])),\r\n               'out':tf.Variable(tf.random_normal([n_classes]))}\r\n\r\n    x = tf.reshape(x, shape=[-1, IMG_PX_SIZE, IMG_PX_SIZE, HM_SLICES, 1])\r\n\r\n    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\r\n    conv1 = maxpool3d(conv1)\r\n    \r\n    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\r\n    conv2 = maxpool3d(conv2)\r\n\r\n    fc = tf.reshape(conv2,[-1, 62720  ])\r\n    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\r\n    fc = tf.nn.dropout(fc, keep_rate)\r\n\r\n    output = tf.matmul(fc, weights['out']) + biases['out']\r\n\r\n    return output\r\n\r\n def train_neural_network(x):\r\n    \r\n    much_data = np.load('muchdata_sampled-50-50-20.npy')\r\n    train_data = much_data[:100]\r\n    validation_data = much_data[-100:]\r\n    \r\n    prediction = convolutional_neural_network(x)\r\n    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\r\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\r\n    \r\n    hm_epochs = 3\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        for epoch in range(hm_epochs):\r\n            epoch_loss = 0\r\n            for data in train_data:\r\n                X = data[0]\r\n                Y = data[1]\r\n                _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\r\n                epoch_loss += c\r\n\r\n            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\r\n\r\n        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\r\n\r\n        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\r\n        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\r\n\r\n train_neural_network(x)\r\n\r\n\r\n\r\n(My System information)\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n* Yes\r\n\r\nOS Platform and Distribution:\r\n* OS = Windows 10\r\n\r\nTensorFlow installed from (source or binary):\r\n*Installed from Source\r\n\r\nTensorFlow version (use command below):\r\n*When I ran => python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n*I get only this result => b'unknown' 1.0.0\r\n\r\nCUDA/cuDNN version:\r\n*cuda_8.0.61_win10\r\n*cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0\r\n\r\nGPU model and memory:\r\n*GeForce GTX 1050 graphics card\r\n*RAM 32GB", "comments": ["Don't use Jupyter notebook. Use a simple .py script.  I know a known problem with notebook. It doesn't allow cuda to deallocate resources once the session is over which creates problems.", "Yes, I understand this point, but what perplexes me is that others with lesser CPU or GPU power is able to run this simple code without much issues. I tried this same code on Spyder instead of Jupyter and I get the error \"ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[54080,1024]\"\r\n\r\nI'm starting to suspect that I must be missing a technique to get this working properly. I've been trying to decipher this for the past 7 hours - no success so far. A friend of mine with similar machine spec can get this to run without any issues, that too on Jupyter.", "It sounds like you are running out of memory.  This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "FWIW I was seeing this after getting an OOM and then reducing the batch size. The video card seems to hold on to memory after crashes sometimes. Restarting the machine fixed the problem for me.", "A good technique to try (I put this at the beginning of any script using tensorflow or keras). This prevents that resource exhausted error assuming you're batch size is within reason.\r\n\r\nimport tensorflow as tf\r\n\r\n``config = tf.ConfigProto()``\r\n``config.gpu_options.allow_growth = True``\r\nNow when creating your session pass this config to it.\r\n``sess = tf.Session(config=config)``\r\n", "If you're running on a gpu, first thing I'd suggest doing is checking if the gpu drivers can be updated to a newer version. If so, this may solve the problem.", "This happened to me (with Keras) but I updated cuDNN (from 7.1 to 7.2), reinstalled tensorflow-gpu, and it worked! Whatever your case, a good starting point is always downloading the Jupyter Notebook to a .py script and running it. At least it will give you more the complete errors/warnings.", "just updating my packages using `conda update anaconda` fixed it for me.", "> A good technique to try (I put this at the beginning of any script using tensorflow or keras). This prevents that resource exhausted error assuming you're batch size is within reason.\r\n> \r\n> import tensorflow as tf\r\n> \r\n> `config = tf.ConfigProto()`\r\n> `config.gpu_options.allow_growth = True`\r\n> Now when creating your session pass this config to it.\r\n> `sess = tf.Session(config=config)`\r\n\r\nworked for me", "> Don't use Jupyter notebook. Use a simple .py script. I know a known problem with notebook. It doesn't allow cuda to deallocate resources once the session is over which creates problems.\r\n\r\nThank you! This was driving me crazy for the longest time.", "Upgrading numpy works for me.\r\n`pip install -U numpy`", "This worked for me ..\r\n!pip install numpy --upgrade. ", "Quite unexpected that it was numpy caused this type of error: \r\nIn jupyter notebook: \r\n    Python Has Stopped Working\r\nIn anaconda: \r\n   KernelRestarter: restarting kernel (1/5), keep random ports\r\n\r\nThis worked for me:\r\n\r\n> Upgrading numpy works for me.\r\n> `pip install -U numpy`\r\n\r\n", "> Upgrading numpy works for me.\r\n> pip install -U numpy\r\n\r\nworked for me.", "In case someone gets stuck (like me) and none of the above work, make sure you have a BLAS library. I installed libopenblas-dev, liblapack-dev through Synaptic (and all additional required changes), and got everything to work.\r\n\r\nSo I agree that the issue is -loosely speaking- numpy related :)", "uninstall tensorflow with **pip uninstall tensorflow** in consola and run again the jupyter.", "It worked for me - Follow the below steps - :) \r\nDownload Python 3.6 - \r\n   **conda install python=3.6**\r\nCreate a new conda environment-\r\n   **conda create --name newEnv**\r\nTo activate or deactive this environment  use below commands - \r\n   **activate newEnv\r\n      conda deactivate**\r\nActivate your new Environment using  activate newEnv\r\nIf you want to use GPU - use this command - \r\n  **conda install -c anaconda keras-gpu**\r\nfor CPU use belwo command -\r\n  **conda install -c anaconda keras**\r\nNow install other necessary libraries - \r\nPandas - \r\n   **conda install -c anaconda pandas**\r\nSklearn - \r\n  **conda install -c anaconda scikit-learn**\r\nI hope it helps! :) :) \r\n\r\n", "pip install --upgrade numpy worked for me\r\nthank you for your suggestions.", "I started a second Mac with exactly the same set-up, the same environment, which had nothing else then Anaconda and all its libraries and the code. Running it gave me the \"Kernel Restarting The kernel appears to have died. It will restart automatically \" msg immediately with the first \"import Numpy as np\" command. I reinstalled, updated, changed the order, renamed the Macs, rebooted 1000 times, tried for 5-6 hours all variations, it gave me always this msg ... I tried all here published ideas, no way. ...  Have anybody an idea how to interrupt this videogame? Thanks", "`conda install nomkl` worked on my computer (iMac, Tensorflow 2.0 which was installed with `conda install -c anaconda tensorflow`). However, it is really slow.", "> > Upgrading numpy works for me.\r\n> > pip install -U numpy\r\n> \r\n> worked for me.\r\n\r\nWorked for me", "I think I tried to re-install Numpy 4-5 times, it didn't help. The strange thing is that on one Powerbook it works, on the other it doesn't. My suspect is that it has to do with the graphicard, this is the only difference between the two Macs and somewhere there is coming a conflict. ", "> > > Upgrading numpy works for me.\r\n> > > pip install -U numpy\r\n> > \r\n> > \r\n> > worked for me.\r\n> \r\n> Worked for me\r\nWorked for me\r\n", "> > > Upgrading numpy works for me.\r\n> > > pip install -U numpy\r\n> > \r\n> > \r\n> > worked for me.\r\n> \r\n> Worked for me\r\n> Worked for me\r\n\r\nWorked for me!", "> pip install -U numpy\r\n\r\nThis worked for me", "This also worked for me:\r\n> pip install -U numpy", "This is what i get after updating numpy \r\n\r\n![image](https://user-images.githubusercontent.com/14892755/84131843-f4488180-aa66-11ea-9e31-2741f729432a.png)\r\n \r\nIt runs at first, then crashes", "pip install -U numpy\r\n\r\nWorked for me!", "Just create a new environment and try to launch jupyter from there. It worked for me : ) ", "conda install nomkl\r\n\r\nWorked for me!", "> A good technique to try (I put this at the beginning of any script using tensorflow or keras). This prevents that resource exhausted error assuming you're batch size is within reason.\r\n> \r\n> import tensorflow as tf\r\n> \r\n> `config = tf.ConfigProto()`\r\n> `config.gpu_options.allow_growth = True`\r\n> Now when creating your session pass this config to it.\r\n> `sess = tf.Session(config=config)`\r\n\r\nWhen I run this code, it shows this error. What I will have to do?\r\nAttributeError: module 'tensorflow' has no attribute 'ConfigProto'", "> conda install nomkl\r\n> \r\n> Worked for me!\r\n\r\nThanks, This worked for me.", "`pip install -U numpy` was not sufficient\r\n`conda update --all` worked for a conda env with `python=3.7` and tensorflow`2.0.0` ", "I tried all. Nothing worked for me.", "Some issue with Jupiter notebook, I was not facing issue when I ran the .py file from the terminal.\r\nI suggest trying a different IDE.", "```\r\ngpu = 3\r\nfrom numba import cuda\r\ncuda.select_device(gpu)\r\ncuda.close()\r\n```", "Faced the same issue on Mac with tf 1.14. Created new conda environment with tf 2.0 there. See the same error. \r\n\r\n1. In the console activated this new environment `conda activate tf`. \r\n2. Did `pip install -U numpy` as proposed above. It uninstalled `numpy-1.19.2` and installed `numpy-1.19.5`\r\n3. Manually restarted a kernel in Jupyter and re-run all cells. \r\n4. It works now.", "> Don't use Jupyter notebook. Use a simple .py script. I know a known problem with notebook. It doesn't allow cuda to deallocate resources once the session is over which creates problems.\r\n\r\ndo you say we delete problem because it doesn't have solution right now !\r\n", "I tried upgrading and downgrading NumPy but it never worked for me.\r\nWhat I later did was to downgrade my python from 3.8 to 3.6\r\nNow I'm fine.\r\n\r\nThanks everyone!", "@[istvan1000](https://github.com/istvan1000)\r\ntry pip install tensorflow-gpu from  [here](https://www.kaggle.com/product-feedback/41221)"]}, {"number": 9828, "title": "Add input function for training and testing (#9617)", "body": "* Add input function for training and testing\r\n\r\nEstimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn\r\n\r\n* remove extra comma", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "Didn't I already do this in https://github.com/tensorflow/tensorflow/pull/9658 ?  Closing but feel free to reopen if somehow we should merge this again."]}]