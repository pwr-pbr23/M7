[{"number": 50642, "title": "RaggedTensor not broadcasting correctly", "body": "Ubuntu 20.04, Tensorflow 2.5\r\n\r\nA broadcast should be able to work across a RaggedTensor, so long as the dimension has fixed size. See the following example:\r\n\r\n```python\r\nimport tensorflow as tf\r\nBATCHES = 4\r\nCHANNELS = 8\r\n\r\n# Two methods to create an \"equivalent\" RaggedTensor\r\nragged_1 = tf.expand_dims(tf.RaggedTensor.from_row_lengths(\r\n\ttf.random.normal((10*BATCHES, CHANNELS)),\r\n\t[12,8,7,13]\r\n), -2)\r\nragged_2 = tf.expand_dims(tf.RaggedTensor.from_row_lengths(\r\n\ttf.RaggedTensor.from_uniform_row_length(\r\n\t\ttf.random.normal((10*BATCHES*CHANNELS, )),\r\n\t\tCHANNELS\r\n\t),\r\n\t[12,8,7,13]\r\n), -2)\r\n\r\n# A simple multiplication kernel\r\nkernel = tf.random.normal((1, 1, 16, CHANNELS))\r\n\r\n# Try to multiply\r\nprint(\"ragged 1 shape: \", ragged_1.shape)\r\nprint(\"ragged 2 shape: \", ragged_2.shape)\r\nprint(\"kernel shape: \", kernel.shape)\r\n\r\n# ragged_1 succeeds\r\nout_1 = tf.multiply(ragged_1, kernel)\r\nprint(\"mult 1 success\")\r\n# ragged_2 fails !\r\nout_2 = tf.multiply(ragged_2, kernel)\r\nprint(\"mult 2 success\")\r\n```\r\nOutput:\r\n```\r\nragged 1 shape:  (4, None, 1, 8)\r\nragged 2 shape:  (4, None, 1, 8)\r\nkernel shape:  (1, 1, 16, 8)\r\nmult 1 success\r\nTraceback (most recent call last):\r\n   ...\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'Unable to broadcast: dimension size mismatch in dimension'\r\n2\r\nb'lengths='\r\n16\r\nb'dim_size='\r\n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n```\r\nEven though `ragged_1` and `ragged_2` have identical shapes, the multiply only succeeds for the first. Seems like an over-zealous assertion is preventing the `ragged_2` multiply?", "comments": ["@Azmisov \r\nAs per the definition of Broadcasting, [Broadcasting](https://www.tensorflow.org/guide/ragged_tensor#broadcasting) is the process of **making** tensors with different shapes have compatible shapes for elementwise operations, there is no need to specify tf.expand_dims explicitly, Tensorflow will take care of it.\r\n\r\nCould you please refer the similar [issue](https://stackoverflow.com/questions/61947237/broadcasting-with-ragged-tensor),Let us know if it helps.Thanks\r\n\r\n\r\n", "> As per the definition of Broadcasting, [Broadcasting](https://www.tensorflow.org/guide/ragged_tensor#broadcasting) is the process of **making** tensors with different shapes have compatible shapes for elementwise operations, there is no need to specify tf.expand_dims explicitly, Tensorflow will take care of it.\r\n\r\nConsider the following two broadcasts:\r\n```\r\na:       (r1) x 3 x 3\r\nb:              3 x 3\r\nresult:  (r1) x 3 x 3\r\n```\r\n```\r\na:       (r1) x 3 x 1 x 3\r\nb:                  3 x 3\r\nresult:  (r1) x 3 x 3 x 3\r\n```\r\nThe two calculations give different results. Mine is the second case, which you see would require inserting a shape 1 dimension at -2. So even though certain shapes are already compatible and tensorflow can handle it, that may not be the computation you want.\r\n\r\n> Could you please refer the similar [issue](https://stackoverflow.com/questions/61947237/broadcasting-with-ragged-tensor),Let us know if it helps.Thanks\r\n\r\nFor the issue you linked, the OP was trying to do this broadcast:\r\n```\r\na:       2 x (r1)\r\nb:              3\r\n```\r\nWhich obviously would not work since the trailing dimension did not match. However, for the test case I gave, the shapes are:\r\n```\r\na:       4 x (r1) x  1 x 8\r\nb:       1 x    1 x 16 x 8\r\nresult:  4 x (r1) x 16 x 8\r\n```\r\nDefinitely compatible there, and the multiplication works fine for `ragged_1`. However, `ragged_2`, which has the exact same shape at runtime/graph-computation, throws an error that the shapes are not compatible.\r\nCould you please take a closer look at the test case I showed, and the different behavior between `ragged_1` and `ragged_2`?", "@rmothukuru \r\n\r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/632c1ce4d196753d98bded818460c12a/-50642.ipynb).Thanks", "The code for broadcasting ragged tensors ([here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/ragged/ragged_tensor_shape.py)) was written before the `uniform_row_length` option was added for partitioning ragged tensors.  (`uniform_row_length` was mainly added to help with multiple *leading* uniform dimensions -- shapes like `[5, 3, None]` -- which otherwise wouldn't be representable.)\r\n\r\nI agree that `ragged_tensor_shape.py` should be updated to handle `uniform_row_length` values.  ", "I'm hitting a similar problem. The difference seems to be:\r\n\r\n```python\r\nprint(f\"ragged 1 shape: {ragged_1.shape}, ragged_rank: {ragged_1.ragged_rank}\")\r\nprint(f\"ragged 2 shape: {ragged_2.shape}, ragged_rank: {ragged_2.ragged_rank}\")\r\n```\r\n```\r\nragged 1 shape: (4, None, 1, 8), ragged_rank: 1\r\nragged 2 shape: (4, None, 1, 8), ragged_rank: 3\r\n```\r\n\r\ni.e. although the Tensors are identical, they have different ragged ranks."]}, {"number": 50637, "title": "model.fit with dataset generator results in deadlock/hang ", "body": "TF: 2.5.0\r\nPython: 3.7.10\r\n\r\n Model.fit hangs during training. It appears dataset generator seems go into a deadlock while reading tfrecords. A simplified naive model is in the github gist below which reproduces the issue. Directly iterating on the generator seems to work fine but feeding it to model.fit seems to deadlock. Appreciate any color. \r\n\r\n\r\nhttps://gist.github.com/talipini/d82d1fa2f5d6b46f3222f2367570543a#file-tf-model-fit-hangs-with-dataset-generator-ipynb", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50637\">No</a>\n", "Anyone have any workarounds or insights on this? ", "@talipini \r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\nThanks\r\n", "I have added github gist link which has the reproducible code to the original post but I will add the details here. \r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Colab jupyter notebok.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Default version on Colab.\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No GPU\r\n- GPU model and memory: No GPU \r\n\r\nDataset generator hangs when reading tfrecords files. This seems to happen after the first set of tfrecord files have been iterated through and starts reading the next block. This seems to happen consistently when using Colab default setup but works fine on my local machine which has significantly more CPU and memory. I am suspecting this has to do with the generator blocks using the CPU cores when there are only few cores available on colab. \r\n\r\n**Describe the expected behavior**\r\nNot hang.  \r\n\r\n**Standalone code to reproduce the issue**\r\nLink to the colab is here - https://colab.research.google.com/gist/talipini/b164e108501f831aff50827ac454d562/tf-model-fit-hangs-with-dataset-generator.ipynb\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pandas as pd\r\nimport glob\r\nimport os\r\nfrom pathlib import Path\r\nused_compression_type = 'GZIP'\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Activation, Dense\r\nfrom tensorflow.keras.optimizers import SGD\r\n\r\n#Wrapper\r\nclass TFRecordConverter(object):\r\n    def __init__(self):\r\n        pass\r\n    def int64_feature(self, value):\r\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n    def float_feature(self, value):\r\n        return tf.train.Feature(float_list=tf.train.FloatList(value=value))\r\n\r\n    def bytes_feature(self, value):\r\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n#Write Sample tfrecords\r\n\r\nbatch_size = 5\r\ntf_archive = \"/tfrecords\"\r\nos.makedirs(tf_archive, exist_ok=True)\r\nprint(f\"TF Records at : {tf_archive}\")\r\n\r\ndef create_test_tfrecords():\r\n    existing_tfrecords = glob.glob(os.path.join(tf_archive,\"*\"))\r\n    if len(existing_tfrecords) < 1:\r\n        for i in range(0,5):\r\n            x_data = np.random.uniform(low=-1.0, high=5, size= (batch_size*5, 6))\r\n            write_to_TFRecords(os.path.join(tf_archive,f\"tffile-{i}.tfr\"),x_data)\r\n\r\ndef create_TFRecordsFile(file):\r\n    Path(os.path.dirname(file)).mkdir(parents=True, exist_ok=True)    \r\n    compression = tf.io.TFRecordOptions(compression_type = used_compression_type)\r\n    writer =  tf.io.TFRecordWriter(file, options=compression)\r\n    return writer\r\n\r\ndef write_to_TFRecords(file, values):\r\n    bucket_writer = create_TFRecordsFile(file)\r\n    shape = np.array(values.shape, np.int32)\r\n    example = tf.train.Example(\r\n            features = tf.train.Features(\r\n               feature = {\r\n                    'data':TFRecordConverter().float_feature(values.ravel()), #Float list can only be 1D\r\n                    'shape':TFRecordConverter().bytes_feature(shape.tobytes())\r\n                    }\r\n               ))\r\n    \r\n    print(f\"Writing to {file} with size: {shape}\")\r\n    bucket_writer.write(example.SerializeToString())\r\n    \r\n\r\ndef parse_function(serialized_example):       \r\n        \r\n    features = {\r\n        'data': tf.io.FixedLenSequenceFeature([], tf.float32,allow_missing=True),\r\n        'shape':tf.io.FixedLenFeature([], tf.string)\r\n        }\r\n\r\n    features = tf.io.parse_single_example(serialized=serialized_example, features=features)\r\n    shape = tf.io.decode_raw(features['shape'], tf.int32 )\r\n    dataset = features['data']\r\n    dataset = tf.reshape(dataset, shape)\r\n\r\n\r\n    return dataset\r\n\r\n#Simple dataset generator\r\ndef data_gen(batch_size):\r\n    tfrecords = glob.glob(os.path.join(tf_archive,\"*\"))\r\n    print(f\"Loading tfrtecords: {tfrecords}\")\r\n    dataset = tf.data.Dataset.from_tensor_slices(tfrecords) \r\n    dataset = dataset.interleave(lambda x: tf.data.Dataset.from_generator(gen_step, \r\n                                output_types=( tf.float32, tf.float32), args=(x,)))\r\n        \r\n    dataset = dataset.repeat().batch(batch_size)\r\n    while True:\r\n        for x, Y in dataset:\r\n            yield x, Y\r\n\r\ndef gen_step(tf_file):\r\n    trRecordDataset = tf.data.TFRecordDataset(tf_file, compression_type=used_compression_type)\r\n    trRecordDataset = trRecordDataset.map(parse_function)\r\n    print(f\"Waiting for dataset in step - {tf_file}\")\r\n    for dataset in trRecordDataset: \r\n        Y = dataset[:,0]\r\n        x = dataset[:,1:]\r\n        count = 0\r\n        while count < dataset.shape[0]:\r\n            yield x[count],Y[count]\r\n            count += 1\r\n    print(f\"Finsihed dataset in - {tf_file}\")\r\n\r\nif __name__ == '__main__':\r\n  create_test_tfrecords()\r\n  #trivial model - hangs in \r\n  model = Sequential()\r\n  model.add(tf.keras.Input(shape=(batch_size,)))\r\n  model.add(Dense(1, activation='linear'))        \r\n  model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics = ['accuracy'])\r\n  \r\n  #model.fit will hang just as it starts to read the second set of tfrecords.\r\n  model.fit(x = data_gen(batch_size=batch_size),\r\n          steps_per_epoch=5,\r\n          epochs=10)\r\n  print(f\"Training Finished.\")\r\n```", "This could be related to [43768](https://github.com/tensorflow/tensorflow/issues/43768) but not sure. ", "@talipini \r\n\r\nCould you Please refer [link1](https://stackoverflow.com/questions/47705684/tensorflow-tf-data-dataset-from-generator-does-not-work-with-strings-on-pyt)  and [link2](https://stackoverflow.com/questions/51556579/using-tensorflows-datasets-api-causes-process-to-hang-in-session-destructor) , hope it helps.Thanks", "Thank you. First one is referring to an issue with strings in the generator, which I am not using. The [second](https://github.com/tensorflow/tensorflow/issues/21277) one is referring to deadlock on session destructor but that is supposed to be fixed in 2018. I went through both chains but I am kinda lost on it.  But [43768](https://github.com/tensorflow/tensorflow/issues/43768) looks interesting and could be related. ", "Any additional insights on this? thanks", "@rmothukuru \r\n\r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/2c404dad2b1abe24fab6f75ad5ab0df0/tf-model-fit-hangs-with-dataset-generator.ipynb).Thanks", "Thank you. Appreciate any color on this. ", "@rmothukuru , @aaudiber - Any updates on this? Appreciate any insights. Thanks ", "This appears to have been fixed in version 2.7\r\n\r\nAt least I can get it to train 10 epochs in colab when it runs TF 2.7"]}, {"number": 50636, "title": "TFLite request to support bulit-in split op for int64 type", "body": "**System information**\r\n- OS Platform and Distribution MacOS\r\n- TensorFlow installed from: recent nightly build, after issue https://github.com/tensorflow/tensorflow/issues/50445\r\n- TensorFlow version (or github SHA if from source): master HEAD\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n\r\n```\r\ndef test_split_export():\r\n\r\n    import tensorflow as tf\r\n\r\n    @tf.function\r\n    def test_fn(inp: tf.Tensor) -> tf.Tensor:\r\n        return tf.concat(tf.split(inp, 3, axis=1), axis=1)\r\n\r\n    concrete_f = test_fn.get_concrete_function(tf.TensorSpec((None, 3), tf.int64))\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_f])\r\n\r\n    tflite_model = converter.convert()\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n```\r\n\r\nafter fix #50445, running this test issues the following converter error: \r\n\r\n```\r\nE         tensorflow.lite.python.convert_phase.ConverterError: /Users/dmlyubim/opt/anaconda3/envs/p38/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2129:0: error: 'tf.Split' op is neither a custom op nor a flex op\r\n```\r\n\r\nWe think it is a fairly easily fixable gap in the tflite builtin op coverage. \r\n\r\nThe supporting arguments are as follows: \r\n\r\n* The main argument is that int64 type is supported in tflite2 built-in set; in fact, the concat op as in following test, works no problem:\r\n\r\n```\r\ndef test_concat_export():\r\n    import tensorflow as tf\r\n\r\n    @tf.function\r\n    def test_fn(*inp: tf.Tensor) -> tf.Tensor:\r\n        return tf.concat(inp, axis=1)\r\n\r\n    concrete_f = test_fn.get_concrete_function(\r\n        *([tf.TensorSpec((None, 3), tf.int64)] * 3)\r\n    )\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_f])\r\n\r\n    tflite_model = converter.convert()\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n```\r\n\r\nSplit is a complementary (reverse) operation of the tensor concat op, we think there's no reason to support one but not another. \r\n\r\n* It should be a rather easy fix (split is already supported for int32 etc., so adding new type should be much easier than if we had to create a whole new op).\r\n\r\n* In the age when models are transplantable, licensable, and/or expensive to train, any work-around solutions assuming model re-graphing or/and re-training to work around int64 issue are non-solutions. We have to deal with the topology and models we already have a license to evaluate. \r\n\r\n* Using TF_SELECT_OPS runitme introduces some problems with our requirements (mostly size, but also build system). This is a smaller and perhaps weaker support argument but it is still a big trouble for us mostly on requirement grounds (we are not targeting Android platform, or at least not exclusively; but we do target a C++ environment, not python). \r\n\r\n* Also, this op was identified as the only missing op precluding us from transition from TFLite1 to TFLite2. That's the only operator that really does not work. Everything else seems to be covered by built-ins for the widest range of models we investigated.\r\n\r\n* Oddly enough, our tflite conversions (and evaluations) for the same models worked in TFLite1. Probably due to another bug or mysterious context of the op use in the models we observed; a similar int64 input and split as in the example, does not work with TFLite1.15 test either for me. However, it mysteriously works for different contexts of int64 split use in the models we tried to convert with TFLite 1.15 but not TFLite2.\r\n\r\n", "comments": ["We are also welcome on the community contribution regarding this feature request. :-)", "Hey @dlyubimov @abattery  I am quite beginner and super interested to begin my contribution towards this repo and this issue seems interesting to me. Can you please let me know that where exactly do I have to make changes or add functionalities?\r\nAny examples corresponding to this would be highly appreciable. Thanks!", "we'll consider a contribution, time allowing"]}, {"number": 50628, "title": "Issue with Conv1D when groups > 1 and using our own TensorFlow builds", "body": "**System information**\r\n- OS Platform and Distribution: Linux Red Hat Enterprise 8.1\r\n- TensorFlow installed from: source\r\n- TensorFlow version: v2.5.0-0-ga4dfb8d1a71 2.5.0 (more generally any version starting from 2.3.1)\r\n- Python version: 3.7.10\r\n- Bazel version: 3.7.2\r\n- GCC/Compiler version: 8.3.1\r\n- CUDA/cuDNN version: 11.2 / 8.0\r\n- GPU model and memory: NVIDIA V100\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using our own TensorFlow builds (starting at version 2.3.1 which added the  `groups` parameter), the following code snippet fails for `groups` greater than 1:\r\n```python\r\nimport tensorflow as tf\r\nimport traceback\r\n\r\nfor g in (1,2,4):\r\n    try:\r\n        print(f\"groups={g}\")\r\n        c = tf.keras.layers.Conv1D(4,4,groups=g)\r\n        print(c(tf.ones((2,16,4))))\r\n    except Exception as e:\r\n        traceback.print_exc()\r\n    finally:\r\n        print()\r\n```\r\n\r\nOutput is as follow:\r\n```\r\ngroups=1\r\ntf.Tensor(\r\n[[[ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]]\r\n\r\n [[ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]\r\n  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]]], shape=(2, 13, 4), dtype=float32)\r\n\r\ngroups=2\r\nTraceback (most recent call last):\r\n  File \"groups.py\", line 8, in <module>\r\n    print(c(tf.ones((2,16,4))))\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 249, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1019, in convolution_v2\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1149, in convolution_internal\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1892, in conv1d\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 932, in conv2d\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: input and filter must have the same depth: 4 vs 2 [Op:Conv2D]\r\n\r\ngroups=4\r\nTraceback (most recent call last):\r\n  File \"groups.py\", line 8, in <module>\r\n    print(c(tf.ones((2,16,4))))\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 249, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1019, in convolution_v2\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1149, in convolution_internal\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 602, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1892, in conv1d\r\n    name=name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 932, in conv2d\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: input and filter must have the same depth: 4 vs 1 [Op:Conv2D]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIf I run the same code snippet using a TensorFlow build from `pip`, I do not get any errors but I have a hard time understanding how this can build dependent:\r\n```\r\ngroups=1\r\ntf.Tensor(\r\n[[[-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]]\r\n\r\n [[-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]\r\n  [-0.05965281 -1.473797   -0.5487337  -0.34858704]]], shape=(2, 13, 4), dtype=float32)\r\n\r\ngroups=2\r\ntf.Tensor(\r\n[[[ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]]\r\n\r\n [[ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]\r\n  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]]], shape=(2, 13, 4), dtype=float32)\r\n\r\ngroups=4\r\ntf.Tensor(\r\n[[[-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]]\r\n\r\n [[-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]\r\n  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]]], shape=(2, 13, 4), dtype=float32)\r\n```\r\n\r\nAny ideas would be greatly appreciated!", "comments": ["@RemiLacroix-IDRIS \r\n\r\nI was able to reproduce the code shared in tf2.5 with no errors.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/1b99df45ccd916c456ecf8d92586a08d/untitled132.ipynb) here.\r\n\r\nsimilar issue tracking [here](https://github.com/tensorflow/tensorflow/issues/50397) follow the thread aswell and also refer this [comment](https://stackoverflow.com/questions/66415623/group-convolution-in-keras).Thanks!", "@UsharaniPagadala : As mentioned, that code works fine with the TensorFlow builds from pip but crashes with the builds we compile from source but that doesn't seem to make any sense.\r\n\r\nI don't see how compilation can have in impact on this.", "I had a closer look and the call path seems to be the same for both our compiled build and the build from pip.\r\n\r\nThe last call I could track is in function `conv2d` in `tensorflow/python/ops/gen_nn_ops.py`:\r\n```python\r\n  _ctx = _context._context or _context.context()\r\n  tld = _ctx._thread_local_data\r\n  if tld.is_eager:\r\n    try:\r\n      _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\n        _ctx, \"Conv2D\", name, input, filter, \"strides\", strides,\r\n        \"use_cudnn_on_gpu\", use_cudnn_on_gpu, \"padding\", padding,\r\n        \"explicit_paddings\", explicit_paddings, \"data_format\", data_format,\r\n        \"dilations\", dilations)\r\n      return _result\r\n    except _core._NotOkStatusException as e:\r\n      _ops.raise_from_not_ok_status(e, name)\r\n    except _core._FallbackException:\r\n      pass\r\n```\r\n\r\nThe call to `pywrap_tfe.TFE_Py_FastPathExecute` returns successfully in the PIP build but fails with ours. Not sure how to get more information on what happens in that call.\r\n\r\nNote that the issue occurs both in a CPU only-context and a GPU context.", "Not sure if it can help troubleshooting the issue but it seems that **non-eager mode** works as expected on both our builds and the PIP builds **when using a GPU device**:\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass test(object):\r\n   def __init__(self):\r\n     self.cg1=tf.keras.layers.Conv1D(4,4,groups=1)\r\n     self.cg2=tf.keras.layers.Conv1D(4,4,groups=2)\r\n     self.cg4=tf.keras.layers.Conv1D(4,4,groups=4)\r\n\r\n   @tf.function\r\n   def call(self, inputs):\r\n      tf.print(\"cg1\", self.cg1(inputs))\r\n      tf.print(\"cg2\", self.cg2(inputs))\r\n      tf.print(\"cg4\", self.cg4(inputs))\r\n\r\ntt=test()\r\ntt.call(tf.ones((2,16,4)))\r\n```\r\nbut **it still crashes when using a CPU device** and **the error is different depending on the build used**:\r\n* our own build:\r\n```\r\nTraceback (most recent call last):\r\n  File \"groups_non_eager.py\", line 19, in <module>\r\n    tt.call(tf.ones((2,16,4)))\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 950, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3024, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1961, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 596, in call\r\n    ctx=ctx)\r\n  File \"/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  input and filter must have the same depth: 4 vs 2\r\n         [[node conv1d_3/conv1d (defined at groups_non_eager.py:16) ]] [Op:__inference_call_162]\r\n\r\nFunction call stack:\r\ncall\r\n```\r\n* PIP build:\r\n```\r\nTraceback (most recent call last):\r\n  File \"groups_non_eager.py\", line 19, in <module>\r\n    tt.call(tf.ones((2,16,4)))\r\n  File \"~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 950, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3024, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1961, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 596, in call\r\n    ctx=ctx)\r\n  File \"~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  Fused conv implementation does not support grouped convolutions for now.\r\n         [[node conv1d_3/conv1d (defined at groups_non_eager.py:16) ]] [Op:__inference_call_162]\r\n\r\nFunction call stack:\r\ncall\r\n```", "Hello,\r\n\r\nI just realized that our builds have XLA enabled (`XLA_FLAGS=--xla_hlo_profile`), I guess it could be relevant for this issue.", "@angerson : Hi. Would it be possible to have some feedback on this issue?", "Hi. Any news regarding this issue?", "Hi @RemiLacroix-IDRIS ! I was not getting any error in [2.8](https://colab.sandbox.google.com/drive/18HX1KyC1AEcZ2nosRaz-518p2B35FTuY?resourcekey=0-aXCTPqWy-yo9LmvlNArjNQ#scrollTo=k2LTRWLzO0uC) though. Can you please let us know from your end?\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Hi @RemiLacroix-IDRIS ! I was not getting any error in [2.8](https://colab.sandbox.google.com/drive/18HX1KyC1AEcZ2nosRaz-518p2B35FTuY?resourcekey=0-aXCTPqWy-yo9LmvlNArjNQ#scrollTo=k2LTRWLzO0uC) though. Can you please let us know from your end?\r\n\r\nWe currently don't have our own build of 2.8 since I couldn't try yet. It matters because the version from pip always was unaffected, my guess would be because it does not have `XLA_FLAGS=--xla_hlo_profile`.\r\n"]}, {"number": 50627, "title": "Cannot link libtensorflowlite.dylib for iOS", "body": "**System information**\r\n- OS Platform and Distribution : OS X 11.3\r\n- TensorFlow version: 2.5\r\n- Python version: python3.9\r\n- Bazel version (if compiling from source): bazel 3.7.2\r\n- GCC/Compiler version (if compiling from source): Apple clang version 12.0.5 (clang-1205.0.22.11), Xcode 12.5.1\r\n\r\nI tried to build C++ API libtensorflowlite.dylib for iOS. I use build command:\r\n\r\n`\r\nbazel build -c opt --config=ios_arm64 //tensorflow/lite:libtensorflowlite.dylib --verbose_failures\r\n`\r\nError:\r\n\r\n```\r\nld: warning: option -s is obsolete and being ignored\r\nld: unknown option: -z\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nSteps:\r\n1. Run ./Configure\r\n2. Run bazel build -c opt --config=ios_arm64 //tensorflow/lite:libtensorflowlite.dylib --verbose_failures\r\n3. Error.\r\n\r\n**Any other info / logs**\r\n```\r\nERROR: /Users/o.sh/work/tensorflow/tensorflow/lite/BUILD:909:24: Linking of rule '//tensorflow/lite:libtensorflowlite.dylib' failed (Exit 1): cc_wrapper.sh failed: error executing command \r\n  (cd /private/var/tmp/_bazel_o.sh/916305f91b5f074316230c12ec2f3c72/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=iPhoneOS \\\r\n    APPLE_SDK_VERSION_OVERRIDE=14.5 \\\r\n    PATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin \\\r\n    PYTHON_BIN_PATH=/usr/local/opt/python@3.9/bin/python3.9 \\\r\n    PYTHON_LIB_PATH=/usr/local/Cellar/python@3.9/3.9.4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    XCODE_VERSION_OVERRIDE=12.5.1.12E507 \\\r\n  external/local_config_cc/cc_wrapper.sh -lc++ -fobjc-link-runtime -framework UIKit -shared -o bazel-out/ios_arm64-opt/bin/tensorflow/lite/libtensorflowlite.dylib -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libbuiltin_op_kernels.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/liblstm_eval.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/flatbuffers/src/libflatbuffers.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libaudio_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libkernel_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libeigen_support.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libkernel_util.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libquantization_util.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/tools/optimize/sparsity/libformat_converter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libtensor_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libportable_tensor_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libtranspose_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libcpu_backend_gemm.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libcpu_check.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libcontext_get_ctx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libfrontend.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_arm.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_avx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_avx2_fma.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_avx512.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libapply_multiplier.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_arm.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_avx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_avx2_fma.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_avx512.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libprepare_packed_matrices.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libtrmul.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libblock_map.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libcpu_backend_context.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libcontext.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libctx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/liballocator.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libhave_built_path_for_avx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libhave_built_path_for_avx2_fma.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libhave_built_path_for_avx512.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libprepacked_cache.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libsystem_aligned_alloc.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libtune.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libcpuinfo.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/cpuinfo/libcpuinfo_impl.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/clog/libclog.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libthread_pool.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libblocking_counter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libwait.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/fft2d/libfft2d.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/profiler/libinstrumentation.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/farmhash_archive/libfarmhash.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/liboptional_debug_tools.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libcc_api.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/liballocation.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libarena_planner.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libexternal_cpu_backend_context.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libmutable_op_resolver.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libsimple_memory_arena.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/core/util/libversion_info.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/delegates/libtelemetry.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/profiling/libplatform_profiler.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/profiling/libsignpost_profiler.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/core/api/libapi.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/core/api/libop_resolver.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/core/api/liberror_reporter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/c/libcommon.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/schema/libschema_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/micro/libdebug_log.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libdenormal.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libstderr_reporter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libminimal_logging.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libutil.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/experimental/resource/libresource.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libstring_util.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libtflite_with_xnnpack_optional.a -headerpad_max_install_names -Wl,-z,defs -Wl,--version-script,tensorflow/lite/tflite_version_script.lds -s '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-install_name,@rpath/libtensorflowlite.dylib -lm -pthread -pthread -pthread -lpthread -lm -pthread -ldl -no-canonical-prefixes -target arm64-apple-ios '-miphoneos-version-min=14.5')\r\nExecution platform: @local_execution_config_platform//:platform\r\nld: warning: option -s is obsolete and being ignored\r\nld: unknown option: -z\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/lite:libtensorflowlite.dylib failed to build\r\n\r\n```", "comments": ["I am able to build if I remove all from this line from file tensorflow/tensorflow/lite/BUILD\r\n`\r\n\"//conditions:default\": [],\r\n`\r\n\r\nBut I am not sure, is it correct to comment?", "> I am able to build if I remove all from this line from file tensorflow/tensorflow/lite/BUILD\r\n> `\"//conditions:default\": [],`\r\n> \r\n> But I am not sure, is it correct to comment?\r\n\r\nI added the linkopts argument using\r\n```\r\n\"//tensorflow:ios\": [\r\n   \"-Wl,-exported_symbols_list,$(location //tensorflow/lite:tflite_exported_symbols.lds)\",\r\n],\r\n```\r\nas that seemed similar to what it done in the `//tensorflow/lite/c/tensorflowlite_c` target\r\n\r\nI was building with\r\n```\r\nbazel build  --verbose_failures --config=ios_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl  //tensorflow/lite:tensorflowlite\r\n```\r\ndefined [here](https://github.com/tensorflow/tensorflow/blob/8e0694e858a301238867e77ad0d9fb1ff75284b8/tensorflow/lite/BUILD#L1032). \r\n\r\nWould be nice if this target was officially supported with a fat binary too!"]}, {"number": 50622, "title": "tf.shape can't infer sparse tensor shape in graph mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n\r\n**Describe the current behavior**\r\n\r\ntf.shape can't infer shape of a sparse tensor in graph mode.\r\n\r\n**Describe the expected behavior**\r\n\r\ntf.shape should infer shape of a sparse tensor in graph mode.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\n\r\ninputs = tf.keras.Input((2,3,4));\r\nprint(tf.shape(inputs)); # tf.shape can infer dense tensor\r\n\r\ninputs = tf.keras.Input((2,3,4), sparse = True);\r\nprint(tf.shape(inputs)); # tf.shape cannot infer sparse tensor\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c3d7a40bd1c3d5ebefd86ed560a1c8c9/untitled107.ipynb),Thanks!", "@breadbread1984,\r\nDocumentation of [tf.shape](https://www.tensorflow.org/api_docs/python/tf/shape#for_example) states \r\n\r\n> **`tf.shape`** and **`Tensor.shape`** should be identical in eager mode.\r\n\r\nUsing **`inputs.shape`** instead of **`tf.shape(inputs))`** resolves your problem. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/72adf7e88659f4178f69dcf42985a9c1/untitled107.ipynb) of the working code. Thanks!\r\n", "in graph mode the batch size is inferable, but it is not when using tensor.shape. that is why i am using tf.shape to infer shape of tensor when building graph model with functional model definition."]}, {"number": 50619, "title": "tf.repeat for RaggedTensor", "body": "Wondering if we can get `tf.repeat` for the `tf.ragged` module?\r\n\r\nThe `tf.stack` and `tf.tile` ops are implemented, so doesn't seem like there would be a technical limitation for `tf.repeat`. My particular case is doing the repeat on one of the non-ragged dims. But seems you could do a repeat on a ragged tensor, so long as the `repeats` arg is scalar or matches the maximum row length. I did see a related issue discussing the possibility of an nd_repeat operation, which would perhaps prove more useful for repeats on ragged dims.", "comments": []}, {"number": 50616, "title": "Multi-GPU issue: second A40 GPU returns zeros ", "body": "\r\n**System information**\r\n- Have I written custom code: yes (code below)\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71\r\n- Python version: Python 3.6.9\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: A40, 45634MiB (2X)\r\n\r\n**Describe the current behavior**\r\nI'm having difficulties running a training script on a server with TF 2.5 and two A40s. Basically, any data on the second gpu defaults to `0.0`.\r\n\r\nThe included example script pinpoints the problem. In few words, the same variable computed on `/GPU:0` and `/GPU:1` must return the same value when queried. Instead, the variable on `/GPU:1` is always read as `0.0`\r\n\r\nI have tried my original training script and the toy example below on another server running TF 1.15 with 8 RTX5000s, and everything works as expected.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow.compat.v1 as tf\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\r\n\r\nif __name__ == \"__main__\":\r\n\t\r\n\tconfig = tf.ConfigProto()\r\n\tconfig.gpu_options.allow_growth = True\r\n\tgraph = tf.Graph()\r\n\tsess = tf.Session(config=config, graph=graph)\r\n\t\r\n\twith graph.as_default():\r\n\t\twith tf.device('/GPU:0'):\r\n\t\t\twith tf.name_scope(\"tower_0\") as scope:\r\n\t\t\t\twith tf.variable_scope(\"model\", reuse=False):\r\n\t\t\t\t\tv1 = tf.Variable(1.0, name=\"v\")\r\n\t\t\t\t\tx1 = v1 * 1.0\r\n\t\twith tf.device('/GPU:1'), tf.variable_scope(\"model\", reuse=True):\r\n\t\t\twith tf.name_scope(\"tower_1\") as scope:\r\n\t\t\t\twith tf.variable_scope(\"model\", reuse=True):\r\n\t\t\t\t\tx2 = v1 * 1.0\r\n\t\t\tinit = tf.global_variables_initializer()\r\n\t\tprint(tf.trainable_variables())\r\n\t\r\n\tsess.run(init)\r\n\tprint(sess.run([x1, x2]))\r\n```\r\n\r\n**Output** \r\n```\r\n2021-07-05 15:58:54.653001: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-05 15:58:55.487115: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-05 15:58:55.490402: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-07-05 15:58:57.440834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:25:00.0 name: A40 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s\r\n2021-07-05 15:58:57.442125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties:\r\npciBusID: 0000:81:00.0 name: A40 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s\r\n2021-07-05 15:58:57.442157: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-05 15:58:57.443855: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-07-05 15:58:57.443881: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-07-05 15:58:57.444443: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-07-05 15:58:57.444587: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-07-05 15:58:57.445076: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\r\n2021-07-05 15:58:57.445566: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-07-05 15:58:57.445701: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-07-05 15:58:57.450860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\r\n2021-07-05 15:58:57.450885: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-05 15:58:57.994357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-07-05 15:58:57.994427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1\r\n2021-07-05 15:58:57.994437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y\r\n2021-07-05 15:58:57.994442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N\r\n2021-07-05 15:58:58.001069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43676 MB memory) -> physical GPU (device: 0, name: A40, pci bus id: 0000:25:00.0, compute capability: 8.6)\r\n2021-07-05 15:58:58.002705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 43676 MB memory) -> physical GPU (device: 1, name: A40, pci bus id: 0000:81:00.0, compute capability: 8.6)\r\n[<tf.Variable 'tower_0/model/v:0' shape=() dtype=float32>]\r\n2021-07-05 15:58:58.031548: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3493525000 Hz\r\n[1.0, 0.0]\r\n```", "comments": ["I was unable to reproduce the failure. I ran with 2xA40 in the tensorflow/tensorflow:2.5.0rc3-gpu docker container.\r\nI'm using the NVIDIA 460.73.01 driver.\r\n\r\n```\r\n2021-07-14 22:01:57.367956: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-14 22:01:58.318380: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-14 22:01:58.320384: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-07-14 22:01:58.548578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: A40 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s\r\n2021-07-14 22:01:58.550283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \r\npciBusID: 0000:41:00.0 name: A40 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s\r\n2021-07-14 22:01:58.550316: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-14 22:01:58.552212: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-07-14 22:01:58.552244: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-07-14 22:01:58.552910: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-07-14 22:01:58.553088: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-07-14 22:01:58.553684: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\r\n2021-07-14 22:01:58.554290: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-07-14 22:01:58.554423: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-07-14 22:01:58.561077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\r\n2021-07-14 22:01:58.561106: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-14 22:01:59.196402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-07-14 22:01:59.196450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \r\n2021-07-14 22:01:59.196457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y \r\n2021-07-14 22:01:59.196462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N \r\n2021-07-14 22:01:59.205025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43676 MB memory) -> physical GPU (device: 0, name: A40, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-07-14 22:01:59.207032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 43676 MB memory) -> physical GPU (device: 1, name: A40, pci bus id: 0000:41:00.0, compute capability: 8.6)\r\n[<tf.Variable 'tower_0/model/v:0' shape=() dtype=float32>]\r\n2021-07-14 22:01:59.216866: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2794920000 Hz\r\n[1.0, 1.0]\r\n```\r\n\r\n@mtassano-gpfw what happens if you reverse the device to `os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,0'`?", "@nluehr just tried no suggestion, but to no avail.\r\n\r\nI used the same docker container, _tensorflow/tensorflow:2.5.0rc3-gpu_, and we have the same NVIDIA driver, version  _460.73.01_.\r\n\r\nI confirm that I get the same erroneous behaviour in a different machine with 2xA100, same docker container and NVIDIA driver.\r\n\r\nOur two machines with the 2xA40 and the 2xA100 do not have NVLink by the way."]}, {"number": 50607, "title": "How to use the TF_VARIANT with C APIs?", "body": "Hi,\r\n\r\nCurrently, we can create or get a `TF_Tensor` with `TF_FLOAT` data type, and then manipulate the raw data buffer by the pointer from `TF_TensorData`. For example,\r\n\r\n```\r\nTF_Tensor* output = TF_AllocateOutput(xxx, TF_FLOAT, xxx);\r\nfloat* output_raw_buffer = reinterpret_cast<float*>(TF_TensorData(output));\r\n// do some calculation on output_raw_buffer\r\n```\r\n\r\nBut for the data type TF_VARIANT, we can't manipulate it directly like float type. Do we have any examples about how to use it ?\r\n\r\nThanks", "comments": ["@yanzhang-dev \r\n\r\nCould you please refer this [`c_api`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_test.cc#L166),hope it helps.Thanks\r\n", "Hi @UsharaniPagadala \r\n\r\nThe test case you provided is parsing the meta data like shape, dtype of Tensor.\r\nYes, we can create a TF_VARIANT Tensor and get these meta data, too.\r\n\r\nBut if we want to get the elements of Tensor, for example,\r\n\r\n1. If the tensor's elements are `TF_FLOAT`, we can parse the raw buffer like [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api_test.cc#L1671)\r\n2. But if the tensor's elements are `TF_Variant`, we can't parse the raw buffer by `TF_TensorData` like above. How should we do ?\r\n\r\nThanks", "@penpornk ", "Hi @yanzhang-dev,\r\n\r\nSorry for the delay! \r\n\r\n[TF_VARIANT](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/tf_datatype.h;l=69;drc=9849fde5e7b4da4b630ffbc517fad68b2b811c0c) / [DT_VARIANT](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/framework/types.proto;l=40;drc=7c3f96bf324cebe443a7eb814ce771c66c073998) is a data type that is used to encapsulate C++ data structures. You can generally treat a DT_VARIANT tensor as a wrapper of a blob of bytes, whose value will be interpreted based on the kernel it is in. \r\n\r\nFor example, \r\n* [CSRSparseMatrixToDense](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/ops/sparse_csr_matrix_ops.cc;l=151;drc=57395f70dbc24d3ac5f97626520194f6773303e8) takes [a variant input](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/ops/sparse_csr_matrix_ops.cc;l=152;drc=57395f70dbc24d3ac5f97626520194f6773303e8) and [interprets the binary data blob](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/sparse/csr_sparse_matrix_to_dense_op.cc;l=129-130;drc=8d72537c6abf5a44103b57b9c2e22c14f5f49698) as a [CSRSparseMatrix C++ object](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/sparse/sparse_matrix.h;l=35;drc=3381da37560d64c7cb62b53879a0a931ff9036c4).\r\n* TensorList ops [interprets the data blob](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/list_kernels.cc;l=81;drc=8d72537c6abf5a44103b57b9c2e22c14f5f49698) in its variant tensor as a [TensorList C++ object](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/tensor_list.h;l=65;drc=3381da37560d64c7cb62b53879a0a931ff9036c4).\r\n\r\nSince [Variant::get()](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/framework/variant.h;l=246;drc=a697dbc60433fc8378680716cef9ab363bf70568) is a templated function, we can't just expose it as-is for the C API. But I think we can make a C API that returns a type-erased pointer to the binary blob, e.g., add `TensorInterface::VariantData` under [TensorInterface::Data](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/tf_tensor.cc;l=219;drc=dfd157595356cbf49303af75277df5b717caed7b). \r\n\r\n```c++\r\nvoid* TensorInterface::VariantData() const {\r\n  return tensor_.scalar<Variant>()().get<void>();\r\n}\r\n```\r\n\r\nThen your kernel will take this void pointer and cast the blob to a C++ object. This requires plug-ins to have the exact same headers as core TensorFlow. C++ classes also don't have versioning guarantees. So I'm not sure if trying to support `DT_VARIANT` will be worth it.", "@penpornk , is there a list  of  python level ops that ends up using variant data types ?  For example tensorflow.keras.layers.LSTM  uses TensorList.  Not sure if the TensorList  comes from the parent class tf.keras.layers.RNN  or not.  If it is the former, then all RNNs are unfriendly to plugins.   One way I  can think of getting around this issue  is replacing  the op implementation with one that does not use variant type ?  Is there any such alternative for LSTM or RNN ? ", "@whatdhack could you point me to where the `tensorflow.keras.layers.LSTM`  --> `TensorList` --> `DT_VARIANT` usage occurs in the source code?"]}, {"number": 50595, "title": "Please support TFLite gradient operations: BroadcastGradientArgs, StridedSliceGrad and some other", "body": "Within our company we trained a breakthrough realtime model which requires automatic differentiation feature at runtime. To be more specific, the model's output should be forward pass outputs + derivative of runtime loss function with respect to some input control variables.\r\n\r\nWe use TFLite to run the model, but we can't use TF Select kernels because our runtime environment is very strict (video gaming consoles). In the current state, when we try to use `tf.gradients` within our model the TFLite converter does not convert because of some operations to be not supported: `BroadcastGradientArgs`, `DynamicStitch`, `EluGrad`, `Sign`, `StridedSliceGrad`, `UnsortedSegmentSum`\r\n\r\nAs a workaround, we approximate the derivatives with finite difference method, but obviously it requires to make extra forward passes + it introduces some numerical instability to the model. It would be very helpful to have cheap analytical derivatives with a backpropagation step within TFLite.\r\n\r\n**System information**\r\n- `Linux rb15 5.12.14-arch1-1 #1 SMP PREEMPT Thu, 01 Jul 2021 07:26:06 +0000 x86_64 GNU/Linux`\r\n- `TensorFlow 2.5.0` from `tensorflow-opt-cuda` package (https://archlinux.org/packages/community/x86_64/tensorflow-opt-cuda)\r\n\r\n**Provide the text output from tflite_convert**\r\n```\r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: BroadcastGradientArgs, DynamicStitch, EluGrad, Sign, StridedSliceGrad, UnsortedSegmentSum\r\nDetails:\r\n\ttf.BroadcastGradientArgs {device = \"\"}\r\n\ttf.DynamicStitch {device = \"\"}\r\n\ttf.EluGrad {device = \"\"}\r\n\ttf.Sign {device = \"\"}\r\n\ttf.StridedSliceGrad {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 1 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 1 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 1 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 1 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 3 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}\r\n\ttf.StridedSliceGrad {begin_mask = 3 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 4 : i64}\r\n\ttf.UnsortedSegmentSum {device = \"\"}\r\n```", "comments": ["It will take time to review and add ops. You can use https://www.tensorflow.org/lite/guide/ops_select at the moment to unblock the conversion.", "@thaink is there any thread or doc where we can see which ops are under review for the upcoming tensorflow releases? It will help us immensely to plan our internal projects that need to run in really constrained tflite runtime environments and cannot use select ops. Thanks!", "The requests are tracked in https://github.com/tensorflow/tensorflow/issues/21526"]}, {"number": 50575, "title": "Grappler error when Softmax input has a variable dimension", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 11.2/8.1.0\r\n- GPU model and memory: NVIDIA Quadro P620 (4GB)\r\n\r\n**Describe the current behavior**\r\n\r\nThe grappler pass is logging an error when Softmax is used in a `tf.function` with one or more variable dimensions (e.g. a variable batch size). This log started appearing in TensorFlow 2.5.\r\n\r\nIt does not seem to cause issues when running the model, but it might indicate a bug in the grappler implementation.\r\n\r\n**Describe the expected behavior**\r\n\r\nThis operation should not produce any warning or error.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? No\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ninput_signature = (tf.TensorSpec([None, 20], tf.float32),)\r\nsoftmax = tf.function(tf.nn.softmax, input_signature=input_signature)\r\nsoftmax(tf.random.uniform([2, 20]))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe code above logs the following error:\r\n\r\n> `2021-07-02 07:34:12.478300: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Quadro P620\" frequency: 1442 num_cores: 4 environment { key: \"architecture\" value: \"6.1\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 524288 shared_memory_size_per_multiprocessor: 98304 memory_size: 3092316160 bandwidth: 96128000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }`", "comments": ["@guillaumekln  The code snippet doesn't logging any warnings and resulting the output as expected on colab with TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/37906ae3ba15f17d11ad5ced51062a4d/untitled107.ipynb).Thanks!", "Google Colab discards the C++ logs by default. You can run the code this way to see the logs:\r\n\r\n> `!python3 -c 'import tensorflow as tf; input_signature = (tf.TensorSpec([None, 20], tf.float32),); softmax = tf.function(tf.nn.softmax, input_signature=input_signature); softmax(tf.random.uniform([2, 20]))'`\r\n\r\nAlso make sure to enable the GPU backend in your Colab session. Here's the [gist](https://colab.research.google.com/drive/18tXO7yY3rg7EagyK9weHbl4v1bZH38cv) showing the issue (see the last line in the log).", "I experience the same error when using models with attention", "@guillaumekln \r\nI ran the code shared by you on gpu, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e9d8127e49e51eff41b739551769cfd4/untitled616.ipynb).", "Please see this comment to make the C++ log visible in Google Colab: https://github.com/tensorflow/tensorflow/issues/50575#issuecomment-875291904. It links to a [gist](https://colab.research.google.com/drive/18tXO7yY3rg7EagyK9weHbl4v1bZH38cv) that contains the full log output.", "@jvishnuvardhan Do you need additional information?", "Hello,\r\n\r\nI get the same warning in my code (input shape to Softmax is `(None, 4)` as I have 4 output classes). \r\n![image](https://user-images.githubusercontent.com/19503950/132494782-b21aea07-0510-4de9-9cda-9c815c7d5c83.png)\r\n\r\nI can also reproduce it by running the [Google Colab](https://colab.research.google.com/drive/18tXO7yY3rg7EagyK9weHbl4v1bZH38cv#scrollTo=tc1bkAWVWz5F) mentioned by @guillaumekln\r\nBeing a warning, do we know if we should worry about it or if it affects the performance in any way?\r\n\r\nThanks for the help!", "Hello, \r\n\r\nany news about Softmax `warning`? I'm curious if it affects a model's performance. I also got the warning after upgrading the tensorflow version from 2.2 to 2.6\r\n\r\n```2021-10-05 13:25:23.086323: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2499 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.3.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 3145728 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }```\r\n\r\nthanks for listening", "The warning still exists in TensorFlow 2.7.", "@guillaumekln I have the same warning.\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.__version__\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data()\r\n# x_train.shape\r\n## (50000, 32, 32, 3)\r\n\r\nX_train = X_train / 255.0\r\nX_test = X_test / 255.0\r\n\r\n\r\ninputs = tf.keras.layers.Input(shape = (32,32,3))\r\n#dropout\r\ndropout1 = tf.keras.layers.Dropout(rate = 0.2)(inputs)\r\ncnn1 = tf.keras.layers.Conv2D(filters=96,kernel_size = 3, strides = 1, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(dropout1)\r\ncnn2 = tf.keras.layers.Conv2D(filters=96,kernel_size = 3, strides = 1, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(cnn1)\r\ncnn3 = tf.keras.layers.Conv2D(filters=96,kernel_size = 3, strides = 2, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(cnn2)\r\n#add dropout\r\ndropout2 = tf.keras.layers.Dropout(rate = 0.5)(cnn3)\r\ncnn4 = tf.keras.layers.Conv2D(filters=192,kernel_size = 3, strides = 1, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(dropout2)\r\ncnn5 = tf.keras.layers.Conv2D(filters=192,kernel_size = 3, strides = 1, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(cnn4)\r\ncnn6 = tf.keras.layers.Conv2D(filters=192,kernel_size = 3, strides = 2, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(cnn5)\r\n#add dropout\r\ndropout3 = tf.keras.layers.Dropout(rate = 0.5)(cnn6)\r\ncnn7 = tf.keras.layers.Conv2D(filters=192,kernel_size = 3, strides = 1, padding = \"valid\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(dropout3)\r\ncnn8 = tf.keras.layers.Conv2D(filters=192,kernel_size = 1, strides = 1, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(cnn7)\r\ncnn9 = tf.keras.layers.Conv2D(filters=100,kernel_size = 1, strides = 1, padding = \"same\", activation = \"relu\",kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(cnn8)\r\n# global averaging per feature map over 6 \u00d7 6 spatial dimensions\r\nglavpool = tf.keras.layers.GlobalAvgPool2D()(cnn9)\r\n# 100-way softmax\r\noutputs = tf.keras.layers.Activation('softmax')(glavpool)\r\n\r\n\r\nmodel = tf.keras.Model(inputs = inputs, outputs = outputs)\r\n\r\n#Compile model\r\nmodel.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), \r\n              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)) #change loss function\r\n\r\n#Fit model\r\nmodel.fit(x=X_train,y=y_train, batch_size=128, epochs=1) \r\nEpoch 1/5\r\n2021-12-07 20:21:05.693838: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\r\n391/391 [==============================] - 14s 29ms/step - loss: 4.6683\r\nEpoch 2/5\r\n391/391 [==============================] - 11s 28ms/step - loss: 4.6052\r\nEpoch 3/5\r\n391/391 [==============================] - 11s 28ms/step - loss: 4.6052\r\nEpoch 4/5\r\n391/391 [==============================] - 11s 28ms/step - loss: 4.6052\r\nEpoch 5/5\r\n391/391 [==============================] - 11s 29ms/step - loss: 4.6052\r\n\r\n#Predict\r\nyhat = model.predict(X_train[0:128])\r\n2021-12-07 20:22:38.215807: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"GeForce GTX 1080 Ti\" frequency: 1582 num_cores: 28 environment { key: \"architecture\" value: \"6.1\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 2883584 shared_memory_size_per_multiprocessor: 98304 memory_size: 10919215104 bandwidth: 484440000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\r\n```\r\nThis only happens when run on GPU. Python 3.7.12, tried with tf 2.3, 2.4, 2.7.0, CUDA/cuDNN version: 11.2/8.1.0, GeForce GTX 1080 Ti, Ubuntu 18.04.6.", "I have implemented a NMT model with attention mechanism ( Luong style ) and it shows the warning,\r\n\r\n```\r\n op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11010\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14465892352 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\r\n2022-01-21 04:33:52.446453: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11010\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14465892352 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\r\n2022-01-21 04:33:52.446923: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11010\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14465892352 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\r\n2022-01-21 04:33:52.447400: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11010\" } environment { key: \"cudnn\" value: \"8005\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14465892352 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\r\n```\r\n\r\nat every training step. I'm getting this warning in Google Colab, with TensorFlow 2.7.", "Same warning in here with softmax.", "also, this warning appears for me as well. TF 2.6", "I have the same problem. I get the following warning, and sometimes the program stops after that:\r\n\r\n```\r\nW tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\"\r\nvalue { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \r\nTesla V100-SXM2-16GB\" frequency: 1530 num_cores: 80 environment { key: \"architecture\" value: \"7.0\" } environment { key: \r\ncuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: \r\n291456 shared_memory_size_per_multiprocessor: 98304 memory_size: 15346958336 bandwidth: 898048000 } outputs { dtype: \r\nT_FLOAT shape { unknown_rank: true } }\r\n```\r\n\r\nDoes anyone know what the problem can be? How can I solve the problem? ", "Facing the same problem. Is there any solution for this issue?\r\nTensorflow version: '2.7.0'\r\n\r\n`2022-02-13 14:56:25.847034: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla T4\" frequency: 1590 num_cores: 40 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 65536 memory_size: 14463795200 bandwidth: 320064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }`", "Hi @jvishnuvardhan, what's the status of this issue? Can it be assigned to a TensorFlow developer working on the grappler implementation?", "Hi, same error/warning on AMD GPUs \r\n\r\n![image](https://user-images.githubusercontent.com/89974426/155928912-e4ab8830-f0cf-47f8-b1b0-29fe808c1303.png)\r\n\r\nAFAIK batch size is fixed to 64 in my code."]}, {"number": 50567, "title": "Fit failed in TPU when model contains conv_transpose", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): On Kaggle's default kernel. \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:   \r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen my model includes `tf.nn.conv_transpose` and compiles on the TPU, it failed. The simplest code for the error is as follows:\r\n```\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import losses,layers,optimizers,Model\r\nimport tensorflow as tf\r\nimport numpy as np \r\n\r\nclass Test(layers.Layer):\r\n    def __init__(self):\r\n        super().__init__() \r\n        self.f = self.add_weight(name = 'kernel', \r\n                                 trainable = True,\r\n                                 shape = [12,12,3,3], \r\n                                 initializer = tf.random_uniform_initializer()) \r\n    def call(self, inp):  \r\n        print(inp.shape)\r\n        _,H,W,C = inp.shape\r\n        y = tf.nn.conv_transpose(inp, self.f, [-1,H*4, W*4,C], (4,4), 'SAME') \r\n        return y \r\n\r\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_cluster(tpu)\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\nstrategy = tf.distribute.experimental.TPUStrategy(tpu) \r\n \r\nwith strategy.scope():\r\n    model = keras.Sequential()\r\n    model.add(Test())\r\n    model.build([1,48, 48, 3])\r\n    model.compile(optimizers.Adam(), losses.mean_absolute_error)\r\n\r\nlr_data = np.zeros([256, 48, 48, 3]).astype(np.float32)\r\nhr_data = np.zeros([256, 48*4, 48*4, 3]).astype(np.float32)\r\nprint('Fit Begin')\r\nmodel.fit(lr_data, hr_data, epochs=5, batch_size=32, verbose=1)  \r\nprint('Fit End')\r\n```\r\nIt outpus:\r\n```\r\n(1, 48, 48, 3)\r\nFit Begin\r\nEpoch 1/5\r\n(None, 48, 48, 3)\r\n(4, 48, 48, 3)\r\n(4, 48, 48, 3)\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-1-3fe6c59dcd42> in <module>\r\n     31 hr_data = np.zeros([256, 48*4, 48*4, 3]).astype(np.float32)\r\n     32 print('Fit Begin')\r\n---> 33 model.fit(lr_data, hr_data, epochs=5, batch_size=32, verbose=1)\r\n     34 print('Fit End')\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1103               logs = tmp_logs  # No error, now safe to assign to logs.\r\n   1104               end_step = step + data_handler.step_increment\r\n-> 1105               callbacks.on_train_batch_end(end_step, logs)\r\n   1106               if self.stop_training:\r\n   1107                 break\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n    452     \"\"\"\r\n    453     if self._should_call_train_batch_hooks:\r\n--> 454       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n    455 \r\n    456   def on_test_batch_begin(self, batch, logs=None):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    294       self._call_batch_begin_hook(mode, batch, logs)\r\n    295     elif hook == 'end':\r\n--> 296       self._call_batch_end_hook(mode, batch, logs)\r\n    297     else:\r\n    298       raise ValueError('Unrecognized hook: {}'.format(hook))\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)\r\n    314       self._batch_times.append(batch_time)\r\n    315 \r\n--> 316     self._call_batch_hook_helper(hook_name, batch, logs)\r\n    317 \r\n    318     if len(self._batch_times) >= self._num_batches_for_timing_check:\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)\r\n    354       hook = getattr(callback, hook_name)\r\n    355       if getattr(callback, '_supports_tf_logs', False):\r\n--> 356         hook(batch, logs)\r\n    357       else:\r\n    358         if numpy_logs is None:  # Only convert once.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n   1018 \r\n   1019   def on_train_batch_end(self, batch, logs=None):\r\n-> 1020     self._batch_update_progbar(batch, logs)\r\n   1021 \r\n   1022   def on_test_batch_end(self, batch, logs=None):\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)\r\n   1082     if self.verbose == 1:\r\n   1083       # Only block async when verbose = 1.\r\n-> 1084       logs = tf_utils.to_numpy_or_python_type(logs)\r\n   1085       self.progbar.update(self.seen, list(logs.items()), finalize=False)\r\n   1086 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)\r\n    512     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n    513 \r\n--> 514   return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n    515 \r\n    516 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    657 \r\n    658   return pack_sequence_as(\r\n--> 659       structure[0], [func(*x) for x in entries],\r\n    660       expand_composites=expand_composites)\r\n    661 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    657 \r\n    658   return pack_sequence_as(\r\n--> 659       structure[0], [func(*x) for x in entries],\r\n    660       expand_composites=expand_composites)\r\n    661 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)\r\n    508   def _to_single_numpy_or_python_type(t):\r\n    509     if isinstance(t, ops.Tensor):\r\n--> 510       x = t.numpy()\r\n    511       return x.item() if np.ndim(x) == 0 else x\r\n    512     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1069     \"\"\"\r\n   1070     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1071     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1072     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1073 \r\n\r\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1037       return self._numpy_internal()\r\n   1038     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1039       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1040 \r\n   1041   @property\r\n\r\n/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: Socket closed\r\n```\r\n**Describe the expected behavior**\r\nI don't know what caused this error. I tried to change ` tf.nn.conv_ Transpose 'transpose' to other up-sampling methods, and there is no error. But I do need to use transposed convolution to build my model. If it can be solved, I will be very grateful.\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Can any one help me?", "Help me! My model really needs transposed convolution implemented on TPU.", "This issue still exists in tf nightly as well, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/bdf0113e18d262ae0a2df801722dbe57/untitled625.ipynb).", "Hi, the issue is the -1 in the output shape of the convolution. TPUs generally require static shapes, and this includes the output_shape. Your code example works for me if I make the following change:\r\n```\r\n        N,H,W,C = inp.shape\r\n        y = tf.nn.conv_transpose(inp, self.f, [N,H*4, W*4,C], (4,4), 'SAME') \r\n```\r\nI have filed an internal bug to track returning a better error to the user in this case."]}, {"number": 50558, "title": "XLA Operation Semantics documentation has self-conflict", "body": "\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/xla/operation_semantics#slice\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nthe number of parameters are not match.\r\n![image](https://user-images.githubusercontent.com/22614078/124128517-74e18200-daaf-11eb-9a91-5007d4dee8b0.png)\r\n", "comments": ["Hey, stride is an optional argument here, it can be used if you want to slice your array with some step-gaps"]}, {"number": 50555, "title": "mbed compile -m DISCO_F746NG -t GCC_ARM error: mbed error: /usr/bin/python3 returned error", "body": "**System information**\r\n- OS Platform and Distribution:Linux ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: STM32F746G\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.3.0\r\n- Python version:3.8\r\n- Installed using virtualenv? pip? conda?:pip\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I followed this project https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world#deploy-to-STM32F746\r\nAfter I finished the whole process, it was successful, but when I run this command(in readme.txt) to use C++11 \r\n\u2018\u2019\u2019\u2019\u2019\u2019\u2019\r\npython3 -c \u2018import fileinput, glob;\r\nfor filename in glob.glob(\u201cmbed-os/tools/profiles/*.json\u201d):\r\nfor line in fileinput.input(filename, inplace=True):\r\nprint(line.replace(\"\"-std=gnu++98\"\",\"\"-std=c++11\", \u201c-fpermissive\u201d\"))\u2019\r\n\r\n\u2018\u2019\u2019\u2019\u2019\u2019\r\n\r\n\r\n**Any other info / logs**\r\nThe error shows below:\r\n\r\n    [mbed] Working path \u201c~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\u201d (library)\r\n    [mbed] Program path \u201c~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\u201d\r\n    WARNING: MBED_ARM_PATH set as environment variable but doesn\u2019t exist\r\n    Traceback (most recent call last):\r\n    File \u201c~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py\u201d, line 421, in\r\n    main()\r\n    File \u201c~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py\u201d, line 354, in main\r\n    build_profile=extract_profile(parser, options, internal_tc_name),\r\n    File \u201c~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/options.py\u201d, line 128, in extract_profile\r\n    contents = load(open(filename))\r\n    File \u201c/usr/lib/python3.8/json/init.py\u201d, line 293, in load\r\n    return loads(fp.read(),\r\n    File \u201c/usr/lib/python3.8/json/init.py\u201d, line 357, in loads\r\n    return _default_decoder.decode(s)\r\n    File \u201c/usr/lib/python3.8/json/decoder.py\u201d, line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n    File \u201c/usr/lib/python3.8/json/decoder.py\u201d, line 355, in raw_decode\r\n    raise JSONDecodeError(\u201cExpecting value\u201d, s, err.value) from None\r\n    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\r\n    [mbed] ERROR: \u201c/usr/bin/python3\u201d returned error.\r\n    Code: 1\r\n    Path: \u201c~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed\u201d\r\n    Command: \u201c/usr/bin/python3 -u ~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM\u201d\r\n    Tip: You could retry the last command with \u201c-v\u201d flag for verbose output", "comments": []}, {"number": 50551, "title": "Using `tf.cond` to change behaviour for training/inference with dropout causes crash when using XLA", "body": "**System information**\r\n_Some irrelevant fields have been deleted_\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 LTS (Bionic Beaver)\r\n- TensorFlow installed from (source or binary): Binary - installed using `pip`\r\n- TensorFlow version (use command below): Tested with both 2.5.0 (`v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0`) and 1.15.5 (`v1.15.4-39-g3db52be 1.15.5`)\r\n- Python version: Python 3.6.9\r\n- GPU model and memory: N/A - Problem occurs just on CPU (originally identified using Graphcore IPU's)\r\n\r\n**Describe the current behavior**\r\n\r\nThe code fails with the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/localdata/callumm/Z2876/upstream_tf_venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/localdata/callumm/Z2876/upstream_tf_venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1348, in _run_fn\r\n    self._extend_graph()\r\n  File \"/localdata/callumm/Z2876/upstream_tf_venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1388, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Merge nodes {cond_1/gradients/cond/Identity/Switch_grad/cond_grad,cond_1/gradients/cond/dropout/mul/Switch_grad/cond_grad} directly dominated by switch nodes with different predicates (cond_1/gradients/cond/Merge_grad/cond_grad/Switch:1 vs is_training_0_arg:0).\r\n```\r\nInterestingly, the code fails at the call to `sess.run(tf.global_variables_initializer())` - if you remove all code after that line it should still fail.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model - if you can even call it that - should run without issue. It runs fine on the CPU without XLA. To see this, uncomment the line `cpu_result = my_net(placeholder, is_training)`, comment out the next two lines and change `xla_result` to `cpu_result` in the `sess.run()` calls.\r\n\r\n**Do you want to contribute a PR? (yes/no):** No, I do not yet understand the issue enough\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe code here isn't for any particular neural network, it's just a minimal reproducer. The original model is much bigger, but gives rise to the same issue.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.training import gradient_descent\r\n\r\ntf.disable_v2_behavior()\r\n\r\n# Config\r\nBATCH_SIZE = 1\r\nMAX_LENGTH = 16\r\nVOCAB_SIZE = 1000\r\nDROPOUT_RATE = 0.9\r\n\r\n# Generate random data\r\ntrain_data = np.random.randint(0, VOCAB_SIZE, size=(BATCH_SIZE, MAX_LENGTH), dtype=np.int32)\r\n\r\n# Placeholders\r\nwith tf.device(\"cpu\"):\r\n    placeholder = tf.placeholder(dtype=tf.int32, shape=[BATCH_SIZE, MAX_LENGTH], name='placeholder')\r\n    is_training = tf.placeholder_with_default(input=tf.constant(True), shape=(), name=\"is_training\")\r\n\r\n# Define network\r\ndef my_net(x, training):\r\n    embedding_table = tf.get_variable('embedding_table',\r\n                                      [VOCAB_SIZE, 2],\r\n                                      dtype=tf.float32)\r\n    input_embedding = tf.nn.embedding_lookup(embedding_table, x)\r\n\r\n    def dropped_inputs():\r\n        #return input_embedding * 3\r\n        return tf.nn.dropout(input_embedding, rate=DROPOUT_RATE)\r\n    def identity():\r\n        return tf.identity(input_embedding)\r\n\r\n    input_embedding = tf.cond(training, dropped_inputs, identity)\r\n    loss = math_ops.reduce_sum(math_ops.square(input_embedding))\r\n    optimizer = gradient_descent.GradientDescentOptimizer(0.0005)\r\n\r\n    def minimize():\r\n        return optimizer.minimize(loss)\r\n    def no_op():\r\n        return tf.no_op()\r\n    train = tf.cond(training, minimize, no_op)\r\n\r\n    return loss, train\r\n\r\n# Compile and run\r\n\r\n# Uncomment to see the code working\r\n#cpu_result = my_net(placeholder, is_training)\r\n\r\n# Comment out these two lines to see code working\r\nwith tf.device('/device:XLA_CPU:0'):\r\n    xla_result = tf.xla.experimental.compile(my_net, inputs=[placeholder, is_training])\r\n\r\n\r\nwith tf.Session() as sess:\r\n\r\n    # Uncomment to write graph Protobuf to file\r\n    # tf.io.write_graph(sess.graph, 'xla_tf_cond_issue', 'xla_tf_cond_issue.pb', as_text=False)    \r\n\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print('Training...')\r\n    for _ in range(5):\r\n        # Change `xla_result` to `cpu_result` to see code working\r\n        result = sess.run(xla_result, feed_dict={placeholder: train_data, is_training: True})\r\n        print(result)\r\n\r\n    print('Testing...')\r\n    for _ in range(5):\r\n        # Change `xla_result` to `cpu_result` to see code working\r\n        result = sess.run(xla_result, feed_dict={placeholder: train_data, is_training: False})\r\n        print(result)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI have traced the issue to `tensorflow/compiler/tf2xla/functionalize_cond.cc`. Looking at the TensorFlow graph, the complaint raised in the error message is legitimate: the Merge nodes named are indeed directly downstream of two Switch nodes - `cond_1/gradients/Switch` with predicate `cond_1/gradients/cond/Merge_grad/cond_grad/Switch:1` and `cond_1/gradients/cond/mul_grad/Mul/Switch` with predicate `cond_1/pred_id`. \r\n\r\nI'm guessing that, assuming that such a situation should even be allowed to arise at all, it's a result of an optimisation which takes advantage of the fact that the two predicates are essentially the same. Either way, the code in `tensorflow/compiler/tf2xla/functionalize_cond.cc` seems to assume that this will never happen. I would like to use XLA because using it is necessary to run TensorFlow programs on the Graphcore IPU.\r\n\r\nIt appears that \r\n\r\nI've included a line in the reproducer so that you can dump and inspect the graph for yourself using a tool such as Netron - simply uncomment the line `# tf.io.write_graph(sess.graph, 'xla_tf_cond_issue', 'xla_tf_cond_issue.pb', as_text=False)`.\r\n\r\nPlease let me know if there is any more useful information I can provide. I will continue to investigate this issue for myself in the meantime. Thank you!", "comments": ["@jvishnuvardhan ,\r\nI was able to reproduce the issue in tf v2.5,v2.4,v1.15.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/7ac2e05888d0fae66f904e3ce59ff080/untitled50551.ipynb).", "Hello,\r\n\r\nI was looking at this a bit more today and learned about `tf.enable_control_flow_v2()`, and learned that adding this works around the issue. For example, instead of this, which results in the error described above being thrown:\r\n\r\n```python\r\n# Many lines skipped for brevity \r\nfrom tensorflow.python.training import gradient_descent\r\n\r\ntf.disable_v2_behavior()\r\n\r\n# Config\r\nBATCH_SIZE = 1\r\n# Many lines skipped for brevity \r\n```\r\n\r\nWe have this, which results in the program running successfully:\r\n\r\n```python\r\n# Many lines skipped for brevity \r\nfrom tensorflow.python.training import gradient_descent\r\n\r\ntf.disable_v2_behavior()\r\ntf.enable_control_flow_v2()\r\n\r\n# Config\r\nBATCH_SIZE = 1\r\n# Many lines skipped for brevity \r\n```\r\n\r\n I assume this is because instead of putting the control flow in the graph using `Switch` and `Merge` nodes which must later be converted to constructs using `If` nodes (which is where the error is thrown), `If` nodes are used in the first place so no conversion is necessary.\r\n\r\nWhile this gets around the issue, it might still be worth figuring out why a graph where a `Merge` node is dominated by two `Switch` nodes with different predicates.\r\n\r\nThank you for taking the time to look at this.\r\n\r\nWith thanks,\r\nCallum"]}, {"number": 50540, "title": "The ball in \u201chello world\u201d can only achieve a quarter cycle", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ubuntu 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: STM32F746\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:2.3.0\r\n-   **Python version**:3.8\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI have followed readme.txt to deploy to STM32F746 with https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world\r\nWhen I finished the whole process, the animation on the screen can only show a quarter cycle of sine?\r\n### Source code / logs\r\nHere is the screenshot of compiling\r\n![\u5fae\u4fe1\u56fe\u7247_20210630213910](https://user-images.githubusercontent.com/52616905/123970379-a6911500-d9eb-11eb-9c63-2682b87c6439.jpg)\r\n\r\n", "comments": []}, {"number": 50539, "title": "Build with tpu support fails with protobuf error during api generation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\n\r\nThe bazel build process fails with a protobuf error during api generation. Tf successfully builds without tpu support enabled (when removing  --config=tpu).\r\nThe tensorflow source code is derived from r2.6.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel --output_user_root=<...> build --verbose_failures  --config=tpu //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\n\r\nThe main error I get is:\r\n\r\n`[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: google/protobuf/any.proto\r\n\r\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):  terminate called after throwing an instance of 'google::protobuf::FatalException' `\r\n\r\nAttached is the full output of the bazel command.\r\n[bazel.log](https://github.com/tensorflow/tensorflow/files/6741250/bazel.log)\r\n\r\nThanks a lot for the help!\r\n", "comments": []}, {"number": 50529, "title": "Linking with TFLite v2.5.0 fails due to undefined reference to rdft2d", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NVIDIA Xavier Jetson AGX, JetPack 4.5.1 (Ubuntu 18.04.5)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): TFLite installed from source\r\n- TensorFlow version: v2.5.0\r\n- Python version: v3.6.9\r\n- CMake version: v1.19.5\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nInstalling TensorFlow Lite via [Collective Knowledge](http://cknowledge.org/) (which uses CMake), as described in this [Jupyter notebook](https://github.com/krai/ck-mlperf/blob/master/jnotebook/image-classification-tflite-loadgen/image-classification-tflite-loadgen.ipynb), with a new patch to support v2.5.0, would go fine but then fail with a linking error with `rdft2d`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\n$ ck install package --tags=lib,tflite,via-cmake,with.ruy,v2.5.0\r\n$ ck compile program:image-classification-tflite --dep_add_tags.library=tflite,v2.5.0\r\n...\r\n/home/katya/CK-TOOLS/lib-tflite-src-static-gcc-7.5.0-v2.5.0-with.ruy-linux-64/lib/libtensorflow-lite.a(rfft2d.cc.o): In function `tflite::ops::builtin::rfft2d::Rfft2dImpl(int, int, double**, int*, double*)':\r\nrfft2d.cc:(.text+0x978): undefined reference to `rdft2d'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n**Any other info / logs**\r\nWhat's weird is that despite the [apparent intent](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#L54) to \"export\" `fftsg2d` which contains `rdft2d`, only `fftsg.c.o` would be linked against `libtensorflow-lite.a`. As a not completely logical workaround, I've managed just about to get it working with the following [patch](https://github.com/krai/ck-tensorflow/commit/09f8a605f4e69cc0e91a19844cef8f95a0d01f84#diff-8c0c02ef5dc0cea5405bd0bf376468e1107e5d721f8a99c534f9912b7ac59f35):\r\n\r\n<pre><b>diff --git a/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt b/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt</b>\r\n<b>index e7a5ed9b443..04c5b7c05db 100644</b>\r\n<b>--- a/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt</b>\r\n<b>+++ b/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt</b>\r\n<font color=\"#2AA198\">@@ -37,7 +37,7 @@</font> target_include_directories(fft2d_alloc PUBLIC &quot;${FFT2D_SOURCE_DIR}&quot;)\r\n add_library(fft2d_fft4f2d &quot;${FFT2D_SOURCE_DIR}/fft4f2d.c&quot;)\r\n target_include_directories(fft2d_fft4f2d PRIVATE &quot;${FFT2D_SOURCE_DIR}&quot;)\r\n\r\n<font color=\"#DC322F\">-add_library(fft2d_fftsg &quot;${FFT2D_SOURCE_DIR}/fftsg.c&quot;)</font>\r\n<font color=\"#859900\">+add_library(fft2d_fftsg &quot;${FFT2D_SOURCE_DIR}/fftsg.c&quot; &quot;${FFT2D_SOURCE_DIR}/fftsg2d.c&quot;)</font>\r\n\r\n # Requires implementation of fft2d_alloc.\r\n add_library(fft2d_fftsg2d &quot;${FFT2D_SOURCE_DIR}/fftsg2d.c&quot;)\r\n</pre>", "comments": []}, {"number": 50521, "title": "Specifying resource subtypes when converting graphdef to MLIR", "body": "**System information**\r\n- TensorFlow version (you are using): 2.7.0-dev20210629\r\n- Are you willing to contribute it (Yes/No): Yes (but may need some guidance).\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe current tools that allow conversion of TensorFlow models to MLIR, do not seem to allow specifying the subtype for resource type tensors in the input arguments.\r\nConsider the following snippet:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.pywrap_mlir import import_graphdef\r\n\r\nSIZE = 3\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def build(self, input_shape):\r\n        self.w = self.add_weight(shape=(SIZE,), trainable=True)\r\n\r\n    def call(self, input):\r\n        self.w.assign_add(input)\r\n        return input\r\n\r\nif __name__ == \"__main__\":\r\n    model = MyModel()\r\n\r\n    func = tf.function(model)\r\n    concrete_func = func.get_concrete_function(\r\n        tf.TensorSpec(shape=(SIZE,), dtype=tf.float32)\r\n    )\r\n    graph = concrete_func.graph\r\n\r\n    mlir_tf = import_graphdef(\r\n        graph.as_graph_def(add_shapes=True),\r\n        \"tf-standard-pipeline\",\r\n        False,\r\n        input_names=[t.name for t in graph.inputs],\r\n        input_data_types=[\"DT_FLOAT\", \"DT_RESOURCE\"],\r\n        input_data_shapes=[\",\".join(str(d) for d in t.shape) for t in graph.inputs],\r\n        output_names=[t.name for t in graph.outputs],\r\n    )\r\n    print(mlir_tf)\r\n\r\n    with open(\"model.mlir\", \"w\") as f:\r\n        f.write(mlir_tf)\r\n```\r\nwhich outputs the following MLIR:\r\n```mlir\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 811 : i32}}  {\r\n  func @main(%arg0: tensor<3xf32>, %arg1: tensor<!tf.resource>) -> tensor<3xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"args_0:0,my_model/AssignAddVariableOp/resource:0\", outputs = \"Identity:0\"}} {\r\n    \"tf.AssignAddVariableOp\"(%arg1, %arg0) {device = \"\"} : (tensor<!tf.resource>, tensor<3xf32>) -> ()\r\n    %0 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<3xf32>) -> tensor<3xf32>\r\n    return %0 : tensor<3xf32>\r\n  }\r\n}\r\n```\r\nTrying to lower this to `hlo` produces an error:\r\n```console\r\n>>> tf-opt --tf-to-hlo-pipeline model.mlir -o out.mlir\r\nmodel.mlir:2:3: error: expects resource type of argument 1 to have one subtype, got '!tf.resource'\r\n  func @main(%arg0: tensor<3xf32>, %arg1: tensor<!tf.resource>) -> tensor<3xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"args_0:0,my_model/AssignAddVariableOp/resource:0\", outputs = \"Identity:0\"}} {\r\n  ^\r\nmodel.mlir:2:3: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<3xf32>, %arg1: tensor<!tf.resource>):  // no predecessors\r\n  %0 = \"tf.ReadVariableOp\"(%arg1) : (tensor<!tf.resource>) -> tensor<*xf32>\r\n  %1 = \"tf.AddV2\"(%0, %arg0) : (tensor<*xf32>, tensor<3xf32>) -> tensor<*xf32>\r\n  \"tf.AssignVariableOp\"(%arg1, %1) : (tensor<!tf.resource>, tensor<*xf32>) -> ()\r\n  \"std.return\"(%arg0) : (tensor<3xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"args_0:0,my_model/AssignAddVariableOp/resource:0\", outputs = \"Identity:0\"}, type = (tensor<3xf32>, tensor<!tf.resource>) -> tensor<3xf32>} : () -> ()\r\n```\r\nAfter manually modifying the type of `%arg1` to `tensor<!tf.resource<tensor<3xf32>>>`, the conversion to `hlo` works as expected.\r\n\r\nIt would be nice if there was a way to directly specify subtypes for resources when using `import_graphdef` (or `tf_mlir_translate` with the `--tf-input-arrays` option).\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes. One idea is that the `input_data_types` argument of `import_graphdef` could accept subtypes for `DT_RESOURCE` with a syntax like this:\r\n```python\r\ninput_data_types=[\"DT_FLOAT\", \"DT_RESOURCE<tensor<3xf32>>\"],\r\n```\r\nor like this\r\n```python\r\ninput_data_types=[\"DT_FLOAT\", \"DT_RESOURCE(3:DT_FLOAT)\"],\r\n```\r\nor something similar.\r\n`tf_mlir_translate --tf-input-arrays` could adopt the same syntax.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who wants to export models with resource inputs to MLIR (e.g. the graph of a single training step).\r\n\r\n**Any Other info.**\r\n\r\nPlease let me know if there is a different method to emit the desired MLIR.", "comments": ["This seems like it would be a nice improvement indeed!"]}, {"number": 50507, "title": "Iterating through DistributedDataset created from dataset with raggedTensor raises InvalidArgumentError ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): dockerhub 2.5.0-gpu\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda_11.2\r\n- GPU model and memory:  4 * Tesla T4  with 15109MiB memory\r\n\r\n\r\n**Describe the current behavior**\r\nAfter creating a DistributedDataset instances from dataset with raggedTensor elements inside, I tried to iterate through it\r\nbut InvalidArgumentError raised.  What' more, I found that only raggedTensor with string dtype has this problem and int raggedtenor works fine. \r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\r\n\r\n# it works\r\nragged_dict = {'ragged':tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], [], [3, 1, 4, 1], [3, 1], [2, 1, 4, 1]])}\r\nragged_ds = tf.data.Dataset.from_tensor_slices(ragged_dict ).batch(2)\r\ndist_dataset = strategy.experimental_distribute_dataset(ragged_ds)\r\nds = iter(dist_dataset)\r\nnext(ds)\r\n\r\n# it doesn't work\r\nragged_dict = {'ragged' : tf.ragged.constant([['3', '1', '4', '1'], [], ['5', '9', '2'], ['6'], [], ['3', '1', '4', '1'], ['3', '1'], ['2', '1', '4', '1']])}\r\nragged_ds = tf.data.Dataset.from_tensor_slices(ragged_dict).batch(2)\r\ndist_dataset = strategy.experimental_distribute_dataset(ragged_ds)\r\nds = iter(dist_dataset)\r\nnext(ds)\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 686, in __next__\r\n    return self.get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 730, in get_next\r\n    self, self._strategy, return_per_replica=False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 604, in _get_next_as_optional\r\n    iterator._iterators[i].get_next_as_list())  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1948, in get_next_as_list\r\n    self._iterator.get_next_as_optional())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py\", line 604, in get_next_as_optional\r\n    result.append(self._device_iterators[i].get_next_as_optional())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 825, in get_next_as_optional\r\n    self.element_spec)), self.element_spec)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2809, in iterator_get_next_as_optional\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\r\n0 successful operations.\r\n0 derived errors ignored.\r\n         [[{{node RemoteCall}}]] [Op:IteratorGetNextAsOptional]\r\n\r\n```\r\n", "comments": ["@kenyonke \r\n\r\nI was able to reproduce the code shared in tf-2.5 and tf-nightly versions  with no errors.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/aae8bf08f59f64f4b7754e6090547a23/untitled124.ipynb) here.Thanks", "> @kenyonke\r\n> \r\n> I was able to reproduce the code shared in tf-2.5 and tf-nightly versions with no errors.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/aae8bf08f59f64f4b7754e6090547a23/untitled124.ipynb) here.Thanks\r\n\r\nHi @UsharaniPagadala , it sitll doesn't work on my equipment. I have no idea about this problem. I work on tensorflow docker with Tesla T4 Gpu so I think the environment is fine.", "@kenyonke \r\n\r\nLooking at the error log it is similar to  [#28007](https://github.com/tensorflow/tensorflow/issues/28007)  and [#71](https://github.com/TensorSpeech/TensorFlowASR/issues/71) and let us know if it helps.Thanks", "> @kenyonke\r\n> \r\n> Looking at the error log it is similar to [#28007](https://github.com/tensorflow/tensorflow/issues/28007) and [#71](https://github.com/TensorSpeech/TensorFlowASR/issues/71) and let us know if it helps.Thanks\r\n\r\nYou need to run this code on gpus, it works on cpu or single gpu. Colab doesn't use multiple gpu even if you add\r\n```strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])```\r\nYou can see this link\r\nhttps://colab.research.google.com/drive/1tEpZ1TQxyfh-Mz5uDPPXy3RDnX4_yCBM?usp=sharing", "@kenyonke \r\n\r\nI was not able to access the gist provided,Could you Please provide the access.Thanks", "@UsharaniPagadala \r\n\r\nSorry, now you should have right of access\r\nhttps://colab.research.google.com/drive/1tEpZ1TQxyfh-Mz5uDPPXy3RDnX4_yCBM?usp=sharing\r\nI think it's necessary to check whether gpu is available before mirroredstrategy", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Saduf2019 \r\n\r\nI  was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/44b953a32137acffd8eb052adb7edc32/untitled0.ipynb).Thanks"]}, {"number": 50487, "title": "OSError: [Errno 9] Bad file descriptor raised on program exit", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0`\r\n- Python version: `Python 3.8.5`\r\n- CUDA/cuDNN version: `11.2` / `8.1.0.77-1`\r\n- GPU model and memory: P100\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using `MirroredStrategy` as a context manager, Python raises an ignored exception on program exit:\r\n\r\n```\r\nException ignored in: <function Pool.__del__ at 0x7f21f942e4c0>\r\nTraceback (most recent call last):\r\n  File \"/root/miniconda3/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\r\n    self._change_notifier.put(None)\r\n  File \"/root/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 368, in put\r\n    self._writer.send_bytes(obj)\r\n  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n    self._send_bytes(m[offset:offset + size])\r\n  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\r\n    self._send(header + buf)\r\n  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\r\n    n = write(self._handle, buf)\r\nOSError: [Errno 9] Bad file descriptor\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nPython exits without the aforementioned exception. (In my testing, there is no such exception raised on TensorFlow 2.4.0, so this seems new in TensorFlow 2.5.0.)\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```py\r\nimport tensorflow\r\n\r\n\r\ndef f():\r\n    strategy = tensorflow.distribute.MirroredStrategy()\r\n    with strategy.scope():\r\n        tensorflow.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(\r\n            tensorflow.keras.layers.Input(shape=(88, 88, 3))\r\n        )\r\n\r\n\r\nf()\r\n```\r\n\r\nRemoving the `strategy.scope()` causes the program to exit without the ignored exception, as does removing the function definition (i.e., getting rid of `def f()` and `f()`, and invoking at the top level).\r\n", "comments": ["can confirm this in tf2.5.0 from pypi", "@crm416 ,\r\n\r\nCan you please try to execute the code in tf v2.5 and let us know if you are facing same issue? Thanks!", "@tilakrayal - Yes, this only occurs for me in tf v2.5 (and not in tf v2.3 or tf v2.4).", "Same issue in tf v2.6. `OSError` on program exit if `strategy.scope()` is called within a function.\r\n\r\nThe following code causes `OSError` on exit.\r\n```\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n  strategy = tf.distribute.MirroredStrategy()\r\n  print(f'\\nNumber of devices: {strategy.num_replicas_in_sync}\\n')\r\n  with strategy.scope():\r\n    model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\r\n    model.compile(\r\n      loss=tf.keras.losses.MSE,\r\n      optimizer=tf.keras.optimizers.Adam(),\r\n      metrics=['accuracy']\r\n    )\r\n\r\n  print('\\nDONE\\n')\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\nwith the following output:\r\n```\r\n2021-08-27 12:00:25.516889: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-08-27 12:00:32.832857: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2021-08-27 12:00:32.832944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9659 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1c:00.0, compute capability: 7.5\r\n2021-08-27 12:00:32.834864: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n2021-08-27 12:00:32.834898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9659 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:1d:00.0, compute capability: 7.5\r\n\r\nNumber of devices: 2\r\n\r\nDONE\r\n\r\nException ignored in: <function Pool.__del__ at 0x7fbecd304040>\r\nTraceback (most recent call last):\r\n  File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\r\n    self._change_notifier.put(None)\r\n  File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/queues.py\", line 368, in put\r\n    self._writer.send_bytes(obj)\r\n  File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n    self._send_bytes(m[offset:offset + size])\r\n  File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\r\n    self._send(header + buf)\r\n  File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\r\n    n = write(self._handle, buf)\r\nOSError: [Errno 9] Bad file descriptor\r\n```\r\n\r\nWhereas the one below is fine\r\n```\r\nimport tensorflow as tf\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(f'\\nNumber of devices: {strategy.num_replicas_in_sync}\\n')\r\nwith strategy.scope():\r\n  model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\r\n  model.compile(\r\n    loss=tf.keras.losses.MSE,\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    metrics=['accuracy']\r\n  )\r\n\r\nprint('\\nDONE\\n')\r\n```\r\n\r\nAlso tested the same code snippet with tf v2.4 and it ran fine in both cases.", "I see a similar error when running the recommendation model from the models repo on TF2.5.0 and later on python3.8\r\nhttps://github.com/tensorflow/models/tree/v2.5.1/official/recommendation\r\n\r\n```\r\npython ncf_keras_main.py --data_dir=./data --dataset=ml-1m\r\n```\r\n\r\n```\r\nI0830 13:25:38.313174 140736018925024 ncf_keras_main.py:331] Keras evaluation is done.\r\nI0830 13:25:38.313945 140736018925024 ncf_keras_main.py:555] Result is {'loss': 0.3801446557044983, 'eval_loss': 0.0, 'eval_hit_rate': 0.09089403396520465, 'step_timestamp_log': ['BatchTimestamp<batch_index: 0, timestamp: 1630344333.780229>', 'BatchTimestamp<batch_index: 100, timestamp: 1630344337.5320396>'], 'train_finish_time': 1630344337.9485285, 'avg_exp_per_second': 2638725.9874249455}\r\nException ignored in: <function Pool.__del__ at 0x7fffa3c7cf70>\r\nTraceback (most recent call last):\r\n  File \"/tmp/furmanek/miniconda3/envs/opence-conda-env-py3.8-cuda-openmpi-11.2/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\r\n    self._change_notifier.put(None)\r\n  File \"/tmp/furmanek/miniconda3/envs/opence-conda-env-py3.8-cuda-openmpi-11.2/lib/python3.8/multiprocessing/queues.py\", line 368, in put\r\n    self._writer.send_bytes(obj)\r\n  File \"/tmp/furmanek/miniconda3/envs/opence-conda-env-py3.8-cuda-openmpi-11.2/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n    self._send_bytes(m[offset:offset + size])\r\n  File \"/tmp/furmanek/miniconda3/envs/opence-conda-env-py3.8-cuda-openmpi-11.2/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\r\n    self._send(header + buf)\r\n  File \"/tmp/furmanek/miniconda3/envs/opence-conda-env-py3.8-cuda-openmpi-11.2/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\r\n    n = write(self._handle, buf)\r\nOSError: [Errno 9] Bad file descriptor\r\n```\r\n", "The other interesting thing is this only happens (for me at least) on py38 and py39. It runs just fine on py37, so maybe this is a python bug. Perhaps this one?\r\nhttps://bugs.python.org/issue39995", "I tried changing the MirroredStrategy to OneDeviceStrategy and the exception went away. So, not sure if it is an issue caused by both combination of python and TF problems.", "This happens in TF 2. 7 too with python 3.9\r\n\r\nI think it's because MirroredStrategy [creates a multiprocessing ThreadPool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104), but doesn't close it before the program ends, so its resources aren't properly cleaned up and it errors on shutdown.\r\n\r\nYou can explicitly close the pool on exit using:\r\n```\r\nimport atexit\r\n\r\n....\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\natexit.register(strategy._extended._collective_ops._pool.close) # type: ignore\r\n```\r\nWhich should prevent the error for now (until there is a fix).", "> This happens in TF 2. 7 too with python 3.9\r\n> \r\n> I think it's because MirroredStrategy [creates a multiprocessing ThreadPool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104), but doesn't close it before the program ends, so its resources aren't properly cleaned up and it errors on shutdown.\r\n> \r\n> You can explicitly close the pool on exit using:\r\n> \r\n> ```\r\n> import atexit\r\n> \r\n> ....\r\n> \r\n> strategy = tf.distribute.MirroredStrategy()\r\n> \r\n> atexit.register(strategy._extended._collective_ops._pool.close) # type: ignore\r\n> ```\r\n> \r\n> Which should prevent the error for now (until there is a fix).\r\n\r\nThis works for me, thank you!", "For me in TF 2.5.0 the problem is hardware-dependant.\r\nIt is present with V100, but not with 2080 Ti.\r\n", "> This happens in TF 2. 7 too with python 3.9\r\n> \r\n> I think it's because MirroredStrategy [creates a multiprocessing ThreadPool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104), but doesn't close it before the program ends, so its resources aren't properly cleaned up and it errors on shutdown.\r\n> \r\n> You can explicitly close the pool on exit using:\r\n> \r\n> ```\r\n> import atexit\r\n> \r\n> ....\r\n> \r\n> strategy = tf.distribute.MirroredStrategy()\r\n> \r\n> atexit.register(strategy._extended._collective_ops._pool.close) # type: ignore\r\n> ```\r\n> \r\n> Which should prevent the error for now (until there is a fix).\r\n\r\nThe same issue occurs with the `MultiWorkerMirroredStrategy` (when using it on one machine as recommended [here](https://github.com/tensorflow/tensorflow/issues/41898#issuecomment-668786507)), on Python `3.9.10` and tf `2.7`\r\n\r\nThe fix is basically the same as this one, but you have to close two pools:\r\n\r\n```\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n\r\natexit.register(strategy._extended._cross_device_ops._pool.close) # type: ignore\r\natexit.register(strategy._extended._host_cross_device_ops._pool.close) #type: ignore\r\n```", "> > This happens in TF 2. 7 too with python 3.9\r\n> > I think it's because MirroredStrategy [creates a multiprocessing ThreadPool](https://github.com/tensorflow/tensorflow/blob/9eb5fdf99053625f6e870e895a7cce6d1d3ed752/tensorflow/python/distribute/cross_device_ops.py#L1104), but doesn't close it before the program ends, so its resources aren't properly cleaned up and it errors on shutdown.\r\n> > You can explicitly close the pool on exit using:\r\n> > ```\r\n> > import atexit\r\n> > \r\n> > ....\r\n> > \r\n> > strategy = tf.distribute.MirroredStrategy()\r\n> > \r\n> > atexit.register(strategy._extended._collective_ops._pool.close) # type: ignore\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Which should prevent the error for now (until there is a fix).\r\n> \r\n> The same issue occurs with the `MultiWorkerMirroredStrategy` (when using it on one machine as recommended [here](https://github.com/tensorflow/tensorflow/issues/41898#issuecomment-668786507)), on Python `3.9.10` and tf `2.7`\r\n> \r\n> The fix is basically the same as this one, but you have to close two pools:\r\n> \r\n> ```\r\n> strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n> \r\n> atexit.register(strategy._extended._cross_device_ops._pool.close) # type: ignore\r\n> atexit.register(strategy._extended._host_cross_device_ops._pool.close) #type: ignore\r\n> ```\r\n\r\nI use python3.8 and tf 2.8, this problem happens too. So I try to close pools, but it doesn't work.\r\n\r\nmy code:\r\n```\r\n    from tensorflow.python.distribute.cross_device_ops import AllReduceCrossDeviceOps\r\n    ......\r\n    dist_strategy = tf.distribute.MirroredStrategy(\r\n            devices=[\"GPU:\" + str(x) for x in range(FLAGS.n_gpus)],\r\n            cross_device_ops=AllReduceCrossDeviceOps('nccl', num_packs=FLAGS.n_gpus))\r\n``` \r\n\r\nif I use `atexit.register(dist_strategy._extended._collective_ops._pool.close)` ,it doesn't work;\r\nif I use `atexit.register(dist_strategy._extended._cross_device_ops._pool.close)`, it raises error:`'AllReduceCrossDeviceOps' object has no attribute '_pool'`\r\n\r\nwhat else can I do...", "I can report this happens with tensorflow 2.7.0 / python 3.8 on power pc.\r\n\r\nThe solution of @tekumara worked for me as well!", "> Same issue in tf v2.6. `OSError` on program exit if `strategy.scope()` is called within a function.\r\n> \r\n> The following code causes `OSError` on exit.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> def main():\r\n>   strategy = tf.distribute.MirroredStrategy()\r\n>   print(f'\\nNumber of devices: {strategy.num_replicas_in_sync}\\n')\r\n>   with strategy.scope():\r\n>     model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\r\n>     model.compile(\r\n>       loss=tf.keras.losses.MSE,\r\n>       optimizer=tf.keras.optimizers.Adam(),\r\n>       metrics=['accuracy']\r\n>     )\r\n> \r\n>   print('\\nDONE\\n')\r\n> \r\n> if __name__ == '__main__':\r\n>   main()\r\n> ```\r\n> \r\n> with the following output:\r\n> \r\n> ```\r\n> 2021-08-27 12:00:25.516889: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2021-08-27 12:00:32.832857: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n> 2021-08-27 12:00:32.832944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9659 MB memory:  -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:1c:00.0, compute capability: 7.5\r\n> 2021-08-27 12:00:32.834864: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\n> 2021-08-27 12:00:32.834898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9659 MB memory:  -> device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:1d:00.0, compute capability: 7.5\r\n> \r\n> Number of devices: 2\r\n> \r\n> DONE\r\n> \r\n> Exception ignored in: <function Pool.__del__ at 0x7fbecd304040>\r\n> Traceback (most recent call last):\r\n>   File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\r\n>     self._change_notifier.put(None)\r\n>   File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/queues.py\", line 368, in put\r\n>     self._writer.send_bytes(obj)\r\n>   File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\r\n>     self._send_bytes(m[offset:offset + size])\r\n>   File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\r\n>     self._send(header + buf)\r\n>   File \"/miniconda3/envs/test/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\r\n>     n = write(self._handle, buf)\r\n> OSError: [Errno 9] Bad file descriptor\r\n> ```\r\n> \r\n> Whereas the one below is fine\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> strategy = tf.distribute.MirroredStrategy()\r\n> print(f'\\nNumber of devices: {strategy.num_replicas_in_sync}\\n')\r\n> with strategy.scope():\r\n>   model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\r\n>   model.compile(\r\n>     loss=tf.keras.losses.MSE,\r\n>     optimizer=tf.keras.optimizers.Adam(),\r\n>     metrics=['accuracy']\r\n>   )\r\n> \r\n> print('\\nDONE\\n')\r\n> ```\r\n> \r\n> Also tested the same code snippet with tf v2.4 and it ran fine in both cases.\r\n\r\nThank you, it worked"]}, {"number": 50484, "title": "The TfLite \"split\" kernel fails with kTfLiteInt64 input tensor, even though \"split_v\" and \"concatenate\" work", "body": "**System information**\r\n- OS Platform and Distribution: MacOS\r\n- TensorFlow: Converter installed from binary, Interpreter built from source.\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nWe run the TF code shown below.\r\nThen we run the resulting TF model through the TF 2.5.0 tflite Converter.\r\nThen we run the resulting tflite model in the TfLite 2.5.0 Interpreter.\r\n\r\nWe get an error about an unsupported input type in `split::Prepare` and (if we ignore that) an error about an unsupported input type in `split::Eval`.\r\n\r\nPerhaps due to operations in the 1.15 TfLite converter, we did not encounter this issue when using TF1.15.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `split` kernel should operate correctly on kTfLiteInt64 input tensors.\r\n\r\nThe `split_v` kernel allows this, and uses the exact same underlying `reference_ops::Split` to do so!\r\n\r\nOur process would be greatly eased if we did not need to modify our C++ build process for the Interpreter. Specifically, SELECT_TF_OPS generates an excessively large binary, and the AAR build steps are not appropriate for our product.\r\n\r\nThe use of a custom op would require extraordinary upgrades of third-party code. \r\n\r\n- Do you want to contribute a PR? If necessary, sure I will! I'll even add a test for this!\r\n\r\n- Briefly describe your candidate solution(if contributing):\r\nAdd kTfLiteInt64 to the assertion in `split::Prepare`, and add the obvious lines in `split::Eval` (see, for example, `split_v::Eval`).\r\nI have added this change locally to the source code without any _obvious_ issues.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\ndef test_split_export():\r\n    @tf.function\r\n    def test_fn(inp: tf.Tensor) -> tf.Tensor:\r\n        return tf.concat(tf.split(inp, 3, axis=1), axis=1)\r\n\r\n    concrete_f = test_fn.get_concrete_function(tf.TensorSpec((None, 3), tf.int64))\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_f])\r\n    tflite_model = converter.convert()\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n```\r\n\r\n", "comments": ["This is NOT a feature request. It is a bug report.\r\n\r\nThe tflite 2.5.0 Converter is generating an artifact that cannot be evaluated by the tflite 2.5.0 Interpreter.\r\n\r\nThat's a bug."]}, {"number": 50479, "title": "BoringSSL external is including OpenSSL headers and bombing the build.", "body": "\r\nWonderful things happening in release 2.4.2 compilation.\r\nwonderful beautiful things indeed.\r\n\r\nBazel 4.1.0\r\nFedore 34\r\ngcc version 11.1.1 20210428 (Red Hat 11.1.1-1) (GCC) \r\n\r\nSo for some reason or another the 'external' code with anything involving bazel is giving me all the trouble.\r\n\r\nHere this code is mixing boringssl with openssl files.\r\n\r\n\r\nWhat compiler suite are the externals usually built on, btw ?  I seem to remember an argument involving how one suite or another was deviating from the standard when it expected includes from the cc file to propagate to the h file which was included before  the file defining the needed symols.\r\n\r\neg   numeric_precision was needed by 1.h  but was included in 1.cc\r\n\r\nThat kind of suggests something different.\r\n\r\n\r\n> ERROR: /home/john/.cache/bazel/_bazel_john/e3cd31b22e9272d157c657db51cec2c6/external/boringssl/BUILD:147:11: Compiling src/ssl/tls_method.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)\r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/ex_data.h:181:52: error: macro \"CRYPTO_cleanup_all_ex_data\" passed 1 arguments, but takes just 0\r\n>   181 | OPENSSL_EXPORT void CRYPTO_cleanup_all_ex_data(void);\r\n>       |                                                    ^\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:200: note: macro \"CRYPTO_cleanup_all_ex_data\" defined here\r\n>   200 | # define CRYPTO_cleanup_all_ex_data() while(0) continue\r\n>       | \r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:120:41: error: macro \"CRYPTO_num_locks\" passed 1 arguments, but takes just 0\r\n>   120 | OPENSSL_EXPORT int CRYPTO_num_locks(void);\r\n>       |                                         ^\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:212: note: macro \"CRYPTO_num_locks\" defined here\r\n>   212 | #  define CRYPTO_num_locks()            (1)\r\n>       | \r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:131:55: error: macro \"CRYPTO_get_locking_callback\" passed 1 arguments, but takes just 0\r\n>   131 | OPENSSL_EXPORT void (*CRYPTO_get_locking_callback(void))(int mode, int lock_num,\r\n>       |                                                       ^\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:214: note: macro \"CRYPTO_get_locking_callback\" defined here\r\n>   214 | #  define CRYPTO_get_locking_callback()         (NULL)\r\n>       | \r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:176:45: error: macro \"CRYPTO_get_dynlock_create_callback\" passed 1 arguments, but takes just 0\r\n>   176 |     *CRYPTO_get_dynlock_create_callback(void))(const char *file, int line);\r\n>       |                                             ^\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:251: note: macro \"CRYPTO_get_dynlock_create_callback\" defined here\r\n>   251 | #  define CRYPTO_get_dynlock_create_callback()          (NULL)\r\n>       | \r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:179:60: error: macro \"CRYPTO_get_dynlock_lock_callback\" passed 1 arguments, but takes just 0\r\n>   179 | OPENSSL_EXPORT void (*CRYPTO_get_dynlock_lock_callback(void))(\r\n>       |                                                            ^\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:252: note: macro \"CRYPTO_get_dynlock_lock_callback\" defined here\r\n>   252 | #  define CRYPTO_get_dynlock_lock_callback()            (NULL)\r\n>       | \r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:183:63: error: macro \"CRYPTO_get_dynlock_destroy_callback\" passed 1 arguments, but takes just 0\r\n>   183 | OPENSSL_EXPORT void (*CRYPTO_get_dynlock_destroy_callback(void))(\r\n>       |                                                               ^\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:253: note: macro \"CRYPTO_get_dynlock_destroy_callback\" defined here\r\n>   253 | #  define CRYPTO_get_dynlock_destroy_callback()         (NULL)\r\n>       | \r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:318:13: error: conflicting declaration 'typedef int CRYPTO_THREADID'\r\n>   318 | typedef int CRYPTO_THREADID;\r\n>       |             ^~~~~~~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:231:3: note: previous declaration as 'typedef struct crypto_threadid_st CRYPTO_THREADID'\r\n>   231 | } CRYPTO_THREADID;\r\n>       |   ^~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:388:29: error: conflicting declaration 'typedef struct ecdsa_sig_st ECDSA_SIG'\r\n>   388 | typedef struct ecdsa_sig_st ECDSA_SIG;\r\n>       |                             ^~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:22,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ec.h:1127:29: note: previous declaration as 'typedef struct ECDSA_SIG_st ECDSA_SIG'\r\n>  1127 | typedef struct ECDSA_SIG_st ECDSA_SIG;\r\n>       |                             ^~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:390:30: error: conflicting declaration 'typedef struct env_md_ctx_st EVP_MD_CTX'\r\n>   390 | typedef struct env_md_ctx_st EVP_MD_CTX;\r\n>       |                              ^~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:25,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ossl_typ.h:92:30: note: previous declaration as 'typedef struct evp_md_ctx_st EVP_MD_CTX'\r\n>    92 | typedef struct evp_md_ctx_st EVP_MD_CTX;\r\n>       |                              ^~~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:391:26: error: conflicting declaration 'typedef struct env_md_st EVP_MD'\r\n>   391 | typedef struct env_md_st EVP_MD;\r\n>       |                          ^~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:25,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ossl_typ.h:91:26: note: previous declaration as 'typedef struct evp_md_st EVP_MD'\r\n>    91 | typedef struct evp_md_st EVP_MD;\r\n>       |                          ^~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:395:34: error: conflicting declaration 'typedef struct evp_encode_ctx_st EVP_ENCODE_CTX'\r\n>   395 | typedef struct evp_encode_ctx_st EVP_ENCODE_CTX;\r\n>       |                                  ^~~~~~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:25,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ossl_typ.h:100:34: note: previous declaration as 'typedef struct evp_Encode_Ctx_st EVP_ENCODE_CTX'\r\n>   100 | typedef struct evp_Encode_Ctx_st EVP_ENCODE_CTX;\r\n>       |                                  ^~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:411:32: error: conflicting declaration 'typedef struct sha256_state_st SHA256_CTX'\r\n>   411 | typedef struct sha256_state_st SHA256_CTX;\r\n>       |                                ^~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:30,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/sha.h:56:3: note: previous declaration as 'typedef struct SHA256state_st SHA256_CTX'\r\n>    56 | } SHA256_CTX;\r\n>       |   ^~~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:412:32: error: conflicting declaration 'typedef struct sha512_state_st SHA512_CTX'\r\n>   412 | typedef struct sha512_state_st SHA512_CTX;\r\n>       |                                ^~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:30,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/sha.h:103:3: note: previous declaration as 'typedef struct SHA512state_st SHA512_CTX'\r\n>   103 | } SHA512_CTX;\r\n>       |   ^~~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:413:29: error: conflicting declaration 'typedef struct sha_state_st SHA_CTX'\r\n>   413 | typedef struct sha_state_st SHA_CTX;\r\n>       |                             ^~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:30,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/sha.h:39:3: note: previous declaration as 'typedef struct SHAstate_st SHA_CTX'\r\n>    39 | } SHA_CTX;\r\n>       |   ^~~~~~~\r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/ex_data.h:181:21: error: variable or field 'CRYPTO_cleanup_all_ex_data' declared void\r\n>   181 | OPENSSL_EXPORT void CRYPTO_cleanup_all_ex_data(void);\r\n>       |                     ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/include/openssl/ex_data.h:184:13: error: conflicting declaration 'typedef int CRYPTO_EX_dup(CRYPTO_EX_DATA*, const CRYPTO_EX_DATA*, void**, int, long int, void*)'\r\n>   184 | typedef int CRYPTO_EX_dup(CRYPTO_EX_DATA *to, const CRYPTO_EX_DATA *from,\r\n>       |             ^~~~~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:170:13: note: previous declaration as 'typedef int CRYPTO_EX_dup(CRYPTO_EX_DATA*, const CRYPTO_EX_DATA*, void*, int, long int, void*)'\r\n>   170 | typedef int CRYPTO_EX_dup (CRYPTO_EX_DATA *to, const CRYPTO_EX_DATA *from,\r\n>       |             ^~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/ex_data.h:194:8: error: redefinition of 'struct crypto_ex_data_st'\r\n>   194 | struct crypto_ex_data_st {\r\n>       |        ^~~~~~~~~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:86:8: note: previous definition of 'struct crypto_ex_data_st'\r\n>    86 | struct crypto_ex_data_st {\r\n>       |        ^~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:123:16: error: declaration does not declare anything [-fpermissive]\r\n>   123 | OPENSSL_EXPORT void CRYPTO_set_locking_callback(\r\n>       |                ^~~~\r\n> external/boringssl/src/include/openssl/thread.h:127:16: error: declaration does not declare anything [-fpermissive]\r\n>   127 | OPENSSL_EXPORT void CRYPTO_set_add_lock_callback(int (*func)(\r\n>       |                ^~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:235:58: error: expected unqualified-id before numeric constant\r\n>   235 | #  define CRYPTO_THREADID_set_callback(threadid_func)   (0)\r\n>       |                                                          ^\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:235:58: error: expected ')' before numeric constant\r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:143:16: error: declaration does not declare anything [-fpermissive]\r\n>   143 | OPENSSL_EXPORT void CRYPTO_THREADID_set_numeric(CRYPTO_THREADID *id,\r\n>       |                ^~~~\r\n> external/boringssl/src/include/openssl/thread.h:147:16: error: declaration does not declare anything [-fpermissive]\r\n>   147 | OPENSSL_EXPORT void CRYPTO_THREADID_set_pointer(CRYPTO_THREADID *id, void *ptr);\r\n>       |                ^~~~\r\n> external/boringssl/src/include/openssl/thread.h:150:16: error: declaration does not declare anything [-fpermissive]\r\n>   150 | OPENSSL_EXPORT void CRYPTO_THREADID_current(CRYPTO_THREADID *id);\r\n>       |                ^~~~\r\n> external/boringssl/src/include/openssl/thread.h:153:16: error: declaration does not declare anything [-fpermissive]\r\n>   153 | OPENSSL_EXPORT void CRYPTO_set_id_callback(unsigned long (*func)(void));\r\n>       |                ^~~~\r\n> external/boringssl/src/include/openssl/thread.h:158:3: error: conflicting declaration 'typedef struct CRYPTO_dynlock CRYPTO_dynlock'\r\n>   158 | } CRYPTO_dynlock;\r\n>       |   ^~~~~~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:63:3: note: previous declaration as 'typedef struct CRYPTO_dynlock CRYPTO_dynlock'\r\n>    63 | } CRYPTO_dynlock;\r\n>       |   ^~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/thread.h:161:16: error: declaration does not declare anything [-fpermissive]\r\n>   161 | OPENSSL_EXPORT void CRYPTO_set_dynlock_create_callback(\r\n>       |                ^~~~\r\n> external/boringssl/src/include/openssl/thread.h:166:16: error: declaration does not declare anything [-fpermissive]\r\n>   166 | OPENSSL_EXPORT void CRYPTO_set_dynlock_lock_callback(void (*dyn_lock_function)(\r\n>       |                ^~~~\r\n> external/boringssl/src/include/openssl/thread.h:170:16: error: declaration does not declare anything [-fpermissive]\r\n>   170 | OPENSSL_EXPORT void CRYPTO_set_dynlock_destroy_callback(\r\n>       |                ^~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/ssl/../crypto/internal.h:642:1: error: expected constructor, destructor, or type conversion before 'typedef'\r\n>   642 | typedef struct {\r\n>       | ^~~~~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:648:3: error: 'CRYPTO_EX_DATA_CLASS' does not name a type; did you mean 'CRYPTO_EX_DATA_FUNCS'?\r\n>   648 | } CRYPTO_EX_DATA_CLASS;\r\n>       |   ^~~~~~~~~~~~~~~~~~~~\r\n>       |   CRYPTO_EX_DATA_FUNCS\r\n> external/boringssl/src/ssl/../crypto/internal.h:658:44: error: 'int CRYPTO_get_ex_new_index' redeclared as different kind of entity\r\n>   658 | OPENSSL_EXPORT int CRYPTO_get_ex_new_index(CRYPTO_EX_DATA_CLASS *ex_data_class,\r\n>       |                                            ^~~~~~~~~~~~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:172:12: note: previous declaration 'int CRYPTO_get_ex_new_index(int, long int, void*, void (*)(void*, void*, CRYPTO_EX_DATA*, int, long int, void*), int (*)(CRYPTO_EX_DATA*, const CRYPTO_EX_DATA*, void*, int, long int, void*), void (*)(void*, void*, CRYPTO_EX_DATA*, int, long int, void*))'\r\n>   172 | __owur int CRYPTO_get_ex_new_index(int class_index, long argl, void *argp,\r\n>       |            ^~~~~~~~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/ssl/../crypto/internal.h:658:44: error: 'CRYPTO_EX_DATA_CLASS' was not declared in this scope; did you mean 'CRYPTO_EX_DATA_FUNCS'?\r\n>   658 | OPENSSL_EXPORT int CRYPTO_get_ex_new_index(CRYPTO_EX_DATA_CLASS *ex_data_class,\r\n>       |                                            ^~~~~~~~~~~~~~~~~~~~\r\n>       |                                            CRYPTO_EX_DATA_FUNCS\r\n> external/boringssl/src/ssl/../crypto/internal.h:658:66: error: 'ex_data_class' was not declared in this scope\r\n>   658 | OPENSSL_EXPORT int CRYPTO_get_ex_new_index(CRYPTO_EX_DATA_CLASS *ex_data_class,\r\n>       |                                                                  ^~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:659:44: error: expected primary-expression before 'int'\r\n>   659 |                                            int *out_index, long argl,\r\n>       |                                            ^~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:659:60: error: expected primary-expression before 'long'\r\n>   659 |                                            int *out_index, long argl,\r\n>       |                                                            ^~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:660:44: error: expected primary-expression before 'void'\r\n>   660 |                                            void *argp,\r\n>       |                                            ^~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:661:59: error: expected primary-expression before '*' token\r\n>   661 |                                            CRYPTO_EX_free *free_func);\r\n>       |                                                           ^\r\n> external/boringssl/src/ssl/../crypto/internal.h:661:60: error: 'free_func' was not declared in this scope\r\n>   661 |                                            CRYPTO_EX_free *free_func);\r\n>       |                                                            ^~~~~~~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:673:21: error: conflicting declaration of C function 'void CRYPTO_new_ex_data(CRYPTO_EX_DATA*)'\r\n>   673 | OPENSSL_EXPORT void CRYPTO_new_ex_data(CRYPTO_EX_DATA *ad);\r\n>       |                     ^~~~~~~~~~~~~~~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:182:5: note: previous declaration 'int CRYPTO_new_ex_data(int, void*, CRYPTO_EX_DATA*)'\r\n>   182 | int CRYPTO_new_ex_data(int class_index, void *obj, CRYPTO_EX_DATA *ad);\r\n>       |     ^~~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/ssl/../crypto/internal.h:677:21: error: variable or field 'CRYPTO_free_ex_data' declared void\r\n>   677 | OPENSSL_EXPORT void CRYPTO_free_ex_data(CRYPTO_EX_DATA_CLASS *ex_data_class,\r\n>       |                     ^~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:677:41: error: 'CRYPTO_EX_DATA_CLASS' was not declared in this scope; did you mean 'CRYPTO_EX_DATA_FUNCS'?\r\n>   677 | OPENSSL_EXPORT void CRYPTO_free_ex_data(CRYPTO_EX_DATA_CLASS *ex_data_class,\r\n>       |                                         ^~~~~~~~~~~~~~~~~~~~\r\n>       |                                         CRYPTO_EX_DATA_FUNCS\r\n> external/boringssl/src/ssl/../crypto/internal.h:677:63: error: 'ex_data_class' was not declared in this scope\r\n>   677 | OPENSSL_EXPORT void CRYPTO_free_ex_data(CRYPTO_EX_DATA_CLASS *ex_data_class,\r\n>       |                                                               ^~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:678:41: error: expected primary-expression before 'void'\r\n>   678 |                                         void *obj, CRYPTO_EX_DATA *ad);\r\n>       |                                         ^~~~\r\n> external/boringssl/src/ssl/../crypto/internal.h:678:67: error: expected primary-expression before '*' token\r\n>   678 |                                         void *obj, CRYPTO_EX_DATA *ad);\r\n>       |                                                                   ^\r\n> external/boringssl/src/ssl/../crypto/internal.h:678:68: error: 'ad' was not declared in this scope\r\n>   678 |                                         void *obj, CRYPTO_EX_DATA *ad);\r\n>       |                                                                    ^~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant\r\n>   130 | #  define OPENSSL_FILE __FILE__\r\n>       |                        ^~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: error: conflicting declaration of C function 'void* CRYPTO_malloc(size_t, int)'\r\n>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:266:7: note: previous declaration 'void* CRYPTO_malloc(size_t, const char*, int)'\r\n>   266 | void *CRYPTO_malloc(size_t num, const char *file, int line);\r\n>       |       ^~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant\r\n>   130 | #  define OPENSSL_FILE __FILE__\r\n>       |                        ^~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:9: error: conflicting declaration of C function 'void CRYPTO_free(void*, int)'\r\n>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:271:6: note: previous declaration 'void CRYPTO_free(void*, const char*, int)'\r\n>   271 | void CRYPTO_free(void *ptr, const char *file, int line);\r\n>       |      ^~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant\r\n>   130 | #  define OPENSSL_FILE __FILE__\r\n>       |                        ^~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:122:9: error: conflicting declaration of C function 'void* CRYPTO_realloc(void*, size_t, int)'\r\n>   122 |         CRYPTO_realloc(addr, num, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:273:7: note: previous declaration 'void* CRYPTO_realloc(void*, size_t, const char*, int)'\r\n>   273 | void *CRYPTO_realloc(void *addr, size_t num, const char *file, int line);\r\n>       |       ^~~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant\r\n>   130 | #  define OPENSSL_FILE __FILE__\r\n>       |                        ^~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:132:9: error: conflicting declaration of C function 'char* CRYPTO_strdup(const char*, int)'\r\n>   132 |         CRYPTO_strdup(str, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:269:7: note: previous declaration 'char* CRYPTO_strdup(const char*, const char*, int)'\r\n>   269 | char *CRYPTO_strdup(const char *str, const char *file, int line);\r\n>       |       ^~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant\r\n>   130 | #  define OPENSSL_FILE __FILE__\r\n>       |                        ^~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:134:9: error: conflicting declaration of C function 'char* CRYPTO_strndup(const char*, size_t, int)'\r\n>   134 |         CRYPTO_strndup(str, n, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:270:7: note: previous declaration 'char* CRYPTO_strndup(const char*, size_t, const char*, int)'\r\n>   270 | char *CRYPTO_strndup(const char *str, size_t s, const char *file, int line);\r\n>       |       ^~~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:130:22: error: 'void* CRYPTO_memdup' redeclared as different kind of entity\r\n>   130 |         CRYPTO_memdup((str), s, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |                      ^\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:268:7: note: previous declaration 'void* CRYPTO_memdup(const void*, size_t, const char*, int)'\r\n>   268 | void *CRYPTO_memdup(const void *str, size_t siz, const char *file, int line);\r\n>       |       ^~~~~~~~~~~~~\r\n> external/boringssl/src/include/openssl/mem.h:137:22: error: expected primary-expression before 'const'\r\n>   137 | OPENSSL_EXPORT void *OPENSSL_memdup(const void *data, size_t size);\r\n>       |                      ^~~~~~~~~~~~~~\r\n> external/boringssl/src/include/openssl/mem.h:137:22: error: expected ')' before 'const'\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:130:23: note: to match this '('\r\n>   130 |         CRYPTO_memdup((str), s, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |                       ^\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant\r\n>   130 | #  define OPENSSL_FILE __FILE__\r\n>       |                        ^~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:126:9: error: conflicting declaration of C function 'void CRYPTO_clear_free(void*, size_t, int)'\r\n>   126 |         CRYPTO_clear_free(addr, num, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:272:6: note: previous declaration 'void CRYPTO_clear_free(void*, size_t, const char*, int)'\r\n>   272 | void CRYPTO_clear_free(void *ptr, size_t num, const char *file, int line);\r\n>       |      ^~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/internal.h:157,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:65:\r\n> external/boringssl/src/include/openssl/mem.h: In static member function 'static void bssl::internal::DeleterImpl<char>::Free(char*)':\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:9: error: 'OPENSSL_free' was not declared in this scope\r\n>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~\r\n> external/boringssl/src/include/openssl/mem.h: In static member function 'static void bssl::internal::DeleterImpl<unsigned char>::Free(uint8_t*)':\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:9: error: 'OPENSSL_free' was not declared in this scope\r\n>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h: In function 'T* bssl::New(Args&& ...)':\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: error: there are no arguments to 'OPENSSL_malloc' that depend on a template parameter, so a declaration of 'OPENSSL_malloc' must be available [-fpermissive]\r\n>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:192:13: note: in expansion of macro 'OPENSSL_malloc'\r\n>   192 |   void *t = OPENSSL_malloc(sizeof(T));\r\n>       |             ^~~~~~~~~~~~~~\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)\r\n>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:192:13: note: in expansion of macro 'OPENSSL_malloc'\r\n>   192 |   void *t = OPENSSL_malloc(sizeof(T));\r\n>       |             ^~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:65:\r\n> external/boringssl/src/ssl/internal.h:194:26: error: expected primary-expression before ',' token\r\n>   194 |     OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);\r\n>       |                          ^\r\n> external/boringssl/src/ssl/internal.h:194:5: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]\r\n>   194 |     OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);\r\n>       |     ^~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h: In member function 'bool bssl::Array<T>::Init(size_t)':\r\n> external/boringssl/src/ssl/internal.h:317:28: error: expected primary-expression before ',' token\r\n>   317 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);\r\n>       |                            ^\r\n> external/boringssl/src/ssl/internal.h:317:30: error: 'ERR_R_OVERFLOW' was not declared in this scope; did you mean 'EOVERFLOW'?\r\n>   317 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);\r\n>       |                              ^~~~~~~~~~~~~~\r\n>       |                              EOVERFLOW\r\n> external/boringssl/src/ssl/internal.h:317:7: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]\r\n>   317 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);\r\n>       |       ^~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/internal.h:157,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:65:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: error: there are no arguments to 'OPENSSL_malloc' that depend on a template parameter, so a declaration of 'OPENSSL_malloc' must be available [-fpermissive]\r\n>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |         ^~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:320:35: note: in expansion of macro 'OPENSSL_malloc'\r\n>   320 |     data_ = reinterpret_cast<T *>(OPENSSL_malloc(new_size * sizeof(T)));\r\n>       |                                   ^~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:65:\r\n> external/boringssl/src/ssl/internal.h:322:28: error: expected primary-expression before ',' token\r\n>   322 |       OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);\r\n>       |                            ^\r\n> external/boringssl/src/ssl/internal.h:322:7: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]\r\n>   322 |       OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);\r\n>       |       ^~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h: In member function 'bool bssl::GrowableArray<T>::MaybeGrow()':\r\n> external/boringssl/src/ssl/internal.h:423:28: error: expected primary-expression before ',' token\r\n>   423 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);\r\n>       |                            ^\r\n> external/boringssl/src/ssl/internal.h:423:30: error: 'ERR_R_OVERFLOW' was not declared in this scope; did you mean 'EOVERFLOW'?\r\n>   423 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);\r\n>       |                              ^~~~~~~~~~~~~~\r\n>       |                              EOVERFLOW\r\n> external/boringssl/src/ssl/internal.h:423:7: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]\r\n>   423 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);\r\n>       |       ^~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h: At global scope:\r\n> external/boringssl/src/ssl/internal.h:732:3: error: 'ScopedEVP_MD_CTX' does not name a type; did you mean 'ScopedEVP_AEAD_CTX'?\r\n>   732 |   ScopedEVP_MD_CTX hash_;\r\n>       |   ^~~~~~~~~~~~~~~~\r\n>       |   ScopedEVP_AEAD_CTX\r\n> external/boringssl/src/ssl/internal.h:1122:7: error: field 'body' has incomplete type 'CBS' {aka 'cbs_st'}\r\n>  1122 |   CBS body;\r\n>       |       ^~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:376:16: note: forward declaration of 'CBS' {aka 'struct cbs_st'}\r\n>   376 | typedef struct cbs_st CBS;\r\n>       |                ^~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:65:\r\n> external/boringssl/src/ssl/internal.h:1125:7: error: field 'raw' has incomplete type 'CBS' {aka 'cbs_st'}\r\n>  1125 |   CBS raw;\r\n>       |       ^~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h:376:16: note: forward declaration of 'CBS' {aka 'struct cbs_st'}\r\n>   376 | typedef struct cbs_st CBS;\r\n>       |                ^~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:65:\r\n> external/boringssl/src/ssl/internal.h:1885:11: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?\r\n>  1885 |     const SSL_CLIENT_HELLO *client_hello, CBS *contents);\r\n>       |           ^~~~~~~~~~~~~~~~\r\n>       |           SSL_CLIENT_HELLO_CB\r\n> external/boringssl/src/ssl/internal.h:1917:31: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?\r\n>  1917 |                         const SSL_CLIENT_HELLO *client_hello);\r\n>       |                               ^~~~~~~~~~~~~~~~\r\n>       |                               SSL_CLIENT_HELLO_CB\r\n> external/boringssl/src/ssl/internal.h:1955:44: error: 'SSL_CLIENT_HELLO' has not been declared\r\n>  1955 | bool ssl_client_hello_init(const SSL *ssl, SSL_CLIENT_HELLO *out,\r\n>       |                                            ^~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:1958:43: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?\r\n>  1958 | bool ssl_client_hello_get_extension(const SSL_CLIENT_HELLO *client_hello,\r\n>       |                                           ^~~~~~~~~~~~~~~~\r\n>       |                                           SSL_CLIENT_HELLO_CB\r\n> external/boringssl/src/ssl/internal.h:1962:11: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?\r\n>  1962 |     const SSL_CLIENT_HELLO *client_hello, uint16_t id);\r\n>       |           ^~~~~~~~~~~~~~~~\r\n>       |           SSL_CLIENT_HELLO_CB\r\n> external/boringssl/src/ssl/internal.h:2249:16: error: 'SSL_TICKET_KEY_NAME_LEN' was not declared in this scope\r\n>  2249 |   uint8_t name[SSL_TICKET_KEY_NAME_LEN] = {0};\r\n>       |                ^~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:2262:3: error: 'ssl_cert_compression_func_t' does not name a type\r\n>  2262 |   ssl_cert_compression_func_t compress = nullptr;\r\n>       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:2263:3: error: 'ssl_cert_decompression_func_t' does not name a type\r\n>  2263 |   ssl_cert_decompression_func_t decompress = nullptr;\r\n>       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/ssl/internal.h:2271:1: error: expected unqualified-id before 'namespace'\r\n>  2271 | BSSL_NAMESPACE_BEGIN\r\n>       | ^~~~~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:65:\r\n> external/boringssl/src/ssl/internal.h:3173:59: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?\r\n>  3173 |   ssl_select_cert_result_t (*select_certificate_cb)(const SSL_CLIENT_HELLO *) =\r\n>       |                                                           ^~~~~~~~~~~~~~~~\r\n>       |                                                           SSL_CLIENT_HELLO_CB\r\n> external/boringssl/src/ssl/internal.h:3179:34: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?\r\n>  3179 |   int (*dos_protection_cb)(const SSL_CLIENT_HELLO *) = nullptr;\r\n>       |                                  ^~~~~~~~~~~~~~~~\r\n>       |                                  SSL_CLIENT_HELLO_CB\r\n> external/boringssl/src/ssl/internal.h:3093:30: error: 'SSL_DEFAULT_SESSION_TIMEOUT' was not declared in this scope\r\n>  3093 |   uint32_t session_timeout = SSL_DEFAULT_SESSION_TIMEOUT;\r\n>       |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:3097:38: error: 'SSL_DEFAULT_SESSION_PSK_DHE_TIMEOUT' was not declared in this scope\r\n>  3097 |   uint32_t session_psk_dhe_timeout = SSL_DEFAULT_SESSION_PSK_DHE_TIMEOUT;\r\n>       |                                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:3377:9: error: 'SSL3_STATE' in namespace 'bssl' does not name a type\r\n>  3377 |   bssl::SSL3_STATE *s3 = nullptr;   // TLS variables\r\n>       |         ^~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:3378:9: error: 'DTLS1_STATE' in namespace 'bssl' does not name a type\r\n>  3378 |   bssl::DTLS1_STATE *d1 = nullptr;  // DTLS variables\r\n>       |         ^~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:3419:45: error: 'ssl_renegotiate_never' was not declared in this scope; did you mean 'ssl_renegotiate_mode_t'?\r\n>  3419 |   ssl_renegotiate_mode_t renegotiate_mode = ssl_renegotiate_never;\r\n>       |                                             ^~~~~~~~~~~~~~~~~~~~~\r\n>       |                                             ssl_renegotiate_mode_t\r\n> external/boringssl/src/ssl/internal.h:3495:22: error: 'SSL_DEFAULT_SESSION_TIMEOUT' was not declared in this scope\r\n>  3495 |   uint32_t timeout = SSL_DEFAULT_SESSION_TIMEOUT;\r\n>       |                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/internal.h:3499:27: error: 'SSL_DEFAULT_SESSION_TIMEOUT' was not declared in this scope\r\n>  3499 |   uint32_t auth_timeout = SSL_DEFAULT_SESSION_TIMEOUT;\r\n>       |                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc: In function 'void bssl::ssl3_on_handshake_complete(SSL*)':\r\n> external/boringssl/src/ssl/tls_method.cc:79:12: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>    79 |   if (ssl->s3->hs_buf && ssl->s3->hs_buf->length == 0) {\r\n>       |            ^~\r\n> external/boringssl/src/ssl/tls_method.cc:79:31: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>    79 |   if (ssl->s3->hs_buf && ssl->s3->hs_buf->length == 0) {\r\n>       |                               ^~\r\n> external/boringssl/src/ssl/tls_method.cc:80:10: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>    80 |     ssl->s3->hs_buf.reset();\r\n>       |          ^~\r\n> external/boringssl/src/ssl/tls_method.cc: In function 'bool bssl::ssl3_set_read_state(SSL*, bssl::UniquePtr<bssl::SSLAEADContext>)':\r\n> external/boringssl/src/ssl/tls_method.cc:87:26: error: expected primary-expression before ',' token\r\n>    87 |     OPENSSL_PUT_ERROR(SSL, SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE);\r\n>       |                          ^\r\n> external/boringssl/src/ssl/tls_method.cc:87:28: error: 'SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE' was not declared in this scope\r\n>    87 |     OPENSSL_PUT_ERROR(SSL, SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE);\r\n>       |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:87:5: error: 'OPENSSL_PUT_ERROR' was not declared in this scope\r\n>    87 |     OPENSSL_PUT_ERROR(SSL, SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE);\r\n>       |     ^~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:88:5: error: 'ssl_send_alert' was not declared in this scope; did you mean 'ssl_process_alert'?\r\n>    88 |     ssl_send_alert(ssl, SSL3_AL_FATAL, SSL_AD_UNEXPECTED_MESSAGE);\r\n>       |     ^~~~~~~~~~~~~~\r\n>       |     ssl_process_alert\r\n> external/boringssl/src/ssl/tls_method.cc:92:23: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>    92 |   OPENSSL_memset(ssl->s3->read_sequence, 0, sizeof(ssl->s3->read_sequence));\r\n>       |                       ^~\r\n> external/boringssl/src/ssl/tls_method.cc:92:57: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>    92 |   OPENSSL_memset(ssl->s3->read_sequence, 0, sizeof(ssl->s3->read_sequence));\r\n>       |                                                         ^~\r\n> external/boringssl/src/ssl/tls_method.cc:93:8: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>    93 |   ssl->s3->aead_read_ctx = std::move(aead_ctx);\r\n>       |        ^~\r\n> external/boringssl/src/ssl/tls_method.cc: In function 'bool bssl::ssl3_set_write_state(SSL*, bssl::UniquePtr<bssl::SSLAEADContext>)':\r\n> external/boringssl/src/ssl/tls_method.cc:102:23: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>   102 |   OPENSSL_memset(ssl->s3->write_sequence, 0, sizeof(ssl->s3->write_sequence));\r\n>       |                       ^~\r\n> external/boringssl/src/ssl/tls_method.cc:102:58: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>   102 |   OPENSSL_memset(ssl->s3->write_sequence, 0, sizeof(ssl->s3->write_sequence));\r\n>       |                                                          ^~\r\n> external/boringssl/src/ssl/tls_method.cc:103:8: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'\r\n>   103 |   ssl->s3->aead_write_ctx = std::move(aead_ctx);\r\n>       |        ^~\r\n> external/boringssl/src/ssl/tls_method.cc: At global scope:\r\n> external/boringssl/src/ssl/tls_method.cc:109:5: error: 'ssl3_new' was not declared in this scope\r\n>   109 |     ssl3_new,\r\n>       |     ^~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:110:5: error: 'ssl3_free' was not declared in this scope; did you mean 'SSL_free'?\r\n>   110 |     ssl3_free,\r\n>       |     ^~~~~~~~~\r\n>       |     SSL_free\r\n> external/boringssl/src/ssl/tls_method.cc:111:5: error: 'ssl3_get_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?\r\n>   111 |     ssl3_get_message,\r\n>       |     ^~~~~~~~~~~~~~~~\r\n>       |     ssl_hs_read_message\r\n> external/boringssl/src/ssl/tls_method.cc:112:5: error: 'ssl3_next_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?\r\n>   112 |     ssl3_next_message,\r\n>       |     ^~~~~~~~~~~~~~~~~\r\n>       |     ssl_hs_read_message\r\n> external/boringssl/src/ssl/tls_method.cc:113:5: error: 'ssl3_open_handshake' was not declared in this scope; did you mean 'ssl_open_handshake'?\r\n>   113 |     ssl3_open_handshake,\r\n>       |     ^~~~~~~~~~~~~~~~~~~\r\n>       |     ssl_open_handshake\r\n> external/boringssl/src/ssl/tls_method.cc:114:5: error: 'ssl3_open_change_cipher_spec' was not declared in this scope; did you mean 'ssl_open_change_cipher_spec'?\r\n>   114 |     ssl3_open_change_cipher_spec,\r\n>       |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>       |     ssl_open_change_cipher_spec\r\n> external/boringssl/src/ssl/tls_method.cc:115:5: error: 'ssl3_open_app_data' was not declared in this scope; did you mean 'ssl_open_app_data'?\r\n>   115 |     ssl3_open_app_data,\r\n>       |     ^~~~~~~~~~~~~~~~~~\r\n>       |     ssl_open_app_data\r\n> external/boringssl/src/ssl/tls_method.cc:116:5: error: 'ssl3_write_app_data' was not declared in this scope; did you mean 'ssl_open_app_data'?\r\n>   116 |     ssl3_write_app_data,\r\n>       |     ^~~~~~~~~~~~~~~~~~~\r\n>       |     ssl_open_app_data\r\n> external/boringssl/src/ssl/tls_method.cc:117:5: error: 'ssl3_dispatch_alert' was not declared in this scope\r\n>   117 |     ssl3_dispatch_alert,\r\n>       |     ^~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:118:5: error: 'ssl3_init_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?\r\n>   118 |     ssl3_init_message,\r\n>       |     ^~~~~~~~~~~~~~~~~\r\n>       |     ssl_hs_read_message\r\n> external/boringssl/src/ssl/tls_method.cc:119:5: error: 'ssl3_finish_message' was not declared in this scope\r\n>   119 |     ssl3_finish_message,\r\n>       |     ^~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:120:5: error: 'ssl3_add_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?\r\n>   120 |     ssl3_add_message,\r\n>       |     ^~~~~~~~~~~~~~~~\r\n>       |     ssl_hs_read_message\r\n> external/boringssl/src/ssl/tls_method.cc:121:5: error: 'ssl3_add_change_cipher_spec' was not declared in this scope; did you mean 'ssl_open_change_cipher_spec'?\r\n>   121 |     ssl3_add_change_cipher_spec,\r\n>       |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n>       |     ssl_open_change_cipher_spec\r\n> external/boringssl/src/ssl/tls_method.cc:122:5: error: 'ssl3_flush_flight' was not declared in this scope\r\n>   122 |     ssl3_flush_flight,\r\n>       |     ^~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:1867:33: error: redefinition of 'const SSL_METHOD* TLS_method()'\r\n>  1867 | #define SSLv23_method           TLS_method\r\n>       |                                 ^~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:197:19: note: in expansion of macro 'SSLv23_method'\r\n>   197 | const SSL_METHOD *SSLv23_method(void) {\r\n>       |                   ^~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:188:19: note: 'const SSL_METHOD* TLS_method()' previously defined here\r\n>   188 | const SSL_METHOD *TLS_method(void) {\r\n>       |                   ^~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:273:19: error: redefinition of 'const SSL_METHOD* TLS_server_method()'\r\n>   273 | const SSL_METHOD *TLS_server_method(void) {\r\n>       |                   ^~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:1868:33: note: 'const SSL_METHOD* TLS_server_method()' previously defined here\r\n>  1868 | #define SSLv23_server_method    TLS_server_method\r\n>       |                                 ^~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:265:19: note: in expansion of macro 'SSLv23_server_method'\r\n>   265 | const SSL_METHOD *SSLv23_server_method(void) {\r\n>       |                   ^~~~~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:277:19: error: redefinition of 'const SSL_METHOD* TLS_client_method()'\r\n>   277 | const SSL_METHOD *TLS_client_method(void) {\r\n>       |                   ^~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/ssl/tls_method.cc:57:\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:1869:33: note: 'const SSL_METHOD* TLS_client_method()' previously defined here\r\n>  1869 | #define SSLv23_client_method    TLS_client_method\r\n>       |                                 ^~~~~~~~~~~~~~~~~\r\n> external/boringssl/src/ssl/tls_method.cc:269:19: note: in expansion of macro 'SSLv23_client_method'\r\n>   269 | const SSL_METHOD *SSLv23_client_method(void) {\r\n>       |                   ^~~~~~~~~~~~~~~~~~~~\r\n> In file included from external/boringssl/src/include/openssl/ex_data.h:112,\r\n>                  from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:64:\r\n> external/boringssl/src/include/openssl/base.h: In instantiation of 'void bssl::internal::Deleter<T>::operator()(T*) [with T = evp_pkey_st]':\r\n> /usr/lib/gcc/x86_64-redhat-linux/11/../../../../include/c++/11/bits/unique_ptr.h:361:17:   required from 'std::unique_ptr<_Tp, _Dp>::~unique_ptr() [with _Tp = evp_pkey_st; _Dp = bssl::internal::Deleter<evp_pkey_st>]'\r\n> external/boringssl/src/ssl/internal.h:2101:39:   required from here\r\n> external/boringssl/src/include/openssl/base.h:508:25: error: 'Free' is not a member of 'bssl::internal::DeleterImpl<evp_pkey_st, void>'\r\n>   508 |     DeleterImpl<T>::Free(ptr);\r\n>       |     ~~~~~~~~~~~~~~~~~~~~^~~~~\r\n> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,\r\n>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,\r\n>                  from external/boringssl/src/ssl/tls_method.cc:57:\r\n> external/boringssl/src/ssl/internal.h: In instantiation of 'void bssl::Delete(T*) [with T = bssl::DC]':\r\n> external/boringssl/src/ssl/internal.h:216:34:   required from 'static void bssl::internal::DeleterImpl<T, typename std::enable_if<T::kAllowUniquePtr>::type>::Free(T*) [with T = bssl::DC]'\r\n> external/boringssl/src/include/openssl/base.h:508:25:   required from 'void bssl::internal::Deleter<T>::operator()(T*) [with T = bssl::DC]'\r\n> /usr/lib/gcc/x86_64-redhat-linux/11/../../../../include/c++/11/bits/unique_ptr.h:361:17:   required from 'std::unique_ptr<_Tp, _Dp>::~unique_ptr() [with _Tp = bssl::DC; _Dp = bssl::internal::Deleter<bssl::DC>]'\r\n> external/boringssl/src/ssl/internal.h:2097:22:   required from here\r\n> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:20: error: 'OPENSSL_free' was not declared in this scope\r\n>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)\r\n>       |                    ^\r\n> external/boringssl/src/ssl/internal.h:207:5: note: in expansion of macro 'OPENSSL_free'\r\n>   207 |     OPENSSL_free(t);\r\n>       |     ^~~~~~~~~~~~\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 633.309s, Critical Path: 42.73s\r\n> INFO: 1201 processes: 71 internal, 1130 local.\r\n> FAILED: Build did NOT complete successfully\r\n> ", "comments": ["Ended up being the same problem as the last time the world ended.\r\nWhat a surprise. Why couldn't I just be allowed to be a pretty 30/40 something that was smart as well ?\r\nSeriously lets get on with our lives and work to fix things !", "oh, this is the build statement \r\n\r\ntensorflow-2.4.2]$ bazel build  --config=noaws --config=nogcp  --config=nohdfs  --config=nonccl  --verbose_failures tensorflow/tools/pip_package:build_pip_package \r\n\r\n", "Where the external code is concerned possible fix in comments here \n\nhttps://github.com/bazelbuild/bazel/issues/13613\n\nhttps://github.com/bazelbuild/bazel/issues/13635"]}, {"number": 50478, "title": "TensorFlow Lite no consistency between C++ and C library names", "body": "Question related with TensorFlow Lite. Library for C API is named `tensorflowlite_c`. Library for C++ API `tensorflow-lite`. What the logic behind? I think consistency should be between library names. For example:\r\n1. `tensorflow-lite-c` and `tensorflow-lite`.\r\n2. `tensorflow_lite_c` and `tensorflow_lite`.\r\n3. `tensorflowlite_c` and `tensorflowlite`.", "comments": []}, {"number": 50474, "title": "Adding a feature of \"Hard\" attention could be very useful", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.5\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs of now we can get probability distribution after attention but would be quite useful if there is a way to use hard attention instead of a soft attention\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nCould be quite useful while implementing ViT related architectures because using hard attention makes more sense for images instead of text \r\n**Any Other info.**\r\n", "comments": ["@rakshith291 \r\nCan you please share a use case to support your request.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Saduf2019 \r\nUse case : In case of dealing image patches as tokens in vision transformer it's quite helpful to have a hard attention where a given patch may have higher attention score with adjacent patch rather than all patches, hence hard attention could be quite useful", "@rakshit291 \r\n\r\nAre you referring to the ViT example of Keras[1]?\r\n\r\nThe network uses a MultiHeadAttention layer which outputs probabilities generated by a softmax[2]. \r\n\r\nBecause softmax preserves the ranking order, I think you may be able to insert a hardmax call after the output of MultiHeadAttention to achieve your desired effect. \r\n\r\n[1] https://keras.io/examples/vision/image_classification_with_vision_transformer/\r\n[2] https://keras.io/api/layers/attention_layers/multi_head_attention/\r\n"]}, {"number": 50463, "title": "TFLiteConverter Generates Quantized i64 bias that MLIR Quant dialect doesn't support", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): nightly\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nI use TFLiteConverter with post-quantization to generate int16 x int8 test, where the bias is int64.\r\nCurrent TFLiteConverter generates bias in quantized form:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantization_utils.cc#L684\r\n\r\nbut in MLIR Quant dialect the max storage bits can be up to 32:\r\nhttps://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Quant/QuantTypes.h#L57\r\n\r\n**Describe the expected behavior**\r\nN/A\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): Yes if applicable.\r\n- Briefly describe your candidate solution(if contributing):\r\n1. TFLite Converter generates non-quantized bias\r\n2. Loosen the restriction in MLIR Quant dialect\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nHere's the example MLIR (test.tflite.mlir) translated from a normal 16x8 conv2d .tflite:\r\n```\r\nmodule attributes {tfl.description = \"MLIR Converted.\", tfl.schema_version = 3 : i32}  {\r\n  func @main(%arg0: tensor<1x32x32x8x!quant.uniform<i16:f32, 6.1037018895149231E-5>>) -> tensor<1x32x32x16x!quant.uniform<i16:f32, 5.6973122991621494E-4>> attributes {tf.entry_function = {inputs = \"placeholder_0\", outputs = \"Identity\"}} {\r\n    %0 = \"tfl.pseudo_qconst\"() {qtype = tensor<16x1x1x8x!quant.uniform<i8<-127:127>:f32:0, {0.014107929542660713,0.015315329656004906,0.012964749708771706,0.015471030026674271,0.010782554745674133,0.01520597655326128,0.012333446182310581,0.013902087695896626,0.012822438031435013,0.014312445186078548,0.012691914103925228,0.015708990395069122,0.013873680494725704,0.014024644158780575,0.014720370061695576,0.0149689931422472}>>, value = dense<\"0x5CFE4C1F6C9E8106C4389D71E8CD6481CB81BC7C9AE67C6B44C58B87BF0FAA7F40CD91A0797F15BBF28844A881A9C422BD7A9C7F33D21EB47E344BCCFE8172075D26812D56EC747481BFBFCB7BC4453D811ECF3622BAD4EF06B9546FB757CF8192F01B818A7244ECDAFDE281DC33308487D2EC74AB8110D829F88281F248D011\"> : tensor<16x1x1x8xi8>} : () -> tensor<16x1x1x8x!quant.uniform<i8<-127:127>:f32:0, {0.014107929542660713,0.015315329656004906,0.012964749708771706,0.015471030026674271,0.010782554745674133,0.01520597655326128,0.012333446182310581,0.013902087695896626,0.012822438031435013,0.014312445186078548,0.012691914103925228,0.015708990395069122,0.013873680494725704,0.014024644158780575,0.014720370061695576,0.0149689931422472}>>\r\n    %1 = \"tfl.pseudo_qconst\"() {qtype = tensor<16x!quant.uniform<i64:f32:0, {8.6110594565980136E-7,9.348020739707863E-7,7.9132968267003889E-7,9.443055546398682E-7,6.581349794032576E-7,9.2812746288473135E-7,7.5279677957951208E-7,8.4854201531925355E-7,7.8264338299049996E-7,8.7358898781531025E-7,7.7467660730690113E-7,9.5882990081008757E-7,8.4680812051374232E-7,8.5602249555449816E-7,8.9848748530130251E-7,9.1366274546089698E-7}>>, value = dense<[1707424, 600679, -527282, -1446348, 2659806, 910839, 1976213, -434242, -2548601, 1757995, 44353, -1558553, 1132077, -1680781, 2215969, -1760827]> : tensor<16xi64>} : () -> tensor<16x!quant.uniform<i64:f32:0, {8.6110594565980136E-7,9.348020739707863E-7,7.9132968267003889E-7,9.443055546398682E-7,6.581349794032576E-7,9.2812746288473135E-7,7.5279677957951208E-7,8.4854201531925355E-7,7.8264338299049996E-7,8.7358898781531025E-7,7.7467660730690113E-7,9.5882990081008757E-7,8.4680812051374232E-7,8.5602249555449816E-7,8.9848748530130251E-7,9.1366274546089698E-7}>>\r\n    %2 = \"tfl.conv_2d\"(%arg0, %0, %1) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x32x32x8x!quant.uniform<i16:f32, 6.1037018895149231E-5>>, tensor<16x1x1x8x!quant.uniform<i8<-127:127>:f32:0, {0.014107929542660713,0.015315329656004906,0.012964749708771706,0.015471030026674271,0.010782554745674133,0.01520597655326128,0.012333446182310581,0.013902087695896626,0.012822438031435013,0.014312445186078548,0.012691914103925228,0.015708990395069122,0.013873680494725704,0.014024644158780575,0.014720370061695576,0.0149689931422472}>>, tensor<16x!quant.uniform<i64:f32:0, {8.6110594565980136E-7,9.348020739707863E-7,7.9132968267003889E-7,9.443055546398682E-7,6.581349794032576E-7,9.2812746288473135E-7,7.5279677957951208E-7,8.4854201531925355E-7,7.8264338299049996E-7,8.7358898781531025E-7,7.7467660730690113E-7,9.5882990081008757E-7,8.4680812051374232E-7,8.5602249555449816E-7,8.9848748530130251E-7,9.1366274546089698E-7}>>) -> tensor<1x32x32x16x!quant.uniform<i16:f32, 5.6973122991621494E-4>>\r\n    return %2 : tensor<1x32x32x16x!quant.uniform<i16:f32, 5.6973122991621494E-4>>\r\n  }\r\n}\r\n```\r\nAnd you can see the problem when you read this with tf-opt, even without specifying any transformation:\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n$ bazel-bin/tensorflow/compiler/mlir/tf-opt test.tflite.mlir\r\ntest.tflite.mlir:4:66: error: illegal storage type size: 64\r\n    %1 = \"tfl.pseudo_qconst\"() {qtype = tensor<16x!quant.uniform<i64:f32:0, {9.4795944960424094E-7,9.1321464879001724E-7,9.4685782414671848E-7,8.5114061221247539E-7,8.9218377752331434E-7,7.9372739492100663E-7,6.839824209237122E-7,9.5105099262582371E-7,6.5581531316638575E-7,6.6347195115668001E-7,8.5451210907194763E-7,9.0817928821707027E-7,9.1536480795184616E-7,8.8241159801327739E-7,8.0079468034455203E-7,9.1778696287292405E-7}>>, value = dense<0> : tensor<16xi64>} : () -> tensor<16x!quant.uniform<i64:f32:0, {9.4795944960424094E-7,9.1321464879001724E-7,9.4685782414671848E-7,8.5114061221247539E-7,8.9218377752331434E-7,7.9372739492100663E-7,6.839824209237122E-7,9.5105099262582371E-7,6.5581531316638575E-7,6.6347195115668001E-7,8.5451210907194763E-7,9.0817928821707027E-7,9.1536480795184616E-7,8.8241159801327739E-7,8.0079468034455203E-7,9.1778696287292405E-7}>>\r\n\r\n```", "comments": []}, {"number": 50458, "title": "C API does not appear to register GPU XLA platform correctly", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:       not mobile device.\r\n-   **TensorFlow installed from (source or binary)**: both\r\n-   **TensorFlow version (use command below)**: 2.4.1, 2.5.0, master branch(as of 6/24)\r\n        ---tested in standard 2.5.0 libtensorflow tarball as well as rebuilt source\r\n-   **Python version**: C_API bug\r\n-   **Bazel version (if compiling from source)**: 3.7.2\r\n            == bazel version  ===============================================\r\n            Build label: 3.7.2- (@non-git)\r\n            Build time: Mon May 24 14:27:25 2021 (1621866445)\r\n            Build timestamp: 1621866445\r\n            Build timestamp as int: 1621866445\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n        gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\n-   **CUDA/cuDNN version**:\r\n       11.0, libcudnn.so.8.0.5\r\n-   **GPU model and memory**:\r\n        Nvidia V100 16G\r\n\r\n\r\n### Describe the problem\r\nThe XLA_GPU ops do not appear to be registered correctly in any recent version of libtensorflow.so.  This includes the standard 2.5.0 tarball, the tf-nightly as of this morning as well as my own compiles from 2.4.1 (patched) 2.5.0 and the master branch as of a day ago.\r\nThe enclosed program demonstrates the problem.  Running it with a saved-model in the current directory\r\ncalled \"saved_model\" results in a failure with the error messages enclosed below.  This problem\r\nis not affected by any environment variable settings.  setting \"TF_XLA_FLAGS=--tf_xla_enable_xla_devices\"\r\nwill result in the XLA_CPU device being registered, but not the XLA_GPU device.\r\nIn my environment all versions of tf-python register XLA ops correctly and do XLA just fine.\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n    ```c++\r\n    #include \"tensorflow/c/c_api.h\"\r\n    #include \"tensorflow/c/c_api_experimental.h\"\r\n    #include <iostream>\r\n\r\n    main()\r\n    {\r\n        auto sessionOpts = TF_NewSessionOptions();\r\n        auto graph = TF_NewGraph();\r\n        auto meta_graph_def = TF_NewBuffer();\r\n        auto status = TF_NewStatus();\r\n        const char *tags[] = {\"serve\"};\r\n\r\n        TF_EnableXLACompilation(sessionOpts, 1u);   // All other methods of doing this also seem to fail\r\n\r\n        auto session = TF_LoadSessionFromSavedModel(\r\n            sessionOpts, /* RunOptions */ nullptr,\r\n            \"saved_model\", tags,  1, graph, meta_graph_def, status);\r\n\r\n        TF_DeleteSession(session, status);\r\n        TF_DeleteSessionOptions(sessionOpts);\r\n        TF_DeleteStatus(status);\r\n        TF_DeleteBuffer(meta_graph_def);\r\n        TF_DeleteGraph(graph);\r\n\r\n        return 0;\r\n    }\r\n    ```\r\n\r\n**Other info / logs** \r\n\r\n2021-06-25 06:40:17.652978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-25 06:40:17.715316: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: saved_model\r\n2021-06-25 06:40:18.241233: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\r\n2021-06-25 06:40:18.241281: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: saved_model\r\n2021-06-25 06:40:18.241348: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-25 06:40:18.244220: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-06-25 06:40:18.245683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-06-25 06:40:18.335973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:0f:00.0 name: Tesla V100-DGXS-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2021-06-25 06:40:18.336004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-25 06:40:18.339161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-06-25 06:40:18.339198: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-06-25 06:40:18.341082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-06-25 06:40:18.341400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-06-25 06:40:18.343734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-06-25 06:40:18.344506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-06-25 06:40:18.344662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-06-25 06:40:18.349429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-06-25 06:40:18.349466: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-06-25 06:40:18.926592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-06-25 06:40:18.926630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-06-25 06:40:18.926642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-06-25 06:40:18.930171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14783 MB memory) -> physical GPU (device: 0, name: Tesla V100-DGXS-16GB, pci bus id: 0000:0f:00.0, compute capability: 7.0)\r\n2021-06-25 06:40:20.699787: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n2021-06-25 06:40:20.971245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2198735000 Hz\r\n2021-06-25 06:40:24.264247: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_ops.cc:406 : Not found: could not find registered platform with id: 0x7f9b890fdb28\r\n2021-06-25 06:40:24.273317: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: fail: Not found: 2 root error(s) found.\r\n  (0) Not found: could not find registered platform with id: 0x7f9b890fdb28\r\n         [[{{node cluster_0_1/xla_compile}}]]\r\n         [[cluster_0_1/data_as_ctrl/_2]]\r\n  (1) Not found: could not find registered platform with id: 0x7f9b890fdb28\r\n         [[{{node cluster_0_1/xla_compile}}]]\r\n0 successful operations.\r\n0 derived errors ignored.. Took 6558004 microseconds.\r\n\r\n```", "comments": ["I should add that I can get this program to work if I link against _pywrap_tensorflow_internal.so -lpython*.  It is clearly an issue of how the libtensorflow.so library is built.", "@cbquillen \r\nCould you please try the tested build configurations for TF2.5\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.5.0 | 3.6-3.9 | GCC 7.3.1 | Bazel 3.7.2 | 8.1 | 11.2\r\n\r\nalso refer this [link1](https://github.com/tensorflow/tensorflow/issues/45044), [link2](https://stackoverflow.com/questions/65907365/tensorflow-not-creating-xla-devices-tf-xla-enable-xla-devices-not-set)\r\n\r\nThanks", "I ran again using the stock 2.5.0 docker image (which I believe satisfies your requirement table).  I linked against the standard released libtensorflow.so.2.5.0.  It fails in exactly the same way.\r\n\r\nI note again that this problem goes away if I link against _pywrap_tensorflow_internal.so + -lpython3.   The libtensorflow.so library simply isn't built correctly.", "I've ran into this as well when trying to enable ConcreteFunction JIT in tensorflow/java, although I didn't run it down.  Setting the `_XlaMustCompile` and `_noinline` attributes to `true` on a function will result in this error.", "Is there any movement on this? Is someone planning to fix this?\r\n\r\nIt cripples the C API for any model that requires XLA for speed, and has broken my project. Currently using @cbquillen's workaround but that is not suitable for production.", "@mihaimaruseac: Is there something concerned parties can do to help you get permission/bandwidth to work this issue, or give to another? This is a pretty egregious error."]}, {"number": 50449, "title": "Load in-cluster config in tf.distribute.cluster_resolver.KubernetesClusterResolver", "body": "\r\n**System information**\r\n- TensorFlow version (use command below): `2.5.0`\r\n\r\n**Describe the current behavior**\r\n\r\n  Currently the `KubernetesClusterResolver` class can only load configs from kube-config file:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py#L97\r\n\r\n**Describe the expected behavior**\r\n\r\n  When running in worker pods in cluster, we want to be able to load configs using `.load_incluster_config()` ([example](https://github.com/kubernetes-client/python/blob/v17.17.0/examples/in_cluster_config.py#L55))\r\n\r\nWe can't use the `override_client` arg with `kubernetes` package either, because once installed the `ImportError` does not happen \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n\r\n  Yes\r\n\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n  Try to do `k8sconfig.load_incluster_config()` first, if failed then do `k8sconfig.load_kube_config()`\r\n\r\n", "comments": []}, {"number": 50443, "title": "OpenImageDebugger ", "body": "Can be somehow TensorFlow used with OpenImageDebugger (Linux platform)? Is there any example of how to use it together for debugging purposes?", "comments": []}, {"number": 50440, "title": "tf.profiler.experimental.Trace only works with name is \"TraceContext\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1080ti\r\n\r\n**Describe the current behavior**\r\nThe name argument to `Trace` has to be exactly `\"TraceContext\"` otherwise the \"overview\" page doesn't give any info. Interesting the other pages still work and have all the right data. But the title page is broken, so most users wouldn't think to look at the other pages and just assume the whole thing was broken.\r\n\r\n**Describe the expected behavior**\r\nIt works with any name\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom datetime import datetime\r\nimport tensorflow as tf\r\n\r\nb = tf.random.uniform([32,28,28,1])\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\ndef test(trace_name):\r\n    logs = f\"logs/{trace_name}\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\n    epoch = 0\r\n    tf.profiler.experimental.start(logs)\r\n    for step in range(100):\r\n        if 1 < step < 100:\r\n            with tf.profiler.experimental.Trace(trace_name):\r\n                model(b)\r\n        else:\r\n            model(b)\r\n    tf.profiler.experimental.stop()\r\n\r\ntest(\"TraceContext\") # works fine\r\ntest(\"Broken\") # no step marker observed error\r\n```\r\n\r\n**Other info / logs**\r\n![trace_context](https://user-images.githubusercontent.com/4010770/123279273-b4b7d080-d4d5-11eb-9e75-e91cff2982cc.png)\r\n![broken](https://user-images.githubusercontent.com/4010770/123279286-b6819400-d4d5-11eb-8515-8b5998658a00.png)\r\n\r\n", "comments": ["@PeterMitrano \r\n\r\nIn order to expedite the trouble-shooting process,Could you please share the colab gist with all the dependencies .Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have reproduce the issue in colab:\r\n\r\nhttps://colab.research.google.com/drive/18bLYBL6hAX8tNgxJ21qGAu81Hvt2h8Sy?usp=sharing", "what permissions does the notebook need? I just set it to \"anyone with link can view\"\r\n", "@Saduf2019 \r\n\r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/drive/1O_9FDa584VYd-CcRjrKl86uc397VzglL?resourcekey=0-H6gHS07dZuDaa8wODjRASw).Thanks", "Can confirm this is happening for me also with Tensorflow 2.6.0.\r\n\r\nAnd it's not only `TraceContext` that works as a name here. `SessionRun`, `FunctionRun`, `RunGraph` and other host events also work. These are defined in:\r\nhttps://github.com/tensorflow/tensorflow/blob/ed4d2472d287cbab529fa6000f24f7ccc623f81f/tensorflow/core/profiler/utils/xplane_schema.cc#L68", "Follow-up: the keras code uses another argument `_r=1`, which apparently allows using any name and still getting the steps: \r\nhttps://github.com/tensorflow/tensorflow/blob/760f9230e9a2fb578f798b05968442fdb551191b/tensorflow/python/keras/engine/training.py#L1182-L1187\r\n\r\nExample that works:\r\n```\r\nwith tf.profiler.experimental.Trace(\"trace_name\", _r=1):\r\n    train_step(b)\r\n```", "The support for `TraceContext` is for legacy traces. Please follow [this](https://www.tensorflow.org/guide/profiler#profiling_custom_training_loops) for collecting new traces."]}]