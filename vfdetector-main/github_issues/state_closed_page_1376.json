[{"number": 11787, "title": "Unexpected behavior in tf.scatter_update", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.3\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CPU\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**:\r\n```\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n>>> dim = 10\r\n>>> probs = tf.Variable(tf.ones(dim), trainable=False)\r\n>>> dist = tf.contrib.distributions.Categorical(probs=probs)\r\n>>> mask = tf.Variable(tf.ones(dim), trainable=False)\r\n>>> b = dist.sample([2])\r\n>>> mask = tf.scatter_update(mask, b, [3,3])\r\n>>> sess.run(tf.global_variables_initializer())\r\n>>> print(mask.eval(session=sess))\r\n[ 1.  1.  3.  3.  1.  1.  1.  1.  1.  1.]  # first call\r\n>>> c = dist.sample([3])\r\n>>> mask = tf.scatter_update(mask, c, [5,5,5])\r\n>>> print(mask.eval(session=sess))\r\n[ 3.  1.  3.  5.  3.  5.  5.  1.  1.  1.]  # second call\r\n```\r\n\r\n### Describe the problem\r\nI am trying to update `mask` at indices sampled by `dist`, and I need past updates to persist as I add more updates to mask. That is, I would expect the second call to be `[ 1.  1.  3.  5.  1.  5.  5.  1.  1.  1.]`. Since `tf.scatter_update` mutates `mask`, I would expect the updates to persist, but it seems like the the updates are made anew every time I call `mask.eval()`. \r\n\r\nFurthermore, perhaps I'm missing something, but the output of the second call is further unexpected in the sense that I see three 3's in `mask`, even when I never assigned three 3's to mask, only two. Is there an explanation for this? \r\n\r\nIf this is not a bug, then could we add a feature to allow for consistent and persistent updates?\r\n\r\n### Source code / logs\r\nPlease see above.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nAnyway, to answer the question, the issue is that in the line\r\n```python\r\nmask = tf.scatter_update(mask, b, [3,3])\r\n```\r\nyou set `mask` to the result of an update op. After that line, whenever you evaluate `mask`, you do another `scatter_update` to it, which sets more elements to 3. Later, the line\r\n```python\r\nmask = tf.scatter_update(mask, c, [5,5,5])\r\n````\r\napplies another update on top of the original update. So, after that point, whenever mask is evaluated, it does two updates: one which sets two 3s, and another which sets three 5s. The first `mask.eval(...)` only applies the original update, and the second applies both updates. Therefore, a total of four 3s and three 5s are set (some updates may overwrite others).\r\n\r\nTo apply each update once, try removing the first `mask.eval(...)`. Alternatively, do not assign mask to the return value of `tf.scatter_update`, but instead do something like\r\n```python\r\nupdate1 = tf.scatter_update(mask, b, [3,3])\r\nsess.run(tf.global_variables_initializer())\r\nprint(update1.eval(session=sess))\r\nc = dist.sample([3])\r\nupdate2 = tf.scatter_update(mask, c, [5,5,5])\r\nprint(update2.eval(session=sess))\r\n```"]}, {"number": 11786, "title": "contrib.data.Dataset - doc issue with Dataset.map / tf.py_func in 1.3.0rc0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: 1.3.0rc0\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 8/6\r\n- **GPU model and memory**: GTX 1080\r\n- **Exact command to reproduce**:\r\n\r\nThe following sample is taken from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md) and works in TF 1.2.1 \r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef _read_py_function(filename, label):\r\n  return np.zeros((100,100,1)), label\r\n\r\ndef _resize_function(image_decoded, label):\r\n  image_decoded.set_shape([None, None, None])\r\n  image_resized = tf.image.resize_images(image_decoded, [28, 28])\r\n  return image_resized, label\r\n\r\nfilenames = np.array([\"/var/data/image1.jpg\", \"/var/data/image2.jpg\"])\r\nlabels = np.array([0, 37])\r\n\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices((filenames, labels))\r\ndataset = dataset.map(\r\n    lambda filename, label: tf.py_func(\r\n        _read_py_function, [filename, label], [tf.uint8, label.dtype]))\r\ndataset = dataset.map(_resize_function)\r\n```\r\nIn 1.3.0rc0 the following error is produced\r\n\r\n```\r\nCannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'uint8'> (Tensor is: <tf.Tensor 'PyFunc:1' shape=<unknown> dtype=int32>)\r\n```\r\n\r\nThis is due to the breaking change mentioned in [release notes](https://github.com/tensorflow/tensorflow/blob/r1.3/RELEASE.md). To fix, one now has to introduce an explicit `tuple()` like so\r\n\r\n```python\r\ndataset = dataset.map(\r\n    lambda filename, label: tuple(tf.py_func(\r\n        _read_py_function, [filename, label], [tf.uint8, label.dtype])))\r\n```\r\nThis should at least be mentioned in the API docs / programmer guide.\r\n", "comments": ["@mrry, could you update the documentation to reflect this change?", "Thanks @cheind for reporting this.", "@mrry You're welcome. While this is certainly a doc issue at this point, I want to raise the concern that assigning `tuple` and `list`  totally different semantics in Python is very uncommon (even in Tensorflow) and could lead to many suprising moments on user side. ", "I have a similar error message, but I'm not sure if I got the proposed solution correctly.\r\n The code raising the error:\r\n```\r\ndataset = tf.contrib.data.TFRecordDataset(shard_files)\r\ndataset = dataset.map(partial(decoder.decode, items=['label', 'image']))\r\n```\r\nwhere `decoder` is a `tf.contrib.slim.tfexample_decoder.TFExampleDecoder()`\r\n\r\nSo, if I got the issue right, since `decoder.decode()` returns a **list** `[label, decoded_image_data]` this is implicitly casted to a tensor (and thus the cast fails because `label` and `image_data` have different types).\r\nHowever, if I write a lambda that wraps tuple() around the result of the call to `decoder.decode()`, this should fix the problem:\r\n\r\n```\r\ndataset = dataset.map( lambda s : tuple(decoder.decode(s, items=['label', 'image'])))\r\n```\r\n~~I still get the error, however:~~\r\n\r\n> ~~TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'float32'> to <dtype: 'int64'> (Tensor is: <tf.Tensor 'distort_image/Mul:0' shape=(224, 224, 3) dtype=float32>)~~\r\n\r\nAm I getting the solution wrong? Is this *really* the intended way to use the API? Even when many other TF APIs return lists instead of tuples?\r\n\r\nEDIT: I had map() in several places and, of course, the new error was coming from the line below the one I fixed.", "@GPhilo We've fixed this issue in the internal branch, and it should appear at HEAD soon. (Follow #12396 to see when the commit lands.) After the fix, the behavior of `Dataset.map()` will be the same if a function returns a tuple or a list containing the same elements, and the `tuple(...)` workaround will no longer be necessary.", "@mrry sounds great!", "TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int32'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Preprocessor/stack_1:0' shape=(1, 3) dtype=int32>)", "@tamizharasank This issue has been closed. If you think you've found a bug, please open a new issue with enough details about your program to reproduce the problem. Otherwise, Stack Overflow may be a more useful venue for your question."]}, {"number": 11785, "title": "proposed new github for tensorflow notebooks", "body": "I would like to propose that we make a separate repository in github for jupyter notebooks.\r\n\r\nI think it should be separate from the main tensorflow github so that people can check it out without having to check out all of tensorflow.\r\n\r\nI've created such a github and will work on it by myself for now but I think eventually there should be something more official or at least maybe what I have can be considered official enough to have pointers to it from the main tensorflow github.\r\n\r\nhttps://github.com/reedkotler/tensorflow-notebooks\r\n", "comments": ["@wolffg, I would be concerned about having divergent copies of the same tutorials. Has there been any progress on what we discussed a few months ago about being able to download a jupyter notebook from any tutorial page (i.e. a dynamically concerted notebook form of any doc page). That would be nice, and it wouldn't have a possibility of out of sync. It would also allow us to unit test our tutorials easily to make sure they didn't get stale.", "I agree with same code/git base.\r\n\r\nHaving notebooks unittested would be great. That probably would solve python version differences on ie https://www.tensorflow.org/get_started/* too.\r\n\r\n@wolffg where is that discussion documented?\r\n\r\n> download a jupyter notebook from any tutorial page (i.e. a dynamically concerted notebook form of any doc page).\r\n\r\nIs a link to raw version not enough?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi.  We're working on an experiment with notebooks for tensorflow.org pages.  It's still in the planning stage, but we are actively looking into it.  I'm going to close this, as there is some internal tooling we need to write, and tracking it here is not going to be helpful.\r\n\r\nThanks for your suggestion!"]}, {"number": 11784, "title": "[docs] Windows Instructions need updating.", "body": "Any way to contribute to the installation instructions for Windows? I deal with a lot of people having installation instructions on Stack Overflow etc. and there are a couple of things on the Install pages that could do with being corrected, clarified or changed. Happy to do these myself if you'd like. I'll list them below:\r\n\r\n- _\"TensorFlow only supports version 3.5.x of Python on Windows.\"_ is no longer true since 1.2 as 3.6 is supported.\r\n\r\n- The r1.3 version of the install page introduces this new line: _\"TensorFlow will not load if it cannot find cuDNN64_5.dll. To use a different version of cuDNN, you must build from source.\"_ however I thought that 1.3 was built with cuDNN 6. Users may find this conflict confusing because, as I found with #11645, 1.3.0 will NOT load with cuDNN64_5.dll. Shouldn't this change to `cuDNN64_6.dll`?\r\n\r\n- Anaconda Installation slightly misleading? I use anaconda to run our tensorflow environments without a problem. I download via `pip install tensorflow(-gpu)` instead of the unsupported `conda` version and, from a user's point of view, there's no reason for it be unsupported. is there a reason that a pypi installation put in an anaconda environment would act differently enough to virtual env/root install to be unsupported? I also don't see why the installation page can't recommend downloading straight from pypi instead of using the long `storage.googleapis...` url. Especially as 3.6 is now supported.\r\n\r\nHappy to implement these changes if I knew where. Are the markdown files in docs_src linked to the website? If so i'll pop a PR into there.\r\n\r\nCheers", "comments": ["@wolffg, could you update the documentation?", "Yes, we generate the website from docs_src.\r\n\r\nAssigning to @gunan for install docs update.\r\n\r\n", "@av8ramit Could you update documentations on the release branch and master branch?", "Hate to be that guy but the current installation instructions still say that you should have cuDNN 5.1 installed then linking to 1.3 binaries which I believe are built with cuDNN 6 which means users won't be able to successfully install. @gunan @av8ramit Unless something has changed where cuDNN 5.1 WILL work. I've not personally tried it since rc1 but I assume that hasn't changed as the release notes suggest otherwise. I've already helped a user today try to get up and running because of these errors.\r\n\r\nCheers.", "@mrry Am I correct in saying that you submitted a PR for this and it's just not showing on the website? If so I can close the issue.", "@jubjamie Yes, #12383 made the change, but I don't know what it takes to get the website to rebuild. (Perhaps @wolffg would know, or know the right person to ask?)", "Ok. I'll leave the issue open to track at least. I know it's not a huge problem but it's caught a lot of people out! If you are tracking this internally however feel free to close. Cheers", "We will republish the website later today.", "In master, linux says 6 but windows still says 5.1.  In r1.3, Windows\ncontains 5.1 -> 6 transformation, but Linux is still at 5.1.\n\nDo we need all four of these docs to say 6?\n\nOn Thu, Aug 24, 2017 at 10:58 AM, gunan <notifications@github.com> wrote:\n\n> We will republish the website later today.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11784#issuecomment-324710316>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1YDu0Jk7mtTip5TchEU2lDQh29oyjLks5sbbm8gaJpZM4Oj8bu>\n> .\n>\n", "All four should say 6.\nAmit is working on fixing all of these.\n\nOn Thu, Aug 24, 2017 at 1:12 PM, Wolff Dobson <notifications@github.com>\nwrote:\n\n> In master, linux says 6 but windows still says 5.1. In r1.3, Windows\n> contains 5.1 -> 6 transformation, but Linux is still at 5.1.\n>\n> Do we need all four of these docs to say 6?\n>\n> On Thu, Aug 24, 2017 at 10:58 AM, gunan <notifications@github.com> wrote:\n>\n> > We will republish the website later today.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 11784#issuecomment-324710316>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AA1YDu0Jk7mtTip5TchEU2lDQh29oyjLks5sbbm8gaJpZM4Oj8bu>\n> > .\n>\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11784#issuecomment-324743577>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOTbrz49PQgXEeIRK14q85M6nc41uks5sbdkTgaJpZM4Oj8bu>\n> .\n>\n", "Ran into this today installing tensorflow-gpu and even worse: the NVIDIA cudnn archive page doesn't have version 6 anymore: https://developer.nvidia.com/rdp/cudnn-archive", "i am very sorry about hear that,my cudnn is shared from baiducloud,so,Do U have a baidu account.\nthis is my share url: https://pan.baidu.com/s/1o8zCUz4 \u5bc6\u7801: 3k7y\n\nAt 2017-08-25 10:43:18, \"Kevin Watters\" <notifications@github.com> wrote:\n\n\nRan into this today installing tensorflow-gpu and even worse: the NVIDIA cudnn archive page doesn't have version 6 anymore: https://developer.nvidia.com/rdp/cudnn-archive\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.", "@WangChengLongWilliam I get \"Oh oh, the page you are visiting does not exist\" at that link.", "ok ,i try to send the file to you email\n\nAt 2017-08-25 11:59:25, \"Kevin Watters\" <notifications@github.com> wrote:\n\n\n@WangChengLongWilliam I get \"Oh oh, the page you are visiting does not exist\" at that link.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.", "@kwvinw Not sure what that page is, but AFAIK cudnn is distributed through here, with a few terms and conditions. \r\nhttps://developer.nvidia.com/cudnn\r\nI cannot log in and check, but can anyone see if this site includes cudnn v6 after logging in?\r\nOur installation instructions still should point to this site.", "false alarm: looks like they brought back the 6.0 link. I had emailed them about it.", "https://developer.nvidia.com/rdp/cudnn-download      has cuDNN 6 it is not on the archive page. kevinw already said this above but adding the link so people are not on the archive page wondering where it is. ", "@gunan This looks like it has mostly been resolved apart from the inclusion of Python 3.6 but that's not as bad of an issue as people being unable to run the binaries. Thanks for sorting it, i'm sure it will help lots of people in the long run. Closing.", "@jubjamie Good catch! @av8ramit can we also add python 3.6 support in windows docs?", "Done. https://github.com/tensorflow/tensorflow/pull/12687", "Hi, \r\n\r\nFrom version 1.4,  TF will depend on scipy, then the user should either have a FORTRAN compiler or get it from a third-party python distribution (e.g. Anaconda).  We need document this."]}, {"number": 11783, "title": "tf.estimate quickstart", "body": "I am in the process of making jupyter notebooks from the tensorflow documentation.\r\n( https://github.com/reedkotler/tensorflow-notebooks )\r\n\r\nThe tensorflow.org page on tf.esimator quickstart (https://www.tensorflow.org/get_started/tflearn)  had numerous problems. \r\n\r\nI've straightened out the coding issues and the fixes are in my notebook https://github.com/reedkotler/tensorflow-notebooks/blob/master/get_started/estimator/estimator.ipynb\r\n\r\nI still have some more markup text to straighten out.\r\n\r\nAide from the python2/3, tensoflow 1.2, etc. issues, the actual test case was not predicting properly. I made my own simple one and it predicted just fine so I did not spend time trying to figure out whether the original was wrong or not.\r\n\r\nI'm not volunteering to fix the main tensorflow markdown because I'm against the idea of having so much code there that can't be tested and is constantly breaking because tensorflow is changing and so I don't want to try and fix that with no way to test it and to give people something bulletproof that will work. There are lots of posts going unanswered in stackoverflow for 6 months of more on many of these pages, and for this scikit learn one especially because people know that already and it's an obvious starting place for newbies.\r\n\r\n", "comments": ["The Iris test cases for training/test worked fine, just the fake \"new samples\" was not predicting as the web page expected. But my three new samples were all classified correctly.", "I'm not quite sure what the problem is. The tutorials aren't designed to be converted to jupyter notebooks, so there will be issues to work out when converting a tutorial to one.", "There are some samples that were not in the training or test set. In the original comments, the two samples are supposed to be iris's of type 1,2 but it predicts to two of type 1. When I make up sample data by looking at the real data, it predicts correctly. But all the self checking from the test and training set show the same predicted accuracy as mentioned in the comments and it's near 100%.", "Good catch! Running the tutorial code, I sometimes get the class predictions being [1, 2] (as claimed in the tutorial) but sometimes they are [1, 1].\r\n\r\nI think this issue was already fixed (perhaps unintentionally). When I run the [tutorial code in the repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md), I always get [1, 2] as claimed (I ran 100 times). Once the tensorflow.org is updated for TensorFlow 1.3, the tensorflow.org tutorial should be fixed."]}, {"number": 11782, "title": "CUDA_ERROR_LAUNCH_FAILED when using conv2d/max_pool on Tensorflow GPU (Windows 10)", "body": "When using conv2d and/or max_pool, the error below shows and stops the code. I used the code available here: [https://github.com/dennybritz/cnn-text-classification-tf](https://github.com/dennybritz/cnn-text-classification-tf)\r\n\r\n```\r\n2017-07-26 21:41:27.467585: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1068] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED\r\n2017-07-26 21:41:27.467797: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:54] Internal: error destroying CUDA event in context 000001178B37E810: CUDA_ERROR_LAUNCH_FAILED\r\n2017-07-26 21:41:27.468679: E c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_timer.cc:59] Internal: error destroying CUDA event in context 000001178B37E810: CUDA_ERROR_LAUNCH_FAILED\r\n2017-07-26 21:41:27.469846: F c:\\tf_jenkins\\home\\workspace\\release-win\\m\\windows-gpu\\py\\35\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:2479] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED\r\n```\r\n\r\n**What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?**\r\n\r\nI have searched through all websites including GitHub and StackOverflow, and these two are the most relevant results and why they did not help:\r\n\r\n1. [https://github.com/tensorflow/tensorflow/issues/6783](https://github.com/tensorflow/tensorflow/issues/6783) - The issue pertains to tf.one_hot. Is this function used in either one of the functions above? The issue seems to be not solved too. I tried using the code on a Linux/GPU and on a Windows/CPU setting as well and it worked.\r\n2. [https://github.com/dennybritz/cnn-text-classification-tf/issues/90](https://github.com/dennybritz/cnn-text-classification-tf/issues/90) - They kind of 'resolved' the issue by reducing the batch size. I tried this and it worked, but it does not make sense. For one thing, I tried using the code on another Windows machine with an older GPU and it works well.\r\n\r\n**Environment info**\r\n\r\n- Tensorflow 1.2.1\r\n- Python 3.5.3\r\n- CUDA 8.0\r\n- cuDNN 5.1\r\n- OS: Windows 10\r\n- GPU: GeForce GTX 1060\r\n\r\n[EDITED] edited the links", "comments": ["The link you provided does not work for me. Could you provide a minimal reproducible test case? Are other models working? Could it be that your driver / cuda toolkit need updating?", "Hey. I forgot to include the links themselves. They should be fixed now.\r\n\r\nOther models (such as LSTMs, etc.) are working fine. The [original theano version](https://github.com/yoonkim/CNN_sentence) of the code works well too. I have my CUDA and cuDNN updated to the latest version (as specified)\r\n\r\nA reproducible test case would be:\r\n\r\n```\r\ninput_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\r\n\r\nembedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -0.1, 0.1), name=\"embedding\")\r\nembedded_chars = tf.nn.embedding_lookup(embedding, input_x)\r\nembedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\r\n\r\nW = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\r\nb = tf.Variable(tf.constant(0.0, shape=[num_filters]), \"b\")\r\nconv = tf.nn.conv2d(\r\n                embedded_chars_expanded,\r\n                W,\r\n                strides=[1, 1, 1, 1],\r\n                padding=\"VALID\",\r\n                name=\"conv\")\r\nh = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\r\npooled = tf.nn.max_pool(\r\n                h,\r\n                ksize=[1, sequence_length-filter_size+1, 1, 1],\r\n                strides=[1, 1, 1, 1],\r\n                padding=\"VALID\",\r\n                name=\"pool\")\r\n```\r\n\r\nHowever, I cannot seem to reproduce this issue on other machines. Although [some people](https://github.com/dennybritz/cnn-text-classification-tf/issues/90) experienced this error before too.", "If you try it in other machines, do those other machines have the same gpu and driver versions. You might swap the working card from another machine to this one. Otherwise, I don't really have any other ideas. Perhaps @mrry, knows something I don't.\r\n", "I am working on a laptop (MSI GS73VR), so swapping the GPU card may be impossible..\r\n\r\nThe other machines I have tested have the same driver versions, but with different (older) GPUs.", "Oh wait, I just saw the Tensorflow 1.3.0 release with cuDNN 6 support. I upgraded TF and cuDNN to the latest versions and solved the issue.\r\n\r\nThanks!"]}, {"number": 11781, "title": "run errors ", "body": "Hi,when I run python for tensorflow ,my python file show errors ,and can't run success\u3002errors likes:\r\n\r\n\r\n017-07-26 19:57:14.274316: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\r\n your machine and could speed up CPU computations.\r\n2017-07-26 19:57:14.372830: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-26 19:57:14.372864: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n/home/ah0818lijhong/CNN-kereas/cnn-kereas.py:155: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\r\n  merged = Merge([model_left, model_right,model_3], mode='concat')\r\nEpoch 1/3\r\nTraceback (most recent call last):\r\n  File \"/home/ah0818lijhong/CNN-kereas/cnn-kereas.py\", line 167, in <module>\r\n    model.fit(x_train, y_train,epochs=3)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 845, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 1485, in fit\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/engine/training.py\", line 1140, in _fit_loop\r\n    outs = f(ins_batch)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 2073, in __call__\r\n    feed_dict=feed_dict)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,868] = 115873 is not in [0, 20001)\r\n         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_1/embeddi\r\nngs/read, _recv_embedding_1_input_0)]]\r\nCaused by op u'embedding_1/Gather', defined at:\r\n  File \"/home/ah0818lijhong/CNN-kereas/cnn-kereas.py\", line 122, in <module>\r\n    model_left.add(embedding_layer)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/models.py\", line 422, in add\r\n    layer(x)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py\", line 554, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/layers/embeddings.py\", line 119, in call\r\n    out = K.gather(self.embeddings, inputs)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 966, in gather\r\n    return tf.gather(reference, indices)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1207, in gather\r\n    validate_indices=validate_indices, name=name)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/ah0818lijhong/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\nInvalidArgumentError (see above for traceback): indices[0,868] = 115873 is not in [0, 20001)\r\n         [[Node: embedding_1/Gather = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_1/embeddi\r\nngs/read, _recv_embedding_1_input_0)]]\r\n\r\n\r\n\r\nI hope someone can fix it .thank you.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nAlso, when posting on StackOverflow, please post the commands you ran that caused the issue.", "ok."]}, {"number": 11780, "title": " Check failed: sss_[idx]->Get(key, &value) Failed to seek to the record for tensor", "body": "hi, \r\n I am so appreciated with your great job. \r\nhere is my question:\r\nwhen i finetune the resnet_v1_50 with pretrained model.   I have exclude the logists layer, when comes to \r\nthe code 'saver.restore(sess, checkpoint_path)', it gives me the error like this:\r\n![image](https://user-images.githubusercontent.com/9246739/28619251-144e03be-723a-11e7-8612-26c2cde56b01.png)\r\ncan you help me?", "comments": ["does it work if you don't exclude logits before loading the checkpoint? if so, load the checkpoint but ignore that logits layer. However, this is more of a stackoverflow question."]}, {"number": 11779, "title": "Fix GradientDescentOptimizer argument", "body": "Both the text (line 370) and python code example example code https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_softmax.py#L59 uses 0.5. \r\n\r\nUsing 0.05 keeps success percentage around 90% instead of the mentioned 92%", "comments": ["Can one of the admins verify this patch?", "I checked the wrong version branch ... this is fixed in master but not yet deployed by 8c868fa"]}, {"number": 11778, "title": "Bug in ctc_ops.py 's documentation", "body": "see in \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/ctc_ops.py#L201\r\nit says the inputs is logits, but from the op test https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/ctc_decoder_ops_test.py#L111 \r\n we can see the input should be log probability .\r\nAlthough seems this does not affect the decoder's output, but it affects the decoder's return of path's log_prob, and the discribe of neg_sum_logits is confusing.\r\nI think the document should be changed, the inputs should be log of probabilities and the output should be -log(p), maybe we can transform the output to p with tf.exp(-p), i think probability output is more comfortable\r\n\r\nand confusing people from another issue #6034", "comments": ["@ebrevdo, please take a look,as it seems you have touched this file quite a bit.", "The op does a normalization in log space, so logits are an acceptable input.  In the unit test you could shift the log probs by any constant value and the output and gradients would still be correct."]}, {"number": 11777, "title": "No OpKernel was registered to support Op 'Ceil' on Android", "body": "I'm running tensorflow on android. And it reports a exception of one of my op 'Ceil'. The exception info is as below:\r\n`Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Ceil' with these attrs.  \r\nRegistered devices: [CPU], Registered kernels:  <no registered kernels>\r\n[[Node: model/frame/Ceil = Ceil[T=DT_DOUBLE](model/frame/truediv)]]\r\n   at org.tensorflow.Session.run(Native Method)`\r\n\r\nI believe Ceil is a basic op and it should has CPU implement. So maybe I miss sth. ?   ( I clone tensorflow master branch and build the lib and java interface on that)", "comments": ["@petewarden can you take a look? I never touched Android or Java side of TensorFlow, but it doesn't seem like there's anything special about `Ceil` that would cause it not to work", "This is most likely due to double not being supported by ops by default on mobile TF. You can try converting your double ops to float, or building with selective registration/adding it manually to [types.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.cc#L240)", "@liuzqt Try including cwise_op_ceil.cc in tensorflow/core/kernels/BUILD file under filegroup \"android_extended_ops_group1\" and rebuild..", "@andrewharp: Casting the argument of tf.ceil to float32 does not help. ", "@anandcu3 \r\n\r\nIt works! My steps are like this:\r\n1. Clone tensorflow1.4.0\r\n2. including cwise_op_ceil.cc in tensorflow/core/kernels/BUILD file under filegroup \"android_extended_ops_group1\"\r\n3. including \"tensorflow/core/kernels/cwise_op_ceil.cc\" in \"tensorflow/contrib/makefile/tf_op_files.txt\"\r\n4. Import projects \"tensorflow/examples/android\" using Android Studio.\r\n5. Modify build.gradle:  def nativeBuildSystem = 'bazel'  --> \"makefile\"\r\n6. Rebuild the android projects.\r\n\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@liuzqt Is this still an issue for you?  Have you tried the suggestions by @andrewharp and @rjlee0 above?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Is there any solution to this using cmake on windows? I'm hitting the exact same problem when trying to launch the demo detect app with my frozen_inference_graph\r\n\r\nEdit: What's interesting: I exported a model which was trained with fixed_shape_resizer and this works. As soon as I use the model which was trained with keep_aspect_ratio_resizer I get the stacktrace + crash of this issue.\r\n\r\nEdit2: @tatatodd please reopen", "> @anandcu3\r\n> \r\n> It works! My steps are like this:\r\n> \r\n>     1. Clone tensorflow1.4.0\r\n> \r\n>     2. including cwise_op_ceil.cc in tensorflow/core/kernels/BUILD file under filegroup \"android_extended_ops_group1\"\r\n> \r\n>     3. including \"tensorflow/core/kernels/cwise_op_ceil.cc\" in \"tensorflow/contrib/makefile/tf_op_files.txt\"\r\n> \r\n>     4. Import projects \"tensorflow/examples/android\" using Android Studio.\r\n> \r\n>     5. Modify build.gradle:  def nativeBuildSystem = 'bazel'  --> \"makefile\"\r\n> \r\n>     6. Rebuild the android projects.\r\nHello, I recently transplanted the tensorflow project to Android. When I transplanted tf-detection, I encountered the same problem as you. When I transplanted the detect-model  to Android, I encounter the same error. I still don't understand your steps. I still can't run through Android at present. Can we contact by email?: 1304020120@qq.com, thank you very much.\r\n\r\n"]}, {"number": 11776, "title": "//py_test_dir/tensorflow/python:bitwise_ops_test failing on Windows", "body": "http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/32/console\r\n```\r\nERROR: testPopulationCountOp (__main__.BitwiseOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\tmp\\Bazel.runfiles_5m16karn\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\ops\\bitwise_ops_test.py\", line 66, in testPopulationCountOp\r\n    inputs = np.array(raw_inputs, dtype=dtype.as_numpy_dtype)\r\nOverflowError: Python int too large to convert to C long\r\n```\r\nCulprit: cl/163090921 ", "comments": ["@ebrevdo what do you recommend about the failure?", "Disable the test on Windows.", "I'll try to really fix when I return.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Closing since it's been a while. Please reopen if this still occurs with the latest version.", "@drpngx  still happens on windows and tensorflow 2.2 (tensorflow 2.3 and nightly have other issues on windows that prevents me from upgrading, and test this)"]}, {"number": 11775, "title": "Fix ./configure on Windows", "body": "Make `configure` and `configure.py` work on Windows.\r\nFix http://ci.tensorflow.org/job/tf-master-win-bzl/1300/console\r\n@yifeif @gunan ", "comments": ["Testing at http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/32/console", "Build is fixed, and caught an test failure:\r\nhttps://github.com/tensorflow/tensorflow/issues/11776", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 11774, "title": "app crashed on android device of api level = 19 when load library \"libtensorflow_inference.so\"", "body": "### System information\r\n- Os version 4.4.4\r\n-sdk level 19\r\n\r\n### logcat:\r\n07-26 15:01:59.429 6521-6521/? E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                 Process: al.omid.tfdroid, PID: 6521\r\n                                                 java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"rand\" referenced by \"libtensorflow_inference.so\"...\r\n                                                     at java.lang.Runtime.loadLibrary(Runtime.java:364)\r\n                                                     at java.lang.System.loadLibrary(System.java:526)\r\n                                                     at al.omid.tfdroid.MainActivity.<clinit>(MainActivity.java:28)\r\n                                                     at java.lang.Class.newInstanceImpl(Native Method)\r\n                                                     at java.lang.Class.newInstance(Class.java:1208)\r\n                                                     at android.app.Instrumentation.newActivity(Instrumentation.java:1061)\r\n                                                     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2114)\r\n                                                     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2257)\r\n                                                     at android.app.ActivityThread.access$800(ActivityThread.java:143)\r\n                                                     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1209)\r\n                                                     at android.os.Handler.dispatchMessage(Handler.java:102)\r\n                                                     at android.os.Looper.loop(Looper.java:136)\r\n                                                     at android.app.ActivityThread.main(ActivityThread.java:5120)\r\n                                                     at java.lang.reflect.Method.invokeNative(Native Method)\r\n                                                     at java.lang.reflect.Method.invoke(Method.java:515)\r\n                                                     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:818)\r\n                                                     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:634)\r\n                                                     at dalvik.system.NativeStart.main(Native Method)\r\n07-26 15:02:01.179 6521-6521/al.omid.tfdroid W/System.err:     at com.android.internal.os.RuntimeInit$UncaughtHandler.uncaughtException(RuntimeInit.java:94)\r\n\r\n\r\n### Describe the problem\r\nCould  you pleas tell me how can I solve this ?  Is \"libtensorflow_inference.so\" usable for android devices of api level < 20 ?", "comments": ["Are you using only the library or are you trying to use the demo app? IIRC the demo app uses Camera2 so needs >21 but the actual library only requires >14.\r\n\r\n[Perhaps this comment chain](https://github.com/tensorflow/tensorflow/issues/6763#issuecomment-272529963) from @andrewharp on #6763 will help you.", "I only used the library. I guess it's because I used an old version library file.  It runs ok now when I use library of version r1.2. Thank you very much ~  \r\n\r\nBTW, I want to ask another question: I see some android example using tensorflow. For example, mnist and de demo app of tensorflow classifier,  They all need to write some c code and generate a new .so file containing both c code written by developer and tensorflow library so.\r\n\r\n Is it necessary to write c code and generate a custom .so file when implement a complicate deep learning task ? \r\n\r\n And how can I build my own c code together with tensorflow library .so file to generate a new one ?\r\n\r\nHope for your answer . Thank you so much ~", "Ah that was going to be my next question because your issue appeared to be fixed in a later release! I've not got around to writing my own android libraries. That might not necessarily be required to create your own program. A lot of it can be done in android/java itself and you can use the inference library for when it's needed. I don't think it contains every op however so there might be some extra work to add in something which you need. \r\n\r\nPerhaps you can find some more information here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android\r\n\r\nI'll let someone else answer your questions properly as I don't have the experience. Good luck!", "@scucheri: @jubjamie is correct, you should be able to accomplish almost any inference task (i.e. non-training) using the Java API and TensorFlowInferenceInterface.java similarily to the examples in the Android demo. You can use the prebuilt Android TF binaries in almost all cases unless you need to add new custom ops.\r\n\r\nFor further support on non-bug/feature requests we suggest referring your questions to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) as there is a larger community there. Thanks!", "Note that https://github.com/tensorflow/tensorflow/pull/10771, which adds camera 1 support, should enable the demo to run on devices < API 21.", "Thank you ~\r\n \r\nI had post this question here : [StackOverFlow](https://stackoverflow.com/questions/45340713/is-it-necessary-to-write-c-code-and-generate-a-custom-so-file-when-use-tensorfl/45340732#45340732)", "i run into the same issue last week , and it has been solved . \r\nany question about cross compile tensor to mobile can contact me, i'm willing to help others cause google members and other guys always helps me.\r\nhere's my way for solving this issue:\r\n\r\ntarget:\r\n i'm using c++ interface and cross compile to ios \\ android \\ linux \\ ...  , and let the app call the interface. android is using jni to call c++ interface. \r\n\r\nsteps:\r\n\r\n1  for android 4.4, we need ndk r12b and api level 14 , my android colleague tells  me that 19 is also for android 4.4, well i didn't try it, i tested api level 14 and it works for android 4.4 \\ 6.0 \\ 7.0\r\n\r\n2  modify the Makefile file in folder contrib/makefile:\r\n        modify:  --sysroot $(NDK_ROOT)/platforms/android-21/arch-arm \\\r\n        to :         --sysroot $(NDK_ROOT)/platforms/android-14/arch-arm \\\r\n        \r\n3  modify the compile_android_protobuf.sh file in folder contrib/makefile:\r\n        modify:  android_api_version=\"${ANDROID_API_VERSION:-21}\"\r\n        to :         android_api_version=\"${ANDROID_API_VERSION:-14}\"\r\n \r\n4 run build_all_android.sh to compile tensorflow and protobuf(which depended by tensorflow).\r\n   also see https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile for detail \r\n\r\n5 get the .h file and .so file .\r\n\r\nin my circumstance, i packeted tensorflow lib to my own lib project , pls aware that my own lib set api level to 19, my own lib also can set to 14.  Theoretically libs in level 19 can call libs in level 14, and 14 cannot call 19.  And also aware that api level above 19 is not support android 4.4. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Hi @jakiechris , how about to build the aar by using docker? I'm trying to use lower NDK version but getting some error like\r\n```\r\nERROR: /tmpfs/bazel_output/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/androidndk/BUILD.bazel:43:18: Traceback (most recent call last):\r\n\tFile \"/tmpfs/bazel_output/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/androidndk/BUILD.bazel\", line 41\r\n\t\tcc_toolchain_suite(<2 more arguments>)\r\n\tFile \"/tmpfs/bazel_output/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/androidndk/BUILD.bazel\", line 43, in cc_toolchain_suite\r\n\t\t{\"arm64-v8a\": \":aarch64-linux-android-4.9-gnu-libstdcpp\", \"arm64-v8a|gcc-4.9\": \":aarch64-linux-android-4.9-gnu-libstdcpp\", \"arm64-v8a\": \":aarch64-linux-android-clang3.8-gnu-libstdcpp\", \"arm64-v8a|clang3.8\": \":aarch64-linux-android-clang3.8-gnu-libstdcpp\", \"armeabi\": \":arm-linux-androideabi-4.9-gnu-libstdcpp\", \"armeabi|gcc-4.9\": \":arm-linux-androideabi-4.9-gnu-libstdcpp\", \"armeabi-v7a\": \":arm-linux-androideabi-4.9-v7a-gnu-libstdcpp\", \"armeabi-v7a|gcc-4.9\": \":arm-linux-androideabi-4.9-v7a-gnu-libstdcpp\", \"armeabi\": \":arm-linux-androideabi-clang3.8-gnu-libstdcpp\", \"armeabi|clang3.8\": \":arm-linux-androideabi-clang3.8-gnu-libstdcpp\", \"armeabi-v7a\": \":arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp\", \"armeabi-v7a|clang3.8\": \":arm-linux-androideabi-clang3.8-v7a-gnu-libstdcpp\", \"mips64\": \":mips64el-linux-android-4.9-gnu-libstdcpp\", \"mips64|gcc-4.9\": \":mips64el-linux-android-4.9-gnu-libstdcpp\", \"mips64\": \":mips64el-linux-android-clang3.8-gnu-libstdcpp\", \"mips64|clang3.8\": \":mips64el-linux-android-clang3.8-gnu-libstdcpp\", \"mips\": \":mipsel-linux-android-4.9-gnu-libstdcpp\", \"mips|gcc-4.9\": \":mipsel-linux-android-4.9-gnu-libstdcpp\", \"mips\": \":mipsel-linux-android-clang3.8-gnu-libstdcpp\", \"mips|clang3.8\": \":mipsel-linux-android-clang3.8-gnu-libstdcpp\", \"x86\": \":x86-4.9-gnu-libstdcpp\", \"x86|gcc-4.9\": \":x86-4.9-gnu-libstdcpp\", \"x86\": \":x86-clang3.8-gnu-libstdcpp\", \"x86|clang3.8\": \":x86-clang3.8-gnu-libstdcpp\", \"x86_64\": \":x86_64-4.9-gnu-libstdcpp\", \"x86_64|gcc-4.9\": \":x86_64-4.9-gnu-libstdcpp\", \"x86_64\": \":x86_64-clang3.8-gnu-libstdcpp\", \"x86_64|clang3.8\": \":x86_64-clang3.8-gnu-libstdcpp\"}\r\nDuplicated key \"arm64-v8a\" when creating dictionary\r\n```\r\nHere is the configuration i used\r\n```\r\nENV ANDROID_API_LEVEL 14\r\nENV ANDROID_NDK_API_LEVEL 12\r\nENV ANDROID_NDK_FILENAME android-ndk-r12b-linux-x86_64.zip\r\n```\r\nthe rest is just the same as in [here](https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile)"]}, {"number": 11773, "title": "Enable Android lib bazel build for mips and mips64", "body": "Currently the Android .so lib bazel build is failing for `--cpu=mips` and `--cpu=mips64`. This PR is fixing this.\r\n\r\nThe changes are:\r\n\r\n- Add \"-latomic\" linkopt for `--cpu=mips`,  this is to avoid failing at the end for `undefined reference to '__atomic_*'` alike errors.\r\n- Remove `-Os` optimization flag for `--cpu=mips` and `--cpu=mips64`, this is to avoid failing in compiling `tensorflow/core/lib/core/threadpool.o` for `failure memory model cannot be stronger than success memory model for '__atomic_compare_exchange'` errors.\r\n- Added some hints in `tensorflow/workspace.bzl` for API level selection when compiling 64-bit lib.\r\n\r\nBy checking online, seems there is a bug in gcc compiler (at least the gcc inside NDK r12b, I did try using newer NDK version, however since r13b, the default compiler is changed to clang, and bazel is only working with clang with NDK r13b or higher, as well as currently, compiling Tensorflow with clang is failing for some strange errors to me, so it did not work), that in some cases, `-Os` flag would crash the compiling. Removing `-Os` for `--cpu=mips` and `--cpu=mips64` is only a sub-optimal solution as the generated .so lib will be relatively bigger (but not much).", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11772, "title": "Feature request: have optimizers handle complex tensors", "body": "When a tensor that contains complex values reaches an optimizer, it is cast to real.\r\nThis means that the net will not learn features that depend on the imaginary part of the tensor.\r\n\r\nWhile we wait for the possible feature, is there a workaround that you recommend? I'm thinking of separating the real and imaginary parts before feeding the optimizer (like, turning a complex tensor [C] into [R,I]), would that work?).", "comments": ["That sounds like it would work fine to me.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11771, "title": "Occur fatal error 1002 when compiling tensorflow 1.3 with vs2015 DEBUG in windows10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n      no custom code(original code cloned from github )\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n      Windows 10\r\n- **TensorFlow installed from (source or binary)**:\r\n      source(cmake from tensorflow/contribe/cmake)\r\n- **TensorFlow version (use command below)**:\r\n      r1.3\r\n- **Visual studio version**: \r\n      vs2015\r\n- **CUDA/cuDNN version**:\r\n      no GPU/CUDA 8.0+cuDNN5.1\r\n- **GPU model and memory**:\r\n      NVIDIA Titan X(12GB)\r\n- **Exact command to reproduce**:\r\n      tf_cc.vcxproj -> A:\\C++\\tensorflow-1.3.0\\Source_GPU\\tf_cc.dir\\Debug\\tf_cc.lib\r\n      65>a:\\c++\\tensorflow-1.3.0\\source_gpu\\external\\eigen_archive\\eigen\\src\\core\\products\\generalblockpanelkernel.h(1977): fatal error C1002:\r\n\r\n### Describe the problem\r\nHello, I get error 1002 when compiling tensorflow r1.3 with vs2015 in tf_core_kernels , use DEBUG mode without GPU, which occured during compilation of GPU version too. I belive I have enough memory for compilation. However, I compiled successfully through RELEASE mode without error.\r\n\r\nSame issue in tensorflow-r1.2.", "comments": ["Can you post the entire error output that occurs?", "I believe debug build is not supported, especially for gpu build. You may try to replace /DEBUG:Full with /DEBUG:Fastlink in your compiler flags. And you may also need to split tf_core_kernerls into multiple static libraries. And you need a debug build of python,cuda,... If I were you, I'll give up", "@reedwm the error output have just 2 sentences, which I have written above. I would post a part of build log for you\r\n1>  count_extremely_random_stats_op.cc\r\n1>  finished_nodes_op.cc\r\n1>  grow_tree_op.cc\r\n1>  reinterpret_string_to_float_op.cc\r\n1>  sample_inputs_op.cc\r\n1>  scatter_add_ndim_op.cc\r\n1>  tree_predictions_op.cc\r\n1>  tree_utils.cc\r\n1>  update_fertile_slots_op.cc\r\n1>  hard_routing_function_op.cc\r\n1>  k_feature_gradient_op.cc\r\n1>  k_feature_routing_function_op.cc\r\n1>  routing_function_op.cc\r\n1>  routing_gradient_op.cc\r\n1>  stochastic_hard_routing_function_op.cc\r\n1>  stochastic_hard_routing_gradient_op.cc\r\n1>  unpack_path_op.cc\r\n1>  utils.cc\r\n1>  skip_gram_kernels.cc\r\n1>  skip_gram_ops.cc\r\n1>  cross_replica_ops.cc\r\n1>  infeed_ops.cc\r\n1>  outfeed_ops.cc\r\n1>  replication_ops.cc\r\n1>  tpu_configuration_ops.cc\r\n1>  tpu_sendrecv_ops.cc\r\n1>a:\\c++\\tensorflow-1.3.0\\source_gpu\\external\\eigen_archive\\eigen\\src\\core\\products\\generalblockpanelkernel.h(1989): fatal error C1002: \u5728\u7b2c 2 \u904d\u4e2d\u7f16\u8bd1\u5668\u7684\u5806\u7a7a\u95f4\u4e0d\u8db3(compiler is out of heap space in pass 2)\r\n1>cl : \u547d\u4ee4\u884c error D8040: \u521b\u5efa\u5b50\u8fdb\u7a0b\u6216\u4e0e\u5b50\u8fdb\u7a0b\u901a\u8baf\u65f6\u51fa\u9519(error creating or communicating with child process)\r\n========== \u751f\u6210: \u6210\u529f(success) 0 \u4e2a\uff0c\u5931\u8d25 (failure)1 \u4e2a\uff0c\u6700\u65b0 0 \u4e2a\uff0c\u8df3\u8fc7 0 \u4e2a ==========\r\nsorry for using the chinese version, I have translated the output in english", "@snnn I agree with you, there are some problem during compiling in Debug mode now. So I post the error for later development of tensorflow.\r\nIt seems like there have some problem in MinSizeRel and RelWithDebInfo mode, because i cannot compile tf in these modes either.", "Did you use the 32 bits cl.exe or 64 bits cl.exe?\r\n\r\n```\r\nset PreferredToolArchitecture=x64\r\n```\r\nRun this command before open tensorflow.sln\r\n\r\nOr, add \"-T host=x64\" to cmake command line args.\r\n\r\n", "@snnn Thank you for your suggestion! Because compile is time-consuming, I would tell you the result later.\r\n**Anthor question**: I found it is necessary to compile a tensorflow project statically in vs, which caused the result bloat. For example, project 'tf_label_image_example' is 164 MB in release mode, but there only one file named 'main.cc' from 'examples\\label_image' in this project. Is there any way to compile it dynamiclly to squeeze the file size?", "/CC @mrry", "@snnn OK, your suggestion works, thank you!", "I found it is easy to build tensorflow project dynamically in vs 2015 for tf-r1.3. Close the issue.", "@HannH  I have the same problem , which solution is useful to you ? Is the \"set PreferredToolArchitecture=x64\"?  I try to add \"-T host=x64\" to cmake command line args. but another error happened . Can I have your qq or wechat number? "]}, {"number": 11770, "title": "tf.contrib.data.Dataset filter documentation broken", "body": "With the old input pipeline functions, in tf.train.batch() you could specify the \"allow_smaller_final_batch\" parameter, which would allow or disallow a smaller final batch. With the new input pipeline functions in tf.contrib.data, the batch function allows a smaller final batch by default, and (to the best of my knowledge) there is no way to skip this last half-batch to ensure all batch sizes are equal.\r\n\r\nIs there a possibility that a \"allow_smaller_final_batch\" flag could be added to the new batch function?", "comments": ["My guess is that this is more cleanly accomplished using the filter() method on Dataset, where the predicate is one that checks that tf.shape(value)[batch_dim] == batch_size.  So something like\r\n\r\ndataset = .. your dataset\r\ndataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))\r\n\r\nand then your iterator will only yield values where the Tensor matches the expected batch size.\r\n\r\n(If this satisfies your use case, feel free to close this issue).", "Thanks a bunch @vrv. That does exactly what I wanted it too, and much neater than just catching an exception and not training on that specific batch.\r\n\r\nIt might be worth bringing this up though: The reason I wasn't able to find the filter function is because the file located [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/data/python/ops/dataset_ops.py) is missing a ``` to end the code example (on line 632), and this has included the filter function in the Enumerate sections code, rendering it unsearchable. [See here](https://www.tensorflow.org/api_docs/python/tf/contrib/data/TFRecordDataset#enumerate)\r\n\r\nIs this something that can be fixed?\r\n\r\nThanks again for the help.\r\n", "Definitely, I'll retitle the bug accordingly!  I'll also point you towards the following, which is also a helpful reference:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md", "Hey, this specific bug has been fixed by the deprecation of enumerate() to enumerate_dataset()\r\n\r\n(see here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/enumerate_ops.py)\r\n\r\nSo I'm closing this out unless there's a reason to keep it open?"]}, {"number": 11769, "title": "Issue for tensorboard --logdir=save/", "body": "### System information\r\n`tensorboard --logdir=save/` this command occurred some issue.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X with AMD.\r\n- **TensorFlow installed from (source or binary)**: pip3.6\r\n- **TensorFlow version (use command below)**: The latest version\r\n- **Python version**: 3.6\r\n- **Memory**: 16G\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/tensorboard/tensorboard.py\", line 32, in <module>\r\n    from tensorflow.python.summary import event_file_inspector as efi\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/summary/event_file_inspector.py\", line 122, in <module>\r\n    from tensorflow.python.platform import gfile\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/platform/gfile.py\", line 22, in <module>\r\n    from tensorflow.python.lib.io.file_io import copy as Copy\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 27, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so, 2): Symbol not found: _PyBytes_AsString\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so\r\n  Expected in: flat namespace\r\n in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so\r\n```\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" = `v1.2.0-5-g435cdfc 1.2.1`", "comments": ["You should file this under the TensorBoard repository here: https://github.com/tensorflow/tensorboard/issues", "thx"]}, {"number": 11768, "title": "Upgrade gRPC", "body": "Just Jenkins testing for now.", "comments": ["This is ready for review. The only test failures are the same failures that are in https://github.com/tensorflow/tensorflow/pull/11789.\r\n\r\nReasons for some changes:\r\n- protobuf -> protobuf_archive because //external:protobuf is now depended on in grpc. That needs to be a bind rule, unfortunately.\r\n- grpc.patch and the grpc cmake files are only needed until these pull requests are submitted:\r\nhttps://github.com/grpc/grpc/pull/11944\r\nhttps://github.com/grpc/grpc/pull/11933\r\n\r\nOnce those are submitted, then we can switch to the BUILD and cmake files that are in the grpc repo.\r\n- A bunch of classes were renamed. See b/62910646", "The cmake file is a direct copy of the grpc one with a single line deletion (patching isn't so easy in vanilla cmake).", "Also thanks to @llhe for his work on the older version of this change that we couldn't merge because of Windows issues.", "Forgot to mention:\r\n- bazel clean is needed in the CI scripts due to the protobuf -> protobuf_archive rename. We should probably leave them in for a week or so and then remove them.", "Let me know if you feel confident merging this now.", "This CL broke sandboxed builds due to missing dependencies.\r\nSample error message:\r\n#include <grpc++/impl/codegen/proto_utils.h>\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nERROR: .../github/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:225:1: Couldn't build file tensorflow/core/distributed_runtime/rpc/_objs/grpc_master_service_impl/tensorflow/core/distributed_runtime/rpc/grpc_master_service_impl.pic.o: C++ compilation of rule '//tensorflow/core/distributed_runtime/rpc:grpc_master_service_impl' failed (Exit 1)"]}, {"number": 11767, "title": "Branch 163121296", "body": "", "comments": ["@tensorflow-jenkins test this please ?", "Existing failures are flakes of some kind, merging."]}, {"number": 11766, "title": "should tf.contrib.data.FixedLengthRecordDataset work with GS?", "body": "Trying to open a GS file (gs://path/), with tf.contrib.data.FixedLengthRecordDataset, I get: tensorflow/core/framework/op_kernel.cc:1158] Out of range: EOF reached, 0 bytes were read out of 262144 bytes requested.\r\n\r\n\r\n", "comments": ["@jhseu, are you the right person to answer this question?", "It looks like Dataset might have an issue accessing resources on GCS (google cloud storage).  Derek might also be the right person or even Asim might be able to identify someone, he might have inherited the API.  \r\n", "@mari-linhares  I know this is bit of work but I would test with head from master and make sure it is still broken.  There also likely needs to be a unit test assuming something is broken so we catch this in the future.", "Just got this same error kind of randomly in the middle of a training session. Same situation, opening a file via gs:// with FixedLengthRecordDataset during a training job on google cloud ml. Here's the full trace if that helps:\r\n\r\n`Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/train/task.py\", line 136, in <module> sess.run(optimize) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 789, in run run_metadata_ptr) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 997, in _run feed_dict_string, options, run_metadata) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1132, in _do_run target_list, options, run_metadata) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _do_call raise type(e)(node_def, op, message) OutOfRangeError: EOF reached, 0 bytes were read out of 262144 bytes requested. [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[-1,1024,1], [-1,1024]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]] [[Node: sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/num_invalid_dims/_43 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_577_sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/num_invalid_dims\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]] Caused by op u'IteratorGetNext', defined at: File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/train/task.py\", line 93, in <module> x_train, y_train = iter_data_train.get_next() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 247, in get_next name=name)) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 254, in iterator_get_next output_shapes=output_shapes, name=name) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op op_def=op_def) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op original_op=self._default_original_op, op_def=op_def) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__ self._traceback = _extract_stack() OutOfRangeError (see above for traceback): EOF reached, 0 bytes were read out of 262144 bytes requested. [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[-1,1024,1], [-1,1024]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]] [[Node: sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/num_invalid_dims/_43 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_577_sigmoid_cross_entropy_loss/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/num_invalid_dims\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]`\r\n\r\nNot able to reproduce it though. Could have just been a network hiccup?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "We think this bug is related to other GCS issues we fixed recently. It should work better in TF 1.5."]}, {"number": 11765, "title": "cond with gradients of map_fn hangs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: `('v1.2.0-5-g435cdfc', '1.2.1')`\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0 / 5.1.5\r\n- **GPU model and memory**: GeForce GTX 1080, 8113MiB\r\n- **Exact command to reproduce**: `python test.py` (see below)\r\n\r\n### Describe the problem\r\nRunning a `cond` in which a lambda (even the one that isn't used) includes the `gradients` of a tensor made by `map_fn` makes TensorFlow hang. See test.py below for reproduction.\r\n\r\nDoing the `gradients` outside of the lambda makes it work. See test-workaround.py for example.\r\n\r\n### Source code / logs\r\ntest.py:\r\n```py\r\nimport tensorflow as tf\r\n\r\nprint 'start'\r\nnumbers = tf.zeros([2], dtype=tf.float32)\r\n\r\nresult = tf.map_fn(lambda image: image, numbers)\r\n\r\ndef get_next_step():\r\n  objective = result[0]\r\n  grads, = tf.gradients(objective, numbers)\r\n  return numbers + grads\r\n\r\ndef current_or_next(use_next):\r\n  return tf.cond(use_next,\r\n                 get_next_step,\r\n                 lambda: numbers)\r\n\r\nalways_old = current_or_next(use_next=tf.constant(False))\r\n\r\nprint 'creating session'\r\nsess = tf.Session()\r\nprint 'before run'\r\nsess.run(always_old)\r\nprint 'after run'\r\n```\r\n\r\ntest-workaround.py:\r\n```py\r\nimport tensorflow as tf\r\n\r\nprint 'start'\r\nnumbers = tf.zeros([2], dtype=tf.float32)\r\n\r\nresult = tf.map_fn(lambda image: image, numbers)\r\n\r\ndef get_next_step():\r\n  objective = result[0]\r\n  grads, = tf.gradients(objective, numbers)\r\n  return numbers + grads\r\nnext_step = get_next_step() # <--\r\n\r\ndef current_or_next(use_next):\r\n  return tf.cond(use_next,\r\n                 lambda: next_step, # <--\r\n                 lambda: numbers)\r\n\r\nalways_old = current_or_next(use_next=tf.constant(False))\r\n\r\nprint 'creating session'\r\nsess = tf.Session()\r\nprint 'before run'\r\nsess.run(always_old)\r\nprint 'after run'\r\n```\r\n\r\nI haven't been able to get a traceback after it hangs.", "comments": ["@skye can you look into this issue?", "Hi, I was on vacation, but will take a look at this soon.", "Nagging Assignee @skye: It has been 183 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 198 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@skye Hi Skye, did you get a chance to look into this ?", "@wh0 This problem should not exist with the latest Tensorflow version. Please try with the latest tensorflow  version using the compatible CUDA/cudNN versions([find here](https://www.tensorflow.org/install/source#tested_build_configurations)) and post your observations here. \r\nAlso, feel free to close this issue if it no longer exists.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 11764, "title": "Minor corrections to tensordot documentation", "body": "Changed some math indices formating and the format of the `axes` argument for Example 3.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11763, "title": "Minor corrections to tensordot documentation.", "body": "_edit: was trying to merge against wrong branch. Will open another PR._\r\n\r\nChanged some math indices formating and the format of the `axes` argument for Example 3.", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?"]}, {"number": 11762, "title": "Cherrypicks", "body": "", "comments": ["Jenkins, test this please.", "GPU tests can be ignored since the tag has changed."]}, {"number": 11761, "title": "404 for mac GPU", "body": "Installation section in readme for Mac GPU: 404 error\r\n\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-mac-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-mac/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0rc0-py2-none-any.whl ", "comments": ["Can you clarify? Where did you get that link?", "Not OP. But the Mac GPU Links are in the Installation section of the README.md", "Ah, thank you for clarifying.\r\n\r\n@av8ramit, can you check why the Mac GPU links are broken?", "Unfortunately, we stopped Mac GPU support after the 1.1 release. The build itself is broken as well and we have disabled it. We are accepting patches for Mac GPU, but no longer offer any official support. Sorry about that.\r\n\r\nAs for the links in the README, thanks for pointing that out! I've created a PR: https://github.com/tensorflow/tensorflow/pull/11790", "@av8ramit  That's really unfortunate \u2013 i recently got myself a 1080 Ti and thought that with the 11 GB VRAM i can finally run all the stuff my older card couldn't lift. And i have just discovered Tensorboard ... Now i can only:\r\n1) Downgrade >= 1.2 TF only functions/code back to 1.1 (won't work for long \ud83d\ude2c)\r\n2) Search for implementations of a TF project in Keras (Theano), pyTorch/Torch etc.\r\n3) Rewrite TF only projects from scratch with another framework\r\n4) Put the GPU in a separate Machine with Linux ...\r\n5) Sell my body and/or organs so i can afford a 128 Core Xeon Machine\r\n\r\nI currently have a working 1.1 GPU install \u2013 can i just make a copy of all files in my python `site-packages/tensorflow`  folder if i want to try out Tensorflow >=1.2 (CPU only ofc) and then revert back?\r\n\r\nHave you seen if someone has managed to build TF 1.2 with GPU support from source? What is the biggest obstacle/time consuming issue that forced the TF team to drop Mac GPU support?", "I compiled TF 1.2 with GPU support for my mac pro. It runs fine so far. R1.3 is not really out yet, and it is a bit broken right now for macs. Biggest issue seems compiler compatibility. llvm/clang 9 is not really getting along with cuda 8 yet and some other stuff seem to be broken too.", "@rafaelsimonmaia Cool! Could you point out what problems i could face when building from source \u2013 how to avoid getting stuck in the process? What did you need to install or change that you could compile it successfully?", "@subzerofun Perhaps you wanna give a try https://github.com/rafaelsimonmaia/tensorflow_for_macpro/blob/master/Compute%206.1/tensorflow-1.2.1-cp35-cp35m-macosx_10_7_x86_64.whl\r\nIt is for python 3 and Compute 6.1, so it should work on your machine. \r\n\r\nTo compile, I downgraded my clang to Apple LLVM version 8.0.0 (clang-800.0.42.1), which is found on the Command Line Tools for Xcode 8.2. Then I updated Cuda to 8, and got the latest cuDNN 6 as well. I configure and said no to everything, save for the CUDA option.  Then I removed -lgomp from the `\"third_party/gpus/cuda/BUILD.tpl\"` file, as it doesn't seem to be needed for linking. Then `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`  and `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`. \r\n\r\nI even got the current master to work as well, but for that I had to edit one variant.h file and remove a constexpr from a construction, as clang was breaking at that point. I may make a PR later to address that, if I find some time.", "@rafaelsimonmaia Thanks for the links! Unfortunately there is only a python 3 install in the Compute 6.1 folder. Do you maybe have build a .whl file for python 2.7 too? I have two cards with Compute Abilities: **3.5** (gtx 780) and **6.1** (gtx 1080 ti). So unfortunately i can't use your python 2.7 file (it's for Compute 5.2).\r\n\r\nDo you maybe also have a working .whl file for TF 1.3 (and some instructions how to prepare other files and configurations) ?\r\n\r\nI'm currently still on 1.1 (which works fine on most projects) with GPU support. Would love to upgrade (if it was as easy as before...).", "@subzerofun I was able to compile master with removing constexpr in variant.h\r\n``` class Variant {\r\n  public:\r\n    #constexpr Variant() noexcept = default;\r\n    Variant() noexcept = default;\r\n```\r\nIt looks working, but I'm wondering if it's broke something?"]}, {"number": 11760, "title": "Update bigquery_reader_ops.py", "body": "Fix example code in comments by replacing tf.training.string_input_producer with tf.train.string_input_producer", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "You have to git amend your commit to use the email you signed the CLA with.  \r\n\r\nhttps://patch-diff.githubusercontent.com/raw/tensorflow/tensorflow/pull/11760.patch shows that it is using the default github one, so you'll have to update it.  Either send a new PR for it or git commit --amend and then force push!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 11759, "title": "Format 1.3.0 release notes; added a new feature to tfdbg section", "body": "", "comments": []}, {"number": 11758, "title": "Unexpected behavior in tf.contrib.distributions.Categorical", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.3\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CPU\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**:\r\n```\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n>>> t = tf.contrib.distributions.Categorical(probs=[0.0,0.0,0.0])\r\n>>> print(sess.run(t.sample([1])))\r\n>>> [3]  # outside the support of the distribution!\r\n```\r\n\r\n### Describe the problem\r\nI wonder if this is a bug or if this is a design choice to have `tf.contrib.distributions.Categorical` output a number outside the support of the distribution if all of the probabilities are 0. In any case, it would be helpful to throw an error when all probabilities are 0, rather than output a number that doesn't work.\r\n\r\n### Source code / logs\r\nSee above.\r\n", "comments": ["@ebrevdo, could you take a look?", "I think there's a construction attribute that enables runtime checks. What happens if you turn it on?", "When I turn it on, I get the error:\r\n```\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session()\r\n>>> t = tf.contrib.distributions.Categorical(probs=[0.0,0.0,0.0], validate_args=True)\r\n>>> print(sess.run(t.sample([1])))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"[...]/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [probs does not sum to 1.] [Condition x ~= y did not hold element-wise: x = ] [Categorical_2/Categorical/validate_probs/Sum:0] [0] [y = ] [Categorical_2/Categorical/validate_probs/Const:0] [1]\r\n\t [[Node: Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_0, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_1, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_2, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch_1, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_4, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_5, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch_2)]]\r\n```\r\nIt get the error only when I call `sess.run(t.sample([1])`, but not when I construct the distribution `t = tf.contrib.distributions.Categorical(probs=[0.0,0.0,0.0], validate_args=True)`. What is the design choice for this? Also, I am curious about the design choice behind `validate_args`, because it is unintuitive to mean that an invalid distribution would be useful at all, and so validating the arguments would be necessary in any case.", "validate_args is very expensive, especially when on a GPU.  So it's\ndisabled by default.  Turn it on when developing models.\n\nOn Jul 26, 2017 1:44 AM, \"mbchang\" <notifications@github.com> wrote:\n\n> When I turn it on, I get the error:\n>\n> >>> import tensorflow as tf\n> >>> sess = tf.Session()\n> >>> t = tf.contrib.distributions.Categorical(probs=[0.0,0.0,0.0], validate_args=True)\n> >>> print(sess.run(t.sample([1])))\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n>   File \"[...]/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\n>     run_metadata_ptr)\n>   File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\n>     feed_dict_string, options, run_metadata)\n>   File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\n>     target_list, options, run_metadata)\n>   File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\n>     raise type(e)(node_def, op, message)\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [probs does not sum to 1.] [Condition x ~= y did not hold element-wise: x = ] [Categorical_2/Categorical/validate_probs/Sum:0] [0] [y = ] [Categorical_2/Categorical/validate_probs/Const:0] [1]\n> \t [[Node: Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_0, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_1, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_2, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch_1, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_4, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_5, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch_2)]]\n>\n> It get the error only when I call sess.run(t.sample([1]), but not when I\n> construct the distribution t = tf.contrib.distributions.\n> Categorical(probs=[0.0,0.0,0.0], validate_args=True). What is the design\n> choice for this? Also, I am curious about the design choice behind\n> validate_args, because it is unintuitive to mean that an invalid\n> distribution would be useful at all, and so validating the arguments would\n> be necessary in any case.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11758#issuecomment-317989211>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3CMdscxD-Oubw5GEI2JdPLGr0mvks5sRvxSgaJpZM4Oi75q>\n> .\n>\n", "Input to the constructor if usually an unknown Tensor which is why we don't\ntry to check until runtime. We could add static checks here though,\nprobably.\n\nOn Jul 26, 2017 5:59 AM, \"Eugene Brevdo\" <ebrevdo@google.com> wrote:\n\n> validate_args is very expensive, especially when on a GPU.  So it's\n> disabled by default.  Turn it on when developing models.\n>\n> On Jul 26, 2017 1:44 AM, \"mbchang\" <notifications@github.com> wrote:\n>\n>> When I turn it on, I get the error:\n>>\n>> >>> import tensorflow as tf\n>> >>> sess = tf.Session()\n>> >>> t = tf.contrib.distributions.Categorical(probs=[0.0,0.0,0.0], validate_args=True)\n>> >>> print(sess.run(t.sample([1])))\n>> Traceback (most recent call last):\n>>   File \"<stdin>\", line 1, in <module>\n>>   File \"[...]/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\n>>     run_metadata_ptr)\n>>   File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\n>>     feed_dict_string, options, run_metadata)\n>>   File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\n>>     target_list, options, run_metadata)\n>>   File \"[...]/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\n>>     raise type(e)(node_def, op, message)tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [probs does not sum to 1.] [Condition x ~= y did not hold element-wise: x = ] [Categorical_2/Categorical/validate_probs/Sum:0] [0] [y = ] [Categorical_2/Categorical/validate_probs/Const:0] [1]\n>> \t [[Node: Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_STRING, DT_FLOAT], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_0, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_1, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_2, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch_1, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_4, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/data_5, Categorical_2/Categorical/validate_probs/assert_close/Assert/AssertGuard/Assert/Switch_2)]]\n>>\n>> It get the error only when I call sess.run(t.sample([1]), but not when I\n>> construct the distribution t = tf.contrib.distributions.Categorical(probs=[0.0,0.0,0.0],\n>> validate_args=True). What is the design choice for this? Also, I am\n>> curious about the design choice behind validate_args, because it is\n>> unintuitive to mean that an invalid distribution would be useful at all,\n>> and so validating the arguments would be necessary in any case.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/11758#issuecomment-317989211>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim3CMdscxD-Oubw5GEI2JdPLGr0mvks5sRvxSgaJpZM4Oi75q>\n>> .\n>>\n>\n", "I see. Checks would be helpful, otherwise the code might silently ignore invalid distributions if the flag is not set. Thanks for the clarifications!", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Im marking this closed since `validate_args=True` should have been used by the user to catch this case. As Eugene indicated, in general checks are expensive and therefore we do not want them to be on by default."]}]