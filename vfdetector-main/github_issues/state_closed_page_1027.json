[{"number": 22524, "title": "assign_add seems giving wrong and random results in broadcasting", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDarwin localhost 17.5.0 Darwin Kernel Version 17.5.0: Fri Apr 13 19:32:32 PDT 2018; root:xnu-4570.51.2~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.10.0-12-g4dcfddc5d1 1.10.1\r\n\r\n- **Python version**:\r\n2.7 & 3.7\r\n\r\n- **Bazel version**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nSee below\r\n- **Mobile device**:\r\nN/A\r\n\r\n\r\n### Describe the problem\r\n\r\nI ran the very simple code below and I've got random meaningless results. The first element of the result is always correct. So it seems that the broadcasting is not working, i.e. only the first element has been correctly processed. But I think people would assume `assign_add` works in simple case like `assign_add(_, 0.0)` (or it should complain rather than silently give wrong results). \r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\n\r\nZ = tfe.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nprint(Z.numpy())\r\ntf.assign_add(Z, 0.0)\r\nprint(Z.numpy())\r\n```\r\n\r\nOne sample result:\r\n\r\n```\r\n[[1. 2. 3.]\r\n [4. 5. 6.]]\r\n[[1.00000000e+00 6.55319777e+13 2.69145827e+20]\r\n [3.84908544e+09 1.23657954e+33 6.00000000e+00]]\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@David-Mao \r\n\r\nMake sure the value given in tf.assign_add() has the same shape as the reference(in this case it is Z).\r\nPlease change the scalar value 0.0 to the matrix of same dimension as Z. It should be something as shown below.\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\nZ = tfe.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nY = tfe.Variable([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\r\nprint(Z.numpy())\r\ntf.assign_add(Z, Y)\r\nprint(Z.numpy())", "> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n> Bazel version\r\n> CUDA/cuDNN version\r\n> GPU model and memory\r\n> Exact command to reproduce\r\n> Mobile device\r\n\r\ndone", "@harshini-gadige\r\n> \r\n> Make sure the value given in tf.assign_add() has the same shape as the reference(in this case it is Z).\r\n> Please change the scalar value 0.0 to the matrix of same dimension as Z. It should be something as shown below.\r\n> \r\n> import tensorflow as tf\r\n> import tensorflow.contrib.eager as tfe\r\n> tf.enable_eager_execution()\r\n> Z = tfe.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n> Y = tfe.Variable([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\r\n> print(Z.numpy())\r\n> tf.assign_add(Z, Y)\r\n> print(Z.numpy())\r\n\r\nObviously that would work. My points are:\r\n\r\na) people would easily assume `Z.assign_add(0.0)` works. (because `Z = Z + 0.0` works. and it's not hard to implement this broadcasting anyway.)\r\n\r\nb) given a), even if the team decides to leave it not working, it should raise an error, not silently give unpredictable result, which apparently leads to bugs that are hard to find.", "Hi @David-Mao !\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Please visit these links to upgrade your codebase to latest versions(2.7).Ref [1](https://www.tensorflow.org/addons),[2](https://www.tensorflow.org/guide/migrate). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I think this is already fixed in later versions. In newer versions of TensorFlow there is now an error if you assign_add the wrong shape i.e.\r\n\r\n![image](https://user-images.githubusercontent.com/326106/148100809-df4c9ad6-0eb7-43da-b1b1-080be8d484ad.png)\r\n\r\n"]}, {"number": 22523, "title": "Tensorflow Lite optimized build for i.MX6 SOC", "body": "This is a basic build to create an optimized tensorflowlite library to\r\nrun on the cortex-a9 based i.MX6\r\n\r\nSigned-off-by: Jon Nettleton <jon@solid-run.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@jnettlet can you sign the CLA please?", "I signed it earlier today.   Let me know how I can verify it, if you this\nis still a problem.\n\nThanks,\nJon\n\nOn Wed, Sep 26, 2018 at 4:53 PM Shanqing Cai <notifications@github.com>\nwrote:\n\n> @jnettlet <https://github.com/jnettlet> can you sign the CLA please?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/22523#issuecomment-424742526>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AhtJbIT-mK8zjBcMn5g4zvucnqkxTlrpks5ue5TvgaJpZM4W6Cwf>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "contrib folder has been deleted , so please push your changes to respective folder again , closing this for now."]}, {"number": 22522, "title": "crosstool_wrapper_driver_is_not_gcc failed: error executing command", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.17.2\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\n- **CUDA/cuDNN version**: 10.0 / 7.3.0\r\n- **GPU model and memory**: GeForce 940MX\r\n- **Exact command to reproduce**: bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nWhen the VERBS support is enabled the tensorflow build fails with the following error message:\r\n**~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command**\r\nDespite the fact that it was reported multiple times before that the changes have been merged into the trunk and will be merged and therefore included in the next release candidates, we already have \"stable\" release and this issue is still observed.\r\nThis issue was observed in the 1.11.0rc1 and 1.11.0rc2 versions of tensorflow as well.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd ~/.cache/bazel/_bazel_vyepishov/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    NCCL_INSTALL_PATH=/usr/local/nccl_2.3.4-1+cuda10.0_x86_64 \\\r\n    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \\\r\n    TF_CUDA_VERSION=10.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/k8-opt/genfiles/external/grpc -iquote bazel-out/k8-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote bazel-out/k8-opt/bin/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/k8-opt/genfiles/external/grpc/include -isystem bazel-out/k8-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem bazel-out/k8-opt/bin/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/com_google_absl -isystem bazel-out/k8-opt/genfiles/external/com_google_absl -isystem bazel-out/k8-opt/bin/external/com_google_absl -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -mavx -mavx2 -mfma '-mfpmath=both' -msse4.2 -c tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o)\r\nIn file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/common_runtime/device.h:43,\r\n                 from ./tensorflow/core/common_runtime/device_mgr.h:24,\r\n                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,\r\n                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,\r\n                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,\r\n                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\n./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':\r\n./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {\r\n                                         ~~~~^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':\r\ntensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'\r\n   string key(std::move(parsed.FullKey().ToString()));\r\n                                         ^~~~~~~~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/refcount.h:22,\r\n                 from ./tensorflow/core/platform/tensor_coding.h:21,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/types.h:31,\r\n                 from ./tensorflow/contrib/verbs/verbs_util.h:21,\r\n                 from ./tensorflow/contrib/verbs/rdma.h:30,\r\n                 from ./tensorflow/contrib/verbs/rdma_mgr.h:24,\r\n                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21,\r\n                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 6111.101s, Critical Path: 179.37s\r\nINFO: 9166 processes: 9166 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@vyepishov Please refer to #22372 The PR @yongtang submitted is approved and pending merge. ", "@wt-huang Could you please let me know in what tensorflow release I can expect this issue to be fixed? Thanks in advance.", "@vyepishov This will be included in the next TensorFlow release. Will keep you posted.", "@wt-huang Thanks. Will be looking forward to this!"]}, {"number": 22521, "title": "fix unbalanced delimiter in benchmark_model doc", "body": "as reported in https://github.com/tensorflow/tensorflow/issues/22499, there is unbalanced delimiter `\"`", "comments": []}, {"number": 22520, "title": "Issue in running tensorflow", "body": "Traceback (most recent call last):\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\zuha\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["As the issue is missing a lot of information from the template, I will close this as a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\r\nThat's my best guess with the information here."]}, {"number": 22519, "title": "Find NCCL2 debians in Tensorflow configure", "body": "Continuation of https://github.com/tensorflow/tensorflow/pull/20155\r\n\r\nThe Tensorlfow configure doesn't work with NCCL2 when it is installed via the debians.\r\nThis PR adds code that will handle the following scenarios:\r\n\r\n    NCCL2 installed via unpacked .tar file and not listed in ldconfig\r\n    NCCL2 installed via unpacked .tar file and listed properly in ldconfig\r\n    NCCL2 installed via debian file and not listed in ldconfig\r\n    NCCL2 installed via debian file and listed properly in ldconfig\r\n\r\nThe debian places the lib and header in the system paths (`/usr/lib/*-linux-gnu` and `/usr/include`). A new environment variable `NCCL_HDR_PATH` was created to deal with the divergence in the lib and header paths in this scenario. The `third_party/nccl/nccl_configure.bzl` file was changed to look up the new environment variable for the header location.", "comments": ["NCCL 2 is open source which should allow us to make this easier.  Not to side track the effort. :-)", "Yup, just posted:\r\nhttps://docs.nvidia.com/deeplearning/sdk/nccl-release-notes/rel_2.3.5.html\r\nhttps://github.com/NVIDIA/nccl\r\n\r\nI think this PR is still valid for folks that have installed .debs manually on their system or perfer to use the .debs for package management.", "agree\n\nOn Thu, Sep 27, 2018 at 10:56 AM jayfurmanek <notifications@github.com>\nwrote:\n\n> Yup, just posted:\n> https://docs.nvidia.com/deeplearning/sdk/nccl-release-notes/rel_2.3.5.html\n> https://github.com/NVIDIA/nccl\n>\n> I think this PR is still valid for folks that have installed .debs\n> manually on their system or perfer to use the .debs for package management.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/22519#issuecomment-425183456>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesnByA-ZjdaZkeGRUrIivbR0WrhlCks5ufRFdgaJpZM4W55TN>\n> .\n>\n", "Marking as \"ready to pull\" to start pull process while tests are running so that this PR can make it to TF 1.12."]}, {"number": 22518, "title": "Support for NVIDIA Video Loader (nvvl)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: GTX 980M 4G\r\n- **Exact command to reproduce**: N/A\r\n\r\nCould you add the support for [NVIDIA Video Loader](https://github.com/NVIDIA/nvvl)?", "comments": ["@fengyang0317 keep an eye on DALI, https://github.com/NVIDIA/DALI", "@samikama Why you are forwarding over DALI? Is DALI going to support video readers like NVVL? I don't think that features overlaps between the two projects or that DALI supersedes NVVL.", "Something is moving in https://github.com/NVIDIA/DALI/pull/240"]}, {"number": 22517, "title": "Update to use python 2-3 compatible function tf_inspect.getfullargspec.", "body": "RE: Issue #22413 \r\n\r\nThe `tf_inspect` module is configured to shim python 2 for use with the `tf_inspect.getfullargspec` wrapper, making the function python 2-3 compatible.\r\n\r\nThe older `tf_inspect.getargspec` is used locally for _getfullargspec with python 2, and called for unit tests in tf_inspect_test.py, but it's also exposed for potential misuse without much documentation. \r\n\r\nSeveral modules fall into the trap of calling the exposed `tf_inspect.getargspec` directly. Python 3 calls to any of these modules will call the deprecated python 2 function.\r\n\r\nThis PR changes the following:\r\n\r\n- Adds a warning to the `tf_inspect.getargspec` docstring recommending use of `tf_inspect.getfullargspec` instead.\r\n\r\n- Updates `tf_inspect.getargspec` calls in other modules to use `tf_inspect.getfullargspec`\r\n\r\n- Adds unit tests for `tf_inspect.getfullargspec` and `tf_inspect.FullArgSpec`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Hey; there seems to be a small issue with tf_inspect_test. Can you take a look?", "@alextp I'm looking at the tests now. The changes passed local build tests, but I think I know what's going on after reading through the build check logs.\r\n\r\nI'll get this updated today.", "@alextp I believe I addressed the issues from the logs.", "@alextp the backwards compatibility checks are flagging that the name of the ** parameter changed from `keywords` to `varkw` with the change from `inspect.getargspec` to `inspect.getfullargspec`. According to [the docs:](https://docs.python.org/3/library/inspect.html)\r\n\r\nFrom `inspect.getargspec`\r\n> Deprecated since version 3.0: Use `getfullargspec()` for an updated API that is usually a drop-in replacement, but also correctly handles function annotations and keyword-only parameters.\r\n\r\nFrom `inspect.getfullargspec`\r\n> Changed in version 3.6: This method was previously documented as deprecated in favour of signature() in Python 3.5, but that decision has been reversed in order to restore a clearly supported standard interface for single-source Python 2/3 code migrating away from the legacy getargspec() API.\r\n\r\n> Changed in version 3.7: Python only explicitly guaranteed that it preserved the declaration order of keyword-only parameters as of version 3.7, although in practice this order had always been preserved in Python 3.\r\n\r\nDepending on the importance of this ** parameter's naming, two initial options come to mind:\r\n\r\n1. Route (some or all of) the functions back to explicitly calling the deprecated `inspect.getargspec` through `tf_inspect.getargspec` \r\n\r\nThis will make the  API compatibility check issue go away, but will also cause python 3 calls to use the deprecated `getargspec` and bypass the (otherwise) cross-compatible `tf_inspect.getfullargspec` wrapper.\r\n \r\n2. Address any upstream issues that would come from the function signature change.\r\n\r\nIt seems prudent to make the cross-compatible wrapper work over counting on a deprecated function, but I can't picture what issues the update could create.\r\n\r\nI will say that I'm happy to help however I can with this and future issues. Please advise.", "Maybe tf_inspect.getargspec can call getfullargspec and repackage the return values into the old struct?", "`tf_inspect` could certainly name the ** parameter based on the call - the module already has two checks to differentiate between python 2/3 and create the current `tf_inspect.getfullargspec` wrapper. \r\n\r\nWe could modify the python 3 struct for this call in `tf_inspect` to match the `getargspec` call from python 2 but, analogous to the current situation, it would likely be flagged by python 3 API compatibility checks for the `FullArgSpec` parameter name change from `varkw` to `keywords`.\r\n\r\nWe could also modify just the python 2 `FullArgSpec` struct to use the `keywords` parameter name; however, the parameter would come back as `keywords` when using python 2 and `varkw` when using python 3. This would keep python 2 environments consistent, but any code used by python 2 and 3 would have to avoid testing for only one instance of this parameter name.\r\n\r\n------\r\n\r\nSomething that just came to mind (maybe this was the intent of your last post) - since both the python 2 and 3 versions of this function are exposed in the `tf_inspect` module, and since there's already a shim to make `tf_inspect.getfullargspec` python 2 compatible - we could add a similar shim such that when `tf_inspect.getargspec` is called from python 3, it made a call to its current `inspect.getfullargspec` and had the `varkw` parameter name changed to `keywords`\r\n\r\n`tf_inspect` would contain two shims: one that made python 2 compatible with `getfullargspec` calls, and a new one that made python 3 compatible with non-deprecated `getargspec` calls. When `tf_inspect.getfullargspec` was called, both would have a ** parameter name of `varkw`; when `tf_inspect.getargspec` was called it would be `keywords`. Under the hood python 2 would always be calling `inspect.getargspec` and python 3 would be calling `inspect.getfullargspec`. \r\n\r\nTo pass the python 2 API compatibility check, I would also have to change the function calls in other modules back to `tf_inspect.getargspec`, but python 3 would no longer call the deprecated `inspect.getargspec`. \r\n\r\nIn this scenario, I would recommend maintaining the docstring recommendation to use `tf_inspect.getfullargspec` moving forward, but neither this function nor `getargspec` should break anything under the hood.", "Shorter answer: I think so. \r\n\r\nI'll update the PR.", "Yes this is what I was suggesting\n\nOn Thu, Sep 27, 2018, 18:22 Isaac Burbank <notifications@github.com> wrote:\n\n> Shorter answer: I think so.\n>\n> I'll update the PR.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/22517#issuecomment-425289886>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWM6IK2uJN0Mme44j3nMLM0yoyaHks5ufXnDgaJpZM4W5z4E>\n> .\n>\n", "That should do it.", "I don't see any errors in the logs associated with these PR changes.", "Searching through the logs:\r\n\r\n- Most issues seem related to `sysctl` not being found\r\n- Could not find the `go` tool in PATH or` /usr/local/go`\r\n- `QuantizedActivationsOpTest.LogSoftmax` is giving a wrong value at index 0\r\n- `examples_test` error for `dump_dir` flag being empty\r\n\r\nI downloaded them to search through, and can'f find a single reference to `tf_inspect` or `tf_inspect_test` module calls in any of the errors. Those that run `tf_inspect_test` appear to show it passing. ", "I looked that the Ubuntu CC test logs of a couple other PRs recently marked `Ready to Pull` and they have the same build errors. I think there's a separate internal issue with the testing pipeline.", "The main thing that stood out in the GPU Python3 invocation log was\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/python/BUILD:5651:1: Couldn't build file tensorflow/python/_objs/framework/fast_tensor_util.so/tensorflow/python/framework/fast_tensor_util.pic.o: undeclared inclusion(s) in rule '//tensorflow/python:framework/fast_tensor_util.so':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/python/framework/fast_tensor_util.cpp':\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/arrayobject.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/ndarrayobject.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/ndarraytypes.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/npy_common.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/numpyconfig.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/_numpyconfig.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/npy_endian.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/npy_cpu.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/utils.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/old_defines.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/__multiarray_api.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/npy_interrupt.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/ufuncobject.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/npy_math.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/npy_common.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/arrayobject.h'\r\n  '/usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/__ufunc_api.h'\r\nIn file included from /usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1816:0,\r\n                 from /usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/ndarrayobject.h:18,\r\n                 from /usr/local/lib/python3.4/dist-packages/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from bazel-out/host/genfiles/tensorflow/python/framework/fast_tensor_util.cpp:581:\r\n...\r\n```\r\n\r\nOf the 3 tests the failed in MacOS Python2 and CC:\r\n- `//tensorflow/core:platform_profile_utils_cpu_utils_test` failed with\r\n```\r\nsh: sysctl: command not found\r\n2018-10-02 18:51:50.312938: F tensorflow/core/platform/profile_utils/cpu_utils_test.cc:64] Check failed: cpu_frequency > 0 (-1 vs. 0)\r\n*** Received signal 6 ***\r\n```\r\n- `//tensorflow/go:test` with\r\n```\r\nCould not find the 'go' tool in PATH or /usr/local/go\r\n```\r\n- `//tensorflow/python/debug:examples_test` with:\r\n```\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value v\r\n\t [[node v/read (defined at /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/debug/examples_test.runfiles/org_tensorflow/tensorflow/python/debug/examples/debug_errors.py:80)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](v)]]\r\n```\r\nand\r\n```\r\nTesting offline_analyzer\r\nERROR: offline_analyzer output didn't match expectation: sh: sysctl: command not found\r\nsh: sysctl: command not found\r\nsh: sysctl: command not found\r\nERROR: dump_dir flag is empty.\r\nExpected output: ERROR: dump_dir flag is empty.\r\n```\r\n\r\nWindows Bazel fails test `tensorflow/tools/ci_build/builds/gen_win_out.exe` with\r\n```\r\n==================== Test output for //py_test_dir/tensorflow/python/data/kernel_tests:optional_ops_test:\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\sre_compile.py\", line 276, in _optimize_charset\r\n    charmap[k] = 1\r\nIndexError: bytearray index out of range\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_1gl6ejmy\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\data\\kernel_tests\\optional_ops_test.py\", line 23, in <module>\r\n    from tensorflow.python.data.kernel_tests import test_base\r\n...\r\nFile \"C:\\Python36\\lib\\sre_compile.py\", line 226, in _compile_charset\r\n    for op, av in _optimize_charset(charset, fixup, fixes):\r\n  File \"C:\\Python36\\lib\\sre_compile.py\", line 287, in _optimize_charset\r\n    charmap += b'\\0' * 0xff00\r\nMemoryError\r\n```\r\nand has a lot of flaky checks.\r\n\r\nClicking the XLA details link leads to a `page not found`.\r\n\r\nMacOS Contrib has an unrelated broken test that's showing up in the logs of all the current PR's that I'm looking through.\r\n```\r\ntensorflow/contrib/lite/kernels/activations_test.cc:561\r\nValue of: m.GetDequantizedOutput<uint8_t>()\r\nExpected: has 8 elements where\r\nelement #0 is approximately -4.1429701 (absolute error <= 0.0625),\r\nelement #1 is approximately -10.14297 (absolute error <= 0.0625),\r\nelement #2 is approximately -2.1429701 (absolute error <= 0.0625),\r\nelement #3 is approximately -0.14297099 (absolute error <= 0.0625),\r\nelement #4 is approximately -7.00104 (absolute error <= 0.0625),\r\nelement #5 is approximately -12.00104 (absolute error <= 0.0625),\r\nelement #6 is approximately -0.00104087 (absolute error <= 0.0625),\r\nelement #7 is approximately -9.0010405 (absolute error <= 0.0625)\r\n  Actual: { -1.375, -1.375, -1.375, -1.375, -1.375, -1.375, -1.375, -1.375 }, whose element #0 doesn't match, which is 2.76797 from -4.14297\r\n```\r\n\r\n\r\n---\r\nI noticed similar log errors for the 6 failing checks in the half-dozen PR's I checked with recent test runs.\r\n\r\nThere may be an issue that needs to be ironed out the current build or test pipeline.\r\n\r\nIf you see anywhere I can be useful - I'm happy to help."]}, {"number": 22516, "title": "TF Lite QuantizationUtilTest failure/bug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MIPS32 Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: SHA: 8c2159a10e53e5301ae26c739a3d09fa53d3352e\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: mips-mti-linux-gnu-g++ (Codescape GNU Tools 2017.10-05 for MIPS MTI Linux) 6.3.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Too complicated\r\n\r\n### Describe the problem\r\nTensorFlow Lite test quantization_util_test failure:\r\n~~~\r\n[==========] Running 17 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 17 tests from QuantizationUtilTest\r\n[ RUN      ] QuantizationUtilTest.SafeCast\r\n[       OK ] QuantizationUtilTest.SafeCast (12 ms)\r\n[ RUN      ] QuantizationUtilTest.ChooseQuantizationParams\r\n[       OK ] QuantizationUtilTest.ChooseQuantizationParams (1 ms)\r\n[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMinBoundary\r\n[       OK ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMinBoundary (0 ms)\r\n[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsZeroNotInRange\r\n[       OK ] QuantizationUtilTest.ChooseQuantizationParamsZeroNotInRange (179 ms)\r\n[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangePositive\r\n[       OK ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangePositive (112 ms)\r\n[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangeZero\r\n[       OK ] QuantizationUtilTest.ChooseQuantizationParamsEmptyRangeZero (0 ms)\r\n[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMaxBoundary\r\n[       OK ] QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMaxBoundary (0 ms)\r\n[ RUN      ] QuantizationUtilTest.IntegerFrExp\r\n[       OK ] QuantizationUtilTest.IntegerFrExp (1 ms)\r\n[ RUN      ] QuantizationUtilTest.IntegerFrExpVersusDouble\r\ntensorflow/contrib/lite/kernels/internal/quantization_util_test.cc:262: Failure\r\nThe difference between result and (0.964453 * (1L << 31)) is 4142294361.764544, which exceeds 1000, where\r\nresult evaluates to 2071147315,\r\n(0.964453 * (1L << 31)) evaluates to -2071147046.764544, and\r\n1000 evaluates to 1000.\r\n[  FAILED  ] QuantizationUtilTest.IntegerFrExpVersusDouble (4 ms)\r\n[ RUN      ] QuantizationUtilTest.DoubleFromFractionAndShift\r\n[       OK ] QuantizationUtilTest.DoubleFromFractionAndShift (1 ms)\r\n[ RUN      ] QuantizationUtilTest.IntegerDoubleMultiply\r\n[       OK ] QuantizationUtilTest.IntegerDoubleMultiply (0 ms)\r\n[ RUN      ] QuantizationUtilTest.IntegerDoubleCompare\r\n[       OK ] QuantizationUtilTest.IntegerDoubleCompare (1 ms)\r\n[ RUN      ] QuantizationUtilTest.ChooseQuantizationParamsInvalidRange\r\n[       OK ] QuantizationUtilTest.ChooseQuantizationParamsInvalidRange (110 ms)\r\n[ RUN      ] QuantizationUtilTest.QuantizeMultiplierSmallerThanOneExp\r\n[       OK ] QuantizationUtilTest.QuantizeMultiplierSmallerThanOneExp (581 ms)\r\n[ RUN      ] QuantizationUtilTest.QuantizeMultiplierGreaterThanOne\r\n[       OK ] QuantizationUtilTest.QuantizeMultiplierGreaterThanOne (103 ms)\r\n[ RUN      ] QuantizationUtilTest.PreprocessSoftmaxScaling\r\n[       OK ] QuantizationUtilTest.PreprocessSoftmaxScaling (1 ms)\r\n[ RUN      ] QuantizationUtilTest.CalculateInputRadius\r\n[       OK ] QuantizationUtilTest.CalculateInputRadius (0 ms)\r\n[----------] 17 tests from QuantizationUtilTest (1106 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 17 tests from 1 test case ran. (1109 ms total)\r\n[  PASSED  ] 16 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] QuantizationUtilTest.IntegerFrExpVersusDouble\r\n\r\n 1 FAILED TEST\r\nTest quantization_util_test failed!\r\n~~~\r\nLooks like there's undefined behavior in the test. The test expects a 64-bit long, whereas it's 32-bit on MIPS32. Changing `1L` to `1LL` fixes the failure.\r\n\r\n### Source code / logs\r\nSee above.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "@tensorflowbutler Updated, though the update is mostly irrelevant.", "@alexfru Thanks for reporting and fixing this. We will certainly note it down.", "Thanks for reporting this Alexey. I'm making the change with your noted fix now.", "Addressed in [this change](https://github.com/tensorflow/tensorflow/commit/100714d9e5eb723525eb54142769f9bd8eec5edd).\r\n\r\nThanks again for reporting this!"]}, {"number": 22515, "title": "[INTEL MKL] Enable reorder cache in MklTranspose.", "body": "Enable reorder primitive reuse in MklTranspose, please review.", "comments": ["Nagging Reviewer @zhangyaobit: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@zhangyaobit Any updates?", "@caisq looks like I don't have write access any more. Could you add another reviewer? Thanks.", "@caisq @zhangyaobit Any updates?", "@caisq @zhangyaobit Any updates?", "@pandaoxin Starting the pulling process now. Sorry for the delay.", "@pandaoxin the pulling process is affected by the recent (ongoing?) GitHub outage. Still trying. Thank you for your patience.", "@pandaoxin Can you take a look at the clang-format error at https://source.cloud.google.com/results/invocations/46bdfb6c-809a-4948-a6eb-6c9953449e68/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_clang_format_out/log?\r\n", "@caisq Have fixed. Thanks!"]}, {"number": 22514, "title": "Assertion `d.nbDims >= 3' failed with int8 for Tensorflow TensorRT integration", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**:source \r\n- **TensorFlow version (use command below)**: r1.11\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9/7.1\r\n- **GPU model and memory**: 1080ti/11gb\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    # We load the protobuf file from the disk and parse it to retrieve the\r\n    # unserialized graph_def\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    # Then, we can use again a convenient built-in function to import a graph_def into the\r\n    # current default Graph\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(\r\n            graph_def,\r\n            name='', #DEBUG\r\n        )\r\n    return graph\r\n\r\nfid = \"model.pb\"\r\noutput_nodenames = 'out1,out2,out3'\r\noutput_node = list(output_nodenames.split(\",\"))\r\ng = load_graph(fid)\r\nwith tf.Session(graph=g) as sess:\r\n    trt_graph = trt.create_inference_graph(\r\n    input_graph_def=tf.get_default_graph().as_graph_def(),\r\n    outputs=output_node,\r\n    max_batch_size=99999,\r\n    max_workspace_size_bytes=1 << 25,\r\n    precision_mode=\"INT8\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"\r\n    minimum_segment_size=2  # minimum number of nodes in an engine\r\n    )\r\n    with tf.gfile.GFile(\"trt.pb\", \"wb\") as f:\r\n        f.write(trt_graph.SerializeToString())\r\n\r\ng2 = load_graph(\"trt.pb\")\r\nwith tf.Session(graph=g2) as sess:\r\n    \"\"\"Run given calibration graph multiple times.\"\"\"\r\n    num_samples = 10\r\n    np.random.seed(0)\r\n    ip1_data = np.random.rand(num_samples,700,800,6).astype(np.float32)\r\n    ip1 = g2.get_tensor_by_name(\"ip1:0\")\r\n\r\n    ip2_data = np.random.rand(4).astype(np.float32)\r\n    ip2 = g2.get_tensor_by_name(\"ip2:0\")\r\n\r\n    ip3_data = np.random.rand(20000,6).astype(np.float32)\r\n    ip3 = g2.get_tensor_by_name(\"ip3:0\")\r\n\r\n    ip4_data = np.random.rand(20000,4).astype(np.float32)\r\n    ip4 = g2.get_tensor_by_name(\"ip4:0\")\r\n\r\n    out1 = g2.get_tensor_by_name(\"out1:0\")\r\n    out2 = g2.get_tensor_by_name(\"out2:0\")\r\n    out3 = g2.get_tensor_by_name(\"out3:0\")\r\n# run over real calibration data here, we are mimicking a calibration set of\r\n# 30 different batches. Use as much calibration data as you want\r\n    for i in range(num_samples):\r\n        val = sess.run([out1, out2, out3], feed_dict={ip1:ip1_data[i], ip2:ip2_data, ip3:ip3_data, ip4:ip4_data})\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n2018-09-25 17:20:14.600365: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f7fb4001730\r\n2018-09-25 17:20:14.600539: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f7fac071a40\r\n2018-09-25 17:20:14.973555: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:577] Starting calibration thread on device 0, Calibration Resource @ 0x7f7fa4004fe0\r\npython: helpers.cpp:56: nvinfer1::DimsCHW nvinfer1::getCHW(const nvinfer1::Dims&): Assertion `d.nbDims >= 3' failed.\r\nAborted (core dumped)\r\n```\r\n", "comments": ["@dhingratul ,\r\n\r\ndo you have a reproducer? I guess what you put above is missing model.pb. As a side note, unless you have a tiny model, setting max batch size to 99999 is a bad idea.", "@samikama I had to set the max batch size to a very big value as a workaround where I was getting error of \"Engine Buffer Full\" and the inferences were 3x slower. I cannot share this model.pb. but let me see if I can come up with another reproducible case for you. But, if you can give me any pointers just looking at the error, it would useful for me.", "@dhingratul one option could be playing with segment size option to leave the problematic engines out, if possible. Your problem seem to be due to data dependent shapes in the graph. TFTRT can not properly handle the data dependent shapes yet. We are working on it.", "@samikama All the shapes in the graph, as far as I can see in tensorboard are fixed. How do i \"leave problematic engines out\"?", "@dhingratul I think @samikama means setting the `minimum_segment_size` to a larger number like `50`. However, that doesn't work for me. :man_shrugging: ", "@benjamintanweihao I have varied it from 1 to 9999, doesn't work.", "@dhingratul Yeah, I have created an issue https://github.com/tensorflow/tensorflow/issues/24032 for something similar to what you are facing too.", "I'm also having this error with INT8 Keras resnet50. \r\n\r\nCode to reproduce the error:\r\nhttps://github.com/jeng1220/KerasToTensorRT/blob/master/tftrt_resnet_example.py\r\nChange line 66:\r\n`    y = np.empty((num_tests, self.y_tensor.shape[1]), np.float32)`\r\nto     `y = np.empty((num_tests, 1000), np.float32)`\r\nline 89:\r\n`batch_size = 128`\r\nto `batch_size = 1`\r\nline 108   `tftrt_engine = TftrtEngine(frozen_graph, batch_size, 'FP32')`\r\nto  `tftrt_engine = TftrtEngine(frozen_graph, batch_size, 'INT8')`\r\nI tried tensorflow-gpu1.12/tensorflow-gpu1.11 and TensorRT 4.0\r\nGTX 1080ti\r\nCUDA 9.0 cudnn 7.0\r\nDriver Version: 390.87 \r\nFP32 works for the keras resnet50.\r\n", "Seems like there is a related issue at:\r\nhttps://github.com/tensorflow/models/issues/5061\r\nwith  the int8 tensorrt example for resnet50 in tensorflow\r\nhttps://github.com/tensorflow/models/blob/master/research/tensorrt/tensorrt.py\r\nnot working. \r\n\r\nAnother code for resnet50 with INT8 works:  \r\nhttps://github.com/tensorflow/models/issues/5061#issuecomment-419235783\r\n> That's similar to this one and INT8 works perfectly. Here is the link:\r\n> https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz\r\n> I found it in this link:\r\n> https://devblogs.nvidia.com/tensorrt-integration-speeds-tensorflow-inference/\r\n> Hope it can solve the problem for you as well.\r\n> Please let me know if you have any question about it.", "Is this still there with TRT5.0?", "@trevor-m @pooyadavoodi  Can you PTAL", "I believe this is fixed in a more recent version of TF.\r\n\r\nI just tried the script that @blackarrow3542  linked in https://github.com/tensorflow/tensorflow/issues/22514#issuecomment-443076374\r\nI tested with TF 1.14, had to make some changes to adjust to the new TF-TRT API.\r\nThe test passed.\r\n\r\nThis is the log:\r\n\r\n```\r\n2019-06-19 23:58:57.567682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22664 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:02:00.0, compute capability: 7.5)\r\n2019-06-19 23:58:57.966968: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:460] There are 4 ops of 3 different types in the graph that are not converted to TensorRT: Softmax, Placeholder, NoOp, (For more information see https://docs.nvidia.com/deeplearning/dgx/tf-trt-user-guide/index.html#supported-ops).\r\n2019-06-19 23:58:58.008931: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:733] Number of TensorRT candidate segments: 1\r\n2019-06-19 23:58:58.024111: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-06-19 23:58:58.109276: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:835] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 503 nodes succeeded.\r\n2019-06-19 23:58:58.129599: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:739] Optimization results for grappler item: tf_graph\r\n2019-06-19 23:58:58.129650: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:741]   constant folding: Graph size after: 555 nodes (0), 570 edges (0), time = 44.491ms.\r\n2019-06-19 23:58:58.129661: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:741]   layout: Graph size after: 560 nodes (5), 572 edges (2), time = 27.786ms.\r\n2019-06-19 23:58:58.129669: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:741]   constant folding: Graph size after: 557 nodes (-3), 572 edges (0), time = 43.335ms.\r\n2019-06-19 23:58:58.129676: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:741]   TensorRTOptimizer: Graph size after: 55 nodes (-502), 54 edges (-518), time = 274.138ms.\r\n2019-06-19 23:58:59.270390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:02:00.0\r\n2019-06-19 23:58:59.270441: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-06-19 23:58:59.270465: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-06-19 23:58:59.270476: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-06-19 23:58:59.270487: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-06-19 23:58:59.270499: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-06-19 23:58:59.270509: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-06-19 23:58:59.270520: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-19 23:58:59.271632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-19 23:58:59.271664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-19 23:58:59.271671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0\r\n2019-06-19 23:58:59.271677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n2019-06-19 23:58:59.273011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22664 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:02:00.0, compute capability: 7.5)\r\n2019-06-19 23:58:59.471499: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:716] Starting calibration thread on device 0, Calibration Resource @ 0x7fe9e4008c40\r\n2019-06-20 00:01:06.687922: I tensorflow/compiler/tf2tensorrt/utils/trt_resources.cc:25] Destroying Calibration Resource\r\n Calibrator = 0x7fe9e40023c0\r\n Builder    = 0\r\n Engine     = 0x7fe38668c5c0\r\n Logger     = 0x7fe9e4008cc0\r\n Allocator  = 0x7fe9e4008770\r\n Thread     = 0x7fe9e4008d00\r\n\r\n2019-06-20 00:01:07.445518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:02:00.0\r\n2019-06-20 00:01:07.445569: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.1\r\n2019-06-20 00:01:07.445589: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\n2019-06-20 00:01:07.445600: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10\r\n2019-06-20 00:01:07.445609: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10\r\n2019-06-20 00:01:07.445620: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10\r\n2019-06-20 00:01:07.445630: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10\r\n2019-06-20 00:01:07.445639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-20 00:01:07.446709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-20 00:01:07.446740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-20 00:01:07.446747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0\r\n2019-06-20 00:01:07.446751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n2019-06-20 00:01:07.448045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22664 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:02:00.0, compute capability: 7.5)\r\n2019-06-20 00:01:07.637419: I tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:632] Building a new TensorRT engine for import/TRTEngineOp_0 input shapes: [[1,224,224,3]]\r\nKeras time 2.380934238433838\r\nTensorflow time 0.5712037086486816\r\nPASSED\r\nTFTRT time 67.95880579948425\r\nPASSED\r\n```\r\n\r\nTo use the new TF-TRT API, you would have to do something like this:\r\n```\r\n    converter = tftrt.TrtGraphConverter(\r\n      input_graph_def=graph.frozen,\r\n      nodes_blacklist=graph.y_name,\r\n      max_batch_size=batch_size,\r\n      max_workspace_size_bytes=1 << 30,\r\n      precision_mode=precision,\r\n      minimum_segment_size=2)\r\n    tftrt_graph = converter.convert()\r\n    batch_size = 1\r\n    img_shape = (224, 224, 3)\r\n    x = np.random.random_sample((batch_size,\r\n      img_shape[0], img_shape[1], img_shape[2]))\r\n    tftrt_graph = converter.calibrate(\r\n      fetch_names=graph.y_name,\r\n      num_runs=10,\r\n      feed_dict_fn=lambda: {graph.x_name[0] + ':0': x[0: batch_size]})\r\n```", "This is fixed in 1.14."]}, {"number": 22513, "title": "Final version strings for 1.11.0", "body": "", "comments": []}, {"number": 22512, "title": "Installation issue with Tensorflow-cpu, no module named '_pwyrap_tensorflow_internal'", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 server in desktop mode\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu 1.10.0\r\n- **Python version**: Anaconda x64 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: Q4000\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\n1. Create a python application that imports tensorflow-gpu\r\n2. Package the application with pyinstaller\r\n3. Start the generated executable, that crashes while trying to import tensorflow.\r\n\r\nThe error trace is this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\***-ecouzwts\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\r\n    exec(bytecode, module.__dict__)\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n  File \"importlib\\__init__.py\", line 126, in import_module\r\nModuleNotFoundError: No module named 'tensorflow.python._pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nOn the other hand, **doing the same process with tensorflow(-cpu) works fine**. \r\n\r\nI ruled out CUDA/cuDNN installation issue, because if I run my python application using python, it works fine with the GPU version.\r\n\r\nI have tried checking at the .exe with dependency_walker, but appart from false-positive missing `API-MS-*` and `EXT-MS-*` DLLs, I don't see anything obvious.\r\n\r\nSo I'm stuck here.\r\nWhat discrepancy could create an import issue with tensorflow-gpu and not with tensorflow-cpu ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "I've updated the form accordingly", "@ymodak I have made some progress. It turns out, I was building tensorflow from an Anaconda shell. I removed Anaconda and installed standard python 3.6.6 64 bits.\r\n\r\nBy doing so, the initial problem went away.\r\nExecutable runs fine on the **build** machine.\r\nHowever, if I transfer the standalone application to another machine, it fails with the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-ecouzwts\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n  File \"importlib\\__init__.py\", line 126, in import_module\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"nxt\\nxt.py\", line 2, in <module>\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-ecouzwts\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\r\n  File \"site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-ecouzwts\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\r\n  File \"site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-ecouzwts\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\nImportError: Traceback (most recent call last):\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-ecouzwts\\lib\\site-packages\\PyInstaller\\loader\\pyimod03_importers.py\", line 627, in exec_module\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in <module>\r\n  File \"site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in swig_import_helper\r\n  File \"importlib\\__init__.py\", line 126, in import_module\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nHere is the code to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef main():\t\r\n\thello = tf.constant('hello TF')\r\n\tsess = tf.Session()\r\n\tprint(sess.run(hello))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nSteps to reproduce:\r\n1. Run the program with python 3.6 and tensorflow-gpu 1.10.0. Works ok.\r\n2. Package into a standalone with PyInstaller\r\n3. Run the executable. Works ok.\r\n4. Move the folder containing the executable and all dependencies to another machine.\r\n5. Run the executable. Fails with error above.", "I finally managed to catch the exception happening at import time, application is raising `EXCEPTION_ILLEGAL_INSTRUCTION`.\r\n\r\n<img width=\"906\" alt=\"exception_illegal_instruction\" src=\"https://user-images.githubusercontent.com/1294805/46080723-f6878100-c19a-11e8-9ea9-3c9d43aa1507.PNG\">\r\n\r\nCould this be related to AXV instructions not supported ? The processor of the VM is an [Intel Xeon E5-2623 v4](https://ark.intel.com/fr/products/92980/Intel-Xeon-Processor-E5-2623-v4-10M-Cache-2_60-GHz) that apparently supports AVX2.", "@Overdrivr This is not a bug. You can run some sanity checks on the second machine to make sure the environment is set up correctly and dependencies are installed properly.\r\n", "You're correct, there is a configuration issue on the second machine.\r\n\r\nI have installed python 3.6.6 on it, created a new virtual env, only containing tensorflow, and can reproduce the issue.\r\n\r\nWhat puzzles me is that I have installed **tensorflow-cpu** on it, and I still get `_pywrap_tensorflow_internal` not found (And it's not the first time I've installed TF on a machine, I've done it for CPU and GPU version a good dozen of times).\r\n\r\nI have ran the following sanity check (updated for tensorflow 1.10.0, Cuda 9.0, cuDNN 7.0.X):\r\n\r\n```\r\nimport ctypes\r\nimport imp\r\nimport sys\r\nfrom ctypes.util import find_library\r\n\r\ndef main():\r\n    check()\r\n\r\ndef check():\r\n    try:\r\n        import tensorflow as tf\r\n        print(\"TensorFlow successfully installed.\")\r\n        if tf.test.is_built_with_cuda():\r\n            print(\"The installed version of TensorFlow includes GPU support.\")\r\n        else:\r\n            print(\"The installed version of TensorFlow does not include GPU support.\")\r\n            #sys.exit(0)\r\n    except ImportError as e:\r\n        print(\"ERROR: Failed to import the TensorFlow module.\")\r\n        print('Reason: {}'.format(e))\r\n\r\n    candidate_explanation = False\r\n\r\n    python_version = sys.version_info.major, sys.version_info.minor\r\n    print(\"- Python version is %d.%d.\" % python_version)\r\n    if not (python_version == (3, 5) or python_version == (3, 6)):\r\n        candidate_explanation = True\r\n        print(\"- The official distribution of TensorFlow for Windows requires \"\r\n          \"Python version 3.5 or 3.6.\")\r\n\r\n    try:\r\n        _, pathname, _ = imp.find_module(\"tensorflow\")\r\n        print(\"- TensorFlow is installed at: %s\" % pathname)\r\n    except ImportError:\r\n        candidate_explanation = False\r\n        print(\"\"\"- No module named TensorFlow is installed in this Python environment. You may\r\ninstall it using the command `pip install tensorflow`.\"\"\")\r\n\r\n    try:\r\n        msvcp140 = ctypes.WinDLL(\"msvcp140.dll\")\r\n        msvcp140_path = find_library(\"msvcp140.dll\")\r\n        print('- msvcp140.dll Found at {}'.format(msvcp140_path))\r\n    except OSError:\r\n        candidate_explanation = True\r\n        print(\"\"\"\r\n- Could not load 'msvcp140.dll'. You may install this DLL by downloading Microsoft Visual\r\n  C++ 2015 Redistributable Update 3 from this URL:\r\n  https://www.microsoft.com/en-us/download/details.aspx?id=53587\"\"\")\r\n\r\n    try:\r\n        cudart64_90 = ctypes.WinDLL(\"cudart64_90.dll\")\r\n        cudart64_90_path = find_library(\"cudart64_90.dll\")\r\n        print('- Cuda 9.0 found at {}'.format(cudart64_90_path))\r\n        # TODO: Look for version.txt file in CUDA path\r\n    except OSError:\r\n        candidate_explanation = True\r\n        print(\"\"\"\r\n- Could not load 'cudart64_90.dll'. Download and install CUDA 9.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\"\"\")\r\n\r\n    try:\r\n        nvcuda = ctypes.WinDLL(\"nvcuda.dll\")\r\n        nvcuda_path = find_library(\"nvcuda.dll\")\r\n        print('- nvcuda.dll found at {}'.format(nvcuda_path))\r\n    except OSError:\r\n        candidate_explanation = True\r\n        print(\"\"\"\r\n- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that\r\n  this DLL be installed in a directory that is named in your %PATH%\r\n  environment variable. Typically it is installed in 'C:\\Windows\\System32'.\r\n  If it is not present, ensure that you have a CUDA-capable GPU with the\r\n  correct driver installed.\"\"\")\r\n\r\n    cudnn7_found = False\r\n    try:\r\n        cudnn7 = ctypes.WinDLL(\"cudnn64_7.dll\")\r\n        cudnn7_path = find_library(\"cudnn64_7.dll\")\r\n        print(\"- cuDNN Found at {}\".format(cudnn7_path))\r\n        cudnn7_found = True\r\n    except OSError:\r\n        candidate_explanation = True\r\n        print(\"\"\"\r\n- Could not load 'cudnn64_7.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Note that installing cuDNN is a\r\n  separate step from installing CUDA, and it is often found in a\r\n  different directory from the CUDA DLLs. You may install the\r\n  necessary DLL by downloading cuDNN 7.0 for Cuda 9.0 from this URL:\r\n  https://developer.nvidia.com/cudnn\"\"\")\r\n\r\n    if not candidate_explanation:\r\n        print(\"\"\"\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\"\"\")\r\n\r\n    sys.exit(-1)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nOutput:\r\n\r\n```\r\nERROR: Failed to import the TensorFlow module.\r\nReason: Traceback (most recent call last):\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test-tf-vXVQRlro\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test-tf-vXVQRlro\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test-tf-vXVQRlro\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test-tf-vXVQRlro\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test-tf-vXVQRlro\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test-tf-vXVQRlro\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n- Python version is 3.6.\r\n- TensorFlow is installed at: C:\\Users\\Paperspace\\.virtualenvs\\test-tf-vXVQRlro\\lib\\site-packages\\tensorflow\r\n- msvcp140.dll Found at C:\\Windows\\system32\\msvcp140.dll\r\n- Cuda 9.0 found at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudart64_90.dll\r\n- nvcuda.dll found at C:\\Windows\\system32\\nvcuda.dll\r\n- cuDNN Found at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\n```\r\n\r\nBasically, the sanity check does not highlight any error, yet there seems to be an issue loading some DLL. This is very weird, because I have installed TF-cpu and GPU on another machine with exactly the same specs two hours before that one (They are both freshly created machines on a cloud provider with the same base Windows 10 image).   ", "I have tried TF 1.5 and it solves the issue. TF can be imported just fine. So, clearly, it has be an issue with AVX.\r\n\r\nBut it is stated [here](https://www.intel.fr/content/www/fr/fr/products/processors/xeon/e5-processors/e5-2623-v4.html) (and everywhere else) that the Xeon E5-2623 v4 supports AVX2, so I guess it should support AVX too, right ?\r\n\r\nTo be sure, I wrote a small script that fetches CPUID and checks if the hardware supports AVX.\r\nInstall [cpuid](https://pypi.org/project/cpuid/) to run it.\r\n\r\n```\r\nfrom cpuid import *\r\n\r\ndef _is_set(id, reg_idx, bit):\r\n    regs = cpuid(id)\r\n\r\n    if (1 << bit) & regs[reg_idx]:\r\n        return \"Yes\"\r\n    else:\r\n        return \"--\"\r\n\r\nprint(\"Vendor ID         : %s\" % cpu_vendor())\r\nprint(\"CPU name          : %s\" % cpu_name())\r\nprint(\"Microarchitecture : %s%s\" % cpu_microarchitecture())\r\nprint(\"Vector instructions supported:\")\r\nprint(\"SSE       : %s\" % _is_set(1, 3, 25))\r\nprint(\"SSE2      : %s\" % _is_set(1, 3, 26))\r\nprint(\"SSE3      : %s\" % _is_set(1, 2, 0))\r\nprint(\"SSSE3     : %s\" % _is_set(1, 2, 9))\r\nprint(\"SSE4.1    : %s\" % _is_set(1, 2, 19))\r\nprint(\"SSE4.2    : %s\" % _is_set(1, 2, 20))\r\nprint(\"SSE4a     : %s\" % _is_set(0x80000001, 2, 6))\r\nprint(\"AVX       : %s\" % _is_set(1, 2, 28))\r\nprint(\"AVX2      : %s\" % _is_set(7, 1, 5))\r\nprint(\"BMI1      : %s\" % _is_set(7, 1, 3))\r\nprint(\"BMI2      : %s\" % _is_set(7, 1, 8))\r\n```\r\n\r\nAnd this is what I get:\r\n\r\n```\r\nVendor ID         : GenuineIntel\r\nCPU name          : Intel(R) Xeon(R) CPU E5-2623 v4 @ 2.60GHz\r\nMicroarchitecture : broadwellnoavx\r\nVector instructions supported:\r\nSSE       : Yes\r\nSSE2      : Yes\r\nSSE3      : Yes\r\nSSSE3     : Yes\r\nSSE4.1    : Yes\r\nSSE4.2    : Yes\r\nSSE4a     : --\r\nAVX       : --\r\nAVX2      : --\r\nBMI1      : Yes\r\nBMI2      : Yes\r\n```\r\n\r\nJust the name of the micro-architecture is pretty self-explanatory.\r\nNot only does he not support AVX2, he does not even support AVX1.\r\n\r\nI would like to give a big **THANKS** to intel for advertising their processor to support AVX when they clearly don't (and for wasting our time).\r\n", "While we're at it, I'm getting the same problem with tensorflow-cpu... on another freshly created machine.. that supports AVX.  Fun ride.\r\n\r\n![Emotionnal rollercoaster](https://media.giphy.com/media/kmwTZSI2ALst2/giphy.gif)\r\n\r\nI'm leaving this here for documentation, it might help others.\r\n\r\nIn my production virtualenv with tensorflow-cpu + a bunch of other packages, I get the following error (the output comes from a tensorflow-install-checker tool that I will release in public domain):\r\n\r\n```\r\nINFO - Vendor ID         : GenuineIntel\r\nINFO - CPU name          : Intel(R) Xeon(R) CPU E5-2623 v4 @ 2.60GHz\r\nINFO - Microarchitecture : broadwell\r\nOK  - Your processor supports AVX instructions\r\nERR - Failed to import the TensorFlow module.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nOK  - TensorFlow found at: c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\site-packages\\tensorflow\r\nOK  - msvcp140.dll found at: c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\site-packages\\scipy\\extra-dll\\msvcp140.dll\r\nOK  - Cuda 9.0 found at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudart64_90.dll\r\nOK  - nvcuda.dll found at C:\\Windows\\system32\\nvcuda.dll\r\nOK  - cuDNN Found at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n```\r\n\r\nIn another virtualenv with only tensorflow-cpu, I get this one:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test_tf--gQQvE-L\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 47, in preload_check\r\n    ctypes.WinDLL(build_info.msvcp_dll_name)\r\n  File \"C:\\Program Files\\Python36\\Lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test_tf--gQQvE-L\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test_tf--gQQvE-L\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test_tf--gQQvE-L\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\Paperspace\\.virtualenvs\\test_tf--gQQvE-L\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 55, in preload_check\r\n    % build_info.msvcp_dll_name)\r\nImportError: Could not find 'msvcp140.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. You may install this DLL by downloading Visual C++ 2015 Redistributable Update 3 from this URL: https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\n```\r\n\r\nHum weird right ? Let's summarize:\r\n\r\n- In the first env, `msvcp140.dll` is found, but TF fails to load\r\n- In the second env, `msvcp140.dll` is missing\r\n\r\nIf you look closely, the `msvcp140.dll` found in the first env does not come from `system32` folder.\r\nIt comes from scipy (`c:\\users\\paperspace\\.virtualenvs\\******-jrq2aiwp\\lib\\site-packages\\scipy\\extra-dll\\msvcp140.dll`).\r\nScipy is ([to solve other issues](https://github.com/scipy/scipy/issues/7969)) releasing `msvcp140.dll` in their extra-dll folder. This folder ends up in the PATH, and the dll in question picked up by the library finder.\r\n\r\nNow, why that's a problem:\r\n\r\n- For machines that already have this DLL in `system32` folder, it's fine because system path takes precedence over user path.\r\n- For machines without this DLL, instead of having TF crash with DLL missing, it crashes with some cryptic `_pywrap_tensorflow_internal...` error, most likely because the DLL found in scipy is different and not compatible that the latest installed by Microsoft Visual C++ 2015 Redistributable Update 3 RC. The workaround is then to simply install [Microsoft Visual C++ 2015 Redistributable 64 bits](https://www.microsoft.com/en-us/download/details.aspx?id=53840). This way, the DLL is installed in system32 folder and takes precedences over the one installed by scipy.\r\n\r\nAfter doing so on my VM, importing TF in both virtualenvs now runs properly. \ud83e\udd42 \r\n\r\n```\r\nINFO - Vendor ID         : GenuineIntel\r\nINFO - CPU name          : Intel(R) Xeon(R) CPU E5-2623 v4 @ 2.60GHz\r\nINFO - Microarchitecture : broadwell\r\nOK  - Your processor supports AVX instructions\r\nOK  - TensorFlow is functional.\r\nERR - TensorFlow does not include GPU support.\r\nOK  - TensorFlow found at: c:\\users\\paperspace\\.virtualenvs\\nexture-studio-dist-jrq2aiwp\\lib\\site-packages\\tensorflow\r\nOK  - msvcp140.dll found at: C:\\Windows\\system32\\msvcp140.dll\r\nOK  - Cuda 9.0 found at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudart64_90.dll\r\nOK  - nvcuda.dll found at C:\\Windows\\system32\\nvcuda.dll\r\nOK  - cuDNN Found at C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\\cudnn64_7.dll\r\n```\r\n\r\nMaybe it might be worth mentioning it in the install docs.", "@lamberta Can we add this requirement to our windows installation docs?\r\n\r\n> For machines without this DLL, instead of having TF crash with DLL missing, it crashes with some cryptic _pywrap_tensorflow_internal... error, most likely because the DLL found in scipy is different and not compatible that the latest installed by Microsoft Visual C++ 2015 Redistributable Update 3 RC. The workaround is then to simply install Microsoft Visual C++ 2015 Redistributable 64 bits. This way, the DLL is installed in system32 folder and takes precedences over the one installed by scipy.\r\n", "@gunan I can highlight the need to install Microsoft Visual C++ 2015 Redistributable and to make sure the DLL is installed in the system32 folder.\r\n\r\nCan also add the error message to this page: https://www.tensorflow.org/install/errors (and link to this issue).\r\n\r\nBut is this something tensorflow can detect and provide a more useful error message? \"Can not find msvcp140.dll in system32 directory\" (or something)\r\n", "Certainly. Both are things we need to do.", "Nagging Assignee @wt-huang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue as the PR is merged. ", "PLEASE HELP!!!\r\nI've tried everything, can't understand why on earth is this hapenning!!\r\nmy system instructions set:-----------------------------------------\r\nInstructions sets | MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, EM64T, VT-x, AES, AVX, AVX2, FMA3\r\n-----------------------------------------------------------------------\r\nWin10-64bit\r\nCore i5 8250U\r\nand using tensorflow cpu 1.12\r\n\r\nwhat I've tried so far\r\ndowngrading Tensorflow to 1.5 and 1.9\r\nInstalling C++ build tools\r\nI've already latest C++ redist 2017\r\n.................................................................................\r\ndon't know what to do else!!!!\r\n--------------------------------------------\r\nError reproduce on importing tensorflow\r\n\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nplease help!!!", "It's because it's not TF that fails to load, it's protobuf: https://github.com/protocolbuffers/protobuf/issues/5046#issuecomment-413726611\r\n\r\nApparently, downgrading to protobuf 3.6.0 (most likely you have 3.6.1) does the trick", "> It's because it's not TF that fails to load, it's protobuf: [protocolbuffers/protobuf#5046 (comment)](https://github.com/protocolbuffers/protobuf/issues/5046#issuecomment-413726611)\r\n> \r\n> Apparently, downgrading to protobuf 3.6.0 (most likely you have 3.6.1) does the trick\r\n\r\nThe problem exists for every version of tensorflow 1.5 - 1.12 (cpu). But downgrading protobuf from 3.6.1 to 3.6.0 does the trick. @Overdrivr thanks for the solution", ">>> import keras\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n**Please help me with this error**", "Downgrading protobuf to 3.6.0 bring me this error, don't know who should support this fix, protobuf or tensorflow...  : \r\ntensorflow-gpu 1.13.1 has requirement protobuf>=3.6.1, but you'll have protobuf 3.6.0 which is incompatible. ", "I am following https://towardsdatascience.com/python-environment-setup-for-deep-learning-on-windows-10-c373786e36d1 blog and I get the following error on writing import tensorflow: \r\n![image](https://user-images.githubusercontent.com/22957479/53945279-46258d80-40e7-11e9-8493-df8a1ea4abdc.png)\r\nPlease help. ", "@gargisri68 same issue. Have you solved the problem?", "No. I'm still struggling.\n\n[image: Mailtrack]\n<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>\nSender\nnotified by\nMailtrack\n<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>\n03/12/19,\n5:27:20 PM\n\nOn Tue, Mar 12, 2019 at 4:00 PM \u9c7c\u4e38\u7c97\u9762666 <notifications@github.com> wrote:\n\n> @gargisri68 <https://github.com/gargisri68> same issue. Have you solved\n> the problem?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22512#issuecomment-471942793>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AV5Np2bLmVqegok3Y-L1D1HvXDf9h6acks5vV4HhgaJpZM4W5htL>\n> .\n>\n", "I encountered this problem when I installed tensorflow-gpu 2.0.0a0. But tensorflow-cpu 2.0.0a0 works normally. After I reinstalled tensorflow-gpu1.12.0, it's fine. Even the tensorflow-gpu 1.13 will have the problem. Unknown reason. \ud83d\ude12 ", "I typed conda search tensorflow-gpu --info\n\nIt lists down different versions of tensorflow-gpu. From that list I kept\ninstalling and uninstalling unless finally tensorflow-gpu 1.8.0 worked.\n\nOn Tue, Mar 12, 2019, 5:50 PM \u9c7c\u4e38\u7c97\u9762666 <notifications@github.com> wrote:\n\n> I encountered this problem when I installed tensorflow-gpu 2.0.0a0. But\n> tensorflow-cpu 2.0.0a0 works normally. After I reinstalled\n> tensorflow-gpu1.12.0, it's fine. Even the tensorflow-gpu 1.13 will have the\n> problem. Unknown reason. \ud83d\ude12\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22512#issuecomment-471976305>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AV5Np1bx2VAT3cljxNI5kRYuaMK7p_D4ks5vV5txgaJpZM4W5htL>\n> .\n>\n", "@gargisri68, @yuwancumian666 solution seems to be installing CUDA 10.0, not CUDA 10.1 (actually, I left both of them installed). Windows 10 Pro in my case.\r\n\r\nI've checked _pywrap_tensorflow_internal.pyd with Dependency Walker and there were some missing DLLs from CUDA 10 in the list (such as cudart64_100.dll). So I've installed CUDA 10.\r\nAlso, I've added \"C:\\Program Files\\NVIDIA Corporation\\NvStreamSrv\" to my PATH in environment variables because one of the missing DLLs were found there. Not sure if it was necessary. \r\n\r\nNow tensorflow is successfully imported and my model is using GPU, according to the performance monitor.\r\n\r\nSo, if you have CUDA 10.1 but not CUDA 10.0 installed, I propose to give this solution a try. I'm using tensorflow-gpu==2.0.0-a0 but I had the same issue with DLL in 1.13. So I would expect that fix will work for 1.13 as well.", "I had a fresh install of tensorflow-gpu and all the other latest NVIDIA requirements. Only had to downgrade CUDA to 10.0 from 10.1 for it to work.", "Here's a list of all of the CUDA packages to download:\r\n\r\nhttps://developer.nvidia.com/cuda-toolkit-archive", "i had the same issue using (conda install tensorflow-gpu) solve the problem.\r\ncuda 10.0\r\ncudnn 7.41.5 for cuda 10.0\r\npython 7.1\r\n", "Similar here on cpu\r\nIt work fine since a while and start to fail with no modification on my set up...\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\cy6112\\AppData\\Local\\Continuum\\anaconda3\\envs\\TimeSeriesForcasting\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\cy6112\\AppData\\Local\\Continuum\\anaconda3\\envs\\TimeSeriesForcasting\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\cy6112\\AppData\\Local\\Continuum\\anaconda3\\envs\\TimeSeriesForcasting\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\cy6112\\AppData\\Local\\Continuum\\anaconda3\\envs\\TimeSeriesForcasting\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\cy6112\\AppData\\Local\\Continuum\\anaconda3\\envs\\TimeSeriesForcasting\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Le module sp\u00e9cifi\u00e9 est introuvable.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "Here is the archived version of cuda-10.0. You would think Google would have this link to help folks download the correct version which in this case is not the most recent stable version of cuda\r\n\r\nhttps://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal", "> Here is the archived version of cuda-10.0. You would think Google would have this link to help folks download the correct version which in this case is not the most recent stable version of cuda\r\n> \r\n> https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal\r\n\r\nit may work for the one working on GPU, but im working on CPU... it is the same issue?\r\n", " This was the fix for the GPU issues that I had as well as others. If you are trying to install the GPU version of Tensorflow to a computer that does not have a GPU then that may be your issue but I doubt that is the problem.Rick\n\n    On Friday, April 5, 2019, 6:48:11 AM PDT, jnthnroy <notifications@github.com> wrote:  \n \n \n\nHere is the archived version of cuda-10.0. You would think Google would have this link to help folks download the correct version which in this case is not the most recent stable version of cuda\n\nhttps://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal\n\n\nit may work for the one working on GPU, but im working on CPU... it is the same issue?\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n   ", "I tried many conbinations, finally,\r\nTF 1.12, cuda 9.0, cudnn 7.0 can successfully work. \r\nI suggest don't try cuda 10.1 since the version may not compatible with old TF version", "I experienced the same issue after upgrading tensorflow-gpu. After uninstalling tensorflow and tensorflow-gpu and reinstalling tenorflow, it works  again", "If you have this issue you can downgrade the lib to tensorflow-gpu==1.10.0, at today the superior versions have problems, this issue is common in windows.\r\nPD. after that you need CUDA to use this lib correctly.", "> @gargisri68, @yuwancumian666 solution seems to be installing CUDA 10.0, not CUDA 10.1 (actually, I left both of them installed). Windows 10 Pro in my case.\r\n> \r\n> I've checked _pywrap_tensorflow_internal.pyd with Dependency Walker and there were some missing DLLs from CUDA 10 in the list (such as cudart64_100.dll). So I've installed CUDA 10.\r\n> Also, I've added \"C:\\Program Files\\NVIDIA Corporation\\NvStreamSrv\" to my PATH in environment variables because one of the missing DLLs were found there. Not sure if it was necessary.\r\n> \r\n> Now tensorflow is successfully imported and my model is using GPU, according to the performance monitor.\r\n> \r\n> So, if you have CUDA 10.1 but not CUDA 10.0 installed, I propose to give this solution a try. I'm using tensorflow-gpu==2.0.0-a0 but I had the same issue with DLL in 1.13. So I would expect that fix will work for 1.13 as well.\r\n\r\nman you saved my sleep tonight.\r\n- tensorflow 2.0.0-alpha0\r\n- cuda **10.1->10.0**\r\n- cudnn **7.6.1 -> 7.4.1**\r\n- python 3.7\r\n\r\n", "What a pain in the ass.  One would expect a company like Google to have a straightforward installation and dependency management protocol for installing their own product.\r\n\r\nReally disappointed and frustrated by the frequency of TF install errors and breaks.\r\n\r\nHow is there so much confusion and problems just to get a framework up and running.   Literally hours upon hours of time sunk.   Maybe this is by design?\r\n\r\nTF team, can we just get a very straightforward installation/configuration process?  Ideally one that integrates well into a virtual environment like Conda?  This can't honestly be that difficult, right?", "I am unable to get rid from this error\r\n\r\n>>>import tensorflow\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: %1 is not a valid Win32 application.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Harsh\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Harsh\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: %1 is not a valid Win32 application.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "I also get the same error.\r\nSomehow upgrading TensorFlow to version1.14.0 solves the problem  :)", "I am stuck on this issue all day today\r\nIn the virtual environment I am able to successfully import Tensorflow 2 but within Visual Studio Jupyter notebook I am getting the below error\r\n\r\n`_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)`\r\n`  File \"C:\\Users\\IT\\envs\\tf1env\\lib\\imp.py\", line 242, in load_module`\r\n `   return load_dynamic(name, filename, file)`\r\n`  File \"C:\\Users\\IT\\envs\\tf1env\\lib\\imp.py\", line 342, in load_dynamic`\r\n`    return _load(spec)`\r\n`ImportError: DLL load failed: The specified module could not be found.`\r\n\r\nI am using Tensorflow 2, CUDA 10.1 and cuDNN 7.6.5", "The prebuilt TF packages for tf 2.0 require cuda 10.0, not 10.1\r\n", "Had this issue with Tensorflow 2.1.0 (CPU version). Managed to fix it by downgrading to Tensorflow 2.0.0:\r\n```\r\npip install tensorflow==2.0.0\r\n```", "> \r\n> \r\n> Had this issue with Tensorflow 2.1.0 (CPU version). Managed to fix it by downgrading to Tensorflow 2.0.0:\r\n> \r\n> ```\r\n> pip install tensorflow==2.0.0\r\n> ```\r\n\r\nJust need you to know that I've been banging my head against this all day. I even went through and did a full on conda install anaconda-clean uninstall, just blowing things apart. \r\n\r\nThe thing that finally worked was just this little downgrade. \r\nI'm using the CPU version for the record. Hope this helps someone else. ", "> Had this issue with Tensorflow 2.1.0 (CPU version). Managed to fix it by downgrading to Tensorflow 2.0.0:\r\n> \r\n> ```\r\n> pip install tensorflow==2.0.0\r\n> ```\r\n\r\nYou made my day. Thanks.", "Because of further issues with PyInstaller I ended up downgrading to `tensorflow == 1.14.0`.", "Same problem for tensorflow==2.1 @ Win 10.\r\n\r\n- NVIDIA\u00ae GPU drivers: 441.66\r\n- CUDA\u00ae Toolkit: 10.1 (as requested by https://www.tensorflow.org/install/gpu)\r\n- cuDNN SDK: 7.6.5\r\n\r\nAny \"import tensorflow\" results in:\r\n\r\n> File \"..\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n>     return _load(spec)\r\n> ImportError: DLL load failed: The specified module was not found.\r\n\r\nDowngrading to CUDA\u00ae Toolkit v10.0 did not solve the problem. \r\nDowngrading to tensorflow==2.0.0 did help. @wedesoft", "Hi guys tried installing tensorflow 2.0.0 - it has changed my error from DLL load to something else (see below).\r\n\r\nPlease could someone help? And let me know if I need to provide more info (first time posting here)\r\n\r\nUsing TensorFlow backend.\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-4-2295cf4c5c1d>\", line 7, in <module>\r\n    import keras\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 6, in <module>\r\n    from tensorflow.python.eager import context\r\nImportError: cannot import name 'context' from 'tensorflow.python.eager' (C:\\Users\\rahan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\eager\\__init__.py)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'ImportError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"C:\\Users\\rahan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n  File \"C:\\Users\\rahan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\nImportError: cannot import name 'audio' from 'tensorflow_core._api.v2' (C:\\Users\\rahan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\_api\\v2\\__init__.py)\r\n\r\n", "> > Tensorflow 2.1.0\uff08CPU\u7248\u672c\uff09\u5b58\u5728\u6b64\u95ee\u9898\u3002\u901a\u8fc7\u964d\u7ea7\u5230Tensorflow 2.0.0\u8fdb\u884c\u4e86\u4fee\u590d\uff1a\r\n> > ```\r\n> > pip install tensorflow==2.0.0\r\n> > ```\r\n> \r\n> \u4f60\u8ba9\u6211\u4eca\u5929\u4e00\u6574\u5929\u90fd\u611f\u89c9\u5f88\u597d\u3002\u8c22\u8c22\u3002\r\n\r\n\u6211\u8fd9\u6837\u505a\u4e86\u3002\u4f46\u8fd8\u662f\u65e0\u6cd5\u89e3\u51b3\u95ee\u9898", "I upgraded to free version of Microsoft Visual Studio Community 2019 Version 16.4.5 including Microsoft Visual C++ 2019 then reinstalled TensorFlow 2.1.0 and it worked without issues.  I really needed Tensorflow 2.1.0 for an application I wanted to test. \r\nRick", "> > > Tensorflow 2.1.0\uff08CPU\u7248\u672c\uff09\u5b58\u5728\u6b64\u95ee\u9898\u3002\u901a\u8fc7\u964d\u7ea7\u5230Tensorflow 2.0.0\u8fdb\u884c\u4e86\u4fee\u590d\uff1a\r\n> > > ```\r\n> > > pip install tensorflow==2.0.0\r\n> > > ```\r\n> > \r\n> > \r\n> > \u4f60\u8ba9\u6211\u4eca\u5929\u4e00\u6574\u5929\u90fd\u611f\u89c9\u5f88\u597d\u3002\u8c22\u8c22\u3002\r\n> \r\n> \u6211\u8fd9\u6837\u505a\u4e86\u3002\u4f46\u8fd8\u662f\u65e0\u6cd5\u89e3\u51b3\u95ee\u9898\r\n\u6211\u53ef\u4ee5\u4e86 \u611f\u8c22\r\n", "pip install tensorflow==2.0.0\r\nsolved the problem :) ", "(3) E:\\GitHub\\3\\EasyRL>python demo/run_dqn_on_pong.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"demo/run_dqn_on_pong.py\", line 20, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Travis\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nI have no idea what I can do with the error message... \r\n`Win 10, Python 3.6`", "> I upgraded to free version of Microsoft Visual Studio Community 2019 Version 16.4.5 including Microsoft Visual C++ 2019 then reinstalled TensorFlow 2.1.0 and it worked without issues. I really needed Tensorflow 2.1.0 for an application I wanted to test.\r\n> Rick\r\n\r\nInstalling Microsoft Visual Studio 2019 community version worked for me on too. Thanks! I am using Tensorflow 2.1.0 + CUDA 10.1", "HI,\r\nPlease For my Case:: \r\n**windows 10 , CPU only , python 3.7 .**\r\ni have read all the issue and the another issue was puplished with this one off TF site, i have tried many things but having same error.\r\ni have installed **Microsoft Visual C++ 2019** too.\r\nWhat Else, any help?\r\n\r\n===========\r\nI'm also tried to dowwngrade the eversion from 2.1.0 to 2.0.0 but got this error\r\n\r\n> ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\hebagamaleldin\\\\anaconda3-1\\\\lib\\\\site-packages\\\\~ensorflow_core\\\\python\\\\_pywrap_tensorflow_internal.pyd'\r\n> Consider using the `--user` option or check the permissions.", "> HI,\r\n> Please For my Case::\r\n> **windows 10 , CPU only , python 3.7 .**\r\n> i have read all the issue and the another issue was puplished with this one off TF site, i have tried many things but having same error.\r\n> i have installed **Microsoft Visual C++ 2019** too.\r\n> What Else, any help?\r\n> \r\n> ===========\r\n> I'm also tried to dowwngrade the eversion from 2.1.0 to 2.0.0 but got this error\r\n> \r\n> > ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\users\\hebagamaleldin\\anaconda3-1\\lib\\site-packages\\~ensorflow_core\\python\\_pywrap_tensorflow_internal.pyd'\r\n> > Consider using the `--user` option or check the permissions.\r\n\r\nSimply running the command-prompt/anaconda-prompt as administrator should solve the problem of the downgrade permission.", "I have tried all the above methods. But still getting this error.\r\n\r\nI have a doubt. Actually what are the things to be installed to get tensorflow work?\r\nIs not we install tensorflow with pip enough? But that shows this error.\r\n\r\nI even have visual c++ redistributable 2005. And tried changing different versions of tensorflow.\r\n\r\n>>>Python 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nimport tensorflow\r\nTraceback (most recent call last):\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n>>>During handling of the above exception, another exception occurred:\r\n\r\n>>>Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"F:\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\r\n    from tensorflow_core import *\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"F:\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"F:\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"F:\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"F:\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"F:\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\n>>>Failed to load the native TensorFlow runtime.", "After successfully installed tensorflow. I get the same error again n again...........\r\n\r\nC:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\Scripts>python\r\nPython 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\_api\\v2\\audio\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\ritu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "> I have tried all the above methods. But still getting this error.\r\n> \r\n> I have a doubt. Actually what are the things to be installed to get tensorflow work?\r\n> Is not we install tensorflow with pip enough? But that shows this error.\r\n> \r\n> I even have visual c++ redistributable 2005. And tried changing different versions of tensorflow.\r\n> \r\n> > > > Python 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)] on win32\r\n> > > > Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> > > > import tensorflow\r\n> > > > Traceback (most recent call last):\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in \r\n> > > > from tensorflow.python.pywrap_tensorflow_internal import *\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in \r\n> > > > _pywrap_tensorflow_internal = swig_import_helper()\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n> > > > _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> > > > File \"F:\\lib\\imp.py\", line 242, in load_module\r\n> > > > return load_dynamic(name, filename, file)\r\n> > > > File \"F:\\lib\\imp.py\", line 342, in load_dynamic\r\n> > > > return _load(spec)\r\n> > > > ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> > > > During handling of the above exception, another exception occurred:\r\n> \r\n> > > > Traceback (most recent call last):\r\n> > > > File \"\", line 1, in \r\n> > > > File \"F:\\lib\\site-packages\\tensorflow__init__.py\", line 99, in \r\n> > > > from tensorflow_core import *\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core__init__.py\", line 28, in \r\n> > > > from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow__init__.py\", line 50, in **getattr**\r\n> > > > module = self._load()\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow__init__.py\", line 44, in _load\r\n> > > > module = _importlib.import_module(self.**name**)\r\n> > > > File \"F:\\lib\\importlib__init__.py\", line 127, in import_module\r\n> > > > return _bootstrap._gcd_import(name[level:], package, level)\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python__init__.py\", line 49, in \r\n> > > > from tensorflow.python import pywrap_tensorflow\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in \r\n> > > > raise ImportError(msg)\r\n> > > > ImportError: Traceback (most recent call last):\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in \r\n> > > > from tensorflow.python.pywrap_tensorflow_internal import *\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in \r\n> > > > _pywrap_tensorflow_internal = swig_import_helper()\r\n> > > > File \"F:\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n> > > > _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> > > > File \"F:\\lib\\imp.py\", line 242, in load_module\r\n> > > > return load_dynamic(name, filename, file)\r\n> > > > File \"F:\\lib\\imp.py\", line 342, in load_dynamic\r\n> > > > return _load(spec)\r\n> > > > ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> > > > Failed to load the native TensorFlow runtime.\r\n\r\nLook if you try to install version 2.0.0 so you must Update pip first then install tensorflow or tensorflow-gpu will work.\r\nIf you have the same problem you probably have a CPU issue with the new version (if your laptop isn't core i3 or i5 or i7, i mean your processor is celeron or something earlier than core i3) in thia case try to install version 1.15 and it will work correctly .\r\nIf you try to install version 2.0.1 so you must install visual C++ 2015-2019\r\n\r\nResource :: https://www.tensorflow.org/install/pip?lang=python3", "This fixed it for me:\r\nWindows 7 or later (64-bit) (Python 3 only)\r\nMicrosoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019", "> Had this issue with Tensorflow 2.1.0 (CPU version). Managed to fix it by downgrading to Tensorflow 2.0.0:\r\n> \r\n> ```\r\n> pip install tensorflow==2.0.0\r\n> ```\r\nthank you so muuuuch\r\n", "This also worked for me after downgrading from 2.1.0.\r\n\r\n", "Check that all the dependencies are installed. I was missing VC redist. ", "> This fixed it for me:\r\n> Windows 7 or later (64-bit) (Python 3 only)\r\n> Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019\r\n\r\nThis worked also for me on 2.1.0\r\nPython 3.6.5", "Hi.  Semi-experiencde programmer, new to Python, novice Linux user taking a course on tensorflow, but stuck.  For context, I'm using Jupyter notebooks because that is the tool the online training videos are using, and easy to follow along in real time.  This is my first post to github too. This thread has been helpful, as I've seen similar errors, but downgrading to 2.0.0 created other issues.  Please read on.\r\n\r\n**SETUP:**\r\nWindows 10 Pro 10.0.18363\r\nDell Optiplex 5060\r\nIntel i7 8700 (x64)\r\nAnaconda 1.9.12 (Navigator)\r\nPython 3.7.6 (installed with Anaconda)\r\nRunning python through Jupyter Notebooks (installed with Anaconda)\r\ninstalled tensorflow using Anaconda powershell prompt (pip install tensorflow) v2.2.0 installed\r\nRunning through Anaconda \"base\" environment\r\n\r\n**ISSUE**\r\nWhen I run the single line, \"import tensorflow as tf\", in a Jupyter notebook, I get the following error message below... which sounds very familiar to other issues people have had above.\r\n\r\n----------\r\nImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59 \r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\nImportError: DLL load failed: The specified module could not be found.\r\nDuring handling of the above exception, another exception occurred:\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     48 import numpy as np\r\n     49 \r\n---> 50 from tensorflow.python import pywrap_tensorflow\r\n     51 \r\n     52 # Protocol buffers\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     67 for some common reasons and solutions.  Include the entire stack trace\r\n     68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 69   raise ImportError(msg)\r\n     70 \r\n     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n-------\r\n\r\nI tried downgrading to tensorflow 2.0.0 using \"pip install tensorflow==2.0.0\" from the Anaconda powershell window.  Going back to the same Jupyter notebook, \"import tensorflow as tf\" executed without any errors. Great.  But then I continued on in the training and found the tensorflow constant and variable methods threw errors.  Code and errors below \r\n\r\n**CODE - Constant**\r\n---------------------------------------------------------------------------\r\nconst_1 = tf.constant(value=[5.0],\r\n                      dtype=tf.float32,\r\n                      shape=(1,),\r\n                      name=\"const_1\",\r\n                      verify_shape=False)\r\nprint(const_1)\r\n\r\n**ERROR**\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-3af6baef2472> in <module>\r\n      3                       shape=(1,),\r\n      4                       name=\"const_1\",\r\n----> 5                       verify_shape=False)\r\n      6 print(const_1)\r\n\r\nTypeError: constant() got an unexpected keyword argument 'verify_shape'\r\n\r\n--------\r\n\r\n**CODE - variable**\r\n---------------------------------------------------------------------------\r\nvar_1 = tf.Variable(initial_value=[1.0],\r\n                   trainable=True,\r\n                   collections=None,\r\n                   validate_shape=True,\r\n                   caching_device=None,\r\n                   name='var_1',\r\n                   variable_def=None,\r\n                   dtype=tf.float32,\r\n                   expected_shape=(1,),\r\n                   import_scope=None)\r\nprint(var_1)\r\n\r\n**ERROR 1**\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-983b260f37f0> in <module>\r\n      8                    dtype=tf.float32,\r\n      9                    expected_shape=(1,),\r\n---> 10                    import_scope=None)\r\n     11 print(var_1)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in __call__(cls, *args, **kwargs)\r\n    258       return cls._variable_v1_call(*args, **kwargs)\r\n    259     elif cls is Variable:\r\n--> 260       return cls._variable_v2_call(*args, **kwargs)\r\n    261     else:\r\n    262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\nTypeError: _variable_v2_call() got an unexpected keyword argument 'collections'\r\n\r\n-------\r\n\r\n**ERROR 2 (after removing _collections_ argument)**\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-5-2048f7ef949b> in <module>\r\n      8                    dtype=tf.float32,\r\n      9                    expected_shape=(1,),\r\n---> 10                    import_scope=None)\r\n     11 print(var_1)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py in __call__(cls, *args, **kwargs)\r\n    258       return cls._variable_v1_call(*args, **kwargs)\r\n    259     elif cls is Variable:\r\n--> 260       return cls._variable_v2_call(*args, **kwargs)\r\n    261     else:\r\n    262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\nTypeError: _variable_v2_call() got an unexpected keyword argument 'expected_shape'\r\n\r\n-------\r\nAnd if I remove the _verify_shape_ argument from constant and _collections_ and _expected_shape_ from variable, they execute fine, but I fear I will run into other issues eventually.  Rather than continuing this guessing game, I thought I'd check here to see if anyone has any thoughts. Any help is GREATLY appreciated.  I ran \"pip list\" and pasted below in case there any version issues.  Thanks in advance!\r\n\r\n-----\r\nPackage                            Version\r\n-----------------------------------------------------\r\nabsl-py                            0.9.0\r\nalabaster                          0.7.12\r\nanaconda-client                    1.7.2\r\nanaconda-navigator                 1.9.12\r\nanaconda-project                   0.8.3\r\nargh                               0.26.2\r\nasn1crypto                         1.3.0\r\nastor                              0.8.1\r\nastroid                            2.3.3\r\nastropy                            4.0\r\nastunparse                         1.6.3\r\natomicwrites                       1.3.0\r\nattrs                              19.3.0\r\nautopep8                           1.4.4\r\nBabel                              2.8.0\r\nbackcall                           0.1.0\r\nbackports.functools-lru-cache      1.6.1\r\nbackports.shutil-get-terminal-size 1.0.0\r\nbackports.tempfile                 1.0\r\nbackports.weakref                  1.0.post1\r\nbcrypt                             3.1.7\r\nbeautifulsoup4                     4.8.2\r\nbitarray                           1.2.1\r\nbkcharts                           0.2\r\nbleach                             3.1.0\r\nbokeh                              1.4.0\r\nboto                               2.49.0\r\nBottleneck                         1.3.2\r\ncachetools                         4.1.0\r\ncertifi                            2019.11.28\r\ncffi                               1.14.0\r\nchardet                            3.0.4\r\nClick                              7.0\r\ncloudpickle                        1.3.0\r\nclyent                             1.2.2\r\ncolorama                           0.4.3\r\ncomtypes                           1.1.7\r\nconda                              4.8.2\r\nconda-build                        3.18.11\r\nconda-package-handling             1.6.0\r\nconda-verify                       3.4.2\r\ncontextlib2                        0.6.0.post1\r\ncryptography                       2.8\r\ncycler                             0.10.0\r\nCython                             0.29.15\r\ncytoolz                            0.10.1\r\ndask                               2.11.0\r\ndecorator                          4.4.1\r\ndefusedxml                         0.6.0\r\ndiff-match-patch                   20181111\r\ndistributed                        2.11.0\r\ndocutils                           0.16\r\nentrypoints                        0.3\r\net-xmlfile                         1.0.1\r\nfastcache                          1.1.0\r\nfilelock                           3.0.12\r\nflake8                             3.7.9\r\nFlask                              1.1.1\r\nfsspec                             0.6.2\r\nfuture                             0.18.2\r\ngast                               0.2.2\r\ngevent                             1.4.0\r\nglob2                              0.7\r\ngoogle-auth                        1.14.2\r\ngoogle-auth-oauthlib               0.4.1\r\ngoogle-pasta                       0.2.0\r\ngreenlet                           0.4.15\r\ngrpcio                             1.28.1\r\nh5py                               2.10.0\r\nHeapDict                           1.0.1\r\nhtml5lib                           1.0.1\r\nhypothesis                         5.5.4\r\nidna                               2.8\r\nimageio                            2.6.1\r\nimagesize                          1.2.0\r\nimportlib-metadata                 1.5.0\r\nintervaltree                       3.0.2\r\nipykernel                          5.1.4\r\nipython                            7.12.0\r\nipython-genutils                   0.2.0\r\nipywidgets                         7.5.1\r\nisort                              4.3.21\r\nitsdangerous                       1.1.0\r\njdcal                              1.4.1\r\njedi                               0.14.1\r\nJinja2                             2.11.1\r\njoblib                             0.14.1\r\njson5                              0.9.1\r\njsonschema                         3.2.0\r\njupyter                            1.0.0\r\njupyter-client                     5.3.4\r\njupyter-console                    6.1.0\r\njupyter-core                       4.6.1\r\njupyterlab                         1.2.6\r\njupyterlab-server                  1.0.6\r\nKeras-Applications                 1.0.8\r\nKeras-Preprocessing                1.1.0\r\nkeyring                            21.1.0\r\nkiwisolver                         1.1.0\r\nlazy-object-proxy                  1.4.3\r\nlibarchive-c                       2.8\r\nllvmlite                           0.31.0\r\nlocket                             0.2.0\r\nlxml                               4.5.0\r\nMarkdown                           3.2.1\r\nMarkupSafe                         1.1.1\r\nmatplotlib                         3.1.3\r\nmccabe                             0.6.1\r\nmenuinst                           1.4.16\r\nmistune                            0.8.4\r\nmkl-fft                            1.0.15\r\nmkl-random                         1.1.0\r\nmkl-service                        2.3.0\r\nmock                               4.0.1\r\nmore-itertools                     8.2.0\r\nmpmath                             1.1.0\r\nmsgpack                            0.6.1\r\nmultipledispatch                   0.6.0\r\nnavigator-updater                  0.2.1\r\nnbconvert                          5.6.1\r\nnbformat                           5.0.4\r\nnetworkx                           2.4\r\nnltk                               3.4.5\r\nnose                               1.3.7\r\nnotebook                           6.0.3\r\nnumba                              0.48.0\r\nnumexpr                            2.7.1\r\nnumpy                              1.18.1\r\nnumpydoc                           0.9.2\r\noauthlib                           3.1.0\r\nolefile                            0.46\r\nopenpyxl                           3.0.3\r\nopt-einsum                         3.2.1\r\npackaging                          20.1\r\npandas                             1.0.1\r\npandocfilters                      1.4.2\r\nparamiko                           2.7.1\r\nparso                              0.5.2\r\npartd                              1.1.0\r\npath                               13.1.0\r\npathlib2                           2.3.5\r\npathtools                          0.1.2\r\npatsy                              0.5.1\r\npep8                               1.7.1\r\npexpect                            4.8.0\r\npickleshare                        0.7.5\r\nPillow                             7.0.0\r\npip                                20.0.2\r\npkginfo                            1.5.0.1\r\npluggy                             0.13.1\r\nply                                3.11\r\nprometheus-client                  0.7.1\r\nprompt-toolkit                     3.0.3\r\nprotobuf                           3.11.3\r\npsutil                             5.6.7\r\npy                                 1.8.1\r\npyasn1                             0.4.8\r\npyasn1-modules                     0.2.8\r\npycodestyle                        2.5.0\r\npycosat                            0.6.3\r\npycparser                          2.19\r\npycrypto                           2.6.1\r\npycurl                             7.43.0.5\r\npydocstyle                         4.0.1\r\npyflakes                           2.1.1\r\nPygments                           2.5.2\r\npylint                             2.4.4\r\nPyNaCl                             1.3.0\r\npyodbc                             4.0.0-unsupported\r\npyOpenSSL                          19.1.0\r\npyparsing                          2.4.6\r\npyreadline                         2.1\r\npyrsistent                         0.15.7\r\nPySocks                            1.7.1\r\npytest                             5.3.5\r\npytest-arraydiff                   0.3\r\npytest-astropy                     0.8.0\r\npytest-astropy-header              0.1.2\r\npytest-doctestplus                 0.5.0\r\npytest-openfiles                   0.4.0\r\npytest-remotedata                  0.3.2\r\npython-dateutil                    2.8.1\r\npython-jsonrpc-server              0.3.4\r\npython-language-server             0.31.7\r\npytz                               2019.3\r\nPyWavelets                         1.1.1\r\npywin32                            227\r\npywin32-ctypes                     0.2.0\r\npywinpty                           0.5.7\r\nPyYAML                             5.3\r\npyzmq                              18.1.1\r\nQDarkStyle                         2.8\r\nQtAwesome                          0.6.1\r\nqtconsole                          4.6.0\r\nQtPy                               1.9.0\r\nrequests                           2.22.0\r\nrequests-oauthlib                  1.3.0\r\nrope                               0.16.0\r\nrsa                                4.0\r\nRtree                              0.9.3\r\nruamel-yaml                        0.15.87\r\nscikit-image                       0.16.2\r\nscikit-learn                       0.22.1\r\nscipy                              1.4.1\r\nseaborn                            0.10.0\r\nSend2Trash                         1.5.0\r\nsetuptools                         45.2.0.post20200210\r\nsimplegeneric                      0.8.1\r\nsingledispatch                     3.4.0.3\r\nsix                                1.14.0\r\nsnowballstemmer                    2.0.0\r\nsortedcollections                  1.1.2\r\nsortedcontainers                   2.1.0\r\nsoupsieve                          1.9.5\r\nSphinx                             2.4.0\r\nsphinxcontrib-applehelp            1.0.1\r\nsphinxcontrib-devhelp              1.0.1\r\nsphinxcontrib-htmlhelp             1.0.2\r\nsphinxcontrib-jsmath               1.0.1\r\nsphinxcontrib-qthelp               1.0.2\r\nsphinxcontrib-serializinghtml      1.1.3\r\nsphinxcontrib-websupport           1.2.0\r\nspyder                             4.0.1\r\nspyder-kernels                     1.8.1\r\nSQLAlchemy                         1.3.13\r\nstatsmodels                        0.11.0\r\nsympy                              1.5.1\r\ntables                             3.6.1\r\ntblib                              1.6.0\r\ntensorboard                        2.0.2\r\ntensorboard-plugin-wit             1.6.0.post3\r\ntensorflow                         2.0.0\r\ntensorflow-estimator               2.0.1\r\ntermcolor                          1.1.0\r\nterminado                          0.8.3\r\ntestpath                           0.4.4\r\ntoolz                              0.10.0\r\ntornado                            6.0.3\r\ntqdm                               4.42.1\r\ntraitlets                          4.3.3\r\nujson                              1.35\r\nunicodecsv                         0.14.1\r\nurllib3                            1.25.8\r\nwatchdog                           0.10.2\r\nwcwidth                            0.1.8\r\nwebencodings                       0.5.1\r\nWerkzeug                           1.0.0\r\nwheel                              0.34.2\r\nwidgetsnbextension                 3.5.1\r\nwin-inet-pton                      1.1.0\r\nwin-unicode-console                0.5\r\nwincertstore                       0.2\r\nwrapt                              1.11.2\r\nxlrd                               1.2.0\r\nXlsxWriter                         1.2.7\r\nxlwings                            0.17.1\r\nxlwt                               1.3.0\r\nxmltodict                          0.12.0\r\nyapf                               0.28.0\r\nzict                               1.0.0\r\nzipp                               2.2.0\r\n\r\n\r\n\r\n", "I see an article and I \uff0cfinally solve this problem.Win10,anaconda3,tensorflow2.2.0.\r\n\r\n\r\nenter cmd\r\npip uninstall pillow\r\nconda update pip  \r\n pip install pillow \r\n\r\n\r\nthat's all.I hope it will help you", "\r\npip install tensorflow==2.0.0\r\nsolved my problem", "how can I solve this problem\r\n\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\HP\\anaconda7\\envs\\PY3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\HP\\anaconda7\\envs\\PY3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\HP\\anaconda7\\envs\\PY3-TF2.0\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\HP\\anaconda7\\envs\\PY3-TF2.0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\HP\\anaconda7\\envs\\PY3-TF2.0\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "> > > > import keras\r\n> > > > Using TensorFlow backend.\r\n> > > > Traceback (most recent call last):\r\n> > > > File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in \r\n> > > > from tensorflow.python.pywrap_tensorflow_internal import *\r\n> > > > File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in \r\n> > > > _pywrap_tensorflow_internal = swig_import_helper()\r\n> > > > File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n> > > > _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> > > > File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 243, in load_module\r\n> > > > return load_dynamic(name, filename, file)\r\n> > > > File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 343, in load_dynamic\r\n> > > > return _load(spec)\r\n> > > > ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n> File \"\", line 1, in\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras__init__.py\", line 3, in\r\n> from . import utils\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils__init__.py\", line 6, in\r\n> from . import conv_utils\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in\r\n> from .. import backend as K\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend__init__.py\", line 89, in\r\n> from .tensorflow_backend import *\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in\r\n> import tensorflow as tf\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow__init__.py\", line 24, in\r\n> from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python__init__.py\", line 49, in\r\n> from tensorflow.python import pywrap_tensorflow\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in\r\n> raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in\r\n> from tensorflow.python.pywrap_tensorflow_internal import *\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in\r\n> _pywrap_tensorflow_internal = swig_import_helper()\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n> _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 243, in load_module\r\n> return load_dynamic(name, filename, file)\r\n> File \"C:\\Users\\Rajesh Kumar Khatik\\AppData\\Local\\Programs\\Python\\Python36\\Lib\\imp.py\", line 343, in load_dynamic\r\n> return _load(spec)\r\n> ImportError: DLL load failed: The specified module could not be found.\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions. Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> **Please help me with this error**\r\n\r\nsame error \r\nIm using windows 10\r\npython 3.7.8 ", ">>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\HP\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "I am having this error. How to solve this?", "Same problem here. Somewhere hidden in the long thread above there is the hint to go back to tensorflow 2.0.0:\r\n  pip install tensorflow=2.0.0\r\n\r\nThat solved it for me.", "I am having that problem, too! Expect solutions, thanks!\r\n\r\n> ImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u7d44\u3002", "> Same problem here. Somewhere hidden in the long thread above there is the hint to go back to tensorflow 2.0.0: pip install tensorflow=2.0.0\r\n> \r\n> That solved it for me.\r\n\r\nCorrection: pip install tensorflow==2.0.0"]}, {"number": 22511, "title": "[Feature Request] DistributionStrategy example not using Keras or estimator API", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nHi guys,\r\n\r\nI was curious if there are any resources to using a DistributionStrategy if some of us to choose to create a model using the low level tensorflow API and manage our own session. I see that with estimators we can define a tf.estimator.RunConfig and pass the strategies there, and with keras you can pass the strategy when you compile the model, but in general where are we supposed to pass the DistributionStrategy? Is it somehow supposed to be passed to tf.ConfigProto or tf.train.MonitoredTrainingSession? If there aren't any resources, I think it would be super helpful to have an example like https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/examples/keras_mnist.py \r\n\r\nThanks!", "comments": ["Hi, the only examples for using the DistributionStrategy directly with low level APIs are in some of the unit tests. \r\nFor e.g https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/minimize_loss_test.py#L103\r\n\r\nWhich particular strategy are you interested in using? Some are more straightforward to use directly than others.\r\n\r\n\r\n", "I see. I can start looking there. I was trying to find a good starting point to trying out CollectiveAllReduceStrategy. What I'm interested in is an alternative to Horovod, and this looks like the way to do it.", "In that case I think this is a better place to start, as CollectiveAllReduceStrategy requires additional setup:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/collective_all_reduce_strategy_test.py", "Cool, so I've tried my best to create a self contained working example but it isn't working and I'm not quite sure what's wrong. Here is what I'm running (this is a box with two GPUs):\r\n\r\n1. Running `CUDA_VISIBLE_DEVICES=0 python train.py --task_type=worker --task_index=0 &`\r\n2. Running `CUDA_VISIBLE_DEVICES=1 python train.py --task_type=worker --task_index=1 &`\r\n3. Running `CUDA_VISIBLE_DEVICES=-1 python train.py --task_type=chief --task_index=0`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 69, in get\r\n    return self._index[device]\r\nKeyError: '/replica:0/task:0/device:CPU:0'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 165, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 154, in main\r\n    master=server.target) as sess:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 195, in finalize\r\n    default_ready_op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 265, in get_or_default\r\n    op = default_constructor()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 190, in default_ready_op\r\n    variables.report_uninitialized_variables(),\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in report_uninitialized_variables\r\n    [s.op.name for s in var_list])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in <listcomp>\r\n    [s.op.name for s in var_list])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 305, in op\r\n    return self.get().op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 73, in get\r\n    (device, self._index.keys(), device_util.current())), e)\r\n  File \"<string>\", line 3, in raise_from\r\nValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/job:chief/replica:0/task:0/device:GPU:0']) (current device /device:CPU:0)\r\n```\r\n\r\nHere's the training script:\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow.contrib as tfc\r\nfrom enum import Enum\r\nimport json\r\n\r\nflags = tf.flags\r\n\r\nflags.DEFINE_integer(\"task_index\", None, \"task_index\")\r\nflags.DEFINE_enum(\"task_type\", None, ('chief', 'worker'), \"task type\")\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nclass Mode(Enum):\r\n    TRAINING = 1\r\n    VALIDATING = 2\r\n    INFERENCE = 3\r\n\r\ntf.set_random_seed(4)\r\nbatch_size = 4\r\ndef get_dataset(x,y):\r\n    dataset1 = tf.data.Dataset.from_tensor_slices(x).batch(batch_size).map(lambda x: tf.reshape(x,[-1,1]))\r\n    dataset2 = tf.data.Dataset.from_tensor_slices(y).batch(batch_size).map(lambda x: tf.reshape(x,[-1,1]))\r\n    dataset = tf.data.Dataset.zip((dataset1,dataset2))\r\n    \r\n    return dataset\r\n\r\nclass Model(object):\r\n    def __init__(self, mode, iterator=None, task_index=0):\r\n        self.task_index = task_index\r\n        self.iterator = iterator\r\n        self.distribution = tf.contrib.distribute.get_distribution_strategy()\r\n        \r\n        self.input_data = tf.placeholder(tf.float32, [None,1], name=\"input_data\")\r\n        \r\n        if mode == Mode.TRAINING:\r\n            tower_train_ops = self.distribution.call_for_each_tower(self.tower_loss_and_backprop, self.iterator.get_next(), True)\r\n            self.train_op = tf.group(self.distribution.unwrap(tower_train_ops))\r\n            \r\n        elif mode == Mode.VALIDATING:\r\n            total_loss = self.distribution.call_for_each_tower(self.tower_loss, *self.iterator.get_next())\r\n            self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss, \"/device:GPU:0\"))[0]\r\n        else:\r\n            self.loss = self.tower_loss(self.input_data)\r\n        \r\n    def tower_loss(self, inputs, labels=None):\r\n        self.w = w = tf.get_variable('w',initializer=tf.random_normal_initializer, shape=(1,1))\r\n        self.b = b = tf.get_variable('b',initializer=tf.constant_initializer(0.0), shape=(1,1))\r\n        \r\n        self.y_pred = tf.matmul(inputs,w) + b\r\n        if labels is None:\r\n            return None\r\n        loss = (labels - self.y_pred)**2\r\n        \r\n        return tf.reduce_sum(loss)\r\n    \r\n    def tower_loss_and_backprop(self, iter_get_next, is_training=False):\r\n        inputs, labels = iter_get_next\r\n        \r\n        self.loss = self.tower_loss(inputs,labels)\r\n        \r\n        if not is_training:\r\n            return None\r\n        optimizer = tf.train.AdamOptimizer(0.01)\r\n        gradients, variables = zip(*optimizer.compute_gradients(self.loss))\r\n        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\n        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\r\n            train_op = optimizer.apply_gradients(zip(gradients, variables), global_step=tf.train.get_or_create_global_step())\r\n        \r\n        return train_op\r\n        \r\n\r\ndef main(_):\r\n    #distribution = tfc.distribute.OneDeviceStrategy('device:CPU:0')\r\n    distribution = tfc.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=1)\r\n    \r\n    worker_hosts = ['localhost:1111', 'localhost:1112']\r\n    chief_host = ['localhost:1113']\r\n    cluster = tf.train.ClusterSpec({\"worker\": worker_hosts, \"chief\": chief_host})\r\n    task_type = FLAGS.task_type\r\n    task_index = FLAGS.task_index\r\n    is_chief = FLAGS.task_type == \"chief\"\r\n    \r\n    sess_config = tf.ConfigProto(\r\n                    log_device_placement=True,\r\n                    allow_soft_placement=True,  # Choose existing device to run ops in case specified one doesn't exist\r\n                )\r\n    sess_config.gpu_options.allow_growth = True  # This is to avoid using all RAM and to only use what it needs\r\n    \r\n    # If this is a worker, then set environment variable and join\r\n    if not is_chief:\r\n        os.environ['TF_CONFIG'] = json.dumps({\"cluster\":\r\n                                                  {\"worker\": worker_hosts, \"chief\": chief_host},\r\n                                              \"task\":\r\n                                                  {\"type\": task_type,\"index\": task_index}\r\n                                              })\r\n        print('TF_CONFIG=' + os.environ['TF_CONFIG'])\r\n        tf.contrib.distribute.run_standard_tensorflow_server(sess_config).join()\r\n    \r\n    server = tf.train.Server(\r\n            cluster, job_name=task_type, task_index=task_index)\r\n    \r\n    if task_type and task_index is not None:\r\n        if type(distribution) == tfc.distribute.OneDeviceStrategy:\r\n            print('Using OneDeviceStrategy')\r\n            distribution.configure(session_config=sess_config)\r\n        elif type(distribution) == tfc.distribute.CollectiveAllReduceStrategy:\r\n            print('Using CollectiveAllReduceStrategy')\r\n            distribution.configure(session_config=sess_config,\r\n                                   cluster_spec=cluster,\r\n                                   task_type=task_type,\r\n                                   task_id=task_index)\r\n    \r\n    foo = lambda a: list(map(lambda x: 3.0*x + 5.0, a))\r\n    x_train = [i*1.0 for i in range(-100,100)]\r\n    x_valid = [i*1.0 for i in range(-10,10)]\r\n    \r\n    with distribution.scope():\r\n        with tf.name_scope('Train'):\r\n            train_dataset = get_dataset(x_train, foo(x_train))\r\n            train_iterator = distribution.distribute_dataset(lambda: train_dataset).make_initializable_iterator()\r\n            with tf.variable_scope('Model'):\r\n                m = Model(Mode.TRAINING, train_iterator)\r\n        \r\n        with tf.name_scope('Valid'):\r\n            valid_dataset = get_dataset(x_valid, foo(x_valid))\r\n            valid_iterator = distribution.distribute_dataset(lambda: valid_dataset).make_initializable_iterator()\r\n            with tf.variable_scope('Model', reuse=True):\r\n                m_valid = Model(Mode.VALIDATING, valid_iterator)\r\n        \r\n        with tf.name_scope('Serving'):\r\n            with tf.variable_scope('Model', reuse=True):\r\n                m_serve = Model(Mode.INFERENCE)\r\n        \r\n    def run_epoch(model, train_op=None):\r\n        sess.run(model.iterator.initializer)\r\n        total_loss = 0\r\n        \r\n        fetches = {'loss':model.loss}\r\n        if train_op is not None:\r\n            fetches['train_op'] = train_op\r\n        while True:\r\n            try:\r\n                vals = sess.run(fetches)\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n            total_loss += vals['loss']\r\n        return total_loss\r\n    \r\n    with tf.train.MonitoredTrainingSession(config=sess_config,\r\n                                          is_chief=is_chief,\r\n                                          master=server.target) as sess:\r\n        for epoch_id in range(50):\r\n            t_loss = run_epoch(m, m.train_op)\r\n            if epoch_id % 10 == 0:\r\n                print('Epoch:', epoch_id)\r\n                print('Training loss:',t_loss)\r\n                print('Validating loss:', run_epoch(m_valid))\r\n                print('w = {} | b = {}'.format(*sess.run([m.w, m.b])))\r\n                print('----'*10)\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n\r\n```\r\n\r\nI also don't understand why this script complains when I don't specify the \"destinations\" when calling `self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss, \"/device:GPU:0\"))[0]`.  If I let \"destinations\" default to None, I get this error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 165, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 131, in main\r\n    m_valid = Model(Mode.VALIDATING, valid_iterator)\r\n  File \"train.py\", line 44, in __init__\r\n    self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss))[0]\r\nTypeError: reduce() missing 1 required positional argument: 'destinations'\r\n```", "To use multi-worker distribution strategies without Estimator training API, you have to use distribute coordinator which is not exposed yet: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/estimator_training.py#L272 It's very error-prone to mimic it by yourself. The details are also subject to change.\r\n\r\n@guptapriya We probably need to think about whether and how to support multi-worker training without high-level APIs.", "This is not right: `# If this is a worker, then set environment variable and join`. For `CollectiveAllReduceStrategy`, all workers including chief should run the same model replica with different \"TF_CONFIG\", which is similar to parameter server architectures.", "@yuefengz i think the replicator story that we've started working on will be a good alternative for when people cannot use high level APIs. in a nutshell, it allows you to distribute an arbitrary step function. It  doesn't deal with the session creation etc. though, so i am not yet clear how we can integrate distributed coordinator with it. \r\n\r\n", "@yuefengz I see. I mistakenly thought I could also just start up the workers with a join() which seemed to be the case here #22321 . Let me see if what you're suggesting helps.\r\n\r\n@guptapriya Is this alternative you have in mind released/exposed yet?\r\n", "@terrykong No, we are working on the design right now so it might be a while before that is released. ", "@terrykong  Is this still an issue?", "I still haven't gotten this to work even with the suggestions. I haven't had time yet to look into the distribute coordinator that yuefengz mentioned, but I'm hoping that there's a simpler way to get this example up and running", "We will be creating a mid-level API for users to customize their training loop. See the [RFC PR](https://github.com/tensorflow/community/pull/25) here."]}, {"number": 22510, "title": "Tensorflow lite aborts due to \"pure virtual method called\" on Raspberry Pi", "body": "Hello.\r\nI compiled the library as described in\r\nhttps://www.tensorflow.org/lite/rpi\r\n\r\nWhen running\r\n`./benchmark_model --graph=mobilenet_v1_1.0_224.tflite`\r\nor\r\n`./minimal mobilenet_v1_1.0_224.tflite`\r\n\r\n\r\nit crashes with the following output:\r\n```\r\n\r\nSTARTING!\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nWarmup runs: [1]\r\nGraph: [mobilenet_v1_1.0_224.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse nnapi : [0]\r\nLoaded model mobilenet_v1_1.0_224.tflite\r\nresolved reporter\r\npure virtual method called\r\nterminate called without an active exception\r\n\r\n\r\n```\r\n\r\nI tried to debug the problem myself: it occurs in a secondary thread.\r\nSome function that is called from \r\n`EigenForTFLite::NonBlockingThreadPoolTempl<EigenForTFLite::StlThreadEnvironment>::WorkerLoop(int)\r\n` \r\ngets the this parameter equal to NULL.\r\n\r\nThe host is a raspberry pi 3B+, with raspbian.\r\nI tried both to cross-compile the code and to compile natively.\r\nI tried more versions: one is r1.11\r\nGcc version: 4.9.2\r\n\r\nDoes anyone else have the same problem or is it just me?\r\nThank you.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n\r\nHello. Here is the form:\r\n\r\n> Have I written custom code\r\n\r\nNo\r\n\r\n> OS Platform and Distribution\r\n\r\nRaspbian 8 Jessie\r\n\r\n> TensorFlow installed from\r\n\r\nSource\r\n\r\n> TensorFlow version\r\n\r\nr1.11\r\n\r\n> Bazel version\r\n\r\nNo\r\n\r\n> CUDA/cuDNN version\r\n\r\nNo\r\n\r\n> GPU model and memory\r\n\r\nNo\r\n\r\n> Exact command to reproduce\r\n\r\n./benchmark_model --graph=mobilenet_v1_1.0_224.tflite\r\n\r\n> Mobile device\r\n\r\nRaspberry Pi 3B+\r\n", "@vvigilante It appears that you are using cmake which is deprecated for TensorFlow. Could you try using Bazel?", "I did not use cmake. I used the steps described in https://www.tensorflow.org/lite/rpi", "I got it running. **Compile flag _-march=armv7-a_ seems to break things**.\r\nJust removing that flag worked for me.\r\nI don't know if it is related to gcc version (mine is 4.9.2). I'm doing other tests.\r\n\r\nMaybe it is linked to this:\r\nhttps://gcc.gnu.org/bugzilla/show_bug.cgi?id=70132\r\n\r\nEdit: I confirm it works with gcc-8.1.0 and it is slightly faster (~15%).\r\nPS: _-D_GLIBCXX_HAVE_OBSOLETE_ISNAN=1 -D_GLIBCXX_HAVE_OBSOLETE_ISINF=1_ flags are needed to compile it with new gcc."]}, {"number": 22509, "title": "Make sure broken tests are filtered out in XLA tests suites.", "body": "PiperOrigin-RevId: 214311663", "comments": ["Looks like it does not really work."]}, {"number": 22508, "title": "Fix memory leak of a Var resource in the multiple variable-handling k\u2026", "body": "\u2026ernels.\r\n\r\nThis change fixes memory leaks in the ScatterNdUpdateOp and StridedSliceAssign kernels, and in training-op kernels that use `GetTrainingVariableMutex()`.\r\n\r\nPiperOrigin-RevId: 214372346", "comments": []}, {"number": 22507, "title": "1.11.0-final cherry-pick request: Update tensorboard dependency to 1.11.x", "body": "PiperOrigin-RevId: 214371640", "comments": []}, {"number": 22506, "title": "toco fails to convert to tflite \"TypeError: 'NoneType' object has no attribute '__getitem__'\"", "body": "`toco --graph_def_file=frozen_inference_graph.pb --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=oren.tflite --inference_type=QUANTIZED_UINT8 --input_type=FLOAT --input_arrays=ImageTensor --output_arrays=SemanticPredictions --input_shapes=1,321,321,3`\r\n\r\n\r\n2018-09-25 19:48:04.857732: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/home/obg1/.local/bin/toco\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 370, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/obg1/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 366, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 143, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/lite.py\", line 374, in convert\r\n    dump_graphviz_video=self.dump_graphviz_video)\r\n  File \"/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 243, in toco_convert\r\n    *args, **kwargs)\r\n  File \"/home/obg1/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/convert.py\", line 210, in build_toco_convert_protos\r\n    input_array.mean_value, input_array.std_value = quantized_input_stats[idx]\r\nTypeError: 'NoneType' object has no attribute '__getitem__'\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "as you can see from the `tensorflow/contrib/lite/python/convert.py`, for `--inference_type=QUANTIZED_UINT8`,  `--std_dev_values` and ` --mean_values` are anticipated.", "@obg2sens Is this issue resolved ?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@ZJPUBG  \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please create a new issue with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22505, "title": "Lookup Error : Gradient Registry has no entry for  StatefulPartitionedCall", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: v9.0 (Cuda)\r\n- **GPU model and memory**: gtx 930m, 2GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI was implementing the gradient penalty for improved stability for wgan in tensorflow eager\r\n\r\n### Source code / logs\r\n``` python\r\nwith tf.GradientTape() as critic_tape:\r\n    generated_images = generator(tf.random_normal([16, 100]), training=True)\r\n    a = tf.convert_to_tensor(images)\r\n    real_output = critic(a, training=True)\r\n    generated_output = critic(generated_images, training=True)               \r\n    with tf.GradientTape() as gtape:\r\n        epsilon = tf.random_uniform([], 0, 1)\r\n        xhat = epsilon*a + (1-epsilon)*generated_images\r\n        dhat = critic(xhat, training=True)\r\n        gtape.watch(xhat)\r\n    dhat2 = gtape.gradient(dhat, xhat)\r\n    slopes = tf.sqrt(tf.reduce_sum(tf.square(dhat2), reduction_indices=[1]))\r\n    gradient_penalty = 10*tf.reduce_mean((slopes-1.0)**2)\r\n    critic_loss = get_critic_loss(real_output, generated_output)\r\n    critic_loss+= gradient_penalty                \r\ngradients_of_critic = critic_tape.gradient(critic_loss, critic.variables)\r\nprint(gradients_of_critic) \r\n```\r\n\r\nand the error stack i recieve is \r\n```--------------------------------------------------------------------------\r\nLookupError                               Traceback (most recent call last)\r\n<ipython-input-512-cbc8ebf905ac> in <module>()\r\n     16     critic_loss = get_critic_loss(real_output, generated_output)\r\n     17     critic_loss+= gradient_penalty\r\n---> 18 gradients_of_critic = critic_tape.gradient(critic_loss, critic.variables)\r\n     19 print(gradients_of_critic)\r\n\r\nc:\\users\\vibhu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py in gradient(self, target, sources, output_gradients)\r\n    856     flat_grad = imperative_grad.imperative_grad(\r\n    857         _default_vspace, self._tape, nest.flatten(target), flat_sources,\r\n--> 858         output_gradients=output_gradients)\r\n    859 \r\n    860     if not self._persistent:\r\n\r\nc:\\users\\vibhu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py in imperative_grad(vspace, tape, target, sources, output_gradients)\r\n     61   \"\"\"\r\n     62   return pywrap_tensorflow.TFE_Py_TapeGradient(\r\n---> 63       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\r\n\r\nc:\\users\\vibhu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\r\n    110   \"\"\"\r\n    111   mock_op = _MockOp(attr_tuple, inputs, outputs, op_name)\r\n--> 112   grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\r\n    113   if grad_fn is None:\r\n    114     return [None] * num_inputs\r\n\r\nc:\\users\\vibhu\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\registry.py in lookup(self, name)\r\n     91     else:\r\n     92       raise LookupError(\r\n---> 93           \"%s registry has no entry for: %s\" % (self._name, name))\r\n\r\nLookupError: gradient registry has no entry for: StatefulPartitionedCall\r\n\r\n", "comments": ["@vibss2397 You would want to check how you define your images, make sure that `convert_to_tensor` takes the correct data format. ", "Hey, @wt-huang i checked all the tensors and they are fine, and both the gradient terms are calculated separately are proper but only when  the gradient penalty added to the critic loss function doed it throw the error. Can i calculate gradients of output of a model wrt input using``tfe.gradients_function() ``", "@wt-huang it started working now, basically the line `` gtape.watch(xhat) ``, i had to write that before calculating `` dhat `` , thanks anyways", "@vibss2397 Glad it worked, yes you would need to have that line before computing `dhat`.", "> @wt-huang it started working now, basically the line `gtape.watch(xhat)`, i had to write that before calculating `dhat` , thanks anyways\r\n\r\nI did like this, but still get that error. My environment is tf_1.11 and in eager mode. Could you please detail the solution for this error? Thanks very much.", "Hey @pengzhou93, i tried that, it just worked because ``dhat `` got assigned a value which didnt update and it just used the previous value, i started getting the same error even after correcting it  but forgot to reopen the issue, reopening, my bad \ud83d\ude05", "I'm having the same issue (also trying to implement a wgan-gp using tensorflow eager).\r\n@vibss2397 Were you able to get it to work?", "@mvacaporale yess. A workaround would be Disabling `tf.contrib.eager.defun` for the critic, but then again you won't get the speedup offered by `defun`", "Does this error still reproduce at tf nightly?", "@alextp wow, no, it does not, why might that be?", "@vibss2397 it's been fixed since. Closing."]}, {"number": 22504, "title": "Weird behaviour of back propagation in a while_loop in Tensorflow", "body": "- **Have I written custom code: Yes\r\n- **OS Platform and Distribution: Windows 10 (primary), Ubunto 16.04.3 LTS (secondary)\r\n- **TensorFlow installed from (source or binary): Binary\r\n- **TensorFlow version (use command below): b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version: Python 3.6.6 :: Anaconda, Inc.\r\n- **CUDA/cuDNN version**: N/A (not relevant)\r\n- **GPU model and memory**: N/A (not relevant) \r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI am trying to see if Tensorflow can backpropagate through a while_loop without a pre-defined number of loops. Something weird caught my attention: the following script results in the error \r\n\r\n> InvalidArgumentError: Retval[0] does not have value\r\n\r\nHowever, when I replaced `tf.add(X,X)` by `tf.multiply(X,2)`, the code can be executed and return the correct answer graVal=64. \r\n\r\n\r\nCould anyone help me to understand where the error came from when I was using `tf.add(X,X)`? Thanks in advance!\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nfrom numpy import *\r\n\r\ntf.reset_default_graph()\r\n\r\nX = tf.Variable(5.0, dtype = float32)\r\nY = tf.Variable(4.0, dtype = float32)\r\n\r\ndef cond(iterN, Y):\r\n    return iterN<6\r\n\r\ndef body(iterN, X):\r\n    return iterN+1, tf.add(X,X)\r\n    # return iterN+1, tf.multiply(X,2)\r\n\r\nfinalOutX = tf.while_loop(cond, body, [0,X], parallel_iterations=1, back_prop=True, swap_memory=True)\r\n\r\ngra = tf.gradients(finalOutX, X)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n\r\n    init.run()\r\n    X_val, graVal, = sess.run([finalOutX, gra])    \r\n\r\nprint(X_val)\r\nprint(graVal)\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "@dblueeye A quick debugging tells me that error arises when the code triggers following line:\r\nX_val, graVal, = sess.run([finalOutX, gra]) in your script.", "@ymodak. That was not the real source, IMHO.  You'll see if you change the tf.add(X,X) to tf.multiply(2.0, X).", "> Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\r\n> Bazel version\r\n> Mobile device\r\n\r\nThey are not relevant. I installed from the binary and not from the source, and I have not tried on a mobile device (I don't think I'll ever be trying:)). \r\n", "@dblueeye `tf.add` works if you set the second parameter constant, i.e., `tf.add(X, 5.0)` would not encounter any error. @aselle Is this expected behavior?", "Closing this, feel free to reopen if problem persists."]}, {"number": 22503, "title": "Training tfgan to restore VMP", "body": "I want to use this to restore VMP.What information can I refer to?", "comments": ["@killpy Hi, you may refer [this](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan) which has information about using tfgan with some examples.\r\n\r\nIf you have further questions, those can be better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there."]}, {"number": 22502, "title": "LookupError when computing tf.gradients of DepthwiseConv2dNativeBackpropInput", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Anaconda\r\n- **TensorFlow version:  v1.10.0-rc0-0-g586a7f1d2c 1.10.0-rc0\r\n- **Python version : 3.6\r\n- **CUDA/cuDNN version**: 9.0 / 7.3\r\n- **GPU model and memory**: Titan X\r\n\r\n\r\n### Describe the problem\r\nI got a LookupError when computing tf.gradients()\r\n\r\nLookupError: No gradient defined for operation 'gradients/discriminator_3/separable_conv2d/depthwise_grad/DepthwiseConv2dNativeBackpropInput' (op type: DepthwiseConv2dNativeBackpropInput)\r\n\r\nBecause DepthwiseConv2dNativeBackpropInput does not have a registered gradient function.\r\n\r\n### Source code / logs\r\ntf.gradients(d_hat, x_hat)[0] where tf.nn.separable_conv2d is used in the intermediate layers.\r\n", "comments": ["@reedwm Can this be added to \"Contributions Welcome\" as a feature request ? ", "This is a quick fix by @rdinse:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/9917#issuecomment-322012032", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Any updates on this?", "Hey, I'd like to work on this. Looks it still hasn't been done. Is that alright?", "@reedwm @SimKuanGOH, I just verified that the issue already be fixed in the latest version of tensorflow. Please consider close this issue.\r\nThe gradient is defined in tensorflow/tensorflow/python/ops/nn_grad.py.", "@astropeak Thank you :-)"]}, {"number": 22501, "title": "Quantization Error:   Converting unsupported operation: Elu", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)** : pip install\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**:3.6\r\n- **GPU model and memory**:NVIDIA 1080\r\n- **Exact command to reproduce**:N/A\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: use CPU\r\n- **Mobile device**:N/A\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\ntf.enable_eager_execution()\r\nconverter = tf.contrib.lite.TocoConverter.from_saved_model(\"/media/user/60EA4260EA423298/Nikishyn/model\")\r\n#converter.post_training_quantize = True\r\ntflite_quantized_model = converter.convert()\r\n```\r\n\r\nI'm trying to quantify my model, but I get an error.\r\nconsole logs:\r\n\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-09-25 08:17:37.600466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.600610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.600833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.601051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.601221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.601441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.601664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.601835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.602063: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.602291: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.603091: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.603218: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.603283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1096] Converting unsupported operation: Elu\\n2018-09-25 08:17:37.606771: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 373 operators, 615 arrays (0 quantized)\\n2018-09-25 08:17:37.610480: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 347 operators, 576 arrays (0 quantized)\\n2018-09-25 08:17:37.615480: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 347 operators, 576 arrays (0 quantized)\\n2018-09-25 08:17:37.615675: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:42] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\\nAborted (core dumped)\\n'\r\nNone\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nMobile device", "updated  fields", "@NikishinRoman Hi, thanks for your post. Please note that tflite does not support the operation Elu and due to which you are running into this error.", "@NikishinRoman For more information about TFLite supported operations, please find [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md).", "@suharshs Is this an op that we should add to your issue?", "@harshini-gadige , Is it expected to support this(Elu) operation in the future?", "@NikishinRoman You can track the operations that are added to tensorflow Lite in the future here #21526.\r\nThank you!"]}, {"number": 22500, "title": "I get \"Session was not created with a graph before Run()\" while running official TFLite Model Benchmark Tool on Android!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nandroid and Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nXiaomi Max2\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.14\r\n- **Python version**:\r\n3.5\r\n- **Bazel version**:\r\n1.7\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI am doing the \"build/install/run\" steps on Android phone following https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark\r\n\r\nHowever, when I run the benchmark using the following command I got error\"Session was not created with a graph before Run()\":\r\n\r\nadb shell /data/local/tmp/benchmark_model \\\r\n  --graph=/data/local/tmp/mobilenet_quant_v1_224.tflite \\\r\n  --num_threads=4\r\n\r\nI did not modify the source code and just follow the official document. Does anyone encounter the same problem?\r\n\r\n![_20180925211348](https://user-images.githubusercontent.com/14329360/46016662-f12d2680-c107-11e8-8d8e-1e1f47b895cb.jpg)\r\n\r\n", "comments": ["It seems the one you ran is the TF benchmark_model rather than the TF Lite one. Please check it."]}, {"number": 22499, "title": "Can not run benchmark on android using \"tensorflow/tensorflow/tools/benchmark\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nandroid and Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nXiaomi Max2\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.14\r\n- **Python version**:\r\n3.5\r\n- **Bazel version**:\r\n1.7\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI am trying to run benchmark on android phone following the instructions here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark/\r\n\r\nI have succeffully build the benchmark and everything is right until step (4) Run the benchmark.\r\nWhen I run the command below, I got no response:\r\nadb shell \"/data/local/tmp/benchmark_model \\\r\n  --graph=/data/local/tmp/tensorflow_inception_graph.pb \\\r\n  --input_layer=\"input:0\" \\\r\n  --input_layer_shape=\"1,224,224,3\" \\\r\n  --input_layer_type=\"float\" \\\r\n  --output_layer=\"output:0\"\r\n\r\nThere is only blinking arrow:\r\n![_20180925202142](https://user-images.githubusercontent.com/14329360/46014065-acea5800-c100-11e8-8975-db7b06c88d10.png)\r\n\r\nSo can someone tell me what's wrong?", "comments": ["unbalance delimiter `\"`. Use something like\r\n```\r\nadb shell \"/data/local/tmp/benchmark_model \\\r\n  --graph=/data/local/tmp/tensorflow_inception_graph.pb \\\r\n  --input_layer=\"input:0\" \\\r\n  --input_layer_shape=\"1,224,224,3\" \\\r\n  --input_layer_type=\"float\" \\\r\n  --output_layer=\"output:0\"\"\r\n```\r\nor \r\n```\r\nadb shell /data/local/tmp/benchmark_model \\\r\n  --graph=/data/local/tmp/tensorflow_inception_graph.pb \\\r\n  --input_layer=\"input:0\" \\\r\n  --input_layer_shape=\"1,224,224,3\" \\\r\n  --input_layer_type=\"float\" \\\r\n  --output_layer=\"output:0\"\r\n```\r\ninstead. I'll send a PR to update the doc later."]}, {"number": 22497, "title": "DataLoss error on TFRecords - happens on one machine doesn't on other", "body": "### System information\r\n#### System 1 (Bug DOESN'T occur)\r\nAll details are from inside the docker which the codes run in\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: True\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1, 4.15.0-29-generic\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow_gpu\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Mobile device**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Python version**: 3.6.6\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA: V9.0.176, cuDNN 7.1.4  \r\n- **GPU model and memory**: GTX 1080 8GB + GTX 980 Ti 6GB\r\n- **Docker details**: \r\n  NVIDIA Docker: 2.0.2\r\n    Client:\r\n    Version:           17.12.0-ce\r\n    API version:       1.35\r\n    Go version:        go1.9.2\r\n    Git commit:        c97c6d6\r\n    Built:             Wed Dec 27 20:11:19 2017\r\n    OS/Arch:          linux/amd64\r\n    Experimental:      false\r\n  Server:\r\n    Engine:\r\n    Version:          17.12.0-ce\r\n    API version:      1.35 (minimum version 1.12)\r\n    Go version:       go1.9.2\r\n    Git commit:       c97c6d6\r\n    Built:            Wed Dec 27 20:09:53 2017\r\n    OS/Arch:          linux/amd64\r\n    Experimental:     false\r\n\r\n- **Exact command to reproduce**: See below\r\n\r\n#### System 2 (Bug DOES occur)\r\nAll details are from inside the docker which the codes run in\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: True\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.1, 4.15.0-34-generic\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow_gpu\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n- **Mobile device**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **Python version**: 3.6.6\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA: V9.0.176, cuDNN 7.2.1  \r\n- **GPU model and memory**: GTX 1080 Ti 12GB x 2\r\n- **Docker details**:\r\n  NVIDIA Docker: 2.0.3\r\n    Client:\r\n    Version:           18.06.0-ce\r\n    API version:       1.38\r\n    Go version:        go1.10.3\r\n    Git commit:        0ffa825\r\n    Built:             Wed Jul 18 19:11:02 2018\r\n    OS/Arch:           linux/amd64\r\n    Experimental:      false\r\n  Server:\r\n    Engine:\r\n    Version:          18.06.0-ce\r\n    API version:      1.38 (minimum version 1.12)\r\n    Go version:       go1.10.3\r\n    Git commit:       0ffa825\r\n    Built:            Wed Jul 18 19:09:05 2018\r\n    OS/Arch:          linux/amd64\r\n    Experimental:     false\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nUsing the same docker image, which is an altered version of [this image](https://github.com/ufoym/deepo/blob/master/docker/Dockerfile.tensorflow-py36-cu90) running the same code over the same TFRecords on one machine (System 2) I get a DataLoss exception (corrupted record) while on the other system (System 1) I don't.\r\nI have verified using checksum that the files were transferred safely. I have also tried to retransfer the files at least two times. I have also have tried using other sets of TFRecords I created. The problem is the same in all cases, it happens on System 2, but does not happen on System 1.\r\n\r\nThe TFRecords were written on System 1 and are over 150GB in size.\r\nI have also tried rebuilding the image and restarting containers.\r\nI have also tried restarting the host machine.\r\nThe code is running on one GPU, I tried using each, all fail.\r\n\r\n### Source code / logs\r\nDue to sensitivity of the code I can't share all of it, but I'll paste the relevant lines.\r\n\r\nIn the training code I have 3 places where I iterate over the dataset, once when I count the number of records\r\n`train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))`. \r\nSecond time when I feed the training loop\r\n\r\n```\r\nwhile True:\r\n    try:\r\n        ...\r\n        _, batch_summary = sess.run([opt, merged], feed_dict={handle: train_handle})\r\n        ...\r\n    except tf.errors.OutOfRangeError:\r\n        ...\r\n```\r\nAnd third time very similar code for the validation set.\r\n\r\nAfter I recreate image + containers and restarting the machine, I will successfully finish the calculation of `train_size` (full loop over TFRecords), then I would start training, and somewhere in the training loop I would fail with the traceback attached below. If then I run the code again, I will fail on the first call to the TFRecords, with the same traceback - meaning this time it would happen on the `train_size` calculation. It will now forever fail on this call, until I restart the machine and the scenario repeats.\r\n\r\nAs I said, same code on different machine (System 1) never fails.\r\n\r\n\r\nTraceback:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"train.py\", line 884, in <module>\r\n>     main()\r\n>   File \"train.py\", line 878, in main\r\n>     train(graph, model_dir, tensorboard_dir, meta, is_recovering=recovering)\r\n>   File \"train.py\", line 846, in train\r\n>     'sequential']})\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 877, in run\r\n>     run_metadata_ptr)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n>     feed_dict_tensor, options, run_metadata)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n>     run_metadata)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 6720637495\r\n> \t [[Node: data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](data/IteratorFromStringHandle)]]\r\n> \r\n> Caused by op 'data/IteratorGetNext', defined at:\r\n>   File \"train.py\", line 884, in <module>\r\n>     main()\r\n>   File \"train.py\", line 874, in main\r\n>     graph, meta = build_model(available_gpus)\r\n>   File \"train.py\", line 502, in build_model\r\n>     return build_classifier(available_gpus)\r\n>   File \"train.py\", line 572, in build_classifier\r\n>     n_gpus=FLAGS.n_gpus)\r\n>   File \"/opt/code/utils/architecture/data/v4.py\", line 309, in data_prep\r\n>     <deleted line for sensitivity issues>\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 410, in get_next\r\n>     name=name)), self._output_types,\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2069, in iterator_get_next\r\n>     output_shapes=output_shapes, name=name)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n>     op_def=op_def)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n>     op_def=op_def)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n>     self._traceback = tf_stack.extract_stack()\r\n> \r\n> DataLossError (see above for traceback): corrupted record at 6720637495\r\n> \t [[Node: data/IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,1], [?,?,?], [?,?,?], [?,?], ..., [?,?], [?,1], [?,?,?], [?,?,?], [?,?]], output_types=[DT_FLOAT, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, ..., DT_INT64, DT_INT64, DT_FLOAT, DT_INT64, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](data/IteratorFromStringHandle)]]\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "Editted :man_facepalming: ", "@eliorc Hi, just to confirm have you tried reading your saved data using [generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator)", "Yep, due to sensitivity I can't post the entire code but this is censored version\r\n\r\n```\r\ndataset = tf.data.TFRecordDataset(tfrecord_path)\r\ndataset = dataset.map(...., num_parallel_calls=n_workers)\r\ndataset = dataset.map(....,  num_parallel_calls=n_workers)\r\ndataset = dataset.map(...., num_parallel_calls=n_workers)\r\n\r\ndataset = dataset.map(...,  num_parallel_calls=n_workers)\r\n\r\ndataset = dataset.shuffle(buffer_size=10000)\r\n\r\ndataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=general.element_length_func,\r\n                                                                  bucket_batch_sizes=[batch_size] * (\r\n                                                                          len(bucket_boundaries) + 1),\r\n                                                                  # +2 for the below [0] and above [-1]\r\n                                                                  bucket_boundaries=bucket_boundaries,\r\n                                                                  padded_shapes={.....}))\r\n\r\ndataset = dataset.prefetch(buffer_size=prefetch)\r\n\r\nhandle = tf.placeholder(tf.string, shape=[], name='handle')\r\niterator = tf.data.Iterator.from_string_handle(handle,\r\n                                                   dataset.output_types,\r\n                                                   dataset.output_shapes)\r\n\r\ntrain_iterator = dataset.make_initializable_iterator()\r\n\r\niterator.get_next()\r\n....\r\n```\r\n\r\nThe code runs on a 16 core machine", "@mrry  Could you please look into this.", "Hi @eliorc! I'm going to need some more details to get to the bottom of this. Can you please answer the following questions?\r\n\r\n* Is the corrupted record always at the same offset, or does it vary?\r\n* Does the error reproduce *every* time you run on worker 2, or only sometimes?\r\n* How was the file generated?\r\n* Are you using any compression options?", "Also:\r\n* What file system are you using? (e.g. Is it a local file, or in GCS, S3, etc.?)", "Hi @mrry \r\n\r\n- _Is the corrupted record always at the same offset, or does it vary?_ - I'm using the `.shuffle` call, so I believe even though the number of the record is different each time I restart the machine, it might be the same one.\r\n- _Does the error reproduce every time you run on worker 2, or only sometimes?_ - The error always reproduces on worker 2, if I restart the host machine it will go through the counting process and somewhere during the training it will fail, and then it will always fail upon trying to count\r\n- _How was the file generated?_ - Pretty standard code, these are `tf.SequenceExample`, pretty similar in technique but more complex than code that can be found [here](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)\r\n- _Are you using any compression options?_ - Not that I know of, unless there is some kind of default compression. What I did do is compress the file before transferring it between systems using zip.\r\n- _What file system are you using?_ (e.g. Is it a local file, or in GCS, S3, etc.?) - Local filesystems\r\n\r\nP.S. thanks for your great support on the SO platform, it helps a lot", "Furthering investigating this issue, I used the same TFRecord and the same code again on an AWS instance and it worked perfectly. \r\n\r\nSystem 3 (Bug DOESN'T occur - AWS instance)\r\nAll details are from inside the docker which the codes run in\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): True\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.4.0-1069-aws #79-Ubuntu x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nTensorFlow installed from (source or binary): pip install tensorflow_gpu\r\n\r\nTensorFlow version (use command below): v1.10.0-0-g656e7a2b34 1.10.0\r\n\r\nMobile device: N/A\r\n\r\nBazel version (if compiling from source): N/A\r\n\r\nPython version: 3.6.6\r\n\r\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\nCUDA/cuDNN version: CUDA: V9.0.176, cuDNN 7.1.4\r\n\r\nGPU model and memory: Tesla V100-SXM2-16GB\r\n\r\nDocker details:\r\nNVIDIA Docker: 2.0.3\r\nClient:\r\n Version:      18.03.1-ce\r\n API version:  1.37\r\n Go version:   go1.9.5\r\n Git commit:   9ee9f40\r\n Built:        Thu Apr 26 07:17:20 2018\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n Orchestrator: swarm\r\n\r\nServer:\r\n Engine:\r\n  Version:      18.03.1-ce\r\n  API version:  1.37 (minimum version 1.12)\r\n  Go version:   go1.9.5\r\n  Git commit:   9ee9f40\r\n  Built:        Thu Apr 26 07:15:30 2018\r\n  OS/Arch:      linux/amd64\r\n  Experimental: false\r\n\r\n\r\nExact command to reproduce: See below", "@mrry Hi, can you please look into this.", "Another step we took; we have just formatted the machine that fails, rebuilt the images and tried again - it still fails on the `DataLoss` error.\r\n\r\nThis week we will run a memtest.", "**PROBLEM IS SOLVED**\r\n\r\nWe ran a memtest on our 4 memory cards and one of the cards was faulty. We removed that card, reran the code and now everything works smoothly.", "Hi @eliorc, this is probably a dumb question but would you mind elaborating on which type of memory was failing for you (RAM, hard drive, etc.)? And/or what you mean by memtest? Was it the memtest86 program that comes with Ubuntu, thus suggesting RAM was the problem...?\r\n\r\nI'm experiencing the same issue (data corruption occurring during training on one Ubuntu machine, but not on the other), and I suspect there's an issue with the memory, but I ran memtest86 and it didn't find any problems. That being the case, would appreciate any extra information about your findings \u2013\u00a0thanks so much!", "@owenwork I'll try to investigate that, since the IT guys ran all the test I don't have that information on hand - if I'll be able to get some answers I'll be sure to update", "how to solute it?"]}, {"number": 22496, "title": "I met the error:__init__() missing 2 required positional arguments: 'message' and 'code' when I tried to run the exact same code on image_captioning_with_attention ", "body": "The details are as follows:\r\ncode:\r\nfor img, path in image_dataset:\r\n    batch_features = image_features_extract_model(img)\r\n    batch_features = tf.reshape(batch_features, \r\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\r\n\r\n    for bf, p in zip(batch_features, path):\r\n        path_of_feature = p.numpy().decode(\"utf-8\")\r\n        np.save(path_of_feature, bf.numpy())\r\nerror:\r\n_FallbackException                        Traceback (most recent call last)\r\nE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)\r\n   1600         \"IteratorGetNextSync\", name, _ctx._post_execution_callbacks, iterator,\r\n-> 1601         \"output_types\", output_types, \"output_shapes\", output_shapes)\r\n   1602       return _result\r\n\r\n_FallbackException: Expecting int64_t value for attr output_shapes, got NoneType\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-17-266d7ad17ef7> in <module>()\r\n----> 1 for img, path in image_dataset:\r\n      2     batch_features = image_features_extract_model(img)\r\n      3     batch_features = tf.reshape(batch_features, \r\n      4                               (batch_features.shape[0], -1, batch_features.shape[3]))\r\n      5 \r\n\r\nE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py in __next__(self)\r\n    484 \r\n    485   def __next__(self):  # For Python 3 compatibility\r\n--> 486     return self.next()\r\n    487 \r\n    488   def _next_internal(self):\r\n\r\nE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py in next(self)\r\n    515     \"\"\"\r\n    516     try:\r\n--> 517       return self._next_internal()\r\n    518     except errors.OutOfRangeError:\r\n    519       raise StopIteration\r\n\r\nE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py in _next_internal(self)\r\n    505             self._resource,\r\n    506             output_types=self._flat_output_types,\r\n--> 507             output_shapes=self._flat_output_shapes)\r\n    508 \r\n    509       return sparse.deserialize_sparse_tensors(\r\n\r\nE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)\r\n   1604       return iterator_get_next_sync_eager_fallback(\r\n   1605           iterator, output_types=output_types, output_shapes=output_shapes,\r\n-> 1606           name=name, ctx=_ctx)\r\n   1607     except _core._NotOkStatusException as e:\r\n   1608       if name is not None:\r\n\r\nE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py in iterator_get_next_sync_eager_fallback(iterator, output_types, output_shapes, name, ctx)\r\n   1633   _result = _execute.execute(b\"IteratorGetNextSync\", len(output_types),\r\n   1634                              inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,\r\n-> 1635                              name=name)\r\n   1636   _execute.record_gradient(\r\n   1637       \"IteratorGetNextSync\", _inputs_flat, _attrs, _result, name)\r\n\r\nE:\\anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     59                                                op_name, inputs, attrs,\r\n---> 60                                                num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nTypeError: __init__() missing 2 required positional arguments: 'message' and 'code", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@LongForCMU I ran the above code snippet without running into any error. You may need to change the batch size depending on your system configuration. In the meantime, please provide more details in the issue template so we can better help you. ", "@wt-huang Thank you for your reply. I have changed the batch size but it didn't work. Here are some details: Win10+tensorflow 1.10.0+CUDA 9.0.\r\nI found that there was someone else who met just the same error as me: https://stackoverflow.com/questions/51811510/typeerror-init-missing-2-required-positional-arguments-message-and-co", "@LongForCMU It did work on my end. You can find Tensorflow/Windows compatibility chart [here](https://www.tensorflow.org/install/source_windows)", "Closing this for now, feel free to reopen if any errors come up."]}, {"number": 22495, "title": "I have two nvidia GPU,but run ours finalAlg.so\uff0cand GPU don't work. ", "body": "first,i cmake tensorflow-GPU,and get tensorflow_cc.so\uff1b\r\nthen ,cmake tensorflow__cc.so and another alg codes have got finalAlg.so;\r\nlast,run finalAlg.so by tensorflow C++ API.\r\n\r\n    string model_path = \"finalAlg.pb\";\r\n    GraphDef graph_def;\r\n    status = ReadBinaryProto(Env::Default(), model_path, &graph_def);\r\n    status = session->Create(graph_def); \r\n    graph::SetDefaultDevice(\"/gpu:0\", &graphdef);\r\n    sess_options.config.mutable_gpu_options()->set_allow_growth(true); \r\n    Tensor phase_train(DT_BOOL, TensorShape());\r\n    phase_train.scalar<bool>()() = false;   \t\r\n    std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {\r\n    { \"input\", input_tensor },{\"phase_train\",phase_train }};\r\n    std::vector<tensorflow::Tensor> outputs;\r\n    status = session->Run(inputs, { \"dsts\"}, {}, &outputs);\r\n    runtime appear:\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N N\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   N N\r\n\r\n \r\nAlthough i have two nvidia GPUs,but ours finalAlg.so cant't run in GPU,just always run in CPU.\r\nGive me some suggesstion.thanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@cvJie Hi, can you try using \"with tf.device(\"/gpu:0\"):\"  Something like as shown below :\r\n\r\nimport tensorflow as tf\r\nwith tf.device(\"/gpu:0\"):\r\n\r\n", "@cvJie Is this issue resolved ?", "> @cvJie Is this issue resolved ?\r\n\r\nIt has resolved.thanks"]}, {"number": 22494, "title": "Compilation v1.10 fails at //tensorflow/core/debug:debug_io_utils", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo it's a compilation issue\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNil\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom source\r\n- **TensorFlow version (use command below)**:\r\ntensorflow 1.10.0\r\n- **Python version**:\r\npython3\r\n- **Bazel version (if compiling from source)**:\r\nbazel 0.16\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc-6\r\n- **CUDA/cuDNN version**:\r\ncuda 9.1 cudnn 7.3.0\r\n- **GPU model and memory**:\r\n1070 8G\r\n- **Exact command to reproduce**:\r\nbazel --host_jvm_args=-Xms512m --host_jvm_args=-Xmx1024m   build --jobs=2 -c opt --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nThe compilation of tensorflow stops during //tensorflow/core/debug:debug_io_utils with grpc_completion_queue_factory does not name a type\r\n1. I have compiled a separate grpc installation and that went well\r\n2. Tried compiling all the dependencies \r\n        \"//tensorflow:grpc++\",\r\n        \"//tensorflow/core:core_cpu_internal\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:graph\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core:lib_internal\",\r\n        \"//tensorflow/core:proto_text\",\r\n        \"//tensorflow/core:protos_all_cc\",\r\nand these sub dependencies compiled fine\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n[errorfile3.txt](https://github.com/tensorflow/tensorflow/files/2414334/errorfile3.txt)\r\n\r\n", "comments": ["I'm not sure what the actual reason why it doesn't work, but I've formulated a workaround.\r\n\r\n1. During compile time, you can add flags to the compile time invocation by changing the functions in crosstool_wrapper_is_not_gcc to emit the correct lines\r\n2. I added a linker path in the bazel file for the rule tf_cc_shared_object\r\n3. Any not found symbols at the end of the compile, manually change it using patchelf.\r\n\r\nAnd that works"]}]