[{"number": 10865, "title": "Java API to get the size of specified input list of operations.", "body": "This opens up access to the TF_OperationInputListLength C API from java, for operations that return specified input lists.", "comments": ["Can one of the admins verify this patch?", "@asimshankar could you take a look? Thanks!", "Jenkins, test this please.", "  @asimshankar  Thanks for your review,  and your suggestion does make sense.\r\n\r\n  @yifeif can you help me merge this pr? thanks."]}, {"number": 10864, "title": "Feature Request: Sequence to Sequence Bucketing/Batching", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra Version 10.12.5\r\n- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0\r\n\r\n### Describe the feature\r\n`tf.contrib.training.bucket_by_sequence_length()` and `tf.contrib.training.bucket()` are useful for creating batches of similar-length sequences for training dynamic RNNs. I'd like to be able to create batches of similar-length `(input sequence, output sequence)` pairs to train Sequence to Sequence models with dynamic encoders and dynamic decoders. I may be wrong, but as far as I can tell, there is no easy way to adapt either of the current bucketing functions to create batches like this. The seq2seq translate [tutorial](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py) relies on a method `def get_batch(self, data, bucket_id)` to get such batches, but I think it'd be nicer if TensorFlow had a built-in function that didn't rely on a custom implementation and that integrated nicely with FIFOQueues for reading data.", "comments": ["When training seq2seq, this function is also useful:\r\n\r\nhttps://youtu.be/RIR_-Xlbp7s?t=6m51s", "@rickyhan, those are the exact functions I mentioned. Maybe I'm just misunderstanding how to use them, but my understanding is that they bucket single sequences, not pairs of sequences.", "@RylanSchaeffer this code sample may be helpful:\r\n\r\nhttps://github.com/tensorflow/tensor2tensor/blob/414cee3e216947b017a6f7535e5c8328b8ab95c2/tensor2tensor/utils/data_reader.py#L256", "Let me take a look and see if that provides the functionality that I'm looking for.", "Good luck :+1: ", "got it :)"]}, {"number": 10863, "title": "Added initial support to ROCm", "body": "1. Changed configure to ask for enabling ROCm if XLA is enabled\r\n2. Changed bazel.rc so that bazel config can take rocm as a value", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "This pull request is for review only. Please don't merge.", "Can one of the admins verify this patch?", "@adityaatluri Do we need to keep this PR around?", "Give me couple of days. Will let you know.", "Sounds good. Also, please make sure the CLA is in order. Thanks!", "The CLA is going to take a while. I'll create a new PR once I sign it."]}, {"number": 10862, "title": "Bug: MultiRNNCell.state_size is a tuple", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: 1080 Ti\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\n\r\nVersion 1:\r\n\r\nThe follow code will return LSTMStateTuple with `h` and `c`:\r\n\r\n```python\r\n\r\n        enc_cell = tf.contrib.rnn.LSTMCell(hidden_dim)\r\n\r\n        enc_inp_len = np.array([seq_length_in for _ in range(batch_size)])\r\n\r\n        ((encoder_fw_outputs,\r\n          encoder_bw_outputs),\r\n         (encoder_fw_final_state,\r\n          encoder_bw_final_state)) = (\r\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\r\n                                            cell_bw=enc_cell,\r\n                                            inputs=enc_inp,\r\n                                            sequence_length=enc_inp_len,\r\n                                            dtype=tf.float32)\r\n            )\r\n        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\r\n\r\n        encoder_final_state_c = tf.concat(\r\n            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\r\n\r\n        encoder_final_state_h = tf.concat(\r\n            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\r\n\r\n        encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\r\n            c=encoder_final_state_c,\r\n            h=encoder_final_state_h\r\n        )\r\n```\r\n\r\nHowever, if I run the following MultiCellRNN:\r\n\r\n```python\r\n        enc_cells = []\r\n        for i in range(0, encoder_depth):\r\n            with tf.variable_scope('enc_RNN_{}'.format(i)):\r\n                cell = tf.contrib.rnn.GRUCell(hidden_dim)  # Or LSTMCell(hidden_dim)\r\n                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\r\n                enc_cells.append(cell)\r\n        enc_cell = tf.contrib.rnn.MultiRNNCell(enc_cells)\r\n\r\n        ((encoder_fw_outputs,\r\n          encoder_bw_outputs),\r\n         (encoder_fw_final_state,\r\n          encoder_bw_final_state)) = (\r\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=enc_cell,\r\n                                            cell_bw=enc_cell,\r\n                                            inputs=enc_inp,\r\n                                            sequence_length=enc_inp_len,\r\n                                            dtype=tf.float32)\r\n            )\r\n\r\n        # encoder_fw_final_state is a Tensor from the .c part of LSTMStateTuple\r\n```\r\n\r\nIt will return state tensor(just the `c` part, not the `h` activation)", "comments": ["I was using GRUCell, I'll change that to LSTMCell and test it again.", "Not a bug. Please ignore this issue", "Turns out there is a related bug:\r\n\r\nThe state_size of output of MultiCellRNN consisting only LSTMCell and DropoutWrapper is a tuple not an LSTMStateTuple.\r\n\r\n # Code Example\r\n\r\n```python\r\nwith tf.variable_scope(\"DECODE\"):\r\n    dec_cells = []\r\n    # name=\"dec_cell_{}\".format(i)\r\n    for i in range(0, decoder_depth):\r\n        with tf.variable_scope('dec_RNN_{}'.format(i)):\r\n            cell = tf.contrib.rnn.GRUCell(hidden_dim * 2)\r\n            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1.0-dropout)\r\n            dec_cells.append(cell)\r\n    decoder_cell = tf.contrib.rnn.MultiRNNCell(dec_cells)\r\n\r\n    print decoder_cell.state_size\r\n    # (24, 24)\r\n    print encoder_final_state #\r\n    # result: LSTMStateTuple(c=<tf.Tensor 'ENCODE/concat_1:0' shape=(?, 24) dtype=float32>, h=<tf.Tensor 'ENCODE/concat_2:0' shape=(?, 24) dtype=float32>)#\r\n\r\n    # decoder_cell = tf.contrib.rnn.LSTMCell(hidden_dim*2) # BUG:\r\n\r\n    decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\r\n\r\n(24, 24)\r\nLSTMStateTuple(c=<tf.Tensor 'ENCODE/concat_1:0' shape=(?, 24) dtype=float32>, h=<tf.Tensor 'ENCODE/concat_2:0' shape=(?, 24) dtype=float32>)\r\nTraceback (most recent call last):\r\n  File \"/home/g/Desktop/seq2seq/run.py\", line 141, in <module>\r\n    decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 954, in raw_rnn\r\n    nest.assert_same_structure(initial_state, cell.state_size)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.py\", line 147, in assert_same_structure\r\n    _recursive_assert_same_structure(nest1, nest2, check_types)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.py\", line 119, in _recursive_assert_same_structure\r\n    % (type_nest1, type_nest2))\r\nTypeError: The two structures don't have the same sequence type. First structure has type <class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'>, while second structure has type <type 'tuple'>.\r\n```", "@tatatodd Could you confirm this is a bug?", "I did some research and found\r\n\r\nhttps://github.com/suriyadeepan/augmented_seq2seq/blob/01706d3869a42f3cf0bfe5c83f069646315a945e/bi_encoder.py#L54.\r\n\r\nHe uses `tf.scan` to manually generate an output and state tuple", "@ebrevdo, could you take a quick look?", "The output type of a MultiRnnCell is always a tuple.  It may be a tuple with one element... a LSTMStateTuple.  This is intended behavior.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Working as documented.\r\nhttps://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L211\r\n"]}, {"number": 10861, "title": "new release for tensorflow 1.2 release", "body": "Any plans for a 1.2 release?", "comments": []}, {"number": 10860, "title": "control_dependencies possibly broken", "body": "Hi all, I've been playing with control_dependencies in the latest stable version, and I get different results to version 1.0.1. The code below is adapted from the TF Adam optimiser, which might be affected as well.\r\n\r\nThe code I ran is\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ngraph = tf.Graph()\r\n\r\nwith graph.as_default():\r\n    x = tf.Variable(1.0, dtype=tf.float32, name='x')    \r\n    y = tf.Variable(10., dtype=tf.float32, name='y')\r\n    update_x = tf.assign(x, y)\r\n    with tf.control_dependencies([update_x]):\r\n        update_y = tf.assign(y, x * y)\r\n    update_op = tf.group(update_x, update_y)\r\n    init_op = tf.global_variables_initializer()\r\nprint tf.__version__\r\n\r\nres = []\r\nfor _ in range(10):\r\n    if 'sess' in locals():\r\n        locals()['sess'].close()\r\n    if 'sess' in globals():\r\n        globals()['sess'].close()\r\n\r\n    sess = tf.Session(graph=graph)\r\n    sess.run(init_op)\r\n    res += [sess.run([update_op, x, y])]\r\nprint np.mean([r[-1] for r in res])\r\nprint np.mean([r[-2] for r in res])\r\n```\r\n\r\nAnd get results \r\n```\r\n1.0.1\r\n100.0\r\n10.0\r\n```\r\nvs\r\n```\r\n1.2.0\r\n28.0\r\n5.5\r\n```\r\nsuggesting that the control dependencies are not used as expected in version 1.2.0. \r\nCan anyone spot any mistakes in the way I use the control dependencies?\r\nCheers,\r\nYarin", "comments": ["You evaluate update_op, x and y with sess.run. And there is no guarantee that x and y should be evaluated before or after update_op. Therefore the program is allowed to give you random results. At least that seems to be what is implemented now.", "changing the code above to\r\n```\r\n    sess.run(update_op)\r\n    res += [sess.run([x, y])]\r\n```\r\ndoes indeed result in \r\n```\r\n1.2.0\r\n100.0\r\n10.0\r\n```\r\nas expected. Is this intended behaviour? (the execution order of the list of ops changing randomly with each call)", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10859, "title": "Branch 159575817", "body": "", "comments": ["@davidzchen @nlopezgi could you take a look at the cuda bazel failures? Thanks!", "Moving to https://github.com/tensorflow/tensorflow/pull/10960 to include a few more fixes. Closing this one."]}, {"number": 10858, "title": "correctly vulcanize <link rel=\"import\" type=\"css\"", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for the PR @maximermilov. \r\n@dandelionmane how are we accepting tensorboard PRs now? Should changes be made in tensorboard repo instead?", "My bad.\r\nhttps://github.com/tensorflow/tensorboard/pull/109", "Yup, all TensorBoard PRs should go to tensorboard repo now :)"]}, {"number": 10857, "title": "No way to freeze fused BN stats", "body": "When fine-tuning networks trained with BN sometimes we want to freeze and use the accumulated moving averages while allowing the gradients to be backpropagated through the BN layer, but currently there is no way of doing so with fused BN, since when is_training = False the layer gives erroneous gradients. Of course, we could use the batch statistics from the new task to accumulate the stats, but it isn't possible in the case of batch_size = 1.\r\n\r\nI understand that due to the nature of the CuDNN kernel it might be hard to implement such feature, but a fused Batch Renorm layer could be a decent compromise, as it uses the moving averages when training as well as during inference.", "comments": ["@zhangyaobit can you comment on this?  Thanks!", "Setting is_training to False and at the same time doing gradient computation is expected to get erroneous gradients, because the second and third outputs of FusedBatchNorm (batch mean and batch variance) are only applicable/valid when is_training is True. \r\n\r\nCould you try train with fused batch norm, and fine-tune with non-fused batch norm?", "Our original network was trained with NCHW using fused batch norm, and non-fused batch norm for NCHW is around 6x slower https://github.com/tensorflow/tensorflow/issues/7551#issuecomment-280421351.", "Not sure if I understand correctly: are you saying fine tune is done with a batch size of 1 and non-fused batch norm doesn't work with a batch size of 1? And even if it works, it is 6x slower?\r\n\r\nHow much time do you expect fine tune currently to take (on the scale of hours or days? say on GPU)?", "Yes, fine tune is done with a batch size of 1 (on each GPU). I am training on a 4x 1080 machine, and fine tuning would take around ~20 hours. Non-fused batch norm does work (gives correct gradients), but it is way too slow and will push the fine tuning time beyond 100 hours. It would be faster to simply retrain the original network in NHWC and fine tune there with non-fused batch norm, which is ironic, considering NCHW is the CuDNN canonic format.", "It may make sense to simply forward the population mean and variance to the second and third outputs of FusedBatchNorm when fused=Tue. There is no performance penalty for this, since it is a forwarding instead of a copy.\r\n\r\nBefore this feature is implemented, you can modify the code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L815)  to set batch_mean and batch_variance to mean and variance when is_training is False. And change op.output[3] and op.outputs[4] respectively to [1] and [2] [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L667). Note that doing this on the python side may introduce actual copies (a slight performance penalty), so it is not as ideal as making this change on the C++ side (for example, use forwarding like [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_select.cc#L101)); but this should get you most of the GPU performance.\r\n\r\nPlease let me know if you think this proposed solution would (or would not) work for you.", "Adding @zheng-xq \r\n\r\nI marked this as contribution welcome for now. Anyone on the TensorFlow team or external developers are welcome to contribute if interested/have time. Potential contributors: please let us know if you have any questions.", "I'm closing this issue, as the freeze mode fused batch norm has been supported now: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L674\r\n", "Thank you!", "@zhangyaobit Thanks for providing the implementation for backward. However, I noticed that the performance of FusedBatchNormGrad when is_training=False is very pool. \r\nI didn't benchmark the op alone, but switching is_training on and off can affect my total training speed by 2.5x. My estimate is that the op itself would have at least 5x difference in speed.\r\nI guess the reason is that the implementation now is not based on fused kernels. This makes it as slow as the non-fused kernels.", "I used the kernel `BatchNormWithGlobalNormalizationGrad` for the backward implementation when is_training=False. This improves the speed a lot (my overall training time is then closed to the fused case when is_training=True).\r\nTo really use this op there are still some issues:\r\n1. The op `BatchNormWithGlobalNormalizationGrad` was marked deprecated. I unmarked it to test but not sure if that's desired. It's a helpful op in this case.\r\n2. The op doesn't support backward. This will break \"grad of grad\" unit tests. I think \"grad of grad\" is less important but it could be implemented if really needed.", "Thanks for bringing this up. We are aware of this issue. Currently in freeze mode the forward op is fused, but backward one is not. As you noticed, the gradient for freeze mode is written in python, composed of other TensorFlow ops, instead of as a fused implementation inside C++ Compute() method. \r\n\r\nI think the best thing is to implement it in C++ and add the second-order gradient for it (needed by models like GAN and other second-order optimizers). Re-opened and anyone is welcome to contribute.", "I'll send a PR on this.", "Hi @ppwwyyxx ,\r\n\r\nI notice that currently only NHWC is supported for FusedBatchNormFreezeGrad and tf.transpose is needed in python if the input is NCHW.  This can take enourmous time if batch norm is adopted in large scale and the data format is NCHW. \r\n\r\nDo you consider adding support for NCHW?  ", "I don't have time for this. \r\nIn fact a thorough benchmark is needed because the performance of the normal reduction ops has been improved a lot since last year. It's quite likely that my naive cuda kernel isn't very performant in NHWC or NCHW compared to a non-fused multi-op implementation.", "Thanks for your prompt response.\r\n\r\nI have done a quick benchmark using my network. For fused BN, it takes ~1.43 second/batch while for non-fused BN, it takes ~1.32 second/batch regardless if NCHW or NHWC is used. It seems for freezed BN, non-fused is faster.", "A quick test shows that fused & non-fused BN have similar backward speed when freezed:\r\n```python\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\nimport time\r\nimport os\r\nfrom tensorflow.python.client import device_lib\r\nlocal_device_protos = device_lib.list_local_devices()\r\nGPU_MODE = len([x.name for x in local_device_protos if x.device_type == 'GPU'])\r\n\r\nN = 64\r\nC = 128\r\nH, W = 128, 128\r\nprint(\"N, C, H, W:\", [N, C, H, W])\r\n\r\ndef benchmark_all(fuse, format):\r\n    shape4d = [N, C, H, W] if format == 'NCHW' else [N, H, W, C]\r\n\r\n    tf.reset_default_graph()\r\n    input = tf.get_variable('input', shape=shape4d, dtype=tf.float32)\r\n\r\n    scale = tf.get_variable('scale', shape=[C])\r\n    offset = tf.get_variable('offset', shape=[C])\r\n    mean = tf.random_normal(shape=[C])\r\n    var = tf.random_normal(shape=[C]) + 1.\r\n\r\n    if fuse:\r\n        output, _, _ = tf.nn.fused_batch_norm(\r\n            input, scale, offset, mean, var, epsilon=1e-5, data_format=format, is_training=False)\r\n    else:\r\n        if format == 'NCHW':\r\n            newshape = [1, C, 1, 1]\r\n            scale = tf.reshape(scale, newshape)\r\n            offset = tf.reshape(offset, newshape)\r\n            mean = tf.reshape(mean, newshape)\r\n            var = tf.reshape(var, newshape)\r\n        output = (input - mean) * (tf.rsqrt(var + 1e-5) * scale) + offset\r\n\r\n\r\n    forward_op = output.op\r\n    cost = tf.reduce_sum(output)\r\n    backward_op = tf.train.GradientDescentOptimizer(0.1).minimize(cost)\r\n\r\n\r\n    def benchmark(op, nr_iter=200, nr_warmup=10):\r\n        if not GPU_MODE:\r\n            nr_iter = nr_iter // 10\r\n            nr_warmup = nr_warmup // 5\r\n        for k in range(nr_warmup):\r\n            op.run()\r\n        start = time.perf_counter()\r\n        for k in range(nr_iter):\r\n            op.run()\r\n        end = time.perf_counter()\r\n        itr_per_sec = nr_iter * 1. / (end - start)\r\n        return itr_per_sec\r\n\r\n    sess = tf.Session()\r\n    with sess.as_default():\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        spd_forward = benchmark(forward_op)\r\n        print(\"Fuse={}, Format={}, Forward: {} itr/s\".format(fuse, format, spd_forward))\r\n        spd_backward = benchmark(backward_op)\r\n        print(\"Fuse={}, Format={}, Backward: {} itr/s\".format(fuse, format, spd_backward))\r\n\r\n\r\nif GPU_MODE:\r\n    formats = ['NHWC', 'NCHW']\r\nelse:\r\n    formats = ['NHWC']\r\nfor format in formats:\r\n    for fuse in [True, False]:\r\n        benchmark_all(fuse, format)\r\n```\r\n\r\nOutputs on GTX1080Ti:\r\n```\r\nN, C, H, W: [64, 128, 128, 128]\r\nFuse=True, Format=NHWC, Forward: 100.62081027587591 itr/s\r\nFuse=True, Format=NHWC, Backward: 61.392495723794156 itr/s\r\nFuse=False, Format=NHWC, Forward: 100.51494045143419 itr/s\r\nFuse=False, Format=NHWC, Backward: 60.875319893753236 itr/s\r\nFuse=True, Format=NCHW, Forward: 261.46341501158184 itr/s\r\nFuse=True, Format=NCHW, Backward: 44.5398623696861 itr/s\r\nFuse=False, Format=NCHW, Forward: 71.64600813406135 itr/s\r\nFuse=False, Format=NCHW, Backward: 48.042794214933586 itr/s\r\n```"]}, {"number": 10856, "title": "Support unknown dimension in tf.SparseTensor", "body": "In doing development calling API (tf.SparseTensor on release 1.1), I got the following error. It appears to be caused by unknown dimension \"batch\" which is present in n_indices, n_val, obj_ref.get_shape(). \r\n\r\nAdditionally, I wonder if we could have a list of APIs not supporting unknown dimension ....\r\n\r\n```\r\n  File \"/nlp/workspace/nlp-models/tf-seq2seq/seq2seq/..../..../...._model.py\", line 362, in _put_new_memory\r\n    delta = tf.SparseTensor(n_indices, n_val, obj_ref.get_shape())\r\n  File \"/home/..../workspace/prun/seq2seq/lib/python3.5/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 127, in __init__\r\n    dense_shape, name=\"dense_shape\", dtype=dtypes.int64)\r\n  File \"/home/..../workspace/prun/seq2seq/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 639, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/..../workspace/prun/seq2seq/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 704, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/..../workspace/prun/seq2seq/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 131, in _tensor_shape_tensor_conversion_function\r\n    \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 16, 30)\r\n```\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10855, "title": "Differentiate Variable (Model Parameters) and Mutable Tensor", "body": "In the current tensorflow, the concept of variables is about same as model parameters and mutable tensors. I would like to see them separate out. For example, it seems perfect legit to me to use tf.scatter_nd_update to update a tensor. However all scatter APIs require \"mutable tensor\". That made operations on tensors with \"batch\" parameter close to impossible.  Having the concept of mutable tensor will help a lot.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@melvinma I think you might have a misunderstanding about variables.  From the documentation:\r\n\r\n```\r\nVariables are in-memory buffers containing tensors.\r\n```\r\n\r\nHere are some links to further reading.\r\nhttps://www.tensorflow.org/versions/master/programmers_guide/variables\r\nhttps://www.tensorflow.org/versions/master/api_docs/python/tf/Variable\r\nhttps://www.tensorflow.org/versions/master/api_guides/python/state_ops#Variables\r\n\r\nNote that this question is better suited for stackoverflow; we use github for tracking bugs and feature requests.  If you have further questions about variables, please ask them on stackoverflow.  Thanks!"]}, {"number": 10854, "title": "attribute error when setting PredictionType.MULTIPLE_VALUE in Dynamic RNN Estimator", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorflow/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Can you share more information or log? The information is missing on your report.", "estimator = tf.contrib.learn.DynamicRNNEstimator(\r\n                                                    problemtype = ProblemType.CLASSIFICATION,\r\n                                                     predictiontype = PredictionType.SINGLE_VALUE,\r\n                                                     sequence_feature_columns = inputs\r\n                                                     num_classes=None,\r\n                                                     num_units=None,\r\n                                                     cell_type='basic_rnn',\r\n                                                     optimizer='SGD',\r\n                                                     learning_rate=0.1, )\r\n\r\nWhen DynamicRNNEstimator is initialized, it gives, prediction_type = PredictionType.SINGLE_VALUE results error: ", "I think your code fields' names and attribute values are wrong. The error you stated is due to that reason I guess. Below is a working code example, the `sequence_feature_columns = None,` in my case and other numeric values too. Please comment below if you have additional questions?\r\n```\r\nestimator = tf.contrib.learn.DynamicRnnEstimator(\r\nproblem_type = 1,\r\nprediction_type = 1,\r\nsequence_feature_columns = inputs\r\nsequence_feature_columns = None,\r\nnum_classes=2,\r\nnum_units=None,\r\ncell_type='basic_rnn',\r\noptimizer='SGD',\r\nlearning_rate=0.1, )\r\n```", "-Thanks. that worked for me. Sure.."]}, {"number": 10853, "title": "Mismatched delete in mkl_tfconv_op.cc", "body": "tensorflow\\core\\kernels\\mkl_tfconv_op.cc line 120\r\ndelete in_sizes;\r\n\r\nshould be: delete [] in_sizes;", "comments": ["@maddin200 Thanks for filing the bug, you're right.  Would you like to submit a PR to fix this?  Thanks!", "Better somebody more familiar with the code will do the change.\r\n", "Added PR #10955 for the fix."]}, {"number": 10852, "title": "GTX 1080 Ti Cuda Launch Failed", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorflow/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n('v1.1.0-rc2-9-gb7f85a7', '1.1.0-rc1')\r\n- **Bazel version (if compiling from source)**:\r\nBazel Release 0.5.0\r\n\r\n- **CUDA/cuDNN version**:\r\nCuda 8.0 cuDNN 8.0\r\n- **GPU model and memory**:\r\nGTX 1080Ti - 11GB memory\r\n- **Exact command to reproduce**:\r\n\r\nbazel-bin/inception/flowers_train \\\r\n  --train_dir=\"${TRAIN_DIR}\" \\\r\n  --data_dir=\"${DATA_DIR}\" \\\r\n  --pretrained_model_checkpoint_path=\"${MODEL_PATH}\" \\\r\n  --fine_tune=True \\\r\n  --initial_learning_rate=0.001 \\\r\n  --input_queue_memory_factor=2\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1088718/tf_env.txt)\r\n\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen fine tuning inception v3 model using the flowers data script, on custom data, during retraining I see the following error:\r\n\r\n2017-06-19 23:46:32.549857: step 5300, loss = 1.40 (61.9 examples/sec; 0.517 sec/batch)\r\n2017-06-19 23:46:39.510009: step 5310, loss = 1.43 (64.2 examples/sec; 0.499 sec/batch)\r\n2017-06-19 23:46:44.667435: step 5320, loss = 1.55 (64.3 examples/sec; 0.498 sec/batch)\r\n2017-06-19 23:46:46.691129: E tensorflow/stream_executor/cuda/cuda_driver.cc:1067] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED :: No stack trace available\r\n2017-06-19 23:46:46.691129: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED\r\n2017-06-19 23:46:46.691162: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n2017-06-19 23:46:46.691165: F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\n./start_image.sh: line 60: 27406 Aborted                 (core dumped) bazel-bin/inception/flowers_train --train_dir=\"${TRAIN_DIR}\" --data_dir=\"${DATA_DIR}\" --pretrained_model_checkpoint_path=\"${MODEL_PATH}\" --fine_tune=True --initial_learning_rate=0.001 --input_queue_memory_factor=2\r\n\r\nI changed the num classes and num examples per epoch as follows:\r\n 7    def num_classes(self):\r\n      8      \"\"\"Returns the number of classes in the data set.\"\"\"\r\n      9 -    return 5\r\n     10 +    return 21\r\n     11  \r\n     12    def num_examples_per_epoch(self):\r\n     13      \"\"\"Returns the number of examples in the data subset.\"\"\"\r\n     14      if self.subset == 'train':\r\n     15 -      return 3170\r\n     16 +      return 6342\r\n     17      if self.subset == 'validation':\r\n     18 -      return 500\r\n     19 +      return 1576\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["tf_env_collect script says you do not have cuDNN installed.\r\nNote that CUDA and cuDNN need to be installed separately.\r\n\r\nPlease install cuDNN following all instructions from NVIDIA and retry.\r\n\r\n\r\n", "I have made sure that the cudnn libraries are located in the /local/usr/cuda/lib64 and /local/usr/cuda/include:\r\n\r\nIf they were not, I could not build tensorflow with cuda support\r\n\r\ncookie@ceres:~/work/machine_learning/tensorflow/tools$ ls -l /usr/local/cuda/lib64/libcudnn*\r\n-rwxr-xr-x 1 root root 148M Jun 21 07:31 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 148M Jun 21 07:31 /usr/local/cuda/lib64/libcudnn.so.6\r\n-rwxr-xr-x 1 root root 148M Jun 21 07:31 /usr/local/cuda/lib64/libcudnn.so.6.0.21\r\n-rw-r--r-- 1 root root 138M Jun 21 07:31 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nIs there anything else I can check or do? The flowers_train script runs for multiple interations, before it crashes each time with different errors, as below:\r\n\r\n2017-06-21 10:32:20.376515: step 17080, loss = 0.96 (62.9 examples/sec; 0.509 sec/batch)\r\n2017-06-21 10:32:25.431246: step 17090, loss = 0.96 (62.7 examples/sec; 0.511 sec/batch)\r\n2017-06-21 10:32:30.495980: step 17100, loss = 0.89 (62.9 examples/sec; 0.509 sec/batch)\r\n2017-06-21 10:32:33.929246: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-06-21 10:32:33.929280: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n", "How about your `LD_LIBRARY_PATH`?\r\nFor running tensorflow, your `LD_LIBRARY_PATH` needs to include directories to your cuda, cudnn libcudart and libcupti so file directories.", "I checked all the libraries and they are located in the LD_LIBRARY_PATH:\r\n\r\ncookie@ceres:~/work/machine_learning/flowers_training$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/usr/local/cuda-8.0/lib64\r\n\r\nI just reran the model:\r\n2017-06-21 21:58:50.719064: step 550, loss = 1.14 (62.5 examples/sec; 0.512 sec/batch)\r\n2017-06-21 21:58:55.847020: step 560, loss = 1.22 (62.6 examples/sec; 0.511 sec/batch)\r\n2017-06-21 21:59:00.008349: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2017-06-21 21:59:00.008379: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n2017-06-21 21:59:00.008384: F tensorflow/stream_executor/cuda/cuda_dnn.cc:1961] failed to enqueue convolution on stream: CUDNN_STATUS_BAD_PARAM\r\n./train_flowers.sh: line 27: 13497 Aborted                 (core dumped) bazel-bin/inception/flowers_\r\n\r\nAnything else I can check?\r\n\r\n\r\n", "@gunan Anything else I can do to attempt to root cause this? I've tried different drivers, different cuda versions/ cudnn versions all with the same results.\r\n\r\nThis is even happening when running flowers_train on the flowers data with no changes to the model.", "Looks like with the update to `LD_LIBRARY_PATH` error mode has changed to `CUDNN_STATUS_BAD_PARAM`. When searching through old issues, it looks like the cause of this error is using TF built with another cudnn version. based on your comments, if you built on the same machine as you are runningg this should not be the case.\r\nBy any chance, do you have multiple versions of cudnn installed?\r\n", "Thank you for your time in helping me with this.\r\n\r\nI only have 1 version of cuda installed, cuda-8.0\r\n\r\nMy /usr/local looks as follows:\r\n\r\n> lrwxrwxrwx  1 root root    8 Jun 21 07:27 cuda -> cuda-8.0\r\n> drwxr-xr-x 14 root root 4.0K Jun 21 07:27 cuda-8.0\r\n\r\n\r\nMy /usr/local/cuda-8.0 looks as follows:\r\n\r\n> drwxr-xr-x  3 root root 4.0K Jun 21 07:27 bin\r\n> drwxr-xr-x  5 root root 4.0K Jun 21 07:27 doc\r\n> drwxr-xr-x  5 root root 4.0K Jun 21 07:27 extras\r\n> drwxr-xr-x  8 root root 4.0K Jun 21 07:27 libnsight\r\n> drwxr-xr-x  7 root root 4.0K Jun 21 07:27 libnvvp\r\n> drwxr-xr-x  3 root root 4.0K Jun 21 07:27 nvml\r\n> drwxr-xr-x  7 root root 4.0K Jun 21 07:27 nvvm\r\n> drwxr-xr-x 11 root root 4.0K Jun 21 07:27 samples\r\n> drwxr-xr-x  3 root root 4.0K Jun 21 07:27 share\r\n> drwxr-xr-x  2 root root 4.0K Jun 21 07:27 src\r\n> drwxr-xr-x  3 root root 4.0K Jun 21 07:27 targets\r\n> drwxr-xr-x  2 root root 4.0K Jun 21 07:27 tools\r\n> -rw-r--r--  1 root root  365 Jan 26 15:48 LICENSE\r\n> -rw-r--r--  1 root root  365 Jan 26 15:48 README\r\n> lrwxrwxrwx  1 root root   28 Jan 26 15:51 include -> targets/x86_64-linux/include\r\n> lrwxrwxrwx  1 root root   24 Jan 26 15:51 lib64 -> targets/x86_64-linux/lib\r\n> -rw-r--r--  1 root root   20 Jan 26 15:48 version.txt\r\n\r\nInside the lib64 symlink, I see the following:\r\n\r\n> <snip>....\r\n> -rwxr-xr-x 1 root root  148M Jun 21 07:31 libcudnn.so\r\n> -rwxr-xr-x 1 root root  148M Jun 21 07:31 libcudnn.so.6\r\n> -rwxr-xr-x 1 root root  148M Jun 21 07:31 libcudnn.so.6.0.21\r\n> <snip>....\r\n\r\nLD_LIBRARY_PATH is: \r\n\r\n> /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-8.0/lib64\r\n\r\nJust to make sure, I rebuilt tensorflow and re-ran the model a few more times with the same result\r\n\r\n> 2017-06-23 22:13:27.662981: step 16670, loss = 1.04 (64.7 examples/sec; 0.494 sec/batch)\r\n> 2017-06-23 22:13:28.589187: E tensorflow/stream_executor/cuda/cuda_driver.cc:1067] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n> 2017-06-23 22:13:28.589246: F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\n> 2017-06-23 22:13:28.589187: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n> 2017-06-23 22:13:28.589273: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n\r\n", "Just to make sure, could you locate `libcublas.so`, `libcuda.so`, `libcufft.so` and `libcurand.so` on your system?", "Everything _but_ libcuda.so is in /usr/local/cuda/lib64\r\n\r\nlibcuda.so is I can find a few:\r\n\r\n> /usr/local/cuda-8.0/targets/x86_64-linux/lib/stubs/libcuda.so\r\n> /usr/lib/i386-linux-gnu/libcuda.so\r\n> /usr/lib/x86_64-linux-gnu/libcuda.so\r\n\r\n\r\nLooking for installed cuda packages I see:\r\n\r\n> cookie@ceres:~$ apt list --installed | grep cuda\r\n> \r\n> WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n> \r\n> cuda/unknown,now 8.0.61-1 amd64 [installed]\r\n> cuda-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-command-line-tools-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-core-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cublas-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cublas-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cudart-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cudart-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cufft-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cufft-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-curand-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-curand-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cusolver-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cusolver-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cusparse-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-cusparse-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-demo-suite-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-documentation-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-driver-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-drivers/unknown,now 375.26-1 amd64 [installed,automatic]\r\n> cuda-license-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-misc-headers-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-npp-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-npp-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-nvgraph-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-nvgraph-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-nvml-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-nvrtc-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-nvrtc-dev-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-repo-ubuntu1604-8-0-local-ga2/now 8.0.61-1 amd64 [installed,local]\r\n> cuda-runtime-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-samples-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-toolkit-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> cuda-visual-tools-8-0/unknown,now 8.0.61-1 amd64 [installed,automatic]\r\n> libcuda1-375/xenial-updates,xenial-security,now 375.66-0ubuntu0.16.04.1 amd64 [installed,automatic]", "This was likely a hardware issue. Replacing the 1080Ti with an identical card, resolves the issue all software staying the same.\r\n\r\nThe issue can be closed. Many thanks for your time!", "@adanaila  may I ask which brand 1080Ti you use when error occurred ? \r\nI met the same situation on EVGA 1080ti ftw3 , not sure if the hardware problem , thanks for your help.", "@t0mst0ne I had a founder's card, it either crashed the system or resulted in the above errors.", "I have a problem same as you on 1080ti,what should i do?", "@t0mst0ne I have a problem same sa you on 1080ti,do i must change anather 1080ti?", "@adanaila , I ran into this issue when overclocking my card.\r\nIf you still have your old card, it might be worth it to give it another try. Perhaps, you could underclock it, to go easy on it."]}, {"number": 10851, "title": "ValueError: Attempt to reuse RNNCell - reuse flag does not work", "body": "### System information\r\n\r\n-  Linux Ubuntu 16.04\r\n- tensorflow-gpu==1.1.0\r\n\r\nI am getting this error in quite a complex graph, but I can reproduce it with a minimal (but hopefully representative) example below:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass Controller(object):\r\n    def __init__(self, batch_size, input_size):\r\n\r\n        self.batch_size = batch_size\r\n        self.input_size = input_size\r\n\r\n        with tf.name_scope(\"controller\"):\r\n            self.network_vars()\r\n\r\n            self.nn_output_size = None\r\n            with tf.variable_scope(\"shape_inference\"):\r\n                self.nn_output_size = self.get_nn_output_size()\r\n\r\n    def network_vars(self):\r\n        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(256)\r\n        self.state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\r\n\r\n    def network_op(self, x, state):\r\n        x = tf.convert_to_tensor(x)\r\n        return self.lstm_cell(x, state)\r\n\r\n    def get_state(self):\r\n        return self.state\r\n\r\n    def update_state(self, new_state):\r\n        return tf.no_op()\r\n\r\n    def process_input(self, x, state=None):\r\n        nn_output, nn_state = self.network_op(x, state)\r\n        return nn_output, nn_state\r\n\r\n    def get_nn_output_size(self):\r\n        input_tensor = np.zeros([self.batch_size, self.input_size], dtype=np.float32)\r\n        output_vector, _ = self.network_op(input_tensor, self.get_state())\r\n        shape = output_vector.get_shape().as_list()\r\n\r\n        if len(shape) > 2:\r\n            raise ValueError(\"Expected the neural network to output a 1D vector\")\r\n        else:\r\n            return shape[1]\r\n\r\n\r\nclass DNC(object):\r\n    def __init__(self, batch_size, input_size):\r\n        self.batch_size = batch_size\r\n        self.input_size = input_size\r\n        self.controller = Controller(batch_size, input_size)\r\n        self.build_graph()\r\n\r\n    def _step_op(self, x, controller_state=None):\r\n        _, nn_state = self.controller.process_input(x, controller_state)\r\n        return [nn_state[0], nn_state[1]]\r\n\r\n    def _loop_body(self, t, controller_state):\r\n        x = np.random.random_sample((self.batch_size, self.input_size)).astype(np.float32)\r\n        output_list = self._step_op(x, controller_state)\r\n        new_controller_state = tf.contrib.rnn.LSTMStateTuple(output_list[0], output_list[1])\r\n        return t+1, new_controller_state\r\n\r\n    def build_graph(self):\r\n        controller_state = self.controller.get_state()\r\n        if not isinstance(controller_state, tf.contrib.rnn.LSTMStateTuple):\r\n            controller_state = tf.contrib.rnn.LSTMStateTuple(controller_state[0], controller_state[1])\r\n\r\n        with tf.variable_scope(\"sequence_loop\") as scope:\r\n            time = tf.constant(0, dtype=tf.int32)\r\n\r\n            final_results = tf.while_loop(\r\n                cond=lambda time, *_: time < 50,\r\n                body=self._loop_body,\r\n                loop_vars=(time, controller_state),\r\n                parallel_iterations=32,\r\n                swap_memory=True\r\n            )\r\n\r\nif __name__ == \"__main__\":\r\n    batch_size = 32\r\n    input_size = 10\r\n    dnc = DNC(batch_size, input_size)\r\n```\r\n\r\nThe traceback of the issue is:\r\n```\r\nfrancescoferroni@francescoferroni:~$ python controller.py \r\nTraceback (most recent call last):\r\n  File \"controller.py\", line 83, in <module>\r\n    dnc = DNC(batch_size, input_size)\r\n  File \"controller.py\", line 52, in __init__\r\n    self.build_graph()\r\n  File \"controller.py\", line 77, in build_graph\r\n    swap_memory=True\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2623, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2456, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2406, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"controller.py\", line 60, in _loop_body\r\n    output_list = self._step_op(x, controller_state)\r\n  File \"controller.py\", line 55, in _step_op\r\n    _, nn_state = self.controller.process_input(x, controller_state)\r\n  File \"controller.py\", line 33, in process_input\r\n    nn_output, nn_state = self.network_op(x, state)\r\n  File \"controller.py\", line 24, in network_op\r\n    return self.lstm_cell(x, state)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\r\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/contextlib.py\", line 82, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/francescoferroni/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 77, in _checked_scope\r\n    type(cell).__name__))\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7ff4075754e0> with a different variable scope than its first use.  First use of cell was with scope 'shape_inference/basic_lstm_cell', this attempt is with scope 'sequence_loop/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n```\r\n\r\nIf I use tensorflow 1.0 rather than 1.1 it causes no issues.\r\n```\r\nfrancescoferroni@francescoferroni:~$ source Repositories/tfr10/bin/activate\r\n(tfr10) francescoferroni@francescoferroni:~$ python controller.py \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n(tfr10) francescoferroni@francescoferroni:~$ \r\n```\r\n\r\nFor the new tensorflow version, I have tried to add a reuse=True flag, as per [#9401](https://github.com/tensorflow/tensorflow/issues/9401), when defining the LSTM cell, but then I  get another error:\r\n`ValueError: Variable shape_inference/basic_lstm_cell/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n`\r\n\r\nAny help would be greatly appreciated.\r\n\r\nBest,\r\nFrancesco", "comments": ["I think you need the define the cell at a global scope. Otherwise the while loop creates one every time step", "@fferroni FYI this question is better asked on stackoverflow; we use github issues to track bugs and features, while we use stackoverflow for general help.\r\n\r\nI believe the description of the problem and the suggested solution is in the last line of your log output:\r\n\r\n---\r\n\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x7ff4075754e0> with a different variable scope than its first use.  First use of cell was with scope 'shape_inference/basic_lstm_cell', this attempt is with scope 'sequence_loop/basic_lstm_cell'.  Please create a new instance of the cell if you would like it to use a different set of weights.  If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If before you were using the same cell instance as both the forward and reverse cell of a bidirectional RNN, simply create two instances (one for forward, one for reverse).  In May 2017, we will start transitioning this cell's behavior to use existing stored weights, if any, when it is called with scope=None (which can lead to silent model degradation, so this error will remain until then.)\r\n\r\n---"]}, {"number": 10850, "title": "Pass rate and other settings to parent classes", "body": "See https://github.com/tensorflow/tensorflow/issues/10845", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 10849, "title": "Java: equality semantics for Operation and Output", "body": "* Add .equals(), .hashCode() and .toString() for value objects Operation\r\n  and Output.\r\n* Annotate these value objects @Immutable for compile-time validation.", "comments": ["@asimshankar mind taking a look? Thanks!", "Jenkins, test this please"]}, {"number": 10848, "title": "[OpenCL] Registers RGBToHSV and HSVToRGB (#91)", "body": "* [OpenCL] Added RGBToHSV and HSVToRGB\r\n\r\n* Aligning \\", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@jeffmak is already added to Corp CLA .. the commit email address matches it.\r\n@jeffmak can you OK this?", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 10847, "title": "Something error when I run  iris_monitors.py ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.0.1\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:import tensorflow\r\n\r\n### Describe the problem\r\nWhen I run iris_monitors.py in Pycharm file(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/monitors/iris_monitors.py), It has an error :\r\nTraceback (most recent call last):\r\n  File \"F:/temp/Python/temp.py\", line 92, in <module>\r\n    tf.app.run()\r\n  File \"C:\\software\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"F:/temp/Python/temp.py\", line 35, in main\r\n    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float)\r\n  File \"C:\\software\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\", line 47, in load_csv_with_header\r\n    header = next(data_file)\r\nStopIteration\r\n\r\n### Source code / logs\r\nC:\\software\\Python\\Python35\\python.exe F:/temp/Python/temp.py\r\nTraceback (most recent call last):\r\n  File \"F:/temp/Python/temp.py\", line 92, in <module>\r\n    tf.app.run()\r\n  File \"C:\\software\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"F:/temp/Python/temp.py\", line 35, in main\r\n    filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float)\r\n  File \"C:\\software\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py\", line 47, in load_csv_with_header\r\n    header = next(data_file)\r\nStopIteration\r\n\r\nProcess finished with exit code 1\r\n", "comments": ["@LuoJiaji Can you try the latest TensorFlow 1.2.0?"]}, {"number": 10846, "title": "Fix Java OperationBuilder documentation example", "body": "\"Constant\" is not a registered Op type, \"Const\" is.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 10845, "title": "Keras Dropout layer changes results with dropout=0.0", "body": "I am using the current version pypi 1.2.0, but also found this \"problem\" in the master I compiled about two weeks ago. I am running Gentoo linux and tensorflow is installed in an virtualenv.\r\n\r\nMaybe I am misunderstanding the concept of a dropout layer, but when I add a Dropout-Layer with 0% dropout, it still alters my results:\r\n\r\nfrom reuters classification example:\r\n```\r\nmodel = Sequential()\r\nmodel.add(Dense(128, input_shape=(max_words,)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.0))\r\nmodel.add(Dense(num_classes))\r\nmodel.add(Activation('softmax'))\r\n```\r\nAfter 3 epochs I get:\r\n```\r\nEpoch 3/3\r\n  32/8083 [..............................] - ETA: 0s - loss: 0.7321 - acc: 0.8125\r\n 512/8083 [>.............................] - ETA: 0s - loss: 0.8597 - acc: 0.8145\r\n 928/8083 [==>...........................] - ETA: 0s - loss: 0.8689 - acc: 0.8017\r\n1344/8083 [===>..........................] - ETA: 0s - loss: 0.9041 - acc: 0.7954\r\n1792/8083 [=====>........................] - ETA: 0s - loss: 0.8979 - acc: 0.7969\r\n2304/8083 [=======>......................] - ETA: 0s - loss: 0.8896 - acc: 0.7969\r\n2784/8083 [=========>....................] - ETA: 0s - loss: 0.8764 - acc: 0.7989\r\n3168/8083 [==========>...................] - ETA: 0s - loss: 0.8678 - acc: 0.8018\r\n3648/8083 [============>.................] - ETA: 0s - loss: 0.8666 - acc: 0.8021\r\n4096/8083 [==============>...............] - ETA: 0s - loss: 0.8656 - acc: 0.8013\r\n4576/8083 [===============>..............] - ETA: 0s - loss: 0.8536 - acc: 0.8018\r\n5120/8083 [==================>...........] - ETA: 0s - loss: 0.8409 - acc: 0.8033\r\n5664/8083 [====================>.........] - ETA: 0s - loss: 0.8369 - acc: 0.8054\r\n6048/8083 [=====================>........] - ETA: 0s - loss: 0.8385 - acc: 0.8042\r\n6592/8083 [=======================>......] - ETA: 0s - loss: 0.8431 - acc: 0.8025\r\n7136/8083 [=========================>....] - ETA: 0s - loss: 0.8448 - acc: 0.8028\r\n7680/8083 [===========================>..] - ETA: 0s - loss: 0.8490 - acc: 0.8025\r\n8083/8083 [==============================] - 0s - loss: 0.8473 - acc: 0.8032 - val_loss: 0.9853 - val_acc: 0.7920\r\n  32/2246 [..............................] - ETA: 0s\r\n1568/2246 [===================>..........] - ETA: 0s\r\nTest score: 0.934106262688\r\nTest accuracy: 0.777382012467\r\n```\r\nand without a dropout layer:\r\n```\r\nEpoch 3/3\r\n  32/8083 [..............................] - ETA: 0s - loss: 0.5419 - acc: 0.8750\r\n 544/8083 [=>............................] - ETA: 0s - loss: 0.4974 - acc: 0.8842\r\n1088/8083 [===>..........................] - ETA: 0s - loss: 0.5429 - acc: 0.8722\r\n1664/8083 [=====>........................] - ETA: 0s - loss: 0.5568 - acc: 0.8762\r\n2208/8083 [=======>......................] - ETA: 0s - loss: 0.5523 - acc: 0.8773\r\n2752/8083 [=========>....................] - ETA: 0s - loss: 0.5494 - acc: 0.8790\r\n3296/8083 [===========>..................] - ETA: 0s - loss: 0.5437 - acc: 0.8799\r\n3808/8083 [=============>................] - ETA: 0s - loss: 0.5420 - acc: 0.8792\r\n4352/8083 [===============>..............] - ETA: 0s - loss: 0.5446 - acc: 0.8750\r\n4896/8083 [=================>............] - ETA: 0s - loss: 0.5405 - acc: 0.8754\r\n5440/8083 [===================>..........] - ETA: 0s - loss: 0.5381 - acc: 0.8756\r\n5984/8083 [=====================>........] - ETA: 0s - loss: 0.5392 - acc: 0.8755\r\n6528/8083 [=======================>......] - ETA: 0s - loss: 0.5459 - acc: 0.8738\r\n7104/8083 [=========================>....] - ETA: 0s - loss: 0.5482 - acc: 0.8740\r\n7648/8083 [===========================>..] - ETA: 0s - loss: 0.5527 - acc: 0.8730\r\n8083/8083 [==============================] - 0s - loss: 0.5525 - acc: 0.8727 - val_loss: 0.9100 - val_acc: 0.7898\r\n  32/2246 [..............................] - ETA: 0s\r\n1664/2246 [=====================>........] - ETA: 0s\r\nTest score: 0.883166806993\r\nTest accuracy: 0.792074799644\r\n```\r\n\r\nWhile the validation and test results are quite similar, the model without dropout overfits much more. \r\n\r\nAlso, if I set dropout to 1.0, the model should not be able to learn anything (as everything is dropped):\r\n```\r\nEpoch 3/3\r\n  32/8083 [..............................] - ETA: 0s - loss: 0.8313 - acc: 0.7812\r\n 576/8083 [=>............................] - ETA: 0s - loss: 0.7561 - acc: 0.8351\r\n1120/8083 [===>..........................] - ETA: 0s - loss: 0.8669 - acc: 0.8000\r\n1632/8083 [=====>........................] - ETA: 0s - loss: 0.8805 - acc: 0.7978\r\n2176/8083 [=======>......................] - ETA: 0s - loss: 0.8780 - acc: 0.7973\r\n2720/8083 [=========>....................] - ETA: 0s - loss: 0.8742 - acc: 0.7978\r\n3264/8083 [===========>..................] - ETA: 0s - loss: 0.8693 - acc: 0.7990\r\n3808/8083 [=============>................] - ETA: 0s - loss: 0.8616 - acc: 0.7996\r\n4352/8083 [===============>..............] - ETA: 0s - loss: 0.8565 - acc: 0.8001\r\n4896/8083 [=================>............] - ETA: 0s - loss: 0.8480 - acc: 0.8025\r\n5440/8083 [===================>..........] - ETA: 0s - loss: 0.8499 - acc: 0.8007\r\n5984/8083 [=====================>........] - ETA: 0s - loss: 0.8492 - acc: 0.8025\r\n6560/8083 [=======================>......] - ETA: 0s - loss: 0.8498 - acc: 0.8023\r\n7136/8083 [=========================>....] - ETA: 0s - loss: 0.8505 - acc: 0.8014\r\n7648/8083 [===========================>..] - ETA: 0s - loss: 0.8511 - acc: 0.8010\r\n8083/8083 [==============================] - 0s - loss: 0.8484 - acc: 0.8021 - val_loss: 0.9544 - val_acc: 0.7909\r\n  32/2246 [..............................] - ETA: 0s\r\n1536/2246 [===================>..........] - ETA: 0s\r\nTest score: 0.92991881655\r\nTest accuracy: 0.780498664292\r\n```", "comments": ["It looks like keras uses the Default of Tensorflow, which is 0.5:\r\n\r\n```python\r\n  def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\r\n    self.supports_masking = True\r\n    # Inheritance call order:\r\n    # 1) tf.layers.Dropout, 2) keras.layers.Layer, 3) tf.layers.Layer\r\n    super(Dropout, self).__init__(**kwargs)\r\n```\r\n\r\nrate and other arguments are not given to parent classes kwargs.", "@ribx Thanks for filing the bug, and the PR for the fix!  Would you mind updating this issue when your PR is merged?  Thanks!"]}, {"number": 10844, "title": "compiling tensorflow/cc/example/example.cc with error \"You must define TF_LIB_GTL_ALIGNED_CHAR_ARRAY for your compiler.\" in Windows 10 Visualstudio 2015", "body": " Hi I am newbie of tensorflow.\r\n\r\n\r\n I wanna use tensorflow through C/C++ api on visual studio 2015.\r\n\r\n I have built tensorflow (library?) described in below link with Cmake.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\r\n\r\n \r\nafter Cmake,\r\n\r\n I tried to build  \"tensorflow/cc/example/example.cc\"\r\n but occured error\r\n\r\n\tC1189\t#error:  \"You must define TF_LIB_GTL_ALIGNED_CHAR_ARRAY for your compiler.\"\ttensortest\td:\\temp\\tensorflow\\tensorflow\\core\\lib\\gtl\\manual_constructor.h\t97\t\r\n\r\n\r\nthen  failed to build example.\r\n\r\n What is that mean and how to define TF_LIB_GTL_ALIGNED_CHAR_ARRAY in Visualstudio 2015?\r\n\r\n \r\n\r\n\r\n thank you in advance.\r\n\r\n\r\n/GS /GL /analyze- /W3 /Gy /Zc:wchar_t /I\"D:\\temp\\tensorflow\\tensorflow\\contrib\\cmake\\build\\protobuf\\src\\protobuf\\src\" /I\"D:\\temp\\tensorflow\" /I\"D:\\temp\\tensorflow\\tensorflow\\contrib\\cmake\\build\\eigen\\src\\eigen\" /I\"D:\\temp\\tensorflow\\tensorflow\\contrib\\cmake\\build\" /Zi /Gm- /O2 /sdl /Fd\"Release\\vc140.pdb\" /Zc:inline /fp:precise /D \"WIN32\" /D \"NDEBUG\" /D \"_CONSOLE\" /D \"_UNICODE\" /D \"UNICODE\" /errorReport:prompt /WX- /Zc:forScope /Gd /Oy- /Oi /MD /Fa\"Release\\\" /EHsc /nologo /Fo\"Release\\\" /Fp\"Release\\tensortest.pch\" \r\n", "comments": ["This may be more of a question for Stack Overflow, but I have run into that error when I omit `#define COMPILER_MSVC` in the header file.  You may also need to `#define NOMINMAX` as well, or you'll run into a bunch of errors about parentheses.  \r\n\r\nI wrote a blog post describing some of the issues I had building on Windows here: https://joe-antognini.github.io/machine-learning/windows-tf-project", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10843, "title": "Compiling tensorflow fails with error: no matching function for call to 'Permute(tensorflow::gtl::ArraySlice<long long int>&, const std::vector<llvm::Value*, std::allocator<llvm::Value*> >&)'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: Trying to compile from source\r\n- **TensorFlow version (use command below)**: 1.2.0 release\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0.61/6.0.21\r\n- **GPU model and memory**: 1080 Ti 11 GB\r\n- **Exact command to reproduce**: The PKGBUILD https://paste.xinu.at/XaChkb/\r\nIf you are not familiar with this format, basically just execute `prepare()` and then `build()`.\r\n\r\n### Describe the problem\r\nI'm the packager for the official tensorflow package in Arch Linux and I was trying to update 1.1.0 to 1.2.0 and it fails to build. Full build log attached. We always build in clean chroots.\r\n\r\nllvm 4.0, gcc 7.1.1\r\n\r\n### Source code / logs\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/1087523/log.txt)\r\n", "comments": ["@svenstaro It seems that the version of gcc you're using doesn't support the same template argument deduction that we get with the version that we're using:\r\n\r\n```\r\nERROR: /build/tensorflow/src/tensorflow-1.2.0/tensorflow/compiler/xla/service/llvm_ir/BUILD:60:1: C++ compilation of rule '//tensorflow/compiler/xla/service/llvm_ir:ir_array' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 133 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/compiler/xla/service/llvm_ir/ir_array.cc: In member function 'xla::llvm_ir::IrArray::Index xla::llvm_ir::IrArray::Index::SourceIndexOfTranspose(const xla::Shape&, const xla::Shape&, tensorflow::gtl::ArraySlice<long long int>, llvm::IRBuilder<>*) const':\r\ntensorflow/compiler/xla/service/llvm_ir/ir_array.cc:161:44: error: no matching function for call to 'Permute(tensorflow::gtl::ArraySlice<long long int>&, const std::vector<llvm::Value*, std::allocator<llvm::Value*> >&)'\r\n       Permute(dimension_mapping, multidim());\r\n                                            ^\r\nIn file included from tensorflow/compiler/xla/service/llvm_ir/ir_array.cc:24:0:\r\n./tensorflow/compiler/xla/util.h:197:16: note: candidate: template<template<class ...> class C, class T> std::vector<T> xla::Permute(tensorflow::gtl::ArraySlice<long long int>, C<T>)\r\n std::vector<T> Permute(tensorflow::gtl::ArraySlice<int64> permutation,\r\n                ^~~~~~~\r\n./tensorflow/compiler/xla/util.h:197:16: note:   template argument deduction/substitution failed:\r\ntensorflow/compiler/xla/service/llvm_ir/ir_array.cc:161:44: note:   candidate expects 1 argument, 2 provided\r\n       Permute(dimension_mapping, multidim());\r\n                                            ^\r\ntensorflow/compiler/xla/service/llvm_ir/ir_array.cc:161:44: note:   candidate expects 1 argument, 2 provided\r\ntensorflow/compiler/xla/service/llvm_ir/ir_array.cc:161:44: note:   'std::vector<llvm::Value*, std::allocator<llvm::Value*> >' is not derived from 'C<T>'\r\n```\r\n\r\nA possible (untested) fix is to change this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/llvm_ir/ir_array.cc#L161\r\n\r\nfrom:\r\n```\r\n  std::vector<llvm::Value*> operand_multidim_index =\r\n      Permute(dimension_mapping, multidim());\r\n```\r\n\r\nto:\r\n```\r\n  std::vector<llvm::Value*> operand_multidim_index =\r\n      Permute<std::vector, llvm::Value*>(dimension_mapping, multidim());\r\n```\r\n\r\nCan you try that out to see if it fixes things?  And if it works, would you mind creating a PR for this fix?  I'm suggesting this route since it'll ensure it actually fixes the problem for you.  Thanks!", "Sorry but could you provide a patch?", "@svenstaro Can you try PR #10868, which has the change I outlined above?", "Doesn't work, there seem to be other places where the same problem occurs. Full build log: \r\n[log.txt](https://github.com/tensorflow/tensorflow/files/1092836/log.txt)\r\n", "@svenstaro Can you give the latest version of PR #10868 another try?", "Yep, that one works fine!", "Cool, PR #10868 has been merged and should fix this issue."]}, {"number": 10842, "title": "Feature request: add more options to Luong attention", "body": "Currently, the best results of the Luong attention paper [(Minh-Thang Luong, Hieu Pham, Christopher D. Manning. \"Effective Approaches to Attention-based Neural Machine Translation.\" EMNLP 2015.)](https://arxiv.org/abs/1508.04025) cannot be reproduced with the implementation of `tf.contrib.seq2seq.LuongAttention`. Features that are missing:\r\n\r\n- **Local attention**: attend to a window of time steps, rather than to all of the time steps of the encoder output (global attention). The window size should be a hyperparameter that the user can tune.\r\n- **Different scoring functions**: currently, the scoring function is limited to a dot products between each encoder output and the decoder output. The paper shows better results with \"general\" scoring (all of the encoder outputs are multiplied by one learnable matrix) and also explores the option of using Bahdanau-like scoring (concatenate and multiply by a learnable matrix, then apply tanh and take a dot product with a learnable vector).\r\n- **Predictive alignments**: while the probability function can be replaced, it would be nice to add predictive alignment as a function, and make the implementation of both monotonic and predictive alignments behave well with local attention limited to a time window (changes shape of learnables).\r\n- **Input-feeding approach**: (please correct me in this one if I am mistaken) the current implementation is missing the final step that computes a prediction by concatenating the context vector with the decoder output, weights them and applies tanh (let this be `s_t=tanh(W [c_t; h_t])`). Passing `s_t` through a softmax layer gives the prediction distribution, but passing it as is to the next input improved performance in the paper.\r\n\r\nOn a sidenote: it also seems to be that there is no difference in the key and query vectors between the Bahdanau and Luong attention mechanisms, when there should be. Bahdanau attention has a computation pathway starting from the previous decoder output `h_{t-1} -> a_t -> c_t -> h_t`, while Luong attention starts from the current output `h_t -> a_t -> c_t -> s_t`.", "comments": ["@ebrevdo. Is anyone going to work on this? Should we mark this contributions welcome if we want this included but won't do it at google.", "Thang, can you comment?\n\nOn Jun 26, 2017 2:30 PM, \"Andrew Selle\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo>. Is anyone going to work on this?\n> Should we mark this contributions welcome if we want this included but\n> won't do it at google.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10842#issuecomment-311187626>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6WvPh_pJ7bk__DQcD-GJavXts62ks5sICL5gaJpZM4N_QJ8>\n> .\n>\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@lmthang which of these features do you feel is important?", "@ebrevdo  Local attention, I think ", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @lmthang, @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi all,\r\n\r\nSorry for a late reply. Here's my brief answer:\r\n* Local attention, predictive alignments: unfortunately, we don't have the bandwidth for this. Local attention, while being an interesting idea, is more complicated to implement and train. Contributions are welcome though I expect the contributors need to demonstrate convincing gains on over nowadays strong [baselines](github.com/tensorflow/nmt) and new [advances in attention](https://arxiv.org/abs/1706.03762).\r\n\r\n* Input-feeding approach: yes, we missed the tanh() part but the performance is found to be similar. See answers in  https://github.com/tensorflow/nmt/issues/257.\r\n\r\n* Different scoring functions: in fact, we are implementing the general approach for Luong attention, see this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L402). Similarly for Bahdanau attention if you read the [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L546) carefully. \r\n\r\nHope that clarifies!\r\n", "Nagging Assignees @lmthang, @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @lmthang, @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @lmthang, @ebrevdo: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @lmthang, @ebrevdo: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @lmthang, @ebrevdo: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 10841, "title": "Fix references", "body": "This PR fixes deprecated URLs.", "comments": ["Can one of the admins verify this patch?", "Thanks @taehoonlee!"]}, {"number": 10840, "title": "Update sample_distorted_bounding_box to use Tensor Input for min_object_covered", "body": "This fix tries to address the issue raised in #10715 where it was not possible\r\nto use dynamic values for min_object_covered in sample_distorted_bounding_box.\r\n\r\nThis fix created an Op of SampleDistortedBoundingBoxV2 so that min_object_covered\r\ntakes input. SampleDistortedBoundingBox remains the same (min_object_covered as attr).\r\n\r\nAdditional test cases have been added.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@vrv could you take a look at this change? Thanks!", "@vrv Thanks for the review. The PR has been updated. Please take a look and let me know if there are any issues.", "@tensorflow-jenkins test this please!", "I think this adds an op and will need API review. Could you run:\r\n\r\n```\r\n   $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\n\r\nand push again? It'll be clear what the API change is.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Thanks @drpngx. The PR has been updated to address the api compatibility test failure. Please take a look and let me know if there are any issues. ", "Ugh, somehow all the commits ended up on the CL. Could you rebase and push again?", "CLAs look good, thanks!\n\n<!-- ok -->", "Adding @martinwicke for API review.", "Thanks @drpngx . The PR has been rebased and pushed.", "Jenkins, test this please.", "@martinwicke @vrv @drpngx The PR has been updated as suggested. Please take a look.", "@martinwicke Thanks for the review. There was some confusion from my side before about the workflow. Now the PR has been updated with the following changes:\r\nThis PR:\r\n1. New kernel (`SampleDistortedBoundingBoxV2Op`) has been registered.\r\n2. `SampleDistortedBoundingBoxV2` is hidden in python (tensorflow/python/ops/hidden_ops.txt)\r\n\r\nNext PR (I will submit it after 3 weeks):\r\n1. `SampleDistortedBoundingBox` will be hidden in python (tensorflow/python/ops/hidden_ops.txt)\r\n2. An wrapper `sample_distorted_bounding_box` will be added in `image_ops_impl.py` and calls `sample_distorted_bounding_box_v2`.\r\n\r\nPlease take a look at the update PR and let me know if the above changes matches the API workflow.", "You can hide the op and write the wrapper now, just make it point to the\nold version.\n\nOn Jun 27, 2017 9:04 AM, \"Yong Tang\" <notifications@github.com> wrote:\n\n@martinwicke <https://github.com/martinwicke> Thanks for the review. There\nwas some confusion from my side before about the workflow. Now the PR has\nbeen updated with the following changes:\nThis PR:\n\n   1. New kernel (SampleDistortedBoundingBoxV2Op) has been registered.\n   2. SampleDistortedBoundingBoxV2 is hidden in python\n   (tensorflow/python/ops/hidden_ops.txt)\n\nNext PR (I will submit it after 3 weeks):\n\n   1. SampleDistortedBoundingBox will be hidden in python\n   (tensorflow/python/ops/hidden_ops.txt)\n   2. An wrapper sample_distorted_bounding_box will be added in\n   image_ops_impl.py and calls sample_distorted_bounding_box_v2.\n\nPlease take a look at the update PR and let me know if the above changes\nmatches the API workflow.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/10840#issuecomment-311404509>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbdsGzRVB6w_6InKX9XqCUleqUYDjks5sISfqgaJpZM4N-_FP>\n.\n", "@martinwicke @drpngx Thanks. The PR has been updated with both `SampleDistortedBoundingBox` and `SampleDistortedBoundingBoxV2` hidden. A wrapper has been added in image_ops_impl.py.\r\n\r\nI will create another PR after 3 weeks. \r\n\r\nPlease take a look.", "Jenkins, test this please."]}, {"number": 10839, "title": "DirectSession Run() : Variable name  typo", "body": "In _core/common_runtime/direct_session.cc (end of DirectSession::Run(...))_ name correction : parition_graph_defs -> partition_graph_defs", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 10838, "title": "Compiling TensorFlow gives 3k lines of warnings!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 (4.8 kernel)\r\n- **TensorFlow installed from (source or binary)**: Compiling TF from Source\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: CUDA 8.0, CuDNN 5.1\r\n- **GPU model and memory**: 1050Ti 4GB (Notebook version) (Intel i5-7300hq CPU)\r\n- **Exact command to reproduce**: Compiling from source [following documentation](https://www.tensorflow.org/install/install_sources).\r\n\r\n### Describe the problem\r\nI compiled tensorflow from source. The process finished successfully and the binary managed to install and run successfully. What seems strange is that during the process I got ~3k lines of warnings. I am linking to them at the end of the issue. I am wondering if that's expected behavior or indication of a (small or _not_so_small_?) problem.\r\n\r\nOne thing that may affect this is bazel installation. I followed [Bazel Installation Instructions](https://bazel.build/versions/master/docs/install.html) and used the recommended apt method. This led to me running into [this](https://github.com/tensorflow/tensorflow/issues/8092) issue. Installing openjdk-8-jdk on top of the ibm-java80-jdk as suggested in a [comment](https://github.com/tensorflow/tensorflow/issues/8092#issuecomment-304957009) solves the problem (although I am not sure how much technical debt this solution caries, which may have manifested in some of the warnings produced during compilation).\r\n\r\n### Source code / logs\r\n**Configuration Script options:**\r\n```{shell}\r\n$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n/usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with MKL support? [y/N] y\r\nMKL support will be enabled for TensorFlow\r\nDo you wish to download MKL LIB from the web? [Y/n] y\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] n\r\nNo VERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] n\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n........\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\n```\r\n[Console output of compilation command](https://github.com/Iolaum/CompileTF/blob/master/CompilationOutput.txt)\r\nBecause the output is too big to be placed within the issue I 've put it in it's own repository.", "comments": ["I don't think this is anything entirely unexpected and is pretty typical for most open source package installs. It is something worth looking into and seeing if we could do some minor things to cut down a bit on these errors. Feel free to look into it and send us a PR!", "Thanks a lot for the reply. I will try to look into it.", "A big chunk of the warning is the protobuf package (external). Not sure if there is a way to inject `-Wwrite-strings` to protobuf's gcc command in tensorflow's bazel specs."]}, {"number": 10837, "title": "Feature Request: Placeholder support for being set from an Op", "body": "Update: I thought on this some more, and perhaps if tf.Variable could directly support being set from an op with the transfer happening entirely on the C++ side that might cleanly and simply solve the problem.\r\n\r\nOften it is not known immediately what kind of Op will be needed for input, but it would be preferable to specify that detail later without the overhead associated with transporting data in python. \r\n\r\nFor example, it would be ideal to be able to define a full model for a task like ImageNet labeling with a LazyOp defined for the Input and Labels. Then later on the output of a `Dataset`, `RecordInput`, or `tf.placeholder` could be supplied to finalize the necessary connection.\r\n\r\nThis request is based on the comments of @fchollet at https://github.com/fchollet/keras/pull/6928#issuecomment-309552423.\r\n\r\n", "comments": ["@fchollet can you comment as to whether this is the TensorFlow feature you were asking for in https://github.com/fchollet/keras/pull/6928#issuecomment-309552423?", "In general, the feature in question would be graph rewiring: being able to replace a node with another node in a computation graph, after the fact. The most common use case would be to build a graph on top of placeholders, then replace the placeholders with ops that yield data (e.g. TF pipelines, TFRecords, etc). Graph editing.\r\n\r\nI don't believe this is currently possible. It would certainly be useful.", "Based on the discussion in [Keras Input Tensor API Design Proposal](https://docs.google.com/document/d/1tf2Nl7wor8rmWPUoxfClLuPLQGqvZryegD7K7-1tTe8/edit?usp=sharing) and https://github.com/fchollet/keras/pull/6928#issuecomment-312088200 implementation of this will be required for the ideal API design that supports TFRecords https://github.com/tensorflow/tensorflow/issues/10837 (and any data input op) to work with Keras.", "The graph is an append-only data structure. You could hack it up with tf.contrib.graph_editor.", "As @TimZaman mentioned, it does look like this might be possible to do manually in a pinch with:\r\n\r\n- [subgraph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/graph_editor/subgraph.py)\r\n- [copy_with_input_replacements](https://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor/copy_with_input_replacements)\r\n\r\nbut a real implementation of this feature request is still needed.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I think this is still valid and will become especially useful considering the current work on eager execution.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@mrry might have some thoughts, as this is related to input data pipelines.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "We're currently mulling over a design for this feature. It seems feasible to extend the `feed_dict` mechanism so that it would be possible to feed a placeholder based on the output of another operation, but it will require some care to ensure that the resulting graph is valid.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I have a prototype implementation that's currently under review. ", "Awesome! Is there a PR # or is it currently internal?", "It looks like the initial Python changes landed as part of 9db93a8db8fafee8092b30ae33a26248867e9b26, and the final C++ change landed in 49c6489368ea98feb3259d54a10c6fdfd01caf44. You can play with the private `Session._make_callable_from_options()` API, but be warned that it's still private so it might change. Example code here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f9c5e71104cb30583127fdc918591cc7604f17ca/tensorflow/python/client/session_test.py#L1374-L1384\r\n\r\nThe eventual plan is to replace the implementation of `Session.make_callable()` with this new mechanism and expose tensor connections as an option in that API.", "Nagging Assignee @mrry: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this is done & has been integrated into keras even. Thanks!", "Wait, sorry to be that person, but I'm confused abt how to do this (with keras specifically, meaning that I'm not trying to touch `sess/feed_dict`). \r\n\r\nIs this just what's happening with `tf.keras.backend.function`? Or is there some way to do `a_placeholder.set_value(tensor)` or `a_placeholder.set_default(tensor)` or something similar"]}, {"number": 10836, "title": "Installation Fails", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: win10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: latest from pip\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  8/5.1\r\n- **GPU model and memory**: Quadro K620\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n\r\n### Describe the problem\r\nHave installed CUDA 8 / cuDNN 5.1, get the below error when attempting to import tensorflow.  Have seen other threads on here that all describe this as a $PATH$ issue.  I've ensured the cuDNN/CUDA necessities are in the path and have used @mrry installation check script https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c\r\n\r\nit returned \r\n\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.5.\r\n\r\n- TensorFlow is installed at: c:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\r\n\r\n- All required DLLs are present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: -1\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\nIn [4]: import tensorflow\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     17         try:\r\n---> 18             return importlib.import_module(mname)\r\n     19         except ImportError:\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 41   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     42   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 21     _pywrap_tensorflow_internal = swig_import_helper()\r\n     22     del swig_import_helper\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     21     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127\r\n\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-a649b509054f> in <module>()\r\n----> 1 import tensorflow\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22\r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48\r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50\r\n     51 # Protocol buffers\r\n\r\nc:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     50 for some common reasons and solutions.  Include the entire stack trace\r\n     51 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 52   raise ImportError(msg)\r\n     53\r\n     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"c:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"c:\\users\\rhalabi\\appdata\\local\\programs\\python\\python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nIn [5]:", "comments": ["The @mrry script which you are using to confirm tensor flow installation, do check for both GPU/CPU \r\noptions. Somehow, your Cuda isn't configured correctly. \r\n\r\n- I think I would suggest looking over [https://www.stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso](https://www.stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso)\r\n\r\n- Also, you may try first with the CPU only option on Windows from scratch than trying to jump on GPU if you like?\r\n", "it's working!  The steps I took were to\r\n-re-download and reinstall CUDA 8.0 x86_64,  cuDNN v5.1 for CUDA 8.0.  \r\n-create a separate virtual env to install tensorflow gpu in", "bad news, reinstalled my computer and now the same exact issue is back...", "Hi,\r\n     Can you elobrate more what steps you took? Before it was working and now it stopped ? ", "I've solved the problem.  Every time I installed CUDA, my 2nd monitor (plugged into a GTX 330) would stop working, but tensorflow-gpu would work.\r\n\r\nAfter this I would try to get the 2nd monitor to work by updating its driver which would then cause importing tensorflow to fail.  Looks like tensorflow doesn't play well with this GPU, probably cause its so old.", "- As a suggestion I would say that clearly, windows approach is highly experimental hence if you can dual boot Ubuntu than would be so easy to get the best out of Tensor flow even with Two GPU.\r\n\r\n- Alienware laptops come up with an external graphic amplifier port where you can put 1,2,4.. external graphic cards which currently I am using."]}]