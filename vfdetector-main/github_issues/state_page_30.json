[{"number": 48965, "title": "Keras model.fit takes long until training begins when sample_weight is provided", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: 1080 Ti 12 GB\r\n\r\n(+ also on colab)\r\n\r\n**Describe the current behavior**\r\nFit function of keras Model class takes long to begin with the training when sample_weight is provided.\r\nI already recognized the following:\r\n- In the backend, the method `training_utils.handle_partial_sample_weights` is used which should not be necessary because the length of my samples corresponds to the length of my labels.\r\n- It looks like that the statement `any(w is None for w in sample_weights)` takes very long. If I execute this with on my sample_weight numpy array, this only takes 154 ms. But could it be that sample_weight is transformed into a Tensor before that operation and it therefore takes that long?\r\n\r\n**Describe the expected behavior**\r\nIt should not have a huge impact on the runtime if sample_weight are provided.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1Q5pTFK56AlhYAXNzlRLX--oOceQJc58_?usp=sharing\r\n", "comments": ["@rmothukuru \r\n\r\nI was able to reproduce the issue error.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/0bd0c009d5cc5ad8f1f3d945dc6817ac/-48965.ipynb) here .Thanks", "I investigated this a bit because it has been annoying me for some time. In my understanding, sample_weights in training_utils.handle_partial_sample_weights is either 'None' or a tensor object. If it is 'None', the check is fast because it checks if it is None before doing the partial check. The second check using list comprehension is indeed very slow and can be optimized by using array operations using numpy array. See my pull request (and change the training_utils.py in your installation manually if you like)."]}, {"number": 48964, "title": "Grappler does not fuse kernels in simple GELU implementation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model and memory: V100 16GB\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nRunning the GELU example below in both graph and eager mode seems to result in the same kernel GPU kernel launches. Using nvprof and NVIDIA Visual Profiler reveals 5 kernel launches in each case:\r\n\r\n3x eigen::internal::scalar_product_op\r\n1x eigen::internal::1x scalar_pow_op\r\n1x eigen::internal::1x scalar_sum_op\r\n\r\n**Describe the expected behavior**\r\n\r\nFrom reading https://www.tensorflow.org/guide/graph_optimization and the Stanford slides here http://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf, I was expecting Grappler to fuse some of these operations in the graph mode case.\r\n\r\nIt would be great to understand either why this does not happen in this case and exactly what kinds of operations can be fused with Grappler, or alternatively what else needs to be done to achieve this behaviour with Grappler.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport timeit\r\nimport math\r\nimport contextlib\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\n\r\nimport tensorflow as tf\r\nimport cupy\r\n\r\n\r\n@contextlib.contextmanager\r\ndef options(options):\r\n    old_opts = tf.config.optimizer.get_experimental_options()\r\n    tf.config.optimizer.set_experimental_options(options)\r\n    try:\r\n        yield\r\n    finally:\r\n        tf.config.optimizer.set_experimental_options(old_opts)\r\n\r\n\r\ndef _gelu(x):\r\n    constant = tf.constant(0.5 * (1 + math.tanh(math.sqrt(2 / math.pi))))\r\n    x_cubed = tf.pow(x, 3, name=\"x_cubed\")\r\n    return tf.multiply(constant * x, tf.add(x, 0.044715 * x_cubed), name=\"gelu\")\r\n\r\n\r\ndef gelu_graph(x):\r\n    @tf.function\r\n    def gelu(x):\r\n        return _gelu(x)\r\n\r\n    return gelu(x)\r\n\r\n\r\ndef gelu_eager(x):\r\n    def gelu(x):\r\n        return _gelu(x)\r\n\r\n    return gelu(x)\r\n\r\n\r\n# with options({'min_graph_nodes': 1}):\r\nprint(tf.config.optimizer.get_experimental_options())\r\n\r\nx = tf.random.uniform((int(1e6), 128))\r\n\r\n# with cupy.cuda.profile(): \r\nprint(\"Graph execution:\", timeit.timeit(lambda: gelu_graph(x), number=100), \"s\")\r\n\r\nprint(\"Eager execution:\", timeit.timeit(lambda: gelu_eager(x), number=100), \"s\")\r\n```\r\n", "comments": ["Can you check what fusion we have with TF Gelu op https://github.com/tensorflow/tensorflow/blob/c097d1f82e237357025fee054adde6adc8e7a760/tensorflow/python/ops/nn_ops.py#L3626-L3667", "> Can you check what fusion we have with TF Gelu op\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/c097d1f82e237357025fee054adde6adc8e7a760/tensorflow/python/ops/nn_ops.py#L3626-L3667\r\n\r\nThe implementation is a bit different to what I have above but it looks like this still results in 5 kernel launches (2 products, 1 sum, 1 quotient and 1 corresponding to `tf.math.erf`).", "I also tried a simpler function `unfused_power_four` function to see if several multiplications would be fused to a pow op:\r\n\r\n```\r\n@tf.function\r\ndef unfused_power_four(x):\r\n    x_squared = tf.multiply(x, x, name=\"x_square\")\r\n    x_cubed = tf.multiply(x, x_squared, name=\"x_cubed\")\r\n    x_four = tf.multiply(x, x_cubed, name=\"x_four\")\r\n    return x_four\r\n```\r\n\r\nEager mode: 3 kernel launches (all Eigen::internal::scalar_product_op)\r\nGraph mode: 3 kernel launches (2x Eigen::internal::scalar_product_op and 1x Eigen::internal::scalar_square_op)\r\n", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/7c476afdf66d5fa0f8ecd639e58558c1/untitled234.ipynb) ..Thanks !"]}, {"number": 48959, "title": "[TFLite] Fully Connected layers are not outputting accurate values with INT16 inputs of rank > 2", "body": "**Describe the current behavior**\r\n\r\nThe Fully connected layers outputs in a TensorFlow Lite model when using INT16 acitvations are completely off when the input rank is greater than 2, but totally correct when the rank is 2. \r\n\r\nIt seems that the computation is somehow broken when the rank >= 3.\r\n\r\nI was able to reproduce this in TensorFlow 2.5.0-rc2 and 2.6.0-dev.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self, squeeze=False):\r\n        super().__init__()\r\n        self.squeeze = squeeze\r\n        self.dense = tf.keras.layers.Dense(5)\r\n    \r\n    def call(self, x):\r\n        if self.squeeze:\r\n            x = tf.squeeze(x)\r\n        return self.dense(x)\r\n    \r\ndef representative_dataset():\r\n    for _ in range(100):\r\n        yield [tf.random.uniform(shape=[1, 30, 5])]\r\n   \r\nmodel = MyModel(squeeze=False)\r\nmodel(tf.random.uniform(shape=[1, 30, 5]))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\ntflite_model = converter.convert()\r\n\r\ninput_ = tf.random.uniform(shape=[1, 30, 5])\r\noutput = model(input_)\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_)\r\ninterpreter.invoke()\r\ntflite_output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\r\n\r\nprint(tf.reduce_max(tf.abs(output - tflite_output)))\r\n```\r\nWhen running this code, the output was : `tf.Tensor(2.128053, shape=(), dtype=float32)`.\r\nI ran it multiple times and always got a value of that order.\r\n\r\nIn the next snippet, I am using the same model, and the same input shape, but I squeeze the input in the forward pass so that the Fully Connected layer effectively receives an input tensor of rank 2:\r\n\r\n```python\r\nmodel = MyModel(squeeze=True)\r\nmodel(tf.random.uniform(shape=[1, 30, 5]))\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\ntflite_model = converter.convert()\r\n\r\ninput_ = tf.random.uniform(shape=[1, 30, 5])\r\noutput = model(input_)\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_)\r\ninterpreter.invoke()\r\ntflite_output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\r\n\r\nprint(tf.reduce_max(tf.abs(output - tflite_output)))\r\n```\r\nThis time the output was: `tf.Tensor(0.0044746697, shape=(), dtype=float32)`", "comments": ["@liyunlu0618 could you triage this issue?", "@Tessil ,\r\n\r\nHi Tessil, is this a known limitation?", "@jianlijianli  No, it seems it's also a problem with the reference int8 kernel. Using `experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF` with int8 will produce the same kind of error.\r\n\r\nApplying the same `batches` and `output_depth` calculations as the [reference float kernel](https://github.com/tensorflow/tensorflow/blob/8e9ff73ded8f1f7a3de47378e9adba92307df5ab/tensorflow/lite/kernels/internal/reference/fully_connected.h#L34) to both the [int8 and int16 reference kernels](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h) seems to solve the problem.", "Thanks Tessil. Confirmed that this happens to int8 reference kernel as well but int8 optimized kernel is OK.\r\n\r\nJust to avoid duplicated work, do you plan to fix the issue (since it happens to 16x8 as well)? Otherwise, I can work on it.", "I can eventually work on a fix but as I will not be able to really work on it until the end of next week it'll be faster to leave it to you if possible.", "Thanks @Tessil,\r\n\r\nThere has been some changes in the tflite conversion workflow and squeeze logic is getting update, and I think https://github.com/tensorflow/tensorflow/issues/48959#issuecomment-883321475 is outdated. This is my current observation: for int8 quantization, int8 kernel (reference) works fine for both \"squeeze = True\" or \"squeeze = False\". For 16x8 quantization, \"squeeze = True\" works but \"squeeze = False\" fails during runtime for multiplier being too small.\r\n\r\nPlease take a look if you are interested. I am working on a fix.\r\n\r\nMichael, I suppose you are not actually blocked by this issue, right?\r\n", "No, I found a workaround for what I was doing, no worries!"]}, {"number": 48947, "title": "keras optimizer.iterations are not properly save & restored", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0rc2\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\noptimizer.iterations are not properly save & restored. After restoring iterations are reset as 0, which leads to wrong lr based on lr_scheduler\r\n\r\n**Describe the expected behavior**\r\n\r\nprovide an option to either reset it or not, for backward compatibility maybe default to reset\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.keras.utils.register_keras_serializable(package='Custom', name='MyScheduler')\r\nclass MyScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n    def __init__(self, **kwargs):\r\n        super(MyScheduler, self).__init__(**kwargs)\r\n\r\n    def __call__(self, step):\r\n        return step\r\n\r\n    def get_config(self):\r\n        return {}\r\n\r\n\r\ninputs = tf.keras.Input(10)\r\noutputs = tf.keras.layers.Dense(10)(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nlr_scheduler = MyScheduler()\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\r\nmodel.compile(optimizer=optimizer, loss=\"mse\")\r\n\r\n\r\ndef get_dataset(repeat):\r\n    inputs_data = tf.ones([16, 10])\r\n    labels_data = tf.ones([16, 10])\r\n    dataset = (\r\n        tf.data.Dataset.from_tensors(inputs_data)\r\n        .map(\r\n            lambda x: (\r\n                inputs_data,\r\n                labels_data,\r\n                None,\r\n            )\r\n        ).repeat(repeat)\r\n    )\r\n    return dataset\r\n\r\n\r\nmodel.fit(get_dataset(3), epochs=1)\r\nprint(model.optimizer.iterations, lr_scheduler(model.optimizer.iterations))\r\n\r\npath = \"./foo/\"\r\nmodel.save(path)\r\nloaded = tf.keras.models.load_model(path)\r\nloaded.fit(get_dataset(4), epochs=1)\r\nprint(loaded.optimizer.iterations, lr_scheduler(loaded.optimizer.iterations))\r\n\r\n```\r\nthe last print shows 4, but it should 3+4=7\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sachinprasadhs \r\n\r\nI was able to reproduce the error in tf 2.4, tf 2.5rc2 and tf-nightly.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/5c1e4f033714eee5e44386834f46eb23/-48947.ipynb) .Thanks", "@blackyang I think this prints the number of individual batches where the updates have been performed, in your case if you are doing n iterations on your data then `optimizer.iterations` prints n.", "> \r\n> \r\n> @blackyang I think this prints the number of individual batches where the updates have been performed, in your case if you are doing n iterations on your data then `optimizer.iterations` prints n.\r\n\r\nThe problem is that OptimizerV2.iterations is exactly what gets passed to a LearningRateSchedule to determine the learning rate. If it reports 4 when it should be 7 then training isn't going to work properly when it saves and resumes.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d05aa3b7d9ca3de1a44c6ca5354a3c859cda727b/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1015", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sachinprasadhs thank you for your reply! yes I completely understand why it prints 4 instead of 7, I was saying that it should print 7 (or have an option to specify whether to print 4 by resetting or still print 7), otherwise the learning rate scheduler is wrong\r\n\r\nanother way is to update learning rate scheduler to not use this iteration\r\n\r\nany thoughts? thank you!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "any updates? thx", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I was able to reproduce the error in tf 2.6 . Please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/7afe8910211d8583b2a6afe23699eb30/-48947.ipynb#scrollTo=InTen7Q8BYLY).Thanks", "https://github.com/keras-team/keras/blob/master/keras/saving/saved_model/load.py#L86-L91\r\n\r\n/cc @k-w-w "]}, {"number": 48943, "title": "[keras] [tensorboard] buggy behavior of keras x tensorboard", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):  v2.4.0-rc4-71-g582c8d236cb 2.4.0 and above (e.g. 2.5.0rc2)\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nin the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard?hl=en) there are examples of how to use `tf.summary.histogram(...)` without specifying `step` in either subclass model or functiona api model. However it is actually broken since the step will always be 0.\r\n\r\nThis is due to `# TODO(b/151339474): Fix deadlock when not using .value() here.`  (in both 2.4.0 and 2.5.0) in  `tensorflow/tensorflow/python/keras/callbacks.py`. If I use `step` instead of `step.value()`, then everything is as expected. This is also explained in here https://github.com/tensorflow/tensorflow/pull/36839/commits/fed7d1a1fd23ca166c256bec0f9085a218f943e3#diff-d2cf8455ca3dbffe505170fa5fe38ed5e0d5969bcacb3b92e0b638b6d3c20461R230-R233 \r\n\r\nMay I ask what's the deadlock issue is and when the fix can be expected? Thx.\r\n\r\nAlso, I think it's better to move [this function call](https://github.com/tensorflow/tensorflow/blob/053ad9ed9222c9f201028f7fc1bd66eb9bd6baf6/tensorflow/python/keras/engine/training.py#L853) from `Model` to `Tensorboard` callback, in case if someone wants to overwrite step in `Tensorboard` (e.g. using a global step that can be save & restore in a continuous daily training model)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): Yes\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\na code snippet\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef my_summary(x):\r\n  tf.summary.scalar(\"mean_functional\", tf.reduce_mean(x))\r\n  return x\r\n\r\nclass MyLayer(tf.keras.layers.Layer):\r\n    def __init(self):\r\n        super(MyLayer, self).__init__()\r\n\r\n    def call(self, inputs):\r\n        tf.summary.scalar(\"mean_subclass\", tf.reduce_mean(inputs))\r\n        return inputs\r\n\r\ninputs = tf.keras.Input(10)\r\nx = tf.keras.layers.Dense(10)(inputs)\r\noutputs = tf.keras.layers.Lambda(my_summary)(x)\r\noutputs = MyLayer()(outputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nmodel.compile('sgd', 'mse')\r\n\r\n# Make sure to set `update_freq=N` to log a batch-level summary every N batches.\r\n# In addition to any `tf.summary` contained in `Model.call`, metrics added in\r\n# `Model.compile` will be logged every N batches.\r\ntb_callback = tf.keras.callbacks.TensorBoard('./logs240', update_freq=1)\r\n\r\ninputs_data = tf.ones([16, 10])\r\nlabels_data = tf.ones([16, 10])\r\n\r\ndataset = (\r\n    tf.data.Dataset.from_tensors(inputs_data)\r\n    .map(\r\n        lambda x: (\r\n            inputs_data,\r\n            labels_data,\r\n            None,\r\n        )\r\n    ).repeat(100)\r\n)\r\nmodel.fit(dataset, callbacks=[tb_callback], epochs=5)\r\n\r\n```\r\n\r\nwith and without `.value()` : \r\n![Screen Shot 2021-05-06 at 12 40 42 PM](https://user-images.githubusercontent.com/3350930/117361380-d0581080-ae6e-11eb-92cc-e9743b2d29b9.png)\r\n![Screen Shot 2021-05-06 at 11 30 19 AM](https://user-images.githubusercontent.com/3350930/117361382-d1893d80-ae6e-11eb-86db-1b4ee51cfab2.png)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["cc @omalleyt12 @foxik ", "I can able to replicate the issue. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/257f7f54b590baf7bb6748aa4781cb6a/untitled.ipynb). Thanks!", "I could reproduce the issue with TF 2.6 .Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/588eb5083e8d5a6f517eed72392f95c3/untitled.ipynb#scrollTo=q5pYuw07JvVj).Thanks!"]}, {"number": 48926, "title": "Installing tensorflow headers and libs for c++", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) \r\n- CUDA/cuDNN version: 10.2/8.0.0\r\n- GPU model and memory: 256-core NVIDIA Pascal\u2122 GPU architecture with 256 NVIDIA CUDA cores\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have successfully compiled the code on Tx2. how should I install it. I see  bazel-bin,  bazel-out,  bazel-tensorflow directory got created. I want to to install .so in /usr/local/lib and headers in /usr/local/include/tensorflow paths. Tried searchign goole but not enough info available.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbelow is the command I tried running for compilation\r\nsudo bazel --host_jvm_args=-Xmx1624m build --config=opt --config=monolithic --config=v2 --local_cpu_resources=2 //tensorflow:libtensorflow_cc.so\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`Target //tensorflow:libtensorflow_cc.so up-to-date:                                                                         \r\n  bazel-bin/tensorflow/libtensorflow_cc.so                                                                                           \r\nINFO: Elapsed time: 34104.697s, Critical Path: 909.56s                                                         \r\nINFO: 3868 processes: 3868 local.                                                                                                        INFO: Build completed successfully, 4367 total actions `", "comments": ["I have the same issue.  I want to build Tesseract with Tensorflow and it needs graph.pb.h and others to function.  I've looked this issue up and tried various bazel calls:\r\n\r\nbazel build --config=opt //tensorflow:libtensorflow_cc.so\r\nbazel build --config=opt //tensorflow/tools/lib_package:libtensorflow\r\nbazel build --config=opt //tensorflow:install_headers\r\n\r\nBut none of them give me what I need and no bazel-genfiles is created during any of them.  Am I doing something wrong?\r\n"]}, {"number": 48924, "title": "Failed init TFL interpreter when enabled XNNPACK", "body": "**System information**\r\n- IDE: XCode 12.5\r\n- Mobile device: iPhone 12 Pro Max\r\n- TensorFlow installed: 'TensorFlowLiteObjC' from CocoaPods\r\n- TensorFlow version: tried '2.3.0', '2.4.0', and '0.0.1-nightly.20210503'\r\n\r\n**Describe the current behavior**\r\nWhenever enabled XNNPACK, model initiation will fail. (see code snippets below)\r\n\r\n**Describe the expected behavior**\r\nExpected to initiate mode successfully\r\n\r\n**Standalone code to reproduce the issue**\r\nNSError *err = nil;\r\nTFLInterpreterOptions *options = [[TFLInterpreterOptions alloc] init];\r\noptions.useXNNPACK = YES;\r\nNSString *modelPath = [[NSBundle mainBundle] pathForResource:@\"mymodel\" ofType:@\"tflite\"];\r\n_modelInterpreter = [[TFLInterpreter alloc] initWithModelPath:modelPath options:options error:&err];\r\n\r\nwhere mymodel.tflite is a custom trained model. If set \r\n         options.useXNNPACK = NO;\r\neverything will work normally. When XNNPACK is enabled, the init will fail for interpreter. Specifically, it fails at \r\n         _interpreter = TfLiteInterpreterCreate(model, cOptions);\r\ninside TFLInterpreter.mm in the framework, and there's no source code to trace further.\r\n", "comments": ["@yyoon @multiverse-tf could you take a look?"]}, {"number": 48913, "title": "Custom Tensorflow optimizer cSGLD (Stochastic Langevin Dynamics) in TF2: correct update ops? ", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): v2.2\r\nPython version: 3.6.9\r\nCUDA/cuDNN version: v10.2\r\nGPU model and memory: GeForce GTX 1070 - 8117MiB\r\n\r\nDescribe the current behavior:\r\n\r\nI implemented Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning as a custom optimizer in TF2. While it does run, I am not exactly sure about my implementation since the documentation for Tensorflow optimizers is almost non-existent... I would appreciate any help!\r\nFor your reference the pytorch implementation (WAY simpler) is here: https://github.com/ruqizhang/csgmcmc/blob/master/experiments/cifar_csgmcmc.py\r\n\r\nDescribe the bug\r\n\r\nIt's not so much a bug, rather asking advice on how to implement the noisy update in TF2 for stochastic gradient langevin dynamics without preconditioning. In Addons there is the TF1 only example of preconditioned SGLD, whereas I want to make it work for TF2.\r\n\r\nCode to reproduce the issue\r\n\r\nSee the implementation with a running example here:\r\nhttps://colab.research.google.com/drive/16kwhGfiat-SkK0RWvf4EADZC_FDVudL8?usp=sharing\r\n\r\nIf you use momentum (i.e. alpha < 1), then the implementation does not work.\r\nI think the community would benefit from having access to this example (once working). Since there is no Tensorflow implementation yet of Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning.", "comments": ["I was able to reproduce the code in TF v2.4,2.5.0rc1,nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/fb326bee06975a6656d005c56d2eea2a/untitled48913.ipynb) here", "@Jordy-VL does [this](https://www.tensorflow.org/probability/examples/Fitting_DPMM_Using_pSGLD) document help you?", "> @Jordy-VL does [this](https://www.tensorflow.org/probability/examples/Fitting_DPMM_Using_pSGLD) document help you?\r\n\r\nHi, \r\n\r\nThanks for the suggestion. It clears up some points, yet it still only treats preconditioned (c)SGLD, which only works in TF1. \r\nI think my current implementation works fine :)\r\n\r\nThere is still an issue with SGHMC, where I cannot do the momentum sparse update due to an embedding lookup operation, any guidance there would be appreciated :) \r\n\r\nKind regards, \r\n\r\nJordy \r\n ", "Have you tried using https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/StochasticGradientLangevinDynamics\r\nfrom `tensorflow-probabilty` module for your case?", "@ymodak That's the one I'm referring to in the intro. \r\nIt does not enable TF2 for the lines mentioned here: https://github.com/tensorflow/probability/blob/4683836be3701e75f459494bd3de12edb7560a2d/tensorflow_probability/python/optimizer/sgld.py#L277 \r\n\r\nI did find https://github.com/google-research/google-research/blob/master/cold_posterior_bnn/core/sgmcmc.py, but I still need to figure out if this implementation supports TF2 out-of-the-box.  "]}, {"number": 48902, "title": "_pywrap_tensorflow_internal.so is enormous", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow version: 2.3\r\n- Python version:  3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv and pip3\r\n\r\n**Describe the problem**\r\n\r\n`_pywrap_tensorflow_internal.so` is 591MB. Based on #40492 it sounds like there was some expectation that it would get smaller, but instead it seems to be getting larger. That issue was unfortunately closed with no action taken.\r\n\r\n", "comments": []}, {"number": 48901, "title": "There's no way to pass class labels from list in tf.keras.preprocessing.image_dataset_from_directory using this", "body": "If there're no subfolders and labels to preprocessing.image_dataset_from_directory function are passed from external list this part of code will generate empty 'results' variable and further in line 88-98 this will lead to an error of not finding any files.\r\nBut if there's at least one subfolder the same code will lead to labels being inherited from subfolder name.\r\nThis means labels at all times behave as \"inferred\" and there's no option to pass labels from list in preprocessing.image_dataset_from_directory that uses this function. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/preprocessing/dataset_utils.py#L84", "comments": []}, {"number": 48888, "title": "Possible memory leak in eager context's thread_local_data", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n5.10.19-1rodete1-amd64 #1 SMP Debian 5.10.19-1rodete1 (2021-03-11) x86_64 GNU/Linux\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip3 binary\r\n- TensorFlow version (use command below): nightly (5/2)\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv1.12.1-52250-g36e43c933a8 2.5.0-dev20210305\r\n\r\n**Describe the current behavior**\r\n\r\nDetails in https://github.com/tensorflow/agents/issues/569.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\npip3 install --upgrade tf-nightly\r\npip3 install --upgrade tf-agents-nightly[reverb]\r\n```\r\n(go to site-local `tf_agents/examples/dqn` directory)\r\n```\r\npython3 dqn_train_eval.py\r\n```\r\n\r\nif you edit that python file to `tracemalloc` snapshot on iterations of `learner.run()`, e.g., between iterations 1000 and 5000, you'll see the (possible) memory leak.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["It's possible the leak was actually found and ignored in https://github.com/tensorflow/tensorflow/commit/11e09d9352066e564db9f0e16cc5f06c6f5f8b63?", "Adding Ed & Kibeom who approved that code.", "It's possible the error stems back to the fact that `eager_context_thread_local_data_map` and `eager_context_thread_local_data_defaults` are not associated with a particular context object (they're defined as `thread_local` in an anonymous namespace in a `.cc` file).  This was done [here](https://github.com/tensorflow/tensorflow/commit/8e1ef83d9ea91f1de915841690264b98b9a75c21).", "From the original bug report, \"One of the possible solutions is to call gc.collect() but it significantly slows down training\". That suggests to me that this is not a memory leak at all, just a reference cycle. I don't see much mention of that later on in the thread; seems like the thread_local bits are definitely a red herring if so.\r\n\r\nThere's a separate question of which objects are participating in the reference cycle. TF has some tests that basic stuff doesn't create reference cycles; I wouldn't rule core TF out entirely, but please check the agents code first. You can debug with the `gc` module, setting the DEBUG_SAVEALL flag.", "Yeah we know certain parts of TF (and e.g. Keras functional models) are susceptible to reference cycles that sometimes require a gc.collect to clear. We have some internal bugs open for Keras to fix these reference cycles, though it'll require a fairly involved rework of the internals of Keras History tracking in functional models, so we haven't been able to put it on the roadmap yet.\r\n\r\nThere's also been discussions of whether a device-memory OOM should trigger a gc.collect (as the reason these cycles don't get auto-cleaned up is standard python garbage collection doesn't take into account TF device memory usage when trying to decide whether to trigger a garbage collection).\r\n\r\n@rxsang who has been involved in those discussions as well (especially around TPUs)", "I'm not sure this is a reference cycle so much as it's some kind of issue with eager context `thread_local` [objects](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/python/eager/pywrap_tfe_src.cc#L4289) \"disappearing\" between calls.  Why aren't these objects defined inside the appropriate eager context?", "Yes, the issue is that the thread_local pointer eager_context_thread_local_data_map is (lazily) initialized to point to an object, but that never gets freed when the thread is destroyed.  (The pointer is destroyed, but not the object it points to.)  So a small amount of memory is leaked per thread created & destroyed.  I'm working on a fix for this.", "Edward, I sent you a change; it looks like this triggers even without any thread creation involved. Just repeatedly clearing and setting the function_call_options field triggers it, and that happens when you change some settings (soft device placement, optimizer options, etc.). ", "@allenlavoie thank you for confirming!  we're seeing something like this even when we don't change config settings.  i expect that if you fix the problem under your test case, it'll resolve the problem in general.", "Thanks, @allenlavoie!", "(I think there is *also* a memory leak when lots of threads are created and destroyed; so I'll go ahead and fix that one as well.  But I think the one @allenlavoie found is the more significant/relevant one.)", "Interesting. I don't know what would have been clearing the function call options if you're not changing config/optimizer/etc. settings. If that change does help with the leak we may also want to figure out why the options are repeatedly recreated, since it sounds like a performance issue."]}, {"number": 48884, "title": "TFLite NNApi / GPU causes results to be outputted as 0.0, while on CPU results are correct", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubunto 20.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 4 / 5\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.4\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using the following code:\r\n\r\nhttps://colab.research.google.com/gist/AiaHaruv/906b7d73395ef437f135c81f2c7754e9/filter_issue.ipynb\r\n\r\nOn the edge device (Pixel 4 or 5), some of the results are reported as 0.0, when GPU or NNAPI delegates are used. \r\nThis is in the context of using TFLite on android, pulling the latest nightly build.\r\nI cannot find any indication as to why this happens. \r\n\r\nUsing CPU, or when ran via the normal TF (not lite), this produces correct results\r\n\r\n**Describe the expected behavior**\r\n\r\nWould expect to receive correct (non 0.0) results when using acceleration \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/gist/AiaHaruv/906b7d73395ef437f135c81f2c7754e9/filter_issue.ipynb\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Some additional info. \r\nWe have tried every combination of flags on both NNApi and GPU delegates, the problem persists.", "@srjoglekar246 could you take a look?", "@arctop-sk What code are you using to run the models with GPU/NNAPI on device?", "@srjoglekar246 - I've tried with all three functions. I've also tried all combinations of flags inside the options.\r\n\r\n     `public static Interpreter.Options getOptionsForGPU() {\r\n        Interpreter.Options options = new Interpreter.Options();\r\n        CompatibilityList compatList = new CompatibilityList();\r\n        if (compatList.isDelegateSupportedOnThisDevice()) {\r\n            // if the device has a supported GPU, add the GPU delegate\r\n            GpuDelegate.Options delegateOptions = compatList.getBestOptionsForThisDevice();\r\n            // Tried with and without these 2 lines.\r\n            delegateOptions.setQuantizedModelsAllowed(false);\r\n            delegateOptions.setPrecisionLossAllowed(false);\r\n            GpuDelegate gpuDelegate = new GpuDelegate(delegateOptions);\r\n\r\n            options.addDelegate(gpuDelegate);\r\n        }\r\n        return options;\r\n    }\r\n\r\n    public static Interpreter.Options getOptionsForNNAPI(){\r\n        Interpreter.Options options = new Interpreter.Options();\r\n        options.setUseNNAPI(true);\r\n        return options;\r\n    }\r\n\r\n\r\n    public static Interpreter.Options getOptionsForNNAPIDelegate(){\r\n        Interpreter.Options options = new Interpreter.Options();\r\n        NnApiDelegate.Options delOptions = new NnApiDelegate.Options();\r\n        // Tried with all variations of ExecutionPreference as well\r\n        // delOptions.setExecutionPreference(NnApiDelegate.Options.);\r\n       // Tried with or without these 2\r\n        delOptions.setUseNnapiCpu(false);\r\n        delOptions.setAllowFp16(false);\r\n        options.addDelegate(new NnApiDelegate(delOptions));\r\n        return options;\r\n    }\r\n\r\nCreating the interpreter, with theses options\r\n\r\n          `tfLite = new Interpreter(tfLiteModel, tfLiteOptions);`\r\n\r\nCreating the buffers is done in a loop with this code:\r\n\r\n            `Tensor ten = tfLite.getInputTensor(i); // or getOutputTensor(i)\r\n            ByteBuffer inputBuffer = ByteBuffer.allocateDirect(ten.numBytes());\r\n            inputBuffer.order(ByteOrder.nativeOrder());`\r\n\r\nRunning inference is done via: (I keep a list of the buffers for quick iteration, while the model gets an array of the inputs and a a map of the outputs, as per examples, and function signatures.\r\n\r\n       `for (InterpreterInputOutput input: mInputsList) {\r\n            input.buffer.rewind();\r\n        }\r\n        for (InterpreterInputOutput output: mOutputsList) {\r\n            output.buffer.clear();\r\n        }\r\n        tfLite.runForMultipleInputsOutputs(mInputs , mOutputs);`", "The code seems fine to me. Have you added the proper dependencies, etc?\r\n@lu-wang-g Can you take a look?", "@srjoglekar246 - yes all dependencies are there. ", "Do you see any errors in adb logcat? Its difficult to know what exactly might be going wrong in your case without looking at some debug logs or the overall code/app.", "+1 on screening the logs. The delegates may not be set up successfully. ", "I haven't seen anything. \r\nThe model runs, and produces results. However, we get something like [ 4 results, 12 0.0 , 6 result ...]\r\nand so on with NNApi / GPU. Works as expected on CPU.\r\nIs there a flag I can send the runtime like adb shell setprop debug.tflite.trace 1 that will give more verbose data in the logcat?", "when I create this model, all i'm getting are these two:\r\n I/tflite: Created TensorFlow Lite delegate for NNAPI.\r\n I/tflite: Initialized TensorFlow Lite runtime.\r\n\r\nEdit: for GPU i'm getting:\r\n I/tflite: Created TensorFlow Lite delegate for GPU.\r\n I/tflite: Initialized TensorFlow Lite runtime.\r\n I/tflite: Initialized OpenCL-based API.\r\n/tflite: Created 1 GPU delegate kernels.\r\n\r\nbut the results are identical", "@srjoglekar246  @lu-wang-g \r\nIt doesn't seem to me that we are using any operations that are not supported by the GPU/NNAPI, but that would have been my first guess. Since i'm getting no errors, I don't really have a lead as to where to start digging into this. ", "@arctop-sk Can you try using our [benchmark tool](https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary) on your model with GPU & NNAPI delegates? It has more verbose logging, and will tell us if the delegates are getting used at all.", "Alright, that actually gave us something:\r\n\r\n2021-05-06 20:57:52.456 27770-27770/org.tensorflow.lite.benchmark E/libc: Access denied finding property \"ro.hardware.chipname\"", "@srjoglekar246 same case with the GPU. not sure if this is the cause, but that's the only issue i'm seeing", "@srjoglekar246 any updates on this?", "TBH, your code for inference seems fine.\r\nAs a sanity check, can you try building & running our image classification [sample](https://www.tensorflow.org/lite/examples/image_classification/overview) to see if it works fine w/GPU on your device? I just want to ensure that the basic runtime works as intended on your device.", "@srjoglekar246 \r\n\r\nLoading that up on my device, first thing I notice while loading is this:\r\n\r\n2021-05-18 19:58:44.058 32248-32273/org.tensorflow.lite.examples.classification I/AdrenoGLES-0: QUALCOMM build                   : 85da404, I46ff5fc46f\r\n    Build Date                       : 11/30/20\r\n    OpenGL ES Shader Compiler Version: EV031.31.04.01\r\n    Local Branch                     : promo490_3_Google\r\n    Remote Branch                    : \r\n    Remote Branch                    : \r\n    Reconstruct Branch               : \r\n2021-05-18 19:58:44.058 32248-32273/org.tensorflow.lite.examples.classification I/AdrenoGLES-0: Build Config                     : S P 10.0.4 AArch64\r\n2021-05-18 19:58:44.058 32248-32273/org.tensorflow.lite.examples.classification I/AdrenoGLES-0: Driver Path                      : /vendor/lib64/egl/libGLESv2_adreno.so\r\n2021-05-18 19:58:44.066 32248-32273/org.tensorflow.lite.examples.classification I/AdrenoGLES-0: PFP: 0x016ee189, ME: 0x00000000\r\n2021-05-18 19:58:44.072 32248-32273/org.tensorflow.lite.examples.classification W/AdrenoUtils: <ReadGpuID_from_sysfs:197>: Failed to open /sys/class/kgsl/kgsl-3d0/gpu_model\r\n2021-05-18 19:58:44.072 32248-32273/org.tensorflow.lite.examples.classification W/AdrenoUtils: <ReadGpuID:221>: Failed to read chip ID from gpu_model. Fallback to use the GSL path\r\n2021-05-18 19:58:44.098 32248-32273/org.tensorflow.lite.examples.classification I/Gralloc4: mapper 4.x is not supported\r\n\r\nThe app crashes right after that due to our dev policy (i believe) that disallows usage of the camera. I'm getting this resolved with IT and will report other findings, but wanted to give first info that I have"]}, {"number": 48881, "title": "Tensorflow cpu memory profiling", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (**Yes**/No):\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTensorflow cpu memory profiling: we want to get detailed profiling as in GPU memory profiling.\r\n(Beginning with a single thread.)\r\nCurrently: when running the memory profiler in CPU mode we do not see any memory profile data captured.\r\n\r\n**Will this change the current api? How?**\r\nno\r\n\r\n**Who will benefit with this feature?**\r\nAnyone profiling memory on CPU and trying to understand the memory behavior of operators.\r\n\r\n\r\n**Any Other info.**\r\n", "comments": ["Are you following this guide https://www.tensorflow.org/guide/profiler?", "@bhack  yes", "@saar-eliad,\r\n**`Tensorboard Profiler`** works with **`CPU`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/db189c30d7acd81df1b2b75423315238/tensorboard_profiling_keras.ipynb) of working code. \r\n\r\nThe code used in [the Colab](https://colab.research.google.com/gist/rmothukuru/db189c30d7acd81df1b2b75423315238/tensorboard_profiling_keras.ipynb) is taken from the [documentation of the Profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) and did the changes mentioned below: \r\n1. Removed the code related to **`GPU`** \r\n2. Executed the **`Colab`** with **`Runtime`** as **`None`** i.e.. **`CPU`**\r\n\r\nPlease find the screenshot below:\r\n\r\n![image](https://user-images.githubusercontent.com/48206667/116984615-11261d00-ace9-11eb-8772-a0564d1c10ca.png)\r\n", "@rmothukuru what you are showing from the gist/colab is CPU *execution*/*trace*, which works.\r\n\r\nHowever, the problem we are discussing is with the *memory profiler* tool.\r\n", "@saar-eliad, \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@saar-eliad Can you select CPU in the dropdown at https://www.tensorflow.org/guide/profiler#memory_profile_summary ?", "@bhack when running with GPU, I am able to select CPU in the dropdown menu (the cpu info is very limited compared to the gpu information).\r\n\r\nwhen running without GPU, i.e., CPU only, there is no menu at all (and even the gunziped json file in tensorboard dir is empty).", "Do you see the https://www.tensorflow.org/guide/profiler#memory_profile_summary on CPU?", "@bhack  I do not see the memory profile summary on CPU. (it says something like - no profile).", "It seems a duplicate of https://github.com/tensorflow/tensorflow/issues/42123. @MarkDaoust can we add a claim in the Docs?", "@rmothukuru since I can't share the whole code, here is a copy & paste of the relevant parts.\r\n(we got this from a 3rd party)\r\nDo you have an example of a working CPU memory profiler without GPU? I did not find any.\r\nIf so let's take the model from there.\r\n\r\nThe 3rd party we are working with claims Tensorflow can profile only GPU memory - and does not work in CPU mode. It did not work according to my check as well, hence I believe this feature does not exist.\r\n(from looking around the TensorFlow code: seems like the GPU memory profiling is based on CUPTI from NVidia for GPUs, so it is probable that in CPU mode it simply does not run)\r\n```\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n    \r\n    def tf_profile(self, iterations=10):\r\n        \"\"\" Runs Profiled and Traced steps of the pipeline \"\"\"\r\n        with tf.profiler.experimental.Profile(self.profiling['logdir'],\r\n                                              options=tf.profiler.experimental.ProfilerOptions(\r\n                                                  **self.profiling['options'])):\r\n            self.run_traced_steps(iterations=iterations)\r\n\r\n    def run_traced_steps(self, iterations=10):\r\n        \"\"\" Run pipeline steps Traced by the TensorFLow's Profiler \"\"\"\r\n        for step in tqdm(range(iterations)):\r\n            with tf.profiler.experimental.Trace('Step', step_num=step, _r=1):\r\n                outputs = self.run_step()\r\n```", "I think that situation is still the same as 2020 https://github.com/tensorflow/tensorflow/issues/42123#issuecomment-675047711. It is why probably a claim in Doc is the better solution to avoid new tickets on this topic.", "@bhack I saw this issue before opening this one, and I checked: even in tf 2.3.1 there is an output of gunzip-ed memory profile in tensorboard directory - it has a lot of information there regarding GPU profiles.\r\n(for GPU: we converted it to xlsx and using it instead of the tensorboard frontend)\r\n\r\nI believe that in tf 2.4 the fix was reading and displaying this file (frontend fix).\r\nIn CPU mode, however, this file is completely empty.\r\n\r\nWill CPU-only profiling in TensorFlow work in any version?\r\nCan you point to the code?\r\nI currently believe that there is no such ability without CUPTI.", "If you read that comment was:\n\n> This tool reports memory consumption on the device (i.e. GPU or TPU); not on the host (CPU).\n\nI think that also now it rely on CUPTI/GPU profiler and It needs a clarification in the docs.", "@bhack Absolutely. This clarifies things beyond all doubt.\r\n (Some of the first comments here gave me false hopes that I might be missing something.)\r\n So it's a valid feature request then."]}, {"number": 48880, "title": "Error occured Bidirectional layer with TPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): Current Colab default (2.4.1)\r\n- Python version: Colab default (3.7.10)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n('v2.4.1-0-g85c8b2a817f', '2.4.1')\r\n\r\n**Describe the current behavior**\r\nThere is code below. The problem is error under the specific conditions.\r\n\r\nHere are conditions under which an error occurs.\r\n1. On TPU\r\n2. Use Bidirectional layer with ``return_sequence==True``\r\n3. Use tf.keras.layers.Masking or ``mask_zero=True`` with tf.keras.layers.Embedding\r\n\r\nUnder the triple of upper conditions, training code puts error below.\r\nWithout even only one of the conditions, error not occured.\r\n```\r\nInternalError: 9 root error(s) found.\r\n  (0) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_1920970171686233974/_5/_79]]\r\n  (1) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_1920970171686233974/_5/_143]]\r\n  (2) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_1920970171686233974/_5/_159]]\r\n  (3) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_1920970171686233974/_5/_95]]\r\n  (4) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_1920970171686233974/_5/_127]]\r\n  (5) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n\t [[tpu_compile_succeeded_assert/_1920970171686233974/_5/_63]]\r\n  (6) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n\t [[cluster_train_function/control_after/_1/_195]]\r\n  (7) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_1920970171686233974/_5}}]]\r\n  (8) Internal: {{function_node __inference_train_function_212595}} Compilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_dimension_inference.cc:1343) operand != nullptr \r\n ... [truncated]\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect to works well under the conditions also.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=os.environ[\"TPU_NAME\"])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.TPUStrategy(resolver)\r\n\r\nclass TestModel(tf.keras.Model):\r\n  def __init__(self):\r\n    super().__init__()\r\n\r\n    self.embedding = tf.keras.layers.Embedding(input_dim=6, output_dim=64)\r\n    self.pad_masking = tf.keras.layers.Masking(0, name=\"masking\")\r\n    lstm = tf.keras.layers.LSTM(64, return_sequences=True)\r\n    self.lstm = tf.keras.layers.Bidirectional(lstm)\r\n    self.dense = tf.keras.layers.Dense(1)\r\n\r\n  def call(self, input, training=None):\r\n    output = self.embedding(input)\r\n    output = self.pad_masking(output)\r\n    output = self.lstm(output)\r\n    output = self.dense(output[:, -1, :])\r\n    return output\r\n\r\nwith strategy.scope():\r\n  x = tf.data.Dataset.from_tensor_slices([tf.constant([1,2,3,4,5]), tf.constant([1,2,3,4,5]), tf.constant([1,2,3,4,5])])\r\n  y = tf.data.Dataset.from_tensor_slices([[1],[2],[3]])\r\n  dataset = tf.data.Dataset.zip((x,y)).repeat().batch(2)\r\n\r\n  model = TestModel()\r\n  model(tf.keras.Input([None]))\r\n\r\n  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.Adam(1e-4),\r\n                metrics=['accuracy'])\r\n  model.fit(dataset, epochs=10, steps_per_epoch=10)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@cosmoquester \r\nIs this still an issue as the pr is merged.", "I bypassed the problem by using custom bidirectional layer in my repository. However, the issue is still alive. You can just reproduce this issue by running attached code on TPU environment. ", "The code snippet fails with latest tf-nightly version as well. See [gist](https://colab.research.google.com/gist/ymodak/f4b0daeca83f6144d019d7fbed2df330/untitled14.ipynb)", "Was able to reproduce the issue in TF v2.6 please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/bce2855ba3276a95dc8012c3b7631b2d/untitled14.ipynb#scrollTo=7cT46uJLEBrp).Thanks!", "Hi David, this is a dynamic padder error. Could you take a look? Thanks!"]}, {"number": 48855, "title": "TF_AddGradientsWithPrefix doesn't seem to lock the gradients properly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using TF-Java\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installed from (source or binary): Maven Central binary\r\n- TensorFlow version (use command below): v2.4.1\r\n\r\n**Describe the current behavior**\r\n\r\nWhen training a model I'm hitting a non-determinism issue, the models are initialized identically, and fed identical data in an identical order, with inter-op and intra-op parallelism both set to 1, but I can get complete training failure when building simple MLPs and CNNs on MNIST.\r\n\r\nTF-Java uses TF_AddGradientsWithPrefix to construct the gradients. We then wrap that in Optimizers to make a higher level front end to training, similar to the v1 optimizers package, and Keras in v2. As gradients are only available in Graph mode in the C API our code is most similar to the v1 optimizers package. I think we want to have the gating behaviour set to `GATE_OP` (https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer), but that's all implemented entirely in python, and the C API codepath doesn't seem to add the required control dependencies to ensure the gradients are all computed before being back-propped any further (in Python that's performed here - https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/gradients_util.py#L692, but there doesn't seem to be an equivalent in the C API here - https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/cc/framework/gradients.cc#L568).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should train identically with identical gradients.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee this test in TF-Java - https://github.com/Craigacp/tensorflow-java/blob/03402b30e271a46438c1feff12409095737f3d3e/tensorflow-framework/src/test/java/org/tensorflow/framework/optimizers/GradientDescentTest.java#L121. This test when run complains that out of the 20 runs some of them diverged even when trained using identical graph defs on identical data, for an identical number of steps, when using single threaded computation.\r\n", "comments": ["Has anyone had a chance to look at this?"]}, {"number": 48850, "title": "CenterNet TFLite crashes while running on Android", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android 9, not specific device(RK3399 cpu, and QC835 cpu)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14 and 2.4\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: not used\r\n- GPU model and memory: not used \r\n\r\n**Describe the current behavior**\r\nWe have built a shared object (.so) with Tensorflow cpp API, and then we are linking it with Android App. The shared object contain CenterNet Model in order to predict skeleton detections. When we create a binary for arm64-v8a or armeabi-v7a which is linked against that .so and run it as executable it works just fine. But when we try to predict within android app we are getting the following crash error:\r\n```\r\n/data/app/com.example.app-XsHutFbryKx6M754dZvzBg==/lib/arm64/lib_centernet.so (tflite::Subgraph::ReportErrorC(TfLiteContext*, char const*, ...)+112)\r\n/data/app/com.example.app-XsHutFbryKx6M754dZvzBg==/lib/arm64/lib_centernet.so (TfLiteStatus tflite::ops::builtin::reduce::EvalLogic<int>(TfLiteContext*, TfLiteNode*, tflite::ops::builtin::reduce::OpContext*, int, tflite::ops::builtin::reduce::OpContext* (*)(tflite::ops::builtin::reduce::OpContext*, tflite::ops::builtin::reduce::OpContext*))+1668)\r\n/data/app/com.example.app-XsHutFbryKx6M754dZvzBg==/lib/arm64/lib_centernet.so (_ZN6tflite3ops7builtin6reduce11EvalGenericILNS2_10KernelTypeE0ELNS2_10ReduceTypeE0EEE12TfLiteStatusP13TfLiteContextP10TfLiteNode+240)\r\n/data/app/com.example.app-XsHutFbryKx6M754dZvzBg==/lib/arm64/lib_centernet.so (tflite::ops::builtin::reduce::EvalSum(TfLiteContext*, TfLiteNode*)+240)\r\n/data/app/com.example.app-XsHutFbryKx6M754dZvzBg==/lib/arm64/lib_centernet.so (tflite::Subgraph::Invoke()+1008)\r\n/data/app/com.example.app-XsHutFbryKx6M754dZvzBg==/lib/arm64/lib_centernet.so (tflite::Interpreter::Invoke()+92)\r\n/data/app/com.example.app-XsHutFbryKx6M754dZvzBg==/lib/arm64/lib_centernet.so (lib::CenterNet::Detect(unsigned char*, int, int)+96)\r\n``` \r\n**Describe the expected behavior**\r\nAs a binary we are getting output from the interpreter and we except to work the same in app. \r\n\r\nIf you find that there is missing information just ask and I will try to add that to. \r\n\r\nThank you in advance!", "comments": ["Have you tested the TFLite model on the Python API? If not, could you verify it? If possible, could you share your TFLite model file for debugging purpose?", "Does your model run correctly with the python API?", "Hi @abattery @thaink , \r\nYes I have trained and tested the TFLite model on the Python API and it works like it should (the model returns the keypoints and the box). Currently I don't have the tflite model but I could provide one in couple of days.", "The kernels are shared for C++ and Python API. So it is weird to have the problem in C++ but not Python.\r\nSure we need the model or someway to reproduce the issue.", "Hi @thaink , here is the model: \r\n[CenterNetModel.zip](https://github.com/tensorflow/tensorflow/files/6432343/CenterNetModel.zip)\r\nJust to clarify this is happening when I try to run the model in Android App, because as executable file is working fine.", "Right. I mean out Java API is also based on C++ so I would expect the behavior to be the same as Python.", "I just run your model on both Linux and Android. It is working OK. ", "Now I have tried to run the model with Java API in Android and its not crashing but the output boxes are not correctly and all detections have accuracy 0.0%. \r\nAlso for do you have any idea for the above error in C++, can I try anything else ? ( so I know to close this issue or not because the error is still happening)\r\n\r\nThank you!", "Might be that version of TFLite is too old. It is very hard to know what went wrong if I cannot reproduce it.", "What is the TFLite version that you recommend to use ?", "In case if there are errors. I would absolutely recommend to check if it happens on the nightly version or not.", "@thaink I was able to run with Java API on Android, but its giving the wrong output, the accuracy of all objects is always 0.0% , and the first coordinate of the rectangle detected is always 0 , and the other 3 are wrong also. Did that model that you tried gave you good detections ?", "In most of the cases I saw, if the result when you run TFLite with Python is different than what you got on Android, the inputs to the model are not the same. So please print the array that you feed the model and check if they are the same first."]}, {"number": 48849, "title": "[For GShard] parameter instruction in HLO could not get sharding information from annotated VarHandleOp", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): (2.6.0)master branch on 30, April\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8.4.0\r\n- CUDA/cuDNN version: 11.2/8\r\n- GPU model and memory: V100 32GB\r\n\r\n\r\n**Describe the current behavior**\r\nI write some codes with GShard API provided in ``` experimental/xla_sharding/xla_sharding.py ```.  As proposed in GShard paper, the annotation can marked anywhere when needed. But As I annotated the trainable variable, it seems no effect on HLO for `parameter` instruction.\r\n\r\nFor example, a trainable variable could be created from the following API.  The ```self.in_weights``` is ```VarHandleOp``` Type\r\n```\r\n self.weight = tf.get_variable(name=\"weights\",\r\n                                      shape=shape,\r\n                                      dtype=tf.float32,\r\n                                      initializer=self.initializer)\r\n```\r\n\r\nAnd When I annotates like the following:\r\n\r\n```    \r\n self.weight = xla_sharding.split(self.weight, 0, 2)\r\n```\r\n\r\nI have checked the annotation on ```NodeDef``` and make sure that the ```_XLA_Sharding``` Attribute has beed added on. But this important information is missing in HLO when entering the ``` GpuCompiler::OptimizeHloModule```.\r\n\r\nTo debug the missing information, I noticed the ```XlaCompiler::CompileGraph()``` function intent to get Sharding Attribute from ```NodeDef``` by ```ComputeArgAndRetvalShardings```.  The  source code of this function is as follows: \r\n\r\n```\r\n// Uses the _Arg and _Retval nodes in the graph to determine an OpSharding for\r\n// each argument and return value.\r\nxla::StatusOr<\r\n    std::pair<std::map<int, xla::OpSharding>, std::map<int, xla::OpSharding>>>\r\nComputeArgAndRetvalShardings(const Graph& graph) {\r\n  auto get_sharding_for_node =\r\n      [](const Node* n) -> xla::StatusOr<absl::optional<xla::OpSharding>> {\r\n    TF_ASSIGN_OR_RETURN(\r\n        auto sharding,\r\n        ParseShardingFromDevice(*n, std::numeric_limits<int32>::max(),\r\n                                /*add_metadata=*/false));\r\n    return sharding;\r\n  };\r\n  std::map<int, xla::OpSharding> arg_shardings;\r\n  std::map<int, xla::OpSharding> retval_shardings;\r\n  for (const Node* n : graph.nodes()) {\r\n    if (n->IsArg()) {\r\n      TF_ASSIGN_OR_RETURN(auto sharding, get_sharding_for_node(n));\r\n      if (!sharding.has_value()) continue;\r\n      int index;\r\n      TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), \"index\", &index));\r\n      TF_RET_CHECK(index >= 0) << \"Negative _Arg index\";\r\n      arg_shardings[index] = std::move(*sharding);\r\n    } else if (n->IsRetval()) {\r\n      TF_ASSIGN_OR_RETURN(auto sharding, get_sharding_for_node(n));\r\n      if (!sharding.has_value()) continue;\r\n      int index;\r\n      TF_RETURN_IF_ERROR(GetNodeAttr(n->attrs(), \"index\", &index));\r\n      TF_RET_CHECK(index >= 0) << \"Negative _Retval index\";\r\n      retval_shardings[index] = std::move(*sharding);\r\n    }\r\n  }\r\n  return std::make_pair(std::move(arg_shardings), std::move(retval_shardings));\r\n}\r\n```\r\n\r\nBut here the ```Arg``` and ```RetVals``` belongs to ```Xla_Compile``` Op , not ```XlaRun``` Op. The indices acquired from ```arg_shardings``` are different from ```XlaRun```'s. And the ```BuildArguments``` function uses these indices to\r\ndecide sharding information for HLO, which leads to the hlo_sharding missing due to the mismatched indices for \r\n ```input_to_args```.\r\n```\r\n    for (std::vector<int>::size_type i = 0; i < input_to_args->size(); ++i) {\r\n      auto it = arg_shardings.find(i);\r\n      xla::XlaScopedShardingAssignment assign_sharding(\r\n          builder, it == arg_shardings.end() ? absl::optional<xla::OpSharding>()\r\n                                             : it->second);\r\n```\r\n\r\nIs that the expected  behavior for ```parameter_instruction``` or a bug for TensorFlow GraphDef?\r\n\r\n\r\n**Describe the expected behavior**\r\nI think the ```arg_shardings``` of ```XlaCompile``` Op is meaningless for deciding ``` parameter```'s sharding information. These arguments indices should be referred to the inputs of ENTRY computation for hlo module. \r\n\r\nCan anyone help for solving this problem? @ukoxyz \r\n\r\n", "comments": []}, {"number": 48845, "title": "BatchNorm generates NaN moving_variance on GPU with fused set to True", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab, Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary (Colab), source (Ubuntu)\r\n- TensorFlow version (use command below): 'v2.4.1-0-g85c8b2a817f', '2.4.1'\r\n- Python version: 3\r\n- Bazel version (if compiling from source): default for TF 2.4.1\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: default for colab, cuda 11.0.3, cudnn 8.1.0.77 (Ubuntu)\r\n- GPU model and memory: default for colab, 1080Ti (Ubuntu)\r\n\r\n**Describe the current behavior**\r\nThis is a continue of https://github.com/tensorflow/tensorflow/issues/34062\r\nOriginal issue was not resolved due to wrong test (GPU is not used with tf-nightly-gpu).\r\n\r\n**Describe the expected behavior**\r\nBatchnorm should correctly work on GPU with batch size == 1 and fused = True\r\n\r\n**Standalone code to reproduce the issue**\r\nMake sure to uncomment line with \"set_log_device_placement\" if you want to test with tf-nightly (you will see only CPU placement in colab logs)\r\nhttps://colab.research.google.com/drive/16LccjOuj9Aj8x0S9Q-hOTxGgqVWdVZ53?usp=sharing\r\n", "comments": ["@shkarupa-alex \r\n \r\n\r\nI ran the code shared on  tf nightly at runtime GPU and BatchNorm did not generated NaN moving_variance on GPU, please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/8dcf4524ea52fa64b23a0887eeff70bf/bn-issue.ipynb) here.Thanks\r\n\r\n\r\n", "> @shkarupa-alex\r\n> \r\n> I ran the code shared on tf nightly at runtime GPU and BatchNorm did not generated NaN moving_variance on GPU, please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/8dcf4524ea52fa64b23a0887eeff70bf/bn-issue.ipynb) here.Thanks\r\n\r\nYou did'n read comments in code and as result didn't uncomment line\r\n>tf.debugging.set_log_device_placement(True)\r\n\r\n\r\n\r\nIf you will, you see that all ops are placed on Cpu (tf-nightly does not work with colab gpu env), but issue is GPU-related.\r\n\r\n```Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\r\n<tf.Variable 'batch_normalization/moving_variance:0' shape=(1,) dtype=float32, numpy=array([0.99], dtype=float32)>\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\r\n<tf.Variable 'batch_normalization_1/moving_variance:0' shape=(1,) dtype=float32, numpy=array([0.99], dtype=float32)>\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Mean in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op StopGradient in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op SquaredDifference in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Mean in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Squeeze in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Squeeze in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignSubVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignSubVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Rsqrt in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\r\n<tf.Variable 'batch_normalization_2/moving_variance:0' shape=(1,) dtype=float32, numpy=array([0.99], dtype=float32)>\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op FusedBatchNormV3 in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:CPU:0\r\n<tf.Variable 'batch_normalization_3/moving_variance:0' shape=(1,) dtype=float32, numpy=array([0.99], dtype=float32)>\r\n```", "@shkarupa-alex you can check issue #[34062](https://github.com/tensorflow/tensorflow/issues/34062#issuecomment-606300106) and comment for the probable reason.", "This issue created because https://github.com/tensorflow/tensorflow/issues/34062 was closed (but not solved due to wrong testing) with comment \"Please create a new issue if you face any using a new template.\"", "TF 2.5.0 + Google Colab = issue is still here", "TF 2.6.0 + Google Colab = issue is still here", "same problem with TF 2.7.0", "> \r\n\r\nupdate: set batch_size > 1 solve this problem", "> update: set batch_size > 1 solve this problem\r\n\r\nIt is not always possible (e.g. if model is too large).\r\nSo, it is still an issue and i hope it will be solved."]}, {"number": 48802, "title": "C API TF_LoadSessionFromSavedModel leaks memory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using TF-Java bindings\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Oracle Linux 7\r\n- TensorFlow installed from (source or binary): binary from TF-Java\r\n- TensorFlow version (use command below): 2.4.1\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc-7\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the reproducer code from https://github.com/tensorflow/java/issues/304 under `valgrind --tool=memcheck --leak-check=full` after recompiling TF with debug symbols shows leaks in the `TF_LoadSessionFromSavedModel` and `TF_NewGraph`.\r\n\r\n```\r\n==64241== 1,452,789 (73,728 direct, 1,379,061 indirect) bytes in 9 blocks are definitely lost in loss record 144,850 of 144,860\r\n==64241==    at 0x4C2C375: memalign (vg_replace_malloc.c:908)\r\n==64241==    by 0x4C2C43F: posix_memalign (vg_replace_malloc.c:1072)\r\n==64241==    by 0x568E8E56: tensorflow::port::AlignedMalloc(unsigned long, int) (port.cc:241)\r\n==64241==    by 0x5688C970: tensorflow::core::Arena::AllocNewBlock(unsigned long, unsigned int) (arena.cc:174)\r\n==64241==    by 0x5688CC98: tensorflow::core::Arena::MakeNewBlock(unsigned int) (arena.cc:111)\r\n==64241==    by 0x5688CD60: tensorflow::core::Arena::GetMemoryFallback(unsigned long, int) (arena.cc:211)\r\n==64241==    by 0x565887E8: GetMemory (arena.h:75)\r\n==64241==    by 0x565887E8: Alloc (arena.h:42)\r\n==64241==    by 0x565887E8: tensorflow::Graph::AllocateNode(std::shared_ptr<tensorflow::NodeProperties>, tensorflow::Node const*, tensorflow::Node::NodeClass) (graph.cc:773)\r\n==64241==    by 0x5658ECAA: tensorflow::Graph::AddNode(tensorflow::NodeDef, tensorflow::Status*) (graph.cc:438)\r\n==64241==    by 0x567DF8A3: MakeNode (graph_constructor.cc:778)\r\n==64241==    by 0x567DF8A3: tensorflow::(anonymous namespace)::GraphConstructor::Convert() (graph_constructor.cc:1246)\r\n==64241==    by 0x567E54C0: tensorflow::(anonymous namespace)::GraphConstructor::TryImport() (graph_constructor.cc:195)\r\n==64241==    by 0x567E8BB4: tensorflow::(anonymous namespace)::GraphConstructor::Construct(tensorflow::(anonymous namespace)::GraphConstructor::Options const&, absl::lts_2020_02_25::Span<tensorflow::NodeDef const* const>, tensorflow::VersionDef const*, tensorflow::FunctionDefLibrary const*, tensorflow::Graph*, tensorflow::ShapeRefiner*, std::vector<std::pair<tensorflow::Node*, int>, std::allocator<std::pair<tensorflow::Node*, int> > >*, std::vector<tensorflow::Node*, std::allocator<tensorflow::Node*> >*, std::vector<tensorflow::SafeTensorId, std::allocator<tensorflow::SafeTensorId> >*) (graph_constructor.cc:503)\r\n==64241==    by 0x567E93AC: tensorflow::ImportGraphDef(tensorflow::ImportGraphDefOptions const&, tensorflow::GraphDef const&, tensorflow::Graph*, tensorflow::ShapeRefiner*, tensorflow::ImportGraphDefResults*) (graph_constructor.cc:1556)\r\n==64241== \r\n==64241== 1,971,801 (1,728 direct, 1,970,073 indirect) bytes in 12 blocks are definitely lost in loss record 144,855 of 144,860\r\n==64241==    at 0x4C2AC38: operator new[](unsigned long) (vg_replace_malloc.c:433)\r\n==64241==    by 0x842C67275: Init (flatrep.h:267)\r\n==64241==    by 0x842C67275: FlatRep (flatrep.h:52)\r\n==64241==    by 0x842C67275: FlatMap (flatmap.h:75)\r\n==64241==    by 0x842C67275: FlatMap (flatmap.h:72)\r\n==64241==    by 0x842C67275: TF_Graph::TF_Graph() (c_api.cc:1534)\r\n==64241==    by 0x842C6732D: TF_NewGraph (c_api.cc:1539)\r\n==64241==    by 0x7F938FCB: Java_org_tensorflow_internal_c_1api_global_tensorflow_TF_1NewGraph (in /home/apocock/.javacpp/cache/tensorflow-core-api-0.3.1-linux-x86_64.jar/org/tensorflow/internal/c_api/linux-x86_64/libjnitensorflow.so)\r\n==64241==    by 0xEBC3C7A: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBB6CC8: ???\r\n==64241==    by 0x6430284: JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, Thread*) (in /usr/java/jdk-16.0.1/lib/server/libjvm.so)\r\n==64241== \r\n==64241== 2,300,256 (15,792 direct, 2,284,464 indirect) bytes in 14 blocks are definitely lost in loss record 144,856 of 144,860\r\n==64241==    at 0x4C2A593: operator new(unsigned long) (vg_replace_malloc.c:344)\r\n==64241==    by 0x842C67322: TF_NewGraph (c_api.cc:1539)\r\n==64241==    by 0x7F938FCB: Java_org_tensorflow_internal_c_1api_global_tensorflow_TF_1NewGraph (in /home/apocock/.javacpp/cache/tensorflow-core-api-0.3.1-linux-x86_64.jar/org/tensorflow/internal/c_api/linux-x86_64/libjnitensorflow.so)\r\n==64241==    by 0xEBC3C7A: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBBF3AA: ???\r\n==64241==    by 0xEBB6CC8: ???\r\n==64241==    by 0x6430284: JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, Thread*) (in /usr/java/jdk-16.0.1/lib/server/libjvm.so)\r\n==64241==    by 0x64C369A: jni_invoke_static(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*) [clone .constprop.1] (in /usr/java/jdk-16.0.1/lib/server/libjvm.so)\r\n==64241==\r\n==64241== 3,249,675 (640 direct, 3,249,035 indirect) bytes in 20 blocks are definitely lost in loss record 144,858 of 144,860\r\n==64241==    at 0x4C2A593: operator new(unsigned long) (vg_replace_malloc.c:344)\r\n==64241==    by 0x842C71031: allocate (new_allocator.h:111)\r\n==64241==    by 0x842C71031: allocate (alloc_traits.h:436)\r\n==64241==    by 0x842C71031: _M_allocate_node<const std::piecewise_construct_t&, std::tuple<const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&>, std::tuple<> > (hashtable_policy.h:2060)\r\n==64241==    by 0x842C71031: std::__detail::_Map_base<std::string, std::pair<std::string const, tensorflow::Node*>, std::allocator<std::pair<std::string const, tensorflow::Node*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&) (hashtable_policy.h:725)\r\n==64241==    by 0x842C717AF: operator[] (unordered_map.h:976)\r\n==64241==    by 0x842C717AF: GraphImportGraphDefLocked (c_api.cc:1722)\r\n==64241==    by 0x842C79442: TF_LoadSessionFromSavedModel (c_api.cc:2235)\r\n==64241==    by 0x7F939893: Java_org_tensorflow_internal_c_1api_global_tensorflow_TF_1LoadSessionFromSavedModel__Lorg_tensorflow_internal_c_1api_TF_1SessionOptions_2Lorg_tensorflow_internal_c_1api_TF_1Buffer_2Lorg_bytedeco_javacpp_BytePointer_2Lorg_bytedeco_javacpp_PointerPointer_2ILorg_tensorflow_internal_c_1api_TF_1Graph_2Lorg_tensorflow_internal_c_1api_TF_1Buffer_2Lorg_tensorflow_internal_c_1api_TF_1Status_2 (in /home/apocock/.javacpp/cache/tensorflow-core-api-0.3.1-linux-x86_64.jar/org/tensorflow/internal/c_api/linux-x86_64/libjnitensorflow.so)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo leaks in the saved model loading code.\r\n\r\n**Standalone code to reproduce the issue**\r\nRun the code from https://github.com/tensorflow/java/issues/304 under valgrind.\r\n\r\n**Other info / logs**\r\nI have a 29M valgrind file but it's got a lot of noise from the JVM in it. If necessary I can snip out most of the JVM related noise.", "comments": ["Thanks for reporting the issue!", "Hi, any update on this issue?, I saw a lot of releases after 2.4.1 and none of them solve this issue \ud83d\ude1e", "I think some of the issue might have been deallocation problems in TF-Java which we've fixed in the more recent releases. If you're still hitting this when using the C API directly then you might have a different problem."]}, {"number": 48786, "title": "Allocation Tensor change output shape from [1,136] to [0,136]: Invalid tensor size on reading output tensor", "body": "### 1. System information\r\n\r\nRunning on Google Colab \r\nTensorFlow : 2.4.1\r\n\r\n[Model.zip](https://github.com/tensorflow/tensorflow/files/6387424/Model.zip)\r\nHave attached the Tflite model and the Tensorflow model graph\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n#### Option A: Reference colab notebooks\r\n\r\n1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/drive/1N2umiVDPnTuj6-F9_V00L8GUidHP2rkB?usp=sharing): Demonstrate code to convert PyTorch to Tflite and the issue.\r\n[](url)\r\n### 3. Failure after conversion\r\n\r\n- Generated model before allocation tensor show output tensor size (1,136), after allocation the tensor size becomes (0,136)\r\n- After invoke, if try to get output tensor it show ValueError: Invalid tensor size.\r\n- The PyTorch model is giving desired output (face landmark) before conversion to Tflite\r\n", "comments": ["I could confirm that the attached TFLite model file works well with the TFLite benchmark tool.", "@vineetkushwaha Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist.", "@abattery I am inferencing the model (face landmark) in python and here is the reference code for inference (Google Colab) that will produce the error. Model.zip file contains the Tflite model and sample input image (face image).\r\nReference [TensorFlow Model Colab] https://colab.research.google.com/drive/1N2umiVDPnTuj6-F9_V00L8GUidHP2rkB?usp=sharing#scrollTo=6Kyyfe3O97Tr \r\n[Model.zip](https://github.com/tensorflow/tensorflow/files/6389646/Model.zip)\r\n", "@vineetkushwaha Thank you! I could reproduce the situation at my end. Will triage this bug report to the team."]}, {"number": 48783, "title": "Help: Using Demo application of tflite java, how to run three different models on CPU, GPU and DSP in parallel ", "body": "System information\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8.1.0 \r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 8 Pro with Qualcomm Snapdragon 845\r\n\r\n\r\nDescribe the problem\r\n\r\nQualcomm Snapdragon 845 contains three hardware devices named CPU, GPU and DSP. I am implementing the demo tflite Android application on the Mi 8 pro mobile phone based on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo. I am trying to write a code such that three different classifier instances execute in parallel on CPU, GPU and DSP simultaneously.\r\n\r\n`  \r\nprivate void startBackgroundThread() {\r\n    backgroundThread = new HandlerThread(HANDLE_THREAD_NAME);\r\n    backgroundThread.start();\r\n    backgroundHandler = new Handler(backgroundThread.getLooper());\r\n    \r\n    for ( int i = 0; i < 200; i++) {\r\n       \r\n     TimeUnit.MILLISECONDS.sleep(1);\r\n\r\n    backgroundHandler.post(periodicClassify);\r\n    backgroundHandler.post(periodicClassify2);\r\n    backgroundHandler.post(periodicClassify3);\r\n\r\n\r\n\r\n    }\r\n\r\n\r\n\r\n    updateActiveModel(); //initializes classifier1 on CPU, classifier2 on GPU, classifier3 on DSP\r\n  }\r\n\r\n  private Runnable periodicClassify =\r\n      new Runnable() {\r\n        @Override\r\n        public void run() {\r\n            if (runClassifier) {\r\n              classifyFrame(classifier1,1);\r\n\r\n            }\r\n        }\r\n      };\r\n\r\n  private Runnable periodicClassify2 =\r\n      new Runnable() {\r\n        @Override\r\n        public void run() {\r\n            if (runClassifier) {\r\n              classifyFrame(classifier2,4);\r\n\r\n            }\r\n        }\r\n      };\r\n\r\n  private Runnable periodicClassify3 =\r\n      new Runnable() {\r\n        @Override\r\n        public void run() {\r\n            if (runClassifier) {\r\n              classifyFrame(classifier3, 3);\r\n\r\n            }\r\n        }\r\n      };\r\n\r\n  private void classifyFrame(ImageClassifier classifierNow, int batchSize) {\r\n    if (classifier == null || getActivity() == null || cameraDevice == null) {\r\n      // It's important to not call showToast every frame, or else the app will starve and\r\n      // hang. updateActiveModel() already puts an error message up with showToast.\r\n      // showToast(\"Uninitialized Classifier or invalid context.\");\r\n      return;\r\n    }\r\n    SpannableStringBuilder textToShow = new SpannableStringBuilder();\r\n    Bitmap bitmap = textureView.getBitmap(classifier.getImageSizeX(), classifier.getImageSizeY());\r\n    classifierNow.classifyFrame(bitmap, textToShow, batchSize); // Execute classifierNow with a batch size.\r\n    bitmap.recycle();\r\n    showToast(textToShow);\r\n  }\r\n\r\n`\r\n\r\nWith this code, I am able to successfully run the classifier1, classifier2, classifier3 on CPU, GPU and DSP respectively. However, the execution is not happening in parallel. i.e. classifier2 starts executing on GPU only after classifier1 is finished. Similarly classifier3 starts executing only after classifier2 is done.\r\n\r\nTo solve this I have written my startBackgroundThread function by creating different threads in this way:\r\n\r\n`\r\nprivate void startBackgroundThread() {\r\n    backgroundThread = new HandlerThread(HANDLE_THREAD_NAME);\r\n    backgroundThread.start();\r\n    backgroundHandler = new Handler(backgroundThread.getLooper());\r\n\r\n    backgroundThread2 = new HandlerThread(HANDLE_THREAD_NAME2);\r\n    backgroundThread2.start();\r\n    backgroundHandler2 = new Handler(backgroundThread2.getLooper());\r\n\r\n    backgroundThread3 = new HandlerThread(HANDLE_THREAD_NAME3);\r\n    backgroundThread3.start();\r\n    backgroundHandler3 = new Handler(backgroundThread3.getLooper());\r\n    \r\n    \r\n    for ( int i = 0; i < 200; i++) {\r\n       \r\n     TimeUnit.MILLISECONDS.sleep(1);\r\n\r\n    backgroundHandler.post(periodicClassify);\r\n    backgroundHandler2.post(periodicClassify2);\r\n    backgroundHandler3.post(periodicClassify3);\r\n\r\n\r\n\r\n    }\r\n\r\n\r\n\r\n    updateActiveModel(); //initializes classifier1 on CPU, classifier2 on GPU, classifier3 on DSP\r\n  }\r\n\r\n`\r\n\r\nThis code creates new problems. If I implement this code, then I have no control over which thread executes at which time (race condition). Mostly the batch sizes are getting mixed up and the application crashes because of it.\r\n\r\nMy runInference function looks like this:\r\n\r\n`    protected void runInference2(int tentativeBatchSize) {\r\n         labelProbArray = new byte[tentativeBatchSize][getNumLabels()];\r\n\r\n\r\n        Tensor t1 = tflite.getInputTensor(0);\r\n\r\n        int[] dims = t1.shape();\r\n\r\n        for ( int dim : dims ){\r\n            System.out.println(\"Dimension \" + dim + getModelPath());\r\n        }\r\n\r\n        long startModel = SystemClock.uptimeMillis();\r\n\r\n\r\n        int[] timings2 = new int[] {tentativeBatchSize,224,224,3};\r\n\r\n        tflite.resizeInput(0,timings2);\r\n\r\n        for ( int dim : dims ){\r\n            System.out.println(\"Dimension \" + dim + getModelPath());\r\n        }\r\n\r\n\r\n        long endModel = SystemClock.uptimeMillis();\r\n\r\n        System.out.println(\"Resize time \" + (endModel-startModel));\r\n\r\n\r\n\r\n        tflite.run(imgData, labelProbArray);\r\n    }\r\n`\r\n\r\nSome one please help me with a fix in such a way that I can execute classifiers on CPU, GPU and DSP in parallel.\r\n\r\nThank you.\r\n", "comments": ["@srjoglekar246 could you triage this issue?", "Are all three 'classifiers' different instances of [ImageClassifier](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L48)? Note that TFLite Interpreters are not thread-safe, so using one from multiple threads will cause issues.", "Yes. To be precise, 'classifier' instance of ImageClassifier runs nasnet_mobile.tflite. 'classifier2' takes care of mobilenet_v1_0.25_224.tflite. classifier3 runs the inference for  inception_v1_224_quant.tflite. All these are from the Hosted models page https://www.tensorflow.org/lite/guide/hosted_models. ", "I am not sure what is going on here. The code samples you posted aren't very clear on why the classifiers would run serially if they are in different threads. This might be an Android thing, because of the memory required by the interpreters?\r\n\r\nAdding @lintian06 who might have some insight.", "This code is making the threads in serial fashion. ( Here it is understandable because I am using only one thread ).\r\n\r\n`\r\n        private void startBackgroundThread() {\r\n        backgroundThread = new HandlerThread(HANDLE_THREAD_NAME);\r\n        backgroundThread.start();\r\n        backgroundHandler = new Handler(backgroundThread.getLooper());\r\n        \r\n        for ( int i = 0; i < 200; i++) {\r\n           \r\n         TimeUnit.MILLISECONDS.sleep(1);\r\n        \r\n        backgroundHandler.post(periodicClassify);\r\n        backgroundHandler.post(periodicClassify2);\r\n        backgroundHandler.post(periodicClassify3);\r\n        \r\n        \r\n        \r\n        }\r\n        \r\n        \r\n        \r\n        updateActiveModel(); //initializes classifier1 on CPU, classifier2 on GPU, classifier3 on DSP\r\n        }\r\n\r\n\r\n`\r\n\r\nHowever, if I use the code below which has different threads, then the application is crashing because the batch size of one thread's interpreter is getting mixed with the batch size of another thread's interpreter.\r\n\r\n`\r\n\r\n        private void startBackgroundThread() {\r\n        backgroundThread = new HandlerThread(HANDLE_THREAD_NAME);\r\n        backgroundThread.start();\r\n        backgroundHandler = new Handler(backgroundThread.getLooper());\r\n        \r\n        backgroundThread2 = new HandlerThread(HANDLE_THREAD_NAME2);\r\n        backgroundThread2.start();\r\n        backgroundHandler2 = new Handler(backgroundThread2.getLooper());\r\n        \r\n        backgroundThread3 = new HandlerThread(HANDLE_THREAD_NAME3);\r\n        backgroundThread3.start();\r\n        backgroundHandler3 = new Handler(backgroundThread3.getLooper());\r\n        \r\n        \r\n        for ( int i = 0; i < 200; i++) {\r\n           \r\n         TimeUnit.MILLISECONDS.sleep(1);\r\n        \r\n        backgroundHandler.post(periodicClassify);\r\n        backgroundHandler2.post(periodicClassify2);\r\n        backgroundHandler3.post(periodicClassify3);\r\n        \r\n        \r\n        \r\n        }\r\n        \r\n        \r\n        \r\n        updateActiveModel(); //initializes classifier1 on CPU, classifier2 on GPU, classifier3 on DSP\r\n        }\r\n\r\n`\r\n\r\nThis is the error if I implement the above code:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: CameraBackground3\r\n    Process: android.example.com.tflitecamerademo, PID: 18540\r\n    java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (input) with 602112 bytes from a Java Buffer with 451584 bytes.\r\n        at org.tensorflow.lite.Tensor.throwIfSrcShapeIsIncompatible(Tensor.java:423)\r\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:189)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:304)\r\n        at com.example.android.tflitecamerademo.ImageClassifierInceptionV1Quant.runInference2(ImageClassifierInceptionV1Quant.java:132)\r\n        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:198)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame2(Camera2BasicFragment.java:4893)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment.access$2000(Camera2BasicFragment.java:101)\r\n        at com.example.android.tflitecamerademo.Camera2BasicFragment$24.run(Camera2BasicFragment.java:3189)\r\n        at android.os.Handler.handleCallback(Handler.java:790)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:164)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n\r\nAs you can see, in the thread which executes inception v1, the batch size isn't being updated. This is the issue I am facing if I use multiple threads.\r\n\r\n\r\n\r\n", "Where do you update the batch size? There is some element of your code that isn't thread safe, and it might be somewhere in `ImageClassifierInceptionV1Quant` (and the other classes like that).", "This is the classifyFrame method I used in the ImageClassifier class. \r\n\r\n```\r\n  void classifyFrame(Bitmap bitmap, SpannableStringBuilder builder, int batchSize) {\r\n    if (tflite == null) {\r\n      Log.e(TAG, \"Image classifier has not been initialized; Skipped.\");\r\n      builder.append(new SpannableString(\"Uninitialized Classifier.\"));\r\n    }\r\n\r\n    tentative_batch_size = batchSize;\r\n\r\n    imgData =\r\n            ByteBuffer.allocateDirect(\r\n                    tentative_batch_size\r\n                            * 224\r\n                            * 224\r\n                            * 3\r\n                            * getNumBytesPerChannel());\r\n\r\n\r\n    imgData.order(ByteOrder.nativeOrder());\r\n\r\n    String model_name = getModelPath();\r\n\r\n\r\n    convertBitmapToByteBuffer(bitmap);\r\n    // Here's where the magic happens!!!\r\n    long startTime = SystemClock.uptimeMillis();\r\n    runInference2(tentative_batch_size);\r\n    long endTime = SystemClock.uptimeMillis();\r\n\r\n\r\n    Log.d(TAG, \"Timecost to run model inference: \" + \" \" + model_name + \" \" + Long.toString(endTime - startTime));\r\n\r\n\r\n\r\n\r\n\r\n    // Smooth the results across frames.\r\n    applyFilter();\r\n\r\n    // Print the results.\r\n    long labelStartTime = SystemClock.uptimeMillis();\r\n    printTopKLabels(builder);\r\n    long duration = endTime - startTime;\r\n    SpannableString span = new SpannableString(model_name + \"  \" + duration + \" ms\");\r\n    span.setSpan(new ForegroundColorSpan(android.graphics.Color.LTGRAY), 0, span.length(), 0);\r\n    builder.append(span);\r\n    long labelEndTime = SystemClock.uptimeMillis();\r\n\r\n    timeCostToPutLabel = labelEndTime - labelStartTime;\r\n\r\n\r\n    Log.d(TAG, \"Timecost to put labels: \" + model_name + \"    \" + Long.toString(timeCostToPutLabel));\r\n\r\n  }\r\n```\r\n\r\nI use this runInference method in ImageClassifierInceptionV1Quant class\r\n\r\n```\r\n    protected void runInference2(int tentative) {\r\n         labelProbArray = new byte[tentative][getNumLabels()];\r\n\r\n\r\n        long startModel = SystemClock.uptimeMillis();\r\n\r\n\r\n        int[] timings2 = new int[] {tentative,224,224,3};\r\n\r\n        tflite.resizeInput(0,timings2);\r\n\r\n        tflite.run(imgData, labelProbArray);\r\n    }\r\n```\r\nDo you think this is the code in classifyFrame method of ImageClassifier class that is not thread safe?\r\n\r\n```\r\n    imgData =\r\n            ByteBuffer.allocateDirect(\r\n                    tentative_batch_size\r\n                            * 224\r\n                            * 224\r\n                            * 3\r\n                            * getNumBytesPerChannel());\r\n```\r\n"]}, {"number": 48775, "title": "Please support native acos/asin/atan/atan2 operations within TFLite library", "body": "**System information**\r\n- `Linux 5.11.4-arch1-1 #1 SMP PREEMPT Sun, 07 Mar 2021 18:00:49 +0000 x86_64 GNU/Linux`\r\n- `TensorFlow 2.4.1` from `tensorflow-opt-cuda` package (https://archlinux.org/packages/community/x86_64/tensorflow-opt-cuda)\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nConverterError: /usr/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:5037:0: error: 'tf.Acos' op is neither a custom op nor a flex op\r\n/usr/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n/home/barabanus/work/4a-games/pfnn-tools/test/tflite-acos-missing.py:6:0: note: called from\r\n/usr/lib/python3.9/site-packages/IPython/utils/py3compat.py:168:0: note: called from\r\n/usr/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2740:0: note: called from\r\n/usr/lib/python3.9/site-packages/IPython/core/shellapp.py:377:0: note: called from\r\n/usr/lib/python3.9/site-packages/IPython/core/shellapp.py:452:0: note: called from\r\n/usr/lib/python3.9/site-packages/IPython/core/shellapp.py:328:0: note: called from\r\n/usr/lib/python3.9/site-packages/IPython/terminal/ipapp.py:323:0: note: called from\r\n/usr/lib/python3.9/site-packages/traitlets/config/application.py:87:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.Acos {device = \"\"}\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\n# use tensorflow API v1\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n\r\nX = tf.placeholder(dtype=tf.float32, shape=[], name=\"X\")\r\nY = tf.acos(X, name=\"Y\")\r\n\r\nwith tf.Session() as session:\r\n    acosZero = session.run(Y, feed_dict={ X: 0., })\r\n    acosOne = session.run(Y, feed_dict={ X: 1., })\r\n    print(f\"TensorFlow: acos(0.)={acosZero}, acos(1.)={acosOne}\")\r\n\r\n    converter = tf.lite.TFLiteConverter.from_session(\r\n        session,\r\n        input_tensors=[X],\r\n        output_tensors=[Y]\r\n    )\r\n    flatbuffer = converter.convert()\r\n```\r\n", "comments": ["Thanks for filing a feature request. You can enable the Select TF option to utilize the existing TF kernels. In this way, you can enable the TF Acos op without implementing a custom op. See https://www.tensorflow.org/lite/guide/ops_select.\r\n\r\n```\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n```", "@abattery Thank you for your suggestion to enable Select TF option, but in our environment TensorFlow is not supported. We can use only TFLite operations.", "To understand the circumstances, could you share why your deployment environment can't have it if possible?", "@abattery Our runtime hardware is game consoles (Microsoft XBox and Sony PlayStation) with realtime inference requirement. They don't support TF with all its uncontrolled memory allocations, signals and threads management, but we hope to use TFLite.", "Do you need to all acos/asin/atan/atan2 ops? If possible, it would be nice to share your detailed op requirements including tensor type and tensor shape requirements. For example, is supporting float32 inputs up to 4 four dimensions sufficient?", "@abattery Oh, I see! Well, actually we need `acos` with `float32` inputs of unrestricted size (`shape=[None]`)", "@thaink could you take a look at this feature request?", "If this feature request is quite urgent one at your side, please consider implementing a custom acos op for your case. Here is the guide document describing how to write down a TFLite custom sin op.\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_custom#defining_the_kernel_in_the_tensorflow_lite_runtime", "These under our list to add to builtin ops. I think I can handle them sometime later this week.", "I get an error even with v2.6.0. FlexDelegate does not seem to be enabled for **`acos`**, **`atan`**, and **`asin`**. **`tf.lite.OpsSet.SELECT_TF_OPS`** seems to cause an error, so I implemented an approximate calculation **`acos`**. It is not recommended because it is very computationally expensive and redundant. However, it can be implemented without custom operations, using only standard OPs.\r\n\r\nhttps://github.com/PINTO0309/openvino2tensorflow/blob/f91027ef98e9e30912505b9a8e9ab455a1c9f08f/openvino2tensorflow/openvino2tensorflow.py#L3557-L3578\r\n\r\n```python\r\ndef pseudo_acos(x, output_myriad):\r\n    x_abs = None\r\n    if output_myriad:\r\n        x_abs = tf.math.sqrt(tf.math.square(x)) # OAK-D (MyriadX) is not compatible with ABS.\r\n    else:\r\n        x_abs = tf.abs(x)\r\n    neg = tf.math.divide(tf.math.multiply(tf.minimum(x, 0), -1), x_abs)\r\n    x = x_abs\r\n    y = tf.constant(-0.0187293)\r\n    y = tf.math.multiply(y, x)\r\n    y = tf.math.add(y, 0.0742610)\r\n    y = tf.math.multiply(y, x)\r\n    y = tf.math.subtract(y, 0.2121144)\r\n    y = tf.math.multiply(y, x)\r\n    y = tf.math.add(y, 1.5707288)\r\n    y = tf.math.multiply(y, tf.sqrt(tf.math.subtract(1.0, x)))\r\n    y = tf.math.multiply(y, tf.math.subtract(1.0, tf.math.multiply(2.0, neg)))\r\n    acos = tf.math.add(tf.math.multiply(neg, 3.14159265358979), y)\r\n    return acos\r\n```\r\n\r\n![Screenshot 2021-09-21 11:49:16](https://user-images.githubusercontent.com/33194443/134512382-4fbfc136-48f4-40da-9a25-c052207ae7f7.png)\r\n.", "@PINTO0309 Thank you for sharing your implementation of `acos`. I managed to do a similar approximation based on its similarity to `sqrt(1 - abs(x))` + polynome to reduce error, but it would be good to have this common operation implemented within TFLite by default\r\n```\r\nimport numpy as np\r\n\r\n# see tau manifest\r\nTAU = 2 * np.pi\r\n\r\ndef acosTF(x, margin=1e-5):\r\n    \"\"\" Approximate arccos() as it's not supported within TFLite\r\n    \"\"\"\r\n    x = tf.clip_by_value(x, margin - 1., 1. - margin)\r\n\r\n    # set initial approximation\r\n    xp = tf.abs(x)\r\n    t = tf.sqrt(1. - xp)\r\n\r\n    # fix with polynomial\r\n    c3 = -0.0200752\r\n    c2 = xp * c3 + 0.0759031\r\n    c1 = xp * c2 - 0.2126757\r\n    c0 = xp * c1 + 1.5707963\r\n    p = t * c0\r\n\r\n    # correct for negative argument\r\n    n = TAU / 2. - p\r\n    y = tf.where(x >= 0., p, n)\r\n\r\n    return y\r\n```", "@barabanus \r\nA structurally simple model was generated, and the output results match perfectly. I would like to thank you as well. :+1: \r\n```\r\nTF\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Float32\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.00447434], dtype=float32)>\r\n\r\nTFLite\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Float32\r\narray([[0.00447434]], dtype=float32)\r\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ INT8\r\narray([[0.00447434]], dtype=float32)\r\n```\r\n![Screenshot 2021-11-09 21:29:24](https://user-images.githubusercontent.com/33194443/140924447-e9215953-4ae6-4723-baa1-164f7efa8726.png)\r\n\r\nThe method you suggested is clearly a better method for approximate calculations. However, I have created models that are compatible not only with TFLite, but also with all of TensorFlow, EdgeTPU, ONNX, OpenVINO, Myriad Blob, TensorRT, TensorFlow.js, and CoreML. In fact, I had consciously avoided using the last GreaterEqual and Select operations because they do not work well with other frameworks. Please ignore this fact, as it has nothing to do directly with this issue.\r\n\r\nThanks again."]}, {"number": 48773, "title": "Wrong results of AvgPoolGrad and AvgPool3dGrad in some specific shapes", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n **Linux Ubuntu 18.04**\r\n- TensorFlow installed from (source or binary):\r\n**source**\r\n- TensorFlow version (use command below):\r\n2.4.1 and 1.15 both tried\r\n- Python version:\r\n3.7.5\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nHello, I'm using MaxPoolGrad and find that in same cases the results seemed to be wrong compared with hand-calculated results.   One of the cases is as follows.\r\n\r\n    **grad_shape = (1, 1, 5, 1)\r\n    input_shape = (1, 1, 5, 1)\r\n    kernel_size = [1, 1, 7, 1]\r\n    stride_size = [1, 1, 1, 1]\r\n    padding = 'SAME'**\r\n\r\nFor the convenience of calculation\uff0c I set the grad with all one value. \r\nThen the result is\r\n```\r\ntf.Tensor(\r\n[[[[0.8 ]\r\n   [1.05]\r\n   [1.05]\r\n   [1.05]\r\n   [1.05]]]], shape=(1, 1, 5, 1), dtype=float32)\r\n```\r\nWhile the hand-calculated result is \r\n```\r\n[[[[0.85 ]\r\n   [1.1]\r\n   [1.1]\r\n   [1.1]\r\n   [0.85]]]]\r\ntips:\r\n0.85 = 1/4 + 1*3/5\r\n1.1 = 1*2/4 +1*3/5\r\n```\r\n\r\nThe differences appear in following cases:\r\n-kernel_h > input_h &  kernel_h // 2 < input_h - 1\r\n-kernel_w > input_w &  kernel_w // 2 < input_w - 1\r\n\r\n**Describe the expected behavior**\r\nFollowing cases can have right calculation results.\r\n-kernel_h > input_h &  kernel_h // 2 < input_h - 1\r\n-kernel_w > input_w &  kernel_w // 2 < input_w - 1\r\n\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import gen_nn_ops\r\n\r\ndef test_tf_avg_pool_grad(input_shape, grad_shape, ksize, strides, padding, data_format):\r\n    x = tf.Variable(tf.ones(shape=grad_shape), dtype=tf.float32)\r\n    avg_pool_grad = gen_nn_ops.avg_pool_grad(orig_input_shape=input_shape,\r\n                                             grad=x, ksize=ksize,\r\n                                             strides=strides,\r\n                                             padding=padding,\r\n                                             data_format=data_format)\r\n\r\n    return avg_pool_grad\r\n\r\nif __name__ == '__main__':\r\n    grad_shape = (1, 1, 5, 1)\r\n    input_shape = (1, 1, 5, 1)\r\n    kernel_size = [1, 1, 7, 1]\r\n    stride_size = [1, 1, 1, 1]\r\n    padding = 'SAME'\r\n    exp_result = test_tf_avg_pool_grad(input_shape, grad_shape,\r\n                                       kernel_size, stride_size, padding,\r\n                                       'NHWC')\r\n    print(\"======================= the expect result of log:\", exp_result)\r\n```\r\n\r\n**Other info / logs** \r\nhand-caculated process\r\n![image](https://user-images.githubusercontent.com/72602911/116206344-45da2780-a771-11eb-8a56-dd37b902b244.png)\r\n\r\n", "comments": ["Was able to reproduce the issue in Tf version 2.4 and Nightly. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/a1c1cb7bda2d7ef7beac7525df1cf529/48773.ipynb).", "Thanks for reporting! Contribution welcome!", "I am working on that.", "Hi @kkimdev,\r\nCould you please help me who I can ask about some parameters of this function and how I can do that?\r\nXlaOp AvgPoolGrad(XlaOp out_backprop, absl::Span<const int64> gradients_size,\r\n                  absl::Span<const int64> kernel_size,\r\n                  absl::Span<const int64> stride,\r\n                  absl::Span<const std::pair<int64, int64>> spatial_padding,\r\n                  const TensorFormat& data_format,\r\n                  const bool counts_include_padding) {", "@sanjoy Do you know who is the best person to reach out?", "@smit-hinsu maybe?", "Thanks Sanjoy.\r\n\r\n@pointhex Happy to answer any questions you have for AvgPoolGrad helper function.", "Hi @smit-hinsu,\r\n\r\nI'm trying to write a unit test for AvgPoolGrad. But I can't do the right data format. In file pooling_test.cc there is a function MakeNCHWFormat. I need the same but for NHWC. I don't fully understand how spatial_dimensions works in this case.\r\nCould you please clarify that for me? Or maybe there are some useful links.  ", "Look at XlaTensorFormat helper in the following file that converts from the tensorflow format to the XLA format.\r\nthird_party/tensorflow/compiler/tf2xla/kernels/pooling_ops.cc\r\n\r\nFor 4D tensor in NCHW format, spatial_dimensions are 2 and 3 as the last two dimensions are spatial dimensions. It would be 1 and 2 for NHWC format.", "Was able to reproduce the issue in Tf version 2.6 . Please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/f8ea671f657c84f914effdd2c87f8da5/48773.ipynb#scrollTo=9SGXZXynTP4Y)."]}, {"number": 48772, "title": "set_intra_op_parallelism_threads and set_inter_op_parallelism_threads has no impact on thread usage", "body": "**System information**\r\n- I am using a custom implementation of DepthwiseConv3D, extends the Conv3D class and is partially based on code from the following repository, https://github.com/alexandrosstergiou/keras-DepthwiseConv3D adapted to work in tensorflow 2.4.1\r\n- CentOS Linux Version 7\r\n- Installed from source\r\n- Tensorflow version 2.4.1\r\n- Python 3.8.8\r\n- Cuda 11.0, CuDNN 8\r\n- Happens on GTX1080, RTX2080\r\n\r\n**Describe the current behavior**\r\n\r\nI am running code on a compute cluster, hence the different GPUs. I am required by the compute cluster admin to restrict thread usage when possible and had been referred to use the functions from tf.config.threading in the tensorflow documentation. I set both intra and inter thread parallelism to 2 and used an interactive session on the node to monitor thread usage with top, however the usage of the thread parameters seems to have no impact on thread usage. I still observe the python process using all available threads. \r\n\r\nMy understanding from the documentation for these functions is that all is required is to call them with the parameters wanted, no errors related to threading have been raised. \r\n\r\nMy github repository is as follows, I use the mult.csh file under the EEGNet folder to execute the code, which runs the runMB3D.py file, using the network model from MB3DEEGNet.py.\r\nhttps://github.com/matt-houk/MB3DCNN\r\n\r\nI have attached both the stderr and stdout output, I canceled the code when I noticed it using excessive threads.\r\n\r\n[err-mult.txt](https://github.com/tensorflow/tensorflow/files/6382379/err-mult.txt)\r\n[out-mult.txt](https://github.com/tensorflow/tensorflow/files/6382380/out-mult.txt)\r\n\r\n", "comments": ["I just wanted to include some followup info from efforts I have made to debug the issue over the past week. \r\n\r\nI had attempted to set OMP_NUM_THREADS prior to running the python file, this also appears to have no impact. \r\n\r\nI have also set the environment variables TF_NUM_INTEROP_THREADS and TF_NUM_INTRAOP_THREADS prior to application runtime, checking them using os.environ from within python both before and after tensorflow is imported, as well as the tf.config.threading.set_intra/inter_op_parallelism_threads calls, the environment variables remain unchanged at 1, however, when observing the python process using top, I observe that it is using 13 threads using the nTH column in top. \r\n\r\nPlease let me know if it seems I am making an incorrect assumption on anything here. Thank you.", "If there are any other issues that are relevant that I have been unable to unearth during my searches that might be able to help, I would love if somebody could link them! While this issue doesn't prevent me from being able to do work, it dramatically slows the rate at which I can do so and none of my attempts to resolve it have been successful.", "Can someone clarify for me whether the functions \r\n`\r\ntf.config.set_intra_op_parallelism_threads(NUM_THREADS)\r\ntf.config.set_inter_op_parallelism_threads(NUM_THREADS)\r\n`\r\nare meant to control the threads used by the python process or the nvidia-cuda-mps process? \r\n\r\nAdding\r\n`\r\ntf.config.set_soft_device_threading(True)\r\n`\r\nWas able to restrict nvidia-cuda-mps to 4 threads when observed using top, which would be in line with my case where NUM_THREADS = 2, if this is the case, I simply need to determine why Python is choosing to use as many threads as it can. If that is the case, while I would appreciate any advice anyone might have, that might be outside of TensorFlow and instead of a Python issue. Thanks!", "Is there a possibility of getting a response on this issue? Because otherwise, it is probably best to close it. The issue is still unresolved but there has been no activity outside of my own updates. Any additional insight would be appreciated.\r\n\r\nAlso, just to clarify my understanding from above, the functions set_inter_op_parallelism and set_intra_op_parallelism should affect all processes involved with TensorFlow, including both the nvidia-cuda-mps and python processes. My issue is that Python is scaling to all available threads and I want to make sure I am understanding correctly that these functions should effect the Python process, not another process.", "Apologies for the delayed response. Would it be possible to provide some sample D_cropped.npy and K_cropped.npy so that I can try and reproduce the issue? Generally the inter op and intra op threads are meant for the C++ tensorflow executor runtime - to be precise the inter op threadpool controls the number of C++ threads we use to dispath ops which executing functions / graphs and the intra op threadpool controls the size of the EIGEN threadpool for individual ops. The GPU runtime with NVIDIA might have its own threadpool and thats why that needs to be configured as well. Usually we rarely do any multi-threading in python but I'd like to reproduce the issue to figure out what might be going on. \r\n\r\nOne other suggestion to get more data would be to run the Tensorflow profiler https://www.tensorflow.org/guide/profiler which would clearly enumerate the list of threads in action. That might provide some insight as well.", "Here is a sample of the files, \r\n[Data files.zip](https://github.com/tensorflow/tensorflow/files/6713345/Data.files.zip)\r\n\r\nSorry for the delayed response, I will also look into the profiler on my end!\r\n", "Thanks... I was able to reproduce the issue and yeah I do see a large number of threads. A hypothesis is that there are some tf.data threads that you're seeing. But it'll be a lot clearer with the profile. ", "Just got the opportunity to try the profiler, sorry for the further delay, was working to meet some deadlines. Any attempt I make to use profiling appears to immediately cause a segfault. I will update the GitHub (I've been behind on that anyway) and the relevant files can be found under the Groups directory, I made some modifications to the code in an attempt to improve runtime, using a different method of implementing a DepthwiseConv3D layer I found. \r\n\r\nBelow is the stack trace from the GDB debugger, everything else should be about the same. The Segmentation Fault consistently occurs at the model.fit call on line 149 in the runMotor.py file. \r\n\r\n`\r\n#0  0x00002aaaaaf70c3c in free () from /lib64/libc.so.6\r\n#1  0x00002aaab742b65d in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReverseOp<Eigen::array<long, 5ul> const, Eigen::TensorShufflingOp<Eigen::array<long, 5ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 5> const, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorForcedEvalOp<Eigen::TensorShufflingOp<Eigen::array<long, 5ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::TensorForcedEvalOp<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorShufflingOp<Eigen::array<long, 5ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::NoOpOutputKernel const> const> const> const> const> const, Eigen::ThreadPoolDevice, true, (Eigen::internal::TiledEvaluation)1>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorReverseOp<Eigen::array<long, 5ul> const, Eigen::TensorShufflingOp<Eigen::array<long, 5ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 5> const, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorForcedEvalOp<Eigen::TensorShufflingOp<Eigen::array<long, 5ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const> const> const, Eigen::TensorForcedEvalOp<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorShufflingOp<Eigen::array<long, 5ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::NoOpOutputKernel const> const> const> const> const> const&, Eigen::ThreadPoolDevice const&) ()\r\n   from /usr/local/usrapps/multibranch/mjhouk/tf241_py377/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00002aaab742c42e in tensorflow::functor::CuboidConvolutionBackwardFilter<Eigen::ThreadPoolDevice, float>::operator()(Eigen::ThreadPoolDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer>, int, int, int) ()\r\n   from /usr/local/usrapps/multibranch/mjhouk/tf241_py377/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00002aaab7435294 in tensorflow::Conv3DCustomBackpropFilterOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*) ()\r\n   from /usr/local/usrapps/multibranch/mjhouk/tf241_py377/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00002aaae1004707 in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::Process(tensorflow::SimplePropagatorState::TaggedNode, long long) ()\r\n   from /usr/local/usrapps/multibranch/mjhouk/tf241_py377/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#5  0x00002aaab4f65d32 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n    ()\r\n   from /usr/local/usrapps/multibranch/mjhouk/tf241_py377/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00002aaab4f62047 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /usr/local/usrapps/multibranch/mjhouk/tf241_py377/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00002aaae10a3d3c in tensorflow::(anonymous namespace)::PThread::ThreadFn(void*) ()\r\n   from /usr/local/usrapps/multibranch/mjhouk/tf241_py377/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#8  0x00002aaaaacd6e25 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00002aaaaafe9bad in clone () from /lib64/libc.so.6\r\n`\r\n\r\nThis might require a separate issue post, if so let me know and I will proceed. I am approaching concluding the project I was working on, but am fully willing to continue with efforts to debug this issue as I am able. \r\n\r\nLet me know if you have any other questions!"]}, {"number": 48762, "title": "gradient error when tf.concat is used on a list of 0-D tensors (should not allow 0-D tensors concatenation at the first place)", "body": "**System information**\r\nJust a small corner case issue, not much related to OS, Python or hardware. \r\n- OS: Win10 and macOS 10.15.7\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen tf.concat is used to concat a list of 0-D tensors (although this very step should not be allowed), forward calculations have no issue, but reporting ZeroDivisionError when one tries to get the gradient.  \r\n\r\n**Describe the expected behavior**\r\n\r\nEither report error when tf.concat is used to concat a list of 0-D tensors as how numpy handles this situation (preferred), or fixed the gradient flow, or give more understandable error message (error message ZeroDivisionError seems somewhat misleading as /0 does not happen in the code).  \r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndef this_works():\r\n    x = tf.zeros([10])  \r\n    with tf.GradientTape() as gt:\r\n        gt.watch(x)\r\n        a = []\r\n        for i in range(10):\r\n            a.append(x[i:i+1])\r\n        y = tf.reduce_sum(tf.concat(a, axis=0))\r\n    print(y)\r\n    print(gt.gradient(y, x))\r\n\r\nthis_works()\r\n\r\n\r\ndef this_fails():\r\n    x = tf.zeros([10])  \r\n    with tf.GradientTape() as gt:\r\n        gt.watch(x)\r\n        a = []\r\n        for i in range(10):\r\n            a.append(x[i]) # should not allowed\r\n        y = tf.reduce_sum(tf.concat(a, axis=0))\r\n    print(y)\r\n    print(gt.gradient(y, x)) # fails here, error message ZeroDivisionError \r\n    \r\nthis_fails()\r\n```\r\n", "comments": ["@ymodak ,\r\nI was able to reproduce the issue in TF v2.4,v2.5.0rc1 and nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/54c89dbc04ec18a9d7f7ed8e796e7106/48762.ipynb) here.", "I was able to reproduce the issue in TF v2.6 .Please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/308d62717c40c0c4cedff8fcf1942d12/48762.ipynb#scrollTo=U1aCfIgn4nj3)."]}, {"number": 48759, "title": "Change the default for num_epochs to 1 from None in make_csv_dataset", "body": "The default for the num_epochs parameter in make_csv_dataset is None:\r\n\r\n```\r\n    num_epochs: An int specifying the number of times this dataset is repeated.\r\n      If None, cycles through the dataset forever.\r\n```\r\n\r\nThis is really confusing to the user (at least, it was very surprising and confusing to me). I could not tell why my training appeared to be 'hanging'. It turned out it was due to this default. I propose that this be changed to 1. Looping forever is highly unexpected and leads to many user time cycles left trying to debug and understand what's happening.\r\n\r\nI'm filing this per the discussion in TFRS regarding this problem which took a long time to troubleshoot and fix. Per that discussion,\r\n\r\n> [the]... default [...] goes against the compositional nature of tf.data APIs (where you'd normally stick a .repeat() at the end if you want an infinite dataset).\r\n\r\n(Opening an issue here to replace https://github.com/tensorflow/io/issues/1377 per @yongtang 's suggestion).", "comments": [":+1:\r\n\r\nIn the `tf.data` world the usual way to get an infinite dataset is to use `dataset.repeat()`. It is therefore surprising that this repeats infinitely by default. I suspect this is because this API predates the modern `tf.data` functions?", "Good point, I guess we'll find out once we hear from some folks @ tf core.", "@dgoldenberg-audiomack I agree 100%, it shouldn't repeat by default. Switching the default on the `experimental.make_csv_dataset` symbol will likely break existing users though, so it is prudent to wait until we migrate `make_csv_dataset` out of \"experimental\". I'm adding a TODO reminder in the code.", "Thanks, @aaudiber, I think that sounds like the right time to do it. Maybe this ticket can be referenced from that epic when it comes around."]}, {"number": 48753, "title": "tf.distribute.MirroredStrategy hangs when custom op used NCCL", "body": "TensorFLow version: 2.4.1\r\npython version: 3.8.5\r\nCUDA version: 11.2\r\nCUDA Driver version: 460.32.03\r\nNCCL version: 2.8.4\r\n\r\nIssue elaboration:\r\nI implemented a custom op where NCCL API are used to exchange data among GPUs. And I used this custom op with `tf.distribute.MirroredStrategy`.\r\nHere is the status:\r\n\r\n- Situation 1:\r\n\r\n```python\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    ...\r\n    def call(inputs, training=True):\r\n        outputs = custom_op(inputs, training=training)\r\n        return outputs\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n     layer = CustomLayer()\r\n\r\n@tf.function\r\ndef _step(input):\r\n    return layer(input)\r\n\r\nfor iterations in range(10):\r\n    output = strategy.run(_step, args=(inputs,)) # where inputs are an PerReplica data\r\n```\r\nI used 8 GPUs and set `NCCL_LAUNCH_MODE=PARALLEL` to run the above code snippet, **it works.**\r\n\r\n\r\n- Situation 2:\r\n```python\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    ...\r\n    def call(inputs, training=True):\r\n        outputs = custom_op(inputs, training=training)\r\n        return outputs\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n     layer = CustomLayer()\r\n     var = tf.Variable(initial_value=1.0, dtype=tf.float32)\r\n\r\n@tf.function\r\ndef _step(input):\r\n    return var * layer(input)\r\n\r\nfor iterations in range(10):\r\n    output = strategy.run(_step, args=(inputs,)) # where inputs are an PerReplica data\r\n```\r\nI still used 8 GPUs and set `NCCL_LAUNCH_MODE=PARALLEL`  to run the above code snippet. But only 1 or 2 iterations will be executed successfully, then the program will **hang**.  <br>\r\nI tried to add `std::cout` inside of my custom op to check which GPU's OpKernel is not executed. And it always shows that `GPU 0` 's OpKernel::Compute() is not executed. And other GPUs are waiting for GPU0 to arrive at NCCL API calling point.\r\n\r\n- Situation 3:\r\n```python\r\nclass CustomLayer(tf.keras.layers.Layer):\r\n    ...\r\n    def call(inputs, training=True):\r\n        outputs = custom_op(inputs, training=training)\r\n        return outputs\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n     layer = CustomLayer()\r\n     var = tf.Variable(initial_value=1.0, dtype=tf.float32)\r\n\r\n@tf.function\r\ndef _step(input):\r\n    return var * layer(input)\r\n\r\nfor iterations in range(10):\r\n    output = strategy.run(_step, args=(inputs,)) # where inputs are an PerReplica data\r\n```\r\nThe same code with situation 2, and 8 GPUs are used. But **not set** environment variable `NCCL_LAUNCH_MODE=PARALLEL`. And it **works**, all iterations are successfully executed.\r\n<br>\r\n\r\n\r\n**My question is:**\r\nHow can I find out why `GPU0` hangs after 1 or 2 iterations in situation 2?  <br>\r\nHow to find out which operation caused TensorFlow stucks for executing the Opkernel::Comput()?", "comments": ["If you want to reproduce this issue, here are the steps:\r\n\r\n1. download files from https://drive.google.com/file/d/1VYqAbJHM_DT2wtVZ4RD5PW1vWNr76hz1/view?usp=sharing\r\n2. extract source files from downloaded files, and all the files I mentioned below can be found here.\r\n3. Build docker image using \"cuda11_tf2_ubuntu20.04.dockerfile\" and start a docker container with that image\r\n4. enter that docker container with interactive mode\r\n5. mkdir -p build\r\n6. (Optional) if your GPU's compute capability is not 70, you should specify the correct \"sm\" in \"compile.sh\"\r\n7. bash compile.sh, after this command, a dynamic lib will be compiled and installed to your /usr/local/lib\r\n8. python main.py, then the program will be hanging forever.", "@Jianbing-D \r\nCan you please let us know if this is still an issue in latest tf version.", "I think it is only related to my customized codes. And I am debugging it.\r\n", "> I think it is only related to my customized codes. And I am debugging it.\r\n\r\nHave you got anything yet? I'm facing the same question while using the code you wrote (Sparse Operation Kit), I think it maybe caused by Gradient Tape", "> > I think it is only related to my customized codes. And I am debugging it.\r\n> \r\n> Have you got anything yet? I'm facing the same question while using the code you wrote (Sparse Operation Kit), I think it maybe caused by Gradient Tape\r\n\r\nCould you please provide more info about your issue, and directly ask it in this \u3010[repo](https://github.com/NVIDIA-Merlin/HugeCTR)\u3011?"]}, {"number": 48752, "title": "microspeech pretrained model is uint8 quantized instead of int8 quantized and doesn't work properly", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/train#trained-models\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThere is a download link for the pre-trained microspeech model; speech_commands.zip.  When you download and unzip the files there are 3:\r\nmodel.cc (encoded array is 18222 bytes long)\r\nmodel.tflite (file is 18222 bytes long)\r\nmodel.tb\r\n\r\nThe documentation says that this is a model that is int8 quantized at the inputs and outputs **but it is wrong**, the model in the zip file is actually uint8 quantified on the inputs and outputs.\r\n\r\nThe model used in the microspeech example is 18712 bytes long:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/micro_features/model.cc\r\n\r\nI picked up the model.tflite from the zip file because I needed to load it as a bytestream instead of as an array.  \r\n\r\nI just found out my code is not working because the quantization is wrong.  The spectrograms generate as int8 and because this model is uint8 I've been dropping out half my data when casting an int8 to an uint8.\r\n\r\nI recreated the model using the https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb starting from the pretrained checkpoints and the output model is correctly quantized using int8's.\r\n\r\nThe zipfile needs to be updated with the correct model files and then the link in the page changed to point at the new zip file.\r\n\r\n### Page\r\n![image](https://user-images.githubusercontent.com/250942/116026221-3f886600-a620-11eb-9ded-f36fa633ef91.png)\r\n\r\n### Contents of speech_commands.zip archive\r\n![image](https://user-images.githubusercontent.com/250942/116026202-34cdd100-a620-11eb-93c4-efb54febc6aa.png)\r\n\r\n### Neutron View of downloaded model\r\n![image](https://user-images.githubusercontent.com/250942/116026326-79f20300-a620-11eb-8426-fc80d730b080.png)\r\n\r\n### Neutron View of retrained model\r\n![image](https://user-images.githubusercontent.com/250942/116026398-ab6ace80-a620-11eb-8033-2ab835c66060.png)\r\n", "comments": ["Hi advaitjain, could you please help triage this issue on micro? Thanks."]}, {"number": 48742, "title": "Tensorflow only sees a fraction of Tensorflow memory", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.8\r\n- Installed using virtualenv? pip? conda?: Virtualenv inside docker\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- CUDA/cuDNN version: CUDA 11.3 (host) CUDA 11.2 (docker), cuDNN8\r\n- GPU model and memory: GT 730, 1gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nMy device has 1gb of memory but Tensorflow only sees 636mb:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n | NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\r\n |-------------------------------+----------------------+----------------------+\r\n | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n |                               |                      |               MIG M. |\r\n |===============================+======================+======================|\r\n |   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 N/A |                  N/A |\r\n | 30%   33C    P8    N/A /  N/A |    118MiB /   980MiB |     N/A      Default |\r\n |                               |                      |                  N/A |\r\n +-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n```\r\n>>> import tensorflow as tf\r\n2021-04-24 12:33:23.660088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> tf.test.is_gpu_available()\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2021-04-24 12:33:36.109192: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-24 12:33:36.112728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-04-24 12:33:36.163350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce GT 730 computeCapability: 3.5\r\ncoreClock: 0.9015GHz coreCount: 2 deviceMemorySize: 980.00MiB deviceMemoryBandwidth: 37.33GiB/s\r\n2021-04-24 12:33:36.163429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-04-24 12:33:36.174055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-04-24 12:33:36.174941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-04-24 12:33:36.179867: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-04-24 12:33:36.181045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-04-24 12:33:36.184262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-04-24 12:33:36.187118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-04-24 12:33:36.188172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-04-24 12:33:36.189819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-04-24 12:33:36.190342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-04-24 12:33:37.537902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-24 12:33:37.537984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-04-24 12:33:37.538003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-04-24 12:33:37.540130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 636 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GT 730, p\r\nci bus id: 0000:01:00.0, compute capability: 3.5)\r\nTrue\r\n\r\n```\r\n\r\nCould this be the result of build config?", "comments": ["These two lines are interesting:\r\n\r\n> 2021-04-24 12:33:36.163350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce GT 730 computeCapability: 3.5\r\ncoreClock: 0.9015GHz coreCount: 2 deviceMemorySize: **980.00MiB** deviceMemoryBandwidth: 37.33GiB/s\r\n\r\nvs. \r\n\r\n> 2021-04-24 12:33:37.540130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with **636 MB** memory) -> physical GPU (device: 0, name: NVIDIA GeForce GT 730, p\r\nci bus id: 0000:01:00.0, compute capability: 3.5)\r\n\r\nNot sure how to interpret these messages?", "Yes, exactly! My supposition is that either the CUDA drivers or Tensorflow or both isn't representing the device properly.", "Looks like a known issue: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1405\r\n", "@asteriske ,\r\n\r\nPlease take a look at [moflo](https://github.com/tensorflow/tensorflow/issues/48742#issuecomment-826384113)   comment and let us know if you are still facing the same issue? Thanks!", "I'm not very experienced with GPUs so I may not be interpreting it correctly, but that comment seems to suggest that the mismatch is a result of reallocation? \r\n\r\nI get these numbers in a fresh python process, so does that mean that some mechanism is enforcing sharing before I initialize the device?", "TensorFlow has to reserve some amount of GPU memory for system libraries like cuDNN, cuBLAS etc.; that is likely the delta you're seeing here."]}, {"number": 48720, "title": "Didn't find op for builtin opcode 'CAST' version '1'", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Platformio\r\n- TensorFlow installed from (source or binary): Platformio Library\r\n- Tensorflow version (commit SHA if source): TensorFlowLite_ESP32 0.9.0, model done on tensorflow 2.4\r\n- \r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32\r\n\r\n**Describe the problem**\r\n\r\nWhen i try to run a custom model i get this error :\r\n```Rebooting...\r\nDidn't find op for builtin opcode 'CAST' version '1'\r\n\r\nFailed to get registration from op code  d\r\n\r\nAllocateTensors() failed\r\nGuru Meditation Error: Core  1 panic'ed (LoadProhibited). Exception was unhandled.\r\nCore 1 register dump:\r\nPC      : 0x400d17f4  PS      : 0x00060130  A0      : 0x800e5830  A1      : 0x3ffb1f80  \r\nA2      : 0x3ffd7c70  A3      : 0x00000000  A4      : 0x000003e8  A5      : 0x3ffc03b8  \r\nA6      : 0x00000008  A7      : 0x00000001  A8      : 0x800d17e3  A9      : 0x3ffb1f70  \r\nA10     : 0x00000000  A11     : 0x447a0000  A12     : 0x3ffc044c  A13     : 0x3ffc1470  \r\nA14     : 0x7f800000  A15     : 0x447a0000  SAR     : 0x0000001f  EXCCAUSE: 0x0000001c  \r\nEXCVADDR: 0x00000004  LBEG    : 0x400014fd  LEND    : 0x4000150d  LCOUNT  : 0xffffffff  \r\n\r\nELF file SHA256: 0000000000000000\r\n\r\nBacktrace: 0x400d17f4:0x3ffb1f80 0x400e582d:0x3ffb1fb0 0x4008627e:0x3ffb1fd0\r\n```\r\n\r\n\r\nThanks", "comments": ["TfLiteRegistration Register_CAST dosn't exist. How i can do without CAST ?\r\n\r\nthanks", "@RJRP44 ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the complete code and the dataset you are using. Thanks!", "I used [Arduino AI tutorial ](https://blog.arduino.cc/2019/10/15/get-started-with-machine-learning-on-arduino/)\r\nMy dataset is a custom one in 2 csv files.\r\n[in.csv](https://github.com/tensorflow/tensorflow/files/6374747/in.csv)\r\n[out.csv](https://github.com/tensorflow/tensorflow/files/6374748/out.csv)\r\n\r\nMy code is horrible i know.\r\nInputs of my AI are normaly all 64 pixels of a AMG8833 (Temperature sensor array) on 20 frames. And in output i need  to get the direction of the movment (like a hand)\r\n\r\n```python \r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nprint(\"TensorFlow version : \", tf.__version__)\r\n\r\nDIRECTIONS = [\r\n    \"in\",\r\n    \"out\",\r\n]\r\n\r\nSAMPLES_PER_DIRECTION = 20\r\n\r\nNUM_DIRECTIONS = len(DIRECTIONS)\r\n\r\nONE_HOT_ENCODED_DIRECTIONS = np.eye(NUM_DIRECTIONS)\r\n\r\ninputs = []\r\noutputs = []\r\n\r\n# read each csv file and push an input and output\r\nfor direction_index in range(NUM_DIRECTIONS):\r\n    direction = DIRECTIONS[direction_index]\r\n    print(f\"Processing index {direction_index} for direction '{direction}'.\")\r\n\r\n    output = ONE_HOT_ENCODED_DIRECTIONS[direction_index]\r\n\r\n    df = pd.read_csv(\"input/\" + direction + \".csv\")\r\n\r\n    # calculate the number of gesture recordings in the file\r\n    num_recordings = int(df.shape[0] / SAMPLES_PER_DIRECTION)\r\n\r\n    print(f\"\\tThere are {num_recordings} recordings of the {direction} direction.\")\r\n\r\n    print(print(df))\r\n\r\n    for i in range(num_recordings):\r\n        tensor = []\r\n        for j in range(SAMPLES_PER_DIRECTION):\r\n            index = i * SAMPLES_PER_DIRECTION + j\r\n\r\n            tensor += [\r\n                (df['p1'][index]),\r\n                (df['p2'][index]),\r\n                (df['p3'][index]),\r\n                (df['p4'][index]),\r\n                (df['p5'][index]),\r\n                (df['p6'][index]),\r\n                (df['p7'][index]),\r\n                (df['p8'][index]),\r\n                (df['p9'][index]),\r\n                (df['p10'][index]),\r\n                (df['p11'][index]),\r\n                (df['p12'][index]),\r\n                (df['p13'][index]),\r\n                (df['p14'][index]),\r\n                (df['p15'][index]),\r\n                (df['p16'][index]),\r\n                (df['p17'][index]),\r\n                (df['p18'][index]),\r\n                (df['p19'][index]),\r\n                (df['p20'][index]),\r\n                (df['p21'][index]),\r\n                (df['p22'][index]),\r\n                (df['p23'][index]),\r\n                (df['p24'][index]),\r\n                (df['p25'][index]),\r\n                (df['p26'][index]),\r\n                (df['p27'][index]),\r\n                (df['p28'][index]),\r\n                (df['p29'][index]),\r\n                (df['p30'][index]),\r\n                (df['p31'][index]),\r\n                (df['p32'][index]),\r\n                (df['p33'][index]),\r\n                (df['p34'][index]),\r\n                (df['p35'][index]),\r\n                (df['p36'][index]),\r\n                (df['p37'][index]),\r\n                (df['p38'][index]),\r\n                (df['p39'][index]),\r\n                (df['p40'][index]),\r\n                (df['p41'][index]),\r\n                (df['p42'][index]),\r\n                (df['p43'][index]),\r\n                (df['p44'][index]),\r\n                (df['p45'][index]),\r\n                (df['p46'][index]),\r\n                (df['p47'][index]),\r\n                (df['p48'][index]),\r\n                (df['p49'][index]),\r\n                (df['p51'][index]),\r\n                (df['p52'][index]),\r\n                (df['p53'][index]),\r\n                (df['p54'][index]),\r\n                (df['p55'][index]),\r\n                (df['p56'][index]),\r\n                (df['p57'][index]),\r\n                (df['p58'][index]),\r\n                (df['p59'][index]),\r\n                (df['p61'][index]),\r\n                (df['p62'][index]),\r\n                (df['p63'][index]),\r\n                (df['p64'][index])\r\n            ]\r\n\r\n        inputs.append(tensor)\r\n        outputs.append(output)\r\n\r\n# convert the list to numpy array\r\ninputs = np.array(inputs)\r\noutputs = np.array(outputs)\r\n\r\nprint(\"Data set parsing and preparation complete.\")\r\n\r\nnum_inputs = len(inputs)\r\nrandomize = np.arange(num_inputs)\r\nnp.random.shuffle(randomize)\r\n\r\n# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\r\ninputs = inputs[randomize]\r\noutputs = outputs[randomize]\r\n\r\nTRAIN_SPLIT = int(0.8 * num_inputs)\r\nTEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\r\n\r\ninputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\r\noutputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\r\n\r\nprint(\"Data set randomization and splitting complete.\")\r\n\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(50, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(15, activation=tf.nn.relu))\r\nmodel.add(tf.keras.layers.Dense(2, activation='softmax'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\nmodel.fit(inputs, outputs, epochs=100)\r\n\r\npredictions = model.predict(inputs_test)\r\n\r\n# print the predictions and the expected ouputs\r\nprint(\"predictions =\\n\", np.round(predictions, decimals=3))\r\nprint(\"actual =\\n\", outputs_test)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]  # , tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\ntfmodel = converter.convert()\r\nopen(\"model.tflite\",\"wb\").write(tfmodel)\r\n```\r\n\r\nI converted my model.tflite to a header file using `xxd -i model.tflite > model_header.cc`\r\nIn my ESP32 code i tried to add CAST ```resolver.AddBuiltin(tflite::BuiltinOperator_CAST ,tflite::ops::micro::Register_CAST);``` but tflite::ops::micro::Register_CAST doesn't exist.\r\n\r\n\r\n```cpp\r\n#include <TensorFlowLite_ESP32.h>\r\n#include <Arduino.h>\r\n\r\n#include \"main_functions.h\"\r\n\r\n#include \"constants.h\"\r\n#include \"output_handler.h\"\r\n#include \"model_data.h\"\r\n#include \"tensorflow/lite/experimental/micro/kernels/micro_ops.h\"\r\n#include \"tensorflow/lite/experimental/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/experimental/micro/micro_interpreter.h\"\r\n#include \"tensorflow/lite/experimental/micro/micro_mutable_op_resolver.h\"\r\n#include \"tensorflow/lite/schema/schema_generated.h\"\r\n#include \"tensorflow/lite/version.h\"\r\n\r\n#include <Melopero_AMG8833.h>\r\n\r\nMelopero_AMG8833 sensor;\r\n\r\nnamespace\r\n{\r\n  tflite::ErrorReporter *error_reporter = nullptr;\r\n  const tflite::Model *model = nullptr;\r\n  tflite::MicroInterpreter *interpreter = nullptr;\r\n  TfLiteTensor *input = nullptr;\r\n  TfLiteTensor *output = nullptr;\r\n  int inference_count = 0;\r\n\r\n  const char *GESTURES[] = {\r\n      \"in\",\r\n      \"out\"};\r\n\r\n#define NUM_GESTURES (sizeof(GESTURES) / sizeof(GESTURES[0]))\r\n\r\n  constexpr int kTensorArenaSize = 90 * 1024;\r\n  uint8_t tensor_arena[kTensorArenaSize];\r\n}\r\n\r\nvoid setup()\r\n{\r\n\r\n  Serial.begin(9600);\r\n\r\n  static tflite::MicroErrorReporter micro_error_reporter;\r\n  error_reporter = &micro_error_reporter;\r\n\r\n  model = tflite::GetModel(model_data);\r\n  if (model->version() != TFLITE_SCHEMA_VERSION)\r\n  {\r\n    error_reporter->Report(\r\n        \"Model provided is schema version %d not equal \"\r\n        \"to supported version %d.\",\r\n        model->version(), TFLITE_SCHEMA_VERSION);\r\n    return;\r\n  }\r\n\r\n  static tflite::MicroMutableOpResolver resolver;\r\n\r\n  // resolver.AddBuiltin(tflite::BuiltinOperator_CAST ,tflite::ops::micro::Register_CAST);\r\n\r\n  resolver.AddBuiltin(tflite::BuiltinOperator_FULLY_CONNECTED, tflite::ops::micro::Register_FULLY_CONNECTED());\r\n\r\n  resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX, tflite::ops::micro::Register_SOFTMAX());\r\n\r\n  static tflite::MicroInterpreter static_interpreter(model, resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n  interpreter = &static_interpreter;\r\n\r\n  TfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n  if (allocate_status != kTfLiteOk)\r\n  {\r\n    error_reporter->Report(\"AllocateTensors() failed\");\r\n    return;\r\n  }\r\n\r\n  input = interpreter->input(0);\r\n  output = interpreter->output(0);\r\n\r\n  inference_count = 0;\r\n\r\n  int statusCode = sensor.resetFlagsAndSettings();\r\n  statusCode = sensor.setFPSMode(FPS_MODE::FPS_10);\r\n}\r\n\r\nconst int numSamples = 20;\r\nint samplesRead = numSamples;\r\n\r\nvoid loop()\r\n{\r\n\r\n  int statusCode = sensor.updateThermistorTemperature();\r\n  statusCode = sensor.updatePixelMatrix();\r\n\r\n  for (int x = 0; x < 8; x++)\r\n  {\r\n    for (int y = 0; y < 8; y++)\r\n    {\r\n      if (sensor.pixelMatrix[y][x] > sensor.thermistorTemperature)\r\n      {\r\n        samplesRead = 0;\r\n      }\r\n    }\r\n  }\r\n\r\n  while (samplesRead < numSamples)\r\n  {\r\n    int statusCode = sensor.updateThermistorTemperature();\r\n    statusCode = sensor.updatePixelMatrix();\r\n    for (int x = 0; x < 8; x++)\r\n    {\r\n      for (int y = 0; y < 8; y++)\r\n      {\r\n        byte pixel = sensor.pixelMatrix[y][x] > sensor.thermistorTemperature;\r\n\r\n        input->data.f[samplesRead * 64 + x * 8 + y] = pixel;\r\n      }\r\n    }\r\n    samplesRead++;\r\n\r\n    if (samplesRead == numSamples)\r\n    {\r\n\r\n      TfLiteStatus invokeStatus = interpreter->Invoke();\r\n      if (invokeStatus != kTfLiteOk)\r\n      {\r\n        Serial.println(\"Invoke failed!\");\r\n        while (1)\r\n          ;\r\n        return;\r\n      }\r\n\r\n      for (int i = 0; i < NUM_GESTURES; i++)\r\n      {\r\n        Serial.print(GESTURES[i]);\r\n        Serial.print(\": \");\r\n        Serial.println(output->data.f[i], 6);\r\n      }\r\n      Serial.println();\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThanks\r\n(sorry my english is bad i am french)"]}]