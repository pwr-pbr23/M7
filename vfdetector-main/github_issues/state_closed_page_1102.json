[{"number": 20202, "title": "Fix doc discrepancy in tf.scatter_add", "body": "This fix fixes doc discrepancy in tf.scatter_add.\r\n\r\nThis fix fixes #20200\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20200, "title": "Documentation and code discrepancy in tf.scatter_add", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nThe code for tf.scatter_add has argument use_locking=False, but the documentation says:\r\n\r\n> use_locking: An optional bool. Defaults to **True**. If True, the assignment will be protected by a lock; otherwise the behavior is undefined, but may exhibit less contention.\r\n\r\n", "comments": ["Added PR #20202 for the fix."]}, {"number": 20199, "title": "No float 16 support for NonMaxSuppressionV2", "body": "Hi , \r\n\r\nWhile running the object detector in keras and tensorflow in float 16 mode,  \r\n\r\nI get the following error -\r\n\r\nTypeError: Input 'boxes' of 'NonMaxSuppressionV2' Op has type float16 that does not match expected type of float32.\r\n\r\nIs there a way to add float 16 support for this operation here -\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9d2abd2ace95e6e352ba1292cc38c77b7bd1adc7/tensorflow/core/ops/image_ops.cc#L653\r\n\r\nThanks  :) \r\n\r\n\r\n\r\nHave I written custom code\r\nOS Platform and Distribution --- Ubuntu 16.04\r\nTensorFlow installed from --- pip \r\nTensorFlow version --- 1.8.0\r\nBazel version --- bazel release 0.13.1\r\nCUDA/cuDNN version ---  9.2, V9.2.88\r\nGPU model and memory - Titan X \r\nExact command to reproduce --- training keras retinanet but NonMaximumSupression doesnt support float 16.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 33 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@angersson If the decision makers agree with this feature, may I submit a PR to enable NonMaxSuppressionV2 Op to support float16/half? ", "The feature does not make sense for the moment because the op is currently only available on CPU and float16 is not useful on CPU.", "@ppwwyyxx Thanks for your comments. I agree that float16 will not make a difference for computing performance. The issue reported here is caused by that the current API does not support the float16 data type yet, but some use cases may need to handle float16 data. Considering this, do you think it will be useful for users? What do you think of this? @angersson @raghavgurbaxani   "]}, {"number": 20198, "title": "Branch 201561361", "body": "", "comments": ["@chsigg Could you help take a look at the depthwise_conv_test failure: https://source.cloud.google.com/results/invocations/5cbea7f9-6c1c-4989-b41e-cfba94835416/targets? Thanks!", "@chsigg, never mind, it passed.  The previous failure might be just flaky."]}, {"number": 20197, "title": "Untrunctaed normal distribution in VarianceScalingInitializer", "body": "Fixes #19996 \r\n\r\nRELNOTES: VarianceScalingInitializer now takes \"truncated_normal\" or \"untruncated_normal\" as values for its distribution argument. Using \"normal\" is equivalent to \"truncated_normal\" (the current behavior), but its use is deprecated. \r\n\r\n@martinwicke ", "comments": ["It turns out that `assert_called()` is only available in Py3.6+", "Now the failures are: \r\n`//tensorflow/tools/api/tests:api_compatibility_test`. I'm not sure how that works or if I should do anything to make it pass. This PR does change the API.\r\n`//tensorflow/core:common_runtime_ring_reducer_test`: that's probably unrelated to this PR.", "The api_compatibility test log has instructions on how to update the api goldens that it compares against. Please run that, you'll have to do that on MacOS or Linux, and I believe in Py2. \r\n\r\n@gunan is this still true? Do we have instructions for this somewhere (maybe we should).", "Thank for the quick PR!", "Sorry, I have not gotten around to making the test work on python 3, so it still  only works on python 2, on linux and macos. The test prints its instructions, and all instructions should already exist in the folder.", "Apart from rebuilding with py2, another painful point is that the linter script will always lint all thousands of python sources, despite the \"--incremental\" option (which is explicitly not used by the linter)."]}, {"number": 20196, "title": "[INTEL_MKL] Check for relative error when using MKL convolution kernels", "body": "", "comments": []}, {"number": 20195, "title": "Move external/ directory in pip package.", "body": "Moving external/ directory in the pip packages (which is currently\r\ninstalled directly into site-packages directory). Moving the\r\ndirectory to tensorflow/include/external/. Also, removing all\r\npython files from external (since it should really only contain\r\nheaders and license files.)", "comments": ["Cherry-picking this fix from master", "Some weird CUDA errors. Dont think they are related to this CL. Going to retrigger the GPU Python3 test."]}, {"number": 20194, "title": "Assigning new values to a protocol buffer frozen model in TensorFlow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: bazel release 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8\r\n- **CUDA/cuDNN version**: 8/7\r\n- **GPU model and memory**: NVIDIA-1060\r\n- **Exact command to reproduce**:\r\n\r\nI am able to assign new values to a tensor by using `tf.assign` and `tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)` when I restore a pretrained model:\r\n\r\n```\r\n >>>sess.run(tf.assign([v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name == 'LAYER_NAME'][0], New_Value_FOR_LAYER_NAME))`\r\n```\r\n\r\nHowever, in the case of a frozen model (`.pb`) file, the equivalent using `tf.assign` and `tf.get_default_graph().get_operations()` does not yeild any results:\r\n\r\n```\r\n>>>sess.run(tf.assign([v for v in tf.get_default_graph().get_operations() if v.name == LAYER_NAME'][0], New_Value_FOR_LAYER_NAME)). \r\n```\r\nI guess the underlying reason is that pretrained models have [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor) and frozen models have [tf.Operation](https://www.tensorflow.org/api_docs/python/tf/Operation):\r\n\r\n\r\n\r\n```\r\n>>>[v for v in tf.get_default_graph().get_operations() if v.name == 'softmax/weights_quint8_const']\r\n    [<tf.Operation 'softmax/weights_quint8_const' type=Const>]\r\n```\r\n\r\nThen, what is the correct way of assigning new values to a tf.Operation in a frozen model. \r\n", "comments": ["no answers on this?", "Nagging Assignee @jart: It has been 77 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@amirjamez sorry for the delay.", "For this it's better to modify the graphdef directly.  you'd export to a graph_def, modify that, then load it back in (or save it to file) depending on how you're going to use it\r\n\r\n```\r\ngd = tf.get_default_graph().as_graph_def()\r\nconst_nodes = [x for x in gd.node if x.op == 'Const']\r\nvalue = const_nodes[0].attr['value'].tensor\r\n# assuming an int32 vector\r\nvalue.tensor_shape.dim[0].size = 3\r\nvalue.tensor_content = np.array([1, 2, 3], dtype=np.int32).tostring()\r\n# load gd back into a graph or store to disk\r\n```\r\n\r\nthis will work for most data types, i think, including float and integer.  however it will break for string types.  presumably you're not freezing strings in graphs.\r\n"]}, {"number": 20193, "title": "add alternative relative paths for cupti and nvvm", "body": "On Ubuntu 18.04, _libcupti-dev_ and _nvidia-cuda-toolkit_ both install under /usr (instead of Nvidia's default /usr/local/cuda). This fixes the ensuing bazel configuration errors when building for CUDA:\r\n```Cuda Configuration Error: Cannot find cupti.h under /usr```\r\nand\r\n```Cuda Configuration Error: Cannot find libdevice.10.bc under /usr```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Thanks for the PR. Can you sign the CLA if you haven't sign it before? Thanks.", "Sorry, but no. That seemingly requires a Google account, which requires giving a phone number. (I would, otherwise.)", "Sorry to hear about this. I am closing this PR for now, feel free to reopen it if you changed your mind. Thanks."]}, {"number": 20192, "title": "tensorflow lite with ndk 17 compilatoin failed", "body": "Hi ,  I took latest tensorflow sources from github and try to build tensorflow lite.\r\nThere was error compilation.\r\n\r\nWe upgraded our NDK version from 16 to 17 couple weeks ago and tensorflow lite compiled **without any error**. But latest sources from today failed to compile.\r\n\r\n\r\n\r\n```\r\nINFO: Found 1 target...\r\nERROR: XXX/tensorflow/contrib/lite/kernels/BUILD:57:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:gemm_support' failed (Exit 1)\r\nIn file included from tensorflow/contrib/lite/kernels/gemm_support.cc:15:\r\nIn file included from ./tensorflow/contrib/lite/kernels/gemm_support.h:18:\r\nIn file included from external/gemmlowp/public/gemmlowp.h:19:\r\nIn file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:20:\r\nIn file included from external/gemmlowp/public/../internal/../internal/kernel_default.h:22:\r\nIn file included from external/gemmlowp/public/../internal/common.h:26:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/cmath:305:\r\nIn file included from external/androidndk/ndk/sources/android/support/include/math.h:32:\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1302:93: error: no member named 'log2f' in the global namespace\r\ninline _LIBCPP_INLINE_VISIBILITY float       log2(float __lcpp_x) _NOEXCEPT       {return ::log2f(__lcpp_x);}\r\n                                                                                          ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1303:93: error: no member named 'log2l' in the global namespace\r\ninline _LIBCPP_INLINE_VISIBILITY long double log2(long double __lcpp_x) _NOEXCEPT {return ::log2l(__lcpp_x);}\r\n                                                                                          ~~^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1308:38: error: call to 'log2' is ambiguous\r\nlog2(_A1 __lcpp_x) _NOEXCEPT {return ::log2((double)__lcpp_x);}\r\n                                     ^~~~~~\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1302:46: note: candidate function\r\ninline _LIBCPP_INLINE_VISIBILITY float       log2(float __lcpp_x) _NOEXCEPT       {return ::log2f(__lcpp_x);}\r\n                                             ^\r\nexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/math.h:1303:46: note: candidate function\r\ninline _LIBCPP_INLINE_VISIBILITY long double log2(long double __lcpp_x) _NOEXCEPT {return ::log2l(__lcpp_x);}\r\n                                             ^\r\n3 errors generated.\r\nTarget //tensorflow/contrib/lite:libtensorflowLite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 60.727s, Critical Path: 13.01s\r\nINFO: 19 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu\r\n- **TensorFlow installed from (source or binary)**:\r\nsources \r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.14\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNDK 17\r\n- **CUDA/cuDNN version**:\r\nNo\r\n- **GPU model and memory**:\r\nNo\r\n- **Exact command to reproduce**:\r\nbazel build //tensorflow/contrib/lite:framework --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++14\"\r\n\r\nThanks for help\r\n", "comments": ["I met the same issue. I always using NDK 17 without any error, but today pull the latest source, the issue happen.\r\n[update]: the issue happen on android_arm64-v8a.  (e.g. bazel build --cxxopt='--std=c++11' -c opt --config=android_arm64-v8a tensorflow/tools/benchmark:benchmark_model)\r\nThere's no any issue on android_armeabi-v7a. (e.g.  bazel build --cxxopt='--std=c++11' -c opt --config=android_armeabi-v7a tensorflow/tools/benchmark:benchmark_model)", "I met same issue while building Android Demo under $TF_Root/tensorflow/example/android. NDK 17 installed.\r\nbuild command: bazel build -c opt tensorflow/examples/android:tensorflow_demo", "Looks like this is due to Android API level, which causes this error. With recent versions of NDK these errors happen if one is compiling with NDK API level below < 21. \r\n\r\nI can fix the error if I change tensorflow/.tf_configure.bazelrc to\r\n\r\nbuild --action_env ANDROID_NDK_API_LEVEL=21\r\n\r\nIt also seems that NDK API level parsing logic is incorrect in configure.py:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure.py#L674\r\n\r\nRight now it parses the NDK version in sources.properties.\r\nInstead it should get the API level lists from:\r\nusing something like:\r\nls $ANDROID_NDK_PATH/platforms | cut -d '-' -f 2 | sort -n | tail -1\r\n@case540 , @angersson  this logic needs to be updated.\r\n\r\n@dimitryn , @kismeter : To get you unblocked, can you try to update ANDROID_NDK_API_LEVEL in .tf_configure.bazelrc to set to 21.\r\nIn the mean time, we fix this issue correctly.\r\n\r\n\r\n", "thanks", "@sunshinemyson Hi  1. can you specify exactly how can I keep working with NDK 17 until you fix it ?! Details you wrote a little confusing ( which way to set NDK_API_LEVEL = 21)\r\n2. When do you expect to commit the fix?\r\nThanks.", "No, just like @shashishekhar said, you need to add one line in tensorflow/.tf_configure.bazelrc:\r\nbuild --action_env ANDROID_NDK_API_LEVEL=21\r\nif that doesn't work, you need to change your workspace file which in tensorflow/WORKSPACE, and modify your ndk repository settings with:\r\n\r\nandroid_ndk_repository(\r\n    name = \"androidndk\",\r\n    path = \"/Users/kyoushouu/Library/Android/sdk/ndk-bundle\",\r\n    api_level = 21,\r\n)\r\n\r\nand rebuild tensorflow lite will be best solution for this issue."]}, {"number": 20191, "title": "`tf.contrib.data.sliding_window_batch` cannot slide further than the window size", "body": "### System information\r\n- **Have I written custom code: yes**:\r\n- **OS Platform and Distribution: macOS High Sierra 10.13.4**:\r\n- **TensorFlow installed from: binary**:\r\n- **TensorFlow version: 1.8.0**:\r\n- **Python version: 3.6.5**: \r\n- **CUDA/cuDNN version, GPU info: N/A; not using GPU support**:\r\n- **Exact command to reproduce**:\r\n\r\n### Problem\r\n`tf.contrib.data.sliding_window_batch` requires that the `stride` argument lie in the range `[1, window_size)`. This limits potential uses for this function. For example; consider the following scenario where the model is fed an element and must predict the previous and next element. Here is the description of the task more precisely:\r\n\r\nWe have data elements `(e0, e1, e2, ...)`. The model has to learn to predict `e_{i-1}, e_{i+1}` given `e_i`. To give the model triples of batches from the original sequence `(e0, e1, e2, ...)`, we could use `tf.contrib.data.sliding_window_batch` as follows, assuming `dataset` yields `(e0, e1, e_2, ...)`:\r\n\r\n```python\r\ndataset = dataset.apply(window_size=batch_size, stride=1)\r\n# yields ([e_0, ..., e_{b-1}], [e_1, ..., e_b], [e_2, ..., e_{b+1}], ...)\r\n\r\ndataset = dataset.apply(window_size=3, stride=batch_size)\r\n# yields:\r\n# [[e_0, ..., e_{b-1}],     [[  e_b  , ..., e_{2b-1}],\r\n#  [e_1, ...,   e_b  ],  ;   [e_{b+1}, ...,   e_2b  ],  ;  ...\r\n#  [e_2, ..., e_{b+1}]]      [e_{b+2}, ..., e_{2b+1}]]\r\n```\r\n\r\nNotice how the training inputs would be the middle row, and if the stride size were smaller, we would feed the same input-labels triples to the model twice. So the stride size in this case needs to be larger than the window size, but the existing implementation does not allow that.\r\n\r\n### Source code / logs\r\nAs soon as one attempts to run the iterator within a session, one gets an error log with this relevant line:\r\n```\r\n...\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Stride must be in [1, window_size).\r\n```\r\n", "comments": ["Seems reasonable to allow this. IIRC this op was a contribution from @facaiy, and we'd welcome contributions to improve it and fix this bug.", "Thanks for ping me, @mrry.\r\n\r\nThe restriction is designed for safety at first, since it means lost data if `stride > batch_size`. I think if people really need it, they should filter their data explicitly and / or apply `batch`, instead . \r\n\r\nTake the case of @danielwatson6 , I think there are at least two solutions:\r\n\r\n1. use filter:\r\n```python\r\ndataset.apply(sliding_window_batch(window_size=batch_size, stride=1))\r\n        .filter()   # drop the data you don't need\r\n        .batch(3)\r\n```\r\n\r\nor\r\n\r\n```python\r\ndataset.apply(sliding_window_batch(window_size=batch_size, stride=1))\r\n        .batch(batch_size)\r\n        .map(lambda x: x[0:3])  # drop the data you don't need\r\n```\r\n\r\n\r\n2. don't use filter\r\n```python\r\ndataset.apply(sliding_window_batch(window_size=3, stride=1))\r\n        .batch(batch_size)\r\n        .map(lambda x: tf.transpose(x))\r\n```\r\n\r\nIn my experience, `batch` is really what I need in most cases (`stride > batch_size`) . \r\n", "@facaiy Thanks for the reply! Do you think it would be possible to set it as a warning instead? It's a more direct solution compared to passing lambdas. Surely the user understands that they are skipping items in the generator if the stride size exceeds the window size.", "@danielwatson6 Hi, what do you think about the second solution I wrote above? I mean:\r\n\r\n```python\r\ndataset.apply(sliding_window_batch(window_size=3, stride=1))\r\n        .batch(batch_size)\r\n        .map(lambda x: tf.transpose(x))\r\n```\r\n\r\nI would prefer to improve codes only if it's necessary, however I agree with @mrry that contributions (to loosen the restriction) are welcome.", "I create PR #20223 to solve the issue. Could you take a look? Thanks. cc @mrry."]}, {"number": 20190, "title": "INTEL-MKL: MKL primitive reuse for reorder op", "body": "Enable MKL primitive reuse for reorder op to improve performance of\r\n(1) model training and\r\n(2) inference of small batch size", "comments": ["Nudge for review.", "cc: @yifeif "]}, {"number": 20189, "title": "Install h5py on ppc64le required for keras_app...", "body": "Recently pip2/3 install keras_applications==1.0.2 was added to\r\ninstall_pip_packages.sh. This pulls in the dependency h5py which fails\r\nto compile from source on ppc64le. (On x86 h5py is pre-compiled so\r\nit installs just fine). To get h5py to install we need to install\r\nthe libhdf5-dev deb package, and symlink the shared libraries to the names\r\nh5py is expecting. Finally we need to include the path to the hdf5.h\r\nheader file.", "comments": []}, {"number": 20188, "title": "there is no idct implementation in tensorflow", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'v1.8.0-0-g93bc2e2072' 1.8.0\r\n- **Python version**:  3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  9.0\r\n- **GPU model and memory**: GTX 780 6G\r\n- **Exact command to reproduce**: none\r\n\r\n### Describe the problem\r\nWe have tf.spectral.dct but no tf.spectral.idct.\r\nWhat's wrong?\r\n\r\n### Source code / logs\r\nNone", "comments": ["Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I can try to work on this and submitted a PR later.", "And I found that the idct is already in master. You can access the file through the link here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/spectral_ops.py#L267\r\nWhat we need to do is to just push this commit into next release version. BTW please closes the issue @aselle  Thanks!", "Close the issue as it is already supported."]}, {"number": 20187, "title": "SetNumThreads() with float models has no observable effect in TFLite", "body": "### Describe the problem\r\nWhen running (a slightly tweaked version of) the example code in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/apis.md, I noticed that no matter what `interpreter->SetNumThreads()` is called with, it __only__ creates (and uses) 4 threads. The only exception is when I set it to 1, in which case the main thread is used. CPU usage is never at or near 100%.\r\n\r\nHere's the code I'm running:\r\n```c++\r\n#include <tensorflow/contrib/lite/kernels/register.h>\r\n#include <tensorflow/contrib/lite/model.h>\r\n#include <memory>\r\n\r\nint main()\r\n{\r\n\tstd::unique_ptr<tflite::FlatBufferModel> model =\r\n\t\ttflite::FlatBufferModel::BuildFromFile(\"model.tflite\");\r\n\r\n\ttflite::ops::builtin::BuiltinOpResolver resolver;\r\n\tstd::unique_ptr<tflite::Interpreter> interpreter;\r\n\ttflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\n\tinterpreter->SetNumThreads(8);\r\n\tinterpreter->AllocateTensors();\r\n\r\n\twhile (true)\r\n\t\tinterpreter->Invoke();\r\n}\r\n```\r\n\r\nTensorflow Lite is built with `make -f tensorflow/contrib/lite/Makefile`, from branch r1.9.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 8.1.1\r\n- **CUDA/cuDNN version**: CPU only\r\n- **GPU model and memory**: CPU only\r\n- **Exact command to reproduce**: (Build the C++ example in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/apis.md)", "comments": ["Are you using floating point number model? If yes, that's expected behavior.", "> Are you using floating point number model?\r\n\r\nYes.\r\n\r\n> If yes, that's expected behavior.\r\n\r\nGood to know. However, I cannot find any docs or think of why there's a limitation on floating point numbers. Could you point out the technical reason behind this?\r\n\r\nThanks!", "The reason is that the number of threads is hard-coded in the source code, see [1]. In case you don't know it, TF Lite relies on [Eigen](http://eigen.tuxfamily.org/) and [gemmlowp](https://github.com/google/gemmlowp) for floating point and integer convolutions respectively. \r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/lite/kernels/internal/optimized/multithreaded_conv.h#L58\r\n", "[multithreaded_conv.h#L55](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/lite/kernels/internal/optimized/multithreaded_conv.h#L55) says:\r\n> since the underlying resource of CPU cores should be consumed by the operations anyway, it shouldn't affect overall performance.\r\n\r\nIs this understanding correct:\r\n- There's little performance gain in increasing the number of threads for a __single invocation__, i.e. by changing `thread_count` at [L58](https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/lite/kernels/internal/optimized/multithreaded_conv.h#L58).\r\n- There is, however, some performance gain if __several invocations__ are run in parallel (e.g. by using one thread for each).\r\n\r\n---------\r\n\r\nHypothetically, apart from the implementation difference between Eigen and gemmlowp, what differs floating point and integer so much that we can't (or shouldn't) even run floating point convolutions from more than *4* threads? IMO not all models are suitable for quantization.\r\n\r\nThank you!", "@sfanxiang Whether increasing the number of threads can get performance gain is actually system and workload dependent. E.g., for large amount of convolution on modern x86 Xeon platform, number of threads > 4 could be useful. I guess 4 is there just as a convenient hack. On most modern cell phones, 4 should work pretty well, except some big.LITTLE systems, such as one with 2xbig + 4xLITTE.\r\n", "@sfanxiang set number of threads should work for after 2bfc7957dad"]}, {"number": 20186, "title": "Update flatbuffers to 1.9.0", "body": "This fix updates flatbuffers to 1.9.0. The previous version used (`971a681`) in tf was released last year, and is not a versioned release. This fix updates to the latest versioned release of 1.9.0.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 20185, "title": "Update re2 to versioned release 2018-04-01", "body": "This fix updates re2 to the most versioned release of 2018-04-01.\r\n(The version used in tf before was released last year and is not a versioned release.)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@gunan Thanks for the review. The PR has been updated with bazel mirror."]}, {"number": 20184, "title": "Update lmdb to 0.9.22", "body": "This fix updates lmdb from 0.9.19 to 0.9.22. The old version (0.9.19) was released in 2016, which is quite old.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20183, "title": "[XLA] Add in a plugin tf2xla dependency for backend registration", "body": "tf2xla backends are slightly separate to the XlaDevices and platform/compiler framework.\r\n\r\nThis change allows a tf2xla backend to be registered separately to the rest of the framework, which enables targets that bind to tf2xla and not the core of XLA to work. specifically this will allow this target to work with plugin extensions:\r\n\r\ntensorflow/compiler/tf2xla:tf2xla_supported_ops", "comments": ["Nudge for review.", "I'm pretty certain that the bazel nobuild failure is not to do with my change (even though my change is in a bazel file)\r\n", "@qlzh727 Hi.  I wonder if you could re-run the tests for this one, so we can see if the change is responsible for the test failure.\r\n\r\nThanks\r\n\r\n", "Sure. Also nudge reviewer @tatatodd for review.", "@qlzh727 Cheers.  That seems ok now.  Not sure what this import/copybara thing is.  Seems to have the same issue for all of them.", "import/copybara is the new sync process we introduced this week. Reviewer need to first LGTM the PR,  before I can move it to copybara step.", "Adding @jlebar in case Todd is OOO.", "If I understand this correctly, the idea is that in your fork of XLA, you modify :plugin and :plugin_backend to add dependencies on your new plugins.  Then these get pulled in as dependencies of the relevant XLA libraries.\r\n\r\nIs that right?\r\n\r\nBut if you're already modifying XLA's build files, to add new deps to :plugin and :plugin_backend, what's the problem created by instead modifying the rules that depend on :plugin and :plugin_backend to depend on your new library?  That is, what does this layer of indirection buy us?\r\n\r\nWe already do the direct-dependencies approach for cpu/gpu, and I'm afraid that the indirect-dependency approach here will be fragile, because by its nature, it's completely untested.\r\n\r\nI'm also not sure what :plugin is used for; it seems to be a dead library.", "Hi.  You are correct.\r\n\r\nOriginally you guys asked me to have a dynamically loadable plugin (hence the name).  That didn't work for various reasons, the main one being that the protobuf library and LLVM can't cope with being loaded into the same process twice.  Hence the building the poplar backend directly with the TF/XLA code.\r\n\r\nI was then asked to try to keep our code independent, to make future merging easy.  Hence the single file which sits somewhat away from the main XLA code base.\r\n\r\nIn an ideal world we would upstream the code, although I do understand why you guys are reluctant to have that happen at the moment.\r\n\r\nI will abandon this change and just bind our stuff into the dependency tree as if it is a similar thing to the CPU or GPU.  I like that, as it brings our driver into closer alignment with XLA as a whole.\r\n\r\nThanks for the review\r\n\r\n"]}, {"number": 20182, "title": "Update jsoncpp to 1.8.4", "body": "This fix updates the jsoncpp to 1.8.4 to address the issue raised in #20170. The jsoncpp used in tf was old and may contain security issues.\r\n\r\nThis fix fixes #20170.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20181, "title": "Update curl library to curl-7.60.0", "body": "This fix updates curl library to 7.60.0. (Previously TensorFlow links to curl 7.49.1, which was\r\nrelesed in 2016)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20180, "title": "Fix a typo", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20179, "title": "[XLA] Add a target so XLA plugins can register tf2xla backends separately", "body": "tf2xla backends are slightly separate to the XlaDevices and platform/compiler framework.\r\n\r\nThis change allows a tf2xla backend to be registered separately to the rest of the framework, which enables targets that bind to tf2xla and not the core of XLA to work.  specifically this will allow this target to work with plugin extensions:\r\n\r\n`tensorflow/compiler/tf2xla:tf2xla_supported_ops`\r\n", "comments": ["will squash the commits...."]}, {"number": 20178, "title": "SignalsHelper python3 compatibility", "body": "Replace dict.iterkeys() with six.iterkeys(dict).", "comments": ["There is an internal change also trying to fix the same issue, and is the progress of merging. See https://github.com/tensorflow/tensorflow/pull/20156/commits/91d98f5403145ad5899ecdaa8a6564da9bd111c9. I will park this PR for the moment and close it once the  internal change is merged.", "Great and thanks.", "The internal change has been merged, closing this PR now."]}, {"number": 20177, "title": "Tensorflow won't compile on windows c++", "body": "when I try to compile tensorflow master on windows10 with vs2015, I got below errors after compiling for half an hour.\r\n\r\n\"D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\ALL_BUILD.vcxproj\" (default target) (1) ->\r\n\"D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\array_ops_gen_cc.vcxproj\" (default target) (2) ->\r\n\"D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_array_ops.vcxproj\" (default target) (3) ->\r\n\"D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default target) (4) ->\r\n(CustomBuild target) -> \r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 3. [D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj]\r\n\r\n\r\n\"D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\ALL_BUILD.vcxproj\" (default target) (1) ->\r\n\"D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\force_rebuild_target.vcxproj\" (default target) (81) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 3. [D:\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\force_rebuild_target.vcxproj]\r\n\r\n    148 Warning(s)\r\n    2 Error(s)\r\n\r\nAttached please find the full log\r\nThanks for your help!\r\n[msbuild.log](https://github.com/tensorflow/tensorflow/files/2122908/msbuild.log\r\n", "comments": ["Got the same errors. As far as I see, the recent TF release is not compatible with the latest Bazel AND the source code for Protobuf is out of date.", "Later I change to compile tensorflow r1.3 with cuda 8.0, cudnn 6 (previously I use cuda 9 with cudnn 7) according to https://www.zybuluo.com/kalluwa/note/1110511.\r\nAnd it successfully compiled!!!\r\n", "we just updated the protobuf version at head. So proto issues should be gone.\r\nfor bazel issues, @meteorcloudy.\r\n\r\nTo debug your problem better, would you mind filling the issue template?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20176, "title": "Mobilenet V2 SSDLite inference test with Android", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS Sierra version 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\nfrom source\r\n- **TensorFlow version (use command below)**:\r\n1.8.0 to train and 1.9.0 to build the inference library\r\n- **Python version**: \r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC 4.2.1\r\n- **CUDA/cuDNN version**:\r\nusing CPU only\r\n- **GPU model and memory**:\r\nusing CPU only\r\n- **Exact command to reproduce**:\r\ncommand to create the appropriate Ops for the Mobilenet V2 SSDLlite (since the default Ops are not enough to make it work on Android) :\r\n```\r\n bazel build tensorflow/python/tools:print_selective_registration_header && \\\r\n bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n  --graphs=path/to/graph.pb > ops_to_register.h \r\n```\r\n\r\ncommand to build `tensorflow_inference.so` (I build it for all possible CPU types): \r\n```\r\n      bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\r\n    --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\r\n    //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n    --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a \r\n```\r\ncommand to optimize the graph for inference : \r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=$DETECT_PB \\\r\n--out_graph=$STRIPPED_PB \\\r\n--inputs='image_tensor' \\\r\n--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,300,300,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms'\r\n```\r\n### Describe the problem\r\nI'm working with the Object Detection Demo for android and I'm using my custom trained model (used Mobilenet V1 SSD for the training part) and everything works as expected. As the Tensorflow team released the Mobilenet V2 SSDLite (supposed to be 35% faster than the V1 SSD version) I wanted to give it a try and retrained my model. I optimized the graph for inference and tested it but I was disappointed with the results. \r\n\r\n**Test device Pixel 2 :** \r\n\r\nMobilenet V2 SSDLite\r\n- nodes observed : 1185\r\n- inference time : around 1200ms  \r\n\r\nMobilenet V1 SSD \r\n- nodes observed : 893\r\n- inference time : around 300ms\r\n\r\nInstead of adding the Tensorflow dependency in my Gradle I'm using the latest stable Nightly build for both test use cases but when testing with the V2 model I replace the `libtensorflow_inference.so` with the manually built one.\r\n\r\nany idea why I'm getting these results ?\r\n", "comments": ["Following as I am also facing the same issue.\r\n", "Please provide the frozen graphs or checkpoints you are using in each case to repro ", "@achowdhery i trained the same model with these checkpoints : \r\n\r\n`ssd_mobilenet_v1_coco_2017_11_17`\r\n\r\n`ssdlite_mobilenet_v2_coco_2018_05_09`\r\n\r\nI downloaded both of them from [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) `ssd_mobilenet_v1_coco `and `ssdlite_mobilenet_v2_coco`\r\n\r\nbut the `ssd_mobilenet_v1` is now updated, when you download it you will get this file : `ssd_mobilenet_v1_coco_2018_01_28` but i think you can use it because it won't affect the problem I mentioned since it occurs with the `ssdlite` model.\r\n\r\n Final question : \r\nDoes this model works better with `TFLite` ? since I'm testing with `TF mobile`.\r\n", "We expect smaller latency values with TFLite if you follow the blogpost instructions (~15fps):\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n\r\nYou may use any of the mobilenet ssd checkpoints with the instructions above.\r\n", "Hi guys.\r\nCan anyone say what should I change android demo for ssdlite mobilenetv2?\r\nPlease help.I need it", "@Davari393   Hey, did you find out about the changes? I also need it\r\nThank you"]}, {"number": 20175, "title": "Where should I learn C++ API\uff1f", "body": "Hello, where should I learn C++ API\uff0c Where is the TensorFlow C++ detailed manual ? How Could Learn Train Code. There are a lot python code , but Few C++ code for reference or learn.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce"]}, {"number": 20174, "title": "Checking of layer objects in seq2seq modules", "body": "Currently the modules in seq2seq checks the layers as an instance of tensorflow.python.layers.base.Layer. However, the docstrings says that it is a legacy and that one must inherit the keras layers directly (The tensorflow base layer also inherits this keras layers). I might it might be better if it checks for the keras one instead of the tensorflow one.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: N/A\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: master branch \r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "It seems like I was checking the master branch instead of the 1.8 branch that I am currently using. I'm closing the issue."]}, {"number": 20173, "title": "Importing error with tensorflow1.8.0 and protobuf 3.6.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary pip\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0 / 7.0\r\n- **GPU model and memory**: GeForce GTX 1080Ti 11GB\r\n- **Exact command to reproduce**: import tensorflow as tf\r\n\r\n\r\n### Problem description\r\nImporting tensorflow gives me following error. This error is only reproducible in a file where a computational graph is defined.\r\n```\r\nKeyError: \"Couldn't find field google.protobuf.FileOptions.php_metadata_namespace\"\r\n```\r\n\r\n### Solution\r\nI already found a solution to this problem. It is related to the pip package protobuf 3.6.0. I solved this problem by issuing following commands:\r\n```\r\npip3 uninstall protobuf\r\npip3 install protobuf==3.5.2\r\n```", "comments": ["This is due to backwards incompatible changes in protobuf 3.6\r\nThis should be largely triaged in 1.9.", "Worked in 1.9.0, thanks!", "thanks", "I have done the same think, uninstall and install, but in my case with the latest version, and it worked for me."]}, {"number": 20172, "title": "Update Keras datasets file handling", "body": "using `with` to be consistent with other code in datasets\r\n\r\nmight also be useful in case file is not present due to corruption of something else", "comments": ["@fchollet, I think it worthwhile to clean up the other dataset file too. Using \"with\" context is always recommended over manual closing.", "Hey @qlzh727 I will do that. I can clean up other datasets as well. ", "Thanks. Can u also fix reuters.py?", "Ah! I missed that. Fixed now. Thanks for catching it."]}]