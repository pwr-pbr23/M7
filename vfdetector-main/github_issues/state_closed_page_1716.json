[{"number": 1420, "title": "Support for double precision complex numbers", "body": "I'd like to use TensorFlow to write an application that would need `complex128` as a dtype.\nBut TensorFlow currently only supports `complex64`.\nAre there any plans for supporting `complex128` as well in the future?\n\nSeems that there is a TODO for this in  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.proto#L36.\n", "comments": ["Yes, `complex128` would be good to have, but I don't think anyone's working on it at the moment.\n", "If someone wants to add it, I'm happy to answer questions.  It should mostly involve duplicating any mention of `complex64` into `complex64` and `complex128`.\n", "When adding this to the [`types.proto`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.proto), should this be added to the enum as `DT_COMPLEX128 = 18;` to maintain backwards-compatibility?\n", "@ibab: Yep, it'd be `DT_COMPLEX128 =  18` and `DT_COMPLEX128_REF = 118`.\n"]}, {"number": 1419, "title": "Dependency required for user Reader ops?", "body": "When creating a Reader op inside core/user_ops that inherits from ReaderBase, Bazel responds with \n\n```\ntensorflow/core/BUILD:325:1: undeclared inclusion(s) in rule '//tensorflow/core:user_ops_op_lib':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/user_ops/user_reader_op.cc':\n  'bazel-out/local_darwin-opt/genfiles/tensorflow/core/kernels/reader_base.pb.h'.\n```\n\nIs this because the user_ops_op_lib depends only on framework? user_reader_op.cc includes reader_base.h which includes reader_base.pb.h. What dependency should I add to make this work? I'm still figuring out Bazel atm. \n\nThis is with latest source code. \n", "comments": ["I was able to resolve this by changing tensorflow/core/BUILD as follows at line 325:\n\n```\ncc_library(\n    name = \"user_ops_op_lib\",\n    srcs = glob([\"user_ops/**/*.cc\"]),\n    copts = tf_copts(),\n    linkstatic = 1,\n    visibility = [\"//visibility:public\"],\n    deps = [\":framework\", \"//tensorflow/core/kernels:reader_base\"],\n    alwayslink = 1,\n)\n\n```\n\nWorks for now but a broader dependency might be better?\n", "@keveman: What's the situation with user_ops these days?  Was it going through some flux? \n", "Not that I know of. There is #1378 for which I'll have a fix soon.\n\nOn Mon, Mar 7, 2016 at 9:04 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @keveman https://github.com/keveman: What's the situation with user_ops\n> these days? Was it going through some flux?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1419#issuecomment-193345938\n> .\n", "@sbyma Can you please verify and close this issue?\n", "I'm getting a very similar error that I can't get around. If I add //tensorflow/core/kernels:reader_base to the Bazel dependencies, though, I get this error:\r\n\r\ntensorflow/core/kernels:reader_base cannot depend on tensorflow/core:framework.\r\n\r\nIt seems both these builds have the other listed in their disallow_deps attribute. I can't find a workaround.", "I had the same issue recently when trying to add dependency to `tensorflow/core:core` from my custom op -- https://github.com/tensorflow/tensorflow/issues/7030#issuecomment-274882446\r\n\r\nI used bazel query to make sure `core` doesn't depend on framework, so I wasn't sure what's going on\r\n```\r\nbazel query \"somepath(//tensorflow/core:core, //tensorflow/core:framework)\"\r\n(nothing)\r\n```\r\n\r\nMy work-around is to use `g++` to compile the op instead of Bazel\r\n\r\n@keveman can you see why having user ops depend on `tensorflow/core` targets causes these issues? (ie, is there a way to get around `disallow_deps` problem @crjensen21 found?)\r\n", "Yeah tf_custom_op_library has some restrictions on what you can depend on. \r\n\r\nFor disallow_deps, you should be able to depend on //tensorflow/core:framework_headers_lib, which may include reader_base.h \r\n\r\n", "Adding //tensorflow/core:framework_headers_lib produces an error saying the dependence is duplicated in the attributes of the build rule:\r\n\r\n`Label '//tensorflow/core:framework_headers_lib' is duplicated in the 'deps' attribute of rule 'wav_reader_op.so_check_deps'.\r\n`\r\n\r\nI think a part of the problem here is that 'reader_base.h' includes 'reader_base.pb.h' which appears to be part of the protocol buffer build output for reader_base. Bazel finds the file in the '/bazel-out/' folder and complains that the dependence wasn't declared. I only know protocol buffers and bazel well enough to be dangerous so I don't know what to do with that.", "To be honest, we actually gave up on inheriting from the `reader_base` framework. It was too much hassle, and the overheads were quite high. \r\n\r\nAnother thing you can do is create a vanilla Op that takes a `string` type tensor filename, that when executed reads your file from disk into a `Tensor` of whatever type/shape you need and output it. However, if you need to read pieces of a file every step, as the `reader_base` stuff does, then it's probably better to wait for a solution from a googler. \r\n\r\nOn the other hand, if you are comfortable enough with the code, you can add a `resource` tensor as an input that references a `QueueInterface`, which you use to get new filenames if you need them that step (this is what the reader_base stuff does). ", "Thanks a lot @sbyma and @yaroslavvb for your help. I'd really like to just get something working before I ditch inheriting from reader_base, but the build is proving difficult. I got it to compile with g++ but I had to copy header files into the user lib folder that tensorflow passes as the \"include\" folder since reader_base.h isn't copied there. It really doesn't seem like the best way to go so I hope to get some kind of response from a tensorflower.", "summoning @keveman the wizard of all user_ops things related", "@crjensen21 & @yaroslavvb I am having the same issue. Patching with below allows compilation into a shared library, but seg faults after any program that loads the library (with `tf.load_op_library`) finishes.\r\n\r\n<pre>\r\ndiff --git a/tensorflow/tensorflow.bzl b/tensorflow/tensorflow.bzl\r\nindex d78cb7b57..7d0d5ae9a 100644\r\n--- a/tensorflow/tensorflow.bzl\r\n+++ b/tensorflow/tensorflow.bzl\r\n@@ -774,8 +774,9 @@ def tf_custom_op_library(name, srcs=[], gpu_srcs=[], deps=[]):\r\n \r\n   check_deps(name=name+\"_check_deps\",\r\n              deps=deps + if_cuda(cuda_deps),\r\n-             disallowed_deps=[\"//tensorflow/core:framework\",\r\n-                              \"//tensorflow/core:lib\"])\r\n+              disallowed_deps=[])\r\n+#            disallowed_deps=[\"//tensorflow/core:framework\",\r\n+#                             \"//tensorflow/core:lib\"])\r\n</pre>\r\n\r\nSegfault backtrace:\r\n<pre>\r\nProgram received signal SIGSEGV, Segmentation fault.\r\nmalloc_consolidate (av=av@entry=0x7ffff7bb5760 <main_arena>) at malloc.c:4151\r\n4151\tmalloc.c: No such file or directory.\r\n(gdb) bt\r\n#0  malloc_consolidate (av=av@entry=0x7ffff7bb5760 <main_arena>) at malloc.c:4151\r\n#1  0x00007ffff7876ce8 in _int_malloc (av=av@entry=0x7ffff7bb5760 <main_arena>, bytes=bytes@entry=1256) at malloc.c:3423\r\n#2  0x00007ffff787a1dc in __libc_calloc (n=<optimized out>, elem_size=<optimized out>) at malloc.c:3219\r\n#3  0x00007fffb8b00361 in ?? () from /usr/local/cuda/lib64/libcudnn.so\r\n#4  0x00007fffb8b00477 in ?? () from /usr/local/cuda/lib64/libcudnn.so\r\n#5  0x00007ffff78331a9 in __run_exit_handlers (status=0, listp=0x7ffff7bb56c8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true) at exit.c:82\r\n#6  0x00007ffff78331f5 in __GI_exit (status=<optimized out>) at exit.c:104\r\n#7  0x00007ffff7818f4c in __libc_start_main (main=0x466e50 <main>, argc=2, argv=0x7fffffffdf58, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, \r\n    stack_end=0x7fffffffdf48) at libc-start.c:321\r\n#8  0x0000000000577c2e in _start ()\r\n</pre>\r\n\r\nHopefully @keveman can help.", "I'm not sure it's the same issue. Maybe build with `-c dbg` to get line numbers? Also, during configure, there's an option that asks if you want `jealloc` so that's another thing to experiment with", "@jkiske : I actually had success by removing the disallowed_deps line as well. Every time I tried to build it with gcc there was an unrecognized symbol that I could never fix. I think @yaroslavvb is right that your segfault could be a separate issue.\r\n\r\nThis is now working and I also hacked the class definition from the inheritance of ReaderBase to use tf.load_op_library instead of the gen_user_ops.py file which I could never get to work. I think the documentation around this stuff needs some updating. ", "Actually, @jkiske , I may have spoken too soon. My reader op works in the tensorflow environment I built myself, but when I test it in the environment where I used the binary install ('pip install tensorflow-gpu'), I get a segmentation fault when I call tf.load_op_library. I would like to distribute this op to others with the binary install, so this is annoying. But it might also be something for you to try if you can't figure out your seg fault.", "@crjensen21 I fixed the segfault by modifying the build file for reader_base. It no longer pulls in the entire framework, which I think is better, overall.\r\n\r\nThis still does not fix the issue with `disallowed_deps` because we still depend on core\r\n\r\n```python \r\ncc_library(\r\n    name = \"reader_base\",\r\n    srcs = [\"reader_base.cc\"],\r\n    hdrs = [\"reader_base.h\"],\r\n    deps = [\r\n        \":reader_base_proto_cc\",\r\n        \"//tensorflow/core:framework_headers_lib\",\r\n        \"//third_party/eigen3\",\r\n    ],\r\n)", "cc @vrv - this is another place where `disallowed_ops` for user_op are blocking needed dependencies", "FYI, I found that when I was implementing [kernels for new hardware](https://github.com/tensorflow/tensorflow/issues/4359#issuecomment-247187582) that I also needed to link to a variety of shared libraries in order to have all the proper symbols in my shared library. Its been a while, but here is what I did in my Makefile\r\n\r\n```\r\nTF_INC         :=$(shell python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\r\nTF_LIB         :=$(shell find $(TF_INC)/../ -name \"*.so\")\r\n```\r\n\r\nWithout it, I would get errors saying stuff about protobuf symbols missing and tensorflow operation kernel symbols missing. Fairly annoying, but it was because I was trying to reuse the kernels that are fairly generic like ConstantOp, VarialeOp, ZeroOp, etc.\r\n\r\nI can't remember exactly the problem, but with some effort I could try to to replicate it. I just need to binary trace in Git since our code base moved on.", "Thanks @jkiske and @aidan-plenert-macdonald . I've come back to this problem and still haven't had any luck. If I change the reader_base deps I either get a missing symbol or a seg fault if I try to load the op from the binary install of tensorflow - it still works just fine for my build of tensorflow and I've been unable to resolve the issue.\r\n\r\nWhat's more though, unless I'm missing something, the new r1.0 documentation is incorrect. The custom reader doc says that you can compile the custom reader as a dynamic library - this links to the custom op documentation which says that you can build a custom op with the binary installation of tensorflow. I don't see any way to bring in the reader_base dependency without the full source code. The reader_base header isn't in the binary install's \"include\" folder. \r\n\r\nIt looks to me like the system for user_ops was changed around the load_op_library dynamic linking, which is great, but the custom reader ops don't appear to fit into that paradigm without modifying the build files. Any help from a tensorflower would be much appreciated because I'm pretty much out of ideas here.", "@crjensen21 I got it to work. You need to link in a bunch of TF libraries. I used the following\r\n\r\n```\r\nTF_INC         :=$(shell python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\r\nTF_LIB         :=$(shell find $(TF_INC)/../ -name \"*.so\")\r\n```\r\n\r\nThose are all the include files you need and the associate libraries. Link them in however your compiler does it. I did,\r\n\r\n```\r\ng++ -std=c++11 -shared $(CC_FILES) -o mylib.so -fPIC $(INC) $(LIBS)\r\n```\r\n\r\nAlso, you need to build TF with more headers. I used this patch,\r\n\r\n```\r\ndiff --git a/tensorflow/tools/pip_package/BUILD b/tensorflow/tools/pip_package/BUILD\r\nindex 6a3f66b..c88840a 100644\r\n--- a/tensorflow/tools/pip_package/BUILD\r\n+++ b/tensorflow/tools/pip_package/BUILD\r\n@@ -26,6 +26,8 @@ transitive_hdrs(\r\n         \"//tensorflow/core:core_cpu\",\r\n         \"//tensorflow/core:framework\",\r\n         \"//tensorflow/core:lib\",\r\n+        \"//tensorflow/core:proto_text\",\r\n+        \"//tensorflow/core:all_kernels\",\r\n         \"//tensorflow/core:protos_all_cc\",\r\n         \"//tensorflow/core:stream_executor\",\r\n         \"//third_party/eigen3\",\r\n```", "Thanks @aidan-plenert-macdonald. I tried that and I still end up with this:\r\n\r\n> /home/crjensen21/tensorflow/tensorflow/core/user_ops/wav_reader_op.so: undefined symbol: _ZN10tensorflow10ReaderBase4ReadEPNS_14QueueInterfaceEPSsS3_PNS_15OpKernelContextE\r\n\r\nOn the bright side, they've moved the reader_base to the framework folder rather than the kernels folder so I can now build it using bazel without having to patch the bazel build.  Unfortunately, building that way I now get this error:\r\n\r\n> /home/crjensen21/tensorflow/bazel-bin/tensorflow/core/user_ops/wav_reader_op.so: undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringB5cxx11E\r\n\r\nSo I still have to remove the disallowed_deps and depend on the reader_base target to get all my symbols linked. Which still only works in my build of tensorflow. So things are changing but the result is staying the same.\r\n\r\n\r\n\r\n\r\n", "Weird. Are you sure you are linking right? I definitely have those symbols in my shared libs\r\n\r\n```\r\namacdonald@slpl-000623:/usr/local/lib/python2.7/dist-packages/tensorflow$ nm -g `find ./ -name *.so` | grep \"\\.so\\|ReaderBase4Read\\|fixed_address_empty_string\"\r\n./python/_pywrap_tensorflow.so:\r\n00000000014ded90 T _ZN10tensorflow10ReaderBase4ReadEPNS_14QueueInterfaceEPSsS3_PNS_15OpKernelContextE\r\n./contrib/layers/python/ops/_sparse_feature_cross_op.so:\r\n0000000000285b50 B _ZN6google8protobuf8internal26fixed_address_empty_stringE\r\n./contrib/layers/python/ops/_bucketization_op.so:\r\n0000000000269b50 B _ZN6google8protobuf8internal26fixed_address_empty_stringE\r\n... A bunch more with the fixed address symbol ...\r\n```", "A co-worker just got @crjensen21 's error. I will let you know if I manage a fix for it", "@crjensen21 I found the symbol in this file `/usr/local/lib/python2.7/dist-packages/external/protobuf/python/google/protobuf/pyext/_message.so` Linking against it fixed the problem", "Any updates on modifying `disallowed_deps` to allow access to core?", "@jkiske you can modify it locally, right? Also, it seems from discussion above, `disallowed_deps` is just part of the issue, and there will be shared libs issues.\r\n\r\nThe thing about formally supporting this use-case, is that it signals that core is part of official API, which means that it can't change quickly, and any change must go through review to avoid breaking too many people. This slows down development and benefits must outweigh the costs.\r\n\r\nI tried to make a case for opening up Allocator API in https://github.com/tensorflow/tensorflow/issues/7581 (just part of the things in core), but the case is not that strong because:\r\n1. I can build those ops by using g++ instead of bazel\r\n2. Two official TensorFlow ops have been added that reduce the need to have access to Allocator API\r\n\r\nIt would make sense to open up some Reader APIs to be officially available to be used in user-ops through Bazel if there are some use-cases which would be made significantly better", "@yaroslavvb Currently, we (as in my coworkers) do pretty well with the hacks we did above. That's not to say we wouldn't appreciate official support, but we can work around it pretty easily.\r\n\r\nI would just note that this isn't *just* about Reader ops. The lack of headers and shared libs affects peoples ability to add custom hardware (ie. Allocators, Devices, DeviceContexts, etc.) and to add drop in support for many of the generic operations like Zeros, Ones, NoOp, Const, etc. and there are a lot of them that could with some clever macros be automatically supported on any hardware.\r\n\r\nI also believe that this affected our development of HLA, but I would have to check again.\r\n\r\nNow, as for your concern that any changes you make upstream might break downstream guys. Well, this already happens to us regardless of your headers being present, but happens very infrequently and is relatively easy to fix.", "@aidan-plenert-macdonald it seems like \"custom hardware for TF\" is in a weird category of being  encouraged, yet requiring hacks/unofficial APIs. Bigger players like Qualcomm/Intel work in a partnernship with Google, so they have a direct channel to the development team. Not sure what the plans are for other 3rd party developers  cc @vrv @tatatodd ", "We are happy to review PRs that open up access to interfaces needed by device developers; it's hard for us to do it ourselves because we don't know all the headers your devices need access to, and our internal development environment is different enough from the external system that we don't run into these issues ourselves (or else we'd fix it).\r\n\r\nSo I think the best step in general is for anybody to send a PR that opens up access or even lists the headers that you need, and we can figure out the right way to make it available.  Just because it is listed as disallowed today isn't necessarily a set-in-stone decision!", "@vrv, Other hardware guys (eg @DavidNorman ) might need something else, but all I need is,\r\n```\r\ndiff --git a/tensorflow/tools/pip_package/BUILD b/tensorflow/tools/pip_package/BUILD\r\nindex 6a3f66b..c88840a 100644\r\n--- a/tensorflow/tools/pip_package/BUILD\r\n+++ b/tensorflow/tools/pip_package/BUILD\r\n@@ -26,6 +26,8 @@ transitive_hdrs(\r\n         \"//tensorflow/core:core_cpu\",\r\n         \"//tensorflow/core:framework\",\r\n         \"//tensorflow/core:lib\",\r\n+        \"//tensorflow/core:proto_text\",\r\n+        \"//tensorflow/core:all_kernels\",\r\n         \"//tensorflow/core:protos_all_cc\",\r\n         \"//tensorflow/core:stream_executor\",\r\n         \"//third_party/eigen3\",\r\n```\r\n\r\nWith that patch, I can do everything I need. @yaroslavvb, we haven't needed to do too many hacks that weren't already present in GPU code. Once I figured out the general core of TF, porting was pretty straight forward.", "It looks like this is best handled by PRs that opens up access to portions of TensorFlow core that maybe needed by specific use cases. Closing this one out as there is nothing generic that can be done.", "I now have the same issue with the following symbol missing in my custom reader op:\r\n```\r\nNotFoundError: custom_ops/librecord_reader_op.so: undefined symbol: _ZN10tensorflow10ReaderBase4ReadEPNS_14QueueInterfaceEPSsS3_PNS_15OpKernelContextE\r\n```\r\nI am using the official Docker image version 1.1.0-devel-gpu and I've already searched all shared libs in the image for this symbol.\r\n\r\nI did not have any issues in a 1.0.0-devel-gpu image in which I manually pulled and built the TF 1.1 RC1 code (I did have a missing symbol initially, but that was solved by linking to _message.so as mentioned above).\r\n\r\nAny ideas? I'll try to build TF 1.1.0 from source to see if that solves the issue.", "That's \"tensorflow::ReaderBase::Read(tensorflow::QueueInterface*, std::string*, std::string*, tensorflow::OpKernelContext*)\", so it won't be in _message, only in _pywrap_tensorflow.so (if at all)", "I had checked _pywrap_tensorflow.so, but the version included in the Docker image is missing this symbol for some reason. I don't know why, but would suggest that this is a bug that should be addressed, as the symbol is present when I build TF from source.\r\n\r\nI managed to get things to work as follows:\r\n\r\nFirst, I built TF locally in the same Docker image, following the instructions on the website, with CUDA enabled and all other optional features disabled. Unfortunately, the generated pip install is missing the following headers:\r\n```\r\ntensorflow/core/framework/reader_base.h\r\ntensorflow/core/framework/reader_base.pb.h\r\n```\r\nThese headers *are* included in the original TF install that came with the Docker image under `/usr/local/lib/python2.7/dist-packages/tensorflow`, and I can build my library either by adding the original install folder as include search path, or by adding the TF source tree `/tensorflow` and `/tensorflow/bazel-genfiles` to the search path.  Does anyone know what I have to do to get these headers included in my own pip install?\r\n\r\nTo load the library from Python, I have to ensure that it links to the shared libs of the locally built version of TF, which include all required symbols.\r\n\r\nUnfortunately, this means that I need two versions of TF installed to be able to both build and load a custom reader op, which isn't a particularly nice solution.", "I just installed pip installed tensorflow from pypi in a virtualenv and I found reader_base.h and reader_base.pb.h in python2.7/site-packages/tensorflow/include/tensorflow/core/framework -- I'm not sure what the difference between our Docker environment and a standard pip install is. :(\r\n\r\n", "Sorry I wasn't clear: The headers are not a problem with the Docker image (which does have the headers in the same folder), but with the Linux build process described [here](https://www.tensorflow.org/install/install_sources).\r\n\r\nThe problem with the Docker image are the missing symbols in /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so.", "@caisq any ideas why our Docker instance's installed TF would have a separate set of symbols than the pip installed ones?\r\n\r\nI indeed verified as well that the pip installed package does contain the missing symbol @aleist needs, but don't know why the Docker version would be different.\r\n\r\n$ nm _pywrap_tensorflow_internal.so  | grep _ZN10tensorflow10ReaderBase4ReadEPNS_14QueueInterfaceEPSsS3_PNS_15OpKernelContextE\r\n00000000028e5280 T _ZN10tensorflow10ReaderBase4ReadEPNS_14QueueInterfaceEPSsS3_PNS_15OpKernelContextE\r\n", "@vrv @aleist mentioned he's using the `devel-gpu` docker image. For the `devel-*` images,the pip package is built and installed during Docker build, instead of downloaded and installed from a centralized location, as in the non-devel images. \r\n\r\nC.f.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L97\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.gpu#L45\r\n\r\n", "Thanks!  So why would the pip package built during docker build be different than the pip package we build for pypi?", "I just noticed that Dockerfile.devel-gpu does not include option `--cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` to `bazel build`, which I added in my build as it was needed in the past to get things working and the build instructions still mention it now. Maybe the Dockerfile is missing this option?", "@aleist Good catch. I think the Dockerfile is missing it. Feel free to send a PR to fix it. Otherwise I'll be happy to fix it.", "Great, will do. Thanks to both of you for your help!", "@aleist The centralized pip wheels (the ones from PyPI and googleapis.com) are built on ubuntu:14.04, which has gcc 4.8, while the devel-* docker images are based on ubuntu:16.04, which has gcc 5.4. This is probably the reason why D_GLIBCXX_USE_CXX11_ABI flag is needed on the docker images.\r\n\r\nAnyway, thank you for reporting this issue and offering to help fix it!", "I tried to follow the [Custom Data Readers](https://www.tensorflow.org/extend/new_data_formats) to build my own data reader. However, after adding the code, the compiler reports \r\n\r\nERROR: /home/ig/Workspace/tensor-graphics/tensorflow-graphics/tensorflow/core/BUILD:1341:1: undeclared inclusion(s) in rule '//tensorflow/core:framework_internal': this rule is missing dependency declarations for the following files included by 'tensorflow/core/framework/reader_base.cc': \r\n'bazel-out/local_linux-py3-opt/genfiles/tensorflow/core/framework/reader_base.pb.h'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 137.550s, Critical Path: 103.09s\r\n\r\nI notice this thread [Dependency required for user Reader ops?](https://github.com/tensorflow/tensorflow/issues/1419) reported the identity problem. I want to know two things:\r\n1. Can we still follow the guideline in the official website to build custom data reader with reader_base?\r\n2. If not, are there any new approaches to create a custom data reader?\r\n\r\nCould someone help me about these two questions?\r\n\r\nAPPEND:\r\nI tried to add \"//tensorflow/core:framework_headers_lib\" but failed, with many un-resolved functions. Then, I add \":reader_base\" directly in deps and pass the compile, as:\r\ncc_library(\r\n    name = \"user_ops_op_lib\",\r\n    srcs = glob([\"user_ops/**/*.cc\"]),\r\n    copts = tf_copts(),\r\n    linkstatic = 1,\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\r\n        \":framework\",\r\n        \":reader_base\",\r\n        # \"//tensorflow/core:framework_headers_lib\"\r\n    ],\r\n    alwayslink = 1,\r\n)\r\n\r\nI am not sure whether will bring some potential risks or not.", "In case anyone stumbles here because of the `undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE` error.\r\n\r\nI compile my file now by using these additional compiler flags:\r\n\r\n```\r\n# Fix for undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE.\r\n# https://github.com/tensorflow/tensorflow/issues/1419\r\nfrom google.protobuf.pyext import _message as msg\r\nlib = msg.__file__\r\n\r\nextra_compiler_flags = [\r\n    \"-Xlinker\", \"-rpath\", \"-Xlinker\", os.path.dirname(lib),\r\n    \"-L\", os.path.dirname(lib), \"-l\", \":\" + os.path.basename(lib)]\r\n```\r\n\r\nI add the `-rpath` so that it finds the file also when it is being loaded.\r\n\r\nEdit: Although I'm not sure if this is correct. I get some strange behavior (see [here for details](https://stackoverflow.com/questions/44455722/create-my-own-resource-types-tf-resource)). Can someone comment if linking this protobuf is really the correct thing? My `ResourceHandle` definitely does not behave correctly.\r\n", "I tried to add a customer ops for Kafka (PR #14098) that utilize `ReaderBase` and encountered a similar issue. Had to comment out clean_deps of tensorflow/core:framework and tensorflow/core:lib for now. Hope there are other nice and clean ways to utilize `ReaderBase`."]}, {"number": 1418, "title": "Using 2 computers - Distributed Tensorflow", "body": "Copied from: http://stackoverflow.com/questions/35824742/tensorflow-distributed-assing-devices\n\nI recently installed the version of tensorflow for distributed computers. From the trend: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime , i tried to implemented multiples gpus at multiples computers, and also looking at \"https://stackoverflow.com/questions/34439045/tensorflow-setup-for-distributed-computing\" found a white paper for some additional specifications. I can run the server and a worker on 2 different computers with 2 and 1 gpu, and using the session gprc, allocate and run the program on remote or local mode. i ran locally tensorflow in the remote computer with:\n\n```\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='local|localhost:2500' --job_name=local --task_id=0 &\n```\n\nand for using on the server\n\n```\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=worker --task_id=0 \\\n--job_name=prs --task_id=0 &\n```\n\nHowever, when i try to specify the device for running on 2 computers at the same time the python show me the error:\n\n`Could not satisfy explicit device specification '/job:worker/task:0'`\n\nwhen i use\n\n```\nwith tf.device(\"/job:prs/task:0/device:gpu:0\"):\n  x = tf.placeholder(tf.float32, [None, 784], name='x-input')\n  W = tf.Variable(tf.zeros([784, 10]), name='weights')\nwith tf.device(\"/job:prs/task:0/device:gpu:1\"):\n  b = tf.Variable(tf.zeros([10], name='bias'))\n# Use a name scope to organize nodes in the graph visualizer\nwith tf.device(\"/job:worker/task:0/device:gpu:0\"):\n  with tf.name_scope('Wx_b'):\n    y = tf.nn.softmax(tf.matmul(x, W) + b)\n```\n\nor even changing the name of job. So I am wondering if it is required to Add a New Device (https://stackoverflow.com/questions/35213172/add-a-new-device-in-tensorflow) or probably i am doing something wrong with the initialization of the cluster.\n\nBest Regards\n", "comments": ["All servers in the same cluster must be initialized with the same cluster_spec I think. On one computer you might do\n\n```\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=worker --task_id=0 &\n```\n\nand the other \n\n```\nbazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server \\\n--cluster_spec='worker|192.168.170.193:2500,prs|192.168.170.226:2500' --job_name=prs --task_id=0 &\n```\n", "@Fhrozen did @sbyma's solution fix your problem?\n", "@sbyma thanks for the help. Yes, it fix my problem. Just wondering two things:\nFirst:\nI got that warning in both computers, but even that i can make a distributed training.\n`Could not find channel for target: /job:prs/replica:0/task:0`\n\nSecond:\nI could not target directly to the gpus using: \n`with tf.device(\"/job:worker/task:0/gpu:1\"):`\nSince In the second computer has 2 gpus and tensorflow create the devices with bazel i supposed that i can target directly to the gpu also change the gpu with device:gpu:0 ... 1 or device:0 and could not make run. \nAny help. If it is possible send a pm, I will close the thread.\n\nRegards\n", "I have a question. If I have two machine, each of which has 4 GPUs. Can I target tasks directly using: `with tf.device(\"/gpu:0\"):` to `with tf.device(\"/gpu:7\")`. Thanks~\n\n[The issue I meet](https://github.com/tensorflow/tensorflow/issues/2322)\n", "I installed tensorflow from source, version `10`\n\nI don't have `grpc_tensorflow_server` in my `rpc` folder what should I do?\n\nThese are the contents of my `rpc` folder\n\n```\n$ bazel-bin/tensorflow/core/distributed_runtime/rpc/ --cluster_spec='local|localhost:2500' --job_name=local --task_id=0 &\n libgrpc_channel.pic.a              libgrpc_remote_master.pic.lo       libgrpc_session.pic.lo             libgrpc_worker_service_impl.pic.a  _objs/                             \n libgrpc_master_service_impl.pic.a  libgrpc_remote_worker.pic.a        libgrpc_tensor_coding.pic.a        libgrpc_worker_service.pic.a       \n libgrpc_master_service.pic.lo      libgrpc_server_lib.pic.lo          libgrpc_worker_cache.pic.a         librpc_rendezvous_mgr.pic.a\n```\n"]}, {"number": 1417, "title": "gradient of diag operator", "body": "@girving Hopefully I learned your lessons.  I used auto in a loop, but initialized it to the unsigned version as I found out why it was comparing signed and unsigned.  Otherwise, hopefully this is good to go.\n\n@vrv I noticed that you changed some additional files for the diag_part operator, I must have missed them.  Thanks.\n\n@cameronphchen Just wanted to let you know about this commit.\n", "comments": ["Can one of the admins verify this patch?\n", "It'd be better to use a pure Python test for this, since then you can use tf.test.compute_gradient_error.\n", "Also, can you implement and test the gradient of diag_part in the same commit (which is diag)?  Should just be a few more lines, and it's always delightful to add gradients of transpose operators in pairs. :)\n", "@girving Added the gradients for both tf.diag and tf.diag_part and the tests for them.  In the separate commit, also fixed a bug when gradient on int type is called.  \n", "Referecing Issue #1293 before I forget.\n", "Looks good once the C++ changes are removed!\n", "@girving Wait, isn't the c++ code necessary?\n", "No, normally we just register gradients in Python.\n", "@girving removed.  \n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1416, "title": "ci_build - debian jessie", "body": "add debian jessie cpu container to ci_build\nand cleanup a few things\n", "comments": ["@caisq @martinwicke looks good? We need to merge this. Thanks,\n", "LGTM, merged.\n", "Thanks.\n"]}, {"number": 1415, "title": "pip install version 0.7.1 from source: No module named google.protobuf", "body": "Hello,\n\nI have an issue with Google Protobuf. Even after the install Python does not find the module \"google.protobuf\".\n### Environment info\n\nOperating System: Ubuntu 14.04\nPython: 2.7.6\nBazel: 0.2.0\nCuda: 7.5\nCudNN: 4\n\nIf installed from sources, provide the commit hash: https://github.com/tensorflow/tensorflow/commit/1e47e2f9bb7112f1a763e758a7bcdaff36edec5b\n### Steps to reproduce\n1. bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n2. bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n3. bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n4. PYTHONUSERBASE=/usr/local/python pip install --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\n\nOutput for pip install:\n\n```\nUnpacking /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/python/lib/python2.7/site-packages (from tensorflow==0.7.1)\nDownloading/unpacking protobuf==3.0.0b2 (from tensorflow==0.7.1)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB): 326kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): wheel in /usr/local/python/lib/python2.7/site-packages (from tensorflow==0.7.1)\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /usr/local/python/lib/python2.7/site-packages (from tensorflow==0.7.1)\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/local/python/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)\nInstalling collected packages: tensorflow, protobuf\nSuccessfully installed tensorflow protobuf\nCleaning up...\n```\n\nError in Python interpreter:\n\n```\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import google.protobuf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named google.protobuf\n>>>\n```\n\nIn the Python prompt, when I run `help('modules')`, I can see the module `tensorflow` but not the module `google`.\n\nThe first thing I tried is:\n1. pip uninstall protobuf \n2. PYTHONUSERBASE=/usr/local/python pip install --user --upgrade /tmp/tensorflow_pkg/tensorflow-0.7.1-cp27-none-any.whl\n\nOutput:\n\n```\nInstalling collected packages: tensorflow, protobuf\nSuccessfully installed tensorflow protobuf\nCleaning up...\n```\n\nSame result, no Protobuf module found.\n\nThe second thing I tried is:\n1. pip uninstall protobuf\n2. pip uninstall tensorflow\n3. PYTHONUSERBASE=/usr/local/python pip install --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\n\nOutput:\n\n```\nInstalling collected packages: tensorflow, protobuf\nSuccessfully installed tensorflow protobuf\nCleaning up...\n```\n\nSame result again, not Protobuf module found.\n\nAny idea of why Protobuf is properly installed but not recognized by Python whereas Tensorflow is propertly recognized by Python?\n\nThanks for any help that you can provide.\n", "comments": ["It's just protobuf, not google.protobuf. Does something in tensorflow not work?\n", "Did you use --recurse-submodules when you clone?\n`git clone --recurse-submodules https://github.com/tensorflow/tensorflow`\n`git submodule update --init` should also work\n", "There seems to have been an issue with the protobuf pypi package, which should have been fixed about 30 minutes ago, please try again.\n", "Sorry for my late reply. But I still have the same issue with the commit https://github.com/tensorflow/tensorflow/commit/34880c4ca1a83b443b44dc415b74deb217c42257\n\nI did a brand new clone of Tensorflow by running:\n\n```\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow\n```\n\nThen I recompiled everything\n\n```\n./configure\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nPYTHONUSERBASE=/usr/local/python pip install --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\ncd tensorflow/models/image/mnist\npython convolutional.py\n```\n\nAnd I still have the same error:\n\n```\nTraceback (most recent call last):\n  File \"convolutional.py\", line 34, in <module>\n    import tensorflow as tf\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named google.protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n\nAnd indeed if I run the command `help('modules')`, I still don't see any `protobuf` or `google` module in the list.\n", "I think the problem would be resolved by doing as follows, since I remember having to do something similar:\n\n```\ngit clone https://github.com/tensorflow/tensorflow\ngit clone https://github.com/google/protobuf\ncd tensorflow\ngit submodule init\ngit config submodule.google/protobuf.url ../protobuf\ngit submodule update\n```\n", "pip uninstall tensorflow\npip uninstall protobuf\npip install path/to/tensorflow.whl\n\nhas also worked for a lot of people.  0.7.0's install broke a lot of things that 0.7.1 fixed.\n", "Hello,\n\nAbout @vrv's answer, still not working:\n\n```\npip uninstall tensorflow\npip uninstall protobuf\nPYTHONUSERBASE=/usr/local/python pip install --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\nUnpacking /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1)\nDownloading/unpacking protobuf==3.0.0b2 (from tensorflow==0.7.1)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB): 326kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): wheel in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1)\nDownloading/unpacking six>=1.10.0 (from tensorflow==0.7.1)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/lib/python2.7/dist-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)\nInstalling collected packages: tensorflow, protobuf, six\nSuccessfully installed tensorflow protobuf six\nCleaning up...\ncd tensorflow/models/image/mnist\npython convolutional.py\nTraceback (most recent call last):\n  File \"convolutional.py\", line 34, in <module>\n    import tensorflow as tf\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named google.protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n\nNow about @RasmooL's answer, again, still the same error:\n\n```\ngit clone https://github.com/tensorflow/tensorflow\ngit clone https://github.com/google/protobuf\ncd tensorflow\ngit submodule init\ngit config submodule.google/protobuf.url ../protobuf\ngit submodule update\n./configure\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\nPYTHONUSERBASE=/usr/local/python pip install --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\nUnpacking /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1)\nDownloading/unpacking protobuf==3.0.0b2 (from tensorflow==0.7.1)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB): 326kB downloaded\nRequirement already satisfied (use --upgrade to upgrade): wheel in /usr/lib/python2.7/dist-packages (from tensorflow==0.7.1)\nDownloading/unpacking six>=1.10.0 (from tensorflow==0.7.1)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): setuptools in /usr/lib/python2.7/dist-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)\nInstalling collected packages: tensorflow, protobuf, six\nSuccessfully installed tensorflow protobuf six\nCleaning up...\ncd tensorflow/models/image/mnist\npython convolutional.py\nTraceback (most recent call last):\n  File \"convolutional.py\", line 34, in <module>\n    import tensorflow as tf\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named google.protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n\nAnd, even after trying once again @vrv's answer, same error.\n", "I tried on the other machines, one with Ubuntu 14.04 with the same config than mine and another on Ubuntu 15.10 with a slighly more recent Python (2.7.10). And on both I still have the same issue.\n", "Ok, I have maybe found something. My protobuf module is installed under:\n\n```\n/usr/local/python/lib/python2.7/site-packages/google/protobuf/\n```\n\nAnd in my PYTHONPATH I have:\n\n```\n/usr/local/python/lib/python2.7/site-packages/\n```\n\nThen to import the Protobuf module we have to do \"google.protobuf\". But apparently Python doesn't like this and I don't know why because if I add the following path in my PYTHONPATH:\n\n```\n/usr/local/python/lib/python2.7/site-packages/google/\n```\n\nI can see the Protobuf module in the list of recognized modules. Nevertheless, in all the python files in Tensorflow, the Protobuf module is imported like this \"google.protobuf\" which result for to the same error:\n\n```\npython convolutional.py\nTraceback (most recent call last):\n  File \"convolutional.py\", line 34, in <module>\n    import tensorflow as tf\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named google.protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n\nAny idea of a possible workaround? Unless turn all the \"google.protobuf\" in the Python files to \"protobuf\" :(\n", "Can you take a look at our install guide on tensorflow.org for \"common problems\" and let us know if that helps ?\n\nhttps://www.tensorflow.org/versions/master/get_started/os_setup.html#pip-installation-issues\n", "I tried all the proposed solutions about \"pip\":\n\n```\npip install --upgrade pip\n```\n\nAnd \n\n```\nPYTHONUSERBASE=/usr/local/python pip install --upgrade --user /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\n```\n\nBut I still have the same issue. The module \"protobuf\" is well installed and recognized by Python, but the module \"google.protobuf\" is still not known.\n\nI have the same behavior with the 3 different machines I'm testing on.\n", "Did you try:\n\npip uninstall tensorflow\npip uninstall protobuf\npip install path/to/tensorflow.whl ?\n", "Yes, and it still doesn't work.\n\nI also tried with virtualenv to see if it can change something, but I have the exact same behavior.\n", "Same problem here after updating from 0.6 to 0.7.1. Not sure if this helps, but I noticed that site-packages/google/protobuf/**init**.py file is missing after recompiling with bazel.\n", "I had the same problem when trying to update from 0.5 to 0.7.1, It seems sth. is wrong with the wheel package\n", "Can you try installing the newest master wheel from here?\nhttp://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl\n\nI just installed it in a clean 2.7.11 virtualenv and it worked fine,\n", "Stil doesn't work:\n\n```\npip uninstall tensorflow\npip uninstall protobuf\nPYTHONUSERBASE=/usr/local/python pip install --user http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl\npython convolutional.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nTraceback (most recent call last):\n  File \"convolutional.py\", line 34, in <module>\n    import tensorflow as tf\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 55, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named google.protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there\n```\n\nOn the 3 different machines I'm using for testing and virtulenv.\n", "I had the similar problem after upgrading to 0.7 ,sorted out by\n1. uninstalling tensorflow\n2. install master\n3. Check whether you are able to import tensorflow --> If it works first,\nthen upgrade to 0.7\n\nYou may want to check the python path and verify the version on protocol\nbuffer during your installation.\n\nCheers,\nMani\n\nCheers,\nMani\n\nOn Wed, Mar 16, 2016 at 8:46 PM, Julien Plu notifications@github.com\nwrote:\n\n> Stil doesn't work:\n> \n> pip uninstall tensorflow\n> pip uninstall protobuf\n> PYTHONUSERBASE=/usr/local/python pip install --user http://ci.tensorflow.org/job/tensorflow-master-gpu_pip/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py2-none-any.whl\n> python convolutional.py\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n> Traceback (most recent call last):\n>   File \"convolutional.py\", line 34, in <module>\n>     import tensorflow as tf\n>   File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n>     from tensorflow.python import *\n>   File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 55, in <module>\n>     raise ImportError(msg)\n> ImportError: Traceback (most recent call last):\n>   File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 49, in <module>\n>     from tensorflow.core.framework.graph_pb2 import *\n>   File \"/usr/local/python/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n>     from google.protobuf import descriptor as _descriptor\n> ImportError: No module named google.protobuf\n> \n> Error importing tensorflow.  Unless you are using bazel,\n> you should not try to import tensorflow from its source directory;\n> please exit the tensorflow source tree, and relaunch your python interpreter\n> from there\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1415#issuecomment-197302527\n", "> 1. uninstalling tensorflow\n> 2. install master\n> 3. Check whether you are able to import tensorflow --> If it works first,\n>    then upgrade to 0.7\n\nAlready done that according to the previous recommandations and it doesn't work.\n\n> You may want to check the python path and verify the version on protocol\n> buffer during your installation.\n\nMy PYTHONPATH variable is perfectly set because all the modules are recognized (including \"protobuf\") but not \"google.protobuf\". The version of the installed \"Protobuf\" is \"3.0.0b2\" which seems to be the good one.\n", "Ok, I succeed to solve the bug. Apprently, as @ajaun said an `__init__.py` file is missing in the \"google\" folder. Then, if I create an empty `__init__.py` file to get the following structure:\n\n```\ngoogle\n|-- __init__.py\n|-- protobuf\n```\n\nIt works.\n", "Until the bug has been fixed with protobuf, a cleaner workaround might be to follow @jonparrott in https://github.com/google/protobuf/issues/1296, namely to edit the source directory by\n1. Adding a new file tensorflow/google/**init**.py:\n\n```\ntry:\n    import pkg_resources\n    pkg_resources.declare_namespace(__name__)\nexcept ImportError:\n    import pkgutil\n    __path__ = pkgutil.extend_path(__path__, __name__)\n```\n1. Add namespace_packages to tensorflow/google/protobuf/python/setup.py:\n\n```\nsetup(\n    name='protobuf',\n    namespace_packages=['google'],\n    ...\n)\n```\n1. Recompile / pip install. Good luck!\n", "This works like a charm as well. Thanks!\n\nI let the issue open, waiting for the final fix.\n", "FWIW, it's not a workaround. It's how things **should** be done. Do we need to push for protobuf to cut a new release?\n", "@vrv: What's the status of this?\n", "I think we fixed this in 0.8 and it hasn't regressed.\n", "That issue is still present in 1.3.0. This is getting borderline ridiculous. It's been a year and a half now, so it's really time for it to get fixed once and for all.\r\n\r\n```\r\n$ pip install --user --no-cache-dir --upgrade --ignore-installed tensorflow-gpu\r\n[...]\r\nSuccessfully installed backports.weakref-1.0rc1 bleach-1.5.0 funcsigs-1.0.2 html5lib-0.9999999 markdown-2.6.9 mock-2.0.0 numpy-1.13.1 pbr-3.1.1 protobuf-3.4.0 setuptools-36.2.7 six-1.10.0 tensorflow-1.3.0 tensorflow-gpu-1.3.0 tensorflow-tensorboard-0.1.4 werkzeug-0.12.2 wheel-0.29.0\r\n$ python -c \"import tensorflow; print tensorflow.__version__\"\r\n[...]\r\n    from google.protobuf import descriptor as _descriptor\r\nImportError: No module named google.protobuf\r\n\r\n$ touch .local/lib/python2.7/site-packages/google/__init__.py\r\n$ python -c \"import tensorflow; print tensorflow.__version__\"\r\n1.3.0\r\n```", "You have protobuf 3.4.0 -- can you downgrade to 3.3 and see if that works? If not, can you file a new issue? I don't know whether an old bug has crept in.", "This is the protobuf version that comes as a dependency of Tensorflow via pip install. I have no local Python module installed before installing TF.", "protobuf released 3.4 a week ago. \r\n\r\n@kcgthb do you have any other google.* packages installed?\r\n\r\n@vrv what was the change required in protobuf that fixed this issue? Is it possible it's been accidentally reverted in 3.4?\r\n\r\n@gunan We're not seeing this in CI -- are we caching/fixing the version?", "In our CI we pin our dependencies to avoid failures whenever a new release happens.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_proto3.sh#L20\r\nLooks like our protobuf dependency is pinned to 3.4", "@martinwicke in the earliest versions of TF pre 1.0 we were packaging protobuf in our pip accidentally.  We haven't done that in over a year, so this is an unrelated issue to that.", "@martinwicke nope, no other google.* package installed. It's on a freshly deployed machine.", "I notices something strange in your log:\r\n\r\n```\r\nSuccessfully installed [...] tensorflow-1.3.0 tensorflow-gpu-1.3.0 [...]\r\n```\r\n\r\nCan you clear this out and reinstall only one TensorFlow (-gpu)? \r\n\r\n@yifeif we're not depending on tensorflow in tensorflow-gpu, are we?\r\n\r\nIs this in a virtualenv? Can you try that?", "tensorflow-gpu should not be depending on tensorflow. This might be due tensorboard's dependency on tensorflow. \r\n\r\nI would recommend trying virtualenv as well. \r\n\r\n\r\n", "I've noticed the tensorboard's dependency on tensorflow while installing tensorflow-gpu in a virtualenv. We probably have some circular dependency issues here.", "Yes, that seems to be the problem. @jart Looks like when installing tensorflow-gpu, tensorboard also tries to install tensorflow (non-gpu). I am not sure what should be out course of action here, maybe we can create a tensorboard-gpu package?\r\n\r\n", "another option is this:\r\nafter you pip install tensorflow-gpu, run these commands:\r\n(Remove both tensorflow and tensorflow-gpu, and reinstall tensorflow-gpu with no deps)\r\n\r\n```\r\npip uninstall tensorflow\r\npip install tensorflow-gpu --no-deps --upgrade\r\n```", "This problem is still present in the current `tensorflow-gpu` install. @jplu 's fix above works - you only need to `touch __init__.py` in the folder he mentions, and after that `import google.protobuf` works fine.", "This is because some Python module pretend to support Python 2.x but rely on features introduced in Python 3.3, such as implicit namespace packages ([PEP 420](https://www.python.org/dev/peps/pep-0420)). And Google's `protobuf` seems to expect everybody to be running a Python version that support native namespace packages.\r\n\r\nThis is all detailed in https://packaging.python.org/guides/packaging-namespace-packages/\r\nProperly supporting all versions of Python will require a `__init__.py` file in the module directory.\r\n\r\nThis has apparently been discussed [numerous times](https://github.com/protocolbuffers/protobuf/issues/4189), fixed in some versions, just to be [re-introduced](https://github.com/protocolbuffers/protobuf/issues/5046) later, so at that point, after several years and countless bug reports, \u00af\\_(\u30c4)_/\u00af", "Hi there, I'm the author of https://packaging.python.org/guides/packaging-namespace-packages/ and the \"gatekeeper\" as such of the `google` and `google.cloud` package namespaces.\r\n\r\n> This is because some Python module pretend to support Python 2.x but rely on features introduced in Python 3.3, such as implicit namespace packages (PEP 420).\r\n\r\nThis isn't true. We use pkg-resources namespaces for `google` and `google.cloud` for the broadest compatibility. This works fine on Python 2.7. We can't switch to any other without a coordinated release of several hundred packages.\r\n\r\n> This has apparently been discussed numerous times, fixed in some versions, just to be re-introduced later, so at that point, after several years and countless bug reports, \u00af_(\u30c4)_/\u00af\r\n\r\nThis isn't necessarily protobuf's fault. As far as I can tell, they've always had the proper incantations in `google/__init__.py` and `setup.py` to activate the namespace package logic and play nicely across all of the other Google packages.\r\n\r\nWhat *could* be happening is various bugs in pip, setuptools, and fun issues introduced by downstream re-packaging (such as debian) where the right combination of things produces this error.\r\n\r\nThis is why we always recommend to *use a virtualenv* and *use only an up-to-date pip* for installing Python packages for development.", "Hi @theacodes, \r\n\r\nThanks for the clarification, but I'm still confused:  I just tried a `pip install --user protobuf` again, with Python 2.7, and the directory tree it created in `~/.local/lib/python2.7/site-packages` looks like this:\r\n\r\n```\r\ngoogle/\r\n\u2514\u2500\u2500 protobuf\r\n    \u251c\u2500\u2500 __init__.py\r\n    \u251c\u2500\u2500 __init__.pyc\r\n    \u251c\u2500\u2500 any_pb2.py\r\n[...]\r\n```\r\nThat is: no `__init__.py` in `google/`, which, unless I'm missing something, makes it a native/implicit namespace, right? \r\n\r\nWouldn't a pkg-resources `google` namespace require a `google/__init__.py` file with `__import__('pkg_resources').declare_namespace(__name__)` for Python <3.3?", "It doesn't mean *anything*. pip (and other installers) can make decisions on how to treat the different kinds of namespace packages when installing. Pip has detected that your environment *should* support `.pth` files for namespace packages, so it *omits* `__init__.py` files for those packages and creates a `.pth` file in your `site-packages` directory for the `google` namespace. That `.pth` file handles wiring up the `google` module's `__path__` to point to *all* places where packages under the `google` namespace could be.", "I see, thanks.\r\nThe thing remains, seeing the number of times this issue has been reported, that it doesn't work for many people using many different distributions, Python versions and ways to install the modules.", "This is insane. I'm dealing with this issue for past 14 hours and it looks like the history goes back to 2016 (5 years ago). All just so I can use the google-cloud-vision Python library. Smh Google."]}, {"number": 1414, "title": "Fixing python3 issues", "body": "", "comments": ["Nice, it seems to fix a few. But not all - 4 done, 6 to go :)\n", "That last commit should fix a couple more, but at least two left, I think. Off to bed. Goodnight.\n", "Jenkins, test this please.\n", "3 left: \n\nsaver_test (Any has no attribute Pack)\ntensorboard:server_test ('very_large_attr' has type <class 'str'>, but expected one of: (<class 'bytes'>,))\nscda_ops_test: abort (trying to register op twice?)\n\n@sherrym, @danmane, can you take a look at the saver and tensorboard issues, respectively?\n\nsee details at\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-mac-python3/93/consoleFull\n", "I'll merge this now for the other fixes.\n", "merged.\n"]}, {"number": 1413, "title": "error when building from source", "body": "bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\nWarning: ignoring LD_PRELOAD in environment.\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:87:1: no such package '@grpc//': Error cloning repository: https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack caused by https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack caused by https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_channel'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 342.460s\n", "comments": ["@mrry: Looks like a distributed-related issue.\n", "Are you able to run `git clone https://boringssl.googlesource.com/boringssl` locally?\n", "#1387 might be relevant...\n", "I can run git clone https://boringssl.googlesource.com/boringssl locally, but in bazel that didn't work.\n\nMaybe bazel didn't honor the proxy env vars when download the deps of deps. (tensorflow -> grpc -> boringssl)\n", "@mrry @Vesnica I can only run it with proxy since I'm behind the firewall. However, when I run bazel with proxy, it seems cannot work\n", "Can you please try upgrading to the latest HEAD of Bazel? It looks like a fix was added for proxy support in https://github.com/bazelbuild/bazel/commit/d4c00d42b7ac3939f7041ba3bcb920088857adbd.\n", "Latest bazel didn't fix this problem:\n\n```\nERROR: /home/vesnica/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:158:1: no such package '@grpc//': Error cloning repository: https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack caused by https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack caused by https://boringssl.googlesource.com/boringssl: cannot open git-upload-pack and referenced by '//tensorflow/core/distributed_runtime/rpc:grpc_remote_master'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 161.436s\n```\n\nAll proxy settings is correct, and there is no issue if I simply run:\n\n```\ngit clone --depth 1 --recurse-submodules https://github.com/grpc/grpc.git\n```\n\n```\nCloning into 'grpc'...\nremote: Counting objects: 3401, done.\nremote: Compressing objects: 100% (2631/2631), done.\nremote: Total 3401 (delta 1667), reused 1305 (delta 596), pack-reused 0\nReceiving objects: 100% (3401/3401), 3.65 MiB | 81.00 KiB/s, done.\nResolving deltas: 100% (1667/1667), done.\nChecking connectivity... done.\nSubmodule 'third_party/boringssl' (https://boringssl.googlesource.com/boringssl) registered for path 'third_party/boringssl'\nSubmodule 'third_party/gflags' (https://github.com/gflags/gflags.git) registered for path 'third_party/gflags'\nSubmodule 'third_party/googletest' (https://github.com/google/googletest.git) registered for path 'third_party/googletest'\nSubmodule 'third_party/nanopb' (https://github.com/nanopb/nanopb.git) registered for path 'third_party/nanopb'\nSubmodule 'third_party/protobuf' (https://github.com/google/protobuf.git) registered for path 'third_party/protobuf'\nSubmodule 'third_party/zlib' (https://github.com/madler/zlib) registered for path 'third_party/zlib'\nCloning into 'third_party/boringssl'...\nremote: Sending approximately 23.14 MiB ...\nremote: Counting objects: 3021, done\nremote: Finding sources: 100% (424/424)\nremote: Total 23388 (delta 14683), reused 23347 (delta 14683)\nReceiving objects: 100% (23388/23388), 22.97 MiB | 327.00 KiB/s, done.\nResolving deltas: 100% (14683/14683), done.\nChecking connectivity... done.\nSubmodule path 'third_party/boringssl': checked out '9f897b25800d2f54f5c442ef01a60721aeca6d87'\n```\n", "@Vesnica I'd like to know whether just set http.proxy and https.proxy for git is enough to fetch google's boringssl repo?\n\nI set my proxy to git via:\n\n``` shell\ngit config --global http.proxy http://localhost:127.0.0.1:8118\ngit config --global https.proxy http://localhost:127.0.0.1:8118\n```\n\nAnd then run the command.\n\n``` shell\nGIT_TRACE_PACKET=1 GIT_TRACE=1 GIT_CURL_VERBOSE=1 git clone https://boringssl.googlesource.com/boringssl\n```\n\nAnd the clone failed, the output is like below:\n\n```\ntrace: built-in: git 'clone' 'https://boringssl.googlesource.com/boringssl'\nCloning 'boringssl'...\ntrace: run_command: 'git-remote-https' 'origin' 'https://boringssl.googlesource.com/boringssl'\n* Couldn't find host boringssl.googlesource.com in the .netrc file; using defaults\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to 127.0.0.1 (127.0.0.1) port 8118 (#0)\n* Establish HTTP proxy tunnel to boringssl.googlesource.com:443\n> CONNECT boringssl.googlesource.com:443 HTTP/1.1\nHost: boringssl.googlesource.com:443\nUser-Agent: git/1.9.1\nProxy-Connection: Keep-Alive\nPragma: no-cache\n\n< HTTP/1.1 503 Connect failed\n< Content-Length: 7690\n< Content-Type: text/html\n< Cache-Control: no-cache\n< Date: Thu, 10 Mar 2016 03:33:42 GMT\n< Last-Modified: Wed, 08 Jun 1955 12:00:00 GMT\n< Expires: Sat, 17 Jun 2000 12:00:00 GMT\n< Pragma: no-cache\n< Connection: close\n< \n* Received HTTP code 503 from proxy after CONNECT\n* Connection #0 to host 127.0.0.1 left intact\nfatal: unable to access 'https://boringssl.googlesource.com/boringssl/': Received HTTP code 503 from proxy after CONNECT\n```\n\nPS: I've tried to set HTTP_PROXY and HTTPS_PROXY environment variable for latest bazel, but it doesn't work.\n\n```\ntensorflow/WORKSPACE:39:1: no such package '@grpc//': Error cloning repository: https://boringssl.googlesource.com/boringssl: 503 Connect failed caused by https://boringssl.googlesource.com/boringssl: 503 Connect failed and referenced by '//external:grpc_cpp_plugin'.\nERROR: Loading failed; build aborted.\n```\n\nSo they all give me `503` as failed code. Is that the problem of my proxy? I use shadowsocks locally with Privoxy to redirect the http 8118 port to shadowsocks. I can confirm that the http proxy for my daily use is ok.\n", "@myme5261314 Make sure your ~/.gitconfig have this section:\n\n```\n[http]\n        proxy = PROXY_IP:PROXY_PORT\n```\n\nThis should be enough for all https git url.\n\nboringssl in bazel is another issue, which I believe not related to your proxy settings(git or system). \n", "I find my issue and have it fixed. My proxy has a configuration problem. And I fixed it. Now I can clone boringssl locally through git. But failed for boringssl in bazel. It's weird that they said the proxy improve commits had been merged but problem still exists about proxy in HEAD of bazel.\n", "I've notified grpc about this problem, and they've fixed it: https://github.com/grpc/grpc/issues/5853\nShould I submit a PR to update the grpc commit version in tensorflow to include this fix?\n", "I tried to use this new version of grpc to build tensorflow, Now I can clone it behind the wall.\nHowever there's another problem:\n\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nERROR: /home/wenjian/.cache/bazel/_bazel_wenjian/d25f05b3e081482e9605a49c384a6c89/external/grpc/BUILD:852:1: Label '@grpc//:src/cpp/common/core_codegen.h' is duplicated in the 'srcs' attribute of rule 'grpc++'.\nERROR: /home/wenjian/pkgs/tensorflow/tensorflow/workspace.bzl:84:3: Target '@grpc//:grpc_cpp_plugin' contains an error and its package is in error and referenced by '//external:grpc_cpp_plugin'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.176s\n", "Hi, I also have this problem. \n\nI'm trying to use `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` to build the package from source. And I met an error on `boringssl`. The error message is as follows:\n![1](https://cloud.githubusercontent.com/assets/8293993/14003987/dc05a9b4-f191-11e5-88aa-1022013d087f.png)\nAfter blocking with several minutes... the error is shown:\n![2](https://cloud.githubusercontent.com/assets/8293993/14003988/dc067cb8-f191-11e5-8171-698bb03f4e29.png)\nAfter checking the `grpc` repository [https://github.com/grpc/grpc.git], I found the `boringssl` submodule has been updated with the new mirror in [https://github.com/google/boringssl.git]. \nAfter cloning the `grpc` repository into my local disk, I try to find where the `boringssl` is needed. \n![3](https://cloud.githubusercontent.com/assets/8293993/14004056/4caa29b0-f192-11e5-9fb2-9e7324464bcc.png)\nIt seems nowhere for [https://boringssl.googlesource.com/boringssl] is needed. \nSo,could you please change the dependency of `grpc` for [https://boringssl.googlesource.com/boringssl] into [https://github.com/google/boringssl.git], completely?\n", "@zszhong https://github.com/tensorflow/tensorflow/issues/1387 this fix works for me\n", "@fayeshine have you successfully completed the build for distributed tensorflow? I hit different compile errors for each build after fix the boringssl issue.\n", "@shengjt Thank you very much! Problem solved!\n", "@zszhong have you passed the rest build process?\n", "@shengjt Yes, I have modified the `tensorflow/workspace.bzl` as follows:\n\n```\n    native.git_repository(\n      name = \"grpc\",\n      # commit = \"73979f4\",\n      commit = \"403cd6c\",\n      init_submodules = True,\n      # remote = \"https://github.com/grpc/grpc.git\",\n      remote = \"https://github.com/melody-rain/grpc.git\",\n     )\n```\n\nThen, use `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` to build the pip package, and install it into python packages. Then, the `tensorflow/models/image/mnist/convolutional.py` can be run correctly.  \n", "Another possible solution is:\n\n```\n    native.git_repository(\n        name = \"grpc\", \n        commit = \"3d62fc6\",\n        init_submodules = True,\n        remote = \"https://github.com/grpc/grpc.git\", \n     )\n```\n\nWhy `3d62fc6`? Because after this commit, the `boringssl` from the original googlesouce was changed into `https://github.com/google/boringssl.git`. See `https://github.com/grpc/grpc/commit/3d62fc68349a5ef31b9c2b0250818343bf9cca68#diff-8903239df476d7401cf9e76af0252622`.\n\nWhy not the latest commit? I have tried to set the value of `commit` as the latest commit, but it seems to still have some problems. \n\nI think the error in the main post is from the commit `73979f4`. In commit `73979f4`, the submodule of `boringssl` was still from googlesource. \n\nMaybe somebody could try this solution and submit a pull request into the master branch. \n", "@zszhong it seems we build different packages, I am trying to run: bazel build -c opt --verbose_failures //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server, the distributed tensorflow, but failed. \n", "There's no need to be so complicated to compile tensorflow via bazel.\nI've just compiled it successfully.\n\n```\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\nINFO: Elapsed time: 591.058s, Critical Path: 552.93s\n```\n\nJust use the bazel compiled from its HEAD instead of the released 0.2 version, and the command I use to compile tensorflow pip package is as follows:\n\n```\nHTTPS_PROXY=http://proxyip:proxyport ../bazel/output/bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nUse bazel compiled from HEAD!\n", "@myme5261314 , thanks, however many people may not have proxy, we could solve for all peoples by using a modified grpc commit version\n", "@fayeshine Yes, you are right, just tell somebody who has proxy but failed to compile tensorflow.\n\nIt will save a lot of time for the people who doesn't have a proxy.\n\nBut, I don't think the contributors of `boringssl` have the need to follow us transfer their coding place from google's site to github or this will waste their time, right?\n\nAnyway, I only know this workaround with proxy.\n\nI also want this useful library (tensorflow) to be easy to use for us who has been blocked from part of the Internet.\n\n:-)\n", "@myme5261314 actually they already have a boringssl in github, and grpc have used this github version boringssl in some commit version, see my pull request #1626  and @zszhong 's idea. Just modify one line and we don't need  proxy and we can build standalone and distributed version of tensowflow successfullt.\n", "@shengjt with #1626 , I can built the distributed version using your command as well.\n", "Hi, @fayeshine @myme5261314 @shengjt , I cloned the latest master branch [b1aeb4495bd10b3f1f0d4aed64880f8896fe990b] and modified the `tensorflow/workspace.bzl` with #1626, along with the following lines:\n\n```\n    # url = \"https://storage.googleapis.com/libpng-public-archive/libpng-1.2.53.tar.gz\",\n    url = \"http://denemo.org/~jjbenham/gub/downloads/libpng/libpng-1.2.53.tar.gz\",\n```\n\nNow the following binaries can be built successfully.\n\n```\n    bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n    bazel build -c opt --verbose_failures //tensorflow/core/distributed_runtime/rpc:grpc_tensorflow_server\n    bazel build -c opt --config=cuda //tensorflow/core/distributed_runtime/rpc:grpc_testlib_server\n    bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n    bazel build -c opt --config=cuda //tensorflow/libtensorflow.so\n    bazel build -c opt --config=cuda //tensorflow/core/ops/compat:update_ops\n    bazel build -c opt --config=cuda //tensorflow/examples/label_image:label_image\n```\n", "repalce \"https://boringssl.googlesource.com/boringssl\" by \"https://github.com/google/boringssl.git\" in tenserflow/tensorflow/workspace.bzl\n"]}, {"number": 1412, "title": "resize_image_with_crop_or_pad() on variable image sizes", "body": "I'm setting up what (I think) should be a straightforward image loading pipeline:\n\n``` python\ndecoded_image = tf.image.decode_jpeg(value)\nresized_image = tf.image.resize_image_with_crop_or_pad(decoded_image, width, height)\n```\n\nHowever, I get an error `'image' must be fully defined.` - the image not being fully defined because the size of the JPEG is variable. When using the `tf.image.resize_images()` function, it seems that this constraint is not enforced. Is it necessary to have fixed image dimensions to use the `resize_image_with_crop_or_pad()` function? That would seem to somewhat defeat the point of an auto-crop/resizing function, no?\n", "comments": ["@shlens: `resize_image_with_crop_or_pad` calls `Check3DImage` with `required_static=True`?  Is there a reason for that?  Fixing it would require changing a few routines to compute height and width via `tf.shape` at runtime rather that at graph construction time.\n", "@shlens: Feel free to remove your name and put contributions welcome if you want.\n\n@zplizzi: I think the only reason for this limitation is that all the existing uses appear as part of a larger image pipeline where the size is known.\n", "This looks like a duplicate of #521.\n", "Yep, closing.\n"]}, {"number": 1411, "title": "Mac OS;import error \"ImportError: cannot import name _message\"", "body": "### Environment info\n\nMAC OS\npython:\nPython 2.7.11 |Anaconda 2.5.0 (x86_64)| (default, Dec  6 2015, 18:57:58) \n[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n\ninstall command:\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n\nerror:\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"//anaconda/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"//anaconda/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 41, in <module>\n> > >     raise ImportError(msg)\n> > > ImportError: Traceback (most recent call last):\n> > >   File \"//anaconda/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 35, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"//anaconda/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n> > >     from google.protobuf import descriptor as _descriptor\n> > >   File \"//anaconda/lib/python2.7/site-packages/google/protobuf/descriptor.py\", line 46, in <module>\n> > >     from google.protobuf.pyext import _message\n> > > ImportError: cannot import name _message\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n\nbut I didn't start python at the tensorfliw source directory\n$ pwd\n/anaconda/bin\n\nHow to  solve the problem?\n", "comments": ["I got something like that\nOS: OS X El Capitan\nVirtualenv\npython 3.4.3\n\n```\nTraceback (most recent call last):\n  File \"/Users/paper/Desktop/qingyang/lib/python3.4/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Users/paper/Desktop/qingyang/lib/python3.4/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"cifar10_eval.py\", line 42, in <module>\n    import tensorflow.python.platform\n  File \"/Users/paper/Desktop/qingyang/lib/python3.4/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/paper/Desktop/qingyang/lib/python3.4/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/Users/paper/Desktop/qingyang/lib/python3.4/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Users/paper/Desktop/qingyang/lib/python3.4/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n", "Solved my problem by uninstalling tensorflow and protobuf, then reinstalling tensorflow.\nfrom https://github.com/tensorflow/tensorflow/issues/1262\n", "thks and I solved the problem.\n", "Thank very much! Though I had the same problem, reinstalling solved my problem too.\n", "Reinstalling doesn't solve my problem:(\n\n```\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"cifar10_cnn.py\", line 17, in <module>\n    from keras.models import Sequential\n  File \"/usr/local/lib/python3.5/site-packages/keras/models.py\", line 15, in <module>\n    from . import backend as K\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/__init__.py\", line 49, in <module>\n    from .tensorflow_backend import *\n  File \"/usr/local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\n    import tensorflow as tf\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: cannot import name 'descriptor'\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n", "Now I solved the problem by reinstalling protobuf using pip instead of homebrew which didn't work.\n"]}, {"number": 1410, "title": "tensorflor cond(pred, fn1, fn2) evaluate fn1 and fn2 together regardless of pred", "body": "```\nimport tensorflow as tf\nimport numpy as np\n\nisTrain = tf.placeholder(tf.bool)\nuser_input = tf.placeholder(tf.float32)\n\nwith tf.device('/cpu:0'):\n    alpha = tf.Variable(tf.zeros([1]))\n    beta = tf.Variable(tf.zeros([1]))\n\n    c = tf.Variable(tf.zeros([1]))\n\n    alpha_incre = alpha.assign(alpha + 1)\n    beta_incre = beta.assign(beta + 1)\n\n    def train():\n        with tf.control_dependencies([ alpha_incre ]):\n            return c.assign(user_input + user_input)\n\n    def test():\n        with tf.control_dependencies([ beta_incre ]):\n            return c.assign(user_input)\n\n    result = tf.cond(isTrain,\n        train,\n        test\n    )\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\naa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: True })\nprint(\"Train\", aa)\naa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: False })\nprint(\"Test\", aa)\n```\n### Environment info\n\nubuntu 14.04\n\nIf installed from binary pip package, provide:\ntensorflow 0.7.1\n### Steps to reproduce\n1. run the code example, expecting to see  \n\noutput \n- iteratio 1: to see alpha = 1, beta = 1 \n- iteratio 2: to see alpha = 2, beta = 2 \n\nexpected\n- iteratio 1: to see alpha = 1, beta = 0 \n- iteratio 2: to see alpha = 1, beta = 1 \n### What have you tried?\n1. Changing the code structure slightly will provide the expected output\n\n```\nimport tensorflow as tf\nimport numpy as np\n\nisTrain = tf.placeholder(tf.bool)\nuser_input = tf.placeholder(tf.float32)\n\nwith tf.device('/cpu:0'):\n    alpha = tf.Variable(tf.zeros([1]))\n    beta = tf.Variable(tf.zeros([1]))\n\n    c = tf.Variable(tf.zeros([1]))\n\n    alpha_incre = alpha.assign(alpha + 1)\n    beta_incre = beta.assign(beta + 1)\n\n    def train():\n        with tf.control_dependencies([ alpha.assign(alpha + 1) ]):\n            return c.assign(user_input + user_input)\n\n    def test():\n        with tf.control_dependencies([ beta.assign(beta + 1) ]):\n            return c.assign(user_input)\n\n    result = tf.cond(isTrain,\n        train,\n        test\n    )\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\naa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: True })\nprint(\"Train\", aa)\naa = sess.run([result, alpha, beta], feed_dict={user_input:[2], isTrain: False })\nprint(\"Test\", aa)\n```\n", "comments": ["If you want something to happen conditionally, it has to go _inside_ the `tf.cond`, so this is expected behavior.  The analogy is\n\n```\nalpha += 1\nbeta += 1\nif pred:\n  ...\n```\n\nBoth `alpha` and `beta` will be incremented regardless of `pred`.\n"]}, {"number": 1409, "title": "Image ops do not all accept NumPy arrays as arguments", "body": "From [Stack Overflow](http://stackoverflow.com/q/35824798/3574081):\n\n> When I run the line :\n> \n> `flipped_images = tf.image.random_flip_left_right(images)`\n> I get the following error :\n> \n> `AttributeError: 'numpy.ndarray' object has no attribute 'get_shape'`\n> My Tensor \"images\" is an ndarray (shape=[batch, im_size, im_size, channels]) of \"batch\" ndarrays (shape=[im_size, im_size, channels]).\n\nOn looking at the code, this happens because our shape checks expect arguments to already have been converted to a `tf.Tensor`. We should call `ops.convert_to_tensor()` on them before performing the check.\n", "comments": ["Same for `tf.keras.activations.softmax()`?"]}, {"number": 1408, "title": "Generalized matrix multiplication with semiring?", "body": "Would it be possible to support (generalized) matrix multiplication with non-standard semirings? \n\nIn matrix multiplication, we have `A times B = C`, where `C(i,j) = sum_k A(i,k) times B(k,j)`\n\nUsing a different semiring practically means redefining the `plus` and `times` operations. \n\nFor example, the log semiring assumes that all numbers in the matrices are log numbers and redefines `plus` as `logplus` (aka [logaddexp](http://docs.scipy.org/doc/numpy/reference/generated/numpy.logaddexp.html#numpy.logaddexp) or [logsumexp](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.misc.logsumexp.html)) and `times` as `plus`. This is useful for computing the log denominator in log-linear models, e.g., `log Z = log sum_y exp(f1(y)) times exp(f2(y))`.\n\nThe [max-plus semiring](https://en.wikipedia.org/wiki/Max-plus_algebra) (aka Viterbi or tropical semiring) redefines `plus` as `max` and `times` as `plus`. This is useful for finding the best path (assuming all matrix entries are log numbers).\n", "comments": ["I think an optimized (blocked, etc.) implementation of semiring matmul is out of scope for tensorflow at the moment, since it would require either code generation or a massive amount of template instantiation.  Of course, for small sizes it can be implemented by writing down all the terms via broadcasting (producing a cubic size tensor) and then reducing, but that's not very interesting.\n\nIt's possible that TensorFlow may be better at this kind of flexibility at some point, so I'll leave this bug open.\n", "Templating `matmult` on a semiring (similar to composition and others in OpenFst) might be too big of a change. But maybe one could just add a few new matrix multiplication functions for particular popular semirings, e.g., `matmult_log`, `matmult_maxplus`, etc.\n", "If a couple particular semirings capture most of the value, this seems quite reasonable.  @benoitsteiner: Can Eigen be convinced to give optimized semiring matrix multiply? \n", "Computing log Z can be rewritten in terms of regular matrix multiplies, but Viterbi is an interesting example. You need efficient way to  compute matrix vector min-plus product r=Av as  r(i) = min_j A(i,j) + v(j)\n", "@yaroslavvb Yes, rewriting the computation of log Z in terms of regular matrix multiplies is possible by explicitly exponentiating all entries in the matrices, but that can be expensive and also prone to underflow problems.\n", "Closing since I think this is out of reach of easy contributions.  A trivial implementation is trivial, but users are likely to want fast versions that are hard to write."]}, {"number": 1407, "title": "C++ documentation broken", "body": "None of the links on this page work:\n\nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html\n", "comments": ["@danmane: Do you know what the issue is here? \n", "You are using a lower case after the final / , but that should be an upper case.\n\nThe link is: https://www.tensorflow.org/versions/r0.7/api_docs/cc/classEnv.html\n\nBut it should be: https://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassEnv.html\n", "It applies to all the links on the page. These are the correct ones:\n\ntensorflow::Env\nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassEnv.html \n\ntensorflow::RandomAccessFile \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassRandomAccessFile.html \n\ntensorflow::WritableFile \nhttps://github.com/delftswa2016/tensorflow/blob/master/tensorflow/g3doc/api_docs/cc/ClassWritableFile.md \n\ntensorflow::EnvWrapper \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassEnvWrapper.html\n\ntensorflow::Session \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassSession.html\n\ntensorflow::SessionOptions \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/StructSessionOptions.html \n\ntensorflow::Status \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassStatus.html \n\ntensorflow::Status::State \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/StructState.html\n\ntensorflow::Tensor \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassTensor.html\n\ntensorflow::TensorShape \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassTensorShape.html \n\ntensorflow::TensorShapeDim \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/StructTensorShapeDim.html \n\ntensorflow::TensorShapeUtils \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassTensorShapeUtils.html \n\ntensorflow::PartialTensorShape \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassPartialTensorShape.html \n\ntensorflow::PartialTensorShapeUtils \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassPartialTensorShapeUtils.html \n\nTF_Buffer \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/structTF_Buffer.html\n\ntensorflow::Thread \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/ClassThread.html\n\ntensorflow::ThreadOptions \nhttps://www.tensorflow.org/versions/r0.7/api_docs/cc/StructThreadOptions.html \n"]}, {"number": 1406, "title": "[feature request] disable harmful broadcast", "body": "Broadcast may lead to hidden bugs. For example, the following code,\n\n```\nxent = tf.nn. sigmoid_cross_entropy_with_logits(logits, y)\nloss = tf.reduce_mean(xent)\n```\n\nWhen `logits` is of the shape `(200,1)` and the `y` is of the shape `(200,)`, the xent becomes `(200, 200)`, which is wrong. What's worse, the `reduce_mean` will hide this problem by reducing everything into a scalar.\n\nRequest,\n1. more shape validation in `sigmoid_cross_entropy_with_logits`\n2. option to totally disable broadcast\n", "comments": ["1. is done: https://github.com/tensorflow/tensorflow/blob/3871973b302b3c8357a4984fb37d934cf4633f21/tensorflow/python/ops/nn.py#L287 -- will be in our next binary release.\n", "Thanks. I guess the second is impossible. so closing.\n", "Nothing is impossible, it would just take a lot of work to plumb that through everywhere, to set the option globally, and for everything to work as well without broadcasting.  For example, when computing gradients, automatic broadcasting is quite useful to simplify gradient expressions, particularly when the shapes are unknown at construction time.  I've seen similar requests made for numpy, and it sounds like higher level libraries typically provide the 'safety' for those users who want it.  However, I'm open to the idea if it can be done in a nice way.\n", "I have spent 8 hours on exactly this bug. An option to disable broadcasting globally or raising a warning, etc. would be very nice.", "Just ran into a bug which would also have been prevented by warning on broadcasting. A `a * b` operation ended up doing `vector * vector -> matrix` multiplication instead of `vector * vector -> vector` elementwise multiplication. Cost me on the order of tens of hours ([writeup](https://agentydragon.com/posts/2021-10-18-rai-ml-mistakes-3.html)). Would be nice to either have the option for such a warning, or at least more specific operators that require the programmer to be more explicit about intent. (e.g.: `tf.math.elementwise_multiply`)."]}, {"number": 1405, "title": "Shouldn't tensorboard warn you that not all summaries have loaded yet?", "body": "Hi,\n\nsince I started using Tensorboard, I noticed some odd behavior that sometimes I'd open it and it seemed like a lot of the summary values were missing, i.e, even though it's already at 10k steps, the summary only shows 2k steps or so. Today I actually finally noticed that it simply meant that it was just still being loaded. I refreshed the page after a minute or so and all data was there. My PC is not bad (i5-2500k) so this was quite surprising. Any reason why the summaries sometimes take so long to load? Also can we get some sort of warning that not all summaries have been completely loaded yet?\n", "comments": ["That would be a really nice enhancement. We're planning to improve all the indications around when TensorBoard is loading, have it automatically refresh data, etc. I don't have a timeline, though. \n", "Hi there\u2014I've just migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/65. Feel free to submit a PR there!"]}, {"number": 1404, "title": "DO NOT MERGE: Replaced python configuration by a Skylark Remote Repository", "body": "This is just an example on how to delete the ./configure script\nwith Skylark Remote Repository (available in Bazel @HEAD).\n\nThis was tested by building the pip package on OS X but should\nbe tested further on various platform.\n\nProtobuf fast python header is deactivated because protobuf\nneeds to be updated to work correctly with that.\n\nIt should be pretty straightforward to do the same things for\ncuda too.\n", "comments": ["Can one of the admins verify this patch?\n", "/cc @martinwicke This is an example on how to get rid of the ./configure script.\n\n/cc @kirilg @keveman, with that kind of mechanism, having tensorflow as a git submodule will no longer be required as the configuration step can be called from the parent repository\n", "closing for now, this is a useful reference for the future when we decide to use this mechanism for configuring.\n", "For my reference, here is the tracking bug for CUDA autoconf: #2873\n"]}, {"number": 1403, "title": "Fix types image ops tests", "body": "Thanks to a recent comment by @martinwicke I realized that there's something wrong with the tests of `test_image_ops.py`. Combined with the use of `use_gpu` instead of `force_gpu`, the tests that were supposed to test the GPU implementation actually ran the CPU implementation. \n\nAfter rectifying this, two bugs were found in the GPU implementation and corrected. \n", "comments": ["Can one of the admins verify this patch?\n", "`force_gpu` will fail the test if there is no GPU available (e.g. if you compile without cuda, or if you don't have a supported GPU). It's really for debugging, not particularly useful in tests. The absence of something in between of `force_gpu` and `use_gpu` is a bit of a bug: we do need something that will fail only if there is a GPU, and TF is compiled with --config=cuda, but it's not used.\n", "I see, thanks for your insight. I changed the tests back to using `use_gpu`. I agree, it would really be useful to have a flag along the lines of `force_gpu_if_available`. \n", "I made #1423 to track this.\n", "Jenkins, test this please.\n", "hmm... not sure what happened with the tests. Checking.\n", "For me it looks like it's properly executing the tests...\n", "Yeah, it was just waiting in the queue.\n", "@martinwicke I do get saver_test problems on my python3.4.3 (fedora) also.  I was going to take a look, but maybe you'll get to it before I will.\n", "@chemelnucfin is that concerned to this PR?\n", "@panmari, I don't think so, but I noticed the mac test failing.\n", "The mac/python3 tests had been failing already, so I merged anyway. \n"]}, {"number": 1402, "title": "Error importing tensorflow.  Unless you are using bazel, you should not try to import tensorflow from its source directory", "body": "These are the sequence of steps i followed to install the distributed version of TF\n\n> > git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n> > ./configure (default with GPU)\n> > bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n> > bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu (at this point saw the expected output and GPU being used)\n> >  bazel build -c opt --config=cuda --define=use_fast_cpp_protos=true //tensorflow/tools/pip_package:build_pip_package\n> > bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n> > sudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl \n> > Processing /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\n> > Installing collected packages: tensorflow\n> > pip show tensorflow\n> > Metadata-Version: 2.0\n> > Name: tensorflow\n> > Version: 0.7.1\n> > Summary: TensorFlow helps the tensors flow\n> > Home-page: http://tensorflow.org/\n> > Author: Google Inc.\n> > Author-email: opensource@google.com\n> > License: Apache 2.0\n> > Location: /usr/local/lib/python2.7/dist-packages\n> > Requires: six, protobuf, wheel, numpy\n> > Classifiers:\n> >   Development Status :: 4 - Beta\n> >   Intended Audience :: Developers\n> >   Intended Audience :: Education\n> >   Intended Audience :: Science/Research\n> >   License :: OSI Approved :: Apache Software License\n> >   Programming Language :: Python :: 2.7\n> >   Topic :: Scientific/Engineering :: Mathematics\n> >   Topic :: Software Development :: Libraries :: Python Modules\n> >   Topic :: Software Development :: Libraries\n> > Entry-points:\n> >   [console_scripts]\n> >   tensorboard = tensorflow.tensorboard.tensorboard:main\n\nNow for the error\n\n> > python\n> > Python 2.7.9 (default, Apr  2 2015, 15:33:21) \n> > [GCC 4.9.2] on linux2\n> > Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n> > \n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 41, in <module>\n> > >     raise ImportError(msg)\n> > > ImportError: Traceback (most recent call last):\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 35, in <module>\n> > >     from tensorflow.core.framework.graph_pb2 import *\n> > >   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n> > >     from google.protobuf import descriptor as _descriptor\n> > > ImportError: No module named protobuf\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n\nCan someone help me understand what might cause this? Thank you.\n", "comments": ["You should start your python outside tensorflow directory. Because python will import tensorflow from current directory.\n", "I had the same errors as you did. I solved this by reinstall protobuf, by `pip install -U protobuf==3.0.0b2`\n", "@shiyemin yes of course I started the interpreter outside the tensorflow directory\n\n@raingo I tried that but in python interpreter I still get same error. Do you have to do a bazel build for tensorflow by pass in this protobuf library into the build command?\n", "Yes. I built from source, but I didn't do anything special for bazel. I\nneeded to upgrade protobuf manually because I install the wheel by\n\npip install wheel --no-deps -U\n\nSent from a mobile device\nOn Mar 6, 2016 3:45 PM, \"ushnish\" notifications@github.com wrote:\n\n> @shiyemin https://github.com/shiyemin yes of course I started the\n> interpreter outside the tensorflow directory\n> \n> @raingo https://github.com/raingo I tried that but in python\n> interpreter I still get same error. Do you have to do a bazel build for\n> tensorflow by pass in this protobuf library into the build command?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1402#issuecomment-192984806\n> .\n", "@raingo did you do **git clone --recurse-submodules https://github.com/tensorflow/tensorflow** or clone without the submodules since you are installing protobuf separately\n", "Uninstall protobuf before building (or dl whl), everything works fine after that. Had the same issue.\n\nsudo pip uninstall protobuf\n", "@Dringite do you mean uninstall protobuf, build from the .whl and then install protobuf the way @raingo mentioned? \n", "Problem solved @Dringite you were right, there was no need to have protobuf installed at all. The steps are\n\n> > git clone --recurse-submodules https://github.com/tensorflow/tensorflow \n> > sudo pip uninstall protobuf\n> > cd tensorflow/\n> > ./configure \n> > bazel build -c opt --config=cuda **--define=use_fast_cpp_protos=true** //tensorflow/tools/pip_package:build_pip_package\n> > bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tenso\n> > rflow_pkg\n> > sudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl \n\nNotice that i added --define=use_fast_cpp_protos=true in the build command so it might have actually helped.\n", "@ushnish I have followed your way to install tensorflow-0.7.1. In the future if I want to upgrade the version of tensorflow, is it necessary for me to follow your way to install new version? Or is there any other convenient ways?\n", "Tensorflow 0.9.0 also suffers from the same issue.   The answer from @Dringite can solve the issue.  (i.e., pip uninstall protobuf and tensorflow, then pip install tensorflow)\n", "Tensorflow 0.10.0 also suffers from the same issue. The answer from @raingo can solve the issue. (i.e. `pip uninstall protobuf`, then `pip install wheel --no-deps -U`)\n", "According to the above method Still not solve\r\nubuntu 64\r\ntensorflow-0.12.0rc0-cp27-cp27mu-manylinux1_x86_64.whl\r\n\r\n`Python 2.7.3 (default, Jun 22 2015, 19:33:41) \r\n[GCC 4.6.3] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/tg2env/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/root/tg2env/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/root/tg2env/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/root/tg2env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/root/tg2env/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.16' not found (required by /root/tg2env/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n`", "I'm running Ubuntu 14.04.5 LTS with `python==2.7`, `tensorflow==0.12.0rc0` (most recent in PyPy) and keras==1.1.0.\r\n\r\nA quick hack to get you by:\r\n```\r\nsudo vim /usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\r\n```\r\nand replace `from tensorflow.python import pywrap_tensorflow` with `import pywrap_tensorflow` around line 49 in the try portion of the try/catch.", "I got a similar error. It was solved when I run python from the parent directory of tensorflow. But I'm curious if I can import tensorflow from another directory? Because when I do, it shows this error.\r\n\"Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\"", "@baregawi Is this a new behavior in 0.12? \r\n\r\n@NewRem how did you install TensorFlow?\r\n\r\n@yifeif It obviously looks fine in our CI, any idea?", "Looks like it is related to this line when running tensorflow from source dir, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/__init__.py#L49\r\n\r\nThis should be resolved by not running from the source dir.\r\n@NewRem also curious how you installed tensorflow. You should be able to import tensorflow from a non-repo-source directory. \r\n", "Thank you all, I solved the problem by upgrading the ubuntu", "@martinwicke @yifeif I used pip installation. To be exact, I used these three commands:\r\n\r\n$ sudo easy_install pip\r\n$ sudo easy_install --upgrade six\r\n\r\n#the CPU version of the binaries to Pypi\r\n$ sudo pip install --ignore-installed tensorflow\r\n", "@martinwicke I just `pip install -U tensorflow` after not having updated since September. But I remember having to do a similar hack in September: https://github.com/tensorflow/tensorflow/issues/4616", "@baregawi #4616 is different, we are removing spurious symbols from our packages, so a lot of these direct imports will stop working. \r\n\r\nThis issue is different, and in the past, this has pointed to tensorflow now being properly installed. It's hard to debug remotely though. @ushnish how many pythons do you have installed and which one is tensorflow installed in?\r\n\r\nCan you install tensorflow in a virtualenv? If that works, then there's some problem with conflicting versions, either python/ipython or more than one tensorflow installation.\r\n", "@martinwicke I installed **tensorflow_gpu-0.12rc1-py2-none-any.whl** step by step via the instruction of [official site](https://www.tensorflow.org/versions/master/get_started/os_setup.html#protobuf-library-related-issues). \r\n\r\nexecute this command: `sudo pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc1-py2-none-any.whl`\r\n\r\n```\r\nCollecting tensorflow-gpu==0.12.0rc1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc1-py2-none-any.whl\r\n  Downloading https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc1-py2-none-any.whl (83.6MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 83.6MB 7.5kB/s\r\nCollecting mock>=2.0.0 (from tensorflow-gpu==0.12.0rc1)\r\n  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 33kB/s\r\nCollecting numpy>=1.11.0 (from tensorflow-gpu==0.12.0rc1)\r\n  Downloading numpy-1.11.2-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.9MB 16kB/s\r\nCollecting protobuf==3.1.0 (from tensorflow-gpu==0.12.0rc1)\r\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348kB 16kB/s\r\nCollecting wheel (from tensorflow-gpu==0.12.0rc1)\r\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 19kB/s\r\nCollecting six>=1.10.0 (from tensorflow-gpu==0.12.0rc1)\r\n  Downloading six-1.10.0-py2.py3-none-any.whl\r\nCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow-gpu==0.12.0rc1)\r\n  Downloading funcsigs-1.0.2-py2.py3-none-any.whl\r\nCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow-gpu==0.12.0rc1)\r\n  Downloading pbr-1.10.0-py2.py3-none-any.whl (96kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 28kB/s\r\nCollecting setuptools (from protobuf==3.1.0->tensorflow-gpu==0.12.0rc1)\r\n  Downloading setuptools-32.0.0-py2.py3-none-any.whl (477kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481kB 30kB/s\r\nInstalling collected packages: six, funcsigs, pbr, mock, numpy, setuptools, protobuf, wheel, tensorflow-gpu\r\nSuccessfully installed funcsigs-1.0.2 mock-2.0.0 numpy-1.11.2 pbr-1.10.0 protobuf-3.1.0 setuptools-32.0.0 six-1.10.0 tensorflow-gpu-0.12.0rc1 wheel-0.29.0\r\n```\r\nBut unfortunately, I got the same error as following output:\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: dlopen(/Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): no suitable image found.  Did find:\r\n\t/Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: mach-o, but wrong architecture\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n>>> exit()\r\n```\r\nThe version of mac system: `OS X EI Captain`\r\nAnd I had try each ways above this issues list, but nothing change.\r\n\r\nThank u 4 any response.", "@R0rs12ach:\r\nThe hint is in this line: \r\nDid find: /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: mach-o, but wrong architecture\r\n\r\nAre you installing this on PPC? If so, you'll have to install from source, our binary packages only support x86_64.", "@R0rs12ach: i had the same problem using mac OS X Yosemite.\r\ni have tried everything written here but it didn't help.\r\nAt last i went back to the homepage of tensor flow and tried other installation methods. After that i have followed 'Anaconda installation' -> 'Using conda' i finally could run it properly. Not sure which action i have taken helped.\r\n\r\nSo far i have tried:\r\n```\r\n$ sudo easy_install pip\r\n$ sudo easy_install --upgrade six\r\n\r\n$ sudo pip install --upgrade $TF_BINARY_URL\r\n```\r\n\r\n\r\n```\r\npip uninstall protobuf\r\npip install wheel --no-deps -U)\r\n\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow\r\nsudo pip uninstall protobuf\r\ncd tensorflow/\r\n./configure\r\nbazel build -c opt --config=cuda --define=use_fast_cpp_protos=true //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tenso\r\nrflow_pkg\r\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-any.whl\r\n\r\n```\r\n\r\nand as said, instead of install with pip i have made it with Anaconda installation from trensorflow homepage: https://www.tensorflow.org/get_started/os_setup#anaconda_installation\r\n\r\nHope it will help you.\r\n", "@martinwicke I think my computer's arch is :\r\n\r\n```\r\nDarwin MacBook-Pro.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 23 18:25:34 PDT 2016; root:xnu-3248.60.10~1/RELEASE_X86_64 x86_64\r\n```\r\nthe fact is that my computer just x86_64\r\n\r\n@MeilingShi : Thank u 4 ur advice, I will try it and paste the result", "@MeilingShi I have tried what you said above, but nothing change. In fact, the cuda sdk do not support intel graphic 4000. So, i give up to install tensorflow_gpu_version on my mac which bought at 2013.", "@R0rs12ach yes, CUDA only works with NVIDIA graphics cards. \r\n\r\nIt's still confusing me that it would complain about the library architecture. \r\n\r\nOthers, I would like to understand this issue, but this thread has become too noisy. Whoever still has this problem, can you open a new issue with your system config according to the template?", "I follow the official pip windows installation and get same error as others.\r\n\r\nI also tried uninstalling protobuf etc beforehand to no avail. I'm running python 3.5.2 and Anaconda. \r\n\r\nFinally, I have also made sure to install Visual C++ x64 version and that MSVCP140.DLL is in my PATH.\r\n\r\nIs everyone having this problem on Windows?\r\n\r\n`C:\\Users\\Rafie>python\r\nPython 3.5.2 |Anaconda 4.2.0 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cublas64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:2294] Unable to load cuBLAS DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3459] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cufft64_80.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_fft.cc:344] Unable to load cuFFT DSO.\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library nvcuda.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.cc:165] hostname: \u00b09l\u251c\u00a0\u007f\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.`\r\n\r\nNOTE: the CPU version works, but not GPU version (above). ", "my environment:\r\ncentos6.5 x86_64\r\npython 2.7\r\n\r\nI use pip install tensorflow version1.2 succuessfully. However I enter python and import tensorflow, I met the error like above. I try above solution, but it still failed.\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.16' not found (required by /data/apps/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\n\r\nAnyone has ideas for it? Thanks very much!", "The binaries we provide do not work on CentOS 6.5, because the system libraries are too old. You will have to build from source. ", "I really have the same error. but when i update my tensorflow, it's resolved.\r\nsudo yum install python-pip python-dev\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl\r\nsudo pip install --upgrade $TF_BINARY_URL\r\n\r\n", "I have the same problem on fedora 25. I tried all the proposed solution. It works for tensorflow, but not for tensorflow-gpu", "Same problem: \"Error importing tensorflow. Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\"\r\n\r\nTried building from source, installing through pip. All the same. I will just move to theano", "@rodrigomfw can you paste the error before that message? For some people on this thread it's that they have very old systems (CentOS 6.x) which don't support the pre-built binaries. For you it's probably something else since it only affects GPU. \r\n\r\n@vrv, I think we should rephrase this error. Most of the time (100% of the time in this thread), this is a failure to import unrelated to the working directory. ", "Yup, something changed and we now raise ImportError much more often, so the error message is unhelpful.", "I just uninstalled tensorflow and tensorflow-gpu both and then I just installed tensorflow-gpu... works perfectly fine for me", "Facing this error when installing tensorflow on windows 8 with python 3.5 and importing it from IDLE. I think it is the same issue.\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python_35_2_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\python_35_2_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#0>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\python_35_2_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\python_35_2_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\python_35_2_64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\n\r\n\r\n", "pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp27-none-macosx_10_11_x86_64.whl\r\n", "I did everything above but I always get this: \r\n\r\n> Using TensorFlow backend.\r\n> RuntimeError: module compiled against API version 0xa but this version of numpy is 0x9\r\n> Traceback (most recent call last):\r\n>   File \"run.py\", line 1, in <module>\r\n>     import lstm\r\n>   File \"/Users/me/Documents/dev/LSTM/lstm.py\", line 5, in <module>\r\n>     from keras.layers.core import Dense, Activation, Dropout\r\n>   File \"/Library/Python/2.7/site-packages/Keras-1.2.0-py2.7.egg/keras/__init__.py\", line 2, in <module>\r\n>     from . import backend\r\n>   File \"/Library/Python/2.7/site-packages/Keras-1.2.0-py2.7.egg/keras/backend/__init__.py\", line 67, in <module>\r\n>     from .tensorflow_backend import *\r\n>   File \"/Library/Python/2.7/site-packages/Keras-1.2.0-py2.7.egg/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n>     import tensorflow as tf\r\n>   File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n>     _pywrap_tensorflow = swig_import_helper()\r\n>   File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n>     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n> ImportError: numpy.core.multiarray failed to import\r\n> \r\n> \r\n> Error importing tensorflow.  Unless you are using bazel,\r\n> you should not try to import tensorflow from its source directory;\r\n> please exit the tensorflow source tree, and relaunch your python interpreter\r\n> from there.", "@gistya https://www.tensorflow.org/get_started/os_setup", "I got over the problem by reinstalling cuda 8.0 and cudnn 5.1", "Maybe this can help you when you got this \"Error importing tensorflow. Unless you are using bazel, you should not try to import tensorflow from its source directory\"  https://github.com/tensorflow/tensorflow/issues/559", "I am also facing the same issues, wondered if anyone has resolved it? But my error is slightly different  **ImportError: numpy.core.multiarray failed to import**\r\nI initially when installing with pip it had the error and i had to add --ignore-installed six to make the installation work, really not sure if that would affect anything. \r\n\r\nBut i have tried all the solutions mentioned above, and still facing that problem \"Error importing tensorflow.\"", "try this one\r\nhttps://github.com/tensorflow/tensorflow/issues/559\r\nfrom author se7en007 commented on 4 Jan 2016\r\nimport numpy as np\r\nnp.path\r\n['/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy']\r\nexit()\r\nsudo rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy\r\n...\r\n", "If anyone had the same problem as mine, i managed to fix it. My error was\r\n **ImportError: numpy.core.multiarray failed to import\r\nError importing tensorflow. Unless yo......**\r\n\r\nBecause i am running on a Mac, there are 2 sets of numpy thats running at the same time, as the Mac came with Python installed, so even if you are updating the numpy, it wasnt updating the one thats needed for Tensorflow. I believe i ran sudo pip install numpy -U --upgrade or something like that so all versions of numpy would be upgraded within my Mac. Then it worked after that. ", "What does it mean to be in the \"source directory\". I am also having the same issue however, I have no idea what the source directory is.", "seems no solution yet. I am installing gpu version on windows 10. The installation seems ok at least no error or warning. \r\n\r\npip install --ignore-installed --upgrade C:\\Users\\dbsnail\\Downloads\\tensorflow_gpu-1.0.0rc2-cp35-cp35m-win_amd64.whl\r\n\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Anaconda2\\envs\\py35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "I got the solution, install cuDNN v5.1 for CUDA 8.0\r\nYou need to register a NVIDIA accelerated computing developer program, then from here (https://developer.nvidia.com/rdp/cudnn-download) download and unzip it.\r\nCopy all three subfolders and past it under CUDA folder, in my case is\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\r\n\r\nThen , you should be able to load tensorflow without any error\r\n\r\nNOTE, I use python 3.5", "I first installed wheel, then the tensorflow was installed on my machine(using windows 10).\r\n\r\nNow, it is not getting imported here is the error :\r\n  \r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 29, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/mzm/PycharmProjects/mzm/FAQ bot/rough.py\", line 36, in <module>\r\n    from keras.models import Sequential, Model\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import activations\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\keras\\activations.py\", line 4, in <module>\r\n    from . import backend as K\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\", line 73, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 29, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\mzm\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\n\r\nDoes any know how to fix it??\r\n", "Guys its simple,\r\nCome out of the tensorflow environment\r\nlike - \r\nsource deactivate tensorflow\r\nthen try your tensor board \r\nEx:    tensorboard --logdir=/tmp/tboard/output\r\n                                         Worked for me : )\r\n", "https://github.com/ravivalluri/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import-error\r\nThere is a explain with this problem.", "I am new to TensorFlow and I am facing the same issue. Kindly suggest me to get started with this journey as soon as possible.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: %1 is not a valid Win32 application.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\WorkspaceMaven\\A-python\\Test.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: %1 is not a valid Win32 application.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Users\\SEM\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n", "Hi, I got the same issue: \r\n```\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```\r\nI do not know where or what is tensorflow source tree, or where is tensorflow source directory. I just followed the instructions here - https://www.tensorflow.org/install/install_windows and now trying to follow this - https://www.tensorflow.org/get_started/get_started\r\n\r\nThis was exactly after following the getting started guide and trying a import tensorflow as tf.\r\nWondering why is this issue closed when so many people are still facing the same.\r\nI couldn't find the solution anywhere on this thread or on stackoverflow.\r\n\r\nPlease help!!", "Hi Abhishek,\nI am still not resolved with this issue if you find potential fixes than\nplease let me know.\n\nOn Sep 12, 2017 1:50 PM, \"Abhishek\" <notifications@github.com> wrote:\n\n> Hi, I got the same issue:\n>\n> Error importing tensorflow.  Unless you are using bazel,\n> you should not try to import tensorflow from its source directory;\n> please exit the tensorflow source tree, and relaunch your python interpreter\n> from there.\n>\n> This was exactly after following the getting started guide and trying a\n> import tensorflow as tf.\n> Wondering why is this issue closed when so many people still have the\n> issue.\n> I couldn't find the solution anywhere on this thread or on stackoverflow.\n>\n> Please help!!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1402#issuecomment-328778159>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ANfiFJR4BmqpKC3Uogd0IgvwdIfrMYv2ks5shj7YgaJpZM4HqPp5>\n> .\n>\n", "Oh,my solution is upgrading the pip to latest version\uff0c then use it to reinstall tensorflow ", "lol, you guys are just naive! It's clearly written that you cant import tensorflow on the terminal itself, C'mon go and create a python file and then compile it silly!", "This thread seems somewhat old, but just to share the problem I had and how I solved it.\r\n\r\nI first tried to import keras, but I faced the same error on this issue title. \r\n\r\nI uninstalled protobuf and updated tensorflow. It worked.\r\n\r\n```\r\n$ pip uninstall protobuf\r\n$ pip install -U tensorflow\r\n```", "1.Download Python 3.5.2 (3.5.2 only!!)\r\n2.In command prompt : pip install tensorflow \r\n(ignore this --Cache entry deserialization failed, entry ignored)\r\n3.:python\r\n4.>>import tensorflow as tf (\r\nif no error then successfully installed\r\n5.>>print(tf.__version__) \r\n(use double underscore above)\r\n![Screenshot (205)](https://user-images.githubusercontent.com/48626909/59749961-9fcb5580-929b-11e9-88de-7debe7c02f0d.png)\r\n)"]}, {"number": 1401, "title": "Error in string_input_producer", "body": "### Environment info\n\nOperating System: Ubuntu 15.04\n\nIf installed from sources, provide the commit hash: c1a40c7\n### Steps to reproduce\n\nRun the following commands:\n\n```\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nfilenames = [\"1\", \"2\", \"3\"]\nfilename_queue = tf.train.string_input_producer(filenames)\ntest_value = tf.convert_to_tensor(filename_queue.size())\nprint(sess.run([test_value]))\n```\n\nFor me, this last command returns `[0]`. If I'm understanding things right, I'm assuming it should return `[3]`. I've also tried to run the dequeue() op, which appears to hang and become unresponsive. \n\nIt's also possible I'm just completely misunderstanding how this is supposed to work - in which case, please disregard!\n", "comments": ["Nevermind, it turns out the command `tf.train.start_queue_runners(sess=sess)` must be run before the queue gets filled. Works as expected when this is done.\n"]}, {"number": 1400, "title": "Fix raising wrong exception.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Good catch. Clearly, this wasn't properly tested (that line never executed), could you add a unit test for the failure behavior?\n", "Oh, in this case, I'm afraid that it might be too much work(lines) for this tiny error. :-)\n\nBy the way, is `Tensorflow` tested by any coverage test tool like `Coverity` now?\n", "Not coverity, but we do measure coverage. It was around 96% or so last time I checked.\n\nI'll merge this.\n", "Thank you, @martinwicke !\n"]}, {"number": 1399, "title": "Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape'", "body": "**Intro**\n\nI am using a modified version of the Tensorflow tutorial \"Deep MNIST for experts\" with the Python API for a medical images classification project using convolutionnal networks.\n\nI want to artificially increase the size of my training set by applying random modifications on the images of my training set.\n\n**Problem**\n\nWhen I run the line : \n\n`flipped_images = tf.image.random_flip_left_right(images)`\n\nI get de following error :\n\n> AttributeError: 'numpy.ndarray' object has no attribute 'get_shape'\n\nMy Tensor \"images\" is an ndarray (shape=[batch, im_size, im_size, channels]) of \"batch\" ndarrays (shape=[im_size, im_size, channels]).\n\nJust to check if my input data was packed in the right shape and type, I have tried to apply this simple function in the (not modified) tutorial \"Tensorflow Mechanics 101\" and I get the same error.\n\nFinally, I still get the same error trying to use the following functions :\n- tf.image.random_flip_up_down() \n- tf.image.random_brightness()\n- tf.image.random_contrast()\n\n**Questions**\n\nAs input data is usually carried in Tensorflow as ndarrays, I would like to know :   \n1. Is it a bug of Tensorflow Python API or is it my \"fault\" because\n   of the type/shape of my input data?\n2. How could I get it to work and be able to apply    \"tf.image.random_flip_left_right\" to my training set?\n", "comments": ["The function requires its input to already be a tensor. You can call `convert_to_tensor` to make a tensor from a numpy array, or you can put the numpy data into a `tf.constant` first. \n\nOr you could fix `flip_left_to_right` to call `convert_to_tensor`, which would be the right thing to do.\n", "It looks like we could use a cleaning pass through some of the image ops.  A lot of them unnecessarily require known shape tensors as well.\n", "On the `random_` ones now, which only work on a single image anyway. I think.\n", "I'll work on this:  fix flip_left_to_right to call convert_to_tensor\n", "I believe this can be closed now that PR #1428 has been merged?  I'm surprised this issue wasn't auto-closed on merge. \n", "PR #1428 didn't include something like \"Fixes #1399\" in the commit message, so it didn't autoclose.\n"]}, {"number": 1398, "title": "Udacity examples should be improved (/tensorflow/examples/udacity)", "body": "https://www.udacity.com/course/viewer#!/c-ud730 is a really cool course in the context of Machine Learning and TensorFlow. After completing Coursera Andrew Ng course, they give valuable reasoning why and how something should be done in TensorFlow.\n\nBut, there is an issue.\n\nTheory part in videos and implementing it in TensorFlow do not click together. Forum is full of questions in the format \"how should it be done?\", \"am I doing it correctly?\".\n\nYes, I can spend 2 days on a one assignment, search internet and take parts from manuals that are on the tensorflow.org page, but still I am not convinced that I am doing it correctly. \n\nPlease improve https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity so that best practice, how previous task would have been correctly solved, is included in the next task. \n", "comments": ["+1\n", "I agree.\n", "There isn't going to be any updates to the course from the instructors beyond fixes at this point.\n"]}, {"number": 1397, "title": "Strange Multi-GPU Performance", "body": "I am encountering a strange Multi-GPU performance issue. To validate I've run the following script [TensorFlow Examples: Multi-GPU basics](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py).\n\nThe performance on a single GPU was 19 seconds while dual GPU computation took 28 seconds.\n\nThe same effect was reproducible on my system when I used multi-GPU for computing my own networks. You can see a short description and an ASCII illustration of my architecture here: [stackoverflow link](http://stackoverflow.com/questions/35822251/tensorflow-one-network-two-gpus).\n\nThe problem seems to get worse with bigger computations, since my own network is quite big and on a **single** GPU it is about **10 times** faster than on two GPUs :/\n### Environment info\n##### Operating System information:\n\n`uname -or`:\n\n> 3.10.0-327.10.1.el7.x86_64 GNU/Linux\n\n`cat /etc/redhat-release`:\n\n> CentOS Linux release 7.2.1511 (Core)\n##### Hardware\n\nGraphics card is a [NVIDIA Dual K80 Tesla](http://www.nvidia.com/object/tesla-k80.html). \n##### Software environment:\n\n`pip2.7 freeze`:\n\n> backports-abc==0.4\n> backports.ssl-match-hostname==3.5.0.1\n> certifi==2015.11.20.1\n> cycler==0.10.0\n> Cython==0.23.4\n> enum34==1.1.2\n> h5py==2.5.0\n> libxml2-python==2.9.3\n> Mako==1.0.1\n> matplotlib==1.5.1\n> mercurial==3.6.2\n> nose==1.3.7\n> numpy==1.10.4\n> pkgconfig==1.1.0\n> protobuf==3.0.0b2\n> pyparsing==2.1.0\n> python-dateutil==2.4.2\n> pytz==2015.7\n> singledispatch==3.4.0.3\n> six==1.10.0\n> tensorflow==0.7.1\n> tornado==4.3\n> virtualenv==13.1.2\n> wheel==0.29.0\n\n`python -c \"import tensorflow; print(tensorflow.__version__)`:\n\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4.0.7 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.0 locally\n> 0.7.1\n\nBuilt with **bazel** from sources, commit hash:\n\n> commit eda89e930cfcbd992ecacafd40267d733e2153dc\n### What have you tried?\n1. different scripts and setups\n### Logs or other output that would be helpful\n\nComplete output of `multigpu_basics.py` with `tf.ConfigProto(log_device_placement=log_device_placement)` is attached.\n[out.txt](https://github.com/tensorflow/tensorflow/files/160210/out.txt)\n", "comments": ["I'm not sure if you are adapting from the cifar 10 example. If yes, you should try place variables on GPU. The cifar10 example uses variables on CPU by default. You may achieve about 1.5x speed up compared to a single GPU setup with 2 GPUs.\n\nI've got about 2x speed up with 4 GPUs, and 3x with 8 GPUs. It is far from linear acceleration. I guess `tf.reduce_mean` has a noticeable overhead here, when averaging grads from different GPUs.\n\nEven when only one GPU is used, adding the `tf.reduce_mean` operation can cause a 50% penalty on speed.\n\nThe above results are collected with Titan X GPUs and a VGG-16 network.\n", "I was not trying the CIFAR10 example, but could do this later. The author of this very simple example: [TensorFlow Examples: Multi-GPU basics](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/multigpu_basics.py) claims a speedup with Multi-GPU setup while on my system it actually took longer compared with single GPU calculation.\n\nThe same thing happened when I placed some `with tf.device(\"/gpu:0\")` and `with tf.device(\"/gpu:1\")` here and there, where my code is able to run in parallel without creating towers.\n\nCould you supply an example? Maybe a complete \"benchmark\" for Multi-GPU, to see if this error is reproducible?\n", "Ah I see. The example is not well written.\n\nThe first run is dominated by the initialization. You should time the second run of `sess.run`.\n\nHere's my result:\n\n```\nSingle GPU computation time: 0:00:08.477408\nMulti GPU computation time: 0:00:04.451062\n```\n\nedit:\n\ntime it like this:\n\n```\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Runs the op.\n    sess.run(sum)\n    t1_1 = datetime.datetime.now()\n    sess.run(sum)\n    t2_1 = datetime.datetime.now()\n```\n", "Aren't `a` and `b` initialized in both runs?! The single- and the multi-GPU calculations.\n\nMy result was:\n\n> Single GPU computation time: 0:00:19.211597\n> Multi GPU computation time: 0:00:28.120005\n", "The graph is not initialized until the first run in a session. Just defining it won't do.\n\nYour results are strange. I cannot explain it.\n", "Do I maybe have to set some memory constraints? Since I do not have two graphics cards but one dual-GPU card? Perhaps TensorFlow is doing something strange on the dual-GPU card's memory?\n", "I think it has to do with the Dual GPU architecture for Tesla K80. It has a PCIe switch to communicate both GPU cards internally. It shall introduce an overhead on communication. See [block diagram](https://www.microway.com/hpc-tech-tips/introducing-nvidia-tesla-k80-gpu-accelerator-kepler-gk210/)\n", "So the shared PCIe lanes are the bottleneck? Any tips on how to avoid this bottleneck?\n", "I believe it has to be addressed at kernel level\n", "Closing since the discussion indicates it may not be a TensorFlow specific issue.  Please comment if more information appears or file a separate issue.\n"]}, {"number": 1396, "title": "Throw out exception to python when errors happen in c++ code", "body": "Required information is after the description.\n\nHi tensorflow team,\n\nI have enjoyed using tensorflow much. Thanks for the commitment to make this happen.\n\nAfter writing many code based on it, I found it is very inconvenient that when the C++ code meets an error, it just breaks instead of passing the error to the python front end, so it is impossible to deal with the error gracefully.\n\nFor example, when training a network with a large learning rate with ReLU without BN, the activations could easily go out of range. As this is unavoidable since learning rate has to be tuned. It would be great to catch the out of range error in python, so things could be logged and shut down properly, instead of just break.\n\nA log could be\n\n```\nW tensorflow/core/common_runtime/executor.cc:1102] 0x33ea350 Compute status: Out of range: Nan in summary histogram for: maxout-relu-cifar10/conv2/HistogramSummary_2\n```\n\nHope this suggestion could be taken into consideration.\n### Environment info\n\nOperating System:\n\n```\n3.13.0-74-generic #118-Ubuntu SMP x86_64 x86_64 x86_64 GNU/Linux\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n```\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n```\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1\n```\n", "comments": ["Can you explain what currently happens?  Our python code is supposed to return C++ status errors, which can then be handled by users.\n", "First I have to confess I did not notice the OpError class before. Now I could\nget the program log something before it quits. Thanks for pointing it out.\n\nThen I thought a little why I did not notice this, and think this may could be\na small improvement usability. I will elaborate below.\n\nThe error with more context is:\n\n``` bash\nI0309 09:26:19.824918 21189 survivors.py:282] Step 100: loss = nan lr = 0.50000 (359.3 examples/sec 0.356 sec/batch)\nW tensorflow/core/common_runtime/executor.cc:1102] 0x2c1c670 Compute status: Out of range: Nan in summary histogram for: maxout-relu-cifar10/conv1/HistogramSummary_2\n     [[Node: maxout-relu-cifar10/conv1/HistogramSummary_2 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](maxout-relu-cifar10/conv1/HistogramSummary_2/tag, maxout-relu-cifar10/conv1/biases/read/_470)]]\n```\n\nI got the same log whether I catch the error or not. For a typical python\nprogram, when an error is raised, it would print out something like\n\n```\nTraceback (most recent call last):\n  File \"divide_by_zero.py\", line 1, in <module>\n    1/0\nZeroDivisionError: integer division or modulo by zero\n```\n\nIf I got something report like it, I would notice I could have error to\ncatch(it seems a little unreasonble to ask user to read what errors one may get\nbefore coding, since python is a lot about interaction).\n\nSo it would be great something could be done on this.\n\n---\n\nLastly, I have a side question, in what cases I could get a NaN number? How\ncould I know?\n\nIn this case, this happens when the learning rate is too larger. I guess it is\nbecause the activation value goes out of range(or some bug I do not know in the\nconvolution op, though I got similar errors in ReLU op as well. Same large learning rate reason).\n\nI think I could do a denser logging or summary to know what actually happens,\nbut it would much more convenient when a NaN number is taken as an exception,\nsince no futher ops can do anything about it(I could file another issue about\nthis if the team prefers).\n", "Closing since questions about how nan works are better suited to StackOverflow.  Generally, nans occur when either a specific singularity is hit (division by zero) or an iterative process explodes to infinity.\n"]}, {"number": 1395, "title": "#1392 fix mnist tutorial doc typo", "body": "fix #1392 tutorial typo\n", "comments": ["Can one of the admins verify this patch?\n", "Merged. Thanks!\n"]}, {"number": 1394, "title": "#1293 Implement Matrix Trace", "body": "As discussed in #1293, this is an attempt to implement matrix trace operator for rank 2 tensors. It only takes tensors with rank 2 (similar to the trace design in Theano). Trace for higher rank tensors requires more discussion on how we want to implement it. One approach is to calculate trace for all rank 2 sub-tensors as numpy. Another is to implement [tensor contraction](https://en.wikipedia.org/wiki/Tensor_contraction) (generalization of trace).\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Just some docs and naming nits, otherwise looks good! \n", "@martinwicke Thank you so much for the review! I have updated them accordingly. \n", "@martinwicke Thanks again for the review! Let me know if I need to squash these commits.\n", "One more run of tests, then we're good.\n\nJenkins, test this please.\n", "Merged.\n", "Thank you!\n"]}, {"number": 1393, "title": "Added GPU implementation for resize nearest neighbor grad.", "body": "And also adapted/added appropriate tests.\n", "comments": ["Can one of the admins verify this patch?\n", "I needed to adapt the test so they use `np.float` instead of `np.float32`, e. g. [here](https://github.com/tensorflow/tensorflow/pull/1393/files#diff-bb1fdb8f5be7a85b4505589762579bf7R31). Isn't that a bit concerning? Shouldn't these types be equivalent? Or at least the op fall back to CPU if it's not supported on the GPU?\n", "np.float is an alias for float64, not float32. Surprised me too. http://docs.scipy.org/doc/numpy-1.10.1/user/basics.types.html\n", "Oh, that's bad news. That means the tests are not using the new written kernel, but are falling back to the CPU version. I'll have to look into that and add a [WIP] for the time being. Thanks @martinwicke for the pointer.\n", "@martinwicke I fixed the PR and tests. Could you take another look at this? I also added the kernel for `SetZero` from `maxpooling_op_gpu.cu.cc` to `tensorflow/core/util/cuda_kernel_helper.h` and consequently removed some duplicated code there after including `cuda_kernel_helper.h`.\n", "Thanks for moving SetZero, that's good. Can you add a blank line after it? Otherwise looks good to me. You can amend your last commit with that, then I'll merge.\n", "Done.\n", "Jenkins, test this please.\n", "Jenkins, test this please\n"]}, {"number": 1392, "title": "Minor Error in Tutorial", "body": "On [this tutorial page](https://www.tensorflow.org/versions/master/tutorials/mnist/download/index.html), it says that the MNIST data has been rescaled to [-.5, .5]:\n\n\"The image data is extracted into a 2d tensor of: [image index, pixel index] where each entry is the intensity value of a specific pixel in a specific image, rescaled from [0, 255] to [-0.5, 0.5].\"\n\nHowever, in [the code](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/input_data.py), the data is rescaled to [0,1] as per the comment on line 123 (and confirmed by loading a sample image and inspecting):\n\n`# Convert from [0, 255] -> [0.0, 1.0].`\n\nJust a minor error, but thought it was worth pointing out! I'm actually curious which is the better method of rescaling, but that's a topic for another discussion..\n", "comments": ["Fixed by #1395.\n"]}, {"number": 1391, "title": "install from pip package fails on Ubuntu 15.10", "body": "### Environment info\n\nOperating System: Ubuntu 15.10\n1. pip package: tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n2. The screen output: \n   Downloading tensorflow-0.7.1-cp27-none-linux_x86_64.whl (13.8MB): 13.8MB downloaded\n   Cleaning up...\n   Exception:\n   Traceback (most recent call last):\n   File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n     status = self.run(options, args)\n   File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 304, in run\n     requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\n   File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 1271, in prepare_files\n     req_to_install)\n   File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 109, in **init**\n     self.prereleases = any([is_prerelease(x[1]) and x[0] != \"!=\" for x in self.req.specs])\n   File \"/usr/lib/python2.7/dist-packages/pip/util.py\", line 739, in is_prerelease\n     return any([any([y in set([\"a\", \"b\", \"c\", \"rc\", \"dev\"]) for y in x]) for x in parsed])\n   TypeError: 'int' object is not iterable\n\nStoring debug log for failure in /home/osboxes/.pip/pip.log\n### Steps to reproduce\n1. sudo apt-get install python-pip python-dev\n   2.sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n", "comments": ["Can you upgrade pip or tell me which version of pip is installed?\n", "Closing due to lack of information.  I'm happy to reopen if more information arises.\n"]}]