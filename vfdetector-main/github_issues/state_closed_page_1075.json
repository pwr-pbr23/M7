[{"number": 21024, "title": "KeyError: 'IteratorGetDevice' in tf.train.import_meta_graph", "body": "------------------------\r\n\r\n### System information\r\nubuntu 16.04LTS   \r\nprotobuf                      3.6.0                 \r\ntensorflow                    1.8.0\r\nand I'm using Python 3.5\r\n\r\n### Describe the problem\r\nI got a [KeyError: 'IteratorGetDevice'] when using [tf.train.import_meta_graph] to import a [tensorflow official resnet model](the model is trained using the source code online).\r\n\r\n### Source code / logs\r\n\r\n[Code:]\r\n\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n  new_saver = tf.train.import_meta_graph(\r\n          '/home/smn/cifar10_model/model.ckpt-3907.meta')\r\n\r\n[Trackback:]\r\n\r\nrunfile('/home/smn/project/code/test2.py', wdir='/home/smn/project/code')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-15-af704926d7fa>\", line 1, in <module>\r\n    runfile('/home/smn/project/code/test2.py', wdir='/home/smn/project/code')\r\n\r\n  File \"/home/smn/tensorflow/lib/python3.5/site-packages/spyder_kernels/customize/spydercustomize.py\", line 678, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/home/smn/tensorflow/lib/python3.5/site-packages/spyder_kernels/customize/spydercustomize.py\", line 106, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/home/smn/project/code/test2.py\", line 4, in <module>\r\n    '/home/smn/cifar10_model/model.ckpt-3907.meta')\r\n\r\n  File \"/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1955, in import_meta_graph\r\n    **kwargs)\r\n\r\n  File \"/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\", line 743, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n\r\n  File \"/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 460, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n\r\n  File \"/home/smn/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 227, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\n\r\nKeyError: 'IteratorGetDevice'", "comments": ["It's probably not a version problem,  my problem was fixed after I added a line \"tf.contrib.resampler\" in my code", "i still have this issue with tensorflow 1.10 and tensorflow 1.9", "Try adding this line in your code:\r\n\r\ntf.contrib.resampler\r\n", "Similar problem again with `Key Error: 'ExperimentalIteratorGetDevice'`.  Unfortunately resampler doesn't work this time.\r\n I am using tf ver=1.11.0 and http://download.tensorflow.org/models/official/20181001_resnet/checkpoints/resnet_imagenet_v2_fp32_20181001.tar.gz "]}, {"number": 21023, "title": "Fold BN after depthwise conv", "body": "BN can be folded to Conv2D for inference, I find it is also correct for DepthwiseConv2dNative. \r\nI got the MobileNet model from https://github.com/tensorflow/models/blob/master/research/slim,  graph transform was applied and the correctness was verified via label_image sample.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "@mingxingtan Can u help review this pr? Thanks.", "Sorry, this had to be reverted, because it broke some internal test. We're investigating.", "Is there any update on this PR merging? Is it possible that it will be merged anytime soon? For mobilenet v1 model, this PR can be useful for performance.  ", "@drpngx can u figure out which internal test failed? Is thera any workaround?", "@drpngx,  a follow-up on this thread, can we get more insights about the error? \r\nAs @ashraf-bhuiyan pointed out, this PR helps to improve inference performance of MobilenetV1 and SSD-mobilenetV1,  and it is also a prerequisite for the quantization work. We tested this PR, it's valid on those two models mentioned above, yielding correct accuracy. \r\n\r\n@zhaoyongke, Thanks for this PR. May I ask you to split the PR into two separate PR? fold_batch_norm being one PR, and fold_old_batch_norm being another PR?  such that they won't be blocked together.\r\n ", "I am looking into this.", "The failures seem to be all about [this](\r\nhttps://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/fold_batch_norms.cc#L81). Could you try to test that?", "@drpngx Hi, can you provide a test case? I haven't replay this failure.", "@drpngx ping you again, need a test case to replicate the failure. thanks.  ", "It's failing on a processing of one of our internal graphs.\r\n@smillius do you seen anything wrong with that CL?", "As mentioned internally, this change incorrectly assumes that the channel_multiplier is 1.\r\nA DepthwiseConv2dNative has a filter tensor of shape:\r\n[filter_height, filter_width, in_channels, channel_multiplier]\r\nSo the output has in_channels * channel_multiplier many channels, and that one needs to match the dimension of mul_values.\r\n\r\nMaybe we can add a test for this?\r\nAlso it may make sense to not hard fail here but gracefully just not handle this case.\r\n", "> As mentioned internally, this change incorrectly assumes that the channel_multiplier is 1.\r\n> A DepthwiseConv2dNative has a filter tensor of shape:\r\n> [filter_height, filter_width, in_channels, channel_multiplier]\r\n> So the output has in_channels * channel_multiplier many channels, and that one needs to match the dimension of mul_values.\r\n\r\nSorry, I missed channel_multiplier param. I'll fix it soon.\r\n\r\n> \r\n> Maybe we can add a test for this?\r\n> Also it may make sense to not hard fail here but gracefully just not handle this case.\r\n\r\nGood suggestion!\r\n", "Hi all,\r\n\r\nI've updated the transform functions and test cases:\r\n\r\n[Transform Functions](https://github.com/zhaoyongke/tensorflow/commit/898bb2ad207d2b4cee188ce8b868c76bda8f0ad2)\r\n[Test Cases](https://github.com/zhaoyongke/tensorflow/commit/c1d9c390847471dc233a4a91725f98fdb861fd5a)\r\n\r\n@drpngx @smillius , should I submit a new PR? \r\n\r\nThanks!\r\n", "No just push a new commit. Thank you @smillius for looking into this!", "> No just push a new commit. Thank you @smillius for looking into this!\r\n\r\nYeah I pushed to my fork at [myfork](https://github.com/zhaoyongke/tensorflow). Current status of this PR is 'Merged', what should I do then? Thank you again @drpngx !", "Oh sorry, yes I forgot it had been merged and rolled back. Yes, please\ncreate a new PR\n\nOn Sat, Nov 17, 2018, 10:55 PM zhaoyongke <notifications@github.com wrote:\n\n> No just push a new commit. Thank you @smillius\n> <https://github.com/smillius> for looking into this!\n>\n> Yeah I pushed to my fork at myfork\n> <https://github.com/zhaoyongke/tensorflow>. Current status of this PR is\n> 'Merged', what should I do then? Thank you again @drpngx\n> <https://github.com/drpngx> !\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21023#issuecomment-439671866>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbbLm0WYdJLExHJA7puO-R-A6BsnGks5uwQRTgaJpZM4VZrdP>\n> .\n>\n", "@drpngx Thanks!\r\n\r\n[New PR is here](https://github.com/tensorflow/tensorflow/pull/23838)"]}, {"number": 21022, "title": "Feature request : Moore-Penrose pseudoinverse", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nDue to its importance in various fields in applied math, it would be beneficial to have a TensorFlow version of NumPy's `pinv` function that calculates the **Moore-Penrose** pseudoinverse of an array : https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.pinv.html\r\n\r\nOf course, it is possible to create such function with current existing TensorFlow's functionality (e.g. using `tf.svd`), but it would be more appropriate to have it as an optimized core function. \r\n\r\n### Source code / logs\r\nN/A", "comments": ["We currently don't have anybody working on this. It would be great if you could help us by working on this and submitting a PR. Let us know if you need further clarification. Thanks!\r\n", "I could write a simple python script for the operation. This could serve as a temporary solution while there is no corresponding optimized core function written in C++. If this is acceptable, I would gladly help.", "Hi @bignamehyp , can you assign this issue to me ? Thank you.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler Yes, it is still an issue.", "@tensorflowbutler This feature has already been incorporated. This issue can be closed.", "@jpohanka Where is this op in tensorflow? I can't find it. I just found an unofficial implementation in tensorflow_probability. Will this feature be implemented officially in tensorflow?"]}, {"number": 21021, "title": "Add C++ gradients for some image operators.", "body": "Added gradients and tests for\r\n- ResizeBilinear\r\n- ResizeBicubic\r\n- ResizeNearestNeighbor\r\n\r\nNote: Some of the tests are for the operator itself rather than\r\nthe gradient, paralleling existing tests in image_grad.py\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/21019", "comments": ["Jenkins test this please"]}, {"number": 21020, "title": "[tf.contrib.slim] Update documentation in evaluation.py", "body": "Fixes #12357.\r\nUpdate the documentation and test the sample code. It works.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!\n\n2018-07-21 23:14 GMT+08:00 googlebot <notifications@github.com>:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21020#issuecomment-406802758>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ARVNely7TR8rhv_M4scYeOPtBzFDyISvks5uI0VSgaJpZM4VZqBn>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21019, "title": "C++: Add gradient for image operators", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**:\r\n3.5.2 (default, Nov 23 2017, 16:37:01) \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nFeature request: Add gradient support for the image operators in the C++ API.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Updated", "As I understand it this issue is here just so it can be referred to by the PR, and no action is needed while that PR is in flight. If that's wrong, please let me know!", "Thanks Michael, yes - that's pretty much it. There is one more image op (`CropAndResize`) that's not covered by the in-flight PR. So if you think it makes sense, could keep the issue open till someone contributes a PR for this last op in the image group.", "Sounds good to me.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@kbsriram @tensorflowbutler, Could I have a try for my first contribution? Thanks.\r\n\r\nAnd @kbsriram, do you have any advice for the image op (`CropAndResize`), I don't find any `CropAndResizeGrad` funtion, so how I could compute the grad and verify in the unit test? I would be very appreciate if you could give me some guidance.", "@kbsriram,\r\nCan you please confirm if we can close this issue as the [in-flight PR](https://github.com/tensorflow/tensorflow/pull/21021) and the [PR corresponding to Image OP](https://github.com/tensorflow/tensorflow/pull/25467) has been merged? Thanks! ", "@rmothukuru - great! Thanks for checking, I'll go ahead and close it."]}, {"number": 21018, "title": "Waste lots of time to redownload grpc when building with CMake", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 7 64 bit.\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No.\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.9\r\n- **Python version**:3.5\r\n- **Bazel version (if compiling from source)**:No.\r\n- **GCC/Compiler version (if compiling from source)**:VS2015\r\n- **CUDA/cuDNN version**:CUDA 8.0 and cuDnn 7.0.\r\n- **GPU model and memory**:No.\r\n- **Exact command to reproduce**:\r\n```\r\ncd $TENDORFLOW_DIR/tensorflow/contrib/cmake\r\ncmake .\r\nmake -j20\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI want to build tensorflow from source with CMake in Windows, but it fails with the error caused by the fail of gprc. I wonder if there is a way to avoid the redownload gprc when rebuilding form source.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n29>  Creating directories for 'grpc'\r\n29>  Cloning into 'grpc'...\r\n29>  Performing download step (git clone) for 'grpc'\r\n29>  fatal: unable to access 'https://boringssl.googlesource.com/boringssl/': Failed to connect to boringssl.googlesource.com port 443: Timed out\r\n29>  fatal: clone of 'https://boringssl.googlesource.com/boringssl' into submodule path 'D:/CNN/tensorflow/BUILD/grpc/src/grpc/third_party/boringssl-with-bazel' failed\r\n29>  Failed to clone 'third_party/boringssl-with-bazel'. Retry scheduled\r\n29>  Failed to clone 'third_party/boringssl-with-bazel' a second time, aborting\r\n29>  CMake Error at D:/CNN/tensorflow/BUILD/grpc/tmp/grpc-gitclone.cmake:93 (message):\r\n29>  Failed to update submodules in: 'D:/CNN/tensorflow/build/grpc/src/grpc'\r\n", "comments": ["I think that package is required. I don't know why you were having trouble downloading but I hope that the connection is working now. I will close for now assuming you were able to build, but please reopen if I have misunderstood.", "Yes, I know grpc is required. But what we need is incremential downloading, not redownload the whole package when failed to build the project. As you known, the grpc is more than 200M! My network is not good and every time in rebuilding it takes serval hours to download the grpc package.\r\nThe url of [boringssl](https://boringssl.googlesource.com/boringssl) is not accesible under the Great Firewall of China.", "I see I'm sorry thanks for clarifying. @mrry do you think this is something that would be feasible without heroic effort?", "It sounds like it should be possible, but we'll need someone with more CMake expertise to suggest a fix, so I'll mark this as \"contributions welcome\".\r\n\r\nAs a potential lead, the `UPDATE_DISCONNECTED` option seems relevant:\r\n\r\n* https://github.com/Kitware/CMake/blob/5bbcf76399e107bbb1712ba8aeee27c160413d2d/Modules/ExternalProject.cmake#L320-L338\r\n* https://stackoverflow.com/q/36254658/3574081", "A couple of days ago the Windows build was switched over to use bazel instead of cmake. That doesn't solve your problem, but it means the best solution now is going to be a feature request to the bazel team I think. Sorry not to be more help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21018\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21018\">No</a>\n"]}, {"number": 21017, "title": "fixed build error on gcc-7", "body": "fixed build error \r\n\r\n```sh\r\nERROR: /home/msca8h/Projects/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:650:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:infeed_manager' failed (Exit 1)\r\nIn file included from ./tensorflow/compiler/xla/service/gpu/infeed_manager.h:23:0,\r\n                 from tensorflow/compiler/xla/service/gpu/infeed_manager.cc:16:\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:68:37: error: 'std::function' has not been declared\r\n   void RegisterOnEmptyCallback(std::function<void()> callback) {\r\n                                     ^~~~~~~~\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:68:45: error: expected ',' or '...' before '<' token\r\n   void RegisterOnEmptyCallback(std::function<void()> callback) {\r\n                                             ^\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:83:20: error: 'function' is not a member of 'std'\r\n   std::vector<std::function<void()>> on_empty_callbacks_;\r\n                    ^~~~~~~~\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:83:20: note: suggested alternative: 'is_function'\r\n   std::vector<std::function<void()>> on_empty_callbacks_;\r\n                    ^~~~~~~~\r\n                    is_function\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:83:20: error: 'function' is not a member of 'std'\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:83:20: note: suggested alternative: 'is_function'\r\n   std::vector<std::function<void()>> on_empty_callbacks_;\r\n                    ^~~~~~~~\r\n                    is_function\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:83:34: error: template argument 1 is invalid\r\n   std::vector<std::function<void()>> on_empty_callbacks_;\r\n                                  ^\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:83:34: error: template argument 2 is invalid\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:83:35: error: expected unqualified-id before '>' token\r\n   std::vector<std::function<void()>> on_empty_callbacks_;\r\n                                   ^~\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h: In member function 'BufferType xla::gpu::XfeedQueue<BufferType>::BlockingGetNextDestination()':\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:61:35: error: 'on_empty_callbacks_' was not declared in this scope\r\n       for (const auto& callback : on_empty_callbacks_) {\r\n                                   ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h: In member function 'void xla::gpu::XfeedQueue<BufferType>::RegisterOnEmptyCallback(int)':\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:69:5: error: 'on_empty_callbacks_' was not declared in this scope\r\n     on_empty_callbacks_.push_back(std::move(callback));\r\n     ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:69:45: error: 'callback' was not declared in this scope\r\n     on_empty_callbacks_.push_back(std::move(callback));\r\n                                             ^~~~~~~~\r\n./tensorflow/compiler/xla/service/gpu/xfeed_queue.h:69:45: note: suggested alternative: 'calloc'\r\n     on_empty_callbacks_.push_back(std::move(callback));\r\n                                             ^~~~~~~~\r\n                                             calloc\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/compiler/xla/status.h:19,\r\n                 from ./tensorflow/compiler/xla/layout_util.h:23,\r\n                 from ./tensorflow/compiler/xla/shape_tree.h:24,\r\n                 from ./tensorflow/compiler/xla/service/gpu/infeed_manager.h:24,\r\n                 from tensorflow/compiler/xla/service/gpu/infeed_manager.cc:16:\r\n./tensorflow/core/platform/default/logging.h: In instantiation of 'std::__cxx11::string* tensorflow::internal::Check_LEImpl(const T1&, const T2&, const char*) [with T1 = long long int; T2 = long unsigned int; std::__cxx11::string = std::__cxx11::basic_string<char>]':\r\n./tensorflow/compiler/xla/shape_util.h:117:5:   required from here\r\n./tensorflow/core/platform/default/logging.h:232:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\r\n./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n                                                 ^\r\n./tensorflow/core/platform/default/logging.h:232:1: note: in expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n TF_DEFINE_CHECK_OP_IMPL(Check_LE, <=)\r\n ^~~~~~~~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\nThe compiler couldn't find std::function\r\nincluding ```<functional>``` fixed the problem", "comments": []}, {"number": 21016, "title": "keras model to estimator in eager mode does not support the use of nested subclassed models", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, code attached below.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 18.04\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom source\r\n\r\n- **TensorFlow version (use command below)**:\r\nb'v1.9.0-rc2-999-g78909bf81e' 1.9.0\r\n\r\n- **Python version**:\r\n3.6.6\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.16.0rc3\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n6\r\n\r\n- **CUDA/cuDNN version**:\r\n9.1 / 7.1\r\n\r\n- **GPU model and memory**:\r\nTitan X, 12GB\r\n\r\n- **Exact command to reproduce**:\r\nRun the attached code.\r\n\r\n### Describe the problem\r\nThis is more of a feature request to add support for the use of nested subclassed models within keras model to estimator eager mode. There is a comment in the relevant section of the code (estimator/keras.py) which says:\r\n\r\n    # This will not work for nested subclassed models used as layers.\r\n    # This would be theoretically possible to support, but would add complexity.\r\n    # Only do it if users complain.\r\n\r\n\r\n### Source code / logs\r\n```\r\nimport copy\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nclass BaseModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(BaseModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(units=10)\r\n        self.dense2 = tf.keras.layers.Dense(units=1)\r\n    def call(self, input):\r\n        \"\"\"Run the model.\"\"\"\r\n        result = self.dense1(input)\r\n        result = self.dense2(result)\r\n        return result\r\n    def get_config(self):\r\n        config = []\r\n        for layer in self.layers:\r\n            config.append({\r\n                'class_name': layer.__class__.__name__,\r\n                'config': layer.get_config()\r\n            })\r\n        return copy.deepcopy(config)\r\n\r\nclass NestedModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(NestedModel, self).__init__()\r\n        self.base_model = BaseModel()\r\n        self.dense = tf.keras.layers.Dense(units=1)\r\n    def call(self, input):\r\n        result = self.base_model(input)\r\n        result = self.dense(result)\r\n        return result\r\n    def get_config(self):\r\n        config = []\r\n        for layer in self.layers:\r\n            config.append({\r\n                'class_name': layer.__class__.__name__,\r\n                'config': layer.get_config()\r\n            })\r\n        return copy.deepcopy(config)\r\n\r\nmodel = NestedModel()\r\nmodel.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.0001), loss='mean_squared_error', metrics=['accuracy'])\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=model)\r\nestimator.train(input_fn=lambda: tf.data.Dataset.from_tensor_slices((tf.random_uniform([100, 10]), tf.random_uniform([100, ]))).batch(2).repeat(10))\r\n```\r\n", "comments": ["Same problem here in Windows 10.", "This is already done in latest version. Please compile up to master version. Thanks.", "Still not working - the exceptions comes from `tensorflow/python/estimator/keras.py`, I think you were referring to commit `d2f3441c0c` (\"Added support for build on subclass models\"), which doesn't fix it for the Estimator API. \r\n", "No I am referring to this #20377", "Or this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/f83a382e87ca09e8f688515a9549c81d0f46554a\r\n\r\nI changed it to use _any_weight_initialized, not sure why it's still seeing _any_variable_initialized here.", "Yeah, had this problem too, but this one is different.\n\n> On 25 Jul 2018, at 15:33, tanzhenyu <notifications@github.com> wrote:\n> \n> No I am referring to this #20377 <https://github.com/tensorflow/tensorflow/issues/20377>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/21016#issuecomment-407774578>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AE2o43ZXVodISPUTB6m8Bx7XExiPAEtWks5uKIHAgaJpZM4VZl4O>.\n> \n\n", "Either way, I built master a few hours ago and it still throws the exception.", "It should be updated in master branch:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/keras.py\r\n\r\nCan you run tf.__version__ and print the output?", "b'v1.9.0-rc2-1241-g3f454e4060' 1.9.0", "should be 1.10 that fixes the issue.", "Same problem here! The problem is the nested models. I tested it with version: `1.10.0-dev20180801`. Nested models seems not supported yet: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/keras.py#L187. Any chance that this could be supported? \r\n", "tf.keras.estimator.model_to_estimator doesn't seem to support nested subclassed models even without eager execution enables in 1.12", "Any update on this?\r\n"]}, {"number": 21015, "title": "How to get object coordinate with tensorflow?", "body": "How to get object coordinate (tensorflow object_detection api)\r\n(boxes, scores, classes, num) = sess.run(\r\n        [detection_boxes, detection_scores, detection_classes, num_detections],\r\n        feed_dict={image_tensor: frame_expanded})\r\n\r\n    # Draw the results of the detection (aka 'visulaize the results')\r\n    vis_util.visualize_boxes_and_labels_on_image_array(\r\n        frame,\r\n        np.squeeze(boxes),\r\n        np.squeeze(classes).astype(np.int32),\r\n        np.squeeze(scores),\r\n        category_index,\r\n        use_normalized_coordinates=True,\r\n        line_thickness=4,\r\n        min_score_thresh=0.80) # sonra elave edecem     \r\n", "comments": []}, {"number": 21014, "title": "Add a missing left bracket in comments", "body": "", "comments": []}, {"number": 21013, "title": "Installing Tensorflow, verifying ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nMicrosoft Windows [Version 6.3.9600]\r\n(c) 2013 Microsoft Corporation. Alle Rechte vorbehalten.\r\n\r\nC:\\Users\\Fritz>pip install \"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_pyth\r\non\\dist\\tensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl\"\r\nRequirement 'C:\\\\tensorflow\\\\tensorflow\\\\contrib\\\\cmake\\\\build\\\\tf_python\\\\dist\\\r\n\\tensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl' looks like a filename, but the f\r\nile does not exist\r\ntensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl is not a supported wheel on this p\r\nlatform.\r\nYou are using pip version 9.0.1, however version 10.0.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' comm\r\nand.\r\n\r\nC:\\Users\\Fritz>pip install \"C:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_pyth\r\non\\dist\\tensorflow_gpu-1.5.0-cp35-cp35m-win_amd64.whl\"\r\n\r\nC:\\Users\\Fritz>pip install --upgrade --no-deps --force-reinstall tensorflow-gpu\r\nCollecting tensorflow-gpu\r\n  Cache entry deserialization failed, entry ignored\r\n  Using cached https://files.pythonhosted.org/packages/51/bc/29202147b513f0ed5fb\r\ndd40f05c6bc2a19722cfb4dd24d77a7c2080a06b4/tensorflow_gpu-1.9.0-cp36-cp36m-win_am\r\nd64.whl\r\nInstalling collected packages: tensorflow-gpu\r\n  Found existing installation: tensorflow-gpu 1.9.0\r\n    Uninstalling tensorflow-gpu-1.9.0:\r\n      Successfully uninstalled tensorflow-gpu-1.9.0\r\nSuccessfully installed tensorflow-gpu-1.9.0\r\nYou are using pip version 9.0.1, however version 10.0.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' comm\r\nand.\r\n\r\nC:\\Users\\Fritz>python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__in\r\nit__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__in\r\nit__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__in\r\nit__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\\r\ntensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Fritz\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__in\r\nit__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\nWhat is wrong ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "What is your CPU make and model?\r\nI suspect this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 21012, "title": "Fix typo in windows build scripts.", "body": "", "comments": []}, {"number": 21011, "title": "[Intel MKL]: Upgrading to MKL-DNN v0.15", "body": "This PR upgrades Intel MKL-DNN library to v0.15. It also includes a small change that makes convolution compatible with that library version. ", "comments": ["@caisq these unsuccessful tests don't seem to be related to this PR. They are failing for others PR too.", "+@gunan Bazel file changes MKL-DNN file from .tar.gz to .zip. Will this work on Linux?", "/cc @yifeif, @gunan  ", "In theory, zip files also should work, but I would still keep the tar.gz files to be sure.", "@mahmoud-abuzaina Could you use tar.gz for MKL-DNN? Also, please fix the comment.", "@tatianashp Both are fixed. ", "it seems there is a conflict introduced few minutes ago because of some change on the master branch. Rebasing now...", "Rebase is done.", "@gunan is this ready to be merged? ", "It is in the process of merging internally."]}, {"number": 21010, "title": "Add no_pip_gpu to the sdca test.", "body": "Second attempt to disable on GPU release build.", "comments": []}, {"number": 21009, "title": "Fine-Grained Control Over TOCO Quantization", "body": "Our (Syntiant Corp\u2019s) neural network inference chips use quantized weights and biases in order to minimize storage and energy consumption. The new Tensorflow experimental quantization feature [tf.contrib.quantize.experimental_create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/experimental_create_training_graph) supports quantizing weights to between 2 and n bits, but the [tf.contrib.lite.toco_convert](https://www.tensorflow.org/api_docs/python/tf/contrib/lite/toco_convert) tool currently only supports 8 bit quantization. As a result, we have to internally fork the TFLite pipeline before generating the Flatbuffer.\r\n\r\nFeature request: Update TOCO to support arbitrary (i.e., 2 to n bit) signed fixed point quantization of weights and biases for both symmetric and asymmetric quantization. Our desired solution would process the quantization [specified at the op or Keras layer level](https://github.com/tensorflow/tensorflow/issues/21008) and not involve quantization specification within the TOCO tool API.\r\n", "comments": ["We are thinking about how to handle things things like this, but a solution is not currently available. It's likely this will be quite difficult. @raghuraman-k , @suharshs , could you please take a look.\r\n", "This will likely involve needing to introduce new inference types to TOCO/TFLite. It will be a non-trivial change, as we will likely need to also support mixed precision model to make this not be very detrimental to accuracy. This is something we will try to determine a priority for. Thanks!", "We are working to make things more configurable in new converter iterations, will close this issue since its a bit stale."]}, {"number": 21008, "title": "Controlling the Quantization Graph-Rewriter", "body": "### System information\r\nNot applicable to this feature request.\r\n\r\n### Describe the problem\r\n\r\nOur (Syntiant Corp's) neural network inference chips support a continuous range of parameter and activation quantization levels for reducing power consumption. Consequently, we aggressively tune our quantization levels for each application. Based on the research literature and product datasheets we are seeing, it is highly likely there are other chip makers with similar requirements. TF's current graph re-writer approach finds matching blocks in the graph and wraps them in fake quantization operations. This approach poorly serves our use cases for the following reasons:\r\n\r\n1. Different layers can have different quantizations. The graph re-writing approach is global to the graph.\r\n2. The graph re-writer attempts to heuristically match the properties of operations that should be re-written. This will generally work for traditional stored-program architectures, but when you are meddling with layers to match silicon you need to drop into TensorBoard to figure out whether the re-writer picked up the unit. If the unit is not picked up, then you are better off not using the re-writer.\r\n3. We have little transparency into changes in the TF codebase on these features. With more explicit specification of layer quantization it is possible to know when the quantization assumptions change and we can track the latest releases of TF.\r\n\r\nOur request: We would like to work with an API in which the quantization operations are more explicitly specified at the layer (Keras) or op level. We could then plug the API into our specification of neural network layers built to explicitly match the low-level operations implemented in silicon.\r\n\r\nThank you for open sourcing TF and your efforts in supporting the community. :)\r\n\r\nFor reference:\r\n\r\n* [tf.contrib.quantize.experimental_create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/experimental_create_training_graph)\r\n* [tf.contrib.quantize.create_eval_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_eval_graph)\r\n* [tf.contrib.quantize.create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_training_graph)\r\n\r\n### Source code / logs\r\nNot applicable to this feature request.\r\n", "comments": ["Again, this is something we have thought of, but we do not currently have a solution. ", "Yes, currently we only focus on rewriting for TFLite existing kernels. This is something we have been thinking about in how to generalize to other backends, and the short answer is that it likely will not be the existing python rewrites that can accomplish this.\r\n\r\nIn the future we may explore doing rewrites on a layer by layer basis, for instance substituting a Conv layer with a TFLiteConv layer or a SyntiantConv layer. But this is still a WIP.\r\n\r\nRegarding (1). You can provide the scope param to the rewrites to only rewrite part of a graph.", "We are working on a Keras approach to this that will allow the proper configurability. Closing this issue.", "Hi @suharshs where can we track/comment/contribute? I don't see anything that is an obvious fit here: https://github.com/tensorflow/tensorflow/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+quantization+keras+bit"]}, {"number": 21007, "title": "[INTEL MKL] Fix for regressions in //tensorflow/python/ops/parallel_for", "body": "", "comments": []}, {"number": 21006, "title": "Use GPU_PLATFORM_NAME macro to define GPU platform name", "body": "Instead of hard-coded \"CUDA\", use a macro which could be set at configure-time\r\nor compile-time.", "comments": ["@hawkinsp following our discussion at #20845 I've extracted changes to GPU common runtime to this pull request, and let #20845 itself be purely XLA.", "The code change in this PR looks fine, the main question is agreeing on how the TF integration will look, for which @zheng-xq is the right reviewer.\r\n\r\n(Sorry this is taking a while. I think it's important that we get agreement about what we want the end state to look like so we don't make work for ourselves later.)", "In general, I am fine with changing Cuda name to more neutral GPU. However, unless we know for sure we don't want to do things different for different GPU platforms, for performance or functionalities, we should start with a \"AMDGPU\" config and then reuse most of the underlying shared code as much as possible.", "@zheng-xq Could you help review the revised PR? Thanks.\r\n\r\nTo support ROCm and minimize impact to user models, GPU common runtime, and its dependent `Eigen`, is designed to be switchable to either CUDA or ROCm, via macros determined at configure-time (`GOOGLE_CUDA` or `TENSORFLOW_USE_ROCM`). The infrastructure to introduce `TENSORFLOW_USE_ROCM` is under review at #20277 . Since the PR is not approved and merged yet, ROCm-specific logic is not yet included in this pull request.", "@zheng-xq / @caisq ping?", "@zheng-xq / @caisq a gentle ping? If you care to check our downstream fork such as:\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/core/common_runtime/gpu/gpu_device.cc\r\n\r\nyou can find most of existing logic in GPU common runtime are retained on both CUDA and ROCm path. We only need to condition between `GOOGLE_CUDA` for CUDA, and `TENSORFLOW_USE_ROCM` on ROCm for GPU platform-specific API calls.", "@hawkinsp now that @zheng-xq is no longer at Google, who is the right person to review this (per your comment https://github.com/tensorflow/tensorflow/pull/21006#issuecomment-407075653)?", "> @hawkinsp now that @zheng-xq is no longer at Google, who is the right person to review this (per your comment [#21006 (comment)](https://github.com/tensorflow/tensorflow/pull/21006#issuecomment-407075653))?\r\n\r\n@jlebar  Hi, could you please review this one. If you are not the right person, could you please assign this to the correct reviewer.", "I think @hawkinsp is probably the right person to review.  I will ping him internally.", "Or perhaps @tatatodd could review this.", "Addressed in #25676"]}, {"number": 21005, "title": "Numpy ndarray should be serialized as Python list", "body": "Pick the fix at https://github.com/keras-team/keras/pull/10727 to ```tf.keras```.\r\nThis PR can fix #22062 and https://github.com/keras-team/keras/issues/11023 as well.", "comments": ["@yanboliang the tests seem to be consistently failing?", "@fchollet I just found the failure caused by bad indentation, and has updated it, let's see the test result.", "I have checked the ```MacOS Contrib``` and ```Windows Bazel GPU``` failures, it seems not relevant to this PR.\r\nThis is the ```MacOS Contrib``` failure:\r\n```\r\ntensorflow/contrib/lite/profiling/profiler_test.cc:34\r\nThe difference between expected_ms and duration_ms is 14.462999999999965, which exceeds eps_ms, where\r\nexpected_ms evaluates to 500,\r\nduration_ms evaluates to 514.46299999999997, and\r\neps_ms evaluates to 10.\r\n```\r\nIt looks like some tests timeout. I saw the same failure at other PRs.\r\nAnd this is ```Windows Bazel GPU``` failure:\r\n```\r\nERROR: T:/src/github/tensorflow/tensorflow/core/BUILD:2911:1: C++ compilation of rule '//tensorflow/core:device_tracer' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n```\r\nAnd I don't find ```Required``` label on these checks, does it mean this is ready to go? Thanks.", "I don't update my code, but the error changed, I think it's still non-relevant.\r\nThis is the ```Ubuntu Python3 PIP``` failure:\r\n```\r\nERROR: missing input file '@ngraph_tf//:license.txt'\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/python/BUILD:1761:1: Creating runfiles tree bazel-out/host/bin/tensorflow/python/gen_training_ops_py_wrappers_cc.runfiles [for host] failed: Process terminated by signal 15: Process terminated by signal 15\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/tools/pip_package/BUILD:210:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@ngraph_tf//:license.txt'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/tools/pip_package/BUILD:210:1 1 input file(s) do not exist\r\n```", "Gently ping @fchollet @caisq . Would mind to have a look at this? I found other bug (https://github.com/keras-team/keras/issues/11023) which can be fixed by this PR as well. Thanks.", "@fchollet I checked ```Windows Bazel GPU``` and ```XLA``` build failures, they are non-relevant to this PR. What need I do to move forward? Thanks.", "@caisq The failed test is irrelevant, this change has got approved by fchollet. I found several issues(https://github.com/tensorflow/tensorflow/issues/22062, https://github.com/keras-team/keras/issues/11023) are caused by this bug, can you move forward for this PR? Thanks. ", "The last commit is changing unit test from deprecated ```test_session``` to ```cache_session``` according https://github.com/tensorflow/tensorflow/commit/e32029541ae270a021b266fcc3929b2528f8dff1 , it won't have effect on the PR.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21004, "title": "Hardcode Protobuf library version in contrib/makefiles (r1.10 branch)", "body": "Note: The Protobuf source in `tensorflow/workspace.bzl` in TensorFlow\r\n1.10 branch does not work. `make distclean` fails and blocks the build\r\nprocess. For now we're hardcoding to the version which is used by\r\nTensorFlow 1.9.", "comments": []}, {"number": 21003, "title": "Disable sdca_ops test on GPU.", "body": "This test is failing on GPU. Some of the ops don't have GPU\r\nimplementations so this test might not even be meant to be\r\nrun on GPU.", "comments": ["Seems like this test probably wasnt meant to be run on GPU. Disabling. Should fix release build issue."]}, {"number": 21002, "title": "Hardcode Protobuf library version in contrib/makefiles", "body": "Note: The Protobuf source in `tensorflow/workspace.bzl` in TensorFlow\r\n1.10 branch does not work. `make distclean` fails and blocks the build\r\nprocess. For now we're hardcoding to the version which is used by\r\nTensorFlow 1.9.", "comments": []}, {"number": 21001, "title": "Error during tf.evaluate when running on TPU: RuntimeError: All tensors outfed from TPU should preserve batch size dimension, but got scalar ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Cloud Platform (Linux Debian)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:Binary\r\n- **TensorFlow version (use command below)**:1.9\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA (TPU)\r\n- **GPU model and memory**: NA (TPU)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am using TPU on GCP to train my model. My model is fully TPU compatible and the training on TPU works as expected when I run `model.train(train_input, steps=num_steps)`. However, when I want to evaluate the model using `train_costs = lfads.evaluate(train_input, name='train_data', steps=100)` I get the following error:\r\n\r\n`RuntimeError: All tensors outfed from TPU should preserve batch size dimension, but got scalar Tensor(\"OutfeedDequeueTuple:0\", shape=(), dtype=float32, device=/job:worker/task:0/device:TPU:0)`\r\n\r\nI constructed the `eval_metrics` in `TPUEstimatorSpec` as described in the MNIST example passing a tuple of (metric_fn, [tensors]) as\r\n```\r\ndef m_fn(input):\r\n       return {'test':tf.metrics.mean(input)}\r\ntpu_eval_metrics = (m_fn, [rec_costs])\r\n```\r\n\r\n\r\nI used the same `input_fn` both for train and evaluate. I also used the same `TPUEstimatorSpec` line in my `model_fn` for training/evaluation. I cannot figure out what is causing this error and didn't have any luck searching online. \r\n\r\n**Note:** I tested the same code but for CPU/GPU and did not run into any errors during evaluation\r\n\r\n### Source code / logs\r\n`RuntimeError: All tensors outfed from TPU should preserve batch size dimension, but got scalar Tensor(\"OutfeedDequeueTuple:0\", shape=(), dtype=float32, device=/job:worker/task:0/device:TPU:0)`\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@mrezak did you ever figure out what was going on? I am having the same problem", "I met the same problem before. I think you should make sure that the parameters of the metrics_fn should be the tensors with the same shape of labels (i.e. the first dimension of these tensors should be the batch size).", "For anyone reaching this issue later: I had the same problem as @pxxgogo. My metrics_fn output a scalar instead of a tensor. Thanks @pxxgogo for making the suggestion."]}, {"number": 21000, "title": "Rename CUDA GPU ID to platform GPU ID", "body": "Rename CUDA GPU ID to platform GPU ID so the notion is applicable on both CUDA\r\nand ROCm platform.", "comments": ["@hawkinsp / @zheng-xq could you help find the right developer to look at this PR? Thanks.", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@dubey could you please take a look?", "This is a pretty widespread rename.  Requesting @poxvoculi to take a look as he probably has a better understanding of the implications of this rename.", "I think there may already be some slow-moving internal effort to get rid of CUDA names in places like this, and @zheng-xq is the right person to look at it.  I'll accept the assignment until I talk to him.", "Assign Lambda", "@whchung Please help to fix the code that cause the tests to fail, and then we can merge this.", "@aaroey sorry for belated reply. I've addressed the failures found in the CI process. Could you help re-run the test?\r\n\r\nAlso I noticed `git clang-format` may result in different indentation with what's expected by CI. Could you let me know the proper formatting instruction should I use?", "@aaroey ping? I've rebased this PR again so it aligns with the tip of `master` branch. Could you help review and re-run the test? Thanks.", "@whchung thanks, I'm rerunning the tests now.", "@aaroey There is one failing test on XLA target:\r\n\r\n```\r\n//tensorflow/compiler/xla/tests:exhaustive_f32_elementwise_op_test_cpu\r\n```\r\n\r\nBut looking at the log and the test itself I believe it should has nothing to do with this PR.", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20999, "title": "incompatibility : Keras LearningRateScheduler callback and tf.train.optimizer", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nTo make it short, there is an incompatibility between Keras LearningRateScheduler callback and tf.train.optimizer.  Keras optimizers have some specific attributes which are required for Keras callbacks and that does not seem to be the case for tf.train.optimizer.\r\n\r\nUnfortunately, Keras optimizers are incompatible with the eager execution mode. So, basically, the user is compelled to choose between using Keras callbacks and the eager execution mode.\r\n\r\nHere is a code proving that. It runs fine with a typical Keras optimizer and fails with the tensorflow one.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras \r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.callbacks import LearningRateScheduler\r\n\r\ndef step_decay(epoch):\r\n  initial_rate = 1e-3\r\n  factor = int(epoch / 5)\r\n  lr = initial_rate / (10 ** factor)\r\n  return lr\r\n\r\nlr_schedule = LearningRateScheduler(step_decay)\r\n\r\ninput1 = Input(shape=(10,), name=\"input\")\r\nout = Dense(5, activation=\"relu\")(input1)\r\nmodel = Model(inputs=input1, outputs=out)\r\nmodel.compile(optimizer= tf.train.AdamOptimizer(1e-3), loss='mse')\r\n\r\nnp.random.seed(0)\r\nX = np.random.random((20, 10)).astype(np.float32)\r\nY = np.random.random((20, 5)).astype(np.float32)\r\n\r\nmodel.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\r\n```\r\n\r\n### Logs\r\n\r\nEpoch 1/10\r\n\r\n\r\nValueErrorTraceback (most recent call last)\r\n<ipython-input-9-c41c71fed68e> in <module>()\r\n     24 Y = np.random.random((20, 5)).astype(np.float32)\r\n     25 \r\n---> 26 model.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1346           initial_epoch=initial_epoch,\r\n   1347           steps_per_epoch=steps_per_epoch,\r\n-> 1348           validation_steps=validation_steps)\r\n   1349 \r\n   1350   def evaluate(self,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    183       m.reset_states()\r\n    184     # Update callbacks\r\n--> 185     callbacks.on_epoch_begin(epoch)\r\n    186     epoch_logs = {}\r\n    187     if steps_per_epoch is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\r\n     79     logs = logs or {}\r\n     80     for callback in self.callbacks:\r\n---> 81       callback.on_epoch_begin(epoch, logs)\r\n     82     self._delta_t_batch = 0.\r\n     83     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\r\n    635   def on_epoch_begin(self, epoch, logs=None):\r\n    636     if not hasattr(self.model.optimizer, 'lr'):\r\n--> 637       raise ValueError('Optimizer must have a \"lr\" attribute.')\r\n    638     lr = self.schedule(epoch)\r\n    639     if not isinstance(lr, (float, np.float32, np.float64)):\r\n\r\nValueError: Optimizer must have a \"lr\" attribute.\r\n\r\n\r\n", "comments": ["Even when I added the attribute \"lr\" to the model  `model.optimizer.lr`. It is producing another error:\r\n\r\n### Log\r\nAttributeErrorTraceback (most recent call last)\r\n<ipython-input-11-3f77add48361> in <module>()\r\n     24 \r\n     25 model.optimizer.lr = 0.01\r\n---> 26 model.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1346           initial_epoch=initial_epoch,\r\n   1347           steps_per_epoch=steps_per_epoch,\r\n-> 1348           validation_steps=validation_steps)\r\n   1349 \r\n   1350   def evaluate(self,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    183       m.reset_states()\r\n    184     # Update callbacks\r\n--> 185     callbacks.on_epoch_begin(epoch)\r\n    186     epoch_logs = {}\r\n    187     if steps_per_epoch is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\r\n     79     logs = logs or {}\r\n     80     for callback in self.callbacks:\r\n---> 81       callback.on_epoch_begin(epoch, logs)\r\n     82     self._delta_t_batch = 0.\r\n     83     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\r\n    640       raise ValueError('The output of the \"schedule\" function '\r\n    641                        'should be float.')\r\n--> 642     K.set_value(self.model.optimizer.lr, lr)\r\n    643     if self.verbose > 0:\r\n    644       print('\\nEpoch %05d: LearningRateScheduler reducing learning '\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/backend.pyc in set_value(x, value)\r\n   2669           (of the same shape).\r\n   2670   \"\"\"\r\n-> 2671   value = np.asarray(value, dtype=dtype(x))\r\n   2672   if context.executing_eagerly():\r\n   2673     x.assign(value)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/backend.pyc in dtype(x)\r\n    914   ```\r\n    915   \"\"\"\r\n--> 916   return x.dtype.base_dtype.name\r\n    917 \r\n    918 \r\n\r\nAttributeError: 'float' object has no attribute 'dtype' ", "Yes there is discrepancy and we're currently working on it. #20619", "still in review.", "Last update: Review is completed and still working on implementation. It appears to be more challenging  in distributed version.", "Hi @tanzhenyu any update? I'm also keen on this :)", "Having the same issue here, any update?", "Hi @brge17, @dmus, @nairouz, still working on this, stay tuned for  [https://www.tensorflow.org/community/roadmap](2.0)", "Same here, news? ", "It's probably because this version is too old. Can you upgrade to 1.13.1 and compile with tf.keras.optimizers.Adam?", "I had same problem I changed the learning rate scheduler callback as below and it worked,\r\n\r\nprevious implementation:\r\n\r\nclass LearningRateScheduler(Callback):\r\n    \"\"\"Learning rate scheduler.\r\n    # Arguments\r\n        schedule: a function that takes an epoch index as input\r\n            (integer, indexed from 0) and current learning rate\r\n            and returns a new learning rate as output (float).\r\n        verbose: int. 0: quiet, 1: update messages.\r\n    \"\"\"\r\n\r\n    def __init__(self, schedule, verbose=0):\r\n        super(LearningRateScheduler, self).__init__()\r\n        self.schedule = schedule\r\n        self.verbose = verbose\r\n\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        if not hasattr(self.model.optimizer, 'lr'):\r\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\r\n        lr = float(K.get_value(self.model.optimizer.lr))\r\n        try:  # new API\r\n            lr = self.schedule(epoch, lr)\r\n        except TypeError:  # old API for backward compatibility\r\n            lr = self.schedule(epoch)\r\n        if not isinstance(lr, (float, np.float32, np.float64)):\r\n            raise ValueError('The output of the \"schedule\" function '\r\n                             'should be float.')\r\n        K.set_value(self.model.optimizer.lr, lr)\r\n        if self.verbose > 0:\r\n            print('\\nEpoch %05d: LearningRateScheduler setting learning '\r\n                  'rate to %s.' % (epoch + 1, lr))\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        logs = logs or {}\r\n        logs['lr'] = K.get_value(self.model.optimizer.lr)\r\n\r\nnew implementation: replacing 'lr' to 'learning_rate' everywhere in the code.\r\n\r\nclass LearningRateScheduler(keras.callbacks.Callback):\r\n    \"\"\"Learning rate scheduler.\r\n    # Arguments\r\n        schedule: a function that takes an epoch index as input\r\n            (integer, indexed from 0) and current learning rate\r\n            and returns a new learning rate as output (float).\r\n        verbose: int. 0: quiet, 1: update messages.\r\n    \"\"\"\r\n\r\n    def __init__(self, schedule, verbose=0):\r\n        super(LearningRateScheduler, self).__init__()\r\n        self.schedule = schedule\r\n        self.verbose = verbose\r\n\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        if not hasattr(self.model.optimizer, 'learning_rate'):\r\n            raise ValueError('Optimizer must have a \"learning_rate\" attribute.')\r\n        learning_rate = float(K.get_value(self.model.optimizer.learning_rate))\r\n        try:  # new API\r\n            learning_rate = self.schedule(epoch, learning_rate)\r\n        except TypeError:  # old API for backward compatibility\r\n            learning_rate = self.schedule(epoch)\r\n        if not isinstance(learning_rate, (float, np.float32, np.float64)):\r\n            raise ValueError('The output of the \"schedule\" function '\r\n                             'should be float.')\r\n        K.set_value(self.model.optimizer.learning_rate, learning_rate)\r\n        if self.verbose > 0:\r\n            print('\\nEpoch %05d: LearningRateScheduler setting learning '\r\n                  'rate to %s.' % (epoch + 1, learning_rate))\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        logs = logs or {}\r\n        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)\r\n        \r\n\r\n", "Thanks for that patch!", "This should have already worked in 1.14 (it seems that the new optimizers is not released before that). Closing it."]}, {"number": 20998, "title": "10 minutes to recover from ResourceExhaustedError", "body": "While it is possible to create a model which is too large for GPU memory, catch the resulting `ResourceExhaustedError` exception, clear the session, and then change the configuration so it fits in memory (such as reducing batch size). However, the time from the initial allocation attempt to catching the exception can take as long as 10 minutes, when I would expect these steps should complete in milliseconds. \r\n\r\nThis problem has remained unchanged since tf 1.0, I'm currently on tf 1.8, linux ubuntu 16.04 with an nvidia geforce titan x.", "comments": ["Here is some example text:\r\n```\r\nlocator.cc:674] 1 Chunks of size 77056 totalling 75.2KiB\r\n2018-07-23 06:49:50.606984: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 92928 totalling 363.0KiB\r\n2018-07-23 06:49:50.606999: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 150 Chunks of size 123904 totalling 17.72MiB\r\n2018-07-23 06:49:50.607014: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 126208 totalling 123.2KiB\r\n2018-07-23 06:49:50.607029: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 141568 totalling 138.2KiB\r\n2018-07-23 06:49:50.607043: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 147200 totalling 143.8KiB\r\n2018-07-23 06:49:50.607058: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 153856 totalling 150.2KiB\r\n2018-07-23 06:49:50.607072: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 17 Chunks of size 185856 totalling 3.01MiB\r\n2018-07-23 06:49:50.607086: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 80 Chunks of size 204800 totalling 15.62MiB\r\n2018-07-23 06:49:50.607101: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 218624 totalling 213.5KiB\r\n2018-07-23 06:49:50.607116: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 220928 totalling 215.8KiB\r\n2018-07-23 06:49:50.607130: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 226560 totalling 221.2KiB\r\n2018-07-23 06:49:50.607145: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 228608 totalling 223.2KiB\r\n2018-07-23 06:49:50.607159: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 234496 totalling 229.0KiB\r\n2018-07-23 06:49:50.607173: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 371712 totalling 1.77MiB\r\n2018-07-23 06:49:50.607187: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 391680 totalling 382.5KiB\r\n2018-07-23 06:49:50.607201: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 393216 totalling 1.50MiB\r\n2018-07-23 06:49:50.607215: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 495616 totalling 2.36MiB\r\n2018-07-23 06:49:50.607229: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 496640 totalling 485.0KiB\r\n2018-07-23 06:49:50.607244: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 548608 totalling 535.8KiB\r\n2018-07-23 06:49:50.607258: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 588288 totalling 574.5KiB\r\n2018-07-23 06:49:50.607272: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 12 Chunks of size 743424 totalling 8.51MiB\r\n2018-07-23 06:49:50.607287: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 902656 totalling 881.5KiB\r\n2018-07-23 06:49:50.607300: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1145856 totalling 1.09MiB\r\n2018-07-23 06:49:50.607314: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1316352 totalling 1.25MiB\r\n2018-07-23 06:49:50.607328: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1356032 totalling 1.29MiB\r\n2018-07-23 06:49:50.607342: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1363200 totalling 1.30MiB\r\n2018-07-23 06:49:50.607356: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2408448 totalling 4.59MiB\r\n2018-07-23 06:49:50.607370: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 196 Chunks of size 16777216 totalling 3.06GiB\r\n2018-07-23 06:49:50.607385: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 22258688 totalling 21.23MiB\r\n2018-07-23 06:49:50.607399: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 23981824 totalling 22.87MiB\r\n2018-07-23 06:49:50.607414: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 26355712 totalling 25.13MiB\r\n2018-07-23 06:49:50.607428: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 27333120 totalling 26.07MiB\r\n2018-07-23 06:49:50.607443: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 27410688 totalling 26.14MiB\r\n2018-07-23 06:49:50.607457: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 27544064 totalling 26.27MiB\r\n2018-07-23 06:49:50.607472: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 27765248 totalling 26.48MiB\r\n2018-07-23 06:49:50.607486: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 28181504 totalling 26.88MiB\r\n2018-07-23 06:49:50.607500: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 31 Chunks of size 100663296 totalling 2.91GiB\r\n2018-07-23 06:49:50.607515: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 139524608 totalling 133.06MiB\r\n2018-07-23 06:49:50.607530: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 140986624 totalling 134.46MiB\r\n2018-07-23 06:49:50.607544: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 159326208 totalling 455.84MiB\r\n2018-07-23 06:49:50.607559: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 191849728 totalling 182.96MiB\r\n2018-07-23 06:49:50.607573: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 243212288 totalling 231.95MiB\r\n2018-07-23 06:49:50.607587: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 7.37GiB\r\n2018-07-23 06:49:50.607605: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:\r\nLimit:                  7909969101\r\nInUse:                  7909968896\r\nMaxInUse:               7909968896\r\nNumAllocs:               244596701\r\nMaxAllocSize:           3114270720\r\n\r\n2018-07-23 06:49:50.609682: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ***************************************************************************************************x\r\n2018-07-23 06:49:50.609731: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor with shape[88] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\r\n```\r\n\r\nIt will basically print this sort of thing over and over for 10 minutes before the ResourceExhaustedError is caught & I can clear the Tensorflow session and resume training a different model. Since I'm trying out a variety of models, some don't work because they don't fit on the GPU. That's fine, so I just catch the error and try a different model. Ideally the recovery from this could be much faster, and there would be some way to minimize the printed logs and resume immediately.", "This logging should only occur if ConfigProto.report_tensor_allocations_upon_oom is true.  Do you not want the logging?  I agree that the logging should not take long to print, and it doesn't for me.  I wonder whether your program may be running into swapping problems or something similar.", "Ah well disabling that printout may help me immensely! I've never seen the logging print quickly, as far as I have seen, catching the error takes an extreme quantity of time.\r\n\r\nThis can sometimes happen right at my program's startup when I have good amounts of system memory free, perhaps using 8GB / 48GB and thus 40GB free. For example, if I tried to extend a NASNetLarge model with 8 consecutive Conv layers with a 3x3 filter size at the bottom, each with 4096 filters, and a batch size of 32, GPU memory (8GB-12GB depending on the card) will be completely exhausted and this error will occur. \r\n\r\nSince I'm doing a random search of those parameters (# layers, # filters, etc), it isn't easy to completely avoid this issue unless there is a way to predict future memory usage. ", "@poxvoculi I tried the following:\r\n```python\r\n    # prevent errors from being printed for hours when memory runs out\r\n    # https://github.com/tensorflow/tensorflow/issues/20998\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    config.report_tensor_allocations_upon_oom = False\r\n    # config.inter_op_parallelism_threads = 40\r\n    tf_session = tf.Session(config=config)\r\n    K.set_session(tf_session)\r\n```\r\nAnd got this error:\r\n```\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"cornell_hyperopt.py\", line 149, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/home/ahundt/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"cornell_hyperopt.py\", line 68, in main\r\n    config.report_tensor_allocations_upon_oom = False\r\nAttributeError: Assignment not allowed (no field \"report_tensor_allocations_upon_oom\" in protocol message object).\r\n```", "Possibly relevant issue: https://github.com/tensorflow/tensorflow/issues/17076", "Possibly relevant stack overflow: \r\nhttps://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information\r\n\r\nUpdate: \r\nI've now encountered multiple cases where printing for over 1 hour does not finish and reach the catch block of the tf ResourceExhaustedError. I killed the process manually.\r\n\r\nThe stack overflow instructions to [disable tf debugging information](https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information) stopped the printing entirely with `export TF_CPP_MIN_LOG_LEVEL=\"3\" && python your_code.py`, but I still believe this qualifies as a TF bug. By default, I propose that printing of extremely large allocation errors should be halted within 1 second unless more detailed results are specifically requested. ", "I'm going to call this 'contributions welcome'.  I agree that debug printing should not run for a long time, but since I don't believe this ever occurs on our internal platforms it's hard to reproduce and come up with a solution.  ", "just make a model with a 3d input with 1billion entries on each side, and a single conv, and it will probably happen. Anything several orders of magnitude beyond the actual memory in the GPU. Oh, also might need to be using nvcc.", "Up above it looks like you're setting the RunOptions incorrectly.   report_tensor_allocations_upon_oom is a member of RunOptions, not Options, so it needs to be passed to session.run, not the Session constructor.  Example:\r\n\r\n`   config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    tf_session = tf.Session(config=config)\r\n    tf_session.run(c, options=config_pb2.RunOptions(\r\n            report_tensor_allocations_upon_oom=False))\r\n`\r\n\r\nHowever, this value is supposed to default False, so something else in your program must be setting it True.\r\n\r\nIs it possible that you're setting GpuOptions.experimental.use_unified_memory to true, and running into some kind of swapping?", "Hi @ahundt! \r\nIt seems you are using older versions(1.x versions) of Tensorflow. Have you check out this feature on [limiting GPU growth](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth) from TF 2.7 yet? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20997, "title": "ValueError: Empty Range for RandRange() in Train Function for Custom Classifier", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nI used stock example script provided by TensorFlow to write my own custom code.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 - Colab Notebook\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\nN/A\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nNot sure\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.9\r\n\r\n- **Python version**:\r\n3.6 (I think)\r\n\r\n- **Bazel version (if compiling from source)**:\r\nUnsure\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nUnsure\r\n\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\n\r\n        # Train the CNN model\r\n        mnist_classifier.train(input_fn=train_input_fn,steps=train_steps)\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI want to train multiple CNNs of the same structure (lenet) using randomly selected subsets of varying sizes from the MNIST training set. I was able to use the same line of code successfully in an earlier program. This work is contributing towards the development of a computational model for executive function deficits in ADHD as part of a PhD thesis in Pharmaceutical Sciences.\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n[Quantitative WM Model.txt](https://github.com/tensorflow/tensorflow/files/2214655/Quantitative.WM.Model.txt)\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. What script are you running? What is the error trace that you are seeing?", "Hello,\n\nI am having trouble with the command to train a classifier (although I have\nbeen able to use this command successfully in a similar application). I am\nusing Tensorflow 1.9, and Python 3.6 (I think). I am getting ValueError:\nempty range for randrange() using the following command:\n\nmnist_classifier.train(input_fn=train_input_fn,steps=train_steps)\n\nI have copied & pasted the error trace to the bottom of this email.\n\nI have attached a text file with my code that I am running in a Colab\nNotebook (I'm not sure how to share this directly so I copied & pasted the\ncode and the output into the text file). I also attached a word doc where I\nhave highlighted the error message and the line where the error occurred.\n\nIs there an email address that I could use to share my notebook if this\nemail does not help clarify the problem?\n\nThank you.\n\nBelow is the error trace:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-16-bd0d2bfa6c12> in <module>()\n    137\n    138 if __name__ == \"__main__\":\n--> 139   tf.app.run()\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py in\nrun(main, argv)\n    123   # Call the main function, passing through any arguments\n    124   # to the final program.\n--> 125   _sys.exit(main(argv))\n    126\n\n<ipython-input-16-bd0d2bfa6c12> in main(unused_argv)\n    111\n    112         # Train the CNN model\n--> 113\n mnist_classifier.train(input_fn=train_input_fn,steps=train_steps)\n    114\n    115         # Evaluate the models and print results\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\nin train(self, input_fn, hooks, steps, max_steps, saving_listeners)\n    364\n    365       saving_listeners = _check_listeners_type(saving_listeners)\n--> 366       loss = self._train_model(input_fn, hooks, saving_listeners)\n    367       logging.info('Loss for final step: %s.', loss)\n    368       return self\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\nin _train_model(self, input_fn, hooks, saving_listeners)\n   1117       return self._train_model_distributed(input_fn, hooks,\nsaving_listeners)\n   1118     else:\n-> 1119       return self._train_model_default(input_fn, hooks,\nsaving_listeners)\n   1120\n   1121   def _train_model_default(self, input_fn, hooks, saving_listeners):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\nin _train_model_default(self, input_fn, hooks, saving_listeners)\n   1127       features, labels, input_hooks = (\n   1128           self._get_features_and_labels_from_input_fn(\n-> 1129               input_fn, model_fn_lib.ModeKeys.TRAIN))\n   1130       worker_hooks.extend(input_hooks)\n   1131       estimator_spec = self._call_model_fn(\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\nin _get_features_and_labels_from_input_fn(self, input_fn, mode)\n    983           lambda: self._call_input_fn(input_fn, mode))\n    984     else:\n--> 985       result = self._call_input_fn(input_fn, mode)\n    986\n    987     return estimator_util.parse_input_fn_result(result)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py\nin _call_input_fn(self, input_fn, mode)\n   1072       kwargs['config'] = self.config\n   1073     with ops.device('/cpu:0'):\n-> 1074       return input_fn(**kwargs)\n   1075\n   1076   def _call_model_fn(self, features, labels, mode, config):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/numpy_io.py\nin input_fn()\n    196         num_threads=num_threads,\n    197         enqueue_size=batch_size,\n--> 198         num_epochs=num_epochs)\n    199\n    200     batch = (\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py\nin _enqueue_data(data, capacity, shuffle, min_after_dequeue, num_threads,\nseed, name, enqueue_size, num_epochs, pad_value)\n    482                 random_start=shuffle,\n    483                 seed=seed_i,\n--> 484                 num_epochs=num_epochs))\n    485       else:\n    486         feed_fns.append(\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py\nin __init__(self, placeholders, ordered_dict_of_arrays, batch_size,\nrandom_start, seed, num_epochs)\n    217     self._epoch = 0\n    218     random.seed(seed)\n--> 219     self._trav = random.randrange(self._max) if random_start else 0\n    220     self._epoch_end = (self._trav - 1) % self._max\n    221\n\n/usr/lib/python3.6/random.py in randrange(self, start, stop, step, _int)\n    186             if istart > 0:\n    187                 return self._randbelow(istart)\n--> 188             raise ValueError(\"empty range for randrange()\")\n    189\n    190         # stop argument supplied.\n\nValueError: empty range for randrange()\n\nOn Tue, Jul 24, 2018 at 12:52 PM, Karmel Allison <notifications@github.com>\nwrote:\n\n> I apologize, but I am having a hard time understanding what the problem\n> is, where the problem is, and what version it affects. What script are you\n> running? What is the error trace that you are seeing?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20997#issuecomment-407474850>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtJIaQVKqZnLvfaO74x8cOy4bBmXgks5uJ1C2gaJpZM4VYT0n>\n> .\n>\n\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n\nQuantitative Working Memory Model\r\nDate & Time\r\n\r\nPurpose: This program will develop a quantitative model of the working memory deficiency. Identically-structured CNNs (lenet) will be trained with varying numbers of examples from the MNIST training set with batch size (=25) and training steps (=300) held constant. The quantitative relationship between % accuracy of identification of handwritten digits in the MNIST test set and the number of image files will be determined by non-linear regression.\r\n\r\nInputs:\r\n\r\n1) Number of ANNs per training group (set to 20 to generate a normal distribution)\r\n\r\n2) Number of images per training group (n=25, 250, 2500, 25000, 55000) Note: a unique set of training images will be used for each individual ANN in every group\r\n\r\nOutputs: N/A\r\n\r\nConclusions\r\n\r\n--------------------------------------------------------------------------------------\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\n# Import Libraries\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport operator\r\nimport sys\r\nfrom datetime import datetime\r\n\r\n------------CNN and MNIST Function Definitions---------\r\n\"\"\"This section is adapted from Tensorflow 1.9 MNIST tutorial called \"Build a CNN Using Estimators\" found here: https://www.tensorflow.org/tutorials/layers \"\"\"\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ndef cnn_model_fn(features, labels, mode):\r\n  \"\"\"Model function for CNN.\"\"\"\r\n  # Input Layer\r\n  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\r\n\r\n  # Convolutional Layer #1\r\n  conv1 = tf.layers.conv2d(\r\n      inputs=input_layer,\r\n      filters=20,\r\n      kernel_size=[5, 5],\r\n      padding=\"same\",\r\n      activation=tf.nn.relu)\r\n\r\n  # Pooling Layer #1\r\n  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n\r\n  # Convolutional Layer #2 and Pooling Layer #2\r\n  conv2 = tf.layers.conv2d(\r\n      inputs=pool1,\r\n      filters=50,\r\n      kernel_size=[5, 5],\r\n      padding=\"same\",\r\n      activation=tf.nn.relu)\r\n  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n\r\n  # Dense Layer\r\n  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 50])\r\n  dense = tf.layers.dense(inputs=pool2_flat, units=500, activation=tf.nn.relu)\r\n\r\n  # Logits Layer\r\n  logits = tf.layers.dense(inputs=dense, units=10)\r\n\r\n  predictions = {\r\n      # Generate predictions (for PREDICT and EVAL mode)\r\n      \"classes\": tf.argmax(input=logits, axis=1),\r\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\r\n      # `logging_hook`.\r\n      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\r\n  }\r\n\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n  # Calculate Loss (for both TRAIN and EVAL modes)\r\n  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n  # Configure the Training Op (for TRAIN mode) ***Learning rate = 0.001\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    optimizer = tf.train.AdamOptimizer()\r\n    train_op = optimizer.minimize(\r\n        loss=loss,\r\n        global_step=tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n  # Add evaluation metrics (for EVAL mode)\r\n  eval_metric_ops = {\r\n      \"accuracy\": tf.metrics.accuracy(\r\n          labels=labels, predictions=predictions[\"classes\"])}\r\n  return tf.estimator.EstimatorSpec(\r\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n  \r\n\r\n----------------------Main Driver Function---------------------------------------\r\n\r\nPurpose: This function will download MNIST data, train ANNs, and evaluate these ANNs\r\n\r\ndef main(unused_argv):\r\n    \r\n  #Set parameters for #ANNs to train & #steps in training\r\n  num_ANNs = 2\r\n  batchsize = 25\r\n  train_steps = 300\r\n  train_set_sizes = [25,250,2500,25000,55000]\r\n  \r\n  # Load training and eval data\r\n  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n  \r\n  #55,000 images in train set (28x28 grayscale)\r\n  train_images = mnist.train.images # Returns np.array\r\n  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\n  \r\n  #10,000 images in test set (28x28 grayscale)\r\n  eval_data = mnist.test.images # Returns np.array\r\n  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)  \r\n  \r\n  #Initialize lists to hold ADHD training examples\r\n  train_set_examples = []\r\n  train_set_labels = []\r\n  \r\n  #create training input function for control-ANNs & evaluation input function\r\n  train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n                              x={\"x\": np.asarray(train_set_examples)},\r\n                              y=np.asarray(train_set_labels),\r\n                              batch_size=25,\r\n                              num_epochs=None,\r\n                              shuffle=True)\r\n  \r\n  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n                      x={\"x\": eval_data},\r\n                      y=eval_labels,\r\n                      num_epochs=1,\r\n                      shuffle=False)\r\n  \r\n  #Initialize list to hold all of the accuracy data\r\n  all_accuracies = []\r\n \r\n  #For each training set group\r\n  for group_number in range(len(train_set_sizes)):\r\n      \r\n      #For each ANN in training set group\r\n      for i in range(num_ANNs):\r\n        \r\n        #Initialize group_accuracies list\r\n        group_accuracies = []\r\n        \r\n        #--------------Generate a training set--------------------------------\r\n        #If size of train set < 100, check to make sure there is at least one \r\n        #example of each digit included\r\n        \r\n        if train_set_sizes[group_number] <= 100:\r\n        \r\n          #Initialize contains_all_digits list. This list will contain one entry per digit (n=10 entries total).\r\n          #The entries will be initialized with zeros, and each entry will be filled with a 1 when its corresponding\r\n          #digit is found in adhd_train_labels. When all entries = 1, then adhd_train set will contain all digits\r\n          contains_all_digits = []\r\n  \r\n          for i in range(10):\r\n      \r\n            contains_all_digits.append(0)\r\n           \r\n          while 0 in contains_all_digits:\r\n      \r\n            #Reinitialize contains_all_digits if a train set containing all digits has not been generated yet\r\n            for i in range(10):\r\n          \r\n              contains_all_digits[i] = 0\r\n    \r\n            train_set_examples = []\r\n            train_set_labels = []\r\n      \r\n            example_select_indices = np.random.randint(0,len(train_images),size=train_set_sizes[group_number])\r\n          \r\n            #randomly select examples\r\n            for i in range(train_set_sizes[group_number]):\r\n              \r\n              random_index = example_select_indices[i]\r\n              train_set_examples.append(train_images[random_index])\r\n              train_set_labels.append(train_labels[random_index])\r\n      \r\n            #check that labels contain all digits 0-9\r\n            for i in range(10):\r\n          \r\n               if i in train_set_labels:\r\n                  contains_all_digits[i] = 1\r\n          \r\n               else:\r\n                  contains_all_digits[i] = 0\r\n      \r\n        else:\r\n          \r\n          #Initialize train_set lists\r\n          train_set_examples = []\r\n          train_set_labels = []\r\n        \r\n          #randomly select indices to pull train set examples\r\n          example_select_indices = np.random.randint(0,55000,size=train_set_sizes[group_number])\r\n        \r\n          #For number of images in the appropriate training set group\r\n          for j in range(train_set_sizes[group_number]):\r\n          \r\n            random_index = example_select_indices[j]\r\n            train_set_examples.append(train_images[random_index])\r\n            train_set_labels.append(train_labels[random_index])\r\n        \r\n        # Create the MNIST Estimator\r\n        mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn)\r\n                                     \r\n        # Train the CNN model\r\n        mnist_classifier.train(input_fn=train_input_fn,steps=train_steps)\r\n        \r\n        # Evaluate the models and print results\r\n        eval_result = mnist_classifier.evaluate(input_fn=eval_input_fn)\r\n        print(\"Group Number \",group_number+1,\", ANN #\",i+1,\" Accuracy: \",eval_result,\" %\")\r\n      \r\n        #Record ANN accuracy\r\n        group_accuracies.append(eval_result)\r\n  \r\n  #record group accuracies in all_accuracies\r\n  all_accuracies.append(group_accuracies)\r\n  \r\n\r\n  #Print All Accuracy Results\r\n\r\n  print()\r\n\r\n  print(\"-------------------------------------Accuracy Results--------------------------------------------\")\r\n  \r\n  for i in range(len(train_set_sizes)):\r\n  \r\n    for j in range(numANNs):\r\n   \r\n      print(\"Group #\",i+1,\", ANN #\",j+1,\" Accuracy: \",eval_result,\" %\")\r\n    \r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n\r\n--------------------Output--------------\r\nExtracting MNIST-data/train-images-idx3-ubyte.gz\r\nExtracting MNIST-data/train-labels-idx1-ubyte.gz\r\nExtracting MNIST-data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST-data/t10k-labels-idx1-ubyte.gz\r\nINFO:tensorflow:Using default config.\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpge3laept\r\nINFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpge3laept', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb6c08b7c88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-16-bd0d2bfa6c12> in <module>()\r\n    137 \r\n    138 if __name__ == \"__main__\":\r\n--> 139   tf.app.run()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py in run(main, argv)\r\n    123   # Call the main function, passing through any arguments\r\n    124   # to the final program.\r\n--> 125   _sys.exit(main(argv))\r\n    126 \r\n\r\n<ipython-input-16-bd0d2bfa6c12> in main(unused_argv)\r\n    111 \r\n    112         # Train the CNN model\r\n--> 113         mnist_classifier.train(input_fn=train_input_fn,steps=train_steps)\r\n    114 \r\n    115         # Evaluate the models and print results\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    364 \r\n    365       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 366       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    367       logging.info('Loss for final step: %s.', loss)\r\n    368       return self\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1117       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1118     else:\r\n-> 1119       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1120 \r\n   1121   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1127       features, labels, input_hooks = (\r\n   1128           self._get_features_and_labels_from_input_fn(\r\n-> 1129               input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n   1130       worker_hooks.extend(input_hooks)\r\n   1131       estimator_spec = self._call_model_fn(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _get_features_and_labels_from_input_fn(self, input_fn, mode)\r\n    983           lambda: self._call_input_fn(input_fn, mode))\r\n    984     else:\r\n--> 985       result = self._call_input_fn(input_fn, mode)\r\n    986 \r\n    987     return estimator_util.parse_input_fn_result(result)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _call_input_fn(self, input_fn, mode)\r\n   1072       kwargs['config'] = self.config\r\n   1073     with ops.device('/cpu:0'):\r\n-> 1074       return input_fn(**kwargs)\r\n   1075 \r\n   1076   def _call_model_fn(self, features, labels, mode, config):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/numpy_io.py in input_fn()\r\n    196         num_threads=num_threads,\r\n    197         enqueue_size=batch_size,\r\n--> 198         num_epochs=num_epochs)\r\n    199 \r\n    200     batch = (\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py in _enqueue_data(data, capacity, shuffle, min_after_dequeue, num_threads, seed, name, enqueue_size, num_epochs, pad_value)\r\n    482                 random_start=shuffle,\r\n    483                 seed=seed_i,\r\n--> 484                 num_epochs=num_epochs))\r\n    485       else:\r\n    486         feed_fns.append(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py in __init__(self, placeholders, ordered_dict_of_arrays, batch_size, random_start, seed, num_epochs)\r\n    217     self._epoch = 0\r\n    218     random.seed(seed)\r\n--> 219     self._trav = random.randrange(self._max) if random_start else 0\r\n    220     self._epoch_end = (self._trav - 1) % self._max\r\n    221 \r\n\r\n/usr/lib/python3.6/random.py in randrange(self, start, stop, step, _int)\r\n    186             if istart > 0:\r\n    187                 return self._randbelow(istart)\r\n--> 188             raise ValueError(\"empty range for randrange()\")\r\n    189 \r\n    190         # stop argument supplied.\r\n\r\nValueError: empty range for randrange()", "It looks like you are using the deprecated queue API. Can you try with tf.data.Dataset?", "Ok, I will try a dataset. Should the code on the Datasets for Estimators\npage (https://www.tensorflow.org/guide/datasets_for_estimators) work?\n\nThank you.\n\nOn Fri, Jul 27, 2018 at 3:37 PM, Karmel Allison <notifications@github.com>\nwrote:\n\n> It looks like you are using the deprecated queue API. Can you try with\n> tf.data.Dataset?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20997#issuecomment-408518344>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtHjiApAq74XdOmUHqD9ja4tKJkiFks5uK2wXgaJpZM4VYT0n>\n> .\n>\n\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n", "Yes, it should. Let me know how it goes--", "I found python scripts in the introductory tensorflow download documents\n(which also include scripts for the iris classification problem used to\ndescribe estimators) for MNIST in the directory\n\\TensorFlow_models-master\\models-master\\official\\mnist. Are there any\naccompanying materials on the Tensorflow website that describe the code in\nthe dataset.py and mnist.py scripts found in this folder?\n\nThank you.\n\nOn Mon, Jul 30, 2018 at 7:33 PM, Karmel Allison <notifications@github.com>\nwrote:\n\n> Yes, it should. Let me know how it goes--\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20997#issuecomment-409045092>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtLriM9h86Rmy6CPfqKsqcsU9p8lSks5uL5fVgaJpZM4VYT0n>\n> .\n>\n\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n", "The documentation and README in that repo are all that is for available for that model, but there are also a number of [tutorials](https://www.tensorflow.org/tutorials/) and a [dataset guide](https://www.tensorflow.org/guide/datasets)", "I'm attempting to use a Dataset as you recommended, but now I am getting a\nnew error that says \" ValueError: Duplicate node name in graph: 'packed'\".\n\nI am trying to convert the MNIST data to a tensor, select subsets of the\ntraining data into lists, convert the lists into tensors, and then convert\nthe \"list\" tensors into datasets.\n\nHowever, the classifier.train command is still not working.\n\nDo you have any suggestions on how to fix this or would you recommend I try\na different strategy?\n\nThank you.\n\n\nOn Wed, Aug 1, 2018 at 1:53 PM, Karmel Allison <notifications@github.com>\nwrote:\n\n> The documentation and README in that repo are all that is for available\n> for that model, but there are also a number of tutorials\n> <https://www.tensorflow.org/tutorials/> and a dataset guide\n> <https://www.tensorflow.org/guide/datasets>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20997#issuecomment-409662630>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtL4XJPAG7DGb6ZD9hEEkYwYibDgzks5uMes0gaJpZM4VYT0n>\n> .\n>\n\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n", "It's hard for me to help debug without the code, data, and the like. Maybe start from scratch with the [Official MNIST](https://github.com/tensorflow/models/tree/master/official/mnist), and see if you can get that running?", "Hello again,\n\nI downloaded the Official MNIST scripts and tried running them but I got\nan  ImportError: No module named official.mnist.\n\nI went to the Tensorflow Official models website (found here\n<https://github.com/tensorflow/models/tree/master/official#running-the-models>),\nand tried to follow the directions under the Requirements section. However,\nI learned that these directions are only for Linux. Are there additional\ndirections available for Windows systems?\n\nThank you.\n\n\n\nOn Sat, Aug 4, 2018 at 12:07 PM, Karmel Allison <notifications@github.com>\nwrote:\n\n> It's hard for me to help debug without the code, data, and the like. Maybe\n> start from scratch with the Official MNIST\n> <https://github.com/tensorflow/models/tree/master/official/mnist>, and\n> see if you can get that running?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20997#issuecomment-410459397>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtHgEFGJfxxV8VNXAsKVpj8kQAXx3ks5uNcbAgaJpZM4VYT0n>\n> .\n>\n\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n", "I was actually able to get the Official models installed and working\nproperly, although I still need to try to manipulate the training set by\ntaking out subsets to train separate networks. Could I still follow up on\nhere if I am not able to do so?\n\nAlso, would you have any advice about how to get the official models to\nwork in a Colaboratory Notebook? I have tried running the MNIST model in my\nNotebook\n<https://colab.research.google.com/drive/13Gl9l-xnIL4OeNgd-Wqrb2bMcG10NUF1>,\nbut I get  ModuleNotFoundError: No module named 'official' on the line \"from\nofficial.mnist import dataset\".\n\nI have tried uploading all of the scripts in the \"official\" folder (that\nincludes all the Official Models) to my google drive but this did not help.\n\nThank you.\n\nOn Wed, Aug 22, 2018 at 3:29 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Nagging Assignee @karmel <https://github.com/karmel>: It has been 14 days\n> with no activity and this issue has an assignee. Please update the label\n> and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20997#issuecomment-415149207>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtPtWaQZlp7yihunMU6vNaSJ1udyBks5uTbEPgaJpZM4VYT0n>\n> .\n>\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n", "Nagging Assignee @karmel: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I was able to get the model working properly by using the Keras library so\nmy issues has been resolved.\n\nThank you for the help.\n\nOn Tue, Jul 24, 2018 at 12:52 PM Karmel Allison <notifications@github.com>\nwrote:\n\n> I apologize, but I am having a hard time understanding what the problem\n> is, where the problem is, and what version it affects. What script are you\n> running? What is the error trace that you are seeing?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20997#issuecomment-407474850>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZZjtJIaQVKqZnLvfaO74x8cOy4bBmXgks5uJ1C2gaJpZM4VYT0n>\n> .\n>\n\n\n-- \nMarkus Ville Tiitto, Pharm.D.\nPhD Student\nClinical & Experimental Therapeutics\nDepartment of Pharmaceutical Sciences\nUniversity of Kentucky, College of Pharmacy\nLee Todd, Jr. Building Rm 466\n789 S. Limestone St.\nLexington, KY 40536\nVilleTiitto2@gmail.com\nmarkus.tiitto@uky.edu\n(412) 296-9192\n", "Great, thanks for updating. Closing this.", "@mtiitto I suffered the same problem \"ValueError: Empty range for randrange()\" when I attempted to train my network. The error occurred in this line:\r\n`mnist_classifier.train(\r\n        input_fn=train_input_fn,\r\n        steps=2000,\r\n        hooks=[logging_hook]\r\n    )` \r\nSo, do you have any suggestions for me to solve this problem. My tensorflow version and python version are 1.13.1 an 3.7, respectively."]}, {"number": 20996, "title": "Terminology changes in bazel scripts", "body": "Quite a few functions in TF bazel scripts can be used on both CUDA and ROCm\r\nplatform. In this commit rename them to more generic term:\r\n\r\n- tf_cuda_* -> tf_gpu_*\r\n- cuda_py_* -> gpu_py_*", "comments": ["@yifei, @gunan I think this requires some copybara changes as well?", "Thanks @whchung and @rmlarsen. Agree that the naming is confusing now. We also have a bunch of internal targets of those two types as well though. WDYT is the best thing we should do here @gunan?", "I do not think this is as straightforward as just renaming. There are some non-opensource google projects which started using these. As soon as this gets submitted, hundreds of internal targets will be broken.\r\nI think this either needs to be done internally, or we need separate targets for rocm and cuda.\r\nIt may be tied to the discussion in #20277, so I would like @jlebar to weigh in here as well.\r\n", "What I did last time I had a renaming change that also affected internal targets was, do the review externally, approve it, import it, and then continue the renaming internally before committing the change.\r\n\r\nSo it should be *possible*.  That's separate from the question of whether it's desirable.\r\n\r\n@Artem-B might have more concrete thoughts.  To me it seems like you may want a level of indirection between how you spell things in BUILD files and the actual BUILD targets.  Just like we have `xla_test(foo)` which expands into foo_gpu and foo_cpu (and you could imagine changing this so it expands into `:foo_nvgpu`, `:foo_amdgpu`, `:foo_cpu`), maybe you want tf_gpu_lib that expands into cuda and rocm variants.  Then if you have a `tf_rocm` mega-library, it can glob together all of the rocm build targets (you can glob based on build tags, not just on name).\r\n\r\nSo in that sense this PR sort of seems to me like it's going in the right direction.  But again I'd want to know what @Artem-B thinks.", "Hi @rmlarsen - Just wanted to re-engage on this PR and to discuss the next steps we should take.  In summary, it sounds like there's support for the PR, but there are some non-open source projects that may be affected.  \r\n\r\nAny thoughts on @jlebar's previously used approach?  \r\n> do the review externally, approve it, import it, and then continue the renaming internally before committing the change", "One comment I have is, the renaming that needs to be done internally is a pretty massive change.\r\n@thirupalanisamy to discuss this internally and find an owner for this task.", "Hi @thirupalanisamy - Just wanted to check if there are any updates from your side regarding @gunan's comment last week.  Let us know if there's anything we can do to help move this forward.  ", "Thanks for the ping.\r\n\r\nUpdate: We started an internal discussion re the scope of work, will\r\nkeep you posted.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Just wanted to ping this PR with an acknowledgement.  We'll be working on this PR with Gunan's offline suggestions: \r\n- rename from tf_cuda* to tf_gpu*\r\n- tf_cuda* must remain operational\r\n- create new tf_cuda* rules that simply call tf_gpu* rules\r\n\r\n(and over time, work on moving all tf_cuda* rules to the new tf_gpu* rules slowly without breaking anything)", "@whchung can we close this PR?", "The plan is to simplify this PR with the details provided above.  ", "@parallelo OK, I see.\r\n@whchung can you rebase this PR, if you intend to continue working on it?a\r\n", "FYI - To simplify the process, I just created a replacement PR.  (#22747)  "]}, {"number": 20995, "title": "Cannot set random_seed for DNNClassifier", "body": "### System information\r\nMy own code \r\nWindows 10\r\nPython 3.6.5\r\nTensorflow 1.9 (problem also occurs with 1.8)\r\nNVIDIA GeForce GTX 1050 (approx total memory 6011MB)\r\nCUDA version 9.0 v9.0.176\r\n\r\nProblem Description\r\nEach time I run the code below, I get a different 'Loss for final step' when training my model. I have checked that the input data from train_test_split is constant. I have set the value of tf.random_seed, turned off shuffling and set the value of num_threads.  \r\nLooking at INFO, I can see that when training my model, the config file has _tf_random_seed set to 'None'.  There does not appear to be a way of setting this  for tf.estimator.DNNClassifier. The only solution I have found is to 'hack' run_config.py and set random_seed there. Is this a serious flaw with tf.estimator.DNNClassifier or am I missing something?\r\n\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nnp.random.seed(1)\r\ntf.set_random_seed(50)\r\n\r\ndf = pd.read_csv('diabetes.csv')\r\nX = df.iloc[:,0:8]\r\ny = df['Outcome']\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\r\n                                   stratify=None, random_state=1)\r\n\r\ndef create_feature_cols():\r\n  return [\r\n    tf.feature_column.numeric_column('Pregnancies'),\r\n    tf.feature_column.numeric_column('Glucose'),\r\n    tf.feature_column.numeric_column('BloodPressure'),\r\n    tf.feature_column.numeric_column('SkinThickness'),\r\n    tf.feature_column.numeric_column('Insulin'),\r\n    tf.feature_column.numeric_column('BMI'),\r\n    tf.feature_column.numeric_column('DiabetesPedigreeFunction'),\r\n    tf.feature_column.numeric_column('Age')\r\n  ]\r\n\r\n\r\ninput_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,\r\n             batch_size=10,num_epochs=1000,shuffle=False,num_threads=1)\r\nmodel  =  tf.estimator.DNNClassifier(hidden_units=[20,20],\r\n          feature_columns=create_feature_cols(),n_classes=2)\r\nmodel.config._tf_random_seed=1\r\ntf.set_random_seed(1)\r\nmodel.train(input_fn=input_func,steps=1000)\r\n\r\neval_input_func = tf.estimator.inputs.pandas_input_fn(\r\n      x=X_test,\r\n      y=y_test,\r\n      batch_size=10,\r\n      num_epochs=1,\r\n      shuffle=False,\r\n      num_threads=1)\r\nresults = model.evaluate(eval_input_func)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution : **Windows 10**\r\nTensorFlow installed from: **Anaconda installation**\r\nTensorFlow **version 1.9 (problem also occurs with 1.8) running with Python 3.6.5**\r\nBazel version: **N\\A**\r\nCUDA/cuDNN version: **CUDA version 9.0 v9.0.176**\r\nGPU model and memory: **NVIDIA GeForce GTX 1050 (approx total memory 6011MB)**\r\nExact command to reproduce: **N\\A**\r\nMobile device: **N\\A**\r\n", "I thought estimator random seed is set through RunConfig, not through tf.set_random_seed().\r\nSo the thing you could do here is initialize your constructor in this way:\r\nconfig = tf.RunConfig(tf_random_seed=???)\r\nclf = DNNClassifier(hidden_units=[20,20],\r\nfeature_columns=create_feature_cols(),n_classes=2, config=config)\r\n\r\nCan you try this and let us know if it works?", "Hi\r\nI tried the above code but got the message:\r\n\r\n\"AttributeError: module 'tensorflow' has no attribute 'RunConfig'\"\r\n\r\nI then tried:\r\n\r\n config = tf.estimator.RunConfig(tf_random_seed=234)\r\n\r\nbut this also failed. Finally I tried:\r\n\r\nconfig = tf.contrib.learn.RunConfig(tf_random_seed=234)\r\n\r\nThis works! Is using contrib.learn the best way to set the random seed?\r\n\r\nThanks", "when you run 'config = tf.estimator.RunConfig(tf_random_seed=234)', what failed?", "ValueError: config must be an instance of RunConfig, but provided <tensorflow.python.estimator.run_config.RunConfig object at 0x000002179AEF3A20>.", "@AdamPWhite I don't have your dataset so I can't test for you. But below please find my code snippet which works fine:\r\n\r\nimport tensorflow as tf\r\ntf.reset_default_graph()\r\nconfig = tf.estimator.RunConfig(tf_random_seed=234)\r\n\r\ninput1_col = tf.feature_column.numeric_column('input1')\r\ninput2_col = tf.feature_column.numeric_column('input2')\r\nmodel = tf.estimator.DNNClassifier(hidden_units=[20,20],\r\nfeature_columns=[input1_col, input2_col],n_classes=2,config=config)\r\n\r\nimport numpy as np\r\ninput1 = np.random.random(size=(100, 1))\r\ninput2 = np.random.random(size=(100, 1))\r\ntarget = np.where(np.sum(input1 + input2, axis = 1) > 0, 1, 0)\r\ndef train_input_fn():\r\n  return ({'input1': input1, 'input2': input2}, target)\r\n\r\nmodel.train(input_fn=train_input_fn, steps = 10)\r\n\r\n# result:\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp70021xj7/model.ckpt.\r\nINFO:tensorflow:loss = 61.109077, step = 1\r\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/tmp70021xj7/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.6393224.\r\n<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f0fb18c6ef0>", "Hi\r\nI guess there must be some difference as to how our systems are configured as I have to use  tf.contrib.learn.RunConfig(tf_random_seed=234). However, this is good enough for me, so let's close the case. Many thanks for your help.", "The only difference I see is tf version. I'm using 1.10."]}]