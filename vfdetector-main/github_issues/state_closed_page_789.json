[{"number": 29875, "title": "Fix generated docker files drift from partial.Dockerfile", "body": "Ubuntu version of docker images was upgraded to 18.04 by #28717, but generated Dockerfiles were not commited.  \r\nThis PR fix the drift. All changes are generated by below command according to [contribution guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/dockerfiles#contributing).\r\n\r\n```bash\r\n$ asm_dockerfiles --release dockerfiles --construct_dockerfiles\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29875) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29875) for more info**.\n\n<!-- ok -->"]}, {"number": 29874, "title": "I think Windows performance might be poor due to WDDM? tensorflow-gpu", "body": "So personally I have very poor performance on windows when it comes to object detection. Doesn't matter if I build from source, pip, conda etc, it's about 5-10x slower on windows vs linux. \r\n\r\nI see sporadic reports across tensorflow and pytorch stating similar things. Some bugs confirmed, but with no follow up. \r\n\r\nI'm hoping someone on this project has access to a non consumer nvidia card. I think that's the titan series. None of the geforce cards will work (thanks nvidia). I ask you to do any of the object detection api tutorials, or even any custom implementation.  Test it on linux, then the same on windows. \r\n\r\nI'm hyper confident you will find a significant discrepancy between the two. Then enable TCC on your non consumer card, and see if the performance then becomes similar. \r\n\r\nI'm basing my assumption on this:\r\nhttps://stackoverflow.com/questions/19944429/cuda-performance-penalty-when-running-in-windows\r\n\r\n", "comments": [" Thanks for reaching out to us. Please let us know which TensorFlow GPU version you are using.", "@achandraa recently I'm using 2.0 beta gpu. From pip and from source.\r\n\r\nI think I also had issues with 1.13 though. I'll spend some time verifying tonight when I get home.\r\n\r\nEdit: I should add for 2.0, I'm using this:\r\nhttps://github.com/zzh8829/yolov3-tf2", "@rlewkowicz Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "@jvishnuvardhan I'm having trouble with the 2.0 release due to https://github.com/tensorflow/tensorflow/issues/31509. I'm debating how much time I want to spend on it. Are you asking me because you know of a specific change that might have fixed the issue, or are you just seeing if it worked itself out?", "@rlewkowicz There were lots of improvements between `2.0beta` to `2.0`. I am thinking your issue might have been resolved. If possible, Check `tf-nightly` also. Thanks!", "I'm running a ResNet56v2 model training experiment, each epoch has 1600 image samples with 224*244 pixels. It is interesting that when I trained it on Windows 10, it took 15 hours per epoch! While the time is only 10 hour on macOS. 1.5x time!\r\nMy environment is, conda python 3.7 on Windows and homebrew python 3.7 on macOS, tf 2.1 for both system.\r\nMy mac is 2015 MBPR with Intel i7 4770HQ.", "Just as a heads up, I had done some basic tests, unfortunately it's not been resolved. Ultimately I'm hyper confident that it's WDDM vs TCC. It's just unfortunate my 1400 dollar graphics card doesn't support TCC because nvidia chooses to gate their hardware. I have to imagine someone on this project has a line into nvidia. Really they just need to stop gating TCC.  Either that or windows needs to provide their own low level interface ", "@rlewkowicz  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Can you please execute your code using Latest stable Version(2.5) and let us know if the issue still persists? Thanks!", "> @rlewkowicz It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Can you please execute your code using Latest stable Version(2.5) and let us know if the issue still persists? Thanks!\r\n\r\nUnfortunately I'm deep into some other projects right now and wont have the time to get this environment off the ground. Ultimately, I'm not confident that anything short of a major architectural shift is going to help. When the OS eats a bunch of space on your GPU out of the gate, you know it's not good. It's hands are in the pie before you can get a slice. \r\n\r\nPick any implementation based on speed. Preferably something in the 60-100 fps range. Yolo v3 tiny for example. Run it on linux, run it on windows. Windows is going to be drastically slower. ", "Is this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.7 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29874\">No</a>\n"]}, {"number": 29873, "title": "[BugFix] Fix bug of regularizer in tf.layers under DistributionStrategy", "body": "There is a bug when building graph in tf.layers under DistributionStrategy. The regularizer subgraph will be only built on `replica 0` and it will be ignored on other `replicas` in `MirroredStrategy`.  Actually, regularizer subgraph should be related to variable type. \r\nAnother, if one builds regularization_loss via tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSS), it may cause some problems:\r\n1. Regroup Assertion error: \r\n    If one builds two layers with regularizers but only trained with first layer\uff0cand get regularization loss from global collection under `MirroredStrategy`, the second layer will has gradient on replica 0 but None on other replicas, which leads to regroup error after call into `merge_call`(Although the second layer's regularization is unnecessary to build, but it is not easy for users to remove in model_fn).\r\n2. Performance issue: \r\n    Global collection is not thread local in `model_fn`, so we will get all regularizers on all replicas. However, before we fix this bug, the regularizer will only exist on `replica 0`. This will involve device communications. \r\n\r\nPlease take a look at this PR. Thanks. @yuefengz @martinwicke @anj-s ", "comments": ["Thanks for the PR! Looks like the unit test file is missing, can you add that as well before we review? thanks", "@guptapriya Sorry, I missed a source file. Now the unit test has been added. Please check it again. Thanks.", "Thanks for adding the test file! I'll let @anj-s review.", "This PR has been pending for some days. Any update for me\uff1f", "The coding style I will refine later. Thanks.", "@anj-s code has been refined. Please check it again. Thanks.", "Can one of the admins verify this patch?", "Thanks, @wangsiyu -- we have discussed this contribution a fair amount internally, and unfortunately, we cannot accept these changes to tf.layers. The tf.layers APIs have been deprecated, and we can only accept critical bug fixes for supported usages. \r\n\r\nThis change has far-reaching effects for tf.layers (eg the addition of mirrored variables to a collection), and we cannot fully anticipate the effects of those changes on existing users. Further, Distribution Strategies with Estimator is experimental currently and not fully supported, and Distribution Strategies with Estimator with tf.layers is even further from the preferred usage, so rather than make changes that could affect current users, we would encourage you to take one of two approaches:\r\n\r\n1. If you want to use Distribution Strategies, please consider migrating to 2.0 APIs-- in this case, tf.keras.layers rather than tf.layers. The Keras Layers have close analogues to each tf.layers layer, and so you can convert one-by-one to the new Layers without changing model architecture significantly.\r\n\r\n2. Update the model_fn to explicitly add L2 loss. This is what we do in the Resnet example here: https://github.com/tensorflow/models/blob/master/official/r1/resnet/resnet_run_loop.py#L428\r\n\r\nThanks for your understanding here.", "thank you for your contribution , closing this PR based on Karmel comments."]}, {"number": 29872, "title": "[TF 2.0] categorical_column_with_vocabulary_list no longer usable with tf.functiion", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, slightly\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 30\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nThe initialization of the lookup table fails with:\r\n```    \r\n    TypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: dense_features/kind_embedding/kind_lookup/Const:0\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nAs with v2.0.0-alpha.0, feature columns with constant vocabulary lists should be usable in tf.function graphs. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```py\r\nimport tensorflow as tf\r\nimport tensorflow.feature_column as fc\r\n\r\nCOLUMNS = [\r\n  fc.embedding_column(\r\n    fc.categorical_column_with_vocabulary_list('kind', ['a', 'b', 'c']),\r\n    2\r\n  )\r\n]\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(COLUMNS, trainable=False)\r\n\r\nfeatures = {\r\n  'kind': tf.constant(['a', 'a', 'b', 'b', 'c', 'c']),\r\n}\r\n\r\n@tf.function\r\ndef func(features):\r\n  return feature_layer(features)\r\n\r\nfunc(features)\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\n\r\n<details>\r\n<summary>Traceback</summary>\r\n<pre>\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 21, in <module>\r\n    print(func(features))\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\", line 416, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\", line 359, in _initialize\r\n    *args, **kwds))\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/eager/function.py\", line 1360, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/eager/function.py\", line 1648, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/eager/function.py\", line 1541, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/framework/func_graph.py\", line 716, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\", line 309, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \".../venv/lib/python3.5/site-packages/tensorflow/python/framework/func_graph.py\", line 706, in wrapper\r\n    raise e.ag_error_metadata.to_exception(type(e))\r\nTypeError: in converted code:\r\n\r\n    bug.py:19 func  *\r\n        return feature_layer(features)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py:667 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:473 call  *\r\n        tensor = column.get_dense_tensor(transformation_cache,\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3123 get_dense_tensor\r\n        transformation_cache, state_manager)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3714 get_sparse_tensors\r\n        transformation_cache.get(self, state_manager), None)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2562 get\r\n        transformed = column.transform_feature(self, state_manager)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3692 transform_feature\r\n        return self._transform_input_tensor(input_tensor)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3686 _transform_input_tensor\r\n        name='{}_lookup'.format(self.key)).lookup(input_tensor)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py:1388 index_table_from_tensor\r\n        table = StaticHashTableV1(init, default_value)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py:284 __init__\r\n        super(StaticHashTable, self).__init__(default_value, initializer)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py:174 __init__\r\n        self._init_op = self._initialize()\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py:177 _initialize\r\n        return self._initializer.initialize(self)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/ops/lookup_ops.py:424 initialize\r\n        self._values)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_lookup_ops.py:785 lookup_table_import_v2\r\n        table_handle, keys, values, name=name, ctx=_ctx)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/ops/gen_lookup_ops.py:820 lookup_table_import_v2_eager_fallback\r\n        attrs=_attrs, ctx=_ctx, name=name)\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/eager/execute.py:71 quick_execute\r\n        raise e\r\n    .../venv/lib/python3.5/site-packages/tensorflow/python/eager/execute.py:61 quick_execute\r\n        num_outputs)\r\n\r\n    TypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: dense_features/kind_embedding/kind_lookup/Const:0\r\n</pre>\r\n</details>\r\n\r\nThis bug was likely introduced by https://github.com/tensorflow/tensorflow/commit/2f0b7b1c2f5638a157af76383bd5a42bd3cc2938#diff-0972bc0b553e347b626ce302457971dfR383\r\n\r\nSomewhat related issue https://github.com/tensorflow/tensorflow/issues/27086\r\n", "comments": ["I am able to reproduce the issue with Tensorflow 2.0.0.beta1 on Google colab. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29872\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29872\">No</a>\n"]}, {"number": 29871, "title": "TF 2.0 Upgrade Script: Unable to handle the @ operator for matrix multiplication", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\nRunning the tf_upgrade_v2 script with a file containing the @ operator results in an exception (see below).\r\n\r\n\r\n**Code to reproduce the issue**\r\nUsing the file tmp.py with the following content:\r\n\r\nimport numpy as np\r\ndef mul(a, b):\r\n     z = a @ b\r\n\r\nThen run: \r\n .\\tf_upgrade_v2.exe --infile \"C:\\any\\path\\tmp.py\" --outfile \"C:\\another\\path\\tmp.py\"\r\n\r\n**Other info / logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"C:\\Program Files\\Python37\\lib\\ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 690, in visit_BinOp\r\n    op_symbol = ast_constants.NODE_TYPE_TO_TOKENS[type(node.op)][0]\r\nKeyError: <class '_ast.MatMult'>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Program Files\\Python37\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\pzobel\\PycharmProjects\\TF2_0_Beta_Test\\venv\\Scripts\\tf_upgrade_v2.exe\\__main__.py\", line 9, in <module>\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\tensorflow\\tools\\compatibility\\tf_upgrade_v2_main.py\", line 139, in main\r\n    args.input_file, output_file, upgrade)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\tensorflow\\tools\\compatibility\\tf_upgrade_v2_main.py\", line 40, in process_file\r\n    upgrader.process_file(in_filename, out_filename)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 900, in process_file\r\n    temp_file)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 960, in process_opened_file\r\n    self.update_string_pasta(\"\".join(lines), in_filename))\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\tensorflow\\tools\\compatibility\\ast_edits.py\", line 916, in update_string_pasta\r\n    t = pasta.parse(text)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\__init__.py\", line 25, in parse\r\n    annotator.visit(t)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"C:\\Program Files\\Python37\\lib\\ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 220, in visit_Module\r\n    self.generic_visit(node)\r\n  File \"C:\\Program Files\\Python37\\lib\\ast.py\", line 270, in generic_visit\r\n    self.visit(item)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"C:\\Program Files\\Python37\\lib\\ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 95, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 411, in visit_FunctionDef\r\n    self.visit(stmt)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 1194, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 132, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"C:\\Program Files\\Python37\\lib\\ast.py\", line 262, in visit\r\n    return visitor(node)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 530, in visit_Assign\r\n    self.visit(node.value)\r\n  File \"c:\\users\\pzobel\\pycharmprojects\\tf2_0_beta_test\\venv\\lib\\site-packages\\pasta\\base\\annotate.py\", line 1196, in visit\r\n    raise AnnotationError(e)\r\npasta.base.annotate.AnnotationError: <class '_ast.MatMult'>\r\n", "comments": ["I have tried to reproduce the issue on my system and was able to do it.", "@soupytwist, FYI. I'll send you a PR.", "Automatically closing this out since I understand it to be resolved by the PR https://github.com/google/pasta/pull/67 (merged already), but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29871\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29871\">No</a>\n", "That will resolve it, but until pasta is released with the change, and I\nadd an updated dependency, users might have to reinstall pasta manually.\nIt's probably ok to close.\n", "I am running into the same issue but not sure how to resolve it. My pasta library is updated but I am still getting the same error and can't upgrade to TensorFlow 2. ", "Hi @TrentBrick,\r\nit seems that the latest release of pasta is from [May 29](https://github.com/google/pasta/releases) and therefore @martinwicke 's fix is not in the latest release yet. \r\nPlease install pasta from source or manually add the [fix](https://github.com/google/pasta/pull/67) to your code until they added a new release.", "Hi, I've just released pasta v0.1.8 which contains the latest fixes. Sorry for the delay!", "I don't yet see a v0.1.8 tag or release on GitHub, am I just not seeing it?", "> I don't yet see a v0.1.8 tag or release on GitHub, am I just not seeing it?\r\n\r\nPushed the tag up now, forgot this. Thanks"]}, {"number": 29870, "title": "Use Tensorflow like Numpy, disable gradients entirely", "body": "I thought about using tensorflow for occasions where I have used numpy so far.\r\n\r\nMost of the time gradients take up the most memory when doing \"NN\" things with tensorflow.  I was wondering if Tensorflow (v 2.0) always collects gradients in the background, even when I don't  use them.\r\n\r\nAnd if that is the case, if there is a way to disable gradients entirely in order to conserve memory.\r\n\r\nDoes Tensorflow v2 only compute/store intermediate gradients when using operations inside `tf.GradientTape`? Or does tf.GradientTape just \"record\" or \"map\" to them?\r\n\r\nI didn't think this is a Stackoverflow issue, as if this is indeed the case it might be a feature request.", "comments": ["GradientTape holds its own reference to intermediate activations, a regular Python refcount increment. Without a tape we don't reference any activations (but they may be referenced somewhere in your code, e.g. in a Python variable). So you're good already; just don't use a tape and memory will be fine.\r\n\r\nGradientTape also doesn't do any computation until `.gradient` is called, it just saves a record of operations and input and output Tensors. TF doesn't do any gradient computation without a tape active.", "Thank you for your answer @allenlavoie. This would be nice to see in the GradientTape documentation.", "@allenlavoie is this applicable to 1.15? When I am building a graph, does the tape come in as a part of optimizer?", "Yes, optimizers will compute gradients (either `tf.gradients` or `tf.GradientTape` depending on v1/v2). Using a gradient-based optimizer (or `GradientTape`/`gradients` manually) will change the structure of the graph to increase memory usage in the same way it happens executing eagerly."]}, {"number": 29869, "title": "Bug in tf.einsum - Returns different values from np.einsum for identical parameters", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary (Anaconda)\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0/7.6.0\r\n- GPU model and memory: NVIDIA Titan X (Pascal), 12 GB\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.einsum` returns buggy values compared to `np.einsum` for identical parameters. Here is the current output (please find code below):\r\n\r\n```\r\nnp: 8.610251426696777\r\ntf: 0.0\r\n```\r\n\r\nPlease also note that the same code works as expected when running on the CPU. It also works correctly on the GPU in TF 1.12.0, CUDA/cuDNN 9.0/7.3.1 and Python 3.6.8.\r\n\r\n**Describe the expected behavior**\r\n\r\nHere is the expected output:\r\n\r\n```\r\nnp: 8.610251426696777\r\ntf: 8.610251426696777\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nfrom __future__ import (\r\n    absolute_import,\r\n    division,\r\n    print_function\r\n)\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\n\r\neinsum_string = \"bijk,bijk->bij\"\r\n\r\n# load values\r\nS_h_numpy = np.load(\"./S_h.npy\")\r\nS_h = tf.convert_to_tensor(S_h_numpy, dtype=tf.float32)\r\n\r\nh_numpy = np.load(\"./h.npy\")\r\nh = tf.convert_to_tensor(\r\n    h_numpy,\r\n    dtype=tf.float32\r\n)\r\n\r\n# perform einsum\r\nht_S_h_numpy = np.einsum(einsum_string, h_numpy, S_h_numpy)\r\nht_S_h = tf.einsum(einsum_string, h, S_h)\r\n\r\nprint(\"np: {np_val}\\ntf: {tf_val}\".format(\r\n    np_val=ht_S_h_numpy[2, 191, 191],\r\n    tf_val=ht_S_h[2, 191, 191],\r\n))\r\n```\r\n\r\n**Other info / logs**\r\n\r\nHere are the input tensors used in the example: [input_tensors.zip](https://github.com/tensorflow/tensorflow/files/3296468/input_tensors.zip).\r\n\r\nThe tensors are of shape (4, 192, 192, 2). I could not choose a smaller example because the buggy values appear only in part of the second and all of the following batch items. The bug therefore seems to be connected to the size of the input.\r\n\r\n", "comments": ["I have to add that after updating my GPU drivers and disabling the eager mode, the code seems to have the correct output. I don't use the eager mode by default, only for debugging. Does this mean that I shouldn't use it at all?\r\n\r\nEDIT: The same type of error persists but with slightly different parameters. I am now certain that this is a Tensorflow issue.", "I am able to reproduce the issue on colab with Tensorflow 1.13.1. Thanks!", "I can confirm I can reproduce a difference between tf and numpy only on GPU for tensors of that shape, even without using the specific values. Running on the CPU makes the problem go away.\r\n\r\nMy repro (on tf-nightly-gpu-2.0-preview) is\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\neinsum_string = \"bijk,bijk->bij\"\r\n\r\n# load values\r\nS_h_numpy = np.random.random([4, 192, 192, 2]).astype(np.float32)\r\nS_h = tf.convert_to_tensor(S_h_numpy, dtype=tf.float32)\r\n\r\nh_numpy = np.random.random([4, 192, 192, 2]).astype(np.float32)\r\nh = tf.convert_to_tensor(\r\n    h_numpy,\r\n    dtype=tf.float32\r\n)\r\n\r\n# perform einsum\r\nwith tf.device('cpu:0'):\r\n  ht_S_h_numpy = np.einsum(einsum_string, h_numpy, S_h_numpy)\r\n  ht_S_h = tf.einsum(einsum_string, h, S_h)\r\n\r\nprint(\"np: {np_val}\\ntf: {tf_val}\".format(\r\n    np_val=ht_S_h_numpy[2, 191, 191],\r\n    tf_val=ht_S_h[2, 191, 191],\r\n))\r\n```\r\n\r\nI think then this must be a bug in the GPU kernels.\r\n\r\n@rmlarsen do you know who is the best person to look at this?", "Is there any progress on that bug? I think I have a similar problem, here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/31022", "Escalating to @tatianashp ", "Anudhyan, would you have time to take a look at this?\n\nOn Thu, Aug 1, 2019 at 9:06 AM Alexandre Passos <notifications@github.com>\nwrote:\n\n> Escalating to @tatianashp <https://github.com/tatianashp>\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/29869?email_source=notifications&email_token=AEA72DVTJYZMUR53BHXGGMDQCMCZ7A5CNFSM4HYVNUZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3LCIZY#issuecomment-517350503>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEA72DXKAJ726VQL6A5UN3LQCMCZ7ANCNFSM4HYVNUZQ>\n> .\n>\n", "Sorry, I don't have spare cycles currently.\r\n\r\nHowever, this is still using the old tf.einsum and not the new EinsumOp. Which means that the graph would have exactly one tf.matmul (BatchMatMul) and one or two Transpose ops. So either of those two GPU kernels would be the source of the problem.", "Sorry, I am really running short on cycles this month :(\n\nHowever, this is still using the old tf.einsum and not the new EinsumOp.\nThe problem is then likely in the GPU kernel for BatchMatMul (which is a\nbit surprising, as it's been around for while).\n\nOn Mon, Aug 5, 2019 at 10:32 AM Rasmus Munk Larsen <rmlarsen@google.com>\nwrote:\n\n> Anudhyan, would you have time to take a look at this?\n>\n> On Thu, Aug 1, 2019 at 9:06 AM Alexandre Passos <notifications@github.com>\n> wrote:\n>\n>> Escalating to @tatianashp <https://github.com/tatianashp>\n>>\n>> \u2014\n>> You are receiving this because you were assigned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/29869?email_source=notifications&email_token=AEA72DVTJYZMUR53BHXGGMDQCMCZ7A5CNFSM4HYVNUZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3LCIZY#issuecomment-517350503>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AEA72DXKAJ726VQL6A5UN3LQCMCZ7ANCNFSM4HYVNUZQ>\n>> .\n>>\n>\n", "No worries. I'll take a look.\n\nOn Mon, Aug 5, 2019 at 3:15 PM Anudhyan Boral <anudhyan@google.com> wrote:\n\n> Sorry, I am really running short on cycles this month :(\n>\n> However, this is still using the old tf.einsum and not the new EinsumOp.\n> The problem is then likely in the GPU kernel for BatchMatMul (which is a\n> bit surprising, as it's been around for while).\n>\n> On Mon, Aug 5, 2019 at 10:32 AM Rasmus Munk Larsen <rmlarsen@google.com>\n> wrote:\n>\n>> Anudhyan, would you have time to take a look at this?\n>>\n>> On Thu, Aug 1, 2019 at 9:06 AM Alexandre Passos <notifications@github.com>\n>> wrote:\n>>\n>>> Escalating to @tatianashp <https://github.com/tatianashp>\n>>>\n>>> \u2014\n>>> You are receiving this because you were assigned.\n>>> Reply to this email directly, view it on GitHub\n>>> <https://github.com/tensorflow/tensorflow/issues/29869?email_source=notifications&email_token=AEA72DVTJYZMUR53BHXGGMDQCMCZ7A5CNFSM4HYVNUZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3LCIZY#issuecomment-517350503>,\n>>> or mute the thread\n>>> <https://github.com/notifications/unsubscribe-auth/AEA72DXKAJ726VQL6A5UN3LQCMCZ7ANCNFSM4HYVNUZQ>\n>>> .\n>>>\n>>\n", "I haven't tried this, but I think in theory, the problem should also be reproducible with:\r\n  a = np.random.random([147456, 1, 2]).astype(np.float32)   # 147456 = 4 * 192 * 192\r\n  b = np.random.random([147456, 2, 1]).astype(np.float32)\r\n  c = tf.matmul(a, b)", "i checked this code and the difference is zero for tf version 1.14\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.__version__)\r\n\r\n@tf.function\r\ndef mul(a,b):\r\n    return tf.matmul(a,b)\r\n\r\na = np.random.random([147456, 1, 2]).astype(np.float32)  # 147456 = 4 * 192 * 192\r\nb = np.random.random([147456, 2, 1]).astype(np.float32)\r\n\r\nconf = tf.ConfigProto()\r\nconf.gpu_options.allow_growth = True\r\nsess = tf.Session(config=conf)\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    c = mul(a,b).eval(session=sess)\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    c_gpu = mul(a,b).eval(session=sess)\r\n\r\nprint('difference', np.sum(np.abs(c-c_gpu)))\r\n```\r\n", "are there any news on this bug?", "I think this bug is a duplicate of this GPU Matmul kernel issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/26969\r\n\r\nI could reproduce this with matmul as well. Both the previous Python implementation of tf.einsum and the new implementation with EinsumOp  (submitted 09/27) uses the same GPU kernels as matmul.\r\n\r\nBelow is my reproduction on `tf-nightly-gpu-2.0-preview`.\r\n\r\n```\r\nwith tf.compat.forward_compatibility_horizon(2019, 10, 20):\r\n  n = 2**16  # gives correct answer with n = 2**16 - 1.\r\n  a = np.random.random([n, 1, 1]).astype(np.float32)\r\n  b = np.random.random([n, 1, 1]).astype(np.float32)\r\n\r\n  with tf.device('gpu:0'):\r\n    x = np.matmul(a, b)\r\n    y = tf.matmul(a, b)\r\n    z = tf.einsum(\"Aij,Ajk->Aik\", a, b)\r\n\r\n  print(\"np: {}\\ntf1: {}\\ntf2: {}\".format(\r\n      x[n - 1,0,0],\r\n      y[n - 1,0,0],\r\n      z[n - 1,0,0]\r\n  ))\r\n```", "Is there any update on this bug? I personally think einsum is a very powerful operation, and people might use it a lot. It would be not safe at all if we use it while knowing there exists a certain bug that makes it failed to work properly.", "Looks like referred GPU issue was closed as not reproducible in 2.1.0-dev20191226. Is the einsum problem still happening?", "Hi There,\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29869\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29869\">No</a>\n"]}, {"number": 29868, "title": "Typo in Tensorflow Core 2.0b's Advanced Tutorial=>Loading data=>Building an image input pipeline", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/images#load_and_format_the_images\r\n\r\n## Description of issue (what needs changing):\r\nUnder \"Load and format the images\" section, the original code below fails to run since it uses a variable `img_path`, which should be `image_path` instead.\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\n\r\nimage_path = all_image_paths[0]\r\nlabel = all_image_labels[0]\r\n\r\nplt.imshow(load_and_preprocess_image(img_path))\r\nplt.grid(False)\r\nplt.xlabel(caption_image(img_path))\r\nplt.title(label_names[label].title())\r\nprint()\r\n```\r\n\r\n### Clear description\r\nThis is a simple typo and needs to be fixed.\r\n\r\n### Correct links\r\nNot related.\r\n\r\n### Parameters defined\r\nNot related.\r\n\r\n### Returns defined\r\nNot related.\r\n\r\n### Raises listed and defined\r\nNot related.\r\n\r\n### Usage example\r\nNot related.\r\n\r\n### Request visuals, if applicable\r\nNot related.\r\n\r\n### Submit a pull request?\r\nNot this time.\r\n", "comments": ["I was able to execute the colab successfully. I see that ```img_path``` is defined under **load_and_format_the_images** section\r\nhttps://www.tensorflow.org/beta/tutorials/load_data/images#load_and_format_the_images\r\nTherefore the code should not break. Can you please confirm?", "I believe it should be the other way around, rename `image_path` to `img_path`. \r\n\r\n@ymodak You did not notice that `image_path `is unused did you? Obviously the example works.", "@ymodak As you mentioned it works, because `img_path` is defined in the beginning of sub-section `Load and format the images`.\r\nIf the use of `img_path` is intended, I think that `image_path` should be removed as @svobora said :-)\r\n", "This is being fixed here (https://github.com/tensorflow/docs/pull/699#discussion_r295488663)", "Closing this issue since the associated PR has been merged. Thanks!"]}, {"number": 29867, "title": "tf.range with tf.constant(int32) limit and dtype=tf.float32 fails", "body": "Tensorflow Version: tf-nightly-gpu-2.0-preview==2.0.0.dev20190611 or CPU equivalent (linux)\r\n\r\nAlso tested on: tf-nightly-2.0-preview==2.0.0.dev20190607 (windows)\r\n\r\nTry running the following two:\r\n\r\n```python\r\ntf.range(tf.constant(102), dtype=tf.float32) # FAILS\r\ntf.range(102, dtype=tf.float32) # WORKS\r\n```\r\n\r\nThe first one fails with\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-145c4a277289> in <module>\r\n----> 1 tf.range(tf.constant(102), dtype=tf.float32)\r\n\r\nc:\\progams\\miniconda\\envs\\tf2-preview-cpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py in range(start, limit, delta, dtype, name)\r\n   1317   with ops.name_scope(name, \"Range\", [start, limit, delta]) as name:\r\n   1318     start = ops.convert_to_tensor(start, dtype=dtype, name=\"start\")\r\n-> 1319     limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\r\n   1320     delta = ops.convert_to_tensor(delta, dtype=dtype, name=\"delta\")\r\n   1321 \r\n\r\nc:\\progams\\miniconda\\envs\\tf2-preview-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)\r\n   1098   preferred_dtype = deprecation.deprecated_argument_lookup(\r\n   1099       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\r\n-> 1100   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n   1101 \r\n   1102 \r\n\r\nc:\\progams\\miniconda\\envs\\tf2-preview-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n   1156       name=name,\r\n   1157       preferred_dtype=dtype_hint,\r\n-> 1158       as_ref=False)\r\n   1159 \r\n   1160 \r\n\r\nc:\\progams\\miniconda\\envs\\tf2-preview-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\r\n   1178       if dtype is not None:\r\n   1179         dtype = dtypes.as_dtype(dtype)\r\n-> 1180         value = _TensorTensorConversionFunction(value, dtype=dtype)\r\n   1181       return value\r\n   1182     else:\r\n\r\nc:\\progams\\miniconda\\envs\\tf2-preview-cpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)\r\n   1034     raise ValueError(\r\n   1035         \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\" %\r\n-> 1036         (dtype.name, t.dtype.name, str(t)))\r\n   1037   return t\r\n   1038 tensor_conversion_registry.register_tensor_conversion_function(\r\n\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'tf.Tensor(102, shape=(), dtype=int32)'\r\n```", "comments": ["Added a PR #29987 for the fix.", "In general this is an error that is easy to fix in user code so I'd rather fix it there. It's ambiguous what to do with a floating point range.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29867\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29867\">No</a>\n", "It works for numpy"]}, {"number": 29866, "title": "Limited tf.compat.v2.summary API due to missing TensorBoard installation.", "body": "I came across this warning after updating to the las nightly release.\r\nAny idea on how to fix it?", "comments": ["I too received the same error\r\nUbuntu 16.4 \r\ninstalled tf 2.0.0 beta1\r\nusing tensorflow models object_detection\r\npython object_detection/builders/model_builder_test.py\r\nError seen\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0621 07:26:07.547648 139914507269888 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\r\nW0621 07:26:07.557060 139914507269888 __init__.py:335] Limited tf.summary API due to missing TensorBoard installation.\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 23, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/home/ai/tensorflow/models/research/object_detection/builders/model_builder.py\", line 20, in <module>\r\n    from object_detection.builders import anchor_generator_builder\r\n  File \"/home/ai/tensorflow/models/research/object_detection/builders/anchor_generator_builder.py\", line 22, in <module>\r\n    from object_detection.protos import anchor_generator_pb2\r\nImportError: cannot import name anchor_generator_pb2\r\n", "Hi @JCMiles Did you solve this issue?\r\nThanks", "Hey,\r\nDid we manage to find a solution to this?\r\n\r\n", "Hi everyone, I fixed this problem by checking my local project files.\r\nI my case I had a file named tensorborad_run.py that was interfering with the correct import of the regular tensorboard files. By changing the file name to run_tensorboard solved the problem.", "See https://github.com/tensorflow/tensorboard/issues/2352 for solution", "My script name was tensorboard.py, i changed the name of file to tb.py solved my problem."]}, {"number": 29865, "title": "Keras fit with mixed dataset/ndarray data results in batch_size arg error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 & macOS 10.14.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc0-55-g1b365ca304 1.14.0-rc1\r\n- Python version: 3.6.8 & 3.7.2\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n**Describe the current behavior**\r\n\r\nCombining tf.data.Dataset with tf.keras.Model fails since 1.14.0rc0 when using a dataset for training but a numpy array for validation data. Logs with errors for both 1.14 and 1.13 below.\r\n\r\n**Describe the expected behavior**\r\n\r\nBeing able to use a dataset only for training but a numpy array for validation data.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```Python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = np.random.randn(1000, 10)\r\ntargets = np.random.randn(1000, 1)\r\nds = tf.data.Dataset.from_tensor_slices(data).batch(32)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(32, input_dim=10))\r\nmodel.add(tf.keras.layers.Dense(32))\r\nmodel.add(tf.keras.layers.Dense(1))\r\n\r\nmodel.compile(optimizer=\"rmsprop\", loss=\"mse\")\r\nmodel.fit(ds, steps_per_epoch=10, validation_data=(data, targets), batch_size=32)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n**Behaviour using 1.14.0rc1:**\r\n\r\nWithout `batch_size=32` set:\r\n\r\n```Bash\r\n% python bug.py\r\n[...]\r\n1/10 [==>...........................] - ETA: 1s - loss: 2.3122\r\nTraceback (most recent call last):\r\n  File \"tensorflow_bug.py\", line 14, in <module>\r\n    model.fit(ds, steps_per_epoch=10, validation_data=(data, targets))\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 409, in model_iteration\r\n    steps_name='validation_steps')\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 335, in model_iteration\r\n    batches = make_batches(num_samples_or_steps, batch_size)\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 493, in make_batches\r\n    num_batches = int(np.ceil(size / float(batch_size)))\r\nTypeError: float() argument must be a string or a number, not 'NoneType'\r\n```\r\n\r\nWith `batch_size=32` set:\r\n\r\n```Bash\r\n% python tensorflow_bug.py\r\n[...]\r\nTraceback (most recent call last):\r\n  File \"tensorflow_bug.py\", line 14, in <module>\r\n    model.fit(ds, steps_per_epoch=10, validation_data=(data, targets), batch_size=32)\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 652, in fit\r\n    batch_size, steps_per_epoch, x)\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1873, in _validate_or_infer_batch_size\r\n    raise ValueError('The `batch_size` argument must not be specified when'\r\nValueError: The `batch_size` argument must not be specified when using dataset as an input.\r\n```\r\n\r\n**Behaviour using 1.13.1:**\r\n\r\nWithout `batch_size=32` set:\r\n\r\n```Bash\r\n% python bug.py\r\n[...]\r\n1/10 [==>...........................] - ETA: 2s - loss: 3.0703\r\nTraceback (most recent call last):\r\n  File \"tensorflow_bug.py\", line 14, in <module>\r\n    model.fit(ds, steps_per_epoch=10, validation_data=(data, targets))\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 364, in model_iteration\r\n    validation_in_fit=True)\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 301, in model_iteration\r\n    batches = make_batches(num_samples_or_steps, batch_size)\r\n  File \"/Users/ahoereth/repos/AutoECG/.venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 488, in make_batches\r\n    num_batches = int(np.ceil(size / float(batch_size)))\r\nTypeError: float() argument must be a string or a number, not 'NoneType'\r\n```\r\n\r\nWith `batch_size=32` set:\r\n\r\n```Bash\r\n% python bug.py\r\n[...]\r\n10/10 [==============================] - 0s 21ms/step - loss: 1.0369 - val_loss: 1.1500\r\n```", "comments": ["I have tried on Colab with TF versions 1.14.0-rc1 and 1.13.1 and was able to reproduce the issue  on 1.14rc1 but not in 1.13.1 as mentioned in the description. Thanks!", "Any news on this issue? I think this is an undocumented breaking change?", "@ahoereth I modified your code little bit by making validation data as a dataset instead of `numpy.ndarray`. In this case, you don't need to provide `batch_size=32` as an input to `model.fit` function. Please take a look at the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/176d60213967b951f724769b0cc34f47/tf_29865.ipynb). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29865\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29865\">No</a>\n"]}, {"number": 29864, "title": "tflite invoke function crash", "body": "I came cross a strange issues only occurred on HuaWei Phone. \r\nFor phone infomation with this image.\r\n![image](https://user-images.githubusercontent.com/17869361/59587579-26572a00-9118-11e9-8b20-67a14a5c8f08.png)\r\n\r\nAt first time run inference no crash, It is always crash at second time. below is crash info.\r\n![image](https://user-images.githubusercontent.com/17869361/59587879-e3498680-9118-11e9-8fe6-bf97e2e2301b.png)\r\n\r\nBelow is processed crash info with ndk-stack, but unable to locate in tensorflow source as build tflite from source with no debug symbols and i do not know how to build with debug symbol\r\n![image](https://user-images.githubusercontent.com/17869361/59588620-9a92cd00-911a-11e9-9fb9-ba68baa06f52.png)\r\n\r\nI have tried many methods for building tflite with debug symbol but not successed\r\nFor example\r\n1\u3001 bazel build -c dbg --strip=never --compilation_mode=dbg --per_file_copt=//tensorflow/lite/.*\\.cc@-g,-O0  //tensorflow/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"\r\nwith -c dbg --strip=never  --compilation_mode=dbg\r\n\r\n![image](https://user-images.githubusercontent.com/17869361/59588889-345a7a00-911b-11e9-9424-c3cfcdd87d99.png)\r\nCrash occurred at 62 line\r\nIt crashed at second time only on HuaWei Phone. Other phones and ios has no crash.\r\n\r\n\r\nps:This issue finally crashed at 277 line with below image\r\n![image](https://user-images.githubusercontent.com/17869361/59751454-217aad80-92b3-11e9-8a98-155da7e58c92.png)\r\n\r\nI guess the bias_data address is unavaible\r\n![image](https://user-images.githubusercontent.com/17869361/59751423-10ca3780-92b3-11e9-971c-99ec5e074624.png)\r\n", "comments": ["Can we close this issue. It looks this is the duplicate of #29739.Please, let us know.Thanks.", "@ravikyram Two are both submitted by me but no answer.", "@weinixuehao It will be better to track with a single issue #29739 for faster resolution .Can we close this issue? Thanks.", "OK \r\nThe other one has included two issues \r\nOne is for ios building with gpu support and one is for this issue", "@weinixuehao I am closing this issue & we can track the resolution for this issue in #29739.Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29864\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29864\">No</a>\n"]}, {"number": 29863, "title": "Error when using unique_with_counts function. ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\nOn Windows Subsystem for Linux. Python 3.6.7. Tensorflow 1.13.1.\r\nCode:\r\n```\r\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=0)\r\nprint(X_train.shape)\r\nprint(X_test.shape)\r\nx = tf.placeholder(float, shape=X_train.shape)\r\ny = tf.placeholder(float, shape=X_test.shape[1:])\r\ncomputeL0Dist = tf.count_nonzero(x - y, axis=[1])\r\nfind_k_closest_tr_products = tf.contrib.framework.argsort(computeL0Dist, direction='ASCENDING')\r\nfind_labels_k_closest_tr_products = tf.gather(y_train, find_k_closest_tr_products[0:paramk])\r\nprint('SHAPE', find_labels_k_closest_tr_products.shape)\r\nfind_u_labels, find_idex, find_counts = tf.unique_with_counts(find_labels_k_closest_tr_products)\r\nfind_predicted_label = tf.gather(find_u_labels, tf.argmax(find_counts))\r\n```\r\nError:\r\n```\r\n(49, 1611)\r\n(49, 1611)\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n2019-06-17 11:51:40.240971: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-17 11:51:40.248082: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\r\n2019-06-17 11:51:40.250639: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x377c490 executing computations on platform Host. Devices:\r\n2019-06-17 11:51:40.252480: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1352, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'UniqueWithCounts' used by {{node UniqueWithCounts}}with these attrs: [T=DT_BOOL, out_idx=DT_INT32]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n\r\n         [[{{node UniqueWithCounts}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"knn.py\", line 101, in <module>\r\n    predicted_label = sess.run([find_predicted_label], feed_dict={x:X_train, y:X_test[i_te_p]})\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'UniqueWithCounts' used by node UniqueWithCounts (defined at knn.py:91) with these attrs: [T=DT_BOOL, out_idx=DT_INT32]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n\r\n         [[node UniqueWithCounts (defined at knn.py:91) ]]\r\n\r\nCaused by op 'UniqueWithCounts', defined at:\r\n  File \"knn.py\", line 91, in <module>\r\n    find_u_labels, find_idex, find_counts = tf.unique_with_counts(find_labels_k_closest_tr_products)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1450, in unique_with_counts\r\n    return gen_array_ops.unique_with_counts(x, out_idx, name)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 10543, in unique_with_counts\r\n    \"UniqueWithCounts\", x=x, out_idx=out_idx, name=name)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/home/psharma/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'UniqueWithCounts' used by node UniqueWithCounts (defined at knn.py:91) with these attrs: [T=DT_BOOL, out_idx=DT_INT32]Registered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n\r\n         [[node UniqueWithCounts (defined at knn.py:91) ]]\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@zendevil Looks like above code snippet is incomplete to reproduce the issue. Please provide the full code snippet to reproduce. Thanks!", "There are some variables that are defined in a file called google_res which I import. You probably can recreate error without that file. Good luck debugging this.\r\n```\r\nfrom sklearn.model_selection import train_test_split\r\nimport time\r\n\r\nimport numpy as np\r\n\r\nimport requests\r\nfrom requests.auth import HTTPBasicAuth\r\n\r\nfrom google_res import *\r\n\r\nfrom imblearn.over_sampling import SMOTE\r\n\r\n\r\n\r\ndef knn(X_train, y_train, X_test, y_test):\r\n    paramk = 11\r\n    x = tf.placeholder(float, shape=X_train.shape)\r\n    y = tf.placeholder(float, shape=X_test.shape[1:])\r\n    computeL0Dist = tf.count_nonzero(x - y, axis=[1])\r\n    find_k_closest_tr_products = tf.contrib.framework.argsort(computeL0Dist, direction='ASCENDING')\r\n    find_labels_k_closest_tr_products = tf.gather(y_train, find_k_closest_tr_products[0:paramk])\r\n    print('SHAPE', find_labels_k_closest_tr_products.shape)\r\n    find_u_labels, find_idex, find_counts = tf.unique_with_counts(find_labels_k_closest_tr_products)\r\n    find_predicted_label = tf.gather(find_u_labels, tf.argmax(find_counts))\r\n    \r\n    # running through the graph\r\n    num_errs = 0\r\n    num_test_products = y_test.shape[0]\r\n    num_train_products = y_train.shape[0]\r\n    \r\n    with tf.Session() as sess:\r\n        for i_te_p in range(0, num_test_products):\r\n            predicted_label = sess.run([find_predicted_label], feed_dict={x:X_train, y:X_test[i_te_p]})\r\n            if predicted_label == y_train[i_te_p]:\r\n                num_errs += 1\r\n                print(num_errs, \"/\", i_te_p)\r\n                print(\"\\t\\t\",predicted_label[0], \"\\t\\t\\t\\t\",t_labels[i_te_p])\r\n\r\n                if (1):\r\n                    plt.figure(1)\r\n                    plt.subplot(1, 2, 1)\r\n                    plt.imshow(y_test[i_te_p])\r\n                    plt.title('test image has label %i' %(predicted_label[0]))\r\n\r\n                    for i in range(num_train_products):\r\n                        if y_train[i] == predicted_label:\r\n                            plt.subplot(1, 2, 2)\r\n                            plt.imshow(x_train[i])\r\n                            plt.title('Correctly labeled as %i' % y_test[i_te_p])\r\n                            plt.draw()\r\n                            break\r\n                    plt.savefig('./results.png')\r\n\r\nstart_time = time.time()\r\nclick_data = np.load('click_data.npy').tolist()\r\n#search_terms_main = list(dict.fromkeys(search_terms_main))\r\n\r\nsearch_terms = [i.split(' ') for i in search_terms_main]\r\n# print(search_terms)\r\nskipgram = []\r\n\r\n# using dynamic programming \r\ndef generate_skipgrams(li, length):\r\n    skipgrams = []\r\n    skipgrams.append([]) # we'll keep the zero length empty\r\n    skipgrams.append([])\r\n    for i in li:\r\n        wrapper = []\r\n        wrapper.append(i)\r\n        skipgrams[1].append(wrapper)\r\n    for i in range(1, length):\r\n        skipgrams.append([])\r\n        for j in range(len(skipgrams[i])):\r\n            for k in range(len(li)):\r\n                if not skipgrams[i][j][-1] == li[k]:\r\n                    wrapper = []\r\n                    wrapper.append(li[k])\r\n                    to_append = skipgrams[i][j] + wrapper\r\n                    \r\n                    skipgrams[i + 1].append('%27' +'%20'.join(to_append) + '%27')\r\n                    #skipgrams[i + 1].append(to_append)\r\n    return (skipgrams[length])\r\n\r\n\r\nskipgrams = []\r\nfor j in range(len(search_terms_main)):\r\n    skipgrams.append(generate_skipgrams(search_terms_main[j].split(' '), 2))\r\n\r\nnum_features = 0 # for computing padding\r\nfor i in range(len(skipgrams)):\r\n    if len(skipgrams[i]) > num_features:\r\n        #print(len(skipgrams[i]))\r\n        num_features = len(skipgrams[i])\r\n\r\n\r\n#print(skipgrams)\r\n#print(num_features)\r\n\r\n\r\ndef pad(skipgrams, max_pad_len):\r\n    padded_skipgrams = []\r\n    for i in range(len(skipgrams)):\r\n        pad_len = max_pad_len - len(skipgrams[i])\r\n        wrapper = []\r\n        for j in range(pad_len):\r\n            wrapper.append('%27%27')\r\n        padded_skipgrams.append(skipgrams[i] + wrapper)\r\n\r\n    return padded_skipgrams\r\n\r\nskipgrams = pad(skipgrams, num_features)\r\n#print(skipgrams)\r\n\r\n# for i in range(len(skipgrams)):\r\n#     print(len(skipgrams[i]))\r\n# Dhiran's code. Expect Errors\r\n\r\nfirst_phrase = [i.replace(' ','%20') for i in search_terms_main]\r\nall_features = [] # stores all the features from feature_clean from various requests\r\n\r\n\r\ndef get_ngrams(l, n):\r\n    ngrams = []\r\n    for i in range(len(l) - n + 1):\r\n        ngrams.append([])\r\n        for j in range(n):\r\n            ngrams[i].append(l[i+j])\r\n    return ngrams\r\n\r\n\r\nmaximum = 0\r\n# query = ''\r\n\r\nfor i in range(len(search_terms_main)):\r\n    if len(search_terms_main[i].split(' ')) > maximum:\r\n        maximum  = len(search_terms_main[i].split(' '))\r\n        # query = search_terms_main[i]\r\nngrams = [] # ngrams for each search term\r\n\r\nfor c, st in enumerate(search_terms_main):\r\n    ngrams.append([]) # this will contain all ngrams for the search term st.\r\n    st_arr = st.split(' ')\r\n    for n in range(1, len(st_arr) + 1):\r\n        ngrams[c].append(get_ngrams(st_arr, n))\r\n    \r\n\r\npadding = []\r\nfor i in range(maximum):\r\n    padding.append(0)\r\n#print(len(padding))\r\n#print(len(ngrams))\r\n\r\n# adding first level padding\r\nfor i in range(len(ngrams)):\r\n    for j in range(len(ngrams[i]), maximum):\r\n            ngrams[i].append([])\r\n\r\nfor i in range(len(ngrams)):\r\n    for j in range(maximum):\r\n        for k in range(maximum - len(ngrams[i][j]) - j):\r\n            padding_unit = []\r\n            # for l in range(j + 1):\r\n            #     padding_unit.append('')\r\n            ngrams[i][j].append(padding_unit)\r\n\r\n#first_phrase = first_phrase[154:]\r\n#print(ngrams)\r\nprint('first phrase', len(first_phrase))\r\n\r\ndef get_prod_list(query):\r\n    product_list = []\r\n    for i in range(len(click_data)):   \r\n        if click_data[i][0] == query:\r\n            product_list.append(int(click_data[i][1]))\r\n    return product_list\r\n\r\n\r\ndef clean_list(li):\r\n    return list(map(lambda x: x.split('=')[1], li))\r\n\r\nfeatures_x = {}\r\nfeatures_y = {}\r\ntotal_conf = [[0,0],[0,0]]\r\n\r\n\r\nfirst_phrase = list(dict.fromkeys(first_phrase))\r\n\r\nfor j, elem in enumerate(first_phrase):\r\n    print('so come here', j)\r\n    #print(elem)\r\n    url = '''http://xx.xxx.xx.xx:xxxx/solr/SomeName/select?defType=edismax&fl=[features%20efi.text_0=%27'''+first_phrase[j] + '%27'\r\n\r\n    feature_count = 1\r\n    # if j == 2:\r\n    #     break\r\n    # for i in range(len(search_terms[j])):\r\n    #     print('search_terms', search_terms[j][i])\r\n    #     url += '%20efi.text_' + str(feature_count) + '=' + search_terms[j][i]\r\n    #     feature_count += 1\r\n\r\n    # add all the ngram terms\r\n    for k in range(len(ngrams[j])): # this goes from 1 gram to 13 grams \r\n        for l in range(maximum - k): # from one grams to 13grams\r\n            to_add = ngrams[j][k][l] \r\n            url_compat = '%27' + '%20'.join(to_add) + '%27'\r\n            url+= '%20efi.text_'+ str(feature_count) + '=' + url_compat\r\n            feature_count += 1\r\n\r\n    # all skipgrams\r\n    for i in range(len(skipgrams[j])):\r\n        url += '%20efi.text_' + str(feature_count) + '=' + skipgrams[j][i]\r\n        feature_count += 1\r\n    \r\n    print('FEATURE COUNT', feature_count)\r\n    # The last part \r\n    url += '''],%20score,%20tm_name,%20ts_title,%20tm_field_category_parent_name,%20ts_field_product_brand_1,%20id&indent=on&q='''+first_phrase[j]+'''&qf=ts_field_summary+ts_field_product_specifications+tm_name+ts_title+tm_field_category_parent_name+ts_field_product_brand_1+ts_field_product_details&wt=json'''\r\n    val = requests.get(url, verify=False, auth=HTTPBasicAuth('user', 'S0MEPass\\/\\/0rd'))\r\n    \r\n    #ids = []\r\n    #category = []\r\n    #title = []\r\n    features = {}\r\n    \r\n    if (val.json() == None):\r\n        print('SKIPPING')\r\n        continue\r\n    print(elem)\r\n    #print(val.json())\r\n    for c, i in enumerate(val.json()['response']['docs']):\r\n        print(i['id'][-7:-3])\r\n        features[int(i['id'][-7:-3])] = i['[features]'].split(',')\r\n   \r\n    features = {k: list(map(lambda x: float(x.split('=')[1]), v)) for k, v in features.items()}    \r\n    #print(features)\r\n    # two extra to store query and id \r\n    features_x[elem] = []\r\n    features_y[elem] = []\r\n    for k, v in features.items():\r\n        features_x[elem].append(v)\r\n        products = get_prod_list(elem.replace('%20', ' '))\r\n        \r\n        if k in products:\r\n            features_y[elem].append(True)\r\n        else:\r\n            features_y[elem].append(False)\r\n    #print('FEATURES X', features_x[elem])\r\n    #print('FEATURES Y', features_y[elem])\r\n    features_x[elem] = np.asarray(features_x[elem], dtype=float)\r\n    if (False in features_y[elem] and list(features_y[elem]).count(True) > 1):\r\n        os = SMOTE(random_state=0, k_neighbors = 1)\r\n        features_x[elem], features_y[elem] = os.fit_sample(features_x[elem], features_y[elem])    \r\n        X_train, X_test, y_train, y_test = train_test_split(features_x[elem], features_y[elem], test_size=0.3, random_state=0)\r\n        knn(X_train, X_test, y_train, y_test)\r\n        total_conf += confusion\r\n\r\n\r\nprint(total_conf)\r\nprint((time.time()-start_time))\r\n\r\n\r\n\r\nprint(conf_all)\r\n\r\n```", "@zendevil I am unable to reproduce the issue with above code snippet. Looks some entities are not defined like search_terms_main. Can you help us to reproduce the issue. Thanks! ", "I think the issue is that bool support is not available for unique_with_counts. Added a PR #29986 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29863\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29863\">No</a>\n"]}, {"number": 29862, "title": "[TF 2.0] Converting keras model to estimator ignores data-types", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nProductName:\tMac OS X\r\nProductVersion:\t10.14.2\r\nBuildVersion:\t18C54\r\n- TensorFlow installed from (source or binary): pip install ...\r\n- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nWhen converting Keras model to estimator it converts all integer inputs to floats or doubles.\r\nThis will result in data-type errors in the converted estimator.  \r\n\r\nI am not entirely sure if this is a bug or \"feature\" because this function is indeed called in the conversion: https://github.com/tensorflow/estimator/blob/c956dd32561bac645a1cd870d3c8cfe8e9fe969b/tensorflow_estimator/python/estimator/keras.py#L62\r\n\r\nHowever, this is blocking using integer-inputs in the converted estimator.\r\n\r\n**Describe the expected behavior**\r\nInput-data-types are preserved when converting keras-model to estimator.\r\n\r\n**Code to reproduce the issue**\r\nSmall example to reproduce the issue:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import layers\r\nfrom tensorflow.python.keras.estimator import model_to_estimator_v2\r\n\r\nx_shape = (3,)\r\nn_class = 5\r\nbatch_size = 10\r\n\r\nx = layers.Input(shape=x_shape, name=\"x\", dtype=tf.int64)\r\n\r\nclass StupidLayer(layers.Layer):\r\n    def build(self, input_shape):\r\n        self.y = tf.random.poisson(\r\n            lam=10, shape=(batch_size,) + x_shape, dtype=tf.int64\r\n        )\r\n        super().build(input_shape)\r\n\r\n    def call(self, inputs, **kwargs):\r\n        return tf.cast(inputs * self.y, tf.float32)\r\n\r\ny = StupidLayer()(x)\r\n\r\nmodel = tf.keras.Model(inputs=x, outputs=y)\r\n\r\nX = np.random.randint(50, size=(batch_size,) + x_shape, dtype=\"int64\")\r\n\r\nmodel.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\")\r\nmodel.predict(X)\r\n\r\ntf_estimator = model_to_estimator_v2(keras_model=model)\r\nnext(tf_estimator.predict(lambda: X))\r\n```\r\n\r\nBecause input is converted to float, last line results in \r\n```\r\nTypeError: Input 'y' of 'Mul' Op has type int64 that does not match type float32 of argument 'x'.\r\n```\r\n", "comments": ["Have tried on Colab with TF 2.0beta and was able to get mentioned error.", "Triage notes: we have a fix for this going in right now. @tanzhenyu will update when it's in the nightlies.", "This works in tf-nightly-2.0-preview since the fix was submitted yesterday.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29862\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29862\">No</a>\n"]}, {"number": 29861, "title": "attention_ocr", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 29860, "title": "[TF 2.0 API Docs] tf.keras.layers.GRU", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU#properties\r\n\r\n### Clear description\r\n\r\nInitialising float variables using 0. rather than 0.0\r\n\r\ndropout=0.,\r\nrecurrent_dropout=0.,\r\n\r\n### Parameters defined\r\n   \r\n time_major Argument not documented.\r\n\r\n### Raises listed and defined\r\nNo\r\n\r\n### Usage example\r\nNo", "comments": ["@math-alpha Can you please elaborate why you want to make that change? I think current initialization is correct.\r\n```python\r\ndropout=0.0,\r\nrecurrent_dropout=0.0\r\n```", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "ok"]}, {"number": 29859, "title": "[TF 2.0 API Docs] tf.errors.DeadlineExceededError", "body": "## System Information\r\nTensorflow version: 2.0\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/errors/DeadlineExceededError\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe description could be better; needs more content for clarification\r\n\r\n### Usage example\r\nNo usage example defined  \r\n\r\n### Submit a pull request?\r\nNo\r\n", "comments": ["@lkmandy,\r\nIn the documentation of [tf.errors.DeadlineExceededError](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/errors/DeadlineExceededError), it is mentioned \r\n\r\n> This exception is not currently used.\r\n\r\nSince we don't use this **`Exception`**, can you please let us know if we can close this issue? Thanks!", "Yes please, you can close this issue if the Exception is deprecated or something like that"]}, {"number": 29858, "title": "[TF 2.0 API Docs] tf.dynamic_stitch", "body": "## System Information\r\nTensorflow version: 2.0\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/dynamic_stitch\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Raises listed and defined\r\nNo errors have been defined\r\n\r\n### Submit a pull request?\r\n", "comments": ["The docs are now updated https://www.tensorflow.org/api_docs/python/tf/dynamic_stitch\r\nThanks!"]}, {"number": 29857, "title": "How tf.image.extract_image_patches works ? ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- TensorFlow installed from (source or binary): Anaconda for 1.13.1 and Pip for TF 2 Beta\r\n- TensorFlow version (use command below): TF 1.13.1 and TF 2 Beta\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CUDA 10 / cuDNN 7.6\r\n- GPU model and memory: GTX 1080 , 11GB\r\n\r\n**Describe the current behavior**\r\nI want to extract a large gray scale image from (1250 x 1250) to patches (512 x 512). So I tried to run tf.image.extract_image_patches on both versions: [TF 1.13.1](https://www.tensorflow.org/api_docs/python/tf/image/extract_image_patches) and [TF 2 Beta](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/extract_patches) follow this [tutorial](https://github.com/lixiangchun/mynotebook/blob/master/machine_learning/Extract%20image%20patches.md).\r\nMy parameters:\r\n```\r\nmy_input_image # shape = ( batch , size_x, size_y)\r\nmy_input_image = tf.expand_dims(my_input_image ,-1)  #add 1 more \"depth\" channel as the last axis \r\nksizes = [1, 512, 512, 1] #size of output patch\r\nstrides = [1, 256, 256, 1] # Stride\r\nrates = [1, 1, 1, 1]  #Rate\r\npadding='SAME' # I want to have zero padding when the stride go out of my_input_image \r\n\r\nimage_patches = tf.image.extract_patches(input_big_pic, ksizes, strides, rates, padding)\r\n\r\n\r\nimage_patches.shape  # => TensorShape([125, 5, 5, 262144]) . Why we have 5 pictures in a row?\r\n\r\npatch1 = image_patches[0,0,0,] # Get the 1st patch\r\npatch1 = tf.reshape(patch1, [512, 512, 1]) # Reshape to the correct shape\r\npatch1 = tf.squeeze(patch1) # Remove the depth channel\r\nplt.imshow(patch1)\r\n```\r\n\r\n tf.image.extract_patches will output a matrix image patches 5x5. \r\nThe zero paddings = 143 on each edge.\r\nI don't understand why we have 5 pictures in a row?  \r\nHow tf.image.extract_patches works ?\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should be 4x4 matrix image patches with zero paddings = 15 on each edge.\r\nDenote **n** is the number of stride steps.\r\nThe number image patchs = **n** + 1\r\n\r\nInput_size = 1250\r\nOutput_size= 512\r\nStride = 256\r\nPadding: \r\n\r\nWe have an equation: \r\n`2*Padding + Input_size = n*Stride + Output`        **(1)**\r\nWe don't know how many zero padding we need. So:\r\n`Input_size <= n*Stride + Output`\r\n`1250 <= n*256 + 512`\r\nThen 2.88 <= **n** . \r\nWe choose the nearest interger **n** = 3.\r\n**(1)** => zero padding = 15\r\n\r\nWill will have 4x4 matrix image patches with zero paddings = 15 on each edge.\r\n\r\nPlease correct me if my calculation is wrong.\r\n", "comments": ["Please the this excellent explanation on StackOverflow: https://stackoverflow.com/questions/40731433/understanding-tf-extract-image-patches-for-extracting-patches-from-an-image\r\n\r\nI don't think there's a bug here, so I will close this issue.", "(That said, I am adding a proper docstring to `tf.image.extract_patches`)", "@martinwicke : Hi, I've read it. \r\nBut I would like to know why there is a difference between a manual calculation result and `tf.image.extract_patches` result ?", "1250/256 = 4.8.., because you use 'same' padding, you round up. \r\n\r\nThe sizes (512) is not relevant to determine how many patches there are, only how big they are (they can overlap).", "Example:\r\n`Input_picture = np.zeros((1,1250,1250,1))`\r\nFrom **tf.extract_image_patches**\r\n```\r\ndef extract_patches(x):\r\n    return tf.extract_image_patches(\r\n        x,\r\n        (1, 512, 512, 1),\r\n        (1, 256, 256, 1),\r\n        (1, 1, 1, 1),\r\n        padding=\"SAME\"\r\n        )\r\nOutput_extracted = extract_patches(Input_picture)\r\n```\r\n\r\nOutput_extracted will have shape:\r\n`TensorShape([Dimension(1), Dimension(5), Dimension(5), Dimension(262144)])`\r\n( 1, 5, 5, 262144 )\r\nIt means there are 5 x 5 = 25 patches.\r\n\r\nBut we calculate: \r\n\r\n\r\n> 1250/256 = 4.8.., because you use 'same' padding, you round up.\r\n> \r\n> The sizes (512) is not relevant to determine how many patches there are, only how big they are (they can overlap).\r\n\r\n4.8 rounds up = 5\r\n256*5 = 1280\r\nIt means we can add zero pad = 1280 - 1250 = 30\r\n=> Padding big image size = 1280 x 1280\r\nThen we extract 4 x 4 = 16 patches by \r\n1st patch:         0----------->512                                                      \r\n2nd patch:                256------------>768\r\n3rd patch:                              512-------------->1024\r\n4th patch:                                           768---------------->1280\r\n\r\nSo the correct answer should be ( 1, 4, 4, 262144 )"]}, {"number": 29856, "title": "tf.keras.layers.UpSampling2D(interpolation='bilinear') has a smearing defect on the right & bottom edges", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I've provided a link to a Colab notebook demonstrating the issue below, comparing keras upsampling to what it should look like with a correct implementation as seen in tf.image.resize.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7\r\n- Python version: 3\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUpsampling using tf.keras.layers.UpSampling2D() results in unnatural smearing of the right and bottom edges of the image.  This problem is amplified when the upsampling is repeated.\r\n\r\n**Describe the expected behavior**\r\nKeras layers should use sensible default behaviour and not have this smearing issue.  This causes serious problems for autoencoders, GANs, and cost months of time.  Correct behaviour is seen with tf.image.resize(o, size=size, method=tf.image.ResizeMethod.BILINEAR).  Keras upsampling should use this as the default instead of the current defective behaviour.  Note: In TensorFlow 1.x, the tf.image.resize method had an 'align_corners' parameter that toggled between defective and proper behaviour and was set to False (defective behaviour) by default.  In TensorFlow 2, this parameter has been removed and the correct behaviour (align_corners=True behaviour) is now the default.  The keras layer should follow the same path.\r\n\r\n**Code to reproduce the issue**\r\nHere is a Colab notebook that demonstrates the issue:\r\nhttps://colab.research.google.com/drive/1rgCzJcMo4DN_9_hutr9l2vSrTRPfcd6K\r\n\r\n**Other info / logs**\r\n", "comments": ["Actually, I think it's even worse than that, it seems bilinear doesn't work\r\nhttps://colab.research.google.com/drive/1BG1gRC86Hj9CqyLTD9quyW0vtNJWZK3j", "Have tried with code snippet provided and was able to reproduce the issue on Colab with TensorFlow version 2.0beta.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29856\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29856\">No</a>\n"]}, {"number": 29855, "title": "[TF 2.0 API Docs] https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/MaxPool1D", "body": "1.\tThe parameters pool_function and input_spec in https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/pooling.py#L59 where not defined the documentation https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/MaxPool1D and no return where defined.", "comments": ["1. The parameters of pool_function at line 65, input_spec at line 70 and base_config at 597 in https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/pooling.py where not defined in https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/MaxPool3D and https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/MaxPool2D.\r\n\r\n2. Return types where not defined in both https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/MaxPool3D and https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/MaxPool2D.", "@allenbangai Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)"]}, {"number": 29853, "title": "[TF 2.0 API Docs] tf.div_no_nan", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/div_no_nan\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe web page corresponding to the link does not exist. Error 404\r\n\r\n### Correct links\r\n\r\nThe link is not correct. The page doesn't exist\r\n", "comments": ["Can you please help us to know from which link this [page](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/div_no_nan) is been directed. Thanks!", "The link is correct but the page does not exists\r\n![Untitled](https://user-images.githubusercontent.com/23650880/59790851-c489f200-92c8-11e9-899b-f33e776bcf29.png)\r\n", "Not able to find out the original page which links to this 404 error page. Is it been directed from [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/). Thanks!", "This  is the [link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/div_no_nan) from the documentation. And as I said the link is as it should be but when you go there you see the 404 error page. Or am I not understanding the question?", "Sorry for not able to convey the message. It looks like you are referring to wrong page. Please have a look to this [link](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/divide_no_nan) which is under tf.math with the name divide_no_nan and let us know if this is the one you are searching for. Thanks!", "Yeah, I looks like this is the correct link. The fact is, in the docs task the name of the API symbol is [tf.div_no_nan](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/div_no_nan) whereas [tf.math.divide_no_nan](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/divide_no_nan) is not even present there. I think this might be the symbol that was meant to be there. Also, sorry for the late reply \ud83d\ude25.", "Good to know that was the correct link. Can we close on this issue since it is resolved. Let us know. Thanks!", "Yeah, and now someone needs to add the correct symbol to the docs task.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29853)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29853)\r\n"]}, {"number": 29852, "title": "[TF 2.0 API Docs] Documentation describes non existing symbol", "body": "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/debugging/check_numerics\r\n\r\nThis link gives a description of a symbol in a module which is supposed to be at https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/python/ops\r\n\r\nBut the module doesn't seem to exist", "comments": ["@xurror The symbol you are looking will be generated file as it was clearly mentioned in the description as `Defined in generated file: python/ops/gen_array_ops.py`. There are files that generates this file. Thanks! Please let me know if you are looking for any other details. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 29851, "title": "[TF 2.0 API Docs] tf.data.experimental.unbatch", "body": "## System information\r\nTensorflow version: 2.0\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/unbatch\r\n\r\n## Description of issue (what needs changing):\r\nThe description needs more content, as no description has been provided for the available functions.\r\nNo recommendations have been given on when and when not to use this symbol\r\n\r\n### Parameters defined\r\nNo parameters have been defined\r\n\r\n### Raises listed and defined\r\nNo errors defined\r\n\r\n### Request visuals, if applicable\r\nNo visuals, the content will be clarified if visuals are provided\r\n\r\n### Others\r\nThis symbol is deprecated, am not sure if this review will  still be useful\r\n\r\n### Submit a pull request?\r\nNo\r\n", "comments": ["`tf.data.experimental.unbatch` is deprecated and recommend to use `tf.data.Dataset.unbatch()` instead.\r\nThe docs are updated with [`tf.data.Dataset.unbatch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#unbatch)"]}, {"number": 29850, "title": "[TF 2.0 API Docs] tf.data.experimental.rejection_resample", "body": "## System  Information\r\nTensorflow version: 2.0\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/rejection_resample\r\n\r\n## Description of issue (what needs changing):\r\nNo recommendations of when and when not to use this symbol have been provided.\r\nThe description is not clear, it needs more content\r\n\r\n### Raises listed and defined\r\nNo raises listed\r\n\r\n### Usage example\r\nNo usage example has been provided\r\n\r\n### Request visuals, if applicable\r\nNo visuals, but they will be very useful if present\r\n\r\n### Submit a pull request? \r\nNo", "comments": ["This function has been deprecated in latest version , closing this issue. Thank you "]}, {"number": 29849, "title": "[TF 2.0 API Docs] tf.debugging.assert_type", "body": "## URL with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/debugging/assert_type\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe API symbol doesn't contain a complete, self-contained, coherent, appropriately formatted, and well-documented usage example code sample and the return values aren't defined\r\n\r\n### Usage example\r\n\r\nNo usage example\r\n\r\n### Returns defined\r\n\r\nThe return values aren't defined\r\n", "comments": ["Closes this issue since commit [52a7bfe](https://github.com/tensorflow/tensorflow/commit/52a7bfee453e449e715e7f0847e74f03ff6b9036) updates the docs for usage examples. Thanks!"]}, {"number": 29848, "title": "[TF 2.0 API Docs] tf.data.experimental.prefetch_to_device", "body": "## URL with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/prefetch_to_device\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe API symbol doesn't contain a complete, self-contained, coherent, appropriately formatted, and well-documented code sample.\r\n\r\n### Usage example\r\n\r\nNo usage example", "comments": []}, {"number": 29847, "title": "[TF 2.0 API Docs] tf.errors.ResourceExhaustedError", "body": "## URL with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/errors/ResourceExhaustedError\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe API symbol doesn't contain a complete, self-contained, coherent, appropriately formatted, and well-documented code sample.\r\n\r\n### Usage example\r\n\r\nNo usage example", "comments": ["@Wil2129 We are checking to see if you still need help on this issue. Could you please have a look on the [link1](https://github.com/tensorflow/tensorflow/blob/v2.7.0/tensorflow/python/framework/errors_impl.py#L367-L379) , [link2](https://www.tensorflow.org/versions/r2.7/api_docs/python/tf/errors/ResourceExhaustedError) and let us know if it helps?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29846, "title": "[TF 2.0 API Docs] tf.errors.ResourceExhaustedError", "body": "## URL with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/errors/ResourceExhaustedError\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe API symbol doesn't contain a complete, self-contained, coherent, appropriately formatted, and well-documented code sample.\r\n\r\n### Usage example\r\n\r\nNo usage example?", "comments": []}, {"number": 29845, "title": "Build issue  Bazel", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: MacOS Mojave 10.14.5\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version:2.0.0-beta1 \r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:No\r\n- Bazel version (if compiling from source):0.26.1\r\n- GCC/Compiler version (if compiling from source):4.2.1\r\n- CUDA/cuDNN version:No\r\n- GPU model and memory: 4G ram\r\n\r\n\r\n\r\n**Describe the problem**\r\nI face these errors so what's solution ? \r\n```\r\nKotozs-MacBook-Air-6:tensorflow kotoz$ ./configure \r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: /usr/local/opt/python@3/bin/python3.6\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nDo you wish to build TensorFlow with iOS support? [y/N]: N\r\nNo iOS support will be enabled for TensorFlow.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apache Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\nKotozs-MacBook-Air-6:tensorflow kotoz$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=89\r\nINFO: Reading rc options for 'build' from /Users/kotoz/Downloads/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /Users/kotoz/Downloads/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python@3/bin/python3.6 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages --python_path=/usr/local/opt/python@3/bin/python3.6 --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:opt in file /Users/kotoz/Downloads/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at /Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl:346:20):\r\n - /Users/kotoz/Downloads/tensorflow/tensorflow/workspace.bzl:69:5\r\n - /Users/kotoz/Downloads/tensorflow/WORKSPACE:94:1\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 344\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 56, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'numpy'\r\nIs numpy installed?\r\nINFO: Call stack for the definition of repository 'jpeg' which is a third_party_http_archive (rule definition at /Users/kotoz/Downloads/tensorflow/third_party/repo.bzl:206:28):\r\n - /Users/kotoz/Downloads/tensorflow/third_party/jpeg/workspace.bzl:6:5\r\n - /Users/kotoz/Downloads/tensorflow/tensorflow/workspace.bzl:45:5\r\n - /Users/kotoz/Downloads/tensorflow/tensorflow/workspace.bzl:73:5\r\n - /Users/kotoz/Downloads/tensorflow/WORKSPACE:94:1\r\nINFO: Call stack for the definition of repository 'pcre' which is a tf_http_archive (rule definition at /Users/kotoz/Downloads/tensorflow/third_party/repo.bzl:126:19):\r\n - /Users/kotoz/Downloads/tensorflow/tensorflow/workspace.bzl:449:5\r\n - /Users/kotoz/Downloads/tensorflow/WORKSPACE:94:1\r\nERROR: /Users/kotoz/Downloads/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 344\r\n\t\t_create_local_python_repository(repository_ctx)\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n\t\t_get_numpy_include(repository_ctx, python_bin)\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n\t\t_execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 56, in _execute\r\n\t\t_fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n\tFile \"/Users/kotoz/Downloads/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n\t\tfail((\"%sPython Configuration Error:%...)))\r\nPython Configuration Error: Problem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'numpy'\r\nIs numpy installed?\r\n and referenced by '//third_party/py/numpy:headers'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 14.536s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (143 packages loaded, 3114 targets configur\\\r\ned)\r\n    Fetching @local_config_xcode; fetching 7s\r\nKotozs-MacBook-Air-6:tensorflow kotoz$ \r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["Is numpy installed?", "Yes installed ", "@Mohamedtareque Could you downgrade Bazel to 0.26.0 as that is the max version allowed. Please check the source [here](https://github.com/tensorflow/tensorflow/blob/8e423e3d56390671f0d954c90f4fd163ab02a9c1/configure.py#L1394). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "I'm using bazel 0.26.0 with numpy installed but the problem still persists (On Ubuntu 18.04)", "Just to check that all is good, please post the output of the following command:\r\n\r\n```console\r\nyes \"\" | ./configure; python -c \"import numpy as np\"\r\n```", "```\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: No MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apache Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n", "Thanks. That confirms that `numpy` is visible from the path you're building tensorflow. Next step in debugging would be to try\r\n\r\n```console\r\npython -c \"import numpy as np; print(np.__path__); print(np.get_include())\"\r\n```\r\n\r\nPlease let us know what that prints", "$ yes \"\" | ./configure; python -c \"import numpy as np\"\r\nbash: ./configure: No such file or directory\r\nI am getting this error. \r\n\r\nI also tried python3, getting the same error\r\n", "The issue is solved. ", "> The issue is solved.\r\n\r\nhi @ayushmankumar7 \r\n\r\nI'm trying to genrate .pb file from terminal in ubuntu.\r\n\r\nAnd i'm geting the same error i.e : ImportError: No module named 'numpy'\r\n\r\nLet me know what can i do to resolve this?\r\n\r\n", "@navi63  Sure....\r\n\r\nSee first tensorflow has considered for both Python3 and Python2 .. so make sure that you have nump installed in both the versions . This error will be solved", "@ayushmankumar7 thanks it works\r\nmay i know if you have any info regarding converting .pb file to tflite file?\r\n", "@navi63 this issue has no relationship with your question. Please open new issues if they are issues with TensorFlow library code or please ask other questions on StackOverflow.\r\n\r\nLocking conversation here as it has been solved already"]}]