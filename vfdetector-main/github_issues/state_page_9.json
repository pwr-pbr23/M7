[{"number": 54455, "title": "Add default settings for cloud tpu client to support AI Platform", "body": "- Changed to refer to the value of `TPU_CONFIG` when no argument is passed to TPU Client.\r\n- Add unit tests in the above case.\r\n\r\n---\r\nWhen using `TPU` with `AI Platform`, it is necessary to get the TPU information from the environment variable `TPU_CONFIG`.\r\n\r\nRef: https://cloud.google.com/ai-platform/training/docs/using-tpus#wait-for-tpu-provisioning\r\n\r\nThese settings aren't interesting and should work with the default settings, just like any other environment like `kubernetes` or `colabolatory`.\r\n\r\nNOTE:\r\nI understand that Google recommends `Vertex AI` and `CloudTPU VM` over `AI Platform`, but it seems that the use of `AI Platform` will continue for a while due to insufficient support for scheduled jobs that specify `TPU`.", "comments": ["If you have any additional information you need, please contact me! \ud83d\udc4d \r\n@michaelbanfield \r\n", "@michaelbanfield CC: @gbaned Any update?", "@allenwang28 @michaelbanfield ping\ud83d\ude3f", "@allenwang28, @michaelbanfield Can you please review this PR ? Thanks!", "When I merged the latest master again and pushed it, it is error with lint.\r\nThe parts pointed out in this lint are different from my changes, should I fix them?", "What is said in this PR seems to be relevant.\r\nhttps://github.com/tensorflow/tensorflow/pull/55453\r\nhttps://github.com/tensorflow/tensorflow/issues/55442", "@pshiko Can you please fix build failures ? Thanks!", "@gbaned \r\nCould you do `force run`?\r\nPerhaps [this PR](https://github.com/tensorflow/tensorflow/pull/55464) solved the pylint error.\r\n", "> @gbaned Could you do `force run`? Perhaps [this PR](https://github.com/tensorflow/tensorflow/pull/55464) solved the pylint error.\r\n\r\n@pshiko Sure, I have done force run. Thank you."]}, {"number": 54453, "title": "[oneDNN] Enable BF16 for DepthwiseConv2D* ops", "body": "Add BFloat16 kernel registration for ops :\r\n- DepthwiseConv2dNative, \r\n- DepthwiseConv2dNativeBackpropInput,\r\n- DepthwiseConv2dNativeBackpropFilter\r\n\r\nSo as to enable some keras models like keypoint_detection for bfloat16 which were crashing with No OpKernel registered for    DepthwiseConv2dNativeBackpropInput. Other 2 kernels are registered anticipating it will be required.", "comments": ["@rohan100jain Can you please review this PR ? Thank you!", "Please provide a description and justification for the added kernels.", "> Please provide a description and justification for the added kernels.\r\n\r\n@cantonios : added details in the description. Mainly to prevent crash in some models for bf16. Also fixed a mistake."]}, {"number": 54450, "title": "Improved MPI support for TensorFlow", "body": "\r\n**System information**\r\n- TensorFlow version (you are using):  I am not a current TensorFlow user\r\n- Are you willing to contribute it (Yes/No): I am willing to help develop it\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI lead the PMIx initiative and have been the runtime lead on OpenMPI since its inception. We have been receiving a steady stream of problem complaints about MPI use from within TensorFlow, mostly due to problems with MPI wireup. We have developed methods for providing a more robust mechanism for MPI support of programming models that do not follow the traditional MPI \"bulk synchronous\" architecture. However, it has become apparent that TensorFlow is not using those methods.\r\n\r\nI would like to initiate a collaboration to resolve these problems by helping TensorFlow developers utilize the dynamic programming model support for MPI. I have given many presentations on this subject, some of which are available as online videos, and am willing to give an in-person version to help kick things off. I can also help a bit with coding and am always available for advice and guidance. I acknowledge that we sometimes have to \"tweak\" our support to better fit a particular programming model, and I am willing to make such adjustments as they are uncovered.\r\n\r\n**Will this change the current api? How?**\r\nI don't believe there will be a need for such changes, but I defer that to the TensorFlow community once we identify what needs to be done.\r\n\r\n**Who will benefit with this feature?**\r\nAll TensorFlow users should benefit as it will make MPI operations more robust\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 54449, "title": "Unrecognized term used by the documentation of the class \"tensorflow.compat.v1.variable_scope\".", "body": "The paragraph following:\r\n\r\n                      \"A note about using variable scopes in multi-threaded environment: Variable\r\n                        scopes are thread local, so one thread will not see another thread's current\r\n                        scope. Also, when using `default_name`, unique scopes names are also generated\r\n                        only on a per thread basis. If the same name was used within a different\r\n                        thread, that doesn't prevent a new thread from creating the same scope.\r\n                        However, the underlying variable store is shared across threads (within the\r\n                        same graph). As such, if another thread tries to create a new variable with\r\n                        the same name as a variable created by a previous thread, it will fail unless\r\n                        reuse is True.\"\r\n\r\nwhich is found in both the documentation (https://www.tensorflow.org/api_docs/python/tf/compat/v1/variable_scope) and the source code (https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/variable_scope.py#L2144-L2579).\r\n\r\nMy question(s):\r\n\r\nFor the last sentence of this paragraph, what is the point it talks about? \r\nIt seems to talk about a condition when two threads cause an error since one tries to have a variable of the same name as the other, previous one, although I am not sure what is the mentioned 'underlying variable store' referring to.\r\nBut after some tests, I haven't found the condition yet, while it is always the case that each thread has a variable scope separated from others.\r\nDoes anyone have learned about the documented condition or the term 'underlying variable store'?\r\n\r\n\r\nAdditional Information:\r\n\r\nHere are codes used for two tests, all run without an error found. (With Python 3.7.12, Tensorflow 1.15.0 as the environment)\r\n\r\n1. Test One\r\n\r\n```\r\n      import tensorflow as tf\r\n      \r\n      def F():\r\n        T = tf.get_variable( 'Mark' , [1,2] )\r\n        print( T.name )\r\n      \r\n      import threading\r\n      t1 = threading.Thread( target=F , args=() )\r\n      t2 = threading.Thread( target=F , args=() )\r\n      \r\n      t1.start()\r\n      t2.start()\r\n      t1.join()\r\n      t2.join()\r\n```\r\n\r\n2. Test Two\r\n```\r\n      import tensorflow as tf\r\n      \r\n      def F(s):\r\n        with tf.variable_scope(s,reuse=tf.AUTO_REUSE) as scope:\r\n          T = tf.get_variable( 'Mark' , [1,2] )\r\n          print( T.name )\r\n      \r\n      import threading\r\n      with tf.variable_scope('s',reuse=tf.AUTO_REUSE):\r\n        s = tf.get_variable_scope()\r\n        T = tf.get_variable( 'Mark' , [1,2] )\r\n        t1 = threading.Thread( target=F , args=(s,) )\r\n        t2 = threading.Thread( target=F , args=(s,) )\r\n      \r\n      t1.start()\r\n      t2.start()\r\n      t1.join()\r\n      t2.join()\r\n```", "comments": ["Hi @chunduriv ! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/e6f1b3a32474af619a01b566b44fc6cf/github_54449.ipynb#scrollTo=xpjbF1-rnslQ), [2.8](https://colab.sandbox.google.com/gist/mohantym/18ea6783dcfc9ea6eacfd692c0ffdaf8/github_54449.ipynb) and [nightly](https://colab.sandbox.google.com/gist/mohantym/ab736270a9a85dde8f9a36297ab3d45a/github_54449.ipynb#scrollTo=-bXGE65_e5EB).", "Variable store is nothing but the thread local store for the current variable scope and scope counts.\r\nvariable store will be maintained on a thread level so that it will not create any problem during multithread environments.\r\n\r\nIn your code when you change the `reuse=None`( which is equivalent to reuse = False) instead of `reuse=tf.AUTO_REUSE`, you will get the expected error outcome, if you want your variable store's scope to be shared to other threads  then you can use `reuse = True`.\r\n\r\nBelow is the code with reuse=None which does not allow variable scope to be shared to other thread.\r\n\r\n```\r\n%tensorflow_version 1.x\r\nimport tensorflow as tf\r\n\r\ndef F(s):\r\n  with tf.variable_scope(s,reuse=None) as scope:\r\n    T = tf.get_variable( 'Mark' , [1,2] )\r\n    print( T.name )\r\n\r\nimport threading\r\nwith tf.variable_scope('s',reuse=None):\r\n  s = tf.get_variable_scope()\r\n  T = tf.get_variable( 'Mark' , [1,2] )\r\n  t1 = threading.Thread( target=F , args=(s,) )\r\n  t2 = threading.Thread( target=F , args=(s,) )\r\n\r\nt1.start()\r\nt2.start()\r\nt1.join()\r\nt2.join()\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sachinprasadhs \r\n\r\nThank you very much for your answer!\r\n\r\nFor your answer, I have two more questions as following.\r\n\r\n(1) Is the variable store mentioned in your answer the variable '_var_store' (should be initialized [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L1994)) or the variable '_var_scope_store' (should be initialized [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L1995)) inside the object of the class _pure_variable_scope (should be defined [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L1942)) which could be found after the scope (should be an object of the class variable_scope, which should be defined [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L2144)) containing, or ready to initialize it, is entered?\r\n\r\n(2) Is the mechanism of deciding whether or not to reuse a variable in a variable scope ([variable_scope](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L2144)), **only** based on the values of the reuse fields, composed of the corresponding argument of the [function](https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/ops/variable_scope.py#L2329-L2342) whereby it should be defined [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L2338), as well as the value of the corresponding member variable (should be initialized [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L2396)) priorly set to the scope, as well as the existence and the content of such store (for example, for _var_scope_store, as it should be an object of the class [_VariableScopeStore](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L1431), check its member variable(s), either 'variable_scopes_count' (should be defined [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L1437)) or 'current_scope' (should be defined [here](https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops/variable_scope.py#L1436)), or both)?\r\n"]}, {"number": 54448, "title": "Problems compiling TensorFlow on M1 Mac within Rosetta", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS Monterey 12.1, MacBook Pro M1 2021**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **2.7.1 (couldn't use the command because the import fails)**\r\n- Python version: **3.8.5 (Miniconda)**\r\n- Bazel version (if compiling from source):\r\n**Build label: 5.0.0\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Jan 19 14:15:55 2022 (1642601755)\r\nBuild timestamp: 1642601755\r\nBuild timestamp as int: 1642601755**\r\n- GCC/Compiler version (if compiling from source):\r\n**Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1\r\nApple clang version 13.0.0 (clang-1300.0.29.30)\r\nTarget: x86_64-apple-darwin21.2.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin**\r\n- CUDA/cuDNN version: **n/a**\r\n- GPU model and memory: **n/a**\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI've been trying to compile TensorFlow from source using a Rosetta terminal.\r\nI use Miniconda.\r\nHere is what I end up with:\r\n```\r\n(sandbox) \u279c  wheels python -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '_PyCMethod_New'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 79, in <module>\r\n    raise ImportError(\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/Users/come/miniconda3/envs/sandbox/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 0x0006): symbol not found in flat namespace '_PyCMethod_New'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\r\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe TensorFlow library to be imported without issue.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): **no**\r\n- Briefly describe your candidate solution(if contributing): **n/a**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n(base) \u279c  wheels arch\r\ni386\r\n(base) \u279c  wheels git clone https://github.com/tensorflow/tensorflow.git\r\n(base) \u279c  wheels cd tensorflow\r\n(base) \u279c  tensorflow git:(master) git checkout v2.7.1\r\n(base) \u279c  tensorflow git:(2a0f59ecfe6) ./configure\r\n(base) \u279c  tensorflow git:(2a0f59ecfe6) bazel build //tensorflow/tools/pip_package:build_pip_package\r\n(base) \u279c  tensorflow git:(2a0f59ecfe6) ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n(base) \u279c  tensorflow git:(2a0f59ecfe6) mv /tmp/tensorflow_pkg/tensorflow-2.7.1-cp39-cp39-macosx_12_0_x86_64.whl \r\n tmp/tensorflow_pkg/tensorflow-2.7.1-py3-none-any.whl\r\n(base) \u279c  tensorflow git:(2a0f59ecfe6) conda create -y --name sandbox python=3.8.5\r\n(base) \u279c  tensorflow git:(2a0f59ecfe6) conda activate sandbox\r\n(sandbox) \u279c  tensorflow git:(2a0f59ecfe6) pip install /tmp/tensorflow_pkg/tensorflow-2.7.1-py3-none-any.whl\r\n(sandbox) \u279c  tensorflow git:(2a0f59ecfe6) python -c \"import tensorflow as tf\"\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nAny help would be really appreciated. Thanks!", "comments": ["@Dalkio,\r\n\r\nCan you please refer to similar issues #44751,#47782, #46044 for installation and let us know if it help? Thanks!\r\n", "@chunduriv it looks like @Dalkio isn't trying to install TF, they are trying to compile it from source.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54448\">No</a>\n", "> @Dalkio,\r\n>\r\n> Can you please refer to similar issues https://github.com/tensorflow/tensorflow/issues/44751,https://github.com/tensorflow/tensorflow/issues/47782, https://github.com/tensorflow/tensorflow/issues/46044 for installation and let us know if it help? Thanks!\r\n\r\nThanks for the reply. I'm indeed trying to compile it from source, not import it from a release.\r\nCan we reopen the issue please?", "M1 related build issues are officially not supported by Tensorflow, since we currently don't have Tensorflow build for M1 officially from us. Please refer to [this](https://developer.apple.com/metal/tensorflow-plugin/) document for building Tensorflow.\r\nIf you have any additional queries, you can post your issues on https://developer.apple.com/forums/. Thanks!", "The document you linked does not talk about building TensorFlow at all. Apple's M1 TensorFlow release is closed-source, as you surely must know.\r\n\r\nYour build page https://www.tensorflow.org/install/source#macos does not say anything about not supporting M1. This is all it says about the macOS build requirements:\r\n\r\n<img width=\"863\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5308024/164063019-e9349ec9-94a9-4ce9-a694-29cd4187c4df.png\">\r\n\r\nMaybe if you don't support Apple M1 machines (which is a large and growing segment of the developer market) you should say that explicitly on the TF build-from-source website.", "Please refer the Note section in the document [here](https://www.tensorflow.org/install/pip#1.-install-the-python-development-environment-on-your-system) and let us know if this helps. Thanks!", "That document is about installing TensorFlow, not building it, and as we have already discussed in this GitHub thread, the OP is asking about building TF, not installing it."]}, {"number": 54442, "title": "Reproducible init of trainable variables (TVs) even when number of TVs changes", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS\r\n- TensorFlow installed from (source or binary): PIP\r\n- TensorFlow version (use command below):2.8\r\n- Python version:3.8.12\r\n- GPU model and memory:8\u00d7v100\r\n\r\nFor example:\r\n\r\nModel has 3 vars, varA, varB and varC\r\n\r\nNow, we change the model with a new varA2, insert between varA and varB. So the model has 4 vars, varA, varA2, varB and varC.\r\n\r\nEven varA2 is not used except weight initialization, e.g. tf.keras.initializers.GlorotUniform(). The model will output different result than before, because now the varA2 may use the weights of pervious varB and so on.\r\n\r\nIt is possible to binding the weight random initialization with its name?\r\n\r\n@duncanriach \r\n\r\n\r\nhttps://colab.research.google.com/drive/19Ui9Fp1JF06Xl440pnHZ8zis-yyTx9jq?usp=sharing", "comments": ["@mohantym  I think it is a bug", "Ok @edwardyehuang ! Did you check this[ thread ](https://github.com/tensorflow/tensorflow/issues/36650#issuecomment-863186242)on using tf.name_scope on the targeted weights though? \r\n Thanks!\r\n", "> \r\n\r\nI just check that issue, this one is different.\r\n\r\nAlso, this is a TensorFlow bug as well, because weights can also be added by tf.Variable", "Ok!  Changing the labels then. But Can you please update the template with standalone code to reproduce this issue? Thanks!", "It does support TF 2.8 as default version now. Thank you!", "https://colab.research.google.com/drive/19Ui9Fp1JF06Xl440pnHZ8zis-yyTx9jq?usp=sharing", "Hi @chunduriv ! Could you please look at this issue? It is replicating in [2.7](https://colab.sandbox.google.com/gist/mohantym/472eff58ce82b9d82c3326c32572564e/issue-54442.ipynb#scrollTo=I62wQEG85Tn9), [2.8](https://colab.sandbox.google.com/gist/mohantym/1c651e44535543a3dfb23f6f5803f8a2/issue-54442.ipynb#scrollTo=vECbf1wT5pfO) and [nightly ](https://colab.sandbox.google.com/gist/mohantym/35362a62b836c1b058515d414cf8f809/issue-54442.ipynb#scrollTo=AgOT6bdS-OIQ)version. Thanks!", "This behavior:\r\n* is not a bug,\r\n* is expected in all versions of TensorFlow,\r\n* is not related to GPU functionality, and\r\n* is not related to `tf.config.experimental.enable_op_determinism`.\r\n\r\nFirst of all, there is no need to restart the runtime to reset the global pseudo-random number generator. You can simply call `tf.random.set_seed`.\r\n\r\n```python\r\nfor i in range(2):\r\n  tf.random.set_seed(0)\r\n  a = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n  c = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n  print(\"run: %d; a: %f; c: %f\" % (i, a, c))\r\n\r\nrun: 0; a: -0.020802; c: 0.005541\r\nrun: 1; a: -0.020802; c: 0.005541\r\n```\r\n\r\nNext, the following demonstrates the behavior you are noticing. The global pseudo-random number generator will be in a different state after initializing `b`, and therefore `c` will be initialized differently than it was before. This is normal and expected behavior.\r\n\r\n```python\r\ntf.random.set_seed(0)\r\na = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\nb = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\nc = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\nprint(\"run: %d; a: %f; b: %f; c: %f\" % (2, a, b, c))\r\n\r\nrun: 2; a: -0.020802; b: 0.005541; c: -0.030479\r\n```\r\n\r\nFinally, if you would like each variable to be initialized using pseudo-random number generator state that is independent of the most recent state of the global pseudo-random number generator, then you can provide a `seed` parameter to the initializers used for the variables initialized after the change in the model (or to all initializers).\r\n\r\n```python\r\nfor i in range(2):\r\n  tf.random.set_seed(0)\r\n  a = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n  if (i == 1):\r\n    b = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n  c = tf.Variable(tf.random_uniform_initializer(seed=42)(shape=(1,)))\r\n  print(\"run: %d; a: %f; c: %f\" % (i, a, c))\r\n  \r\nrun: 0; a: -0.020802; c: -0.021757\r\nrun: 1; a: -0.020802; c: -0.021757\r\n```\r\n\r\nThe above code can be run in [this colab notebook](https://colab.research.google.com/drive/1LX3FmX0Vdka14SnXuRpvpxioCrJxseLF?usp=sharing).", "I also suggest the following for a more specific title:\r\n\r\nReproducible init of trainable variables (TVs) even when number of TVs changes", "> This behavior:\r\n> \r\n> * is not a bug,\r\n> * is expected in all versions of TensorFlow,\r\n> * is not related to GPU functionality, and\r\n> * is not related to `tf.config.experimental.enable_op_determinism`.\r\n> \r\n> First of all, there is no need to restart the runtime to reset the global pseudo-random number generator. You can simply call `tf.random.set_seed`.\r\n> \r\n> ```\r\n> for i in range(2):\r\n>   tf.random.set_seed(0)\r\n>   a = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n>   c = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n>   print(\"run: %d; a: %f; c: %f\" % (i, a, c))\r\n> \r\n> run: 0; a: -0.020802; c: 0.005541\r\n> run: 1; a: -0.020802; c: 0.005541\r\n> ```\r\n> \r\n> Next, the following demonstrates the behavior you are noticing. The global pseudo-random number generator will be in a different state after initializing `b`, and therefore `c` will be initialized differently than it was before. This is normal and expected behavior.\r\n> \r\n> ```\r\n> tf.random.set_seed(0)\r\n> a = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n> b = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n> c = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n> print(\"run: %d; a: %f; b: %f; c: %f\" % (2, a, b, c))\r\n> \r\n> run: 2; a: -0.020802; b: 0.005541; c: -0.030479\r\n> ```\r\n> \r\n> Finally, if you would like each variable to be initialized using pseudo-random number generator state that is independent of the most recent state of the global pseudo-random number generator, then you can provide a `seed` parameter to the initializers used for the variables initialized after the change in the model (or to all initializers).\r\n> \r\n> ```\r\n> for i in range(2):\r\n>   tf.random.set_seed(0)\r\n>   a = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n>   if (i == 1):\r\n>     b = tf.Variable(tf.random_uniform_initializer()(shape=(1,)))\r\n>   c = tf.Variable(tf.random_uniform_initializer(seed=42)(shape=(1,)))\r\n>   print(\"run: %d; a: %f; c: %f\" % (i, a, c))\r\n>   \r\n> run: 0; a: -0.020802; c: -0.021757\r\n> run: 1; a: -0.020802; c: -0.021757\r\n> ```\r\n> \r\n> The above code can be run in [this colab notebook](https://colab.research.google.com/drive/1LX3FmX0Vdka14SnXuRpvpxioCrJxseLF?usp=sharing).\r\n\r\nHi. I understand your workaround. However, I would like if this is automatically done in framework-level, since this will be much easier to debug the performance of nerual network.", "Using the `seed` parameter of the initializers is not a work-around, @edwardyehuang; there is definitely _**not**_ a bug that requires a work-around. For pseudo-random initialization of any given trainable variable to be non-variant under changes in the number and/or size of the trainable variables in the model (or any other changes in the use of the global pseudo-random number generator, PRNG), the `seed` parameters of the trainable variable initializers must be used.\r\n\r\nThis current issue is developing into a feature request. That would require the feature requirements to be clarified and an API to be defined.\r\n\r\n-------\r\n\r\nHere is a suggested proposal: when op determinism is expected (after [`tf.config.experimental.enable_op_determinism`](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) has been called), the instantiation of any initializer without a `seed` parameter would throw an exception.\r\n\r\nAn argument in favor of this functionality could be something like: without having a `seed` parameter, the initializer will produce a different sequence of initialization values if the code-path changes such that the state of the global PRNG is different when the initializer is executed.\r\n\r\nAn argument against this functionality could be something like: we make no guarantee about the reproducibility of the overall program when the code-path is changed in any way. If you want this functionality, you can explicitly reset the PRNGs used in the initializers.\r\n\r\n-------\r\n\r\nThis seems to be a new requirement entirely: the requirement that existing trainable variables are forced to initialize the same way even when the global PRNG is used differently elsewhere in the program. This goes beyond run-to-run determinism (with an unmodified program). I'm wondering how universally critical this requirement is and therefore whether this functionality should be forced (through exceptions) upon anyone who seeks only run-to-run determinism (with an unmodified program).", "@edwardyehuang or @mohantym please will you change the tag from bug to feature.\r\n\r\nAnother potential functionality, when op determinism is expected, might be for initializers to assume a seed value of `0` if one is not provided via the `seed` parameter, or to generate a seed value from `name` if `name` is provided. The priority would be: use the `seed` parameter provided if one is provided, otherwise generate a `seed` from `name` if `name` is provided, else set `seed` to `0`.\r\n\r\nI can't promise that any of this will be approved for the API, partly because I think it goes beyond the intended effects of [tf.config.experimental.enable_op_determinism](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism). I'm only trying to think through some possible ways this might work.\r\n\r\n@edwardyehuang, can you provide more information about why this is an important feature, how it would be used, and give a sense of how commonly this feature might be needed by the community in general?", "@edwardyehuang,\r\n\r\nPlease can you refer [comment1](https://github.com/tensorflow/tensorflow/issues/54442#issuecomment-1049458996), [comment2](https://github.com/tensorflow/tensorflow/issues/54442#issuecomment-1049526825) by @[duncanriach](https://github.com/duncanriach). \r\n\r\nCan you please provide more information  about why this is an important usecase? Thanks!", "> @edwardyehuang,\r\n> \r\n> Please can you refer [comment1](https://github.com/tensorflow/tensorflow/issues/54442#issuecomment-1049458996), [comment2](https://github.com/tensorflow/tensorflow/issues/54442#issuecomment-1049526825) by @[duncanriach](https://github.com/duncanriach).\r\n> \r\n> Can you please provide more information about why this is an important usecase? Thanks!\r\n\r\nBinding weights name with a fixed seed can help researcher figure out the extractly module/layer/design that helps to improve the overall performance:\r\n\r\nFor example, recently years sota publications often battle between pre-ln (layer normalization) or post-ln of Transformer:\r\n\r\n> Pre-LN:\r\n> 1. LayerNormalization\r\n> 2. Token-mixer (e.g. Multi heads Self-Attention)\r\n\r\n> Post-LN\r\n> 1. Token-mixer (e.g. Multi heads Self-Attention)\r\n> 2. LayerNormalization\r\n\r\nThat why the binding is important, since different order can have effect on seed if not specified.\r\n\r\nBTW, after binding, we still can change the global seed to do more modifications.\r\n\r\n\r\n\r\n", "The general principle behind run-to-run determinism with respect to data scientific exploration is that holding as many unrelated independent variables constant as possible helps to focus the experiment on the effect of the changes on the dependent variable. In other words, reducing unrelated noise increases the signal-to-noise ratio, and therefore the power of the experiment.\r\n\r\nIn this context, to paraphrase what you have written, when making a change to an architecture, the effect of that change is more easily determined when the initial state of the trainable variables is held constant when possible.\r\n\r\nThe example you have given is when the order of layers is changed. The original example you gave was equivalent to adding another layer. I imagine that another example might be removing a layer, or changing the number of trainable variables in a layer.\r\n\r\nThis seems like a reasonable motive for the proposed feature, but let's remember that the required functionality is basically already available by setting the (relevant) initializer seeds; it only requires that the user knows about and understands the principles that we are discussing in this issue.\r\n\r\n> BTW, after binding, we still can change the global seed to do more modifications.\r\n\r\nI'll address this idea using a code example, which I have added to the end of [my colab notebook](https://colab.research.google.com/drive/1LX3FmX0Vdka14SnXuRpvpxioCrJxseLF?usp=sharing):\r\n\r\n```python\r\nfor i in range(2):\r\n  tf.random.set_seed(i)\r\n  print(\"global seed: %d\" % i)\r\n  for j in range(2):\r\n    a = tf.Variable(tf.random_uniform_initializer(seed=10)(shape=(1,)))\r\n    if (i == 1):\r\n      b = tf.Variable(tf.random_uniform_initializer(seed=11)(shape=(1,)))\r\n    c = tf.Variable(tf.random_uniform_initializer(seed=12)(shape=(1,)))\r\n    print(\"run: %d; a: %f; c: %f\" % (j, a, c))\r\n    \r\nglobal seed: 0\r\nrun: 0; a: -0.039600; c: 0.040857\r\nrun: 1; a: -0.039600; c: 0.040857\r\nglobal seed: 1\r\nrun: 0; a: -0.039600; c: 0.040857\r\nrun: 1; a: -0.039600; c: 0.040857\r\n```\r\n\r\nThis shows that when the initializer seeds are set, the global seed will, in fact, have no effect at all on the initializers. With the existing functionality, if we want a given initializer to follow the global seed, we just don't provide a local seed. With the proposed functionality, we would need to provide a local seed in order to override the automatically-generated local seed.", "@reed, what are your thoughts on this proposed feature (see [this comment](https://github.com/tensorflow/tensorflow/issues/54442#issuecomment-1049526825)), which has motivation described in [this comment](https://github.com/tensorflow/tensorflow/issues/54442#issuecomment-1051545505)?", "The way random.set_seed works is in two different ways, `global seed` and `operation level seed`. \r\nYou can see the complete details in the document here https://www.tensorflow.org/api_docs/python/tf/random/set_seed.\r\nYou can also set the seed for the initializers, for example for `GlorotNormal` [here](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> The way random.set_seed works is in two different ways, `global seed` and `operation level seed`. You can see the complete details in the document here https://www.tensorflow.org/api_docs/python/tf/random/set_seed. You can also set the seed for the initializers, for example for `GlorotNormal` [here](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal).\r\n\r\nHi. I think you may overlooked discussion above."]}, {"number": 54430, "title": "Add appropriate value check for `tf.clip_by_norm`", "body": "This PR tries to address the issue raised in #54414 where\r\nthere is no check for clip_norm for tf.clip_by_norm.\r\nAs a result an invalid result was silently returned.\r\n\r\nThis PR fixes #54414.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["Can you fix Ubuntu CPU?", "Hi @yongtang Can you please check @mihaimaruseac's comments and keep us posted ? Thank you!", "@yongtang  Any update on this PR? Please. Thank you!"]}, {"number": 54425, "title": "Update sonartype Maven repo URL to HTTPS", "body": "Update sonartype Maven repo URL to HTTPS.\r\nAdditional files to the fix https://github.com/tensorflow/examples/commit/d315ddb", "comments": []}, {"number": 54422, "title": "`tf.nn.depth_to_space` related. What is the equivalent of ONNX DepthToSpace with mode `CRB` in TF?", "body": "**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n- Are you willing to contribute it (Yes/No): Yes.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI what to convert an [ONNX DepthToSpace](https://github.com/onnx/onnx/blob/main/docs/Operators.md#DepthToSpace) layer with mode `CRB` to an equivalent TensorFlow layer. I found [tensorflow::ops::DepthToSpace](https://www.tensorflow.org/api_docs/python/tf/nn/depth_to_space) which, based on my understanding, is equivalent to ONNX DepthToSpace layer with mode `DCR`.\r\n\r\nI could not find an implementation of `CRB` mode in TF. Can you confirm this? If this is the case, with your guidance, I can work on this feature.\r\n\r\n**Will this change the current api? How?**\r\nAddition of an argument `mode` which accepts `CRB` or `DCR` (Default `DCR`).\r\n\r\n**Code to experiment**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n@tf.function(experimental_compile=True)\r\ndef depth_to_space_nchw(x, block_size):\r\n    return tf.nn.depth_to_space(x, block_size=block_size, data_format=\"NCHW\")\r\n\r\ndef onnx_depth_to_space_dcr(x, block_size):\r\n    b, c, h, w = x.shape\r\n    tmp = np.reshape(x, [b, block_size, block_size, c // (block_size**2), h, w])\r\n    tmp = np.transpose(tmp, [0, 3, 4, 1, 5, 2])\r\n    y = np.reshape(tmp, [b, c // (block_size**2), h * block_size, w * block_size])\r\n    return np.copy(y, order='C')\r\n\r\ndef onnx_depth_to_space_crd(x, block_size):\r\n    b, c, h, w = x.shape\r\n    tmp = np.reshape(x, [b, c // (block_size ** 2), block_size, block_size, h, w])\r\n    tmp = np.transpose(tmp, [0, 1, 4, 2, 5, 3])\r\n    y = np.reshape(tmp, [b, c // (block_size ** 2), h * block_size, w * block_size])\r\n    return np.copy(y, order='C')\r\n\r\narr = np.array([x for x in range(12)]).reshape(1, 1, 1, 12)\r\nblock_size = 2\r\n\r\narr_nchw = np.copy(np.transpose(arr, [0, 3, 1, 2]),  order='C')\r\nout = tf.nn.depth_to_space(arr, block_size=block_size, data_format='NHWC')\r\nout_nchw = depth_to_space_nchw(arr_nchw, block_size=block_size)\r\n\r\nonnx_out_dcr = onnx_depth_to_space_dcr(arr_nchw, block_size)\r\nonnx_out_crb = onnx_depth_to_space_crd(arr_nchw, block_size)\r\n\r\n>> out\r\n<tf.Tensor: shape=(1, 2, 2, 3), dtype=int32, numpy=\r\narray([[[[ 0,  1,  2],\r\n         [ 3,  4,  5]],\r\n\r\n        [[ 6,  7,  8],\r\n         [ 9, 10, 11]]]])>\r\n\r\n>> np.transpose(out_nchw, [0, 2, 3, 1])\r\narray([[[[ 0,  1,  2],\r\n         [ 3,  4,  5]],\r\n\r\n        [[ 6,  7,  8],\r\n         [ 9, 10, 11]]]])\r\n\r\n>>np.transpose(onnx_out_dcr, [0, 2, 3, 1]) ## Note that CRB of ONNX is same as TF\r\narray([[[[ 0,  1,  2],\r\n         [ 3,  4,  5]],\r\n\r\n        [[ 6,  7,  8],\r\n         [ 9, 10, 11]]]])\r\n\r\n>>np.transpose(onnx_out_crb, [0, 2, 3, 1])\r\narray([[[[ 0,  4,  8],\r\n         [ 1,  5,  9]],\r\n\r\n        [[ 2,  6, 10],\r\n         [ 3,  7, 11]]]])\r\n```", "comments": ["We do not have a CR(D?) equivalent.  To properly add it to TF, you'll need to update all the files here:\r\n- https://github.com/tensorflow/tensorflow/search?q=filename%3Adepthtospace_op\r\n\r\nOne of them is for CPU, one for GPU, one is XLA (which is called by your `experimental_compile=True` function).\r\n\r\nand add the option to the `array_ops.depth_to_space` definition [here](https://github.com/tensorflow/tensorflow/blob/e929486e5863406770f5193dad854c067bd115cc/tensorflow/python/ops/array_ops.py#L4071).\r\n\r\nIs there also an equivalent mode for space-to-depth?\r\n"]}, {"number": 54417, "title": "```tf.GradientTape``` returns None gradient in fitting RNN", "body": "Define h_t as the hidden states of SimpleRNN layer at step t. h_0 the initial hidden states. In a model with an embedding layer and SimpleRNN layer, I would like to compute the partial derivative dh_t/dh_0 for each step t. \r\n\r\nThe structure of my model:\r\n```\r\n# For each h_t, compute gradient:\r\nbatch_size = 100; input_length = 1403\r\ninp= Input(batch_shape= (batch_size, input_length), name= 'input') \r\nemb_out= Embedding(input_dim, output_dim, input_length= input_length, \r\n                         weights= [Emat], trainable= False, name= 'embedding')(inp)\r\nrnn= SimpleRNN(200, return_sequences= True, return_state= False, stateful= True, name= 'simpleRNN')\r\n\r\nh0 = tf.convert_to_tensor(np.random.uniform(size= (batch_size, 200)).astype(np.float32))\r\nrnn_allstates= rnn(emb_out, initial_state=h0) \r\nmodel_rnn = Model(inputs=inp, outputs= rnn_allstates, name= 'model_rnn')\r\nmodel_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n\r\nds = tf.data.Dataset.from_tensor_slices((xtr_pad[:100], ytr[:100])).batch(100)\r\nembedding_layer = model_rnn.layers[1]\r\nrnn_layer = model_rnn.layers[2]\r\n\r\ngrads_allsteps= []\r\nfor b, (x_batch_train, y_batch_train) in enumerate(ds):\r\n    for t in range(input_length):\r\n        with tf.GradientTape() as tape:\r\n            tape.watch(h0)\r\n            et = embedding_layer(x_batch_train)\r\n            states = rnn_layer(et, initial_state= h0)   # (100, 1403, 200)\r\n            ht = states[:,t,:]  # (100, 200)\r\n\r\n        grad_t= tape.gradient(ht, h0)  # (100, 200)\r\n        print('Computed gradient dht/dh0 at step ', t+1, 'in batch', b+1)\r\n        grads_allsteps.append(grad_t)\r\n``` \r\nAt each step t, h_t has shape (100,200), h_0 has shape (100,200). However ```tape.gradient(ht, h0)``` returns None for every t. Below is the result of the first step:\r\n```\r\nfor t in range(1):\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(h0)\r\n        et = embedding_layer(x_batch_train)\r\n        #tape.watch(et)\r\n        states = rnn_layer(et, initial_state= h0)   # (100, 1403, 200)\r\n        ht = states[:,t,:] \r\n        print(ht)\r\n        print(h0)\r\n    grad_t = tape.gradient(ht, h0)\r\n    tf.print(grad_t)\r\n\r\n>>\r\n# h_t:\r\ntf.Tensor(\r\n[[ 0.25634336  0.5259362   0.60045886 ... -0.4978792   0.62755316\r\n   0.09803997]\r\n [ 0.58387524  0.26037565  0.5646103  ...  0.31233114  0.4853201\r\n   0.10877549]\r\n [ 0.17190906  0.68681747 -0.32054633 ... -0.6139967   0.48944488\r\n   0.06301598]\r\n ...\r\n [ 0.1985917  -0.11821499 -0.47709295 ... -0.05718012  0.16089934\r\n   0.20585683]\r\n [ 0.73872745  0.503326    0.25224414 ... -0.5771631   0.03748894\r\n   0.09212588]\r\n [-0.6597108  -0.43926442 -0.23546427 ...  0.26760277  0.28221437\r\n  -0.4039318 ]], shape=(100, 200), dtype=float32)\r\n\r\n# h_0:\r\ntf.Tensor(\r\n[[0.51580787 0.51664346 0.70773274 ... 0.45973232 0.7760376  0.48297063]\r\n [0.61048764 0.26038417 0.60392565 ... 0.7426153  0.15507504 0.57494944]\r\n [0.11859739 0.33591187 0.68375146 ... 0.59409297 0.5302879  0.28876984]\r\n ...\r\n [0.12401487 0.39376178 0.9850304  ... 0.21582918 0.9592233  0.5257605 ]\r\n [0.9401199  0.2157638  0.6445949  ... 0.36316434 0.5799403  0.3749675 ]\r\n [0.37230062 0.18162128 0.0739954  ... 0.21624395 0.66291    0.7807376 ]], shape=(100, 200), dtype=float32)\r\n\r\n# dh_t/dh_0:\r\nNone\r\n```\r\nI have successfully used GradientTape watch the inputs e_t to the RNN layer, and computed the gradients dh_t/de_t, but this does not really provide much information about the quality of model fitting. I would like to base the gradient on a quantity only at the start of the sequence.\r\n\r\nThere seems to be some difficulty for Gradient tape to watch this fixed-time quantity h_0, and perform gradient computation for dh_t/dh_0. Is there a way I can obtain this gradient shaped (100,200)? Thanks in advance for any help.\r\n\r\n", "comments": ["@DagonArises \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),\r\nThanks!", "@sushreebarsa I have filled in what I can find out. Please let me know if you need anything else. Thanks in advance for helping me with the GradientTape operation.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.3.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): v2.6.1-9-gc2363d6d025 2.6.2\r\n- Python version:  Python 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 2.3 GHz Quad-Core Intel Core i7; 32 GB 3733 MHz LPDDR4X\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nPlease refer to the code below for details.\r\n```tape.gradient(states, h0)``` returns a gradient of shape (100, 200). However I would like to obtain the gradient to contain the step index. Then I added a loop over the steps to compute ```tape.gradient(ht, h0)``` instead. At each step t, ht has shape (100,200), h0 has shape (100,200). However ```tape.gradient(ht, h0)``` returns dh_t/dh_0 but it is None type.\r\n\r\n**Describe the expected behavior**\r\nI am expecting ```tape.gradient(ht, h0)``` to return me a gradient object shaped (100,200), instead of None. \r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\n### 1. Imports \r\nfrom __future__ import print_function\r\nimport numpy as np\r\nfrom numpy import array, asarray, zeros\r\nimport pandas as pd \r\nfrom tqdm import tqdm\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimport tensorflow as tf\r\nfrom keras import Input, Model\r\nfrom keras.models import Sequential\r\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\r\nfrom keras.layers.core import Dense, Activation, Dropout, Flatten\r\nfrom keras.layers.embeddings import Embedding\r\nfrom tensorflow.keras.layers import BatchNormalization, PReLU\r\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\r\nfrom keras.preprocessing import sequence, text\r\nfrom keras import backend as k\r\n\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n%matplotlib inline\r\n\r\n### 2. Simulated data and gradient computation:\r\nbatch_size = 100; input_length = 5\r\nxtr_pad = tf.random.uniform((batch_size, input_length), maxval = 500, dtype=tf.int32)\r\nytr = tf.random.normal((batch_size, input_length, 200))\r\n\r\n\r\ninp= Input(batch_shape= (batch_size, input_length), name= 'input') \r\nemb_out= Embedding(500, 100, input_length= input_length, trainable= False, name= 'embedding')(inp)\r\nrnn= SimpleRNN(200, return_sequences= True, return_state= False, stateful= True, name= 'simpleRNN')\r\n\r\nh0 = tf.convert_to_tensor(np.random.uniform(size= (batch_size, 200)).astype(np.float32))\r\n\r\nrnn_allstates= rnn(emb_out, initial_state=h0) \r\nmodel_rnn = Model(inputs=inp, outputs= rnn_allstates, name= 'model_rnn')\r\nmodel_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\r\n\r\nds = tf.data.Dataset.from_tensor_slices((xtr_pad, ytr)).batch(100)\r\nembedding_layer = model_rnn.layers[1]\r\nrnn_layer = model_rnn.layers[2]\r\n\r\ngrads_allsteps= []\r\nfor b, (x_batch_train, y_batch_train) in enumerate(ds):\r\n    for t in range(input_length):\r\n        with tf.GradientTape(persistent = True) as tape:\r\n            tape.watch(h0)\r\n            states= model_rnn(x_batch_train)\r\n            ht = states[:,t,:] \r\n\r\n        grad_t= tape.gradient(ht, h0)  \r\n        print('Computed gradient dht/dh0 at step ', t+1, 'in batch', b+1)\r\n        grads_allsteps.append(grad_t)\r\n\r\n \r\ngrads_allsteps\r\n\r\n>>\r\n[<tf.Tensor: shape=(100, 200), dtype=float32, numpy=\r\n array([[ 1.2307187 , -1.0343404 ,  0.52859926, ..., -0.09879799,\r\n         -1.1407609 , -0.7241671 ],\r\n        [ 1.142821  , -1.312029  ,  0.37148148, ...,  0.2300478 ,\r\n         -1.1440411 , -0.36673146],\r\n        [ 1.2778691 , -1.2225235 ,  0.69951147, ...,  0.17701946,\r\n         -1.2816343 , -0.52648413],\r\n        ...,\r\n        [ 1.1717036 , -1.2444504 ,  0.5874837 , ..., -0.13161334,\r\n         -1.3752006 , -0.376719  ],\r\n        [ 1.1333262 , -1.0013355 ,  0.3363382 , ..., -0.22350994,\r\n         -1.299541  , -0.5073889 ],\r\n        [ 1.18489   , -0.90809333,  0.55045474, ..., -0.10550319,\r\n         -1.0866506 , -0.58325446]], dtype=float32)>, None, None, None, None]\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "@sushreebarsa  Hi I have updated with a simple test case, and found something interesting: the first-step gradient is computed and looks fine. But the rest are Nones even when I set persistent = True. ", "@DagonArises I tried to replicate this issue on colab using TF v2.8.0 and faced different outcome .Could you please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/633bfbe5a5a4c867f487c4729c98eefa/54417.ipynb) and let me know if I am missing something to reproduce this issue? Thank you!", "@sushreebarsa \r\nHave you looked up the content in ```grads_allsteps```? I have run your code and got the same output as before, did you find different outputs?\r\n```\r\n[<tf.Tensor: shape=(100, 200), dtype=float32, numpy=\r\n array([[-1.1518437 ,  0.09250898,  0.63095075, ...,  1.0590861 ,\r\n         -1.1120602 ,  1.8342849 ],\r\n        [-1.2972618 ,  0.07211287,  0.7869835 , ...,  1.1403981 ,\r\n         -0.976047  ,  1.7088349 ],\r\n        [-1.318328  ,  0.20531593,  0.89947236, ...,  0.97652054,\r\n         -0.8525504 ,  1.6919007 ],\r\n        ...,\r\n        [-1.1943431 ,  0.18806057,  0.7745916 , ...,  1.2284715 ,\r\n         -0.78275585,  1.8919656 ],\r\n        [-1.112908  ,  0.22119978,  0.8069727 , ...,  0.9464315 ,\r\n         -1.2341805 ,  1.6773576 ],\r\n        [-1.2471178 ,  0.34073132,  0.60667646, ...,  0.8718521 ,\r\n         -1.0613108 ,  1.4353436 ]], dtype=float32)>, None, None, None, None]\r\n```\r\n\r\nAnyways I have found a way around it: simply ```tf.gradients(ht, h0)``` will consistently compute the gradients. Seems that Tape really needs the source to be the 'real input' to NN. Probably h0 is not a typical source.\r\n", "@DagonArises Thank you for the update!\r\n@chunduriv I was able to replicate the issue on colab using TF [v2.8.0](https://colab.research.google.com/gist/sushreebarsa/633bfbe5a5a4c867f487c4729c98eefa/54417.ipynb#scrollTo=hZuT2KJz-bYy) and [tf-nightly](https://colab.research.google.com/gist/sushreebarsa/e648d2fb4e48ee6a38e1f7d86ec2dbad/54417.ipynb#scrollTo=VNtcMcCwPjhd)(2.9.0.dev20220224) , please find the attached gists for reference.Thanks!"]}, {"number": 54414, "title": "`tf.clip_by_norm` gives WRONG results when given negative `norm`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)\r\nclip_norm = -6.0\r\nx_clipped = tf.clip_by_norm(x, clip_norm, )\r\nprint(x)\r\nprint(x_clipped)\r\n\r\n```\r\nOutputs:\r\n```\r\ntf.Tensor([[1. 2. 3. 4. 5.]], shape=(1, 5), dtype=float32)\r\ntf.Tensor([[-0.80903983 -1.6180797  -2.4271195  -3.2361593  -4.0451994 ]], shape=(1, 5), dtype=float32)\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.clip_by_norm` has an argument `clip_norm` which should be a **positive** floating point. However, it does not perform any validity checking and can accept a negative value like `-6.0`.  When applied to a tensor, it produces wrong output silently.\r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.clip_by_norm` should check the value of `clip_norm`.\r\n", "comments": ["Added PR #54430 for the fix."]}, {"number": 54410, "title": "`tf.math.cumsum` lack checking for bool arguments", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = [0.5, 1.0, 2.0, 4.0]\r\naxis = 0\r\nexclusive = -1\r\nreverse = -1\r\nres_1 = tf.math.cumsum(x, axis=axis, exclusive=exclusive, reverse=reverse)\r\nprint(res_1) # tf.Tensor([7. 6. 4. 0.], shape=(4,), dtype=float32)\r\nres_2 = tf.raw_ops.Cumsum(x=x, axis=axis, exclusive=exclusive, reverse=reverse)\r\nprint(res_2) # TypeError: Expected bool for argument 'exclusive' not -1.\r\n```\r\n\r\n**Describe the current behavior**\r\n`tf.math.cumsum` has an argument `exclusive` which should be a `bool`. However, it does not perform any validity checking and can accept a non-bool value like `-1`.  `tf.raw_ops.Cumsum` (another API which does the same job) can detect this error and raise an `TypeError`.\r\n\r\n\r\n**Describe the expected behavior**\r\n`tf.math.cumsum` should check the type of input values for `exclusive` and `reverse`.\r\n", "comments": ["@chunduriv ,\r\nI was able to reproduce the issue in tf v2.7,v2.8 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/e28662bd282cf264ba33fb906a21105d/54410.ipynb).", "@ArrowIntoTheSky Agree with you that this is a bug we need to fix. Thanks for raising it.\r\nI am working on to fix the issue. Thanks!\r\n\r\nRaised internal cl [cl/429441939](https://critique.corp.google.com/cl/429441939) to fix the issue"]}, {"number": 54399, "title": "Add nnapi/sl/SupportLibrary.cc to TFLITE_NNAPI_SRCS", "body": "nnapi/sl/SupportLibrary.cc is missing from the source when TFLITE_ENABLE_NNAPI option is set.\r\nThis patch add it to TFLITE_NNAPI_SRCS.", "comments": ["Any update?", "@karimnosseir Can you please review this PR ? Thank you!"]}, {"number": 54397, "title": "Eigen bug - build failed with MSVC when enable F16C", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.19043.1526\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.8.0\r\n- Python version: Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 4.2.1\r\n- GCC/Compiler version (if compiling from source): Microsoft Visual Studio 2019 v16.11.0 / Microsoft (R) C/C++ Optimizing Compiler Version 19.29.30133 for x64\r\n- CUDA/cuDNN version: cuda_11.5.2_496.13_windows / cudnn_8.3.2.44_windows\r\n- GPU model and memory: NVIDIA GeForce 2080Ti 11G\r\n\r\n**Describe the problem**\r\n\r\nhttps://docs.microsoft.com/en-us/cpp/intrinsics/x64-amd64-intrinsics-list\r\nhttps://gitlab.com/libeigen/eigen/-/issues/2395\r\n\r\nMSVC supports F16C instruction set but gitlab#2395 only fix for gcc.\r\nPlease help to report this issue to upstream repo and update eigen lib version.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: /MP /cgthreads8 /Qpar /fp:fast /D__F16C__ /arch:AVX2 -Ob2\r\nbazel build --config=opt --copt=-nvcc_options=use_fast_math --incompatible_strict_action_env=false --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR: C:/users/user/source/repos/tensorflow/tensorflow/tools/proto_text/BUILD:31:10: Compiling tensorflow/tools/proto_text/gen_proto_text_functions.cc failed: (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/jcmowys7/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/User/anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/User/anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\User\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=compute_35,sm_50,sm_52,sm_61,sm_70,sm_75,compute_86\r\n    SET TMP=C:\\Users\\User\\AppData\\Local\\Temp\r\n  C:/Users/User/anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /MP /cgthreads8 /Qpar /fp:fast /D__F16C__ /arch:AVX2 -Ob2 -nvcc_options=use_fast_math /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/tools/proto_text/_objs/gen_proto_text_functions/gen_proto_text_functions.obj /c tensorflow/tools/proto_text/gen_proto_text_functions.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\nC:\\users\\user\\_bazel_user\\jcmowys7\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/arch/Default/Half.h(538): error C3861: '_cvtss_sh': \u627e\u4e0d\u5230\u8b58\u5225\u9805\r\nC:\\users\\user\\_bazel_user\\jcmowys7\\execroot\\org_tensorflow\\external\\eigen_archive\\Eigen\\src/Core/arch/Default/Half.h(599): error C3861: '_cvtsh_ss': \u627e\u4e0d\u5230\u8b58\u5225\u9805\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/users/user/source/repos/tensorflow/tensorflow/core/BUILD:1718:23 Middleman _middlemen/_S_Stensorflow_Score_Cframework_Uheaders_Ulib-BazelCppSemantics_build_arch_x64_windows-opt failed: (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/jcmowys7/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.5\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/User/anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/User/anaconda3/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\User\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=compute_35,sm_50,sm_52,sm_61,sm_70,sm_75,compute_86\r\n    SET TMP=C:\\Users\\User\\AppData\\Local\\Temp\r\n  C:/Users/User/anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /MP /cgthreads8 /Qpar /fp:fast /D__F16C__ /arch:AVX2 -Ob2 -nvcc_options=use_fast_math /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/tools/proto_text/_objs/gen_proto_text_functions/gen_proto_text_functions.obj /c tensorflow/tools/proto_text/gen_proto_text_functions.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\n```", "comments": ["For future reference, Eigen bugs get reported here: https://gitlab.com/libeigen/eigen/issues\r\n\r\nFor this issue, it turns out MSVC doesn't have the basic scalar conversion methods (only the SIMD vector ones).  I have an MR to fix this in upstream Eigen [!924](https://gitlab.com/libeigen/eigen/-/merge_requests/924).  It will get pulled into TF in a couple weeks."]}, {"number": 54396, "title": "Floating point exception (core dumped)", "body": "**System information**\r\nGPU : 3060, 12G\r\nLinux Ubuntu 16.04\r\ngcc6.3\r\nbazel3.1\r\npython3.8\r\ncuda11.1\r\ncudnn8.1\r\nTensorflow2.4.1\r\n\r\nI use tensorflow by python api, which compiled by the source code\uff0c Got the right answer.\r\n\r\nI use tensorflow by c++ ,  When the code executes to \" session->Run(inputs, {\"ssd_pc_loc:0\",\"ssd_pc_score:0\",\"ssd_pc_height:0\",\"ssd_pc_classes:0\"}, {}, &outputs);\"  , the error occurred! \r\n\r\n2022-02-15 20:27:17.160612: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n/home/yj/auto_introduce/perception/v2/tf/main.cpp 5\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 21\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 61\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 75\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 80\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 85\r\n2022-02-15 20:27:17.237107: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2022-02-15 20:27:17.271412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz\r\n2022-02-15 20:27:17.272708: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f6a920 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2022-02-15 20:27:17.272721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2022-02-15 20:27:17.274320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2022-02-15 20:27:17.373277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.378295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.378866: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f69b60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2022-02-15 20:27:17.378881: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\r\n2022-02-15 20:27:17.378885: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3060, Compute Capability 8.6\r\n2022-02-15 20:27:17.379032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.379463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\r\ncoreClock: 1.777GHz coreCount: 28 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 335.32GiB/s\r\n2022-02-15 20:27:17.379486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.379893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 3060 computeCapability: 8.6\r\ncoreClock: 1.777GHz coreCount: 28 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 335.32GiB/s\r\n2022-02-15 20:27:17.379928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2022-02-15 20:27:17.381703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2022-02-15 20:27:17.381747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2022-02-15 20:27:17.382384: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2022-02-15 20:27:17.382658: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2022-02-15 20:27:17.384218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2022-02-15 20:27:17.384762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2022-02-15 20:27:17.384927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2022-02-15 20:27:17.384982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.385427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.385869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.386493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.386967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2022-02-15 20:27:17.387013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2022-02-15 20:27:17.814844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2022-02-15 20:27:17.814870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \r\n2022-02-15 20:27:17.814874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \r\n2022-02-15 20:27:17.814876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \r\n2022-02-15 20:27:17.814992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.815439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.815868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.816278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10631 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2022-02-15 20:27:17.816574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-02-15 20:27:17.817033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11090 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:02:00.0, compute capability: 8.6)\r\nSession created successfully\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 108\r\nLoad graph protobuf successfully\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 120\r\nAdd graph to session successfully\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 130\r\n/home/yj/auto_introduce/perception/v2/tf/tf_model_api.cpp 141\r\n2022-02-15 20:27:21.165762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2022-02-15 20:27:21.494311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2022-02-15 20:27:21.501505: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n2022-02-15 20:27:21.502071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\nFloating point exception (core dumped)\r\n\r\n\r\n", "comments": ["@yujun2019 \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose),and please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "here is my code :\r\n\r\n\r\n    Tensor thresh(tensorflow::DT_FLOAT, TensorShape({4}));\r\n    auto threshPtr = thresh.tensor<float,1>();\r\n    threshPtr(0) = 0.5;\r\n    threshPtr(1) = 0.5;\r\n    threshPtr(2) = 0.5;\r\n    threshPtr(3) = 0.5;\r\n\r\n    Tensor box(tensorflow::DT_FLOAT, TensorShape({4}));\r\n    auto boxPtr = box.tensor<float,1>();\r\n    boxPtr(0) = -36.2f;\r\n    boxPtr(1) = 66.2f;\r\n    boxPtr(2) = -51.2f;\r\n    boxPtr(3) = 51.2f;\r\n\r\n    Tensor proj_mat(tensorflow::DT_FLOAT, TensorShape({4, 4}));\r\n    auto proj_matPtr = proj_mat.tensor<float,2>();\r\n    proj_matPtr(0,0) = 956.843;\r\n    proj_matPtr(0,1) = -871.006;\r\n    proj_matPtr(0,2) = -49.2127;\r\n    proj_matPtr(0,3) = -142.83;\r\n\r\n    proj_matPtr(1,0) = 586.18;\r\n    proj_matPtr(1,1) = 20.0639;\r\n    proj_matPtr(1,2) = -860.008;\r\n    proj_matPtr(1,3) = -28.2397;\r\n\r\n    proj_matPtr(2,0) = 0.999623;\r\n    proj_matPtr(2,1) = -0.217058;\r\n    proj_matPtr(2,2) = -0.0168331;\r\n    proj_matPtr(2,3) = -0.14631;\r\n\r\n    proj_matPtr(3,0) = 0.f;\r\n    proj_matPtr(3,1) = 0.f;\r\n    proj_matPtr(3,2) = 0.f;\r\n    proj_matPtr(3,3) = 1.f;\r\n\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    cv::Mat mat_in = cv::imread(\"/home/yj/testImage/raven0.jpg\");\r\n    Tensor img_input(tensorflow::DT_UINT8,TensorShape({mat_in.rows, mat_in.cols, 3})); //h w c\r\n    uint8 *img_p = img_input.flat<uint8>().data();\r\n    cv::Mat mat_out(mat_in.rows, mat_in.cols, CV_8UC3, img_p);\r\n    mat_out = mat_in.clone();\r\n\r\n    // cv::Mat mat_in(image_ptr->height, image_ptr->width,\r\n    //                 CV_8UC1, const_cast<uint8_t*>(image_ptr->data.data()));\r\n    // if (image_ptr->encoding == sensor_msgs::image_encodings::BAYER_RGGB8) {\r\n    //     cv::cvtColor(mat_in, mat_out, CV_BayerBG2BGR);\r\n    // } else if (image_ptr->encoding == sensor_msgs::image_encodings::BAYER_BGGR8) {\r\n    // cv::cvtColor(mat_in, mat_out, CV_BayerRG2BGR);\r\n    // }\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    \r\n\r\n\r\n    Tensor input(tensorflow::DT_FLOAT,TensorShape({117546, 4}));\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    auto inputPtr = input.tensor<float,2>();\r\n    std::ifstream pcdFile(\"/home/yj/testImage/pcd1.txt\");\r\n    std::string temp;\r\n    int pointIdx = 0;\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    while(getline(pcdFile,temp)) \r\n    { \r\n        float x,y,z;\r\n        sscanf(temp.c_str(),\"%f %f %f\",&x,&y,&z);\r\n        inputPtr(pointIdx,0) = x;\r\n        inputPtr(pointIdx,1) = y;\r\n        inputPtr(pointIdx,2) = z;\r\n        inputPtr(pointIdx,3) = 0;\r\n        pointIdx++;\r\n        \r\n    } \r\n    pcdFile.close(); \r\n \r\n\r\n    Session* session;\r\n    Status status = NewSession(SessionOptions(), &session);\r\n    if (!status.ok()) {\r\n        std::cerr << status.ToString() << std::endl;\r\n        //return 1;\r\n    } else {\r\n        std::cout << \"Session created successfully\" << std::endl;\r\n    }\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n\r\n    // Load the protobuf graph\r\n    GraphDef graph_def;\r\n    std::string graph_path = \"/opt/auto/data/model/raven_model/raven_model_2wf_95ws_tf1.8.pb\";\r\n    status = ReadBinaryProto(Env::Default(), graph_path, &graph_def);\r\n    if (!status.ok()) {\r\n        std::cerr << status.ToString() << std::endl;\r\n        //return 1;\r\n    } else {\r\n        std::cout << \"Load graph protobuf successfully\" << std::endl;\r\n    }\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    \r\n    // Add the graph to the session\r\n    status = session->Create(graph_def);\r\n    if (!status.ok()) {\r\n        std::cerr << status.ToString() << std::endl;\r\n        //return 1;\r\n    } else {\r\n        std::cout << \"Add graph to session successfully\" << std::endl;\r\n    }\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    \r\n\r\n    \r\n    std::vector<std::pair<string, tensorflow::Tensor>> inputs = {\r\n            {\"xmin_xmax_ymin_ymax:0\", box},\r\n            {\"proj_mat:0\", proj_mat },\r\n            {\"img_input:0\",img_input},\r\n            {\"input_pc:0\",input},\r\n            {\"input_thresh:0\",thresh}\r\n    };\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    \r\n    // The session will initialize the outputs\r\n    std::vector<tensorflow::Tensor> outputs;\r\n \r\n    // Run the session, evaluating our \"c\" operation from the graph\r\n    status = session->Run(inputs, {\"ssd_pc_loc:0\",\"ssd_pc_score:0\",\"ssd_pc_height:0\",\"ssd_pc_classes:0\"}, {}, &outputs);\r\n    if (!status.ok()) {\r\n        std::cerr << status.ToString() << std::endl;\r\n        //return 1;\r\n    } else {\r\n        std::cout << \"Run session successfully\" << std::endl;\r\n    }\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    \r\n    // Grab the first output (we only evaluated one graph node: \"c\")\r\n    // and convert the node to a scalar representation.\r\n    auto output_c = outputs[0].scalar<float>();\r\n \r\n    // (There are similar methods for vectors and matrices here:\r\n    // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/tensor.h)\r\n \r\n    // Print the results\r\n    std::cout << outputs[0].DebugString() << std::endl; // Tensor<type: float shape: [] values: 30>\r\n    std::cout << \"output value: \" << output_c() << std::endl; // 30\r\n    std::cout << __FILE__ << \" \" << __LINE__<< std::endl;\r\n    \r\n    // Free any resources used by the session\r\n    session->Close();"]}, {"number": 54391, "title": "Consider supporting png images for modelmaker object detector", "body": "**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the `object_detector` class only supports jpg images. This can be a problem, especially if you have already created label annotations for the images in another image format. I suggest that we consider adding support for png files as well.\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nEveryone using png images instead of jpg for training data.\r\n**Any Other info.**\r\nAs far as I can see then there is a check for file format here on line 133 of the `dict_to_tf_example` function: https://github.com/tensorflow/examples/blob/035fb73d5ff8b74958c9e3f83f44fc20dfc39119/tensorflow_examples/lite/model_maker/third_party/efficientdet/dataset/create_pascal_tfrecord.py#L133", "comments": ["@Malthehave \r\nCan you please elaborate about your feature and please specify the use cases for this feature? Thanks!", "Hi @sushreebarsa, sure thing. \r\nFor clarification I'm talking about the `tflite_model_maker` library.\r\nAs of right now you will get an error if you try to train the network with .png images, by calling the [ObjectDetector.create](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/ObjectDetector#create) method. \r\nA basic use case, is that you have already created your annotations in PASCAL VOC format, pointing to your PNG images."]}, {"number": 54390, "title": "[TFLite] Refactor and centralize LUT generation and access code into headers", "body": "Hello,\r\n\r\nThis PR refactors and centralizes the `PopulateLookupTable` and `gen_lut` functions into a single `LUTPopulate` function that can generate uint8, int8 and int16 LUTs and be used by multiple operators.\r\n\r\nThe change is done in multiple steps:\r\n* The int8/uint8 `PopulateLookupTable` from `tensorflow/lite/kernels/activations.cc` is moved into `tensorflow/lite/kernels/internal/common.h` with a slight signature change and `gen_lut` is renamed to `PopulateLookupTable<int16_t>` with also a slight signature change. This change centralizes all the LUT generation in a single templated `PopulateLookupTable<T>` function.\r\n* The `std::function` of the int8/uint8 `PopulateLookupTable` is replaced by a raw function pointer so that it can be used in TFLite Micro.\r\n* The code used for `gen_lut` is now only used for int16 -> int16 LUT generation. int8 -> int8, int8 -> int16 and int16 -> int8 support was removed as unused and not fundamentally necessary.\r\n* All the LUT generation calls are adapted to these changes.\r\n* We made sure that the change is numerically backward compatible, all the generated LUTs have the exact same values.\r\n* The LUT lookup code was also moved into separate headers `tensorflow/lite/kernels/internal/optimized/integer_ops/lut.h` and `tensorflow/lite/kernels/internal/reference/integer_ops/lut.h` to centralize the LUT access solving the `// TODO(b/143696793): move this to optimized_ops.` comment.\r\n* `gen_lut/PopulateLookupTable`, `lut_size` and `lut_lookup` were renamed to `LUTPopulate`, `LUTSize` and `LUTLookup` respectively in a separate commit. This change is done to have more uniform names for these related functions and be consistent with the CamelCase function naming used in the codebase. This separate commit is minor and optional and could eventually be removed if deemed unnecessary.\r\n\r\nThese modifications don't change the numerical behaviour of the operators and don't add any new feature. They are made to avoid any potential future conflicts and confusions between the `PopulateLookupTable` and `lut_gen` functions and make the LUT generation code accessible to other operators (example of a [potential EXP quantization PR](https://github.com/Tessil/tensorflow/commit/b12f4980e26a0a79bc0019f6965c518d24062f57) that is based on these changes).\r\n\r\nThibaut\r\n", "comments": []}, {"number": 54386, "title": "tf.function with jit_compile runs on CPU when there is GPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip install\r\n- TensorFlow version (use command below):\r\nv2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: Python 3.9.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0 (ptxas cherrypicked from CUDA 11.3)\r\n- GPU model and memory: RTX A6000\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`@tf.function(jit_compile=True)` runs on both CPU and GPU, and mostly CPU\r\n**Describe the expected behavior**\r\nShould mainly use GPU\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function(jit_compile=True)\r\ndef func(a, b):\r\n    for i in tf.range(100):\r\n        c = a + b\r\n        a = b\r\n        b = c \r\n    return c\r\nconcrete_fn = func.get_concrete_function(\r\n    tf.TensorSpec(shape=[3], dtype=tf.float32, name='a'),\r\n    tf.TensorSpec(shape=[3], dtype=tf.float32, name='b')\r\n    )\r\na = tf.random.normal([3])\r\nb = tf.random.normal([3])\r\ntf.profiler.experimental.start('profile_results_simple')\r\nfor step in range(300):\r\n    with tf.profiler.experimental.Trace(\"Train\", step_num=step, _r=1):\r\n        concrete_fn(a, b)\r\ntf.profiler.experimental.stop()\r\n```\r\n\r\n![Screen Shot 2022-02-15 at 5 30 14 PM](https://user-images.githubusercontent.com/20623744/154033461-2cb94471-666d-4fba-87f0-aac898e2d08e.png)\r\n\r\nThe full tensorboard file can be downloaded from [here](https://drive.google.com/file/d/1IYNdjCkko4u2ktaxRe1vdjbqCMjrkgtg/view?usp=sharing).\r\n\r\nAbove is a short code snippt to reproduce the problem. In my actual project, this problem is more severe. I can see the GPU usage drops to 0% and stays there for a few seconds before rise to ~70% again and this pattern repeats for every step. This GPU drop cannot be explained by disk IO because I am working on a toy example and input is some randomly generated tensors.\r\n\r\nMoreover, adding `with tf.device('/gpu:0'):` doesn't solve this issue.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@charlielam0615 ,\r\nWhile executing the provided code i haven't found any issue.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/e35a9f5983135d4cbb7ba29525bb6fc1/untitled228.ipynb) and provide complete code to debug.Thanks!", "> @charlielam0615 ,\r\n> While executing the provided code i haven't found any issue.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/e35a9f5983135d4cbb7ba29525bb6fc1/untitled228.ipynb) and provide complete code to debug.Thanks!\r\n\r\nRunning this gist will create profile files inside the `profile_results_simple` folder. Visualizing this profile using tensorboard shows exactly the same issue. Below is the screenshot where you can see lots of ops happened in CPU instead of GPU.\r\n\r\n![Screen Shot 2022-02-16 at 4 53 10 PM](https://user-images.githubusercontent.com/20623744/154229250-11ee340e-f3a8-45e5-9602-4ffabd070d8d.png)\r\n\r\nI further tested this code with \r\nCUDA 11.2, RTX A6000, TF 2.7.0\r\nCUDA 11.2, RTX A6000, TF 2.6.0\r\nCUDA 11.2, Tesla V100, TF 2.6.0\r\n\r\nThis problem persists.\r\n\r\nSetting `jit_compile=False` to get rid of XLA would resolve this issue (ops run only on GPU) but loses JIT benefits.\r\n\r\n", "@charlielam0615 ,\r\nThe code provided is not complete hence it would be difficult for us to pinpoint the issue in the tensorboard. Please share complete stand alone code to replicate the issue or a colab gist with the error reported.?", "I don't understand. The code is complete and it is a somewhat minimum stand alone code to replicate this issue. Am I missing something?\r\n\r\nIf I have to break this down into steps...\r\n\r\nStep 1. Run the notebook in the [gist](https://colab.research.google.com/gist/tilakrayal/e35a9f5983135d4cbb7ba29525bb6fc1/untitled228.ipynb) (which you provide)\r\nStep 2. Click 'Files' on the left panel, and in the file panel right-click and choose \"refresh\", you should see a directory named \"profile_results_simple\"\r\nStep 3. Create a new cell, and compress \"profile_results_simple\" into a zip file in order to download it by running `!zip -r profile_results_simple.zip profile_results_simple`\r\nStep 4. Download \"profile_results_simple.zip\", uncompress it, visualize it using tensorboard, and voila.", "Hi @charlielam0615 ! I used **tf.debugging.set_log_device_placement(True)** to see device usage in [Colab](https://colab.sandbox.google.com/gist/mohantym/9ca0a2807d64f33cf11feec023f672e2/untitled228.ipynb#scrollTo=GT0_l7GOz5hx) with 2.8. It was using GPU in entire operation.  Can you try in TF 2.8/nightly and let us know?", "> Hi @charlielam0615 ! I used **tf.debugging.set_log_device_placement(True)** to see device usage in [Colab](https://colab.sandbox.google.com/gist/mohantym/9ca0a2807d64f33cf11feec023f672e2/untitled228.ipynb#scrollTo=GT0_l7GOz5hx) with 2.8. It was using GPU in entire operation. Can you try in TF 2.8/nightly and let us know?\r\n\r\nInteresting. I am confused then. Why does the profiler show lots of ops running on CPU when visualized in tensorboard? There seems to be some contradiction in results given by `tf.debugging.set_log_device_placement(True)` and tensorflow profiler.", "Hi @gadagashwini ! Could you please look at this issue ? It is replicating in [2.7](https://colab.research.google.com/gist/mohantym/b8e01d36c61ae847c3fd40d90868605a/untitled228.ipynb#scrollTo=CumlNBz0QYXy), [2.8](https://colab.research.google.com/gist/mohantym/9ca0a2807d64f33cf11feec023f672e2/untitled228.ipynb#scrollTo=a3BsA6yhQbwG) and [nightly](https://colab.research.google.com/gist/mohantym/b8e01d36c61ae847c3fd40d90868605a/untitled228.ipynb#scrollTo=CumlNBz0QYXy). Thanks", "In the Gist [here](https://colab.sandbox.google.com/gist/mohantym/9ca0a2807d64f33cf11feec023f672e2/untitled228.ipynb#scrollTo=GT0_l7GOz5hx) it shows all the Ops are running in GPU and tensorboard also shows device used as GPU. Could you please point out in tensorboard for the ops running in CPU. Thanks!", "Hi @sachinprasadhs! If you select \"Tools\u2192Trace Viewer\", the tensorboard shows almost every op runs both on GPU **and** host CPU, which is very confusing. I verified this on the gist you provided."]}, {"number": 54385, "title": "Load tflite model meet The model allocation is null/empty", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: armv7\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): tensorflow-2.8.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): arm-linux-gnueabihf-gcc\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nI built a libtensorflowlite_c.so with cmake\r\n```\r\nARMCC_FLAGS=\"-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations\"\r\nARMCC_PREFIX=/tools/toolchain/gcc-x86_64_arm-linux-gnueabihf/bin/arm-linux-gnueabihf-\r\ncmake -DCMAKE_C_COMPILER=${ARMCC_PREFIX}gcc \\\r\n  -DCMAKE_CXX_COMPILER=${ARMCC_PREFIX}g++ \\\r\n  -DCMAKE_C_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n  -DCMAKE_CXX_FLAGS=\"${ARMCC_FLAGS}\" \\\r\n  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \\\r\n  -DCMAKE_SYSTEM_NAME=Linux \\\r\n  -DCMAKE_SYSTEM_PROCESSOR=armv7 \\\r\n  ../tensorflow/lite/c\r\n  \r\ncmake --build . -j  \r\n```\r\n\r\nhowever, when loading model by\r\n```\r\nTfLiteModel *model;\r\nmodel = TfLiteModelCreateFromFile(\"model.tflite);\r\n```\r\n\r\ni meets the following problem\r\n```\r\nERROR: Mmap of '5' at offset '0' failed with error '22'.  \r\nERROR: The model allocation is null/empty\r\n```\r\n\r\nhere is my model:\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/8067674/model.zip)\r\n\r\n", "comments": ["By the way, if i just use gcc to compile a lib for PC by\r\n```\r\ncmake ../tensorflow/lite/c ..\r\ncmake --build . -j \r\n```\r\nIt's all fine.", "Hi @Ryuk17 ! Could  you check this [instructions](https://www.tensorflow.org/lite/guide/build_cmake_arm#build_for_armv7_neon_enabled) for ARMV7?", "Thanks for reply @mohantym. I have tried that instructions last month but I met problems as #54069. So I try current method to build the libtensorflowlite_c.so with the CMakeLists in /tensorflow/lite/c instead of  libtensorflow-lite.a with the CMakeLists in /tensorflow/lite."]}, {"number": 54379, "title": "[INTEL oneDNN] Fix mkl_fused_ops_test failure and disable blocked format", "body": "The PR\r\n\r\n(1) fixes unit test failure with mkl_fused_ops_test.cc\r\n\r\n(2) Code cleanup by disabling all blocked format related tests - most code change is for (2)\r\n\r\nRef: https://github.com/tensorflow/tensorflow/pull/53288\r\n(merged public PR which disables oneDNN blocked format support)", "comments": ["> Thank you for the PR and sorry for the delay! (1) and (2) don't need to be in the same PR. For future PRs that can be decoupled, please put the changes in separate PRs.\r\n\r\nYes, I will separate in future. There will be multiple blocked format related PRs in near feature. I will make sure that every PR will have limited scope (say, one PR for each MKL op). Thanks", "@penpornk Thank you for the review, suggestions and approval!\r\n\r\n", "@gzmkl Could you please help take a look at the [mkl_fused_ops_test failure](https://source.cloud.google.com/results/invocations/9310146f-00de-43bc-a489-f5b8959515d2/targets/%2F%2Ftensorflow%2Fcore%2Fkernels%2Fmkl:mkl_fused_ops_test/tests) in the [Ubuntu CPU](https://source.cloud.google.com/results/invocations/9310146f-00de-43bc-a489-f5b8959515d2/targets) test?", "Yess, will do!\r\n\r\n", "@penpornk I have addressed the Ubuntu CPU test failure with a code change (my mistake when creating public PR from internal PR). \r\n\r\nThe new code is similar to the previous new code, except that the \"else\" block is removed. I have explained in the updated comments.\r\n\r\nThanks!\r\n\r\n", "It seems we still got the [Ubuntu CPU](https://source.cloud.google.com/results/invocations/87b01a5b-8b87-415d-bf05-864eeff01ac6) error\r\n```\r\ntensorflow/core/kernels/mkl/mkl_fused_ops_test.cc:794\r\nExpected equality of these values:\r\n  ::tensorflow::Status::OK()\r\n    Which is: OK\r\n  (RunOpKernel())\r\n    Which is: ABORTED: Operation received an exception:Status: 3, message: could not create a primitive descriptor iterator, in file tensorflow/core/kernels/mkl/mkl_conv_ops.cc:893\r\n```", "@penpornk I am investigating this test failure (original test case failure passed; but pad with fused conv2d test case failure).\r\n\r\nLocally I am troubleshoot with latest master + PR changes. I will let you know once there is progress.  Thanks!\r\n", "Thank you for the update!", "@gbaned  Hi, I just made coding style fixes.  I wonder if you can help to trigger any round of PR test.\r\n\r\n@penpornk for the previous [Ubuntu CPU] error, I could not reproduce the problem with latest master & my PR, with stock TF\r\n(TF_ENABLE_ONEDNN_OPTS=1, or =0) or Intel TF (--config=mkl). The post logs did not tell much detail. I would appreciate if you can provide exact \"bazel test\" command. Thanks!", "@gzmkl Thank you for the coding style fixes. I've triggered the test rerun. Let's see if Ubuntu CPU still fails. It might be some infra error."]}, {"number": 54375, "title": "tflite with CoreML delegate errors with mediapipe models", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: IPAD pro M1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.8.0 C api\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): 4.1\r\n- GCC/Compiler version (if compiling from source): apple clang\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior**\r\nI've built a tflite 2.8.0 with coreML delegate to run mediapipe models on ipad M1,\r\nfor the build i edit the BUILD.apple according to one of the issues here, changing the coreml dependencies to the c api build:\r\n\r\n`# bazel build -c opt --config=ios_fat //tensorflow/lite/ios:TensorFlowLiteC_framework\r\n\r\ntflite_ios_framework(\r\n    name = \"TensorFlowLiteC_framework\",\r\n    hdrs = [\r\n        \":builtin_ops.h\",\r\n        \":c_api.h\",\r\n        \":c_api_experimental.h\",\r\n        \":common.h\",\r\n        \":coreml_delegate.h\",\r\n        \":xnnpack_delegate.h\",\r\n        \"//tensorflow/lite/c:c_api_types.h\",\r\n    ],\r\n    allowlist_symbols_file = \":allowlist_TensorFlowLiteC.txt\",\r\n    bundle_name = \"TensorFlowLiteC\",\r\n    minimum_os_version = TFL_MINIMUM_OS_VERSION,\r\n    deps = [\r\n        \":tensorflow_lite_c\",\r\n        \"//tensorflow/lite/delegates/coreml:coreml_delegate\",\r\n    ],\r\n)`\r\n\r\nit faster than the XNNPACK delegate but i get the following errors:\r\n\r\n`2022-02-14 18:13:53.989664+0200 Blackbox[17707:4340397] [espresso] [Espresso::ANERuntimeEngine::__forward_segment 0] evaluate[RealTime]WithModel returned 0; code=5 err=Error Domain=com.apple.appleneuralengine Code=5 \"processRequest:model:qos:qIndex:modelStringID:options:error:: 0x2: Program Inference overflow\" UserInfo={NSLocalizedDescription=processRequest:model:qos:qIndex:modelStringID:options:error:: 0x2: Program Inference overflow}\r\n\r\n2022-02-14 18:13:53.989716+0200 Blackbox[17707:4340397] [espresso] [Espresso::overflow_error] /private/var/mobile/Containers/Data/Application/D959913C-E34A-44C9-8706-3FD4944AF1ED/tmp/B14B4C52-2C7C-4C96-A4CC-DECBF54971E5-17707-0000111529177368.mlmodelc/model.espresso.net:0`\r\n\r\nIm using Xcode 13.1 \r\nwith XNNPACK everything is working perfect\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\ntflite code:\r\n`TfLiteModel* m_model{ nullptr };\r\nTfLiteInterpreter* m_interpreter{ nullptr };\r\nTfLiteDelegate* m_delegateXNNPACK{ nullptr };\r\nTfLiteDelegate* m_delegateCoreML{ nullptr };\r\n\r\nm_model = TfLiteModelCreateFromFile(modelFile.c_str());\r\nif (m_model == nullptr)\r\n\treturn false;\r\n\r\nTfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\nTfLiteCoreMlDelegateOptions optionsCoreML;\r\noptionsCoreML.enabled_devices = TfLiteCoreMlDelegateAllDevices;\r\nm_delegateCoreML = TfLiteCoreMlDelegateCreate(&optionsCoreML);\r\nTfLiteInterpreterOptionsAddDelegate(options, m_delegateCoreML);\r\n\r\nif(!m_delegateCoreML)\r\n{\r\n\tTfLiteXNNPackDelegateOptions optionsXNNPACK = TfLiteXNNPackDelegateOptionsDefault();\r\n\tm_delegateXNNPACK = TfLiteXNNPackDelegateCreate(&optionsXNNPACK);\r\n\tTfLiteInterpreterOptionsAddDelegate(options, m_delegateXNNPACK);\r\n}\r\n\r\nm_interpreter = (TfLiteInterpreter*)TfLiteInterpreterCreate(m_model, options);\r\nif (m_interpreter == nullptr)\r\n\treturn false;\r\n\r\n// Allocate tensor buffers\r\nif (TfLiteInterpreterAllocateTensors(m_interpreter) != kTfLiteOk)\r\n\treturn false;\r\n\r\nif (TfLiteInterpreterInvoke(m_interpreter) != kTfLiteOk)\r\n\treturn false;\t`\r\n\r\nmodels:\r\nhttps://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_detection/pose_detection.tflite\r\nhttps://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_lite.tflite\r\n\r\ninit the model and interpreter yeilds:\r\n`2022-02-14 19:51:21.711179+0200 Blackbox[17727:4362469] coreml_version must be 2 or 3. Setting to 3.\r\n2022-02-14 19:51:21.722082+0200 Blackbox[17727:4362469] Initialized TensorFlow Lite runtime.\r\nINFO: Initialized TensorFlow Lite runtime.\r\n2022-02-14 19:51:21.727229+0200 Blackbox[17727:4362469] CoreML delegate: 37 nodes delegated out of 291 nodes, with 33 partitions.\r\nINFO: CoreML delegate: 37 nodes delegated out of 291 nodes, with 33 partitions.\r\n2022-02-14 19:52:12.292307+0200 Blackbox[17727:4362469] coreml_version must be 2 or 3. Setting to 3.\r\n2022-02-14 19:52:12.293704+0200 Blackbox[17727:4362469] CoreML delegate: 112 nodes delegated out of 283 nodes, with 6 partitions.\r\nINFO: CoreML delegate: 112 nodes delegated out of 283 nodes, with 6 partitions.`\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 54374, "title": "[TF-TRT] Add parameter to the rewriter config to disable the optimization of the function library.", "body": "This is a simpler replacement for #53114. What we want for now is to avoid running optimizations on the segments after segmentation by TF-TRT. This is equivalent to disabling optimization of the function library.\r\n\r\nThere is a setting for that in `GrapplerItem::OptimizationOptions`: `optimize_function_library`, but I'm not sure if there's any proper way to control that from the Python configuration, so I create an option in the rewriter config to override it. If there's a better way of doing that, please let me know and I'll be happy to change this PR.", "comments": ["@bixia1 Can you please review this PR ? Thank you!"]}, {"number": 54373, "title": "[TF:TRT] Add additional conv2d test case", "body": null, "comments": ["Would you please provide a description of the PR?", "@christopherbate Can you please check @bixia1's comments and keep us posted ? Thanks!", "@christopherbate  Any update on this PR? Please. Thank you!"]}, {"number": 54357, "title": "Add complex support for `tf.sparse.segment_sum`", "body": "This PR address the issue raised in #53655 where tf.sparse.segment_sum does not have complex support (while tf.math.segment_sum has). This PR adds complex support for tf.sparse.segment_sum.\r\n\r\nThis PR fixes #53655.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Here are the internal errors, @yongtang can you please verify ?  Thank you!\r\n\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/python/client/session.py\", line 1377, in _do_call\r\n    return fn(*args)\r\n  File \"/tensorflow/python/client/session.py\", line 1359, in _run_fn\r\n    self._extend_graph()\r\n  File \"/tensorflow/python/client/session.py\", line 1400, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ngoogle3.third_party.tensorflow.python.framework.errors_impl.UnknownError: <unknown>:0: error: loc(fused[\"SparseSegmentSum:\", \"SparseSegmentSum_16\"]): 'tf.SparseSegmentSum' op operand #0 must be tensor of integer or floating-point values, but got 'tensor<400x1xcomplex<f32>>'\r\n<unknown>:0: note: loc(fused[\"SparseSegmentSum:\", \"SparseSegmentSum_16\"]): see current operation: %108 = \"tf.SparseSegmentSum\"(%89#0, %90#0, %32#0) {device = \"\"} : (tensor<400x1xcomplex<f32>>, tensor<210xi32>, tensor<210xi32>) -> tensor<?x1xcomplex<f32>>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/python/kernel_tests/math_ops/segment_reduction_ops_test.py\", line 612, in testValues\r\n    tf_ans = self.evaluate(s)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 2643, in evaluate\r\n    return sess.run(tensors)\r\n  File \"/tensorflow/python/framework/test_util.py\", line 2051, in run\r\n    return super(ErrorLoggingSession, self).run(*args, **kwargs)\r\n  File \"/tensorflow/python/client/session.py\", line 968, in run\r\n    run_metadata_ptr)\r\n  File \"/tensorflow/python/client/session.py\", line 1191, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/tensorflow/python/client/session.py\", line 1371, in _do_run\r\n    run_metadata)\r\n  File \"/tensorflow/python/client/session.py\", line 1396, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ngoogle3.third_party.tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:\r\n\r\n<unknown>:0: error: loc(fused[\"SparseSegmentSum:\", \"SparseSegmentSum_16\"]): 'tf.SparseSegmentSum' op operand #0 must be tensor of integer or floating-point values, but got 'tensor<400x1xcomplex<f32>>'\r\n<unknown>:0: note: loc(fused[\"SparseSegmentSum:\", \"SparseSegmentSum_16\"]): see current operation: %108 = \"tf.SparseSegmentSum\"(%89#0, %90#0, %32#0) {device = \"\"} : (tensor<400x1xcomplex<f32>>, tensor<210xi32>, tensor<210xi32>) -> tensor<?x1xcomplex<f32>>", "@mihaimaruseac  @gbaned  I have updated the PR with additional updates. Since the PR expands the ops' of SparseSegmentSum to take additional complex input, the `ops.pbtxt` may need to be updated. This is now covered with the latest change in the PR (after running `bazel-bin/tensorflow/core/ops/compat/update_ops tensorflow/core/ops/`)\r\n\r\nIn addition, the `tf_generated_ops.td` from MLIR also has a reference. I am not sure if this needs manual update or the update can be done automatically.\r\n\r\nCan you give the internal test a try? If everything pass then the `tf_generated_ops.td` is automatically generated and this PR does not needs additional changes anymore.\r\n", "Hi @yongtang Can you please check @mihaimaruseac's comments and keep us posted ? Thank you!", "@yongtang  Any update on this PR? Please. Thank you!"]}, {"number": 54354, "title": "Enable XNNPACK in tflite builds for armhf", "body": "Reverting b4ffa4cfcc50e7fcb8186b5cedbd0457acbe8ffb To enable XNNPACK for python tflite on Raspberry Pi etc.\r\n\r\n@terryheo - given you made the change originally (and I'm not sure why) you're best to comment if this is a bad idea.", "comments": ["Oh, I don't have a Google Account, so I can't sign the CLA. @terryheo - happy for you to close this and make the changes yourself (if you agree with it), as it's trivial.", "AFAIK, the build is broken with XNNPACK. Did you test it on master?", "I was hoping there'd be some CI tests here for it, but they seem to be failing for other reasons. But I'm building it in CI (hackily) as per \r\n\r\n```sh\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout 86e7c98adae7c8ebad616cac54b2f79b355dac3a\r\n# Hack to enable xnnpack which got disabled https://github.com/tensorflow/tensorflow/commit/b4ffa4cfcc50e7fcb8186b5cedbd0457acbe8ffb\r\nsed -i 's/XNNPACK=OFF/XNNPACK=ON/g' ./tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh\r\n# Try reduce memory ... TODO: just set the variable somewhere\r\nsed -i 's/${BUILD_NUM_JOBS}/1/g' ./tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh\r\ncat ./tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh\r\n./tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh armhf\r\n```\r\nand it succeeds\r\n\r\n![image](https://user-images.githubusercontent.com/8204904/153699164-41e04a6a-b08a-499b-b417-fad90f2293a0.png)\r\n\r\nAre there some existing TF tests that we can trigger to validate this?", "AArch32 iOS of XNNPACK used to be broken, fixed in google/XNNPACK@88d06fc82ba0b4c368f76fd049f4888c1706816a", "@kodonnell  Can you please sign CLA. Thanks!", "@gbaned - sorry, I don't have a Google Account, so I can't sign the CLA AFAIK. Happy for you to close this and make the changes yourself (if you agree with it), as it's trivial."]}, {"number": 54352, "title": "Update tensorflow:grpc++", "body": " Updated targets to //tensorflow:grpc++ from @com_github_grpc_grpc//:grpc++\r\nCloses #51770 ", "comments": ["Hi @ishark @yuefengz , could you please have a look at this PR? Thanks!"]}, {"number": 54348, "title": "TFTRT and Ragged operations", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18 (using docker image from nvidia: [nvcr.io/nvidia/tensorflow:21.12-tf2-py3](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow)\r\n- TensorFlow installed from (source or binary): 2.6.2, but the problem persists on older versions too (all versions in 2021). Newer versions are not yet stable for tensorrt. \r\n- TensorFlow version (use command below): via docker image. \r\n- Python version: Python 3.8.10\r\n- CUDA/cuDNN version: CUDA 11.5.0\r\n- GPU model and memory: Tesla T4 16 GB, but also on GTX 1080 TI. \r\n\r\n**Describe the current behaviour**\r\nWhen trying to optimize the model with ragged operations via TFTRT, optimization fails with a following error:\r\n```\r\n2022-02-09 14:56:57.575556: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at segment_reduction_ops_impl.h:422 : Invalid argument: data.shape = [48,256] does not start with segment_ids.shape = [168]\r\nTraceback (most recent call last):\r\n  File \"optimize.py\", line 41, in <module>\r\n    converter.build(input_fn=my_input_fn)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1320, in build\r\n    self._converted_func(*map(ops.convert_to_tensor, first_input))\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1707, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/wrap_function.py\", line 246, in _call_impl\r\n    return super(WrappedFunction, self)._call_impl(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1725, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1774, in _call_with_flat_signature\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 1963, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 591, in call\r\n    outputs = execute.execute(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  data.shape = [48,256] does not start with segment_ids.shape = [168]\r\n\t [[node StatefulPartitionedCall/ragged_net/lambda/RaggedReduceMax/RaggedReduce_1/RaggedReduce_1/UnsortedSegmentMax (defined at optimize.py:25) ]] [Op:__inference_pruned_2056]\r\n```\r\n After some investigation, I have found out that the problem is happening in ``row_splits_to_segment_ids``, invoked within RaggedReduceMax( actually under the hood it is invoked in ``_ragged_segment_aggregate()``,  invoked by ``ragged_reduce_aggregate`` which is invoked by ``reduce_max`` in ``tensorflow.python.ops.ragged.ragged_math_ops``). The problem is with correctly calculating ``row_lenghts`` which in most cases are done correctly (while training and first sweeps of optimization, but in the second part it produces wrong results). The current version is: \r\n```python\r\n    row_lengths = splits[1:] - splits[:-1]\r\n```\r\nThe proposed version is:\r\n```python\r\n    s = splits[::-1][1:][::-1]\r\n    row_lengths = splits[1:] - s\r\n```\r\nWhich essentially does the very same thing, but at least it does not result in the presented crash. On the other hand, it produces a soft crash (non-breaking one but indicating that no significant optimization was run):\r\n```\r\n2022-01-25 13:24:56.322210: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:40] DefaultLogger 3: [executionContext.cpp::enqueueInternal::328] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/executionContext.cpp::enqueueInternal::328, condition: bindings[x] != nullptr\r\n```\r\n\r\n**Describe the expected behaviour**\r\nI would like to get a model which is TFTRT optimized with smaller ``min_segment_size``.  My current workaround is to use a larger value for the parameter ``min_segment_size`` but this gives me a suboptimal solution in terms of performance.\r\n  \r\n**Standalone code to reproduce the issue and logs**\r\nGist with required code and logs: [https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24). \r\nExplanation of how to use gist:\r\n- to train a model **without custom changes** in ``row_splits_to_segment_ids()``: use [train_round1.py](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-train_round1-py)\r\n- to train a model **with custom changes** in ``row_splits_to_segment_ids()``: use [train_round2.py](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-train_round2-py)\r\n- to optimize a trained model: use [optimize.py](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-optimize-py)\r\n- my logs for optimizing model trained with no custom changes: [log_after_optimize_for_round_1.txt](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-log_after_optimize_for_round_1-txt)\r\n- my logs for optimizing model trained with custom changes: [log_after_optmize_for_round_2.txt](https://gist.github.com/rpytel1/6f6d57f68a5e3d69922cd1ec43eb6a24#file-log_after_optmize_for_round_2-txt)\r\n\r\nDue to a nightmare with configuring TensorRT in Google Colab, I provide only gists. \r\n", "comments": ["@chunduriv ,\r\nI have tried in colab with TF version 2.7 and nightly version and noticed that session is being crashed. Please, find the gist [here](https://colab.research.google.com/gist/tilakrayal/522ec71abfb70331e9d30d48887bd5b1/untitled221.ipynb). Thanks!", "> @chunduriv , I have tried in colab with TF version 2.7 and nightly version and noticed that session is being crashed. Please, find the gist [here](https://colab.research.google.com/gist/tilakrayal/522ec71abfb70331e9d30d48887bd5b1/untitled221.ipynb). Thanks!\r\n\r\n@tilakrayal this is due to the fact that ``tf-2.7`` and Colab are not compiled with Cudnn (CUDA). For TFTRT you need CUDA. If you want to check if the problem is solved in TF2.7 I would recommend using [docker image from Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow/tags) with tag 22.01-tf2-py3, but from what I checked it is not yet solved.\r\nBut as I said for this you need to check on a real machine, not a Colab. "]}, {"number": 54347, "title": "Specify what axes are independent in `batch_jacobian`, instead of it being always the first axis only. ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\nHi everyone,\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, `GradientTape`'s `batch_jacobian` method assumes that `target[i,...]` is independent of `source[j,...]` for `j != i`.\r\nSee: https://www.tensorflow.org/api_docs/python/tf/GradientTape#batch_jacobian\r\n\r\nAlso from the docs: The function is logically equivalent to `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.\r\nThe first dimension gets a special role, it \"contains\" the independent tensors whose Jacobian should be calculated.\r\n\r\nHowever, the shape of independent tensors doesn't have to be of rank one, it may be multidimensional.\r\nIt may be nice to be able to specify where the line passes, between the independent tensors' shape and tensors whose jacobian we want to find.\r\n\r\nThis can be done today using `reshape` before and after the `batch_jacobian` call: merging the independent dimensions into the first dimension, applying `batch_jacobian`, and then reshaping back.\r\nThis is not so nice though :(\r\n\r\nHere is an example of both a use case and the reshaping trick: https://colab.research.google.com/drive/1oXsmJl9GuQihJsxq0t3PusQ_-xBSiAyk\r\n\r\n**Will this change the current api? How?**\r\n\r\nMaybe the function can have an `axis` parameter to specify what dimensions to calculate the jacobian over, while the other dimensions will have the same \"special\" role the first dimension of `target` has now.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nOne use case may be calculating the batch_jacobian over the result of `meshgrid`, as I did to calculate the normals of a parametric surface (See colab notebook link above).\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 54346, "title": "\"iterating over `tf.Tensor` is not allowed\" when training object detection model with pyinstaller", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2016\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2/8\r\n- GPU model and memory: Nvidia GTX 1080 Ti\r\n\r\nI am training a detection model with a Python script using the object_detection module (with TF2). The script works perfectly fine when executed with Python (python TrainSuspendedElementsDetection.py ...), apart from a load of deprecation warnings. Problems arise when I try to convert the Python script to an executable with pyinstaller (last version 4.9). When launching the executable an exception occurs at the beginning of the training loop:\r\n\r\nTraceback (most recent call last):\r\n  File \"TrainSuspendedElementsDetection.py\", line 256, in <module>\r\n  File \"tensorflow\\python\\platform\\app.py\", line 40, in run\r\n  File \"absl\\app.py\", line 303, in run\r\n  File \"absl\\app.py\", line 251, in _run_main\r\n  File \"TrainSuspendedElementsDetection.py\", line 181, in main\r\n  File \"object_detection\\model_lib_v2.py\", line 678, in train_loop\r\n  File \"tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\r\n  File \"tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\r\n  File \"tensorflow\\python\\eager\\def_function.py\", line 910, in __call__\r\n  File \"tensorflow\\python\\eager\\def_function.py\", line 958, in _call\r\n  File \"tensorflow\\python\\eager\\def_function.py\", line 780, in _initialize\r\n  File \"tensorflow\\python\\eager\\function.py\", line 3157, in _get_concrete_function_internal_garbage_collected\r\n  File \"tensorflow\\python\\eager\\function.py\", line 3557, in _maybe_define_function\r\n  File \"tensorflow\\python\\eager\\function.py\", line 3392, in _create_graph_function\r\n  File \"tensorflow\\python\\framework\\func_graph.py\", line 1143, in func_graph_from_py_func\r\n  File \"tensorflow\\python\\eager\\def_function.py\", line 672, in wrapped_fn\r\n  File \"tensorflow\\python\\framework\\func_graph.py\", line 1118, in autograph_handler\r\n  File \"tensorflow\\python\\autograph\\impl\\api.py\", line 440, in converted_call\r\n  File \"tensorflow\\python\\autograph\\impl\\api.py\", line 490, in _fall_back_unconverted\r\n  File \"tensorflow\\python\\autograph\\impl\\api.py\", line 464, in _call_unconverted\r\n  File \"object_detection\\model_lib_v2.py\", line 659, in _dist_train_step\r\n  File \"tensorflow\\python\\framework\\ops.py\", line 572, in __iter__\r\n  File \"tensorflow\\python\\framework\\ops.py\", line 561, in _disallow_iteration\r\n  File \"tensorflow\\python\\framework\\ops.py\", line 525, in _disallow_when_autograph_unavailable\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph is unavailable in this runtime. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information. \r\n\r\nAttached are:\r\n- TrainSuspendedElementsDetection.py: The source code of the Python script (the problem occurs at line 181 when calling the training loop of the object_detection module)\r\n- pipeline.config: The configuration file of the training process\r\n- TrainSuspendedElementsDetection.spec: the spec file used by pyinstaller\r\n- log.txt: the log file of the pyinstaller output. I saw numerous errors relative to modules not found, but it does not seem to have any impact in the execution of the progra\r\n[pyinstaller.zip](https://github.com/tensorflow/tensorflow/files/8047968/pyinstaller.zip)\r\nm", "comments": ["Update: I found some workarounds.\r\n\r\n1) Decorate the \"_dist_train_step\" function in model_lib_v2.py in the object_detection module with \"@tf.autograph.experimental.do_not_convert\". It will apparently avoid to convert the function into a TF graph, but doing this is not satisfying since it makes the training about 10x slower.\r\n2) In the train_loop function of the object_detection module, set the parameter num_steps_per_iteration to 1 (instead of 100), which prevents the dist_train_step function from using a \"for\" loop, which apparently was the cause of the previous failure. However, I do not know how this parameter will affect the efficiency of the training. If someone could enhance my knowledge with the solution to this mystery is would be greatly appreciated :-)", "@Rayndell \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Here is a minimal example (models\\research should be added to the PYTHON_PATH environment variable). It works by giving to the pipeline_config_path parameter the config file added in the zip file in the message above, but it should work with any config file from TF2 samples.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom object_detection import model_lib_v2\r\n\r\ntf.config.set_soft_device_placement(True)\r\nstrategy = tf.compat.v2.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n\tmodel_lib_v2.train_loop(\r\n\t\t\tpipeline_config_path=pipeline_config_path,\r\n\t\t\tmodel_dir=\"./Train\",\r\n\t\t\ttrain_steps=100000,\r\n\t\t\tuse_tpu=False,\r\n\t\t\tcheckpoint_every_n=1000,\r\n\t\t\tcheckpoint_max_to_keep=5,\r\n\t\t\trecord_summaries=False,\r\n\t\t\tnum_steps_per_iteration=100)\r\n```\r\n\r\nNote that this code works perfectly fine when executing it within the Pyton environment. Problems arise when converting the program to an executable with pyinstaller. As noted above, it works well when putting num_steps_per_iteration to the value 1, since it skips the \"for\" loop in the dist_train_step function."]}, {"number": 54342, "title": "Random test failures in tflite/xnnpack due to precision error", "body": "**System information**\r\n- OS Platform and Distribution: WSL2 `tensorflow/tensorflow:devel-gpu`\r\n- TensorFlow installed from source\r\n- TensorFlow version: master\r\n\r\nTests `tensorflow/lite/delegates/xnnpack/...` can randomly fail due to precision error.\r\n\r\nThis issue caused my PR to be blocked from merging: https://github.com/tensorflow/tensorflow/pull/53864\r\n\r\nThe reason why it can fail is because the test data is generated randomly with seeding directly from `std::random_device`. The seeds are not logged when tests fail, so it is hard to reproduce a failure when it happens.\r\n\r\nAfter the tests failed during pre-merge checks in my PR, I tried to reproduce it in my local environment inside `tensorflow/tensorflow:devel-gpu` docker. But on my machine the tests always passed.\r\n\r\nHowever, I did find a way to get a consistent failure on `tensorflow/lite/delegates/xnnpack/softmax_test` (a different one from what failed in my PR). When compiled in `tensorflow/tensorflow:devel-gpu` with `copt` flags set to `-march=haswell -mtune=haswell -ffast-math` it always fails.\r\n\r\nI think the tests should be improved to avoid random failures in the future:\r\n\r\n- make the tests deterministic\r\n- use gtest's `random_seed` for seeding\r\n- log seeds when tests fail\r\n- reconsider precision threshold", "comments": ["@itmo153277 ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and tensorflow version to reproduce the issue reported here.", "@tilakrayal \r\nI was not able to reproduce the same failure that I got in my PR. If you have access to the test logs from that pre-merge build, you have better luck reproducing it.\r\n\r\nHow to reproduce `softmax_test` failure:\r\n\r\n1. Start shell in `tensorflow/tensorflow:devel-gpu` docker\r\n2. Clone `master` branch\r\n3. Set `--config=opt` to `-march=haswell -mtune=haswell -ffast-math`\r\n4. Run tests: `bazel test --config=opt -k tensorflow/lite/delegates/xnnpack/...`\r\n\r\nExample session: [tf-test-fail.log](https://github.com/tensorflow/tensorflow/files/8060086/tf-test-fail.log)\r\n ", "@itmo153277 , I see that your PR is manually merged, is this issue good to close?. Thanks!", "I believe the issue itself has not been resolved, If the tensorflow team has decided that the issue is not worth fixing, feel free to close it.", "I checked this PR internally and this got merged without any error, you can close the issue. Thanks!", "Sorry, I don't understand what you mean. If the PR was merged without errors it only confirms the issue, doesn't it?\r\n\r\nI opened this issue not because my PR was blocked from merging but because I believe the tests should be improved."]}]