[{"number": 31915, "title": "tf logging error ", "body": "**System information**\r\n```\r\n== check python ===================================================\r\npython version: 3.6.0\r\npython branch: \r\npython build version: ('default', 'Dec 23 2016 12:22:00')\r\npython compiler version: GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Wed Apr 12 15:04:24 UTC 2017\r\nos release version: 3.10.0-514.16.1.el7.x86_64\r\nos platform: Linux-3.10.0-514.16.1.el7.x86_64-x86_64-with-centos-7.3.1611-Core\r\nlinux distribution: ('CentOS Linux', '7.3.1611', 'Core')\r\nlinux os distribution: ('centos', '7.3.1611', 'Core')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='dfdd64ab2304', release='3.10.0-514.16.1.el7.x86_64', version='#1 SMP Wed Apr 12 15:04:24 UTC 2017', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                         1.16.4   \r\nprotobuf                      3.9.0    \r\ntensorflow-estimator          1.14.0   \r\ntensorflow-gpu                1.14.0   \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 1.14.0\r\ntf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5\r\ntf.version.COMPILER_VERSION = 4.8.5\r\nSanity check: array([1], dtype=int32)\r\n     21634:     find library=libpython3.6m.so.1.0 [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib              (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls/x86_64/libpython3.6m.so.1.0\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/tls/libpython3.6m.so.1.0\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/x86_64/libpython3.6m.so.1.0\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libpython3.6m.so.1.0\r\n     21634:\r\n     21634:     find library=libpthread.so.0 [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libpthread.so.0\r\n     21634:      search path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/tls/x86_64:/usr/local/cuda/lib64/tls:/usr/local/cuda/lib64/x86_64:/usr/local/cuda/lib64:tls/x86_64:tls:x86_64:           (LD_LIBRARY_PATH)\r\n     21634:       trying file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0\r\n     21634:       trying file=/usr/local/nvidia/lib/tls/libpthread.so.0\r\n     21634:       trying file=/usr/local/nvidia/lib/x86_64/libpthread.so.0\r\n     21634:       trying file=/usr/local/nvidia/lib/libpthread.so.0\r\n/../../x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../..          (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../tls/x86_64/liblzma.so.5\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../tls/liblzma.so.5\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../x86_64/liblzma.so.5\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../liblzma.so.5\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../liblzma.so.5\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/.local/lib/python3.6/site-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/.local/lib/python3.6/site-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:     find library=libssl.so.1.0.0 [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libssl.so.1.0.0\r\n     21634:\r\n     21634:     find library=libcrypto.so.1.0.0 [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libcrypto.so.1.0.0\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libcrypto.so.1.0.0\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/../../libssl.so.1.0.0\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_blake2.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_sha3.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_bisect.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/lib-dynload/_random.cpython-36m-x86_64-linux-gnu.so\r\n     21634:\r\n     21634:     find library=libtensorflow_framework.so.1 [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls/x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../x86_64:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..             (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls/x86_64/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/tls/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/x86_64/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls/x86_64/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../tls/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../x86_64/libtensorflow_framework.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.1\r\n     21634:\r\n     21634:     find library=libstdc++.so.6 [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libstdc++.so.6\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libstdc++.so.6\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libstdc++.so.6\r\n     21634:      search path=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:tls/x86_64:tls:x86_64:         (LD_LIBRARY_PATH)\r\n     21634:       trying file=/usr/local/nvidia/lib/libstdc++.so.6\r\n     21634:       trying file=/usr/local/nvidia/lib64/libstdc++.so.6\r\n     21634:       trying file=/usr/local/cuda/lib64/libstdc++.so.6\r\n     21634:       trying file=tls/x86_64/libstdc++.so.6\r\n     21634:       trying file=tls/libstdc++.so.6\r\n     21634:       trying file=x86_64/libstdc++.so.6\r\n     21634:       trying file=libstdc++.so.6\r\n     21634:      search cache=/etc/ld.so.cache\r\n     21634:       trying file=/lib64/libstdc++.so.6\r\n     21634:\r\n     21634:     find library=libgcc_s.so.1 [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libgcc_s.so.1\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libgcc_s.so.1\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libgcc_s.so.1\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libgcc_s.so.1\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /lib64/libstdc++.so.6\r\n     21634:\r\n     21634:\r\n     21634:     calling init: /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so.1\r\n     21634:\r\n     21634:     find library=libhdfs.so [0]; searching\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..          (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libhdfs.so\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python:/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/..           (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/libhdfs.so\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/../libhdfs.so\r\n     21634:      search path=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib                (RPATH from file /home/luban/zhanghui/env/py3.6_tf1.14pip/bin/python3)\r\n     21634:       trying file=/home/luban/zhanghui/env/py3.6_tf1.14pip/bin/../lib/libhdfs.so\r\n     21634:      search path=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:tls/x86_64:tls:x86_64:         (LD_LIBRARY_PATH)\r\n     21634:       trying file=/usr/local/nvidia/lib/libhdfs.so\r\n     21634:       trying file=/usr/local/nvidia/lib64/libhdfs.so\r\n     21634:       trying file=/usr/local/cuda/lib64/libhdfs.so\r\n     21634:       trying file=tls/x86_64/libhdfs.so\r\n     21634:       trying file=tls/libhdfs.so\r\n     21634:       trying file=x86_64/libhdfs.so\r\n     21634:       trying file=libhdfs.so\r\n     21634:      search cache=/etc/ld.so.cache\r\n     21634:      search path=/lib64/tls/x86_64:/lib64/tls:/lib64/x86_64:/lib64:/usr/lib64/tls/x86_64:/usr/lib64/tls:/usr/lib64/x86_64:/usr/lib64                (system search path)\r\n     21634:       trying file=/lib64/tls/x86_64/libhdfs.so\r\n     21634:       trying file=/lib64/tls/libhdfs.so\r\n     21634:       trying file=/lib64/x86_64/libhdfs.so\r\n     21634:     calling fini: /lib64/libutil.so.1 [0]\r\n     21634:\r\n     21634:\r\n     21634:     calling fini: /lib64/libdl.so.2 [0]\r\n     21634:\r\n     21634:\r\n     21634:     calling fini: /lib64/libpthread.so.0 [0]\r\n     21634:\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Aug 23 12:51:21 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P40           Off  | 00000000:02:00.0 Off |                  N/A |\r\n| N/A   23C    P0    53W / 250W |     10MiB / 22919MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla P40           Off  | 00000000:03:00.0 Off |                  N/A |\r\n| N/A   20C    P8    10W / 250W |     10MiB / 22919MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla P40           Off  | 00000000:83:00.0 Off |                  N/A |\r\n| N/A   46C    P0    73W / 250W |  22041MiB / 22919MiB |     34%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla P40           Off  | 00000000:84:00.0 Off |                  N/A |\r\n| N/A   44C    P0   114W / 250W |  21935MiB / 22919MiB |     97%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-10.0/lib64/libcudart_static.a\r\n/usr/local/cuda-10.0/lib64/libcudart.so.10.0.130\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 0, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\nBuild label: 0.16.1\r\nBuild time: Mon Aug 13 13:43:36 2018 (1534167816)\r\nBuild timestamp: 1534167816\r\nBuild timestamp as int: 1534167816\r\n```\r\n\r\n**Describe the current behavior**\r\nusing `evaluate_generateor` do evaluation, tf logging has error on flush info to stream.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n**Other info / logs**\r\n```\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 166, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense1.layer.kernel\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 166, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).dense1.layer.bias\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 166, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).lstm1.forward_layer.cell.kernel\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 166, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).lstm1.forward_layer.cell.recurrent_kernel\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.forward_layer.cell.recurrent_kernel\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 166, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.forward_layer.cell.bias\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 166, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.backward_layer.cell.kernel\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\nCall stack:\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py\", line 244, in __del__\r\n    .format(pretty_printer.node_names[node_id]))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/platform/tf_logging.py\", line 166, in warning\r\n    get_logger().warning(msg, *args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1313, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1437, in _log\r\n    self.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1447, in handle\r\n    self.callHandlers(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 1509, in callHandlers\r\n    hdlr.handle(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 891, in handle\r\n    return self._current_handler.handle(record)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 858, in handle\r\n    self.emit(record)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/logging/__init__.py\", line 829, in emit\r\n    super(PythonHandler, self).emit(record)\r\nMessage: \"Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).lstm1.backward_layer.cell.recurrent_kernel\"\r\nArguments: ()\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/logging/__init__.py\", line 989, in emit\r\n    stream.write(msg)\r\nValueError: I/O operation on closed file.\r\n```\r\n", "comments": ["@zh794390558 Can you provide a standalone code to reproduce the issue? Add any more details to resolve the issue faster. Thanks!", "replace https://github.com/didi/delta/blob/master/egs/iemocap/emo/v1/run.sh#L50 `--cmd train_and_eval` to `--cmd eval` will reproduce this bug.", "@zh794390558 Can you provide a simple standalone code reproduce the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31915\">No</a>\n"]}, {"number": 31914, "title": "lite: enable (u)int8 quantization and int32 for ABS", "body": "Hi, this PR adds uint8/int8/int32 support for ABS kernel in TFLite. Detail change includes:\r\n1. Move ABS out routine and kernel from elementwise generic as ops have different quantization handling.\r\n2. Float/Int32 ABS invoke `std::abs` directly.\r\n3. Int8/UInt8 ABS involving requantization after `std::abs`.\r\n4. Float as version 1, Int32/Int8/UInt8 as version 2.\r\n5. Test for kernel and TOCO.", "comments": ["One open discussion is that, do we really need requantization in quantized ABS.\r\n\r\nIntuitively, that is not needed because the representation of FP32 in INT8 won't change, which is the OP's property. However, we can safely remove the requantize **only** when TOCO (or any model builder) enforcing the same quantization representation for input and output tensors, which seems not the case currently. Our team has a model which gets different quantization representation of ABS input/output after converted to TFLite model with TOCO. So maybe, we can investigate whether should TOCO (and any similar quantization tools) enforce such check (or any needed logic) when converting models.\r\n\r\nWould you please help to CC people who may have interest or expertise in this topic (I mean where need to evaluate quantization representation handling in model conversion)? Looking forward to hear from you guys. Thank you.", "Hi there, any update needed?", "Two more commits to extend Quantization-aware Training to operator Sub, Abs, Squeeze, ExpandDims and so on; and remove requantize in Abs TFLite kernel as it should have same quantization representation.\r\n\r\nAlso loop @aselle and @suharshs who may have interest.\r\n", "I will spend some time to think more about the requantization case for abs, for now i think going to int32 like you are doing is fine. Thanks!", "> I will spend some time to think more about the requantization case for abs, for now i think going to int32 like you are doing is fine. Thanks!\r\n\r\nThank you for the review @suharshs !  For the Abs requantization, I was paying too much attention on the representation precision about input/output - for most case overlapping zero, the output range is smaller than the input range, but the data representation cannot be more accurate in output. I mean, the information of output cannot be more than input, so I thought a requantization is not needed. Leading to miss the range case `[-2, -1]` you pointed out, for which a requantization seems needed. Thank you for the patient review!\r\n\r\nThe min/max design choice also impacts `contrib/quantize`, where I took ABS as *pass through* operators - while should not. I think learn min/max for Abs rather than hardcode it is a good idea, just like what was done for Concat in this PR.\r\n\r\nAlso, out of the scope for this PR, I am thinking that the open sourced Quantization-aware Training may need redesign. Current design basically takes operators which holding weights as something like *master* node, and processes based on them. Many more operators, such as the already added `Add` and `Mul`, can be improved by QAT. I guess we can add FakeQuant to anywhere, unless it is not necessary - `Reshape` for example. We may let TensorFlow operators holding some attributions describing where the quantization representation of their inputs/outputs may be different, and the `contrib/quantize` can simply walk the network and insert FakeQuant nodes. That is what I am thinking, as someone who don't hold a full sight of the design. I will be very happy to contribute such things. Hoping it were not going too far wrong. :)\r\n\r\nI will update the PR next week to let Abs learn min/max. I am thinking that the `contrib/quantize` and tflite part code can be in one PR to let them have consistent behavior on quantization related things (int32 is really insignificant). What do you think?", "Removed `contrib/quantize` changes from this PR. ~~Will open another for it~~. Draft it in https://github.com/tensorflow/tensorflow/pull/32337. Please review this again @suharshs :)", "@jackwish Could you please check build failures and resolve the conflicts? Thanks!", "> @jackwish Could you please check build failures and resolve the conflicts? Thanks!\r\n\r\nHi @gbaned , conflicts has been fixed, thank you for the reminder.\r\nHi @suharshs and @wangtz , could you please have another look in this?", "@jackwish Could you please resolve the conflicts? Thanks!", "Hi @gbaned the conflicts have been fixed. The latest two rounds of conflict resolving are due to the long progress of this PR. Hoping that we can accelerate the review progress before new conflicts emerge :)", "@jackwish Sorry for the slow response. Could you please resolve the conflicts? Thanks!", "@jackwish Could you please check build failures and resolve the conflicts? Thanks!", "Hi @gbaned the conflicts have been fixed, and @suharshs @wangtz @jianlijianli please have another look.\r\n\r\nPS. As tree round of conflicts resolving has been conducted in this PR, I am now unsubscribing from this PR thread and leaving it as it is. Thank you guys for all the effort, best regards.", "@jackwish Could you please resolve the conflicts? Thanks!", "switch to #34263"]}, {"number": 31913, "title": "All_reduce of collective_ops hangs in a distributed environment", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.6.1810\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8951\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nThe monitored session hangs in there fetching the `reduced_weight`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe all_reduce tensor `reduced_weight` gives proper answer on all workers.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\"\"\"Illustrate AllReduce\"\"\"\r\n\r\nimport multiprocessing as mp\r\n\r\nMP_METHOD = 'fork'  # 'fork' (UNIX), 'spawn' (WINDOWS);\r\nNUM_PROCESSES = 2\r\n\r\ndef process_fn(worker_hosts, task_index):\r\n    \"\"\"allreduce process\"\"\"\r\n    import time\r\n    import tensorflow as tf\r\n    from tensorflow.python.ops import collective_ops\r\n\r\n    num_workers = len(worker_hosts)\r\n\r\n    cluster_spec = tf.train.ClusterSpec({'worker': worker_hosts})\r\n\r\n    server = tf.train.Server(cluster_spec,\r\n                             job_name='worker', task_index=task_index)\r\n    group_key = 0\r\n    instance_key = 0\r\n    with tf.Graph().as_default():\r\n        weights = list()\r\n        reduced_weight = list()\r\n        for worker_index in range(num_workers):\r\n            with tf.variable_scope('worker{}'.format(worker_index)), \\\r\n                    tf.device('job:worker/task:{}/device:CPU:0'.format(\r\n                            worker_index)):\r\n                weight = tf.get_variable('weight', shape=[])\r\n                weights.append(weight)\r\n                if worker_index == task_index:\r\n                    reduced_weight = collective_ops.all_reduce(\r\n                        weight, num_workers, group_key, instance_key,\r\n                        'Add', 'Div')\r\n\r\n        session_creator = tf.train.ChiefSessionCreator(master=server.target)\r\n        with tf.train.MonitoredSession(session_creator=session_creator) \\\r\n                as mon_sess:\r\n            print('task {} have {}'.format(task_index, mon_sess.run(weights)))\r\n            result = mon_sess.run(reduced_weight)\r\n        print('task {} reduce {}'.format(task_index, result))\r\n        time.sleep(1)\r\n\r\ndef start_process():\r\n    \"\"\"start process\"\"\"\r\n    port = 60000\r\n    host_fmt = 'localhost:{}'\r\n    worker_hosts = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        worker_hosts.append(host_fmt.format(port + process_index))\r\n    mp_ctx = mp.get_context(MP_METHOD)\r\n    processes = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        process = mp_ctx.Process(target=process_fn,\r\n                                 args=(worker_hosts, process_index,))\r\n        processes.append(process)\r\n        process.start()\r\n    for process in processes:\r\n        process.join()\r\n\r\nif __name__ == '__main__':\r\n    start_process()\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```console\r\n(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python ./tf_distribute_collective_ops.py\r\n2019-08-23 10:55:20.150797: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-23 10:55:20.152951: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-23 10:55:20.163464: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-08-23 10:55:20.163852: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4364100 executing computations on platform Host. Devices:\r\n2019-08-23 10:55:20.163883: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-23 10:55:20.165614: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-08-23 10:55:20.165828: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-08-23 10:55:20.166148: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4363fe0 executing computations on platform Host. Devices:\r\n2019-08-23 10:55:20.166174: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-23 10:55:20.166519: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60001\r\n2019-08-23 10:55:20.167632: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-08-23 10:55:20.168829: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60000\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-08-23 10:55:20.262106: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session c45b1693e334d401 with config: \r\n2019-08-23 10:55:20.269965: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session a7c551a16b557bd8 with config: \r\ntask 1 have [-0.82924074, -0.72853804]\r\ntask 0 have [-0.82924074, -0.72853804]\r\n```\r\nThe `collective_ops.all_reduce` seems to be referenced only once in [build_collective_reduce](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/distribute/cross_device_utils.py#L360-L362), where the instruction suggests \"input_tensors: tensors within a single worker graph that are to be reduced together; must be one per device.\" Is that mean the `all_reduce` is only applicable to [In-graph replication](https://github.com/tensorflow/docs/blob/r1.9/site/en/deploy/distributed.md#replicated-training)?", "comments": ["Reassigning this to @dubey since it involves `collective_ops`.", "Collective ops support both in-graph and between-graph communication.\r\n\r\nTo enable between-graph communication, each worker needs to know the [`collective_group_leader`](https://github.com/tensorflow/tensorflow/blob/87c3f32e3c9c187cc1a9dfd4667ce2e117be76a5/tensorflow/core/protobuf/config.proto#L464).  If left unconfigured, each worker assumes it is the leader and waits for a message from other workers in the group.\r\n\r\nThe fix is to add the following snippet:\r\n```\r\nfrom tensorflow.core.protobuf import config_pb2\r\nconfig = config_pb2.ConfigProto()\r\nconfig.experimental.collective_group_leader = '/job:worker/replica:0/task:0'\r\n```\r\nand pass this config to the `tf.train.Server` constructor.", "Thanks very much to all you guys! The solution fix the hanging. \ud83d\ude42", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31913\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31913\">No</a>\n", "Here comes a further question. Does this fix work concurrently for the both in-graph and between-graph communications? Two distributed workers are created with three towers on each, where the between-graph communication works fine. Nevertheless, the in-graph allreduce among the three towers on the same worker fails with a message\r\n```console\r\nCheck failed: cp->group.group_size == cp->instance.device_names.size() (3 vs. 2)0x7ff630142180\r\n```\r\nand `worker0` exits. It seems that the `group_key` doesn't have the same effect as an mpi communicator. I wonder is there some method to carry out the communications under the same `mon_sess`? The complete code and log are attached below.\r\n\r\nCode:\r\n```python\r\n\"\"\"Illustrate AllReduce\"\"\"\r\n\r\nimport multiprocessing as mp\r\n\r\nMP_METHOD = 'fork'  # 'fork' (UNIX), 'spawn' (WINDOWS);\r\nNUM_PROCESSES = 2\r\nNUM_TOWERS = 3\r\n\r\ndef process_fn(worker_hosts, task_index):\r\n    \"\"\"allreduce process\"\"\"\r\n\r\n    import time\r\n    import tensorflow as tf\r\n    from tensorflow.python.ops import collective_ops\r\n\r\n    cluster_spec = tf.train.ClusterSpec({'worker': worker_hosts})\r\n\r\n    # unconfigured collective_group_leader make each worker the leader\r\n    # '/replica:0' is necessary in the configuration.\r\n    config = tf.ConfigProto(device_count={'CPU': NUM_TOWERS})\r\n    config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'\r\n    server = tf.train.Server(cluster_spec, config=config,\r\n                             job_name='worker', task_index=task_index)\r\n    with tf.Graph().as_default():\r\n        # create weight\r\n        all_weights = list()\r\n        for worker_index in range(NUM_PROCESSES):\r\n            worker_weights = list()\r\n            with tf.variable_scope('worker{}'.format(worker_index)):\r\n                for tower_index in range(NUM_TOWERS):\r\n                    device = '/job:worker/replica:0/task:{}/device:CPU:{}'.format(\r\n                        worker_index, tower_index)\r\n                    with tf.device(device):\r\n                        worker_weights.append(tf.get_variable(\r\n                            'weight{}'.format(tower_index), shape=[]))\r\n            all_weights.append(worker_weights)\r\n\r\n        intra_reduced = list()\r\n        inter_reduced = None\r\n        with tf.variable_scope('worker{}'.format(task_index)):\r\n            # intra-worker allreduce\r\n            for weight in all_weights[task_index]:\r\n                with tf.device(weight.device):\r\n                    intra_reduced.append(collective_ops.all_reduce(\r\n                        weight, NUM_TOWERS, task_index, 0, 'Add', 'Div'))\r\n            # inter-worker allreduce\r\n            weight = all_weights[task_index][0]\r\n            with tf.device(weight.device):\r\n                inter_reduced = collective_ops.all_reduce(\r\n                    weight, NUM_PROCESSES, NUM_PROCESSES, 0, 'Add', 'Div')\r\n\r\n        if task_index == 0:\r\n            session_creator = tf.train.ChiefSessionCreator(\r\n                master=server.target)\r\n        else:\r\n            session_creator = tf.train.WorkerSessionCreator(\r\n                master=server.target)\r\n        with tf.train.MonitoredSession(session_creator=session_creator) \\\r\n                as mon_sess:\r\n            result_all = mon_sess.run(all_weights)\r\n            print('task {} sense {}'.format(task_index, result_all))\r\n            result_inter = mon_sess.run([inter_reduced])\r\n            print('task {} inter_reduce {}'.format(task_index, result_inter))\r\n            result_intra = mon_sess.run([intra_reduced])\r\n            print('task {} intra_reduce {}'.format(task_index, result_intra))\r\n            time.sleep(1)\r\n\r\ndef start_process():\r\n    \"\"\"start process\"\"\"\r\n    port = 60000\r\n    host_fmt = 'localhost:{}'\r\n    worker_hosts = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        worker_hosts.append(host_fmt.format(port + process_index))\r\n    mp_ctx = mp.get_context(MP_METHOD)\r\n    processes = list()\r\n    for process_index in range(NUM_PROCESSES):\r\n        process = mp_ctx.Process(target=process_fn,\r\n                                 args=(worker_hosts, process_index,))\r\n        processes.append(process)\r\n        process.start()\r\n    for process in processes:\r\n        process.join()\r\n\r\nif __name__ == '__main__':\r\n    start_process()\r\n```\r\n\r\nLog:\r\n```console\r\n(tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python ./tf_distribute_collective_ops_rev2.py \r\n2019-08-29 15:28:57.955533: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-29 15:28:57.957342: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-29 15:28:57.969872: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-08-29 15:28:57.970223: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3c8cb80 executing computations on platform Host. Devices:\r\n2019-08-29 15:28:57.970257: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-29 15:28:57.971219: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz\r\n2019-08-29 15:28:57.971524: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3c8ce20 executing computations on platform Host. Devices:\r\n2019-08-29 15:28:57.971544: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-29 15:28:57.972298: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-08-29 15:28:57.973081: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:60000, 1 -> localhost:60001}\r\n2019-08-29 15:28:57.973289: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60000\r\n2019-08-29 15:28:57.974456: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:60001\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-08-29 15:28:58.154712: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 35144c1b1c60a213 with config: device_count { key: \"CPU\" value: 3 } experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\n2019-08-29 15:28:58.170439: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 3c9b794a1f1e9112 with config: device_count { key: \"CPU\" value: 3 } experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\ntask 0 sense [[-0.8433976, 1.4618217, 1.4098305], [-0.9607944, -0.2295152, 1.4139572]]\r\n2019-08-29 15:29:28.195776: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 12138e7e76d8c500 with config: device_count { key: \"CPU\" value: 3 } experimental { collective_group_leader: \"/job:worker/replica:0/task:0\" }\r\ntask 1 sense [[-0.8433976, 1.4618217, 1.4098305], [-0.9607944, -0.2295152, 1.4139572]]\r\ntask 0 inter_reduce [-0.90209603]\r\ntask 1 inter_reduce [-0.90209603]\r\n2019-08-29 15:29:28.244516: F tensorflow/core/common_runtime/collective_param_resolver_local.cc:389] Check failed: cp->group.group_size == cp->instance.device_names.size() (3 vs. 2)0x7ff630142180\r\n2019-08-29 15:29:30.742565: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Unavailable: OS Error\r\n\t [[{{node worker1_1/CollectiveReduce}}]]\r\n2019-08-29 15:29:30.742915: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Unavailable: OS Error\r\n\t [[{{node worker1_1/CollectiveReduce_1}}]]\r\n2019-08-29 15:29:30.743016: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Unavailable: OS Error\r\n\t [[{{node worker1_1/CollectiveReduce_2}}]]\r\n2019-08-29 15:29:40.753678: W tensorflow/core/distributed_runtime/master_session.cc:1363] Timeout for closing worker session\r\n2019-08-29 15:29:50.758593: I tensorflow/core/distributed_runtime/master.cc:267] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\nTerminated\r\n```", "Are you trying to do something like a hierarchical all-reduce?\r\n\r\nI haven't tried this myself but one thing I see that confuses me is the assignment of group keys and instance keys.  You want a unique group key for each set of devices participating in a collective, and you want a unique instance key for every instance of a collective.  ", "Thank you for the fruitful advice. :smiley:  It seems a global unique `instance_key` across all groups is required for the `all_reduce` op, in spite of different group is specified by the `group_key` in a distributed environment. I think this issue should be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31913\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31913\">No</a>\n"]}, {"number": 31912, "title": "Not able to build GPU custom op example ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce GTX 980M\r\n\r\n**Describe the current behavior**\r\nFollowing the [instructions here](https://www.tensorflow.org/guide/extend/op#gpu_support) I navigate to ```tensorflow/tensorflow/examples/adding_an_op``` and run\r\n```\r\nnvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\r\n```\r\n\r\nand I get the error\r\n\r\n```\r\nIn file included from cuda_op_kernel.cu.cc:19:0:\r\n/home/alex/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/util/gpu_kernel_helper.h:22:53: fatal error: third_party/gpus/cuda/include/cuda_fp16.h: No such file or directory\r\ncompilation terminated.\r\n```\r\n", "comments": ["@alexminnaar \r\n1. Could you mention with little more what were the steps you took before this error?\r\n\r\n2. It is complaining cuda related kernel: `No such file or directory` \r\nDid you check the directory?\r\nDid you ran any TF code on GPU without the custom_op?\r\n\r\n3. Please check these two resources [1](https://github.com/tensorflow/tensorflow/issues/30632) and [2](https://github.com/tensorflow/tensorflow/issues/30956). \r\nThanks!\r\n\r\n", "@jvishnuvardhan sorry for the delay.  Here are some more detailed steps.\r\n\r\n1.  Setup new venv and pip installed tensorflow-gpu\r\n2. cloned the TensorFlow repo\r\n3.  ran ```TF_CFLAGS=( $(python -c 'import tensorflow as tf; print(\"\".join(tf.sysconfig.get_compile_flags()))') )```\r\n4.  Navigated to tensorflow/tensorflow/examples/adding_an_op within the cloned TensorFlow repo.\r\n5.  Followed the instructions outlined [here](https://www.tensorflow.org/guide/extend/op#compiling_the_kernel_for_the_gpu_device) i.e. ```nvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc\r\n  ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC``` and received the error mentioned above.\r\n\r\n", "@alexminnaar \r\nI've stumbled across same problem.\r\n\r\nThe file you are looking for, third_party/gpus/cuda/include/cuda_fp16.h really comes from your CUDA installation i.e. $CUDA_HOME/include/cuda_fp16.h . \r\n\r\nSomehow copy/link does not exist. \r\n\r\nHow about you go there:\r\n/home/alex/.local/lib/python3.5/site-packages/tensorflow/include/\r\ncreate directory third_party/gpus/cuda\r\nand then symlink your CUDA include dir here (/usr/local/cuda/include on my Debian box)?\r\n\r\n", "Relevant comment from @Artem-B: https://github.com/tensorflow/tensorflow/issues/34428#issuecomment-564281533", "@alexminnaar\r\nPlease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31912\">No</a>\n"]}, {"number": 31911, "title": "Data Augmentation from PREPROCESSING_FUNCTION_MAP?", "body": "How do we utilize Data Augmentation features in the Tensorflow API as indicated from the PREPROCESSING_FUNCTION_MAP in preprocessor_builder.py? Is there a file we have to modify in the /object_detection folder or is there additional command lines we need to be aware of?", "comments": ["@AeroWRX\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31911\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31911\">No</a>\n"]}, {"number": 31910, "title": "r1.15 cherry-pick request: [Intel MKL] Upgrade MKL-DNN to 0.20.3", "body": "This fixes a critical bug that affects convergence of 3D-GANs, U-Net, V-NET, and most other 3D models.\r\n\r\nOriginal PR: https://github.com/tensorflow/tensorflow/pull/31906 (merged in master).\r\n", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31910) for more info**.\n\n<!-- need_author_consent -->", "@agramesh1 Could you please post \"@googlebot I consent.\" to resolve the CLA issue? Thank you!", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31910) for more info**.\n\n<!-- ok -->"]}, {"number": 31909, "title": "Explicitly disable XLA for AMP test", "body": "This test checks for certain graph nodes to verify AMP correctness,\r\nbut XLA changes the graph in ways that make these checks fail.", "comments": []}, {"number": 31908, "title": "[XLA:GPU][ROCm] Refactor redzone_allocator and ptxas_utils", "body": "This is a follow-up PR to #30884. This PR bring `Cuda` specific interface including \r\n\r\n- `RedzoneAllocator` \r\n- `ptxas_util` ---renamed to----> `gpu_asm_opts`\r\n\r\nto parallel with `ROCm`. It take advantage of scoped macros, and removed all compile time dispatches. The scoped macro is in `ptxas_util` component only. We will implement `ROCm` corresponding ptx utilities once hip compile time utilities become available to us.\r\n\r\nWith this PR, the dependent #30884 will be able to compile without compile time dispatch.\r\n\r\nNote to the reviewer: a decent number of changes are trivial and related to `RedzoneAllocator` and `ptxas_utils`. Please review those two first.\r\n\r\n@cheshire @timshen91 ", "comments": ["@jerryyin can you please check build failures ?", "@rthadur Thanks for reminding, working on it.", "Sorry for the back and forth. Looks like Ubuntu CPU is still failing. I'm on it.\r\n\r\n-----\r\n\r\nFixed linker issue in the latest commit. Somehow exposed an older bug where multiple targets complaining a linker error on `stream_executor::gpu::ExtractGpuContext(stream_executor::gpu::GpuExecutor*)`. It is due to this symbol used at `cuda/rocm_activation`, and defined in `cuda/rocm_gpu_executor`. The build_config component needs to include both to compile it correctly.\r\n\r\n------\r\n\r\nThanks @rthadur for help kicking off the CI. Looks like the results are expected now.", "FYI, Re-based again to pick changes from `buffer_comparator.cc`.\r\n\r\n------\r\n\r\n@cheshire Let me know what I can do to speed up this PR. This is the last blocking piece for a functioning `ROCm` stack.", "Sorry for the difficulties, I keep approving it, but apparently merge issues keep piling up. Hopefully it will go through this time.", "Thanks for your effort in updating it! When this is done, I will create a new PR for the remainder of #30884, and ping you there.", "This CL does not build, I'll try to fix the issues today.", "@cheshire I don't have access to result of feedback/copybara, but let me know if there's anything I can do to help.", "I understand, so for now I'm looking at it. In general, smaller commits help. The problem I'm facing now is that XLA tests previously could run with GOOGLE_CUDA variable injected, and we should try to preserve this behavior. I'm trying to reverse the guard: give the ROCM version if TENSORFLOW_USE_ROCM is defined, and use CUDA instead.", "I had to do a few renames as well: `gpu_asm_opts` became `gpu/asm_compiler`, and `redzone_allocator` became `gpu/redzone_allocator`, since both are specific to GPUs.", "@cheshire I walked through you changes and there are several thing I like: \r\n\r\n- Moving shared files under stream_executor/gpu\r\n- Setting the `visibility` field to scope the binary under certain targets. I assume this together with `if_gpu_is_configured` helped address linker issues, if any\r\n\r\nI'm not too sure about the macro injection part of the story. As long as switch from `GOOGLE_CUDA` to `TENSORFLOW_USE_ROCM` help you address the issue.", "Could you clarify what is \"macro injection part of story\"? Have I missed injecting macros setting ROCM?", "> The problem I'm facing now is that XLA tests previously could run with GOOGLE_CUDA variable injected, and we should try to preserve this behavior. I'm trying to reverse the guard: give the ROCM version if TENSORFLOW_USE_ROCM is defined, and use CUDA instead.\r\n\r\nAhh, it is related to your comments referred above. I saw the description but didn't see relevant code changes, so was suspecting this is related to your internal code base?", "Yes, internally XLA has to keep working with CUDA if no variables are injected.\r\nSo for now I suggest to guard ROCM changes on special variables, and use CUDA otherwise.", "Sounds good"]}, {"number": 31907, "title": "please add tensor factorization", "body": "**System information**\r\n- TensorFlow version (you are using): 2b1\r\n- Are you willing to contribute it (Yes/No): I want to but I don't think I'm qualified\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntensorflow is about tensors but there's no built-in tensor factorization. \r\ni do believe this should be a core feature of the framework...what do you think?\r\n\r\n**Will this change the current api? How?**\r\ntf.linalg (i guess) would have something like Tucker Decomposition, higher order SVD\r\nwould this be a good 2.0 feature?\r\n\r\n**Who will benefit with this feature?**\r\npeople who need to compress / factorize big tensors\r\n\r\n**Any Other info.**\r\nhttp://tensorly.org/stable/user_guide/quickstart.html#tensor-decomposition\r\nhttps://github.com/hottbox/hottbox", "comments": ["@bionicles,\r\nSorry for the delayed response. \r\n\r\nCan you please refer to the documentation of [Cholskey Factorization](https://www.tensorflow.org/probability/api_docs/python/tfp/experimental/linalg/simple_robustified_cholesky?hl=es) and [LDL Factorization](https://www.tensorflow.org/probability/api_docs/python/tfp/experimental/linalg/no_pivot_ldl?hl=es-ESpodemos&skip_cache=true) and let us know if this is what you are looking for? Thanks!", "@bionicles it is indeed a very reasonable assumption that a framework named TensorFlow should have a tensor factorization. All the building blocks to write one should be present. Would you be able and willing to contribute such a factorization as a pull request?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31906, "title": "[INTEL MKL] Upgrade MKL DNN to 0.20.3", "body": "", "comments": []}, {"number": 31905, "title": "mnist_cnn.py major performance loss after switching to TF 1.14", "body": "**System information**\r\n- Windows 10 Enterprise 64-bit\r\n- TensorFlow GPU installed via Anaconda\r\n- TensorFlow GPU version: 1.14\r\n- Python version: 3.7.3\r\n- CUDA version: 10.0.130\r\n- cuDNN version: 7.6.0\r\n- GPU model and memory: NVIDIA Quadro P600 (2 GB)\r\n\r\n**Describe the current behavior**\r\nThe mnist.py script achieves a test accuracy of <85% after 12 epochs of training when using tensorflow-gpu=1.14.\r\n\r\n**Describe the expected behavior**\r\nThe mnist.py script is supposed to to achieve >99% test accuracy after 12 epochs of training. Doing a clean install of tensorflow-gpu=1.13 and running the script achieves this result.\r\n\r\n- TensorFlow GPU version: 1.13\r\n- Python version: 3.7.3\r\n- CUDA version: 10.0.130\r\n- cuDNN version: 7.6.0\r\n\r\nThis discrepancy between versions exists even when setting the numpy and TensorFlow random seeds before training. There are potentially major implications for the reproducibility of any work done in TensorFlow across these versions. Why is this happening and/or how can it be fixed?\r\n\r\n**Code to reproduce the issue**\r\nUse [mnist.py](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) and replace keras with tf.keras\r\n\r\n**Other info / logs**\r\nNone.\r\n", "comments": ["This issue appears to be due to a regression in the 'Adadelta' optimizer in TF1.14. I tested this with TF1.13 and 1.14 with Adadelta and Adam and noticed the following:\r\n\r\nIn TF1.13 with Adadelta = script works as intended\r\nin TF1.14 with Adadelta = script doesn't work\r\nin TF1.14 with Adam = script works as intended", "Upon further investigation this is due to a change in the default learning rate in Adadelta in TF1.14, see Issue #31024. Changing the learning rate to 1.0 when in TF1.14 fixes the issue.\r\n\r\n", "Thanks for looking into this! That seems to be the issue."]}, {"number": 31904, "title": "[XLA] Scatter: allow to not use atomic operation", "body": "Add an option to Scatter to not use atomic operation.\r\nIt should be done in a backward compatible way. All new code that doesn't use the option still generate the same protobuf as before, so work on older version.", "comments": ["@nouiz thanks for your contribution , can you please resolve conflicts ?", "rebased. I also checked the commits that was in conflict and this made me add documentation and tests for this new feature in the last commit.\r\n\r\n@thomasjoerg @sanjoy ", "+CC @blakehechtman as FYI", "All is done.", "I rebased as there was a conflict."]}, {"number": 31903, "title": "Fix Tensorflow Lite Documentation", "body": "Hi,\r\n\r\nOn https://www.tensorflow.org/lite/guide/python the sample Interpreter code is wrong \r\n\r\n![image](https://user-images.githubusercontent.com/2943831/63538358-9b562f80-c4cc-11e9-8cac-d81786a1acad.png)\r\n\r\n\"from tflite_runtime import Interpreter\"  should be changed to\r\n\r\n\"from tflite_runtime.interpreter import Interpreter\"\r\n\r\n\r\nThanks\r\n\r\nHakan\r\n\r\n", "comments": ["Similar issue [#31369](https://github.com/tensorflow/tensorflow/issues/31369).  Will close this now, we can track the solution on that thread.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31903\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31903\">No</a>\n"]}, {"number": 31902, "title": "[tflite] Support INT8 quantisation for UNPACK with TFLITE_BUILTINS_INT8 OpsSet", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14 and built from sources, master branch\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe new TFLiteConverter post-training quantisation flow, as described in https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations, does not support quantisation of UNPACK/UNSTACK operation when only integer operations are requested in the output model. When such conversion is attempted the following error is reported: \r\n\r\n> RuntimeError: Quantization not yet supported for op: UNPACK\r\n\r\nFor example, the script below\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef representative_dataset_gen():\r\n\tinput = np.ones([2, 10],dtype=np.float32)\r\n\tfor _ in range(10):\r\n\t\tyield [input]\r\n\r\n# tf Graph Input\r\nin_stacked = tf.compat.v1.placeholder(\"float32\", [2, 10])\r\nout_unstacked = tf.unstack(in_stacked, axis=0)\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n\ttf.io.write_graph(tf.compat.v1.get_default_graph(), '.','unpack.pb', as_text=False)\r\n\r\ninput_name = [\"Placeholder\"]\r\noutput_name = [\"unstack\", \"unstack:1\"]\r\n\r\ntflite_model_name = \"int8_unpack.tflite\"\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\"unpack.pb\", input_name, output_name)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_model = converter.convert()\r\nopen(tflite_model_name, \"wb\").write(tflite_model)\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(tflite_model_name)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\n```\r\n\r\nproduces\r\n```\r\n\r\n2019-08-22 17:44:45.800226: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3492095000 Hz\r\n2019-08-22 17:44:45.800687: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c7b280 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-08-22 17:44:45.800705: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-08-22 17:44:45.805127: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-08-22 17:44:45.805220: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\nTraceback (most recent call last):\r\n  File \"TF_Tests/test.py\", line 24, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Quantization not yet supported for op: UNPACK\r\n\r\n```\r\n\r\nWe propose adding the required support into the TFLiteConverter as it seems to be a natural extension of the existing functionality and will make the int8 quantisation process consistent across all supported  conversion options.\r\n\r\nIt appears that the UNPACK operator for kTfLiteInt8 type is already implemented (see [unpack.cc](https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/kernels/unpack.cc#L93)). The converstion fails on the [check of \"quantizable\" property](https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/tools/optimize/quantize_model.cc#L561) because [GetOperatorProperty()](https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/tools/optimize/operator_property.cc#L20) does not contain a case statement for BuiltinOperator_UNPACK.\r\n\r\nIt looks like modifying GetOperatorProperty() could unlock this feature. The corresponding unit test(s) will need to be added as well.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who attempts full 8-bit fixed point quantisation of models containinig UNPACK/UNSTACK operation, e.g. some versions of DeepSpeech.\r\n\r\n**Any Other info.**\r\n", "comments": ["In addition to the proposed changes in [GetOperatorProperty()](https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/tools/optimize/operator_property.cc#L20), it appears that it is also necessary to add support of the arbitrary number of operator outputs, similar to how it is done for the arbitrary number of inputs in Concatenation operator.\r\n\r\nWe will soon follow up with the patch that implements the proposed changes.", "@akarmi, @MohamedNourArm: is there an effort to adding deepspeech quantized support? If so, it'd be worth creating a specific issue for that, which includes pointers to the original TF model variants.\r\nIt'd be a clear and impactful way to contribute. \r\n\r\nIt's great that you specifically mentioned DeepSpeech as motivating this unpack int8 support. In general, it's more meaningful to create an issue based on a task/model as opposed to an individual op. ", "@alanchiao, yes, we are trying various quantization options, including the [proposed 16-bit activations](https://github.com/tensorflow/tensorflow/pull/33343), with DeepSpeech, in its unrolled form for now. Depending on how far we get with that, we may be able to share the results of our efforts. For now, I am closing this issue as the [corresponding fix](https://github.com/tensorflow/tensorflow/pull/33971) has now been merged. Thanks!"]}, {"number": 31901, "title": "Handle AddV2 and FusedBatchNormV3 in tflite_convert", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.0.0b1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONCATENATION, CONV_2D, EXPAND_DIMS, LOGISTIC, MAX_POOL_2D, MEAN, MIRROR_PAD, MUL, PACK, RELU, RELU6, RESIZE_NEAREST_NEIGHBOR, RSQRT, SHAPE, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: AddV2, FusedBatchNormV3.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\nhttps://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Looks like it's in master.", "I have the same problem, but I have no idea what you do to solve this.\r\n\r\nHere is my tracebacks:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"E:\\anaconda\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 898, in convert\r\n    **converter_kwargs)\r\n  File \"E:\\anaconda\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 404, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"E:\\anaconda\\envs\\Face_Recognition\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 172, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2019-09-03 20:25:13.931495: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.932230: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"CPU\"') for unknown op: NoOp\r\n2019-09-03 20:25:13.932603: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"NoOp\" device_type: \"GPU\"') for unknown op: NoOp\r\n2019-09-03 20:25:13.932957: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostRecv\r\n2019-09-03 20:25:13.933454: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"CPU\"') for unknown op: _Send\r\n2019-09-03 20:25:13.933827: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostRecv\" device_type: \"CPU\"') for unknown op: _HostRecv\r\n2019-09-03 20:25:13.934313: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Send\" device_type: \"GPU\"') for unknown op: _Send\r\n2019-09-03 20:25:13.934666: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"CPU\"') for unknown op: _Recv\r\n2019-09-03 20:25:13.935109: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"GPU\" host_memory_arg: \"tensor\"') for unknown op: _HostSend\r\n2019-09-03 20:25:13.935581: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_Recv\" device_type: \"GPU\"') for unknown op: _Recv\r\n2019-09-03 20:25:13.935940: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"_HostSend\" device_type: \"CPU\"') for unknown op: _HostSend\r\n2019-09-03 20:25:13.936406: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\r\n2019-09-03 20:25:13.936840: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\r\n2019-09-03 20:25:13.937432: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\r\n2019-09-03 20:25:13.937872: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\r\n2019-09-03 20:25:13.938585: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.939038: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.939337: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.939688: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.940042: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.940503: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.940784: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.941269: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.941562: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.941985: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.942400: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.942873: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.943286: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.943748: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.944123: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.944613: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.945035: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.945454: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: AddV2\r\n2019-09-03 20:25:13.945790: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: AddV2\r\n2019-09-03 20:25:13.946425: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: IdentityN\r\n2019-09-03 20:25:13.946883: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: IdentityN\r\n2019-09-03 20:25:13.966304: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 378 operators, 701 arrays (0 quantized)\r\n2019-09-03 20:25:13.997206: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 378 operators, 701 arrays (0 quantized)\r\n2019-09-03 20:25:14.035854: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 72 operators, 187 arrays (0 quantized)\r\n2019-09-03 20:25:14.040310: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 72 operators, 187 arrays (0 quantized)\r\n2019-09-03 20:25:14.044211: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 72 operators, 187 arrays (0 quantized)\r\n2019-09-03 20:25:14.051806: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 10523008 bytes, theoretical optimal value: 9720192 bytes.\r\n2019-09-03 20:25:14.053806: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MEAN, PAD, SOFTMAX. Here is a list of operators for which you will need custom implementations: AddV2, IdentityN.\r\nTraceback (most recent call last):\r\n  File \"e:\\anaconda\\envs\\face_recognition\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"e:\\anaconda\\envs\\face_recognition\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"E:\\anaconda\\envs\\Face_Recognition\\Scripts\\toco_from_protos.exe\\__main__.py\", line 9, in <module>\r\n  File \"e:\\anaconda\\envs\\face_recognition\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"e:\\anaconda\\envs\\face_recognition\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"e:\\anaconda\\envs\\face_recognition\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"e:\\anaconda\\envs\\face_recognition\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"e:\\anaconda\\envs\\face_recognition\\lib\\site-packages\\tensorflow\\lite\\toco\\python\\toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, MEAN, PAD, SOFTMAX. Here is a list of operators for which you will need custom implementations: AddV2, IdentityN.\r\n\r\nI sincerely hope you could help me", "Have the same problem, specifically with `AddV2`, which is nowhere to be found in the TFLiteConverter code.", "I also have the same problem\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, PAD, RELU6, SOFTMAX. Here is a list of operators for which you will need custom implementations: FusedBatchNormV3.", "Uncaught (in promise) Error: Tensorflow Op is not supported: AddV2\r\n    at t.mapNode (operation_mapper.ts:125)\r\n    at operation_mapper.ts:86\r\n    at Array.reduce (<anonymous>)\r\n    at t.transformGraph (operation_mapper.ts:85)\r\n    at t.<anonymous> (graph_model.ts:125)\r\n    at callbacks.ts:17\r\n    at Object.next (callbacks.ts:17)\r\n    at a (callbacks.ts:17)\r\n\r\nmy error I can not figure out what the problem", "    Uncaught (in promise) Error: Tensorflow Op is not supported: AddV2\r\n    at t.mapNode (operation_mapper.ts:125)\r\n    at operation_mapper.ts:86\r\n    at Array.reduce (<anonymous>)\r\n    at t.transformGraph (operation_mapper.ts:85)\r\n    at t.<anonymous> (graph_model.ts:125)\r\n    at callbacks.ts:17\r\n    at Object.next (callbacks.ts:17)\r\n    at a (callbacks.ts:17)\r\nGetting the same error  but I cant figure out what the problem  using the latest tensorflowjs 1.7.4 ", "This error is mainly come your model is make by  separate version of\ntensorflow coding,either 1 or 2 then, if you can make model using only one\nversion of coding then problem solved\n\nIf you don't figure out yet set me model code if you like I will figure out\nit\n\nOn Fri, 15 May 2020, 20:36 timothyb365, <notifications@github.com> wrote:\n\n> Uncaught (in promise) Error: Tensorflow Op is not supported: AddV2\n> at t.mapNode (operation_mapper.ts:125)\n> at operation_mapper.ts:86\n> at Array.reduce (<anonymous>)\n> at t.transformGraph (operation_mapper.ts:85)\n> at t.<anonymous> (graph_model.ts:125)\n> at callbacks.ts:17\n> at Object.next (callbacks.ts:17)\n> at a (callbacks.ts:17)\n>\n> Getting the same error but I cant figure out what the problem using the\n> latest tensorflowjs 1.7.4\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31901#issuecomment-629286871>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI5VMMDSWIKAPUHJUFQZG6LRRVK7XANCNFSM4IOXGA3A>\n> .\n>\n", "def setup_layer(input, weight_dim, bias_dim, name):\r\n    with tf.name_scope(name):\r\n        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\r\n        w = tf.Variable(initial_value=initial_w, name='W')\r\n\r\n        initial_b = tf.constant(value=0.0, shape=bias_dim)\r\n        b = tf.Variable(initial_value=initial_b, name='B')\r\n\r\n        layer_in = tf.matmul(input, w) + b\r\n        \r\n        if name=='out':\r\n            layer_out = tf.nn.softmax(layer_in)\r\n        else:\r\n            layer_out = tf.nn.relu(layer_in)\r\n        \r\n        tf.summary.histogram('weights', w)\r\n        tf.summary.histogram('biases', b)\r\n        \r\n        return layer_out\r\n\r\nlayer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1],\r\n                      bias_dim=[n_hidden1],name='layer_1' )\r\n\r\nlayer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\r\n\r\nlayer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2],\r\n                      bias_dim=[n_hidden2],name='layer_2' )\r\n\r\noutput = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES],\r\n                      bias_dim=[NR_CLASSES],name='out' )\r\nmodel_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\r\n\r\n\r\ndef next_batch(batch_size, data, labels):\r\n    \r\n    global num_examples\r\n    global index_in_epoch\r\n    \r\n    start = index_in_epoch\r\n    index_in_epoch += batch_size\r\n    \r\n    if index_in_epoch > num_examples:\r\n        start = 0\r\n        index_in_epoch = batch_size\r\n    \r\n    \r\n    \r\n    end = index_in_epoch\r\n    \r\n    return data[start:end], labels[start:end]\r\n\r\n\r\nfor epoch in range(nr_epochs):\r\n    # ============== Training Dataset ======\r\n    for i in range(nr_iterations):\r\n        batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\r\n        \r\n        feed_dictionery = {X:batch_x, y:batch_y}\r\n        \r\n        sess.run(train_step, feed_dict=feed_dictionery)\r\n        \r\n    s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionery)\r\n     \r\n    train_writer.add_summary(s, epoch)    \r\n    print(f'Epoch {epoch} \\t! Training Accuracy = {batch_accuracy}')\r\n    \r\n    # ================ Validation =================\r\n    \r\n    summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, y:y_val})\r\n    validation_writer.add_summary(summary, epoch)\r\n    \r\nprint('done training!')\r\n\r\n\r\nthis is a bit clear code @chanakasandaruwan  ", "Give me time I working with your information,if I figure out I will tell you\n\nOn Fri, 15 May 2020, 21:21 timothyb365, <notifications@github.com> wrote:\n\n> def setup_layer(input, weight_dim, bias_dim, name):\n> with tf.name_scope(name):\n> initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n> w = tf.Variable(initial_value=initial_w, name='W')\n>\n>     initial_b = tf.constant(value=0.0, shape=bias_dim)\n>     b = tf.Variable(initial_value=initial_b, name='B')\n>\n>     layer_in = tf.matmul(input, w) + b\n>\n>     if name=='out':\n>         layer_out = tf.nn.softmax(layer_in)\n>     else:\n>         layer_out = tf.nn.relu(layer_in)\n>\n>     tf.summary.histogram('weights', w)\n>     tf.summary.histogram('biases', b)\n>\n>     return layer_out\n>\n> layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n> bias_dim=[n_hidden1],name='layer_1' )\n>\n> layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n>\n> layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2],\n> bias_dim=[n_hidden2],name='layer_2' )\n>\n> output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n> bias_dim=[NR_CLASSES],name='out' )\n> model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n>\n> def next_batch(batch_size, data, labels):\n>\n> global num_examples\n> global index_in_epoch\n>\n> start = index_in_epoch\n> index_in_epoch += batch_size\n>\n> if index_in_epoch > num_examples:\n>     start = 0\n>     index_in_epoch = batch_size\n>\n>\n>\n> end = index_in_epoch\n>\n> return data[start:end], labels[start:end]\n>\n> for epoch in range(nr_epochs):\n> # ============== Training Dataset ======\n> for i in range(nr_iterations):\n> batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train,\n> labels=y_train)\n>\n>     feed_dictionery = {X:batch_x, y:batch_y}\n>\n>     sess.run(train_step, feed_dict=feed_dictionery)\n>\n> s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionery)\n>\n> train_writer.add_summary(s, epoch)\n> print(f'Epoch {epoch} \\t! Training Accuracy = {batch_accuracy}')\n>\n> # ================ Validation =================\n>\n> summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, y:y_val})\n> validation_writer.add_summary(summary, epoch)\n>\n> print('done training!')\n>\n> this is a bit clear code @chanakasandaruwan\n> <https://github.com/chanakasandaruwan>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31901#issuecomment-629333233>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI5VMMEBZCQBUL4HQIZZJQ3RRVQJLANCNFSM4IOXGA3A>\n> .\n>\n", "Setup Tensorflow Graph\n\n#add this code before placeholder try it is working\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nX = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\ny = tf.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')\n\nOn Fri, May 15, 2020 at 10:27 PM chanaka sandaruwan <schanaka665@gmail.com>\nwrote:\n\n> Give me time I working with your information,if I figure out I will tell\n> you\n>\n> On Fri, 15 May 2020, 21:21 timothyb365, <notifications@github.com> wrote:\n>\n>> def setup_layer(input, weight_dim, bias_dim, name):\n>> with tf.name_scope(name):\n>> initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n>> w = tf.Variable(initial_value=initial_w, name='W')\n>>\n>>     initial_b = tf.constant(value=0.0, shape=bias_dim)\n>>     b = tf.Variable(initial_value=initial_b, name='B')\n>>\n>>     layer_in = tf.matmul(input, w) + b\n>>\n>>     if name=='out':\n>>         layer_out = tf.nn.softmax(layer_in)\n>>     else:\n>>         layer_out = tf.nn.relu(layer_in)\n>>\n>>     tf.summary.histogram('weights', w)\n>>     tf.summary.histogram('biases', b)\n>>\n>>     return layer_out\n>>\n>> layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1],\n>> bias_dim=[n_hidden1],name='layer_1' )\n>>\n>> layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n>>\n>> layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2],\n>> bias_dim=[n_hidden2],name='layer_2' )\n>>\n>> output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES],\n>> bias_dim=[NR_CLASSES],name='out' )\n>> model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n>>\n>> def next_batch(batch_size, data, labels):\n>>\n>> global num_examples\n>> global index_in_epoch\n>>\n>> start = index_in_epoch\n>> index_in_epoch += batch_size\n>>\n>> if index_in_epoch > num_examples:\n>>     start = 0\n>>     index_in_epoch = batch_size\n>>\n>>\n>>\n>> end = index_in_epoch\n>>\n>> return data[start:end], labels[start:end]\n>>\n>> for epoch in range(nr_epochs):\n>> # ============== Training Dataset ======\n>> for i in range(nr_iterations):\n>> batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train,\n>> labels=y_train)\n>>\n>>     feed_dictionery = {X:batch_x, y:batch_y}\n>>\n>>     sess.run(train_step, feed_dict=feed_dictionery)\n>>\n>> s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionery)\n>>\n>> train_writer.add_summary(s, epoch)\n>> print(f'Epoch {epoch} \\t! Training Accuracy = {batch_accuracy}')\n>>\n>> # ================ Validation =================\n>>\n>> summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, y:y_val})\n>> validation_writer.add_summary(summary, epoch)\n>>\n>> print('done training!')\n>>\n>> this is a bit clear code @chanakasandaruwan\n>> <https://github.com/chanakasandaruwan>\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/31901#issuecomment-629333233>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AI5VMMEBZCQBUL4HQIZZJQ3RRVQJLANCNFSM4IOXGA3A>\n>> .\n>>\n>\n", "@chanakasandaruwan thanks so much the code is now working ", "\ud83d\udcaaNice to help you Sir\n\nOn Sun, 17 May 2020, 00:36 timothyb365, <notifications@github.com> wrote:\n\n> @chanakasandaruwan <https://github.com/chanakasandaruwan> thanks so much\n> the code is now working\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31901#issuecomment-629692521>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI5VMMA6JHRMNU2KMEGADYTRR3P2XANCNFSM4IOXGA3A>\n> .\n>\n"]}, {"number": 31900, "title": "'is_final' is not a member of 'std'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RedHat 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): cc version 8.2.1 20180905 (Red Hat 8.2.1-3)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCompilation of LLVM is failing because '-std=c++0x' is being passed, but current LLVM uses C++14 features.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build --verbose_failures --config opt --config mkl //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nNote \"-std=c++0x\" option. I could probably fix this myself if I could figure out where it is coming from...\r\n```\r\n  /opt/rh/devtoolset-8/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/compiler/mlir/lite/quantization/_objs/op_quant_spec_getters_gen/op_quant_spec_getters_gen.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/mlir/lite/quantization/_objs/op_quant_spec_getters_gen/op_quant_spec_getters_gen.o' -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/host/bin -iquote external/llvm -iquote bazel-out/host/bin/external/llvm -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_mlir -iquote bazel-out/host/bin/external/local_config_mlir -iquote external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -isystem external/llvm/include -isystem bazel-out/host/bin/external/llvm/include -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_mlir/include -isystem bazel-out/host/bin/external/local_config_mlir/include -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DINTEL_MKL=1' -DEIGEN_USE_VML -DENABLE_MKL -fopenmp -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/compiler/mlir/lite/quantization/tools/op_quant_spec_getters_gen.cc -o bazel-out/host/bin/tensorflow/compiler/mlir/lite/quantization/_objs/op_quant_spec_getters_gen/op_quant_spec_getters_gen.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from external/llvm/include/llvm/TableGen/Record.h:27,\r\n                 from tensorflow/compiler/mlir/lite/quantization/tools/op_quant_spec_getters_gen.cc:21:\r\nexternal/llvm/include/llvm/Support/TrailingObjects.h: In static member function 'static void llvm::TrailingObjects<BaseTy, TrailingTys>::verifyTrailingObjectsAssertions()':\r\nexternal/llvm/include/llvm/Support/TrailingObjects.h:252:24: error: 'is_final' is not a member of 'std'\r\n     static_assert(std::is_final<BaseTy>(), \"BaseTy must be final.\");\r\n                        ^~~~~~~~\r\n```", "comments": ["Reverting 09c588de fixes this for me.\r\n\r\nYes this is a duplicate of issue #31866, which was closed even though the problem remains.", "Update: The -std=c++0x flag is coming from inside Bazel.\r\n\r\nSetting the environment variable BAZEL_CXXOPTS to \"-std=c++14\" makes the compilation error go away. (So presumably you can set it to -std=c++0x to reproduce this issue.)\r\n\r\nSeems like the LLVM build ought to be setting its build options explicitly, and not relying on whatever is compiled in to Bazel (?)", "Hmm, I wonder if this is a side effect of our compiler, which is based on devtoolset. We haven't been seeing this issue in our internal builds, so perhaps you're encountering a surprising build configuration error.\r\n\r\nJust to double-check, have you tried re-running `./configure`? The setting might have been set in the locally generated .bazelrc.", "Perhaps, but I do not think so...\r\n\r\nI recursively \"grepped\" the Tensorflow tree post-configure trying to determine where the \"c++0x\" was coming from. I even manually edited every instance (although all of them are under \"third_party/toolchains\"), and it did not change anything.\r\n\r\nHowever, when I recursively grep the *Bazel* source code (version 0.25.2 or 0.26.1 or 0.28.1) for \"c++0x\", I get this:\r\n\r\n`./tools/cpp/unix_cc_configure.bzl:    cxx_opts = split_escaped(get_env_var(repository_ctx, \"BAZEL_CXXOPTS\", \"-std=c++0x\", False), \":\")`\r\n\r\n...which in turn is how I decided to try setting BAZEL_CXXOPTS to work around this issue. So I am pretty sure this is coming from inside Bazel.\r\n\r\nI do not claim to fully understand Bazel, much less its source code... But I believe a stock Bazel build sets \"-std=c++0x\" for cxx_opts unless it is overridden. I am pretty sure the LLVM build inside Tensorflow is not doing so.\r\n\r\nI am not sure why most people do not see this issue, but my guess is that it has to do with how their Bazel is compiled.", "I am pretty sure you could reproduce this failure by setting the BAZEL_CXXOPTS environment variable to \"-std=c++0x\" and running a build.\r\n\r\nYou will find the build still gets pretty far because Tensorflow mostly specifies the cxxopts explicitly. But not when it compiles LLVM, which gets whatever happened to be compiled into Bazel (or set by this environment variable).", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31900\">No</a>\n"]}, {"number": 31899, "title": "[r1.15 cherrypick] Fix tf.gradients() performance regression", "body": "In _GradientsHelper() compute the ObjectIdentitySet(xs) once and reuse it.\r\n\r\nThis avoids a potentially quadratic execution time in building the gradient graph, because we were previously creating the set multiple times for each op in the graph.\r\n\r\nPiperOrigin-RevId: 264826531", "comments": []}, {"number": 31898, "title": "[r2.0 cherrypick] Fix tf.gradients() performance regression", "body": "In _GradientsHelper() compute the ObjectIdentitySet(xs) once and reuse it.\r\n\r\nThis avoids a potentially quadratic execution time in building the gradient graph, because we were previously creating the set multiple times for each op in the graph.\r\n\r\nPiperOrigin-RevId: 264826531", "comments": []}, {"number": 31897, "title": "Add documentation about Dataset shard's deterministic nature", "body": "Hi everyone,\r\nThis pull request addresses the issue raised in issue #31698. \r\nSpecifically, I have edited the documentation so that it clearly mentions that the `shard` operation in the Dataset API is deterministic.\r\n\r\n[Here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/core/kernels/data/shard_dataset_op.cc) is the file containing the implementation of `shard` in r2.0.\r\nIf I understand correctly, [this line](https://github.com/tensorflow/tensorflow/blob/b8b60ae09afb3b05dde9f183bac587c7dd437c78/tensorflow/core/kernels/data/shard_dataset_op.cc#L132) dictates exactly how the target Dataset is split into its \"shards\".\r\n\r\nPlease let me know your feedback.\r\nThanks for your time.\r\n\r\n", "comments": ["@aaudiber sorry this was merged automatically , @anubh-v do you want to address @aaudiber in a new PR if not i can take up this internally , thank you for your contribution.", "Thanks @aaudiber, I will raise a new PR to make the suggested changes."]}, {"number": 31896, "title": "tf-2.0.0rc0-gpu tf.dataset bug", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: yes \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): docker image\r\n- TensorFlow version (use command below): tensorflow/tensorflow:2.0.0rc0-gpu-py3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am using tf.keras for training with the dataset api and TFrecords. At the beginning of the training the dataset api tries to load and shuffle the whole training dataset so the memory fills up and the process dies.  I cannot train my model at all. I am not using any .shuffle() calls in my dataset pipeline.\r\n\r\n**Describe the expected behavior**\r\nThe dataset should not load and shuffle the records unless the .shuffle() call is issued.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport os\r\nimport cv2\r\nimport tensorflow.keras.layers as K\r\n\r\ndef extract_fn(data_record):\r\n    features = {\r\n        'data': tf.io.FixedLenFeature([], tf.string)\r\n    }\r\n    sample = tf.io.parse_single_example(data_record, features)\r\n    data = tf.image.decode_image(sample['data'])\r\n\r\n    return data, 1.\r\n\r\n\r\nclass DataGenerator(tf.keras.utils.Sequence):\r\n\r\n    def __init__(self, dataset_iterator, len):\r\n        self.dataset_iterator = dataset_iterator\r\n        self.len = len\r\n\r\n    def __len__(self):\r\n        # number of batches per epoch\r\n        return self.len\r\n\r\n    def __getitem__(self, index):\r\n        # Generate one batch of data\r\n\r\n        next_element = next(self.dataset_iterator)\r\n        x = next_element[0]\r\n        y = next_element[1]\r\n\r\n        return x, y\r\n\r\n\r\nwith tf.io.TFRecordWriter(\"dummy_dataset.tfrecords\") as writer:\r\n    data = np.float32(np.random.random(size=(1000, 1000, 3)) * 255)\r\n    data = cv2.imencode(\".png\", data)[1].tostring()\r\n    example = tf.train.Example(features=tf.train.Features(\r\n        feature={'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[data]))}))\r\n    for i in range(10000):\r\n        writer.write(example.SerializeToString())\r\n\r\ndataset = tf.data.TFRecordDataset([\"dummy_dataset.tfrecords\"])\r\ndataset = dataset.map(extract_fn)\r\nn_batch = 3\r\ndataset = dataset.batch(batch_size=n_batch, drop_remainder=True)\r\n\r\ndataset = dataset.repeat(5)\r\ndataset_iterator = iter(dataset)\r\nnext_element = next(dataset_iterator)\r\n\r\ndata_generator = DataGenerator(dataset_iterator, int(10000/n_batch))\r\n\r\ninput = K.Input(shape=(1000, 1000, 3), name='input')\r\nnet = K.Conv2D(1, 3, activation='sigmoid')(input)\r\noutput = K.GlobalAveragePooling2D()(net)\r\nmodel = tf.keras.models.Model(inputs=input, outputs=output)\r\n\r\nmodel.compile(loss='mse', optimizer='sgd')\r\n\r\nmodel.fit(x=data_generator, epochs=10)\r\n```\r\n**Other info / logs**\r\nnote that code generates about 28GB dummy data in a tfrecord file since I am having this issue when I am trying to use the dataset api with tf.keras.utils.Sequence on a tfrecord file.\r\n\r\nEDIT(robieta): code formatting.", "comments": ["@LaCandela ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!\r\n", "I found that the issue arises when I am trying to use tf.keras.utils.Sequence to create a data generator and using that to feed the model.fit. I provided a small script. Please note that it generates about 28GB dummy data in a tfrecord file since I am having this issue when I am trying to use the dataset api with tf.keras.utils.Sequence on a tfrecord file. I need to use the tf.keras.utils.Sequence because it allows me to do some preprocessing that I can performe on GPU. As best I know, I cannot do that with dataset.map().\r\n", "Hi! I was wondering if there was any progress in this issue...\r\nIt still exists in the recently released 2.0 docker image.", "@LaCandela I reduced the dummy_data size and ran it with `TF2.0`. I don't see any issues. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/007ed87a42cce8f0993933fd6c1c8b83/tf31896.ipynb) is the gist. Thanks!", "@jvishnuvardhan Well, My TFrecords file size for the training is in the range of ~30GB and I cannot reduce that. The dataset api tries to fill a shuffle buffer which I am not calling at all (no dataset.shuffle() call) and quickly fills up my memory. It seems as if the whole TFrecords is loaded into memory. The issue is still present in TF2.0.", "Why are using `DataGenerator` instead of passing your `dataset` directly to `model.fit`?", "Because I want to do the preprocessing on the GPU and if I use the dataset.map() function the operations are performed on the CPU (as best I know). Now, I have the feeling that it is against the Tensorflow \"mindset\" but it would be great to have the possibility to experiment with the preproc (augmentation, ground truth generation) on the GPU approach.", "The `map` dataset op will be placed on CPU but ops inside of the user-defined function passed into the `map` transformation can be placed on GPU.\r\n\r\nAs for the problem you are seeing, my best guess is that something in Keras internals might end up prefetching the contents of the entire generator ahead of time. @robieta could you please comment on what could be going on? Thanks.", "I believe the issue is that shuffle defaults to True in model.fit, which will call .shuffle(len(sequence)) internally. (Prefetching all of the elements.) It does, however, strike me that for a Sequence this is not very efficient. (Since we can just directly index said sequence) I am already working on the data adapter; I will update it accordingly.", "@LaCandela Could you check with latest TF version and let us know if the problem still persists or not.    Thanks!", "@LaCandela  Any updates regarding this issue? Thanks!", "@LaCandela  Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31896\">No</a>\n"]}, {"number": 31895, "title": "How to fix the ConcatOp dimension error when trying to compile a model using tensorflow XLA AOT?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Built using source\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have a LSTM model which I need to compile using XLA AOT. I am getting the ConcatOp dimensionality error when building using tensorflow. \r\n**ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1**\r\nI am not able to understand the problem here with the dimensionality as the model works fine for inference.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBUILD File\r\n\r\nload('@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl', 'tf_library')\r\n\r\ntf_library(\r\n    name = 'graph',\r\n    config = 'graph.config.pbtxt',\r\n    cpp_class = 'Graph',\r\n    graph = 'graph.pb',\r\n)\r\n\r\ncommand to compile model\r\n:~bazel build --show_progress_rate_limit=600 @org_tensorflow//:graph --verbose_failures\r\n\r\n**Any other info / logs**\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e7f4ecf01a350c19085daa7be2d44aba/external/org_tensorflow/BUILD:3:1: Executing genrule @org_tensorflow//:gen_graph failed (Exit 1): bash failed: error executing command\r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/e7f4ecf01a350c19085daa7be2d44aba/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/usr/local/cuda-9.0/lib/: \\\r\n    PATH=/home/ubuntu/anaconda3/bin/:/home/ubuntu/bin:/home/ubuntu/.local/bin:/home/ubuntu/anaconda3/bin/:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/home/ubuntu/src/cntk/bin:/usr/local/mpi/bin:/usr/local/cuda/bin:/usr/local/bin:/opt/aws/bin:/usr/local/mpi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; CUDA_VISIBLE_DEVICES='\\'''\\'' bazel-out/host/bin/external/org_tensorflow/tensorflow/compiler/aot/tfcompile --graph=external/org_tensorflow/graph.pb --config=external/org_tensorflow/graph.config.pbtxt --entry_point=__xla___graph --cpp_class=Graph --target_triple=x86_64-pc-linux --out_header=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph.h --out_metadata_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_metadata.o --out_function_object=bazel-out/k8-opt/genfiles/external/org_tensorflow/graph_tfcompile_function.o  ')\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n2019-08-21 16:38:57.472380: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nINVALID ARGUMENTS: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1\r\n         [[{{node rnn/basic_lstm_cell/concat}}]]", "comments": []}, {"number": 31894, "title": "tf.keras.layers.BatchNormalization() throws TypeError: Incompatible types: <dtype: 'resource'> vs. int64. Value is 0 ", "body": "```\r\nimport tensorflow as tf\r\nbatch_size = 20\r\ninp = tf.placeholder(tf.float32, [batch_size, 19, 64, 64, 3])\r\nout = tf.placeholder(tf.float32, [batch_size, 19, 60, 60, 16])\r\ndef model(inp):\r\n\r\n  enc = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(128, activation='relu', kernel_size=3,kernel_initializer='glorot_uniform'))(inp)\r\n  enc = tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization())(enc)\r\n  enc = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(16, activation='relu',kernel_size=3,kernel_initializer='glorot_uniform'))(enc)\r\n  return enc\r\n\r\npred = model(inp)\r\n\r\nloss = tf.reduce_mean(tf.keras.backend.binary_crossentropy(out, pred))\r\nlr = 0.0001\r\ntrain_op = tf.train.AdamOptimizer(lr).minimize(loss)\r\n```\r\n\r\nThrows error::\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-43-c9408a385d78> in <module>()\r\n      1 lr = 0.0001\r\n----> 2 train_op = tf.train.AdamOptimizer(lr).minimize(reconstuction_loss)\r\n\r\n7 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    401         aggregation_method=aggregation_method,\r\n    402         colocate_gradients_with_ops=colocate_gradients_with_ops,\r\n--> 403         grad_loss=grad_loss)\r\n    404 \r\n    405     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\r\n    510         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\r\n    511         aggregation_method=aggregation_method,\r\n--> 512         colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n    513     if gate_gradients == Optimizer.GATE_GRAPH:\r\n    514       grads = control_flow_ops.tuple(grads)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\r\n    156         ys, xs, grad_ys, name, colocate_gradients_with_ops,\r\n    157         gate_gradients, aggregation_method, stop_gradients,\r\n--> 158         unconnected_gradients)\r\n    159   # pylint: enable=protected-access\r\n    160 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    718               # issue here because of zeros.\r\n    719               if loop_state:\r\n--> 720                 out_grads[i] = loop_state.ZerosLike(op, i)\r\n    721               else:\r\n    722                 out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py in ZerosLike(self, op, index)\r\n   1229       # If the shape is known statically, just create a zero tensor with\r\n   1230       # the right shape in the grad loop context.\r\n-> 1231       result = constant_op.constant(0, shape=shape.dims, dtype=val.dtype)\r\n   1232       if dead_branch:\r\n   1233         # op is a cond switch. Guard the zero tensor with a switch.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    244   \"\"\"\r\n    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 246                         allow_broadcast=True)\r\n    247 \r\n    248 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    282       tensor_util.make_tensor_proto(\r\n    283           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 284           allow_broadcast=allow_broadcast))\r\n    285   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    286   const_tensor = g.create_op(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    499                             dtype.base_dtype != numpy_dtype.base_dtype):\r\n    500     raise TypeError(\"Incompatible types: %s vs. %s. Value is %s\" %\r\n--> 501                     (dtype, nparray.dtype, values))\r\n    502 \r\n    503   # If shape is not given, get the shape from the numpy array.\r\n\r\nTypeError: Incompatible types: <dtype: 'resource'> vs. int64. Value is 0\r\n```\r\nUPDATE:: \r\nthis works but if you set the trainable boolean to True, it throws the same error\r\n```\r\n  enc = tf.keras.layers.TimeDistributed(tf.layers.BatchNormalization(trainable = False))(enc)\r\n```\r\n\r\n", "comments": ["I could reproduce the issue with Tensorflow 1.14.0 and tf-nightly. Here is the [gist](https://colab.research.google.com/drive/1CL4dYS0LLbTyoe7Dt8MQ_kD2yA3o6u3T).\r\n@iamnotahumanbecauseiamabot, Which version of tensorflow you using. Thanks!", "@gadagashwini I am using 1.14.0, though if you don't apply TimeDistributed layer on BatchNormalisation, it will work.", "@iamnotahumanbecauseiamabot, Thanks for the update. ", "@robieta I have updated the issue please see the update.", "I am seeing the same thing when using the MirroredStrategy. The model works fine when executing normally. We don't have TimeDistributed Layers. If you want another ticket I am happy to make one but I am probably not going to be able to generate a repro case as the model is quite large.", "@sseveran It would be good if you create another issue with MirroredStrategy. It's even better if you can provide a simple standalone code. Thanks!", "I'm getting the same error on tensorflow 1.15 when using  batch normalization inside a custom RNNCell + RNN layer. The error only appears in the while loop of the rnn. \r\n\r\nIt also seems like the problem was resolved in tensorflow version 2. Would be great if the fix could be ported back into version 1 if that is possible. (I tried adapting control_flow_ops.py accordingly, but ran into more errors that I don't understand)\r\n", "@ akloss Closing this issue as it was resolved in TF  version 2. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31894\">No</a>\n"]}, {"number": 31893, "title": "[TF2] tf.saved_model.save fails on models re-using other models", "body": "**System information**\r\n- Have I written custom code: Yes.\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0-dev20190821\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\nThis bug might be related to the behavior observed by @cysmnl in https://github.com/tensorflow/tensorflow/issues/28923#issuecomment-514736847 . However I think the bug present is distinct from the original bug described in https://github.com/tensorflow/tensorflow/issues/28923.\r\n\r\n**Describe the current behavior**\r\nThe minimal working example below fails with the following exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"mwe.py\", line 33, in <module>\r\n    tf.saved_model.save(second_convolution_model, '/tmp/model2')  # does NOT work\r\n  File \".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 860, in save\r\n    meta_graph_def, saveable_view, signatures)\r\n  File \".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 590, in _fill_meta_graph_def\r\n    signatures = _generate_signatures(signature_functions, resource_map)\r\n  File \".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 464, in _generate_signatures\r\n    function, mapped_inputs, resource_map)\r\n  File \".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 416, in _call_function_with_mapped_captures\r\n    function.graph.captures, resource_map)\r\n  File \".../python3.7/site-packages/tensorflow_core/python/saved_model/save.py\", line 338, in _map_captures_to_created_tensors\r\n    .format(interior))\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"StatefulPartitionedCall/args_1:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\n\r\nAs a side note, the error message misses a space after the period: https://github.com/tensorflow/tensorflow/blob/d1583caf72add28ad997616fe4aa08b5b5181b71/tensorflow/python/saved_model/save.py#L334 \r\n\r\n**Describe the expected behavior**\r\nPossibility to save models with `tf.saved_model.save` as they would be saveable with `model.save`.\r\n\r\n**Code to reproduce the issue**\r\n(This is example is nonsensical, but shows the problem. I've boiled down the problem to a MWE.)\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import Model\r\nfrom tensorflow.python.keras.layers import Input, Conv1D, Lambda\r\n\r\nfirst_input = Input(shape=(1,))\r\nfirst_result = Conv1D(filters=1, kernel_size=1)(first_input[..., tf.newaxis])\r\nfirst_convolution_model = Model(inputs=[first_input], outputs=[first_result])\r\n\r\n\r\ndef inner_loop(tensor):\r\n    the_len = tf.shape(tensor)[0]\r\n    collector = tf.TensorArray(tf.float32, size=the_len)\r\n\r\n    _, collector = tf.while_loop(\r\n        cond=lambda i, _: i < the_len,\r\n        body=lambda i, c_: (i+1, c_.write(i, first_convolution_model(tensor[i:i + 1]))),\r\n        loop_vars=(0, collector)\r\n    )\r\n\r\n    return collector.stack()\r\n\r\n\r\nsecond_input = Input(shape=(1,))\r\nsecond_result = Lambda(inner_loop)(second_input)[..., 0]\r\nsecond_convolution_model = Model(inputs=[second_input], outputs=[second_result])\r\n\r\nprint(first_convolution_model.predict([1, 2, 3]))\r\nfirst_convolution_model.save('/tmp/model1.h5')  # works\r\ntf.saved_model.save(first_convolution_model, '/tmp/model1')  # works\r\n\r\nprint(second_convolution_model.predict([1, 2, 3]))\r\nfirst_convolution_model.save('/tmp/model2.h5')  # works\r\ntf.saved_model.save(second_convolution_model, '/tmp/model2')  # does NOT work\r\nprint(\"(not reached)\")\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue. Here's the colab [notebook](https://colab.research.google.com/gist/gowthamkpr/ba0632fc201dd096c95a70c6586a06f8/untitled106.ipynb)", "Lambda layers are not supposed to contain any state (e.g. variables, which the first model has).  Creating a custom layer would work:\r\n\r\n```\r\nclass InnerLoopLayer(tf.keras.layers.Layer):\r\n  def __init__(self, model):\r\n    self.model = model\r\n    super(InnerLoopLayer, self).__init__()\r\n\r\n  def call(self, tensor):\r\n    the_len = tf.shape(tensor)[0]\r\n    print(the_len)\r\n    collector = tf.TensorArray(tf.float32, size=the_len)\r\n\r\n    _, collector = tf.while_loop(\r\n        cond=lambda i, _: i < the_len,\r\n        body=lambda i, c_: (i+1, c_.write(i, self.model(tensor[i:i + 1]))),\r\n        loop_vars=(0, collector)\r\n    )\r\n\r\n    return collector.stack()\r\n\r\n\r\nsecond_input = Input(shape=(1,), batch_size=3)\r\nsecond_result = InnerLoopLayer(first_convolution_model)(second_input)[..., 0]\r\nsecond_convolution_model = Model(inputs=[second_input], outputs=[second_result])\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31893\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31893\">No</a>\n", "Actually, reopening because there appears to be an issue when loading the model. Looking into this.", "Thanks for the response! Interesting, I was puzzled because it worked at runtime \u2026\r\nhowever when I save a model like this, it still seems to be in an invalid state:\r\nActually, I'm using `tf.saved_model.save` because I want to use the Keras models with TensorFlow serving. When I serve the model generated using your `InnerLoopLayer`, I however get the following error:\r\n```bash\r\n# curl -X POST -H \"Content-Type: application/json\" -d '{\"inputs\":[[1,2,3]]}' http://localhost:8501/v1/models/1:predict\r\n{ \"error\": \"2 root error(s) found.\\n  (0) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\\n\\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\\n\\t [[StatefulPartitionedCall/_33]]\\n  (1) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\\n\\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\\n0 successful operations.\\n0 derived errors ignored.\" }\r\n```\r\n(Same call works for the first model).", "> Thanks for the response! Interesting, I was puzzled because it worked at runtime \u2026\r\n> however when I save a model like this, it still seems to be in an invalid state:\r\n> Actually, I'm using `tf.saved_model.save` because I want to use the Keras models with TensorFlow serving. When I serve the model generated using your `InnerLoopLayer`, I however get the following error:\r\n> \r\n> ```shell\r\n> # curl -X POST -H \"Content-Type: application/json\" -d '{\"inputs\":[[1,2,3]]}' http://localhost:8501/v1/models/1:predict\r\n> { \"error\": \"2 root error(s) found.\\n  (0) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\\n\\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\\n\\t [[StatefulPartitionedCall/_33]]\\n  (1) Invalid argument: PartialTensorShape: Incompatible shapes during merge: [?,1,1] vs. [1,3,1]\\n\\t [[{{node model_1/inner_loop_layer/TensorArrayV2Stack/TensorListStack}}]]\\n0 successful operations.\\n0 derived errors ignored.\" }\r\n> ```\r\n> \r\n> (Same call works for the first model).\r\n\r\nActually, I was calling it wrong apparently, I've retried today and it works as expected even with TensorFlow Serving, furthermore, with your suggestion I could even fix the problem in my own, larger code \u2026 thanks a lot!", "That's great! I've submitted a fix internally that resolves the bug when loading this model back into python, so re-closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31893\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31893\">No</a>\n"]}, {"number": 31892, "title": "java.lang.IllegalArgumentException: Expects arg[0] to be float but string is provided (java)", "body": "I have own trained model and use label image demo code.\r\n\r\n```\r\ntry (Graph graph = new Graph();\r\n        Session session = new Session(graph)) {\r\n        graph.importGraphDef(fileContent);\r\n        \r\n        try (Tensor<String> input = Tensors.create(imageBytes);\r\n            Tensor output =\r\n                session\r\n                    .runner()\r\n                    .feed(\"Placeholder\", input)\r\n                    .fetch(\"final_result\")\r\n                    .run()\r\n                    .get(0)) {\r\n          if (probabilities == null) {\r\n            probabilities = new float[(int) output.shape()[0]];\r\n          }\r\n          output.copyTo(probabilities);\r\n          int label = argmax(probabilities);\r\n          System.out.printf(\r\n              \"%-30s --> %-15s (%.2f%% likely)\\n\",\r\n              filename, labels.get(label), probabilities[label] * 100.0);\r\n        }\r\n```\r\nI see error `Exception in thread \"main\" java.lang.IllegalArgumentException: Expects arg[0] to be float but string is provided`. This model works perfect in python, but i have issue in java ", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31892\">No</a>\n", "Has this problem been solved\uff1fI also encountered this problem. When I used python training and golang deployment, I reported the following error.\r\n```\r\n2019-11-12 10:21:43.102960: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: ./static/1\r\n2019-11-12 10:21:43.111154: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2019-11-12 10:21:43.113390: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n2019-11-12 10:21:43.119036: I tensorflow/cc/saved_model/loader.cc:182] Restoring SavedModel bundle.\r\n2019-11-12 10:21:43.158766: I tensorflow/cc/saved_model/loader.cc:132] Running initialization op on SavedModel bundle.\r\n2019-11-12 10:21:43.163222: I tensorflow/cc/saved_model/loader.cc:285] SavedModel load for tags { serve }; Status: success. Took 60273 microseconds.\r\npanic: Expects arg[0] to be string but uint8 is provided\r\n```\r\nI can provide my training and loading code, as well as all version Numbers, if needed."]}, {"number": 31891, "title": "TF2.0   'NoneType' object has no attribute 'shape'", "body": "windows 10\r\nanaconda python 3.7 \r\n\r\n\r\n\r\n**Describe the current behavior**\r\n![image](https://user-images.githubusercontent.com/27112868/63515144-ba7cad80-c51c-11e9-93cb-167363519e4f.png)\r\n\r\nProblem \r\nLine 32 input_x[:,1] occurs the error.\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport time\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow as keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\nimport tensorflow.keras.backend as K\r\n\r\n\r\nfrom tensorflow.keras.utils import plot_model\r\nfrom scipy . stats import multivariate_normal as normal\r\n\r\ntf.keras.backend.set_floatx('float64')\r\n\r\n\r\ndef test_tf (x):\r\n    z = x[:,0]** 3 + x[:,1]**3\r\n    return z\r\n\r\n\r\ndef grad (y, x):\r\n    z = layers.Lambda ( lambda z : K.gradients(z[0], z[1]) ) ([y, x])\r\n    return z\r\n\r\ninput_x  = keras.Input( shape = (2))\r\ny =  test_tf (input_x)\r\n\r\nprint(\"y = \", y )\r\nprint (\"input_x[0] = \", input_x[:1])\r\n\r\nz0 = grad(y, input_x[:,1])\r\n\r\nprint(\"z0 = \", z0)\r\n\r\nmodel =  keras.Model(inputs = [input_x], outputs = [z0] )\r\n\r\nprint (\"gradient  = \", model.predict(  tf.constant( [[2,3]]  )   ))\r\n\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The error is caused in the line `z0 = grad(y, input_x[:,1])` as there is a shape mismatch\r\nPlese find the [github gist](https://colab.research.google.com/gist/gowthamkpr/a1acdcf518d8930f81cde19acfe98ac3/gradients.ipynb) where I have solved this problem and let me know if it helps. Thanks!", "No, this  is not my expected solution.\r\n\r\nI know  that   z0 = grad(y, input_x)  will make the code run.\r\n\r\nWhat  wanted is that   we only want part of derivatives with input_x,\r\nso  I write it as    z0 = grad(y, input_x[:. 0]).     \r\n\r\nCould you solve this bug   without changing Z0 = grad(y, input_x[:,0]) ?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "Hi @zwenju, to my best knowledge, it's impossible to make your code run without changing `z0 = grad(y, input_x[:, 0])`.\r\n\r\nSince *slicing operations* create a new tensor, if you try to compute gradients from the return value of `input[:, 0]`, you will get the value of `None` because of unconnected computational graph. The solution is to compute gradients w.r.t. `x`, and slice whatever you want from it. So the runnable example should be like this:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ntf.keras.backend.set_floatx('float64')\r\n\r\n\r\ndef test_tf(x):\r\n    z = x[:, 0] ** 3 + x[:, 1]**3\r\n    return z\r\n\r\n\r\ninput_x = keras.Input(shape=(2))\r\ny = layers.Lambda(test_tf)(input_x)\r\n\r\n\r\ndef grad_fn(w):\r\n    z = K.gradients(w[0], w[1])[0][:, 0]\r\n    return z\r\n\r\n\r\nz0 = layers.Lambda(grad_fn)([y, input_x])\r\n\r\nmodel = keras.Model(inputs=[input_x], outputs=[z0])\r\n\r\nprint(\"gradient  = \", model.predict(tf.constant([[2, 3]])))\r\n```\r\n\r\nBTW, the example without functional API is in the following:\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef fn(x):\r\n    return x[0] ** 2 + x[1] ** 2\r\n\r\n\r\nx = tf.constant([1., 2.])\r\n\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(x)\r\n    w = x[0]\r\n    tape.watch(w)\r\n    y = fn(x)\r\n\r\ngrads = tape.gradient(y, [x, w])\r\nprint(grads)\r\n```\r\nIf you run it, you will see the gradients w.r.t. `x` is [2.0, 4.0] while the one w.r.t. `w` is `None`. Thanks.", "Thanks\uff0c \r\nIn  issue  #31516\uff0c  similarly,  Input[:,0] creates a connected computational graph.\r\nIn this part,  it may exist a  non-consistency between **the slicing operation** and the the **gradient functions.**\r\nCould you further consider it ???\r\n\r\n\r\nSince slicing operations create a new tensor, if you try to compute gradients from the return value of input[:, 0], you will get the value of None because of unconnected computational graph.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "I do not really understand how two examples are related. If you plot your model in #31516, you can find that paths from prediction to `model.trainable_variables`, that is, batch normalization and dense layer (`BLOCK_*_{BN,DENSE}` in the following figure), do not go through any sliced tensor/op. Therefore the gradients are computed safely without obtaining any `None` value. However, if you try to compute gradient w.r.t. to `X` or `dW`, gradients will still be `None`.\r\n\r\n![test](https://user-images.githubusercontent.com/11615393/63670387-15f5b680-c80f-11e9-98f4-0e41fdb6f9af.png)", "thanks WinQAQ, thanks for you solution.\r\nGreat!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31891\">No</a>\n"]}, {"number": 31890, "title": "[tflite] Fix Unpack operator UINT8 quantisation", "body": "When trying to quantise the networks with Unpack/Unstack operator for UINT8 inference, the conversion process completes without errors, or warnings, but at inference, the tensor allocation fails with the following runtime error:\r\n\r\n`RuntimeError: tensorflow/lite/kernels/unpack.cc:71 input->params.zero_point != output->params.zero_point (0 != 128)Node number 0 (UNPACK) failed to prepare.`\r\n\r\nTo reproduce the problem, the following script can be used:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# tf Graph Input\r\nin_stacked = tf.compat.v1.placeholder(\"float32\", [2, 10])\r\nout_unstacked = tf.unstack(in_stacked, axis=0)\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n\ttf.io.write_graph(tf.compat.v1.get_default_graph(), '.','unpack.pb', as_text=False)\r\n\r\n# conversion with UINT8 quantisation\r\ninput_name = [\"Placeholder\"]\r\noutput_name = [\"unstack\", \"unstack:1\"]\r\ntflite_model_name = \"uint8_unpack.tflite\"\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\"unpack.pb\", input_name, output_name)\r\nconverter.quantized_input_stats = {input_name[0] : (0., 1.)}\r\nconverter.default_ranges_stats = (-50, 50)\r\nconverter.inference_type = tf.uint8\r\ntflite_model = converter.convert()\r\nopen(tflite_model_name, \"wb\").write(tflite_model)\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(tflite_model_name)\r\ninterpreter.allocate_tensors() # <-- Runtime error\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\n\r\n```\r\n\r\nThe following is the sample output of the script above without this patch:\r\n```\r\n2019-08-21 17:17:05.016390: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3492095000 Hz\r\n2019-08-21 17:17:05.032340: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557b5c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-08-21 17:17:05.032380: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"TF_Tests/check_unpack.py\", line 79, in <module>\r\n    uint8_quantization()\r\n  File \"TF_Tests/check_unpack.py\", line 63, in uint8_quantization\r\n    interpreter.allocate_tensors()\r\n  File \"/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/antkac01/lab/mlt/venv-runtime/lib/python3.5/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/unpack.cc:71 input->params.zero_point != output->params.zero_point (0 != 128)Node number 0 (UNPACK) failed to prepare.\r\n\r\n```\r\n\r\nThis fix contains the following modifications:\r\n1. Input quantisation parameters are propagated to the output for Unpack operator, similar to other re-arrangement ops.\r\n2. Unit test for checking quantization parameters preservation is added", "comments": []}, {"number": 31889, "title": "how to add new label(say mobilephone) to the existing Image classification pre trained model :android", "body": "\r\nI'm working on Android project to use Image classification , my requirement is to add new label (say mobilephone )to the existing Image classification(detection) pre trained model , is that possible ? If yes how to do that . I see when we train new category it does not add to the existing pre trained model ,example referred is mentioned below \r\n\r\nhttps://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#8\r\n\r\n", "comments": ["Can you clarify the question? It sounds like you want to extend the original model's classes to include your new class, rather than replacing the original model's classes with your new class?", "Yes, I want to extend the pre trained model's(https://www.tensorflow.org/lite/guide/hosted_models) provided by TensorFlow with my new class ex: mobilephones .", "Got it. It's unfortunately not quite as easy to _add_ a class to an existing image classification model as it is to _replace_ the original classes.\r\n\r\nYou'll need to add an additional element to the output tensor and then fine-tune the model using data for all of the classes, along with your new class's data.\r\n\r\nThere's some discussion of the technique in this StackOverflow answer:\r\n\r\nhttps://datascience.stackexchange.com/questions/15656/how-to-add-a-new-category-to-a-deep-learning-model\r\n\r\nIt sounds like an example of doing this would be helpful, so I will add this to our feature requests. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31889\">No</a>\n"]}, {"number": 31888, "title": "[tflite] Fix for CalibratorTest", "body": "CalibratorTest.test_invalid_model_buffer and\r\nCalibratorTest.test_invalid_shape_calibrator_gen fail when called using the test target:\r\n//tensorflow/lite/python/... and python3.\r\n\r\nLog error message for test_invalid_model_buffer test:\r\n\r\n> ======================================================================\r\n> ERROR: test_invalid_model_buffer (__main__.CalibratorTest)\r\n> test_invalid_model_buffer (__main__.CalibratorTest)\r\n> ----------------------------------------------------------------------\r\n> Traceback (most recent call last):\r\n>   File \"/home/antkac01/.cache/bazel/_bazel_antkac01/3a4b9f64af38ced70a092937bf072e49/execroot/org_tensorflow/bazel-out/k8-py2-opt/bin/tensorflow/lite/python/optimize/calibrator_test.runfiles/org_tensorflow/tensorflow/lite/python/optimize/calibrator_test.py\", line 83, in test_invalid_model_buffer\r\n>     with self.assertRaisesWithRegexpMatch(ValueError,\r\n> AttributeError: 'CalibratorTest' object has no attribute 'assertRaisesWithRegexpMatch'\r\n> \r\n\r\nThe function assertRaisesRegex is provided by unittest module and it looks that it should be used here.\r\nThe old function is not found in the class CalibratorTest.", "comments": ["@akarmi Could you please check reviewer comments and keep us posted. Thanks!", "@gbaned We have addressed the reviewer's comments. Thanks.", "@akarmi  Could you please resolve the conflicts? Thanks!", "> \r\n> \r\n> @akarmi Could you please resolve the conflicts? Thanks!\r\n\r\n@gbaned, done, ready to merge."]}, {"number": 31887, "title": "//tensorflow/contrib/distributions/python/kernel_tests/independent_test.py fails  with assertion error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 s390x\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.15\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n```\r\npython tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\r\n======================================================================\r\nFAIL: testMnistLikeDynamicShape (__main__.ProductDistributionTest)\r\ntestMnistLikeDynamicShape (__main__.ProductDistributionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 275, in testMnistLikeDynamicShape\r\n    self._testMnistLike(static_shape=False)\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 269, in _testMnistLike\r\n    rtol=1e-6, atol=0.)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 1073, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2303, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2272, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2207, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 1501, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 827, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=0\r\nMismatched value: a is different from b.\r\nnot close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))\r\nnot close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]\r\nnot close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]\r\nnot close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]\r\nnot close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]\r\ndtype = float64, shape = (4, 5, 10)\r\nMismatch: 2.5%\r\nMax absolute difference: 0.00059155\r\nMax relative difference: 1.29257756e-06\r\n x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,\r\n         -456.784984, -448.14827 , -453.583166, -486.295655,\r\n         -468.533898, -481.740375],...\r\n y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,\r\n         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],\r\n        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...\r\n\r\n======================================================================\r\nFAIL: testMnistLikeStaticShape (__main__.ProductDistributionTest)\r\ntestMnistLikeStaticShape (__main__.ProductDistributionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 272, in testMnistLikeStaticShape\r\n    self._testMnistLike(static_shape=True)\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 269, in _testMnistLike\r\n    rtol=1e-6, atol=0.)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 1073, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2303, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2272, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2207, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 1501, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 827, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=0\r\nMismatched value: a is different from b.\r\nnot close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))\r\nnot close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]\r\nnot close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]\r\nnot close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]\r\nnot close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]\r\ndtype = float64, shape = (4, 5, 10)\r\nMismatch: 2.5%\r\nMax absolute difference: 0.00059155\r\nMax relative difference: 1.29257756e-06\r\n x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,\r\n         -456.784984, -448.14827 , -453.583166, -486.295655,\r\n         -468.533898, -481.740375],...\r\n y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,\r\n         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],\r\n        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...\r\n\r\n----------------------------------------------------------------------\r\nRan 10 tests in 1.036s\r\n\r\nFAILED (failures=2)\r\n\r\n```\r\n**Describe the expected behavior**\r\n```\r\nThe test should pass on s390x.\r\n```\r\n**Code to reproduce the issue**\r\n```\r\npython tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe following solution resolves the issue.\r\n``` diff\r\n--- a/tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\r\n+++ b/tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\r\n@@ -20,7 +20,7 @@ from __future__ import print_function\r\n\r\n import importlib\r\n import numpy as np\r\n-\r\n+import sys\r\n from tensorflow.contrib.distributions.python.ops import independent as independent_lib\r\n from tensorflow.contrib.distributions.python.ops import mvn_diag as mvn_diag_lib\r\n from tensorflow.python.framework import dtypes\r\n@@ -231,9 +231,15 @@ class ProductDistributionTest(test.TestCase):\r\n     def expected_log_prob(x, logits):\r\n       return (x * logits - np.log1p(np.exp(logits))).sum(-1).sum(-1).sum(-1)\r\n\r\n-    with self.cached_session() as sess:\r\n-      logits_ph = array_ops.placeholder(\r\n-          dtypes.float32, shape=logits.shape if static_shape else None)\r\n+      with self.cached_session() as sess:\r\n+\r\n+       if sys.byteorder == \"big\":\r\n+            logits_ph = array_ops.placeholder(\r\n+            dtypes.float64, shape=logits.shape if static_shape else None)\r\n+       else:\r\n+            logits_ph = array_ops.placeholder(\r\n+            dtypes.float32, shape=logits.shape if static_shape else None)\r\n+\r\n       ind = independent_lib.Independent(\r\n           distribution=bernoulli_lib.Bernoulli(logits=logits_ph))\r\n       x = ind.sample(sample_shape, seed=42)\r\n```\r\nThe test passes with the above change on s390x as well as x86.\r\nCould you please comment if this change is appropriate?", "comments": ["Discussion going on a similar fix in this [PR ](https://github.com/tensorflow/tensorflow/pull/26692)"]}, {"number": 31886, "title": "[tf.data] replace flat_structure with _flat_structure", "body": "@jsimsa \r\n\r\nThis is a bug fix from JIZHI, the AI platform in Tencent.\r\n\r\nIn TF r1.14, _flat_structure is used in file tensorflow/python/data/ops/readers.py, but its definition has not been updated in file tensorflow/python/data/ops/dataset_ops.py.\r\n\r\nreplace all flat_structure with _flat_structure to fix the error like below:\r\n  File \"transformer_main.py\", line 670, in <module>\r\n    absl_app.run(main)\r\n  File \"/usr/lib/python2.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/lib/python2.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"transformer_main.py\", line 664, in main\r\n    run_transformer(flags.FLAGS)\r\n  File \"transformer_main.py\", line 644, in run_transformer\r\n    vocab_file=flags_obj.vocab_file)\r\n  File \"transformer_main.py\", line 343, in run_loop\r\n    hooks=train_hooks)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1156, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1219, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1255, in _actual_train_model_distributed\r\n    input_fn, ModeKeys.TRAIN, strategy)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1009, in _get_iterator_from_input_fn\r\n    lambda input_context: self._call_input_fn(input_fn, mode,\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 774, in make_input_fn_iterator\r\n    input_fn, replication_mode)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 406, in make_input_fn_iterator\r\n    input_fn, replication_mode=replication_mode)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/distribute/one_device_strategy.py\", line 100, in _make_input_fn_iterator\r\n    self._container_strategy())\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/distribute/input_lib.py\", line 550, in __init__\r\n    result = input_fn(ctx)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1010, in <lambda>\r\n    input_context))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1113, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"/dockerdata/xinan/tensorflow/pub/models/official/transformer/utils/dataset.py\", line 272, in train_input_fn\r\n    repeat=params[\"repeat_dataset\"], static_batch=params[\"static_batch\"])\r\n  File \"/dockerdata/xinan/tensorflow/pub/models/official/transformer/utils/dataset.py\", line 227, in _read_and_batch_from_files\r\n    _load_records, sloppy=shuffle, cycle_length=num_parallel_calls))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1853, in apply\r\n    return DatasetV1Adapter(super(DatasetV1, self).apply(transformation_func))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1290, in apply\r\n    dataset = transformation_func(self)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/experimental/ops/interleave_ops.py\", line 94, in _apply_fn\r\n    buffer_output_elements, prefetch_input_elements)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/readers.py\", line 253, in __init__\r\n    **self._flat_structure)\r\nAttributeError: 'ParallelInterleaveDataset' object has no attribute '_flat_structure'", "comments": ["Can you check if you get the same error in master?", "Also, we don't accept PRs against release branches after the release, since we're not doing patch releases except for security purposes.\r\n\r\nIf the issue manifests on master, please open a PR there. Then, after merging, if the issue is significant enough that it warrants fixing in 2.0 and 1.15 (the current releases yet to be releases) then please create cherry-picks to these branches (and mention in the PR that this is a cherrypick)."]}]