[{"number": 46431, "title": "Blas SGEMM launch failed : m=25600, n=64, k=64 [Op:Conv2D] when building model and restoring weights", "body": "\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: tf-nightly-gpu 2.4.0-dev20201016\r\n-   **Python version**: 3.8.5\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:  Cuda = 11.2 \r\n-   **GPU model and memory**: GeForce RTX 2060 6000MB\r\n-   **Exact command to reproduce**: \r\n\r\nnum_classes = 1\r\npipeline_config = 'object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\r\ncheckpoint_path = 'object_detection/test_data/checkpoint/ckpt-0'\r\n\r\n\r\nconfigs = config_util.get_configs_from_pipeline_file(pipeline_config)\r\nmodel_config = configs['model']\r\nmodel_config.ssd.num_classes = num_classes\r\nmodel_config.ssd.freeze_batchnorm = True\r\ndetection_model = model_builder.build(\r\n      model_config=model_config, is_training=True)\r\n\r\nfake_box_predictor = tf.compat.v2.train.Checkpoint(\r\n    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\r\n    # _prediction_heads=detection_model._box_predictor._prediction_heads,\r\n    #    (i.e., the classification head that we *will not* restore)\r\n    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\r\n    )\r\nfake_model = tf.compat.v2.train.Checkpoint(\r\n          _feature_extractor=detection_model._feature_extractor,\r\n          _box_predictor=fake_box_predictor)\r\nckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\r\nckpt.restore(checkpoint_path).expect_partial()\r\n\r\nimage, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\r\nprediction_dict = detection_model.predict(image, shapes)\r\n_ = detection_model.postprocess(prediction_dict, shapes)\r\n\r\n\r\nIt fails ath the second last line\r\n", "comments": ["I have the same problem.My error message\r\n2021-01-15 10:37:37.265487: E tensorflow/stream_executor/cuda/cuda_blas.cc:428] failed to run cuBLAS routine: CUBLAS_STATUS_INTERN                                                 AL_ERROR\r\nInternal: 2 root error(s) found.\r\n  (0) Internal: Blas GEMM launch failed : a.shape=(135, 1296), b.shape=(1296, 120), m=135, n=120, k=1296\r\n         [[{{node layer_0_type_0/MatMul}}]]\r\n         [[o_energy/_43]]\r\n  (1) Internal: Blas GEMM launch failed : a.shape=(135, 1296), b.shape=(1296, 120), m=135, n=120, k=1296\r\n         [[{{node layer_0_type_0/MatMul}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nHave I written custom code (as opposed to using a stock example script\r\nprovided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7.6\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\nhappens on a mobile device:\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): tf1.14\r\nPython version: 3.7.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: Cuda = 10.1\r\nGPU model and memory: V100 16130MB", "@Echambouleyron\r\nCould you please try with cuda 11.0 and let us know.\r\n\r\nzhangbei07 \r\nwe see that you are using a old version of tf, there is no support for 1.x please upgrade to 2.x, verify the cuda version compatibility [here](https://www.tensorflow.org/install/source) and let us know if you face any issues.", "@Saduf2019 This worked. No need to downgrade Cuda version to 11.0. Many thanks.\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)", "@Echambouleyron \r\nThank you for your update, glad the issue is resolved, please move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46431\">No</a>\n"]}, {"number": 46430, "title": "Error when trying to save transformer model to saved model format.", "body": "Hello,\r\n\r\nI am following transformer example below\r\n\r\nhttps://www.tensorflow.org/tutorials/text/transformer\r\n\r\nEverything is working as expected and saving checkpoint as well, and I want to now save this to SavedModel format using \"transformer.save()\"  but it is throwing the below error.\r\n\r\n**TypeError: call() missing 4 required positional arguments: 'tar', 'enc_padding_mask', 'look_ahead_mask', and 'dec_padding_mask'**\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-107-92d91001b9bf> in <module>\r\n----> 1 transformer.save('./saved_model/')\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in save(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n   1976     ```\r\n   1977     \"\"\"\r\n-> 1978     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n   1979                     signatures, options)\r\n   1980 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py in save_model(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\r\n    131         model, filepath, overwrite, include_optimizer)\r\n    132   else:\r\n--> 133     saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n    134                           signatures, options)\r\n    135 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save.py in save(model, filepath, overwrite, include_optimizer, signatures, options)\r\n     78     # we use the default replica context here.\r\n     79     with distribution_strategy_context._get_default_replica_context():  # pylint: disable=protected-access\r\n---> 80       save_lib.save(model, filepath, signatures, options)\r\n     81 \r\n     82   if not include_optimizer:\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    973   meta_graph_def = saved_model.meta_graphs.add()\r\n    974 \r\n--> 975   _, exported_graph, object_saver, asset_info = _build_meta_graph(\r\n    976       obj, export_dir, signatures, options, meta_graph_def)\r\n    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)\r\n   1044   checkpoint_graph_view = _AugmentedGraphView(obj)\r\n   1045   if signatures is None:\r\n-> 1046     signatures = signature_serialization.find_function_to_export(\r\n   1047         checkpoint_graph_view)\r\n   1048 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)\r\n     73   # If the user did not specify signatures, check the root object for a function\r\n     74   # that can be made into a signature.\r\n---> 75   functions = saveable_view.list_functions(saveable_view.root)\r\n     76   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)\r\n     77   if signature is not None:\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj, extra_functions)\r\n    142     obj_functions = self._functions.get(obj, None)\r\n    143     if obj_functions is None:\r\n--> 144       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n    145           self._serialization_cache)\r\n    146       self._functions[obj] = obj_functions\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)\r\n   2587     self.test_function = None\r\n   2588     self.predict_function = None\r\n-> 2589     functions = super(\r\n   2590         Model, self)._list_functions_for_serialization(serialization_cache)\r\n   2591     self.train_function = train_function\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)\r\n   3016 \r\n   3017   def _list_functions_for_serialization(self, serialization_cache):\r\n-> 3018     return (self._trackable_saved_model_saver\r\n   3019             .list_functions_for_serialization(serialization_cache))\r\n   3020 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)\r\n     85         `ConcreteFunction`.\r\n     86     \"\"\"\r\n---> 87     fns = self.functions_to_serialize(serialization_cache)\r\n     88 \r\n     89     # The parent AutoTrackable class saves all user-defined tf.functions, and\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)\r\n     76 \r\n     77   def functions_to_serialize(self, serialization_cache):\r\n---> 78     return (self._get_serialized_attributes(\r\n     79         serialization_cache).functions_to_serialize)\r\n     80 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)\r\n     92       return serialized_attr\r\n     93 \r\n---> 94     object_dict, function_dict = self._get_serialized_attributes_internal(\r\n     95         serialization_cache)\r\n     96 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n     49     # cache (i.e. this is the root level object).\r\n     50     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:\r\n---> 51       default_signature = save_impl.default_save_signature(self.obj)\r\n     52 \r\n     53     # Other than the default signature function, all other attributes match with\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)\r\n    203   original_losses = _reset_layer_losses(layer)\r\n    204   fn = saving_utils.trace_model_call(layer)\r\n--> 205   fn.get_concrete_function()\r\n    206   _restore_layer_losses(original_losses)\r\n    207   return fn\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n   1165       ValueError: if this object has not yet been called on concrete values.\r\n   1166     \"\"\"\r\n-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access\r\n   1169     return concrete\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n   1071       if self._stateful_fn is None:\r\n   1072         initializers = []\r\n-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)\r\n   1074         self._initialize_uninitialized_variables(initializers)\r\n   1075 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    695     self._concrete_stateful_fn = (\r\n--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    697             *args, **kwds))\r\n    698 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3063     arg_names = base_arg_names + missing_arg_names\r\n   3064     graph_function = ConcreteFunction(\r\n-> 3065         func_graph_module.func_graph_from_py_func(\r\n   3066             self._name,\r\n   3067             self._python_function,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)\r\n    132     with base_layer_utils.call_context().enter(\r\n    133         model, inputs=inputs, build_graph=False, training=False, saving=True):\r\n--> 134       outputs = model(inputs, training=False)\r\n    135 \r\n    136     # Outputs always has to be a flat dict.\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    983 \r\n    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n    986 \r\n    987         if self._activity_regularizer:\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    300   def wrapper(*args, **kwargs):\r\n    301     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 302       return func(*args, **kwargs)\r\n    303 \r\n    304   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nTypeError: call() missing 4 required positional arguments: 'tar', 'enc_padding_mask', 'look_ahead_mask', and 'dec_padding_mask'\r\n\r\n\r\n\r\n", "comments": ["@bmblr497 \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46430\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46430\">No</a>\n"]}, {"number": 46429, "title": "[tf-nightly] `keras.callbacks.EarlyStopping` stops training immediately after the first epoch even if monitored score has improved", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n### Code to reproduce \r\n\r\n```python\r\nimport tensorflow as tf\r\nimport keras\r\n\r\nfrom unittest import mock\r\nfrom functools import reduce\r\n\r\n\r\ndef print_attr(obj, attr):\r\n    print(attr.ljust(20), \":\", reduce(lambda a, b: getattr(a, b), attr.split(\".\"), obj))\r\n\r\n\r\ndef print_divider(title):\r\n    print(f\"\\n========== {title} ==========\")\r\n\r\n\r\ndef print_es_attrs(es):\r\n    for attr in [\r\n        \"patience\",\r\n        \"stopped_epoch\",\r\n        \"wait\",\r\n        \"best\",\r\n        \"best_weights\",\r\n        \"model.stop_training\",\r\n    ]:\r\n        print_attr(es, attr)\r\n\r\n\r\nes = keras.callbacks.EarlyStopping(\r\n    monitor=\"loss\", mode=\"min\", patience=0, restore_best_weights=True, verbose=1\r\n)\r\nes.model = mock.MagicMock(stop_training=False)\r\n\r\nprint_divider(\"TF and Keras version\")\r\nprint(\"tf:\", tf.version.GIT_VERSION, tf.version.VERSION)\r\nprint(\"keras:\", keras.__version__)\r\n\r\n\r\nes.on_train_begin()\r\nprint_divider(\"After train_begin\")\r\nprint_es_attrs(es)\r\n\r\nprint_divider(\"Call on_epoch_end\")\r\nes.on_epoch_end(epoch=0, logs={\"loss\": 1.0})\r\n\r\nprint_divider(\"After on_epoch_end\")\r\nprint_es_attrs(es)\r\n```\r\n\r\nThis prints out:\r\n\r\n```diff\r\n  ========== TF and Keras version ==========\r\n+ tf: v1.12.1-48978-gad524691510 2.5.0-dev20210114 <- using tf-nightly\r\n  keras: 2.4.3\r\n  \r\n  ========== After train_begin ==========\r\n  patience             : 0\r\n  stopped_epoch        : 0\r\n  wait                 : 0\r\n+ best                 : inf\r\n  best_weights         : None\r\n+ model.stop_training  : False\r\n  \r\n  ========== Call on_epoch_end ==========\r\n  Restoring model weights from the end of the best epoch.\r\n  \r\n  ========== After on_epoch_end ==========\r\n  patience             : 0\r\n  stopped_epoch        : 0\r\n  wait                 : 0\r\n+ best                 : 1.0\r\n  best_weights         : <MagicMock name='mock.get_weights()' id='140324772920720'>\r\n+ model.stop_training  : True  <- should this be False because score has improved from inf to 1.0?\r\n```\r\n\r\nSorry if I'm wrong but is this intended behavior?", "comments": ["`tensorflow==2.4.0` gave:\r\n\r\n```diff\r\n  ========== TF and Keras version ==========\r\n  tf: v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n  keras: 2.4.3\r\n\r\n  ========== After train_begin ==========\r\n  patience             : 0\r\n  stopped_epoch        : 0\r\n  wait                 : 0\r\n+ best                 : inf\r\n  best_weights         : None\r\n+ model.stop_training  : False\r\n  \r\n  ========== Call on_epoch_end ==========\r\n  Restoring model weights from the end of the best epoch.\r\n  \r\n  ========== After on_epoch_end ==========\r\n  patience             : 0\r\n  stopped_epoch        : 0\r\n  wait                 : 0\r\n+ best                 : 1.0\r\n  best_weights         : <MagicMock name='mock.get_weights()' id='140324772920720'>\r\n+ model.stop_training  : False\r\n```", "https://github.com/tensorflow/tensorflow/blob/abcabe12666d064da4710d965c36a59c937eab12/tensorflow/python/keras/callbacks.py#L1766-L1790\r\n\r\nAt line 1783, if both `self.wait` and `self.patience` are 0, `self.wait >= self.patience` returns True, then training early stops.", "Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/287a3a54ee2ab01da24f28b06a6e0ae1/46429-2-3.ipynb), [TF v2.4](https://colab.research.google.com/gist/amahendrakar/66188c31e71e58fb365e7556a2d6cbe7/46429.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/8bfa154e314e3e5c8ccd76c9b76ff2b6/46429-2-5.ipynb). Please find the gist of it here. Thanks!", "@amahendrakar Thanks!", "@jvishnuvardhan Does tensorflow 2.5.0 contain a patch for this issue?", "@harupy It looks like this is intended behavior. Please check the source code here https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/callbacks.py#L1801-L1803\r\n\r\nDuring `on_train_begin`, self.wait is set as `0`. On `on_epoch_end`, self.wait += 1. \r\n\r\nSo `on_epoch_end`, the above condition becomes `True` and `stop_training` (self.model.stop_training = True) flag is set as True (which is what you are reporting). [Here](https://colab.research.google.com/gist/jvishnuvardhan/b0674feb0ef2e7221546f0a172827762/untitled.ipynb) is a gist for our reference. Please let us know what you think. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thanks@jvishnuvardhan  for the explanation! Looks like this is an intended behavior.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46429\">No</a>\n"]}, {"number": 46427, "title": "Error when load model with layer tensorflow.keras.layers.experimental.preprocessing.Normalization()", "body": "Hello,\r\nI have an issue with tensorflow.keras.layers.experimental.preprocessing.Normalization(). \r\nI can't load my model when I use it. \r\n\r\n\r\n**System information**\r\n- Have I custom un example script provided TensorFlow code\r\n- Linux Ubuntu 20.04\r\n- TensorFlow installed from binary\r\n\r\n```\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Jan 14 12:47:29 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.27.04    Driver Version: 460.27.04    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 166...  On   | 00000000:08:00.0  On |                  N/A |\r\n|  0%   49C    P8    12W / 120W |    535MiB /  5941MiB |      7%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1217      G   /usr/lib/xorg/Xorg                346MiB |\r\n|    0   N/A  N/A      1943      G   /usr/bin/kwin_x11                  57MiB |\r\n|    0   N/A  N/A      1956      G   /usr/bin/plasmashell               72MiB |\r\n|    0   N/A  N/A      2367      G   ...gAAAAAAAAA --shared-files       10MiB |\r\n|    0   N/A  N/A      3078      G   ...AAAAAAAA== --shared-files       41MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0.221\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-11.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-11.0/doc/man/man7/libcudart.so.7\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.4.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/user/works/test-libs/venv/lib/python3.8/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 5, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\nI can't load my model when I use it. \r\n\r\n\r\n**Describe the expected behavior**\r\nI would like to be able to read a model with this layer. I know it's an experimental layer but it should work.  If you have no idea how to solve it, can you advise me of an alternative?\r\n\r\nThank you :-)\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers.experimental import preprocessing\r\n\r\n\r\ndata = [{\"a\": 20.0, \"b\": 5.0, \"c\": 0.2972786923202068}, {\"a\": 20.0, \"b\": 10.0, \"c\": 0.10673704592967688}]\r\ntrain_dataset = pd.DataFrame.from_dict(data)\r\n\r\ntrain_features = train_dataset.copy()\r\ntrain_labels = train_features.pop('c')\r\n\r\nnormalizer = preprocessing.Normalization()\r\nnormalizer.adapt(np.array(train_features))\r\n\r\ndnn_model = keras.Sequential([\r\n    normalizer,\r\n    layers.Dense(64, activation='relu'),\r\n    layers.Dense(64, activation='relu'),\r\n    layers.Dense(1)\r\n])\r\n\r\ndnn_model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\r\ndnn_model.save('file.h5')\r\n\r\n\r\n# ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\nnew_model = tf.keras.models.load_model('file.h5')\r\n```\r\n\r\n\r\n**logs**\r\n```\r\n2021-01-14 12:27:41.950145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-14 12:27:42.758932: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-14 12:27:42.759555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-01-14 12:27:42.796836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:42.797239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\ncoreClock: 1.8GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\r\n2021-01-14 12:27:42.797264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-14 12:27:42.798974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-14 12:27:42.799021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-14 12:27:42.799743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-14 12:27:42.799901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-14 12:27:42.801635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-14 12:27:42.802036: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-01-14 12:27:42.802131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-01-14 12:27:42.802239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:42.802663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:42.802998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-14 12:27:42.803643: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-14 12:27:42.803719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:42.804062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\ncoreClock: 1.8GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\r\n2021-01-14 12:27:42.804079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-14 12:27:42.804092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-14 12:27:42.804101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-14 12:27:42.804110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-14 12:27:42.804119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-14 12:27:42.804128: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-14 12:27:42.804136: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-01-14 12:27:42.804145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-01-14 12:27:42.804191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:42.804558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:42.804884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-14 12:27:42.804907: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-14 12:27:43.183218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-14 12:27:43.183266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-01-14 12:27:43.183273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-01-14 12:27:43.183488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:43.183873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:43.184214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-14 12:27:43.184529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4868 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5)\r\nTraceback (most recent call last):\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2873, in zeros\r\n    shape = constant_op._tensor_shape_tensor_conversion_function(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 355, in _tensor_shape_tensor_conversion_function\r\n    raise ValueError(\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (None,)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"save.py\", line 30, in <module>\r\n    new_model = tf.keras.models.load_model('file.h5')\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\", line 206, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 183, in load_model_from_hdf5\r\n    model = model_config_lib.model_from_config(model_config,\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py\", line 64, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\", line 173, in deserialize\r\n    return generic_utils.deserialize_keras_object(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 354, in deserialize_keras_object\r\n    return cls.from_config(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 494, in from_config\r\n    model.add(layer)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 517, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 223, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 951, in __call__\r\n    return self._functional_construction_call(inputs, args, kwargs,\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1090, in _functional_construction_call\r\n    outputs = self._keras_tensor_symbolic_call(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 822, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 862, in _infer_output_signature\r\n    self._maybe_build(inputs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2710, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/preprocessing/normalization.py\", line 174, in build\r\n    self.mean = self._add_state_variable(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py\", line 110, in _add_state_variable\r\n    weight = self.add_weight(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 623, in add_weight\r\n    variable = self._add_variable_with_custom_getter(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 805, in _add_variable_with_custom_getter\r\n    new_variable = getter(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 130, in make_variable\r\n    return tf_variables.VariableV1(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 260, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 206, in _variable_v1_call\r\n    return previous_getter(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 199, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\", line 2604, in default_variable_creator\r\n    return resource_variable_ops.ResourceVariable(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1574, in __init__\r\n    self._init_from_args(\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1712, in _init_from_args\r\n    initial_value = initial_value()\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py\", line 114, in __call__\r\n    return array_ops.zeros(shape, dtype)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2819, in wrapped\r\n    tensor = fun(*args, **kwargs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 2877, in zeros\r\n    shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1540, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 339, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/user/works/test-libs/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\r\n\r\n```\r\n", "comments": ["I have tried in colab with TF-GPU version 2.4, nightly version(`2.5.0-dev20210114`) and I am not seeing any issue.Colab uses cuda 10.1.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/f63d9dea8f90ba37ba2ea9f594e79cd5/untitled0.ipynb).Thanks!\r\n`", "Hello,\r\nThank you for your reply.\r\nDid you run the last line of the program? \r\nIn your dashboard I do not see this line ...\r\nThe problem is in the load model function.\r\n\r\n", "I just added it to your [colab](https://colab.research.google.com/gist/ravikyram/f63d9dea8f90ba37ba2ea9f594e79cd5/untitled0.ipynb) and the problem is the same as on my computer.\r\n\r\n``` python\r\nnew_model = tf.keras.models.load_model('file.h5')\r\n```", "I have tried in colab with TF nightly version and was able to reproduce the issue .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/37a8c56e964e09ddb33e36e74ff1ae74/untitled623.ipynb).Thanks!", "@manuel-masiello Looks like there is an issue when saving in `h5` format. However, there was no issue when `tf` format is used for saving the model. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d574813ec3faf5530e6ffa2159be3858/untitled623.ipynb). Thanks!\r\n\r\n```\r\n#dnn_model.save('file.h5')\r\ndnn_model.save('file',save_format='tf')\r\n\r\nnew_model = tf.keras.models.load_model('file')\r\n```", "Hello. Thanks for finding me an alternative.  :-)\r\nI will modify my code. \r\n\r\nFYI, I just tested with Json serialization and the error also exists. Please found the [gist here](https://colab.research.google.com/drive/1KXpiuHnnfOspsftzBjt5HJvD7a4_XLG8?usp=sharing).\r\n\r\n\r\n```python\r\njson_config = dnn_model.to_json()\r\nnew_model = keras.models.model_from_json(json_config)\r\n```\r\n\r\nIs there a parameter equivalent to save format = 'tf' for json?\r\n\r\nBest regards,\r\nManuel\r\n", "@manuel-masiello There is no equivalent to `save format` = 'tf'. `save_format` has two options only ('tf' or 'h5'). Thanks!", "@manuel-masiello I think the issue with 'h5' is due to its limitation. There are some limitations on what can be saved as part of `h5` format. Please check this [guide](https://www.tensorflow.org/guide/keras/save_and_serialize#keras_h5_format) to know more about it.\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46427\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46427\">No</a>\n"]}, {"number": 46426, "title": "[RNN] models cannot run in TFLite with delegates", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Qualcomm Snapdragon 865\r\n- TensorFlow installed from (source or binary): Source \r\n- TensorFlow version (or github SHA if from source): Nightly\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ntf_model = tf.keras.models.load_model('./my_model.h5')\r\n\r\nfor i in range(4):\r\n    tf_model.inputs[i].shape._dims[0] = tf_python.framework.tensor_shape.Dimension(1)\r\n\r\nmodel_func = tf.function(lambda a: tf_model(a))\r\nconcrete_func = model_func.get_concrete_function([tf.TensorSpec(tf_model.inputs[0].shape, tf_model.inputs[0].dtype), tf.TensorSpec(tf_model.inputs[1].shape, tf_model.inputs[1].dtype), tf.TensorSpec(tf_model.inputs[2].shape, tf_model.inputs[2].dtype), tf.TensorSpec(tf_model.inputs[3].shape, tf_model.inputs[3].dtype)])\r\n\r\ntf_out = concrete_func([input_1, gru0_1_h_in, gru1_1_h_in, out_quats_gru1_h_in])\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2021-01-14 11:16:28.325520: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:933] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 389 nodes (195), 590 edges (225), time = 6.75ms.\r\n  function_optimizer: Graph size after: 389 nodes (0), 590 edges (0), time = 3.64ms.\r\nOptimization results for grappler item: while_body_2339\r\n  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nOptimization results for grappler item: while_cond_2338\r\n  function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nOptimization results for grappler item: while_cond_2708\r\n  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0ms.\r\nOptimization results for grappler item: while_body_2709\r\n  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nOptimization results for grappler item: while_body_1969\r\n  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nOptimization results for grappler item: while_cond_1968\r\n  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n\r\n2021-01-14 11:16:28.851727: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:332] Ignored output_format.\r\n2021-01-14 11:16:28.851756: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:335] Ignored drop_control_dependency.\r\n2021-01-14 11:16:28.895692: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nMy Keras model contains a GRU layer, here is what the Keras layer converts to in TFLite:\r\n\r\n![Screenshot from 2021-01-14 06-12-45](https://user-images.githubusercontent.com/67419721/104586228-348e6480-5633-11eb-9558-047cd559280b.png)\r\n\r\n\r\n```\r\n\r\n**Failure details**\r\nModel conversion was successful. and runs on CPU. However, when I try to initialize an OpenCL delegate, I get the following run time error:\r\n\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\ninterpreter->ModifyGraphWithDelegate failed\r\n\r\nI suspect the problem is with those `While` ops generated by the converter. My GRU layer is stateless, where I manually manage the layer state (tf.keras.layers.GRU is defined with arguments: return_sequences=True, stateful=False, unroll=False. It is called with one of the model's inputs passed to the initial_state argument), and with the sequence dimension of 1. Is it possible to tell the TFLite converter to generate a graph that doesn't contain a while loop? Since in my situation I don't actually need to have a loop in my GRU.\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Yeah, this is a question to the TFLiteConverter folks.  Un-assigning myself.", "Hi, I think our gpu delegate does not support control flow yet, you probably can try with unroll the GRU.\r\n\r\nthanks", "Hi, have you tried GRU with unroll?", "I ended up breaking up the GRU layer into its constituent matrix and elementwise ops. Closing this issue as I have found a workaround.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46426\">No</a>\n"]}, {"number": 46424, "title": "how to restore  part of the model,  then save the whole model after training?", "body": "I want to finetine a base model using a new dataset.\r\nThe model have two part: part1 + part2\r\nI only want to restore the variables of part1.\r\n\r\n1.restore the model using:\r\n    saver = tf.train.Saver(variables_of_part1, max_to_keep=1000)\r\n2.Training\r\n3.Save the model\r\nBut the new model saved lost the variables of part2.\r\n\r\nHow can I realize \"restore part of a model, trainning, then save the whole model\"?\r\n\r\nMy tensorflow version is 1.10.", "comments": ["@Liujingxiu23 \r\n\r\nTF 1.10 is very old.TensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version TFv2.4 and the below tutorial helps you .\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load", "@ravikyram  Thank you for you reply, I tried to solve the problem just by adding another saver before training and found it works.  ", "@Liujingxiu23 \r\n\r\nGlad to know its resolved.Please, close this thread as your issue was resolved. Thanks!"]}, {"number": 46423, "title": "writer_test of serialization failed for squeeznet", "body": "**System information**\r\n- OS Platform and Distribution  Linux Ubuntu 16.04\r\n- TensorFlow installed from source\r\n- TensorFlow version (use command below):\r\n- Python version: Python 3.5.2\r\n- Bazel version (if compiling from source): Build label: 3.7.2\r\n- GCC/Compiler version (if compiling from source) gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n\r\n**Describe the current behavior**\r\n\r\n- download squeezenet (squeezenet.tflit)\r\n- build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test\r\n-  bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite\r\n```\r\nERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)\r\nERROR: Node number 38 (RESHAPE) failed to prepare.\r\n\r\nAllocateTensors failed on the round-tripped model.\r\n```\r\n**Describe the expected behavior**\r\npass write test\r\n\r\n**Standalone code to reproduce the issue**\r\n- download squeezenet (squeezenet.tflit)\r\n- build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test\r\n-  bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\nERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)\r\nERROR: Node number 38 (RESHAPE) failed to prepare.\r\n\r\nAllocateTensors failed on the round-tripped model.\r\n```\r\nA proposal of fix is at: https://github.com/tensorflow/tensorflow/pull/46422 \r\n", "comments": ["@thaink could you take a look at this issue and the proposal fix #46422 as well?", "How did you create the model? Could you share your conversion code to reproduce and some valid sample inputs?", "Dear @abattery\r\nThe model directly comes from tflite hosted_models zoo.\r\nURL:\r\nhttps://www.tensorflow.org/lite/guide/hosted_models\r\nfile:\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/squeezenet_2018_04_27.tgz", "Hey @WeiChungChang\r\n\r\nI could reproduce your problem. The root cause is that, the deprecated new_shape attribute is not being copied into the result by the writer library. Instead of your fix at #46422 , we will create a simple fix, instead in the writer library.\r\n\r\nThank you for finding out this issue and reporting to us!", "@abattery \r\nIt sounds great.\r\nCould you put the link of the patch here when close please?\r\nI would like to know the detail and how to fix it from the other way.\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46423\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46423\">No</a>\n"]}, {"number": 46422, "title": "Fix AllocateTensors fails for  tflite squeezenet", "body": "**[Purpose]**\r\n\r\nFix AllocateTensors fails for  tflite squeezenet\r\n\r\n**[How to reproduce]**\r\n1. download squeezenet (squeezenet.tflit)\r\n2. build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test\r\n3. bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite\r\n\r\n**[Result]**\r\n```\r\nERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)\r\nERROR: Node number 38 (RESHAPE) failed to prepare.\r\n\r\nAllocateTensors failed on the round-tripped model.\r\n```\r\n**[Root cause]**\r\nhttps://drive.google.com/file/d/1aJGy2lvEkfAMApQV4NrY67KKc4OYFpe1/view?usp=sharing\r\n\r\n```\r\n// Check if the shape tensor is valid. Shapes should be int32 vectors.\r\ninline bool ShapeIsVector(TfLiteContext* context, TfLiteNode* node) {\r\n  const TfLiteTensor* shape = GetInput(context, node, kShapeTensor);\r\n  return (shape != nullptr && shape->dims->size == 1 &&\r\n          shape->type == kTfLiteInt32);\r\n}\r\n```\r\nNotice that the reshape module has Shape Tensor's shape = 2 x 1.\r\nCompared to mobilenet, where the Shape Tensor's shape = 2.  \r\nThe check within **ShapeIsVector** only considers the case where dim of shape needs to be 1.\r\nObviously, squeezenet fails to satisfy the condition: \r\n```\r\nshape->dims->size == 1\r\n\r\n```\r\nHence it uses GetOutputShapeFromParam to get output shape.\r\n```\r\nTfLiteIntArray* GetOutputShape(TfLiteContext* context, TfLiteNode* node) {\r\n  if (NumInputs(node) == 2 && ShapeIsVector(context, node)) {\r\n    return GetOutputShapeFromTensor(context, node);\r\n  } else {\r\n    return GetOutputShapeFromParam(context, node);\r\n  }\r\n``` \r\nHowever we don't have **TfLiteReshapeParams** so issue happens.\r\n\r\n**[Proposal of fix]**\r\nAccording to the semantic of function **ShapeIsVector**, allow additional ones(tailed).\r\nOnly think of the Shape Tensor **is NOT a vector** when **there are more than one \"non-one\" dimensions**.\r\n\r\n**[Result]**\r\nsqueezenet.tflite is O.K (bug fix)\r\nmobilenet.tflit is O.K (have no impact to the model which originally pass) \r\n\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46422) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46422) for more info**.\n\n<!-- need_author_cla -->", "Thank you @WeiChungChang  for your contributions!\r\n\r\nInstead of this one, we will land another form of the fix directly in the writer library."]}, {"number": 46421, "title": "LSTM doesnot work after tf_upgrade_v2 with keras.models.load_model, but tf.keras.models.load_model works", "body": "**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow installed from pip\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Keras version: 2.3.1\r\n- Python version: 3.6.12\r\n- CUDA/cuDNN version: 11.1/7\r\n\r\nAt first, I have codes written by tensorflow v1 that can't run. so I use \r\n\r\n```shell\r\ntf_upgrade_v2 --infile tensorflow_backend.py --outfile tensorflow_backend.py\r\n```\r\nto  upgrade the keras .py file in xx/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/\r\n\r\nit works after upgrade, however, later I found it goes wrong when load LSTM model(.h5)\r\n\r\nthe LSTM model can be acessed [here](https://github.com/SingleBone/origin_models/blob/main/lstm0-sinewave_origin.h5)\r\n\r\n**the current behavior**\r\nwhen load a LSTM model with **keras.models.load_model**\r\n\r\n```python\r\nimport keras\r\nmodel = keras.models.load_model('lstm0-sinewave_origin.h5')\r\n```\r\n\r\n, got RuntimeError\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 492, in load_wrapper\r\n    return load_function(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 584, in load_model\r\n    model = _deserialize_model(h5dict, custom_objects, compile)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 274, in _deserialize_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 627, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/__init__.py\", line 168, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 147, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 302, in from_config\r\n    model.add(layer)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 166, in add\r\n    layer(x)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 541, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 76, in symbolic_fn_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 463, in __call__\r\n    self.build(unpack_singleton(input_shapes))\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 502, in build\r\n    self.cell.build(step_input_shape)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 1942, in build\r\n    constraint=self.bias_constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 282, in add_weight\r\n    constraint=constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 621, in variable\r\n    value, dtype=dtype, name=name, constraint=constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 814, in variable\r\n    constraint=constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 260, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 254, in _variable_v2_call\r\n    shape=shape)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 235, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2645, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1411, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1543, in _init_from_args\r\n    name=\"initial_value\", dtype=dtype)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1280, in convert_to_tensor\r\n    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n```\r\n\r\n**the expected behavior**\r\nwhen use **tensorflow.keras.models.load_model**\r\n\r\n```python\r\nimport tensorflow\r\nmodel = tensorflow.keras.models.load_model('lstm0-sinewave_origin.h5')\r\n```\r\n, it works without error\r\n\r\n**the whole process to triger this issue**\r\n```bash\r\n(tensorflow) root@53125cc00c44:/data/origin_model# python\r\nPython 3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n# here is where it goes wrong\r\n>>> import keras\r\n>>> model = keras.models.load_model('lstm0-sinewave_origin.h5')\r\nWARNING:tensorflow:From /root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2021-01-14 07:36:41.711121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2021-01-14 07:36:41.728295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-14 07:36:41.729095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-14 07:36:41.729486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-14 07:36:41.732935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-01-14 07:36:41.735810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-01-14 07:36:41.736327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-01-14 07:36:41.739862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-14 07:36:41.741934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-14 07:36:41.749515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-14 07:36:41.753192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2021-01-14 07:36:41.753681: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2021-01-14 07:36:41.772855: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2197465000 Hz\r\n2021-01-14 07:36:41.776310: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b853a3c2f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-01-14 07:36:41.776354: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-01-14 07:36:42.013802: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b853aa2730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-01-14 07:36:42.013851: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2021-01-14 07:36:42.013862: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2021-01-14 07:36:42.015095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-14 07:36:42.015643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-14 07:36:42.015709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-14 07:36:42.015745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-01-14 07:36:42.015784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-01-14 07:36:42.015809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-01-14 07:36:42.015829: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-14 07:36:42.015849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-14 07:36:42.015883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-14 07:36:42.018395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2021-01-14 07:36:42.018444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-14 07:36:42.020294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-14 07:36:42.020317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 \r\n2021-01-14 07:36:42.020328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N \r\n2021-01-14 07:36:42.020337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N \r\n2021-01-14 07:36:42.022501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10171 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2021-01-14 07:36:42.023440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2116 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 492, in load_wrapper\r\n    return load_function(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 584, in load_model\r\n    model = _deserialize_model(h5dict, custom_objects, compile)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 274, in _deserialize_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/saving.py\", line 627, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/__init__.py\", line 168, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 147, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 302, in from_config\r\n    model.add(layer)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/sequential.py\", line 166, in add\r\n    layer(x)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 541, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 76, in symbolic_fn_wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 463, in __call__\r\n    self.build(unpack_singleton(input_shapes))\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 502, in build\r\n    self.cell.build(step_input_shape)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 1942, in build\r\n    constraint=self.bias_constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 282, in add_weight\r\n    constraint=constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 621, in variable\r\n    value, dtype=dtype, name=name, constraint=constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 814, in variable\r\n    constraint=constraint)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 260, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 254, in _variable_v2_call\r\n    shape=shape)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 235, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2645, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1411, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1543, in _init_from_args\r\n    name=\"initial_value\", dtype=dtype)\r\n  File \"/root/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1280, in convert_to_tensor\r\n    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n# tensorflow.keras.models.load_model \r\n>>> import tensorflow\r\n>>> model = tensorflow.keras.models.load_model('lstm0-sinewave_origin.h5')\r\n2021-01-14 07:39:55.578518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:02:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-14 07:39:55.579080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:84:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-01-14 07:39:55.579141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-01-14 07:39:55.579165: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-01-14 07:39:55.579186: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-01-14 07:39:55.579207: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-01-14 07:39:55.579224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-01-14 07:39:55.579241: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-01-14 07:39:55.579259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-01-14 07:39:55.581588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0, 1\r\n2021-01-14 07:39:55.581630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-14 07:39:55.581644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 1 \r\n2021-01-14 07:39:55.581652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N N \r\n2021-01-14 07:39:55.581659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 1:   N N \r\n2021-01-14 07:39:55.583609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10171 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2021-01-14 07:39:55.584122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2116 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n>>> import tensorflow\r\n>>> model = tensorflow.keras.models.load_model('lstm0-sinewave_origin.h5')\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n```", "comments": ["@SingleBone I ran your code with your model without any issue. May be the older tensorflow version you are using might have some bug. Can you please try with recent TF versions and let us know whether the issue persists with recent TF version. Thanks!\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/1a5d43f2a946f6cb5512f8e05cd50612/untitled72.ipynb) is the gist with `keras.load_model`.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/447737fc5c57186cd46eef4296223c7f/untitled71.ipynb) is the gist with `tensorflow.keras.load_model`. \r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46421\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46421\">No</a>\n"]}, {"number": 46420, "title": "PIP Tensorflow-gpu", "body": "I read in docs that for new version of tensorflow gpu support is integrated \r\nso if i run: pip install tensorflow, i have both cpu and gpu support.\r\n\r\nNow the question is: why is still present \r\npip install tensorflow-gpu 2.4.0 ???\r\nhttps://pypi.org/project/tensorflow-gpu/\r\n\r\nanyway\r\ni installed  TS whit command \"pip install tensorflow\"\r\n\r\nif i run \r\n```\r\nC:\\Users\\Kit>nvcc -V \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Wed_Jul_22_19:09:35_Pacific_Daylight_Time_2020\r\nCuda compilation tools, release 11.0, V11.0.221\r\nBuild cuda_11.0_bu.relgpu_drvr445TC445_37.28845127_0\r\n```\r\nso everithink seems fine\r\nalso for \r\nC:\\Users\\Kit>nvidia-smi\r\nThu Jan 14 08:02:30 2021\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.89       Driver Version: 460.89       CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105... WDDM  | 00000000:01:00.0  On |                  N/A |\r\n| 35%   14C    P8    N/A /  75W |    758MiB /  4096MiB |      2%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nbut when i run \r\n```\r\nfrom keras import backend as K\r\nprint(tf.config.list_physical_devices('GPU'))\r\nor\r\nimport tensorflow as tf\r\nprint(tf.test.gpu_device_name())\r\n```\r\ni can see only cpu\r\n\r\nbut if i install whit\r\npip install tensorflow-gpu 2.4.0\r\n\r\ni can see cpu AND gpu in python\r\n\r\nI'M A BIT CONFUSED\r\n\r\n\r\n\r\n\r\n", "comments": ["There are CI scripts that still use `tensorflow-gpu`. So we upload the same pip package (modulo renaming) twice.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46420\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46420\">No</a>\n"]}, {"number": 46419, "title": "TF2.4/XLA assertion in Electra model when deferring compilation on GPU", "body": "Attached are gzip versions of\r\n1) git.patch\r\n2) dummy_p1.tfrecord\r\n3) run_electra_bug.sh\r\n\r\nEnvironment:\r\n1) Google TF-2.4 container: tensorflow/tensorflow:2.4.0-gpu\r\n2) single GV100 32GB\r\n\r\nReproduction steps:\r\n\r\n1) git clone git@github.com:NVIDIA/DeepLearningExamples.git\r\n2) cd DeepLearningExamples/TensorFlow2/LanguageModeling/ELECTRA/\r\n3) copy the attached git.patch file to ./\r\n4) cp the attached run_electra_bug file to ./scripts\r\n5) cp the attached dummy_p1.tfrecord file to ./data\r\n6) git apply git.patch\r\n7) bash scripts/docker/build.sh\r\n8) bash scripts/docker/launch.sh\r\n9) TF_XLA_FLAGS=--tf_xla_always_defer_compilation=true bash scripts/run_electra_bug.sh 1 6e-3 amp 1  (run this within the docker container started in 8).\r\n\r\nThe resulting error:\r\n```\r\n0]<stderr>:Traceback (most recent call last):\r\n[0]<stderr>:  File \"/workspace/electra/run_pretraining.py\", line 493, in <module>\r\n[0]<stderr>:    args = main(start_time)\r\n[0]<stderr>:  File \"/workspace/electra/run_pretraining.py\", line 427, in main\r\n[0]<stderr>:    local_step==1, take_step=local_step % args.gradient_accumulation_steps == 0)\r\n[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n[0]<stderr>:    result = self._call(*args, **kwds)\r\n[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n[0]<stderr>:    return self._stateless_fn(*args, **kwds)\r\n[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\r\n[0]<stderr>:    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n[0]<stderr>:    ctx, args, cancellation_manager=cancellation_manager))\r\n[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n[0]<stderr>:    ctx=ctx)\r\n[0]<stderr>:  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n[0]<stderr>:    inputs, attrs, num_outputs)\r\n[0]<stderr>:tensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.\r\n[0]<stderr>:\t [[{{node cluster_11_1/merge_oidx_69/_148}}]] [Op:__inference_train_one_step_88467]\r\n[0]<stderr>:\r\n[0]<stderr>:Function call stack:\r\n[0]<stderr>:train_one_step\r\n[0]<stderr>:\r\nProcess 0 exit with status code 1.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/horovodrun\", line 8, in <module>\r\n    sys.exit(run_commandline())\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 768, in run_commandline\r\n    _run(args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 758, in _run\r\n    return _run_static(args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 615, in _run_static\r\n    _launch_job(args, settings, nics, command)\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 731, in _launch_job\r\n    args.verbose)\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 704, in run_controller\r\n    gloo_run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/launch.py\", line 720, in gloo_run_fn\r\n    gloo_run(settings, nics, env, driver_ip, command)\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/gloo_run.py\", line 284, in gloo_run\r\n    launch_gloo(command, exec_command, settings, nics, env, server_ip)\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/runner/gloo_run.py\", line 271, in launch_gloo\r\n    .format(name=name, code=exit_code))\r\n```", "comments": ["[\r\n[git.patch.gz](https://github.com/tensorflow/tensorflow/files/5812626/git.patch.gz)\r\n[dummy_p1.tfrecord.gz](https://github.com/tensorflow/tensorflow/files/5812628/dummy_p1.tfrecord.gz)\r\n[run_electra_bug.sh.gz](https://github.com/tensorflow/tensorflow/files/5812634/run_electra_bug.sh.gz)\r\n\r\n\r\n](url)", "@bas-aarts,\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!", "I was able to reproduce this.\r\n\r\nWhen I look at the [entire log](https://gist.github.com/sanjoy/cb32509002a089b0c5a4aad6a9f28627) though, I see an OOM.  Do you see that as well?  If yes, it is possible that the OOM is the root cause and the `_Recv` failed because it was waiting on a `_Send` that failed because of the OOM.  Can you try adjusting the batch size etc. to make the OOM disappear and check if the failed recv error also goes away?", "@sanjoy, shooting from the hip here, but can you try running on 1 GPU only?\r\nI reproduced on 1 GPU only (tried again just now on GV100 32GB), and see no OOM.\r\nPerhaps multiple GPUs results in more allocation per GPU\r\n\r\ntry running with:\r\nTF_XLA_FLAGS=--tf_xla_always_defer_compilation=true bash scripts/run_electra_bug.sh 1 6e-3 amp 1", "I actually tried to simplify the repro, but was not able to reduce significantly. I therefore choose to use the model as a repro.", "Bas, is there a way to reduce the batch size so that it fits in a 16G P100 or V100?  I don't have access to a 32G V100.", "With the last  command line, BS=1", "Bas, [this](https://gist.github.com/sanjoy/a5eb963dbc567f98a3e0fa4d888cabe3) is the full log with 1GPU and the command line you mentioned in https://github.com/tensorflow/tensorflow/issues/46419#issuecomment-764001365. In the log I see many lines like:\r\n\r\n```\r\n[0]<stderr>:2021-01-26 05:17:39.245145: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at resource_variable_ops.cc:536 : Not found: Resource localhost/_AnonymousVar837/N10tensorflow3VarE does not exist.\r\n```\r\n\r\npreceding the `RecvAsync` failure.  So I suspect these errors are causing a previous Send to fail, resulting in the later failed Recv.  Does this sound plausible?", "I'm certain the warnings have something to do with the problem.  Since these are no errors, I do not know how serious the issue is.", "Any updates on this? perhaps even an ETA?", "Bas,\r\n\r\nSorry for the delay, after https://github.com/tensorflow/tensorflow/issues/46419#issuecomment-767343956 I assumed that you're going to look into this further.  Looks like that was a mis-parsing on my part.\r\n\r\nI strongly suspect that some Grappler-esque pass is running after auto-clustering and DCE'ing an XLA cluster.  The following change seems to fix the symptom at least ([log](https://gist.github.com/sanjoy/deb3a6c6a0a4821e93d3de4389ef074e)):\r\n\r\n```\r\ndiff --git a/tensorflow/compiler/jit/build_xla_ops_pass.cc b/tensorflow/compiler/jit/build_xla_ops_pass.cc\r\nindex a340b9d3f45..e7c0c10a225 100644\r\n--- a/tensorflow/compiler/jit/build_xla_ops_pass.cc\r\n+++ b/tensorflow/compiler/jit/build_xla_ops_pass.cc\r\n@@ -309,9 +309,9 @@ xla::StatusOr<Node*> ReplaceFunctionCallWithPartitionedCall(\r\n     }\r\n   }\r\n \r\n-  ops::PartitionedCall call(\r\n-      root.WithOpName(\"partitioned_call\"), args, n->output_types(), func,\r\n-      ops::PartitionedCall::Attrs{}.ConfigProto(config_string));\r\n+  ops::StatefulPartitionedCall call(\r\n+      root.WithOpName(\"stateful_partitioned_call\"), args, n->output_types(),\r\n+      func, ops::StatefulPartitionedCall::Attrs{}.ConfigProto(config_string));\r\n \r\n   for (const Edge* e : n->in_edges()) {\r\n     if (e->IsControlEdge()) {\r\n```\r\n\r\nSo I think the correct solution is to use `StatefulPartitionedCall` when the XLA cluster has any resource variable operations.  But before we do that, I'd prefer confirm that this theory is correct -- would you be able to put some time into verifying this theory?  For this theory to be correct, there has to be a DCE-like pass that runs after build_xla_ops_pass that deletes non-stateful partitioned call ops.  This pass is likely being run from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/optimization_registry.cc).\r\n\r\n@cheshire and @nnigania also ran into a variant of this issue IIRC.", "I'll take a look @sanjoy ", "@bas-aarts As discussed, there is the patch I've used.\r\n\r\nIn file `def_function.py`, after inserting `_noinline=True`, add\r\n\r\n> `attributes.update(_grappler_do_not_remove=True)` (line 658)\r\n\r\nInside `functional_ops.py`, at line 1223, add:\r\n\r\n```\r\n  if grappler_do_not_remove_attr in f.definition.attr:\r\n    op_attrs[grappler_do_not_remove_attr] = f.definition.attr[\r\n      grappler_do_not_remove_attr]\r\n```\r\n\r\nwhere `grappler_do_not_remove_attr = '_grappler_do_not_remove`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46419\">No</a>\n", "Undoing auto-close.", "@cheshire, would you mind attaching a git patch instead? want to make sure I do the right thing. thanks. ", "hi Bas,\n\nI don't have a git patch, as this change is internal. Do you think you\ncould try it and maybe show the patch you create to double check?\n\nGeorge\n\nOn Wed, Feb 17, 2021 at 2:22 PM Bas Aarts <notifications@github.com> wrote:\n\n> @cheshire <https://github.com/cheshire>, would you mind attaching a git\n> patch instead? want to make sure I do the right thing. thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/46419#issuecomment-780893717>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH2AWJLHSVVMPHBVEKDS7Q6S3ANCNFSM4WB547YA>\n> .\n>\n", "Hi George.\r\n\r\nwith the following diff the original error is still there\r\n```\r\ndiff --git a/tensorflow/python/eager/def_function.py b/tensorflow/python/eager/def_function.py\r\nindex da19acad243..475fa810433 100644\r\n--- a/tensorflow/python/eager/def_function.py\r\n+++ b/tensorflow/python/eager/def_function.py\r\n@@ -655,6 +655,7 @@ class Function(object):\r\n       attributes.update(_XlaMustCompile=bool(self._jit_compile))\r\n       if self._jit_compile:\r\n         attributes.update(_noinline=True)\r\n+        attributes.update(_grappler_do_not_remove=True)\r\n     if not attributes:\r\n       attributes = None\r\n     return function_lib.defun_with_attributes(\r\ndiff --git a/tensorflow/python/ops/functional_ops.py b/tensorflow/python/ops/functional_ops.py\r\nindex bdd20cda991..2caae0dc24e 100644\r\n--- a/tensorflow/python/ops/functional_ops.py\r\n+++ b/tensorflow/python/ops/functional_ops.py\r\n@@ -1218,6 +1218,9 @@ def partitioned_call(args,\r\n   }\r\n   if xla_compile_attr in f.definition.attr:\r\n     op_attrs[xla_compile_attr] = f.definition.attr[xla_compile_attr]\r\n+  grappler_do_not_remove_attr = \"_grappler_do_not_remove\"\r\n+  if grappler_do_not_remove_attr in f.definition.attr:\r\n+    op_attrs[grappler_do_not_remove_attr] = f.definition.attr[grappler_do_not_remove_attr]\r\n   op = graph.create_op(op_name, args, tout, name=op_name, attrs=op_attrs)\r\n   outputs = op.outputs\r\n   if hasattr(f, \"graph\"):\r\n```\r\n\r\nplease confirm if this is the intended change or not.\r\n\r\nTrying @sanjoy's change next", "I have confirmed that @sanjoy's proposed change does not trigger the error.", "@cheshire , can you please let me know if the diff above is the one you expected to work? Thanks", "@bas-aarts yes. To clarify: I do not know whether it solves your problem or not. It solved one problem for me, I'm not sure whether you have the same one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46419\">No</a>\n"]}, {"number": 46418, "title": "Update TFLM docker image to be based off Ubuntu 20.04", "body": "With this, we can easily install gcc-9, and Ubuntu is also part of the CI for the broader Tensorflow project.\n\nFixes #46415\n", "comments": ["gcc-9 did not work either. I have an internal change that uses clang+bazel instead of gcc+bazel. That should fix the CI.\r\n\r\nIt does mean that TFLM contributors who need to build with bazel might also have to start using clang. I will update CONTRIBUTING.md once the continuous build is green again."]}, {"number": 46417, "title": "when i use custom loss and gpu, fit kernel died", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10\r\n- TensorFlow version (use command below): tf-nightly-gpu 2.5.0 210112\r\n- Python version:3.8\r\n- CUDA/cuDNN version: 11.1 / maybe 8(?)\r\n- GPU model and memory: RTX 3090 24GB MEM\r\n\r\n2. TF 2.0: v1.12.1-48890-g670cc3fa48f 2.5.0-dev20210113\r\n\r\n\r\n\r\nmy custom loss function looks like this,\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\nfrom tensorflow.keras.losses import Loss\r\n\r\n@tf.function\r\ndef mase(y_true, y_pred, seasonality=1):\r\n    def _naive_forecasting(actual, seasonality: int = 1):\r\n        return actual[:-seasonality]\r\n    \r\n    def _error(actual, predicted):\r\n        return actual - predicted\r\n    \r\n    def _mae(actual, predicted):\r\n        return K.mean(K.abs(_error(actual, predicted)))\r\n    \r\n#     K.print_tensor(y_true,message='\\ny_true==')\r\n#     K.print_tensor(y_pred,message='\\ny_pred==')\r\n#     K.print_tensor(_mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality)),message='\\nminus==')\r\n#     print(y_true, y_pred)\r\n    return _mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality))\r\n\r\n```\r\n\r\nand use like this,\r\n```\r\nmodel.compile(loss=MASE(1), optimizer=Adam(lr=0.001))\r\n...\r\nmodel.fit(x_concat_data[train], y_concat_data[train], batch_size=batch_size, epochs=epoch, verbose=2, shuffle=True)\r\n...\r\nmodel.evaluate(x_concat_data[validation], y_concat_data[validation], batch_size=batch_size, callbacks=[early_stopping])\r\n```\r\nI use rtx 3090 and want to train with gpu\r\n\r\ncpu training is good!\r\nBUT! when i use gpu, python kernel is dead!\r\nkernel output like this\r\n\r\n2021-01-14 09:33:55.296929: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/1000\r\nd:/>\r\n\r\nwhat is my problem?", "comments": ["when i use not custom loss func 'mean_absolute_percentage_error' in keras,\r\ni have same issue\r\n\r\nonly working on 'mean_squared_error'", "conda install keras \r\n          or \r\nClose another notebook which might be running or was used earlier to train a model on GPU", "> conda install keras\r\n> or\r\n> Close another notebook which might be running or was used earlier to train a model on GPU\r\n\r\nalready doin that way. that is not work\r\n", "> _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10\r\n> * TensorFlow version (use command below): tf-nightly-gpu 2.5.0 210112\r\n> * Python version:3.8\r\n> * CUDA/cuDNN version: 11.1 / maybe 8(?)\r\n> * GPU model and memory: RTX 3090 24GB MEM\r\n> \r\n> 1. TF 2.0: v1.12.1-48890-g670cc3fa48f 2.5.0-dev20210113\r\n> \r\n> my custom loss function looks like this,\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from keras import backend as K\r\n> from tensorflow.keras.losses import Loss\r\n> \r\n> @tf.function\r\n> def mase(y_true, y_pred, seasonality=1):\r\n>     def _naive_forecasting(actual, seasonality: int = 1):\r\n>         return actual[:-seasonality]\r\n>     \r\n>     def _error(actual, predicted):\r\n>         return actual - predicted\r\n>     \r\n>     def _mae(actual, predicted):\r\n>         return K.mean(K.abs(_error(actual, predicted)))\r\n>     \r\n> #     K.print_tensor(y_true,message='\\ny_true==')\r\n> #     K.print_tensor(y_pred,message='\\ny_pred==')\r\n> #     K.print_tensor(_mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality)),message='\\nminus==')\r\n> #     print(y_true, y_pred)\r\n>     return _mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality))\r\n> ```\r\n> \r\n> and use like this,\r\n> \r\n> ```\r\n> model.compile(loss=MASE(1), optimizer=Adam(lr=0.001))\r\n> ...\r\n> model.fit(x_concat_data[train], y_concat_data[train], batch_size=batch_size, epochs=epoch, verbose=2, shuffle=True)\r\n> ...\r\n> model.evaluate(x_concat_data[validation], y_concat_data[validation], batch_size=batch_size, callbacks=[early_stopping])\r\n> ```\r\n> \r\n> I use rtx 3090 and want to train with gpu\r\n> \r\n> cpu training is good!\r\n> BUT! when i use gpu, python kernel is dead!\r\n> kernel output like this\r\n> \r\n> 2021-01-14 09:33:55.296929: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\n> Epoch 1/1000\r\n> d:/>\r\n> \r\n> what is my problem?\r\n\r\n2\r\n\r\nThis informative message is saying MLIR is not being used. Usually its not expected to use the MLIR implementation and are instead expected to use the non-MLIR feature complete implementation.", "> > _Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template_\r\n> > **System information**\r\n> > \r\n> > * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10\r\n> > * TensorFlow version (use command below): tf-nightly-gpu 2.5.0 210112\r\n> > * Python version:3.8\r\n> > * CUDA/cuDNN version: 11.1 / maybe 8(?)\r\n> > * GPU model and memory: RTX 3090 24GB MEM\r\n> > \r\n> > \r\n> > 1. TF 2.0: v1.12.1-48890-g670cc3fa48f 2.5.0-dev20210113\r\n> > \r\n> > my custom loss function looks like this,\r\n> > ```\r\n> > import tensorflow as tf\r\n> > from keras import backend as K\r\n> > from tensorflow.keras.losses import Loss\r\n> > \r\n> > @tf.function\r\n> > def mase(y_true, y_pred, seasonality=1):\r\n> >     def _naive_forecasting(actual, seasonality: int = 1):\r\n> >         return actual[:-seasonality]\r\n> >     \r\n> >     def _error(actual, predicted):\r\n> >         return actual - predicted\r\n> >     \r\n> >     def _mae(actual, predicted):\r\n> >         return K.mean(K.abs(_error(actual, predicted)))\r\n> >     \r\n> > #     K.print_tensor(y_true,message='\\ny_true==')\r\n> > #     K.print_tensor(y_pred,message='\\ny_pred==')\r\n> > #     K.print_tensor(_mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality)),message='\\nminus==')\r\n> > #     print(y_true, y_pred)\r\n> >     return _mae(y_true, y_pred) / _mae(y_true[seasonality:], _naive_forecasting(y_true, seasonality))\r\n> > ```\r\n> > \r\n> > \r\n> > and use like this,\r\n> > ```\r\n> > model.compile(loss=MASE(1), optimizer=Adam(lr=0.001))\r\n> > ...\r\n> > model.fit(x_concat_data[train], y_concat_data[train], batch_size=batch_size, epochs=epoch, verbose=2, shuffle=True)\r\n> > ...\r\n> > model.evaluate(x_concat_data[validation], y_concat_data[validation], batch_size=batch_size, callbacks=[early_stopping])\r\n> > ```\r\n> > \r\n> > \r\n> > I use rtx 3090 and want to train with gpu\r\n> > cpu training is good!\r\n> > BUT! when i use gpu, python kernel is dead!\r\n> > kernel output like this\r\n> > 2021-01-14 09:33:55.296929: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\n> > Epoch 1/1000\r\n> > d:/>\r\n> > what is my problem?\r\n> \r\n> 2\r\n> \r\n> This informative message is saying MLIR is not being used. Usually its not expected to use the MLIR implementation and are instead expected to use the non-MLIR feature complete implementation.\r\n\r\nI uninstalled tf-nightly version, and installing tensorflow-gpu 2.4.0.\r\nthat version work good\r\n\r\nissue happend only tf-nightly version. maybe i think that is nightly version\\`s bug. but i don`t know why happened like that.\r\n\r\ni will use tensorflow-gpu 2.4.0. so, this issue is complete to me.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46417\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46417\">No</a>\n"]}, {"number": 46415, "title": "TFLM CI is currently broken", "body": "@tensorflow/micro\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/02e9e26b27a59fef75f69575d54aa4af96605af5 added a compiler flag that [breaks the TFLM bazel build](https://source.cloud.google.com/results/invocations/b6a8674a-60c7-4426-8e9d-b5d96294f455/log) with the following error:\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/lite/kernels/internal/BUILD:685:11: C++ compilation of rule '//tensorflow/lite/kernels/internal:portable_tensor_utils' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 35 argument(s) skipped)\r\ngcc: error: unrecognized argument to -fsanitize= option: 'shift-base'\r\n```\r\n\r\nThe underlying issue seems to be that the gcc version on the TFLM docker image (gcc (Debian 8.3.0-6) 8.3.0) does not support this argument.\r\n\r\nDepending on your local gcc version, this may or may not be reproducible locally.\r\n\r\nFor example, for me: gcc (Debian 10.2.0-19) 10.2.0\r\n\r\n```\r\nbazel clean\r\nbazel build -s tensorflow/lite/c:common\r\n```\r\n\r\nhas the shift-base argument:\r\n```\r\n/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/tensorflow/lite/c/_objs/common/common.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/lite/c/_objs/common/common.o' -iquote . -iquote bazel-out/k8-opt/bin -w -DAUTOLOAD_DYNAMIC_KERNELS -DFARMHASH_NO_CXX_STRING -Wno-sign-compare -O3 -fno-exceptions '-fno-sanitize=shift-base' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/lite/c/common.c -o bazel-out/k8-opt/bin/tensorflow/lite/c/_objs/common/common.o)\r\n```\r\n\r\nbut the build passes.", "comments": ["https://github.com/tensorflow/tensorflow/pull/46418#issuecomment-759943878 gives some more context on the direction that we are likely headed with this.", "https://github.com/tensorflow/tensorflow/commit/f2922b713c8816b1f5cde96ede4c150149faef4b didn't fix the issue. Reopening."]}, {"number": 46414, "title": "InceptionResNetV2: BatchNormalization layer does not work outside during validation/evaluation", "body": "TF version : **v2.4.0-0-g582c8d236cb 2.4.0**\r\nPython: 3.6.9/3.8\r\nOS Version: Ubuntu 18.04/Windows 10\r\n\r\n`BatchNormalization` layer does not work during evaluation and validation phase in `InceptionResNetV2`. Please note that the same data was used for training, validation and evaluation. Dataset consists of 40 examples of cats/dogs. Dataset is balanced (20cat/20dog)\r\n\r\n1. Train history **with** Batch Normalization :\r\n\r\n```shell\r\nstarting script\r\nstarting training\r\nEpoch 1/12\r\n4/4 [==============================] - 21s 2s/step - loss: 1.3195 - accuracy: 0.4600 - val_loss: 0.7235 - val_accuracy: 0.5000\r\nEpoch 2/12\r\n4/4 [==============================] - 3s 715ms/step - loss: 1.8840 - accuracy: 0.5500 - val_loss: 0.7115 - val_accuracy: 0.5000\r\nEpoch 3/12\r\n4/4 [==============================] - 3s 714ms/step - loss: 0.6748 - accuracy: 0.6833 - val_loss: 3.8985 - val_accuracy: 0.5000\r\nEpoch 4/12\r\n4/4 [==============================] - 3s 714ms/step - loss: 0.3799 - accuracy: 0.7900 - val_loss: 7.1380 - val_accuracy: 0.5000\r\nEpoch 5/12\r\n4/4 [==============================] - 3s 718ms/step - loss: 0.2764 - accuracy: 0.9267 - val_loss: 7.7212 - val_accuracy: 0.5000\r\nEpoch 6/12\r\n4/4 [==============================] - 3s 718ms/step - loss: 0.1726 - accuracy: 0.9467 - val_loss: 6.9867 - val_accuracy: 0.5000\r\nEpoch 7/12\r\n4/4 [==============================] - 3s 723ms/step - loss: 0.0975 - accuracy: 1.0000 - val_loss: 5.7449 - val_accuracy: 0.5000\r\nEpoch 8/12\r\n4/4 [==============================] - 3s 724ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 4.4831 - val_accuracy: 0.5000\r\nEpoch 9/12\r\n4/4 [==============================] - 3s 726ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 3.7662 - val_accuracy: 0.5000\r\nEpoch 10/12\r\n4/4 [==============================] - 3s 733ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 3.0623 - val_accuracy: 0.5000\r\nEpoch 11/12\r\n4/4 [==============================] - 3s 732ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.4364 - val_accuracy: 0.5000\r\nEpoch 12/12\r\n4/4 [==============================] - 3s 736ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.9331 - val_accuracy: 0.5000\r\nstarting testing\r\n4/4 [==============================] - 1s 197ms/step - loss: 1.9331 - accuracy: 0.5000\r\n[1.9330692291259766, 0.5]\r\nscript complete\r\n```\r\n\r\n1. Train history **without** Batch Normalization:\r\n```shell\r\nstarting script\r\nstarting training\r\nEpoch 1/50\r\n4/4 [==============================] - 14s 1s/step - loss: 0.7497 - accuracy: 0.3833 - val_loss: 0.6936 - val_accuracy: 0.5000\r\nEpoch 2/50\r\n4/4 [==============================] - 2s 674ms/step - loss: 0.7000 - accuracy: 0.4100 - val_loss: 0.6927 - val_accuracy: 0.5000\r\nEpoch 3/50\r\n4/4 [==============================] - 2s 671ms/step - loss: 0.6941 - accuracy: 0.4300 - val_loss: 0.6923 - val_accuracy: 0.5000\r\nEpoch 4/50\r\n4/4 [==============================] - 2s 673ms/step - loss: 0.6930 - accuracy: 0.4633 - val_loss: 0.6913 - val_accuracy: 0.5000\r\nEpoch 5/50\r\n4/4 [==============================] - 2s 677ms/step - loss: 0.6923 - accuracy: 0.4600 - val_loss: 0.6884 - val_accuracy: 0.5000\r\nEpoch 6/50\r\n4/4 [==============================] - 2s 678ms/step - loss: 0.6889 - accuracy: 0.4767 - val_loss: 0.6775 - val_accuracy: 0.5000\r\nEpoch 7/50\r\n4/4 [==============================] - 2s 686ms/step - loss: 0.6793 - accuracy: 0.5033 - val_loss: 0.6452 - val_accuracy: 0.6750\r\nEpoch 8/50\r\n4/4 [==============================] - 2s 679ms/step - loss: 0.6553 - accuracy: 0.7533 - val_loss: 0.6098 - val_accuracy: 0.6750\r\nEpoch 9/50\r\n4/4 [==============================] - 2s 680ms/step - loss: 0.5917 - accuracy: 0.7000 - val_loss: 0.6536 - val_accuracy: 0.6250\r\nEpoch 10/50\r\n4/4 [==============================] - 2s 679ms/step - loss: 0.6050 - accuracy: 0.6333 - val_loss: 0.4657 - val_accuracy: 0.8250\r\nEpoch 11/50\r\n4/4 [==============================] - 2s 683ms/step - loss: 0.5150 - accuracy: 0.7667 - val_loss: 0.4900 - val_accuracy: 0.8250\r\nEpoch 12/50\r\n4/4 [==============================] - 2s 681ms/step - loss: 0.4826 - accuracy: 0.8333 - val_loss: 0.4770 - val_accuracy: 0.7250\r\nEpoch 13/50\r\n4/4 [==============================] - 2s 677ms/step - loss: 0.4328 - accuracy: 0.8267 - val_loss: 0.3850 - val_accuracy: 0.8250\r\nEpoch 14/50\r\n4/4 [==============================] - 2s 683ms/step - loss: 0.3578 - accuracy: 0.8600 - val_loss: 0.2969 - val_accuracy: 0.8500\r\nEpoch 15/50\r\n4/4 [==============================] - 3s 736ms/step - loss: 0.2813 - accuracy: 0.8533 - val_loss: 0.2176 - val_accuracy: 0.9000\r\nEpoch 16/50\r\n4/4 [==============================] - 2s 680ms/step - loss: 0.1951 - accuracy: 0.9100 - val_loss: 0.1658 - val_accuracy: 0.9000\r\nEpoch 17/50\r\n4/4 [==============================] - 2s 677ms/step - loss: 0.2913 - accuracy: 0.8467 - val_loss: 0.2642 - val_accuracy: 0.9000\r\nEpoch 18/50\r\n4/4 [==============================] - 2s 677ms/step - loss: 0.2781 - accuracy: 0.9300 - val_loss: 0.3536 - val_accuracy: 0.8250\r\nEpoch 19/50\r\n4/4 [==============================] - 2s 680ms/step - loss: 0.3337 - accuracy: 0.8200 - val_loss: 0.3058 - val_accuracy: 0.8500\r\nEpoch 20/50\r\n4/4 [==============================] - 2s 669ms/step - loss: 0.3164 - accuracy: 0.8533 - val_loss: 0.2551 - val_accuracy: 0.9000\r\nEpoch 21/50\r\n4/4 [==============================] - 2s 671ms/step - loss: 0.2508 - accuracy: 0.8900 - val_loss: 0.2346 - val_accuracy: 0.9500\r\nEpoch 22/50\r\n4/4 [==============================] - 2s 668ms/step - loss: 0.2010 - accuracy: 0.9800 - val_loss: 0.1793 - val_accuracy: 0.9250\r\nEpoch 23/50\r\n4/4 [==============================] - 2s 671ms/step - loss: 0.1348 - accuracy: 0.9433 - val_loss: 0.1695 - val_accuracy: 0.9250\r\nEpoch 24/50\r\n4/4 [==============================] - 2s 664ms/step - loss: 0.1983 - accuracy: 0.9167 - val_loss: 0.4384 - val_accuracy: 0.8250\r\nEpoch 25/50\r\n4/4 [==============================] - 2s 670ms/step - loss: 0.6678 - accuracy: 0.7767 - val_loss: 0.2072 - val_accuracy: 0.9250\r\nEpoch 26/50\r\n4/4 [==============================] - 2s 667ms/step - loss: 0.3736 - accuracy: 0.7867 - val_loss: 0.3285 - val_accuracy: 0.8750\r\nEpoch 27/50\r\n4/4 [==============================] - 2s 671ms/step - loss: 0.3321 - accuracy: 0.8600 - val_loss: 0.4592 - val_accuracy: 0.7250\r\nEpoch 28/50\r\n4/4 [==============================] - 2s 668ms/step - loss: 0.4189 - accuracy: 0.7733 - val_loss: 0.3381 - val_accuracy: 0.8750\r\nEpoch 29/50\r\n4/4 [==============================] - 2s 671ms/step - loss: 0.2884 - accuracy: 0.9167 - val_loss: 0.2362 - val_accuracy: 0.9250\r\nEpoch 30/50\r\n4/4 [==============================] - 2s 669ms/step - loss: 0.2109 - accuracy: 0.9433 - val_loss: 0.1939 - val_accuracy: 0.9250\r\nEpoch 31/50\r\n4/4 [==============================] - 2s 666ms/step - loss: 0.1309 - accuracy: 0.9633 - val_loss: 0.2058 - val_accuracy: 0.9000\r\nEpoch 32/50\r\n4/4 [==============================] - 2s 673ms/step - loss: 0.2605 - accuracy: 0.8733 - val_loss: 0.2232 - val_accuracy: 0.8750\r\nEpoch 33/50\r\n4/4 [==============================] - 2s 675ms/step - loss: 0.1198 - accuracy: 0.9367 - val_loss: 0.1587 - val_accuracy: 0.9250\r\nEpoch 34/50\r\n4/4 [==============================] - 2s 667ms/step - loss: 0.1421 - accuracy: 0.9567 - val_loss: 0.1156 - val_accuracy: 0.9250\r\nEpoch 35/50\r\n4/4 [==============================] - 2s 669ms/step - loss: 0.0929 - accuracy: 0.9367 - val_loss: 0.0473 - val_accuracy: 1.0000\r\nEpoch 36/50\r\n4/4 [==============================] - 2s 671ms/step - loss: 0.0733 - accuracy: 0.9667 - val_loss: 0.0537 - val_accuracy: 0.9750\r\nEpoch 37/50\r\n4/4 [==============================] - 2s 670ms/step - loss: 0.0925 - accuracy: 0.9833 - val_loss: 0.0829 - val_accuracy: 0.9250\r\nEpoch 38/50\r\n4/4 [==============================] - 2s 667ms/step - loss: 0.1385 - accuracy: 0.9400 - val_loss: 0.4641 - val_accuracy: 0.8500\r\nEpoch 39/50\r\n4/4 [==============================] - 2s 666ms/step - loss: 0.4502 - accuracy: 0.8567 - val_loss: 0.1805 - val_accuracy: 0.9000\r\nEpoch 40/50\r\n4/4 [==============================] - 2s 669ms/step - loss: 0.2034 - accuracy: 0.9200 - val_loss: 0.0498 - val_accuracy: 1.0000\r\nEpoch 41/50\r\n4/4 [==============================] - 2s 667ms/step - loss: 0.0675 - accuracy: 0.9800 - val_loss: 0.1448 - val_accuracy: 0.9250\r\nEpoch 42/50\r\n4/4 [==============================] - 2s 670ms/step - loss: 0.0969 - accuracy: 0.9533 - val_loss: 0.0311 - val_accuracy: 1.0000\r\nEpoch 43/50\r\n4/4 [==============================] - 2s 678ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.0047 - val_accuracy: 1.0000\r\nEpoch 44/50\r\n4/4 [==============================] - 2s 678ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9750\r\nEpoch 45/50\r\n4/4 [==============================] - 2s 676ms/step - loss: 0.2596 - accuracy: 0.9267 - val_loss: 0.2702 - val_accuracy: 0.9000\r\nEpoch 46/50\r\n4/4 [==============================] - 2s 674ms/step - loss: 0.3273 - accuracy: 0.9167 - val_loss: 0.7219 - val_accuracy: 0.7500\r\nEpoch 47/50\r\n4/4 [==============================] - 2s 672ms/step - loss: 0.5580 - accuracy: 0.7633 - val_loss: 0.1916 - val_accuracy: 0.9500\r\nEpoch 48/50\r\n4/4 [==============================] - 2s 674ms/step - loss: 0.3112 - accuracy: 0.9300 - val_loss: 0.1998 - val_accuracy: 0.9750\r\nEpoch 49/50\r\n4/4 [==============================] - 2s 676ms/step - loss: 0.1679 - accuracy: 0.9533 - val_loss: 0.2122 - val_accuracy: 0.9000\r\nEpoch 50/50\r\n4/4 [==============================] - 2s 675ms/step - loss: 0.1413 - accuracy: 0.9733 - val_loss: 0.1079 - val_accuracy: 0.9500\r\nstarting testing\r\n4/4 [==============================] - 1s 184ms/step - loss: 0.1079 - accuracy: 0.9500\r\n[0.10788657516241074, 0.949999988079071]\r\nscript complete\r\n```\r\n\r\nLink to fully reproduce (with dataset, takes about 3 minutes) :\r\n https://colab.research.google.com/drive/1QJVZC-2u9aq2RaSCUlmRQeMbZ2QzMHyx?usp=sharing\r\n\r\nThis was inspired by https://stackoverflow.com/q/65415799/9730862 (I'm not the OP, slightly modified setup)\r\n\r\nI know similar issues were raised in the past but taking into account that this behaviour also affects  built-in models  such as `InceptionResNetV2` I'm starting this thread in hope for reveletion. \r\n\r\nCheers!\r\n\r\nEdit: Normally, I would be happy to debug the model and see what is going on but frankly I'm not sure how to do this. In PyTorch I can run each operation step-by-step, does eagerly running models provide similar opprtunity?\r\n\r\n\r\n", "comments": ["It's not limited to TF2.4, just tested it on 2.2", "@prokotg \r\nI ran the code on nightly please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d7c95a146344f4df19f6e49d9b0ca639/untitled499.ipynb#scrollTo=5Xg8pFt-Up2I) please let us know if it confirms your issue.", "@Saduf2019  Yes, it's the same issue", "@prokotg Code is too long. I didn't see BatchNormLayer from Keras. Did you implement your own batchnorm layer?\r\n\r\n GitHub is mainly for bugs and performance related issues. This issue looks more like a support issues. We encourage you to post it in Stackoverflow where there is a large community to support this kind of questions. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46414\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46414\">No</a>\n"]}, {"number": 46413, "title": "How the trim existed model for a given tflite model?", "body": "Given a tflite model, is there any way to trim the model?\r\n\r\nIt is, for example, for squeezent, \r\n```\r\n1. Placeholder\r\n2. maxpool\r\n3. fire 1\r\n4. fire 2\r\n5. fire 3\r\n.....\r\n41. average pooling\r\n42. flattern\r\n43. softmax\r\n```\r\nI need a way to trim and keep \r\n```\r\n3. fire 1\r\n4. fire 2\r\n5. fire 3\r\n```\r\nfor this case, the input is replaced to fire1 and output is as fire 3.\r\n\r\nI wonder if tensorflow provides such functionaliry?\r\n\r\nThanks~", "comments": ["You can modify your TensorFlow Lite model file by using the flatbuffer library manually, which is not the recommended way but it is possible since TensorFlow Lite model itself is based on the flatbuffer format. TFLite serialization library can do that.  https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/serialization\r\n\r\nHowever, I would recommend modifying the original TensorFlow model and converting again, which is much a clean way to achieve the same goal.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46412, "title": "Speedup SpaceToDepth", "body": "Speedup SpaceToDepth on CPU. Thread over batch and memcpy over channel. Speedup 1.1x ~ 3x for large number of batches or channels.\r\n\r\nOn **Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz**\r\n\r\n| Input shape `(N x H x W x C)` | Block size | Original (M items / s) | This PR (M items / s) |\r\n| --------------------------------| -----------| ----------| ---------|\r\n| 1 x 64 x 64 x 1                        | 2              |   83            |   73 |\r\n| 16 x 64 x 64 x 1                        | 2              |  241              |  337 |\r\n| 32 x 64 x 64 x 1                        | 2              |   261            |  559 |\r\n| 1 x 128 x 128x 1                        | 2              |   162            |   132 |\r\n| 16 x 128 x 128 x 1                        | 2              | 308              |   772 |\r\n| 32 x 128 x 128 x 1                        | 2              |   318            |    931 |\r\n| 1 x 64 x 64 x 3                        | 2              |     238          |   221 |\r\n| 16 x 64 x 64 x 3                        | 2              |   617            | 1059 |\r\n| 32 x 64 x 64 x 3                        | 2              |    665           |  1609 |\r\n| 1 x 128 x 128x 3                        | 2              |  437             | 391 |\r\n| 16 x 128 x 128 x 3                        | 2              | 737              |   2146 |\r\n| 32 x 128 x 128 x 3                        | 2              |   754            |   2221 |\r\n| 1 x 64 x 64 x 15                        | 2              |     691           |   841 |\r\n| 16 x 64 x 64 x 15                        | 2              |   1375            |   2752 |\r\n| 32 x 64 x 64 x 15                        | 2              |    1369           |  2024 |\r\n| 1 x 128 x 128x 15                        | 2              |   1216            |  1770 |\r\n| 16 x 128 x 128 x 15                        | 2              |   1248            |    2159 |\r\n| 32 x 128 x 128 x 15                        | 2              |    415           |    2062 |\r\n| 1 x 64 x 64 x 64                        | 2              |  1455             |   2590 |\r\n| 16 x 64 x 64 x 64                        | 2              |  1478             |  2165 |\r\n| 32 x 64 x 64 x 64                        | 2              |   1259            |   1658 |\r\n| 1 x 128 x 128x 64                        | 2              |   1725            |   2080 |\r\n| 16 x 128 x 128 x 64                        | 2              |  1255             |   1552 |\r\n| 32 x 128 x 128 x 64                        | 2              |  1209             |    1644 |\r\n| 1 x 64 x 64 x 64                        | 4              |   1541            |   2698 |\r\n| 16 x 64 x 64 x 64                        | 4              | 1602              |   2136 |\r\n| 32 x 64 x 64 x 64                        | 4              |  1346             |   1616 |\r\n| 1 x 128 x 128x 64                        | 4              |   1618            |   1969 |\r\n| 16 x 128 x 128 x 64                        | 4              |  1252             |   1579 |\r\n| 32 x 128 x 128 x 64                        | 4              |  1243             |   1656 |\r\n\r\nwhere the number is computed as `time / (iterations * N * H * W * C)`.", "comments": ["@WindQAQ Can you please check @cantonios's comments and keep us posted ? Thanks!\r\n", "@WindQAQ  Any update on this PR? Please. Thanks!", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 46411, "title": "Update xtensa fully_connected kernel to call into xa_nnlib.", "body": "This is the relevant code snippet from https://github.com/tensorflow/tensorflow/pull/44581 that is needed to use xa_nnlib with the fully_connected kernel for fusion_f1.\r\n\r\nProfiling the keyword benchmark shows a reduction of ~20,000 ticks for invoke (if we use the updated xa_nnlib from https://github.com/tensorflow/tensorflow/pull/44581)\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_keyword_benchmark -j8\r\n```\r\n\r\ngives:\r\n```\r\nInitializeKeywordRunner() took 279792 ticks (279 ms)\r\nKeywordRunNIerations(1) took 151304 ticks (151 ms)\r\nKeywordRunNIerations(10) took 1512547 ticks (1512 ms)\r\n```\r\n\r\nAlso confirmed that the kernel_fully_connected_test passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_fully_connected_test -j8\r\n```\r\n\r\nProgress towards http://b/177457688", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys", "@njeffrie, ready for review again."]}, {"number": 46410, "title": "Use CudnnRNNV3 for the GRU and LSTM", "body": "This PR replaces the use of CudnnRNN with CudnnRNNV3, which supports inputs with different layouts (time/batch major) and variable sequence lengths. With this PR, we can avoid unnecessary layout permutation and the code will be clearer.\r\n\r\n\r\nfyi. @nluehr  ", "comments": ["> Would it possible for cudnn v3 kernel to make sequence length an optional input (take care of the default sequence length logic)?\r\n\r\nDo you mean to let the cudnnrnnv3 takes care of calling the cudnnRNNforwardTraining if no sequence lengths is given or calling the cudnnRNNforwardTrainingEx if seq lengths is given?", "> > Would it possible for cudnn v3 kernel to make sequence length an optional input (take care of the default sequence length logic)?\r\n> \r\n> Do you mean to let the cudnnrnnv3 takes care of calling the cudnnRNNforwardTraining if no sequence lengths is given or calling the cudnnRNNforwardTrainingEx if seq lengths is given?\r\n\r\nSomething like that. My main concern is about the performance, and it will be nice if sequence length is just an optional input tensor for cudnnRNNforwardTrainingEx (save the cost of creating sequence length from input shapes).", "> Something like that. My main concern is about the performance, and it will be nice if sequence length is just an optional input tensor for cudnnRNNforwardTrainingEx (save the cost of creating sequence length from input shapes).\r\n\r\nYes, I understand. In fact, this PR is to pave the way for another following PR to replace the backend of CudnnRNNV3 with the new cudnnRNNForward and cudnnRNNBackwardxxx_v8, both of which require the sequence lengths array (though I think we could request CUDNN to treat it as optional if we do see performance issue for fixed seq length cases). And all the backends of CudnnRNN and CudnnRNNV2 (cudnnRNNForwardxxx(Ex) and cudnnRNNBackwardxxx(Ex)) will be deprecated.", "> > Something like that. My main concern is about the performance, and it will be nice if sequence length is just an optional input tensor for cudnnRNNforwardTrainingEx (save the cost of creating sequence length from input shapes).\r\n> \r\n> Yes, I understand. In fact, this PR is to pave the way for another following PR to replace the backend of CudnnRNNV3 with the new cudnnRNNForward and cudnnRNNBackwardxxx_v8, both of which require the sequence lengths array (though I think we could request CUDNN to treat it as optional if we do see performance issue for fixed seq length cases). And all the backends of CudnnRNN and CudnnRNNV2 (cudnnRNNForwardxxx(Ex) and cudnnRNNBackwardxxx(Ex)) will be deprecated.\r\n\r\nAck. Good to know. Approved.", "We have detected a performance regression caused by this PR, will rollback soon. ", "The regression is detected via our internal platform, and unfortunately the information is not publicly visible at the moment. @zongweiz and @sanjoy will help you troubleshoot and provide guidance for rolling forward this PR. ", "Sure. Please share more info about the regression if possible. Is it caused by the creation of seq_lengths array? Or did you see slower kernels are used in the new API?\r\n\r\nAt the same time, I've sent a request to our CUDNN team to allow the v8 RNN API takes the seq_length as nullptr and fallback to the old execution path.", "We ran 2 benchmarks internally, one on GCP and one on another internal platform. The number from GCP is stable before and after the change, but the other platform was experiencing 30% regression based on steps per second when training the model. \r\n \r\nIt might related to cuda version/env difference between GCP and the other platform, but we are not sure. @sanjoy and @zongweiz might be able to provide more insights."]}, {"number": 46409, "title": "TfLite: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'", "body": "I was trying to create the digit classifier Android app from [codelabs](https://developer.android.com/codelabs/digit-classifier-tflite#0)\r\n\r\nAt runtime, I get this error:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: pool-2-thread-1\r\n    Process: org.tensorflow.lite.codelabs.digitclassifier, PID: 18122\r\n    java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n    \r\n    Registration failed.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:72)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:237)\r\n        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.initializeInterpreter(DigitClassifier.kt:66)\r\n        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier.access$initializeInterpreter(DigitClassifier.kt:31)\r\n        at org.tensorflow.lite.codelabs.digitclassifier.DigitClassifier$initialize$1.run(DigitClassifier.kt:48)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\n        at java.lang.Thread.run(Thread.java:923)\r\n\r\nI have tried changing the tflite version on build.gradle. Any other ideas on how to resolve this issue?", "comments": ["try to use the same version of Tensorflow in the Android app in build.gradle.", "Check out the version of Tensorflow which you use while training, Use the same version in android build.gridle(app).\r\nI use tensorflow 2.4.0 (which is latest) in colab while training, so I put (implementation 'org.tensorflow:tensorflow-lite:2.4.0')  in my android build.gridle(app)"]}, {"number": 46408, "title": "tf.math.tanh makes silently crash Python", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 Bits build 21286\r\n- TensorFlow installed from (source or binary): binary from pip\r\n- TensorFlow version (use command below): 2.5.0-dev20210113\r\n- Python version: Python 3.8.5 with Miniconda 4.9.2\r\n- CUDA/cuDNN version: CUDA 11.0 / CuDNN 8.0.6\r\n- GPU model and memory: GeForce RTX 2080 Super with Max-Q Design with 8Go\r\n\r\n**Describe the current behavior**\r\nThe Python interpreter crashes silently without any error when using the `tf.math.tanh()` function.\r\n\r\n**Describe the expected behavior**\r\nWorks without crashing\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = tf.constant([-float(\"inf\"), -5, -0.5, 1, 1.2, 2, 3, float(\"inf\")])\r\ntf.math.tanh(x)\r\n```\r\n**Other info / logs** \r\nWhen running `python -u -m trace -t crash.py` the last lines are:\r\n```\r\n --- modulename: gen_math_ops, funcname: tanh\r\ngen_math_ops.py(10769):   _ctx = _context._context or _context.context()\r\ngen_math_ops.py(10770):   tld = _ctx._thread_local_data\r\ngen_math_ops.py(10771):   if tld.is_eager:\r\ngen_math_ops.py(10772):     try:\r\ngen_math_ops.py(10773):       _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\ngen_math_ops.py(10774):         _ctx, \"Tanh\", name, x)\r\ngen_math_ops.py(10773):       _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\n```\r\nYou can also find in attachment a screenshot of the crash.\r\n![crash](https://user-images.githubusercontent.com/959590/104497672-011feb80-55db-11eb-8b6b-a65b8f6aeb69.png)\r\n", "comments": ["@jplu \r\n\r\nI have tried in colab with TF nightly -GPU version(`2.5.0-dev20210113`) and I am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/90c043459bf2e5f16ea087772ab8114d/untitled618.ipynb).Colab uses cuda 10.1. \r\n\r\nCan you check with latest stable version 2.4 and see if the issue still persists.Thanks!", "I'm on Windows, Colab in on Linux. Unfortunately, there is no point for me to test on a colab.\r\n\r\nAnyway, TF 2.4 works on Windows but I want to use the nightly builds because it contains an important fix that the 2.4 doesn't have.", "I have a similar issue - fairly basic DCGAN from the Tensorflow Tutorial (https://www.tensorflow.org/tutorials/generative/dcgan) which worked just fine in TF 2.4 is crashing now in 2.5 (I need to use 2.5 to enable other models to function properly with all the version issues between Ampere and CUDA/CUDNN/TF)\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Mostly No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 Bit, Version 20H2 / 19042.746\r\nTensorFlow installed from (source or binary): binary from pip\r\nTensorFlow version (use command below): 2.5.0-dev20210122\r\nPython version: Python 3.8 using Anaconda\r\nCUDA/cuDNN version: CUDA 11.1.1_456.81_win10 / CuDNN 8.0.5.39\r\nGPU model and memory: GeForce RTX 3080 \r\n\r\nIt's the same model as the DCGAN in the tutorial, just with a few tweaks to the kernels and image sizes for my dataset... worked just fine in TF 2.4. \r\n\r\nThis line in the generator causes a Kernel Error (no error codes given / big red screen in the Spyder Ipython console)\r\n\r\n_model.add(layers.Conv2DTranspose(3, (3, 3), strides=(2, 2), padding='same', use_bias=False, activation='**tanh**'))_\r\n\r\nbut if I change it to \r\n\r\nmodel.add(layers.Conv2DTranspose(3, (3, 3), strides=(2, 2), padding='same', use_bias=False, activation='**relu**'))\r\n\r\nit seems to run ... although training results are quite bad, so I can't tell if its just the activation or other issues are happening at the same time but not driving errors. \r\n\r\n\r\n\r\n", "I should mention I get the exact same result when using the OP's code \r\n\r\n![image](https://user-images.githubusercontent.com/39113957/105802454-6fa86600-5f58-11eb-90b5-7f06622ab0a6.png)\r\n", "I confirm identical behavior as @RhynoTime described.\r\nI checked various tf-nightly versions (installed via `pip`) against `tanh` crashes, results below.\r\n\r\n```\r\ntf-nightly==2.5.0.dev20201212 - OK\r\ntf-nightly==2.5.0.dev20201213 - OK\r\ntf-nightly==2.5.0.dev20201214 - OK\r\ntf-nightly==2.5.0.dev20201215 - ERR\r\ntf-nightly==2.5.0.dev20201216 - ERR\r\n\r\ntf-nightly==2.5.0.dev20201225 - ERR\r\n\r\ntf-nightly==2.5.0.dev20210101 - ERR\r\n```\r\n\r\nEdit:\r\nI found suspicious commit (https://github.com/tensorflow/tensorflow/commit/9618f238aad1d0547b7e96519e089ab0ab38cab1) from 14 Dec 2020, relating to `tanh` and `abs`. **Additionaly i figured out `abs` crashes too.**\r\n\r\nEdit2:\r\nProbably related to https://github.com/tensorflow/tensorflow/issues/46078", "this issue still persists on latest version\r\n`2.5.0.dev20210210`", "Thank you for reporting this. I have verified that this is fixed in a recent build from source. The next nightly build should also contain the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46408\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46408\">No</a>\n"]}, {"number": 46407, "title": "Support for directly importing Dense layer from keras API", "body": "**System information**\r\n- TensorFlow version (you are using): TF 4.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhy Dense?\r\n\r\nWhile i was writing code i noticed Dense layer is not supported in importing directly in keras but input layer is there. In that case anyhow we have to mention full path dependency because this layer is the most basic layer and frequently used while building the architecture even if not initially used then at least last few layers are dense layers only mostly. It can be used as feed forward neural network as well as in the output layer.so I think it will be good if this layer can be directly imported from keras without any path dependencies,\r\n\r\n[ `from tensorflow.python.keras.engine.input_layer import Input`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/__init__.py) \r\n\r\nJust adding Dense here like this will do\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nNothing is going to change in api. This will be just an additional functionality for the users to serve their purpose easily.\r\n\r\n\r\n", "comments": ["@zyberg2091 , Can we work on this issue ?\r\nIs it assigned to any one ?", "@zyberg2091 Just want to understand your use case. \r\nCurrently we import `Input` and `Dense` layers as follows\r\n\r\n```\r\nfrom tensorflow.keras import Input\r\nfrom tensorflow.keras.layers import Dense\r\n```\r\nConsidering a simple example model, \r\n```\r\n  model = tf.keras.models.Sequential([\r\n    Dense(512, activation='relu', input_shape=(784,)),\r\n    Dense(10)\r\n  ]) \r\n```\r\nIn the above model, we are using `Dense` layer as simply `Dense`.\r\n\r\nIf I understand correctly, you want to import Dense directly as \r\n\r\n`from tensorflow.keras import Dense`\r\n\r\nCurrently `Dense` layer is part of `layers` API. Whether `Dense` is part of `Layers` or part of `keras` (similar to `Input` layer), the model building part of the code is same. \r\n\r\nI want to understand what do we accomplish from this requested feature (by moving `Dense` layer ). Is there anything missing from the current implementation that cannot work with your use-case? If we simply move out `Dense`, it may confuse the large community of existing users. Please let us know your use case with little more details. Thanks!\r\n\r\n\r\n", "@jvishnuvardhan considering your simple sequential API model.\r\ni can just import those layers in just one 1 line from keras engine.These is preventing me from just writing extra useless line.well all of us are aware that these two layers have to be imported in most use cases because these two layers are basic layers.for my project i just needed these two layers and if i have to use another layer other than input than i have to write like this-\r\n\r\n`from tensorflow.keras.layers import Input,Dense`\r\n\r\nThen i don't understand this approach-\r\n\r\n`from tensorflow.keras import Input`\r\n`from tensorflow.keras.layers Import Dense`\r\n\r\nUsers may have different use cases according to their needs and so importing layers become specific.Therefore i understand other layers might be imported from keras layers API but if we are able to import Input layer directly why can't we do this to dense layer which is equally basic as Input layer.\r\n\r\nThis also creates confusion and i am unable to think where can we use this `From tensorflow.keras import Input` alone\r\n\r\nMost simple use case can be -\r\n\r\n`from tensorflow.keras import Input,Dense` for importing layers from keras engine\r\n", "To clarify, you want us to export tf.keras.layers.Dense with an alias tf.keras.Dense?", "yes @tomerk ", "Notes from our internal discussion about this.\r\nWe don't want to blur layer/non-layer api boundaries in Keras like this.\r\n\r\n`keras.Input` is not a layer, it is a symbolic input object that can be passed to layers.\r\n`keras.layers.InputLayer` is the corresponding input layer, which is *not* aliased under `keras` directly.\r\n\r\n(`from tensorflow.python.keras.engine.input_layer import Input` might refer to the internal file location of `keras.Input`, but it's not a layer itself and as such we don't expose it under `keras.layers`.)"]}, {"number": 46406, "title": "Cleanup `c/experimental/gradients/` part 3", "body": "@saxenasaurabh \r\nCould you take a look at this PR ? Thank you !", "comments": ["@saxenasaurabh \r\n\r\nWith a396295, each gradient in `math_grad` is tested and their tests ( as well as their helpers like `*GradModel` ) in other places ( `gradient_tests` ) are removed ( except `TestMatMulGrad`, please see my comment in the code ).\r\n\r\n2 things I would like to discuss with you:\r\n- With `gradient_checker`, I think we should compute `(f(x + eps) - f(x))/eps` instead of `(f(x + eps) - f(x - eps))/2*eps`. This way, we could avoid round-off errors when computing `2*eps` and `x-eps`.\r\n- I think we will need one more PR for cleaning up `c/experimental/gradients` ( mostly `mnist_gradients_test` and friends ). We should start thinking what we gonna do with [unified_api_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/experimental/unified_api_test.py).\r\n\r\nWDYT ?", "> @saxenasaurabh\r\n> \r\n> With [a396295](https://github.com/tensorflow/tensorflow/commit/a3962951d81e08526edf6eef232d346148008af2), each gradient in `math_grad` is tested and their tests ( as well as their helpers like `*GradModel` ) in other places ( `gradient_tests` ) are removed ( except `TestMatMulGrad`, please see my comment in the code ).\r\n> \r\n> 2 things I would like to discuss with you:\r\n> \r\n> * With `gradient_checker`, I think we should compute `(f(x + eps) - f(x))/eps` instead of `(f(x + eps) - f(x - eps))/2*eps`. This way, we could avoid round-off errors when computing `2*eps` and `x-eps`.\r\n\r\nCould you explain a bit more here? Right now this matches the logic in the [python gradient checker](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/ops/gradient_checker_v2.py;l=236;drc=1427bfc12ec5a3a2c6a4ffd57fc5b465d3eedfae)\r\n\r\n> * I think we will need one more PR for cleaning up `c/experimental/gradients` ( mostly `mnist_gradients_test` and friends ). We should start thinking what we gonna do with [unified_api_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/experimental/unified_api_test.py).\r\n> \r\n> WDYT ?\r\n\r\nWe could punt on cleaning that up. I am working on calling C++ gradient function from production python APIs. Once we have that we could simply delete the gradient tests here. The MNIST MLP benchmark is still useful. We should add a similar one for ResNet once we have all gradient functions for that.", "> Could you explain a bit more here?\r\n\r\nYou could see that `gradient_checker` does not work very well with `MatMul`. If we use `(f(x + eps) - f(x - eps))/2*eps`, we have to compute `a:=x+eps`, `b:=x-eps` and `c:=2*eps`. And because of the limitation of precision, `a` does not mathematically equal to `x+eps`, it is just an approximation. The same thing holds for `b` and `c`. So we have 3 sources of error. If we use `(f(x + eps) - f(x))/eps`, we only have 1 source.\r\n\r\nI am not sure if I am missing something and why the `gradient_checker` in python is implemented that way \ud83d\ude15 ( I am really curious about this, could you elaborate it ? Thank you ! )\r\n\r\nWhat do you think ?\r\n\r\nEdit: No, it isn't the root cause. I will take a look at somewhere else. I could say that the error is not related to `MatMulGradient` because we have `TestMatMulGrad` inside `mnist_gradients_test`. I just take a look at python and it seems that python is having a problem with `gradient_checker` and `MatMulGrad` too. IIUC, `float` in `c++` has the same precision as `bfloat16`, right ? https://github.com/tensorflow/tensorflow/blob/120bd289f4483dbd10e44a0038d05d5451f95453/tensorflow/python/kernel_tests/sparse_matmul_op_test.py#L177-L185\r\n\r\nSo I think we could leave the `abs_error = 0.4` and remove the TODO inside the code. What do you think ?", "@saxenasaurabh Could you take a look at this PR ? Thank you !", "> You could see that gradient_checker does not work very well with MatMul. If we use (f(x + eps) - f(x - eps))/2*eps, we have to compute a:=x+eps, b:=x-eps and c:=2*eps. And because of the limitation of precision, a does not mathematically equal to x+eps, it is just an approximation. The same thing holds for b and c. So we have 3 sources of error. If we use (f(x + eps) - f(x))/eps, we only have 1 source.\r\n\r\nIn general the [symmetric derivative](https://en.wikipedia.org/wiki/Symmetric_derivative) is a much better approximation of the derivative. To understand this you could try writing down the taylor series expansion which shows that the error for the symmetric derivative is `O(h^2)` compared to `O(h)` for the non-symmetric one.", "@saxenasaurabh \r\n\r\nLinux GPU is falling because `abs_error` is not big enough. Do you have any solution for it ?\r\n\r\nEdit: I think `RunAndMaybeSum` might be the culprit. Do you think we should try to completely mimic the implementation of python (`_compute_theoretical_jacobian` and `_compute_numeric_jacobian` ) ? ", "> @saxenasaurabh\r\n> \r\n> Linux GPU is falling because `abs_error` is not big enough. Do you have any solution for it ?\r\n> \r\n> Edit: I think `RunAndMaybeSum` might be the culprit. Do you think we should try to completely mimic the implementation of python (`_compute_theoretical_jacobian` and `_compute_numeric_jacobian` ) ?\r\n\r\n\r\nAbs error of 0.4 seems too high. We should try to get to the bottom of this. If the python implementation for gradient checker is more accurate we should copy that. For now we could disable the Matmul test if that is only one failing and fix the gradient checker and enable this test in a follow up.", "> For now we could disable the Matmul test\r\n\r\nDone", "Linux GPU is falling because of unrelated tests so no need to worries."]}, {"number": 46405, "title": "LSTM crashing during training - Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Win 10 Enterprice LTSC\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: Python 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: Build cuda_11.0_bu.relgpu_drvr445TC445_37.28845127_0 / cuDNN v 8.0.5 for CUDA 11.0\r\n- GPU model and memory: GTX 960 (Asus Strix Direct DCU2 OC 4GB)\r\n\r\n**Describe the current behavior**\r\nWhen training a network with LSTM layers it crashes randomly during epochs. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/Kalhama/tensorflow-lstm-crash\r\n\r\n**Things I've checked and are ok / have no effect**\r\n\u2705 Only occurs when training on GPU\r\n\u2705 Only occurs when using LSTM layers\r\n\u2705 Underclocking GPU, raising voltages etc.. (in case stock overclock would be unstable)\r\n\u2705 CPU and GPU temps (either never exceeds 60 deg C)\r\n\u2705 Different driver versions. I earlier had 460.89 but it's the same on 451.82\r\n\u2705 There is enough VRAM / RAM. (I used `tf.config.experimental.set_memory_growth()` and then Win task manager to check VRAM utilization. Only used max 1GB of VRAM on large networks)\r\n\r\n**Things I've checked and have some effect, but do not remove problem completely**\r\n\u2705 Different network sizes. All networks seem to have this issue but larger networks feel to be more prone to crash fast\r\n\u2705 Different batch sizes and different train data sizes. Overall larger batches and longer epochs are more stable. \r\n\r\n**Other info / logs**\r\n```2021-01-12 21:38:15.476512: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2021-01-12 21:38:15.476581: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1859): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2021-01-12 21:38:15.483573: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n\r\n2\r\n```\r\n\r\nSometimes the error looks like this:\r\n```\r\n2021-01-12 21:41:41.096689: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1972): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2021-01-12 21:41:41.127515: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 3, 200, 1, 200, 32, 200]\r\nTraceback (most recent call last):\r\n  File \"c:/Users/User/Code/tensorflow/main.py\", line 61, in <module>\r\n    main()\r\n  File \"c:/Users/User/Code/tensorflow/main.py\", line 40, in main\r\n    model.train(\r\n  File \"c:\\Users\\User\\Code\\tensorflow\\core\\model.py\", line 47, in train\r\n    self.model.fit(\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InternalError:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 3, 200, 1, 200, 32, 200]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[Adam/gradients/PartitionedCall_2]] [Op:__inference_train_function_7900]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```", "comments": ["Updates:\r\n\r\n1. I updated python to 3.8.7, no effect\r\n2. I tried using tf-nightly (2.5.0-dev20210113) but still got same error\r\n3. I was able to replicate the issue on another computer:\r\n\r\n- OS Platform and Distribution: Win 10 Enterprice LTSC\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: \r\n- Python version: Python 3.8.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 11.0  / cuDNN v8.0.4 (September 28th, 2020), for CUDA 11.0\r\n- GPU model and memory: GTX 1060 6GB", "@Kalhama,\r\nAs per this comment from issue [#45987](https://github.com/tensorflow/tensorflow/issues/45987#issuecomment-751934074) with a similar error log, updating the NVIDIA drivers seems to work. Could you please take a look at it and let us know if it helps. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hello!\n\nI updated the driver as you suggested and this hasnt occured since. Today I also ran the example repo I provided for quite good time +30min without errors.", "Can be closed", "@Kalhama,\r\nThank you for the update. Closing the issue as it is resolved. Please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46405\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46405\">No</a>\n"]}, {"number": 46404, "title": "Add support of tf.qint16 and tf.quint16 for tf.stack", "body": "This PR is part of the effort for #26069 where `tf.stack` support most of the qtypes (`tf.qint8/tf.quint8/tf.qint32`)\r\nbut not `tf.qint16` and `tf.quint16`. The reason was that `TF_CALL_QUANTIZED_TYPES` does not include `qint16` and `quint16`.\r\n\r\nThis PR also update to add `qint32` for tf.equal and tf.not_equal to be consistent with other ops.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 46403, "title": "IndexError webcam object detection", "body": "Hi,\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.10\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.6\r\n- GPU model and memory: None\r\n\r\nI followed the tutorial [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/object_detection_camera.html#sphx-glr-download-auto-examples-object-detection-camera-py) to make a real time object detection. I followed all the step to install tensorflow and tensorflow object detection API and everything was working on their test files. I came back to the tutorial and launch the code everything was working good (model downloaded, ...) but I came with an IndexError issue.\r\n\r\nError :\r\n```\r\nobject_detection_camera.py:167 detect_fn  *\r\n        image, shapes = detection_model.preprocess(image)\r\n    /home/name/.local/lib/python3.8/site-packages/object_detection/meta_architectures/ssd_meta_arch.py:482 preprocess  *\r\n        normalized_inputs = self._feature_extractor.preprocess(inputs)\r\n    /home/name/.local/lib/python3.8/site-packages/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py:204 preprocess  *\r\n        if resized_inputs.shape.as_list()[3] == 3:\r\n\r\n    IndexError: list index out of range\r\n```\r\n\r\nHere the code :\r\n```\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\"\"\"\r\nDetect Objects Using Your Webcam\r\n================================\r\n\"\"\"\r\n\r\n# %%\r\n# This demo will take you through the steps of running an \"out-of-the-box\" detection model to\r\n# detect objects in the video stream extracted from your camera.\r\n\r\n# %%\r\n# Create the data directory\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# The snippet shown below will create the ``data`` directory where all our data will be stored. The\r\n# code will create a directory structure as shown bellow:\r\n#\r\n# .. code-block:: bash\r\n#\r\n#     data\r\n#     \u2514\u2500\u2500 models\r\n#\r\n# where the ``models`` folder will will contain the downloaded models.\r\nimport os\r\n\r\nDATA_DIR = os.path.join(os.getcwd(), 'data')\r\nMODELS_DIR = os.path.join(DATA_DIR, 'models')\r\nfor dir in [DATA_DIR, MODELS_DIR]:\r\n    if not os.path.exists(dir):\r\n        os.mkdir(dir)\r\n\r\n# %%\r\n# Download the model\r\n# ~~~~~~~~~~~~~~~~~~\r\n# The code snippet shown below is used to download the object detection model checkpoint file,\r\n# as well as the labels file (.pbtxt) which contains a list of strings used to add the correct\r\n# label to each detection (e.g. person).\r\n#\r\n# The particular detection algorithm we will use is the `SSD ResNet101 V1 FPN 640x640`. More\r\n# models can be found in the `TensorFlow 2 Detection Model Zoo <https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md>`_.\r\n# To use a different model you will need the URL name of the specific model. This can be done as\r\n# follows:\r\n#\r\n# 1. Right click on the `Model name` of the model you would like to use;\r\n# 2. Click on `Copy link address` to copy the download link of the model;\r\n# 3. Paste the link in a text editor of your choice. You should observe a link similar to ``download.tensorflow.org/models/object_detection/tf2/YYYYYYYY/XXXXXXXXX.tar.gz``;\r\n# 4. Copy the ``XXXXXXXXX`` part of the link and use it to replace the value of the ``MODEL_NAME`` variable in the code shown below;\r\n# 5. Copy the ``YYYYYYYY`` part of the link and use it to replace the value of the ``MODEL_DATE`` variable in the code shown below.\r\n#\r\n# For example, the download link for the model used below is: ``download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.tar.gz``\r\nimport tarfile\r\nimport urllib.request\r\n\r\n# Download and extract model\r\nMODEL_DATE = '20200711'\r\nMODEL_NAME = 'ssd_resnet101_v1_fpn_640x640_coco17_tpu-8'\r\nMODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'\r\nMODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'\r\nMODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME\r\nPATH_TO_MODEL_TAR = os.path.join(MODELS_DIR, MODEL_TAR_FILENAME)\r\nPATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))\r\nPATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))\r\nif not os.path.exists(PATH_TO_CKPT):\r\n    print('Downloading model. This may take a while... ', end='')\r\n    urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)\r\n    tar_file = tarfile.open(PATH_TO_MODEL_TAR)\r\n    tar_file.extractall(MODELS_DIR)\r\n    tar_file.close()\r\n    os.remove(PATH_TO_MODEL_TAR)\r\n    print('Done')\r\n\r\n# Download labels file\r\nLABEL_FILENAME = 'mscoco_label_map.pbtxt'\r\nLABELS_DOWNLOAD_BASE = \\\r\n    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\r\nPATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))\r\nif not os.path.exists(PATH_TO_LABELS):\r\n    print('Downloading label file... ', end='')\r\n    urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)\r\n    print('Done')\r\n\r\n# %%\r\n# Load the model\r\n# ~~~~~~~~~~~~~~\r\n# Next we load the downloaded model\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging\r\nimport tensorflow as tf\r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import config_util\r\nfrom object_detection.utils import visualization_utils as viz_utils\r\nfrom object_detection.builders import model_builder\r\n\r\ntf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\r\n\r\n# Enable GPU dynamic memory allocation\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n# Load pipeline config and build a detection model\r\nconfigs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\r\nmodel_config = configs['model']\r\ndetection_model = model_builder.build(model_config=model_config, is_training=False)\r\n\r\n# Restore checkpoint\r\nckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\r\nckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\r\n\r\n@tf.function\r\ndef detect_fn(image):\r\n    \"\"\"Detect objects in image.\"\"\"\r\n\r\n    image, shapes = detection_model.preprocess(image)\r\n    prediction_dict = detection_model.predict(image, shapes)\r\n    detections = detection_model.postprocess(prediction_dict, shapes)\r\n\r\n    return detections, prediction_dict, tf.reshape(shapes, [-1])\r\n\r\n\r\n# %%\r\n# Load label map data (for plotting)\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# Label maps correspond index numbers to category names, so that when our convolution network\r\n# predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility\r\n# functions, but anything that returns a dictionary mapping integers to appropriate string labels\r\n# would be fine.\r\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\r\n                                                                    use_display_name=True)\r\n\r\n# %%\r\n# Define the video stream\r\n# ~~~~~~~~~~~~~~~~~~~~~~~\r\n# We will use `OpenCV <https://pypi.org/project/opencv-python/>`_ to capture the video stream\r\n# generated by our webcam. For more information you can refer to the `OpenCV-Python Tutorials <https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html#capture-video-from-camera>`_\r\nimport cv2\r\n\r\ncap = cv2.VideoCapture(0)\r\n\r\n# %%\r\n# Putting everything together\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# The code shown below loads an image, runs it through the detection model and visualizes the\r\n# detection results, including the keypoints.\r\n#\r\n# Note that this will take a long time (several minutes) the first time you run this code due to\r\n# tf.function's trace-compilation --- on subsequent runs (e.g. on new images), things will be\r\n# faster.\r\n#\r\n# Here are some simple things to try out if you are curious:\r\n#\r\n# * Modify some of the input images and see if detection still works. Some simple things to try out here (just uncomment the relevant portions of code) include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\r\n# * Print out `detections['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\r\n# * Set ``min_score_thresh`` to other values (between 0 and 1) to allow more detections in or to filter out more detections.\r\nimport numpy as np\r\n\r\nwhile True:\r\n    # Read frame from camera\r\n    ret, image_np = cap.read()\r\n\r\n    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n    image_np_expanded = np.expand_dims(image_np, axis=0)\r\n\r\n    # Things to try:\r\n    # Flip horizontally\r\n    # image_np = np.fliplr(image_np).copy()\r\n\r\n    # Convert image to grayscale\r\n    # image_np = np.tile(\r\n    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\r\n\r\n    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\r\n    detections, predictions_dict, shapes = detect_fn(input_tensor)\r\n\r\n    label_id_offset = 1\r\n    image_np_with_detections = image_np.copy()\r\n\r\n    viz_utils.visualize_boxes_and_labels_on_image_array(\r\n          image_np_with_detections,\r\n          detections['detection_boxes'][0].numpy(),\r\n          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\r\n          detections['detection_scores'][0].numpy(),\r\n          category_index,\r\n          use_normalized_coordinates=True,\r\n          max_boxes_to_draw=200,\r\n          min_score_thresh=.30,\r\n          agnostic_mode=False)\r\n\r\n    # Display output\r\n    cv2.imshow('object detection', cv2.resize(image_np_with_detections, (800, 600)))\r\n\r\n    if cv2.waitKey(25) & 0xFF == ord('q'):\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n```\r\n", "comments": ["@Yosonix \r\nPlease post this in models repo and close this.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46403\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46403\">No</a>\n"]}, {"number": 46401, "title": "DataFrameIterator is not exported", "body": "https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/preprocessing/image.py#L464\r\n\r\nThe `DataFrameIterator` class is not exported, so it can't be extended from outside the framework.", "comments": ["It is used in [ImageDataGenerator's](https://github.com/keras-team/keras/blob/v2.7.0/keras/preprocessing/image.py#L597-L1137) [flow_from_directory](https://github.com/keras-team/keras/blob/v2.7.0/keras/preprocessing/image.py#L898) where it [returns](https://github.com/keras-team/keras/blob/v2.7.0/keras/preprocessing/image.py#L1117) `DataFrameIterator`. \r\nIt is not directly exposed with export but internally being used as mentioned above.", "If the class was exported, we could extend it to change its default behavior. For example, `DirectoryIterator`, `NumpyArrayIterator`, `ImageDataGenerator`, etc. are all exported.\r\n\r\nCan we also export `DataFrameIterator`?", "Please check latest comments here https://github.com/keras-team/keras/pull/16008#issuecomment-1030343173 , this has been deprecated and meant for internal use only. Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46401\">No</a>\n"]}, {"number": 46400, "title": "Installation of OpenCL backend Tensorflow Lite runtime package", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04 aarch64\r\n- Mobile device:  RK3399 with Mali T860 GPU\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:  virtualenv\r\n- GCC/Compiler version (if compiling from source): gcc 7.5\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: Mali T860\r\n\r\n**Describe the problem**\r\n\r\nI am going to install OpenCL backend TensorFlow Lite runtime for Python on RK3399 Ubuntu 18.04 aarch64 platform.\r\nI looked at the document and blogs but could NOT find any method to install OpenCL backend TensorFlow Lite runtime for Python.\r\nI think that OpenCL backend TensorflowLite for Python should be installed from the source.\r\nIf so, how should it be built and installed?\r\nCould u provide a proper method asap?\r\nThanks\r\n", "comments": ["Dup of https://github.com/tensorflow/tensorflow/issues/46498", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46400\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46400\">No</a>\n"]}, {"number": 46399, "title": "Add RFC for pre-allocated tensors.", "body": "Add a RFC-document for the pre-allocated tensors feature.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@jenselofsson  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "> looks good to me.\r\n> \r\n> It might be worth a sub-folder in images called preallocated_tensors so that there is a bit of a directory structure there.\r\n> \r\n\r\nGood idea!\r\n\r\n> And could you also link to this from the top-level README (just add to the bottom of the list for now, and we'll reorganize later):\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/78fdd635abb65422c5a4b72750e9fafccf8bee61/tensorflow/lite/micro/README.md#L65-L69\r\n\r\nSure!", "copybara failed, triggering it again."]}, {"number": 46398, "title": "Switching to Cadence HiFi 4 NN Library v2.3.0", "body": "Using the latest version of HiFi 4 NN Library.\r\n\r\nThis version has optimized implementation of FC and softmax for int8 datatype.\r\n\r\nTested the change using following commands:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifi4 XTENSA_TOOLS_VERSION=RI-2020.5-linux XTENSA_CORE=AE_HiFi4_LE5_FP_XC clean_downloads\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifi4 XTENSA_TOOLS_VERSION=RI-2020.5-linux XTENSA_CORE=AE_HiFi4_LE5_FP_XC test_kernel_softmax_test\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @advaitjain @nyadla-sys "]}]