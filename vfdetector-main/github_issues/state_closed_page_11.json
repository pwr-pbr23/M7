[{"number": 55157, "title": "Fix Reference Type Part 2", "body": "Fix Reference Type Part 2\n", "comments": []}, {"number": 55156, "title": "[XNNPACK] Change ConcatenationTester to be able to take multiple input shapes", "body": "[XNNPACK] Change ConcatenationTester to be able to take multiple input shapes\n\nThis will allow us to test concatenation with more than 2 inputs.\n", "comments": []}, {"number": 55155, "title": "[XNNPACK] Rename concatenation tests to include number of ouputs in name", "body": "[XNNPACK] Rename concatenation tests to include number of ouputs in name\n\nAlso introduce a helper to create shapes that match in all dimensions except the axis.\n", "comments": []}, {"number": 55154, "title": "Bump the number of samples for MultinomialOpTestSuite.NonDeterministicOutputWithSeedsEqualToZero to 30", "body": "Bump the number of samples for MultinomialOpTestSuite.NonDeterministicOutputWithSeedsEqualToZero to 30\n\nThis matches the number of samples used in MultinomialOpTestSuite.DeterministicOutputWithNonZeroSeeds and also makes the test less flaky. Previously, running the test 100 times will result in some failures.\n", "comments": []}, {"number": 55153, "title": "1. Stop the service in the event of heartbeat failure if there is no server-side connection.", "body": "1. Stop the service in the event of heartbeat failure if there is no server-side connection.\n\n2. Log other errors if there is no server-side connection.\n", "comments": []}, {"number": 55152, "title": "[KernelGen][JIT] Do not apply CL options when JIT-compiling", "body": "[KernelGen][JIT] Do not apply CL options when JIT-compiling\n\nWhen CL options are not available, i.e. when compilation is invoked through the\nJIT, do not try to apply them. This eliminates a redundant warning observed\nwhen running JIT-compiled kernels.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/55031 from wamuir:worker-proto-unused-import 3a84bebfbb72ba837102305e203d741c365ba40f\n", "comments": []}, {"number": 55151, "title": "Canonicalize CPU device name used in tfrt_fallback_async.corert_tensorhandle_to_fallback_tensor", "body": "Canonicalize CPU device name used in tfrt_fallback_async.corert_tensorhandle_to_fallback_tensor\n\nCoreRT library only recoganize the default CPU device name \"/job:localhost/replica:0/task:0/device:CPU:0\" and it cannot recoganize other variants like \"/device:CPU:0\".\n", "comments": []}, {"number": 55150, "title": "Tag `tf.keras.preprocessing` as deprecated when generating docs.", "body": "Tag `tf.keras.preprocessing` as deprecated when generating docs.\n\nAdd a \"status:deprecated\" to `tf.keras.preprocessing` in the tensorflow.org TOC, just like for contrib here:\n\nhttps://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib\n", "comments": []}, {"number": 55149, "title": "[KernelGen][JIT] Set target triple for GPU kernels", "body": "[KernelGen][JIT] Set target triple for GPU kernels\n\nSet target triple explicitly for GPU kernels and do not rely on it being linked\nin through the devicelib. For kernels that to no require the devicelib, the\ntriple was missing, causing a warning at compile time, and at runtime for\njitted kernels.\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/55031 from wamuir:worker-proto-unused-import 3a84bebfbb72ba837102305e203d741c365ba40f\n", "comments": []}, {"number": 55148, "title": "Build of tests //tensorflow/core/transforms/... fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.8.12\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 5.0.0\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAttempting to build //tensorflow/core/transforms/... fails with many undefined references\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --config=nonccl --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --jobs=75 -- //tensorflow/core/transforms/...\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\nERROR: /tmp/workspace/tensorflow-git/tensorflow/core/transforms/BUILD:92:20: Linking tensorflow/core/transforms/tfg-transforms-opt failed: (Exit 1): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/db210f68f81d95ddcca9ae96b16ed72c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/opt/rh/devtoolset-10/root/usr/lib64:/opt/rh/devtoolset-10/root/usr/lib:/opt/rh/devtoolset-10/root/usr/lib64/dyninst:/opt/rh/devtoolset-10/root/usr/lib/dyninst:/usr/local/lib64 \\\r\n    PATH=/tmp/workspace/venv-cp38-cp38/bin:/opt/rh/devtoolset-10/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/tmp/workspace/venv-cp38-cp38/bin/python3 \\\r\n    PYTHON_LIB_PATH=/tmp/workspace/venv-cp38-cp38/lib/python3.8/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /opt/rh/devtoolset-10/root/usr/bin/gcc @bazel-out/aarch64-opt/bin/tensorflow/core/transforms/tfg-transforms-opt-2.params)\r\n# Configuration: adf76bbb856b8d7e1075d9fa81311a63f070542e9be48c5b759e905be0f8e507\r\n# Execution platform: @local_execution_config_platform//:platform\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::~FactOp(): error: undefined reference to 'tensorflow::OpKernel::~OpKernel()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::~FactOp(): error: undefined reference to 'tensorflow::OpKernel::~OpKernel()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function register_kernel_1::{lambda(tensorflow::KernelDef const*)#1}::operator()(tensorflow::KernelDef const) const::{lambda(register_kernel_1::OpKernelConstruction*)#1}::_FUN(register_kernel_1::{lambda(tensorflow::KernelDef const*)#1}): error: undefined reference to 'tensorflow::OpKernel::OpKernel(tensorflow::OpKernelConstruction*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::Tensor::CheckIsAlignedAndSingleElement() const'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::TensorShapeRep::DestructorOutOfLine()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::CheckNotInComputeAsync(tensorflow::OpKernelContext*, char const*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function FactOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'tensorflow::OpKernelContext::CtxFailureWithWarning(char const*, int, tensorflow::Status const&)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::OpDefBuilder::OpDefBuilder(std::string)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::OpDefBuilder::Output(std::string)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::shape_inference::UnknownShape(tensorflow::shape_inference::InferenceContext*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::shape_inference::UnknownShape(tensorflow::shape_inference::InferenceContext*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::OpDefBuilder::SetShapeFn(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)>)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::register_op::OpDefBuilderWrapper::operator()()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::OpDef::~OpDef()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::KernelDefBuilder::KernelDefBuilder(char const*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::DEVICE_CPU'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::DEVICE_CPU'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::KernelDefBuilder::Device(char const*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::KernelDefBuilder::Build()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'vtable for tensorflow::kernel_factory::OpKernelRegistrar::PtrOpKernelFactory'\r\n/opt/rh/devtoolset-10/root/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'vtable for tensorflow::kernel_factory::OpKernelRegistrar::PtrOpKernelFactory'\r\n/opt/rh/devtoolset-10/root/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_20211102::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:function __static_initialization_and_destruction_0(int, int) [clone .constprop.0]: error: undefined reference to 'tensorflow::KernelDefBuilder::~KernelDefBuilder()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:typeinfo for FactOp: error: undefined reference to 'typeinfo for tensorflow::OpKernel'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/user_ops/_objs/user_ops_op_lib/fact.o:fact.cc:vtable for FactOp: error: undefined reference to 'tensorflow::OpKernel::TraceString(tensorflow::OpKernelContext const&, bool) const'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::register_op19::{lambda(tensorflow::shape_inference::InferenceContext*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&): error: undefined reference to 'tensorflow::shape_inference::UnchangedShape(tensorflow::shape_inference::InferenceContext*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::register_op10::{lambda(tensorflow::shape_inference::InferenceContext*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&): error: undefined reference to 'tensorflow::shape_inference::ConcatShape(tensorflow::shape_inference::InferenceContext*, int)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function tensorflow::shape_inference::ScalarShape(tensorflow::shape_inference::InferenceContext*): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::Scalar()'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::register_op57::{lambda(tensorflow::shape_inference::InferenceContext*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::Vector(tensorflow::shape_inference::DimensionOrConstant)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::register_op57::{lambda(tensorflow::shape_inference::InferenceContext*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, long, tensorflow::shape_inference::ShapeHandle*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::register_op59::{lambda(tensorflow::shape_inference::InferenceContext*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::Vector(tensorflow::shape_inference::DimensionOrConstant)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function tensorflow::(anonymous namespace)::ScatterNdTensorShape(tensorflow::shape_inference::InferenceContext*): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::WithRankAtLeast(tensorflow::shape_inference::ShapeHandle, long, tensorflow::shape_inference::ShapeHandle*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function tensorflow::(anonymous namespace)::ScatterNdTensorShape(tensorflow::shape_inference::InferenceContext*): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::WithRankAtLeast(tensorflow::shape_inference::ShapeHandle, long, tensorflow::shape_inference::ShapeHandle*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function tensorflow::(anonymous namespace)::ScatterNdTensorShape(tensorflow::shape_inference::InferenceContext*): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::WithRankAtLeast(tensorflow::shape_inference::ShapeHandle, long, tensorflow::shape_inference::ShapeHandle*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function tensorflow::(anonymous namespace)::ScatterNdTensorShape(tensorflow::shape_inference::InferenceContext*): error: undefined reference to 'tensorflow::shape_inference::ScatterNdShapeHelper(tensorflow::shape_inference::InferenceContext*, tensorflow::shape_inference::ShapeHandle, tensorflow::shape_inference::ShapeHandle, tensorflow::shape_inference::ShapeHandle)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::register_op118::{lambda(tensorflow::shape_inference::InferenceContext*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&): error: undefined reference to 'tensorflow::shape_inference::UnchangedShape(tensorflow::shape_inference::InferenceContext*)'\r\nbazel-out/aarch64-opt/bin/tensorflow/core/ops/_objs/array_ops_op_lib/array_ops.o:array_ops.cc:function std::_Function_handler<tensorflow::Status (tensorflow::shape_inference::InferenceContext*), tensorflow::register_op118::{lambda(tensorflow::shape_inference::InferenceContext*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::shape_inference::InferenceContext*&&): error: undefined reference to 'tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, long, tensorflow::shape_inference::ShapeHandle*)'\r\n\r\nand much more....", "comments": ["@cfRod @nSircombe ", "Tagging @penpornk ", "This can be avoided by using \"--build_tests_only\" on the command line.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55148\">No</a>\n"]}, {"number": 55147, "title": "MultiWorkerMirroredStrategy reinitializes reduce operations multiple times at start and also from time to time.", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.7.0-rc1-69-gc256c071bb2 2.7.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4 \r\n- GPU model and memory: V100 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nFyi: I use a custom training loop. \r\nOn a multi worker setup with `MultiWorkerMirroredStrategy` I get this output for the first 3 train steps and they are also super slow:\r\n```\r\n[08/03/22 16:50:11] [tensorflow] INFO : Collective all_reduce tensors: 236 all_reduces, num_devices = 8, group_size = 48, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 48, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n[08/03/22 16:50:20] [tensorflow] INFO : Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 6, implementation = NCCL, num_packs = 1\r\n``` \r\n\r\nThen the training runs as expected but it seems that the above init/retracing can happen randomly in the middle of the training again with the same logs. So an intermediate step after 1000 iterations can again take minutes (the more workers/devices the longer)\r\nThe step at which this happens is not the same for multiple runs, that being the reason why I say randomly.\r\n\r\nCould this be related to non-constant input sizes (number of points in a pointcloud? They are of course padded within a batch) Do I need to globally pad here?\r\n\r\n**Describe the expected behavior**\r\nOnly initialize the Collective all_reduce tensors once and especially not in the middle of training.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@andre-bauer,\r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@chunduriv Thx for the reply I actually figured it out myself and it was my fault. Still documenting the problem I created here:\r\n\r\nThis output above happens either on model build or on ``tf.function`` (re)tracing. Apparently I was passing some python object into the ``tf.function`` that could change from time to time cause function retracing. With one worker there is no output (default logging verbosity, relax_shapes=True) and it also does not take a crazy amount of time. That is the reason why I only noticed it when using ``MultiWorkerMirroredStrategy``. So to everyone having something similar, check if your ``tf.function`` is retraced for some reason.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 55146, "title": "[XLA:CPU] Add support for spmd partioning", "body": "[XLA:CPU] Add support for spmd partioning\n\n- Run StatefulRngSpmdPartitioner\n- Implement PartitionId\n- Pass use_global_device_ids down to allreduce\n- Enable multireplica allreduce when there is 1 replica but multiple partitions\n- Fix AllToAll when there are multiple partitions by using device ids instead\n  of replica ids\n", "comments": []}, {"number": 55145, "title": "Cwolf22 patch 1", "body": "something here", "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55145/checks?check_run_id=5467703448)."]}, {"number": 55144, "title": "[KernelGen][JIT] Re-enable JIT kernels by default", "body": "[KernelGen][JIT] Re-enable JIT kernels by default\n\nRe-enable the kernels by default after fixing the Windows-specific issues in\n21264a552155922ec9db03e2b025a6fbea676560.\n", "comments": []}, {"number": 55143, "title": "How to retrain a model with changed shape of intermediate layer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):2.3\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI have a custom layer in my model that has a switch behavior for its output, For the first behavior say it gives an output shape of tensor shape(100,24,24,6) and for the second shape it gives the output shape of (100,24,24,1). My model is already trained for first behavior how can I retrain it for second behavior with the same model architecture and the weights but with a different output shape for a intermediate layer?\r\n Is it possible in TensorFlow to change the output shape of a intermediate layer? Any suggestions are appreciated. Thank you in advance.\r\n\r\n**Describe the expected behavior**\r\nSuccessful retraining of the model with changed shape of intermediate layer\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@saylideshmukh  Could you please have a look at the [link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape) and refer [this ](https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction)link as well.Please let us know if it helps?\r\nWe see that you're using old version of TF 2.3 that does not really contain all the changes to prevent code using invalid shapes from crashing the interpreter via CHECK-failures.Please make sure you are using latest tf version 2.4.0 or later for your case and let us know if it is still an issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55143\">No</a>\n"]}, {"number": 55142, "title": "[XLA] Add IsDead method to hlo_instruction and use it. NFCI", "body": "[XLA] Add IsDead method to hlo_instruction and use it. NFCI\n", "comments": []}, {"number": 55141, "title": "[XLA] Move GPU SPMD Partitioner into a generic place", "body": "[XLA] Move GPU SPMD Partitioner into a generic place\n\nThis has nothing GPU-specific, so rename it to StatefulRngSpmdPartitioner. I'm\nplanning to use this on CPU too.\n", "comments": []}, {"number": 55140, "title": "[XLA] Cleanup / optimize `HloValueSet` and make it more \"set-like\".", "body": "[XLA] Cleanup / optimize `HloValueSet` and make it more \"set-like\".\n", "comments": []}, {"number": 55139, "title": "Bug in feature_column.embedding_column based on vocabulary size", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  AL2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.3-137-gea90cf44f73 2.3.4\r\n- Python version: 3.6\r\n\r\n- CUDA/cuDNN version: replicable on CPU\r\n- GPU model and memory: replicable on CPU \r\n\r\n**Describe the current behavior**\r\nWhile trying to use `tf.feature_column.embedding_column` API. While I don't think is relevant I'm generating the input data via `tf.data.Dataset.from_generator`.\r\n\r\nCode   \r\n```python\r\n for colmn_name in indicator_colms:\r\n        feature_col = feature_column.categorical_column_with_vocabulary_list(\r\n            colmn_name, unique_values(colmn_name))\r\n        indicator_column = feature_column.indicator_column(feature_col)\r\n        feature_columns.append(indicator_column)\r\n```\r\n\r\n**Describe the expected behavior**\r\nFor small vocabulary sizes everything works fine but the moment I try to add more elements to my vocabulary I weirdly get the following exception:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/feature_column/dense_features.py in call(self, features, cols_to_output_tensors, training)\r\n    165           tensor = column.get_dense_tensor(\r\n--> 166               transformation_cache, self._state_manager, training=training)\r\n    167         except TypeError:\r\n\r\nTypeError: get_dense_tensor() got an unexpected keyword argument 'training'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)\r\n   2457       transformed = column.transform_feature(\r\n-> 2458           self, state_manager, training=training)\r\n   2459     except TypeError:\r\n\r\nTypeError: transform_feature() got an unexpected keyword argument 'training'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)\r\n   2457       transformed = column.transform_feature(\r\n-> 2458           self, state_manager, training=training)\r\n   2459     except TypeError:\r\n\r\nTypeError: transform_feature() got an unexpected keyword argument 'training'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-69-8136fbe34af6> in <module>\r\n      2     feature_columns = _create_feature_columns()\r\n      3     feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n----> 4     demo(feature_columns)\r\n\r\n<ipython-input-10-65d9d3c8db7a> in demo(feature_column)\r\n      1 def demo(feature_column):\r\n      2     demo_feature_layer = layers.DenseFeatures(feature_column)\r\n----> 3     print(demo_feature_layer(example_batch).numpy())\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n   1035         with autocast_variable.enable_auto_cast_variables(\r\n   1036             self._compute_dtype_object):\r\n-> 1037           outputs = call_fn(inputs, *args, **kwargs)\r\n   1038 \r\n   1039         if self._activity_regularizer:\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/keras/feature_column/dense_features.py in call(self, features, cols_to_output_tensors, training)\r\n    167         except TypeError:\r\n    168           tensor = column.get_dense_tensor(transformation_cache,\r\n--> 169                                            self._state_manager)\r\n    170         processed_tensors = self._process_dense_tensor(column, tensor)\r\n    171         if cols_to_output_tensors is not None:\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get_dense_tensor(self, transformation_cache, state_manager)\r\n   4301     # Feature has been already transformed. Return the intermediate\r\n   4302     # representation created by transform_feature.\r\n-> 4303     return transformation_cache.get(self, state_manager)\r\n   4304 \r\n   4305   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)\r\n   2458           self, state_manager, training=training)\r\n   2459     except TypeError:\r\n-> 2460       transformed = column.transform_feature(self, state_manager)\r\n   2461     if transformed is None:\r\n   2462       raise ValueError('Column {} is not supported.'.format(column.name))\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)\r\n   4238     \"\"\"\r\n   4239     id_weight_pair = self.categorical_column.get_sparse_tensors(\r\n-> 4240         transformation_cache, state_manager)\r\n   4241     return self._transform_id_weight_pair(id_weight_pair,\r\n   4242                                           self.variable_shape[-1])\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get_sparse_tensors(self, transformation_cache, state_manager)\r\n   3725     \"\"\"See `CategoricalColumn` base class.\"\"\"\r\n   3726     return CategoricalColumn.IdWeightPair(\r\n-> 3727         transformation_cache.get(self, state_manager), None)\r\n   3728 \r\n   3729   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in get(self, key, state_manager, training)\r\n   2458           self, state_manager, training=training)\r\n   2459     except TypeError:\r\n-> 2460       transformed = column.transform_feature(self, state_manager)\r\n   2461     if transformed is None:\r\n   2462       raise ValueError('Column {} is not supported.'.format(column.name))\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in transform_feature(self, transformation_cache, state_manager)\r\n   3703     input_tensor = _to_sparse_input_and_drop_ignore_values(\r\n   3704         transformation_cache.get(self.key, state_manager))\r\n-> 3705     return self._transform_input_tensor(input_tensor, state_manager)\r\n   3706 \r\n   3707   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py in _transform_input_tensor(self, input_tensor, state_manager)\r\n   3691             num_oov_buckets=self.num_oov_buckets,\r\n   3692             dtype=key_dtype,\r\n-> 3693             name=name)\r\n   3694       if state_manager is not None:\r\n   3695         state_manager.add_resource(self, name, table)\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py in index_table_from_tensor(vocabulary_list, num_oov_buckets, default_value, hasher_spec, dtype, name)\r\n   1507 \r\n   1508   with ops.name_scope(name, \"string_to_index\"):\r\n-> 1509     keys = ops.convert_to_tensor(vocabulary_list)\r\n   1510     if keys.dtype.is_integer != dtype.is_integer:\r\n   1511       raise ValueError(\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)\r\n    161         with Trace(trace_name, **trace_kwargs):\r\n    162           return func(*args, **kwargs)\r\n--> 163       return func(*args, **kwargs)\r\n    164 \r\n    165     return wrapped\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1564 \r\n   1565     if ret is None:\r\n-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1567 \r\n   1568     if ret is NotImplemented:\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    344                                          as_ref=False):\r\n    345   _ = as_ref\r\n--> 346   return constant(v, dtype=dtype, name=name)\r\n    347 \r\n    348 \r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    270   \"\"\"\r\n    271   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 272                         allow_broadcast=True)\r\n    273 \r\n    274 \r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    281       with trace.Trace(\"tf.constant\"):\r\n    282         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n--> 283     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    284 \r\n    285   g = ops.get_default_graph()\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n    306 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n    307   \"\"\"Creates a constant on the current device.\"\"\"\r\n--> 308   t = convert_to_eager_tensor(value, ctx, dtype)\r\n    309   if shape is None:\r\n    310     return t\r\n\r\n~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    104       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n    105   ctx.ensure_initialized()\r\n--> 106   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n    107 \r\n    108 \r\n\r\nValueError: Can't convert Python sequence with mixed types to Tensor.\r\n```\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\n\r\ndef _create_feature_columns(): \r\n    indicator_columns = [...] # list of columns\r\n    for colmn_name in indicator_colms:\r\n        feature_col = feature_column.categorical_column_with_vocabulary_list(\r\n            colmn_name, unique_values(colmn_name))\r\n        indicator_column = feature_column.indicator_column(feature_col)\r\n        feature_columns.append(indicator_column)\r\n\r\ndef demo(feature_column):\r\n    demo_feature_layer = layers.DenseFeatures(feature_column)\r\n    print(demo_feature_layer(example_batch).numpy())\r\n\r\nwith tf.device(device):\r\n    feature_columns = _create_feature_columns()\r\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\n    demo(feature_columns)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@h1t35h,\r\n\r\nIn order to expedite the troubleshooting process, please provide a standalone code to reproduce the issue reported here. Thanks!", "My sincere apologies it seemed my vocabulary was getting dirtied upon fixing that the behavior no longer present. Closing this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55139\">No</a>\n", "\u60a8\u597d\uff0c\u6211\u662f\u7f8a\u5cfb\u9704\uff0c\u6211\u5df2\u7ecf\u6536\u5230\u60a8\u7684\u90ae\u4ef6\uff0c\u8c22\u8c22"]}, {"number": 55137, "title": "please publish TensorFlowLiteObjC 2.8", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\nAndroid 2.8\r\niOS 2.7\r\n- Are you willing to contribute it (Yes/No):\r\nNO\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@v7lin \r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your feature and please specify the use cases for this feature. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55136, "title": "[ROCm] Fixes to enable triangular solve and cholesky BEF thunk unit tests for ROCm.", "body": "/cc @chsigg @hanbinyoon \r\n\r\nCreating a new PR based on previous one due to git rebase error.", "comments": ["I included both the triangular solve and cholesky commits in this PR.", "@chsigg gentle ping"]}, {"number": 55135, "title": "Setting `converter.inference_type=uint8` does not produce quantized TFLite model of uint8 weight data type", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI was trying to convert [a TF model](http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NHWC.tar.gz) to TFLite model and quantize by the way. With `converter.optimizations = [tf.lite.Optimize.DEFAULT]` set, I could get a quantized model of int8 weight data type. I would also want a quantized model of uint8 type, so I tried `converter.inference_type=uint8`, but the produced model is still in int8 data type.\r\n\r\n**Describe the expected behavior**\r\nSet `converter.inference_type=uint8` to produce a quantized TFLite model of **uint8 weight data type**.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\n import tensorflow as tf\r\n\r\n tf_path = \"./resnet_v1_fp32_savedmodel_NHWC/1538686669\"\r\n tflite_path = \"{}.tflite\".format(\"resnet50.uint8.nhwc\")\r\n\r\n converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\r\n converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n converter.inference_type = tf.uint8\r\n tf_lite_model = converter.convert()\r\n with open(tflite_path, 'wb') as f:\r\n     f.write(tf_lite_model)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n$ python tf2tflite.py\r\n2022-03-08 19:46:26.965733: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'global_step:0' shape=() dtype=int64_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/conv2d/kernel:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it\r\n is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_r\r\nesource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_model/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n2022-03-08 19:46:28.726330: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\r\n2022-03-08 19:46:28.726362: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\r\n2022-03-08 19:46:28.727314: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: ./resnet_v1_fp32_savedmodel_NHWC/1538686669\r\n2022-03-08 19:46:28.732979: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\r\n2022-03-08 19:46:28.733019: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: ./resnet_v1_fp32_savedmodel_NHWC/1538686669\r\n2022-03-08 19:46:28.765293: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\r\n2022-03-08 19:46:28.997319: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: ./resnet_v1_fp32_savedmodel_NHWC/1538686669\r\n2022-03-08 19:46:29.010577: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 283266 microseconds.\r\n2022-03-08 19:46:29.097206: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2022-03-08 19:46:30.323709: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 1049.200 G  ops, equivalently 524.600 G  MACs\r\n\r\nEstimated count of arithmetic ops: 1049.200 G  ops, equivalently 524.600 G  MACs\r\n```\r\n\r\n", "comments": ["https://github.com/tensorflow/model-optimization/issues/775#issuecomment-894908619 this comment mentioned that TF2 converter does not support `uint8` quantization any more. Is this the reason why setting `converter.inference_type=uint8` does not help?", "Hi @fengyuentau ! Could you please share the model file too? I was suggesting to use a representative dataset and use supported_ops as mentioned in this [link ](https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization) for post integer quantization. Thanks!", "@mohantym Link to the model: http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v1_fp32_savedmodel_NHWC.tar.gz, which is the same hyper link in the description.", "@fengyuentau ! I replicated this issue in 2.8 . Attaching [gist ](https://colab.sandbox.google.com/gist/mohantym/7c24424c13a240f57711d07645601c07/git_55135.ipynb)for reference. Thanks!", "> [tensorflow/model-optimization#775 (comment)](https://github.com/tensorflow/model-optimization/issues/775#issuecomment-894908619) this comment mentioned that TF2 converter does not support `uint8` quantization any more. Is this the reason why setting `converter.inference_type=uint8` does not help?\r\n\r\nYes, you're right. TF2 converter does not support `uint8` quantization. \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55135\">No</a>\n"]}, {"number": 55134, "title": "Parser.h has moved inside llvm_project", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/55133", "comments": ["Made moot by https://github.com/tensorflow/tensorflow/commit/214d2ed3ff4228e92f246873d3ff535bdb7de35a"]}, {"number": 55133, "title": "Unit test build failure - fatal error: mlir/Parser.h: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 5.0.0\r\n- GCC/Compiler version (if compiling from source): 10.3.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nUnit test build fails with\r\n\r\n# Execution platform: @local_execution_config_platform//:platform\r\ntensorflow/core/function/runtime_client_test.cc:25:10: fatal error: mlir/Parser.h: No such file or directory\r\n   25 | #include \"mlir/Parser.h\"  // from @llvm-project\r\n      |          ^~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel test --test_timeout=300,500,-1,-1 --flaky_test_attempts=3 --test_output=all --cache_test_results=no --config=nonccl --verbose_failures --build_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --test_tag_filters=-no_oss,-oss_serial,-gpu,-tpu,-benchmark-test,-v1only,-no_aarch64,-requires-gpu --jobs=75 -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... -//tensorflow/core/tpu/... -//tensorflow/go/... -//tensorflow/java/... -//tensorflow/python/integration_testing/... -//tensorflow/tools/toolchains/... -//tensorflow/lite/... -//tensorflow/python/tools/... -//tensorflow/compiler/mlir/lite/tests:const-fold.mlir.test -//tensorflow/compiler/mlir/lite/tests:prepare-tf.mlir.test -//tensorflow/python/data/experimental/kernel_tests/service:fault_tolerance_test -//tensorflow/python/eager:function_test -//tensorflow/python/kernel_tests/linalg:self_adjoint_eig_op_test\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHeader file was moved by https://github.com/llvm/llvm-project/commit/9eaff42360f4430e2baba28dd8d119137caae486\r\n", "comments": ["@cfRod @nSircombe ", "@elfringham !Thanks for the PR #55134 . This issue will be closed once PR is merged. ", "Made moot by https://github.com/tensorflow/tensorflow/commit/214d2ed3ff4228e92f246873d3ff535bdb7de35a", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55133\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55133\">No</a>\n"]}, {"number": 55131, "title": "Fixes undefined behavior caused by usage of std::uniform_int_distribution<uint8_t>.", "body": "Fixes undefined behavior caused by usage of std::uniform_int_distribution<uint8_t>.\n", "comments": []}, {"number": 55130, "title": "Fix header include.", "body": "Fix header include.\n", "comments": []}, {"number": 55129, "title": "Removed references to g-no-augmented-assignment in pylint directives", "body": "Removed references to g-no-augmented-assignment in pylint directives\n", "comments": []}, {"number": 55128, "title": "I cannot start my tensorflow certificate exam due to error", "body": "So i purchased the tensorflow exam and tried to start it, however for some wierd reason i cannot start the exam. the error in the screenshot keeps coming at every try\r\n![Uploading Screen Shot 2022-03-08 at 3.17.40 AM.png\u2026]()\r\n\r\nDoes anyone know the solution to this", "comments": ["@deeplearner2022,\r\n\r\nI am not able to access attached screenshot. Please can you share once again. Thanks!", "<img width=\"896\" alt=\"Screen Shot 2022-03-08 at 3 17 40 AM\" src=\"https://user-images.githubusercontent.com/101177094/157224841-163bd9b6-bf4c-4486-9dd3-9c9b22468147.png\">\r\n@chunduriv @tilakrayal here it is", "<img width=\"998\" alt=\"Screen Shot 2022-03-08 at 3 45 19 PM\" src=\"https://user-images.githubusercontent.com/101177094/157261868-d952c82c-30fd-46ce-ac66-9486b28d82e8.png\">\r\nJust to add, This is what it shows on the Trueability website, yet the plugin doesnt work", "@deeplearner2022,\r\n\r\nCan you please try updating the TF certification plugin https://plugins.jetbrains.com/plugin/13812-tensorflow-developer-certificate/versions ? Thanks!", "I have the latest updates and, its still the same error @chunduriv \r\n<img width=\"470\" alt=\"Screen Shot 2022-03-08 at 6 14 16 PM\" src=\"https://user-images.githubusercontent.com/101177094/157290129-92b2b3f4-ea28-4192-b825-6d22f8a21bce.png\">\r\n", "@deeplearner2022,\r\n\r\nFor general support and questions regarding the Google Tensorflow Developer Certification, please contact tensorflow-certificate-support@google.com. Thanks!", "Have you tried to uninstall  the plugin and re-install it?", "Thanks for your response. Yes I did that and still I keep getting the same\nerror message\n\nOn Thu, Mar 10, 2022, 20:54 Kyriakos Pelekanos ***@***.***>\nwrote:\n\n> Have you tried to uninstall the plugin and re-install it?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/55128#issuecomment-1064445895>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AYD5OBRRM6DCF4LUZ5HNNULU7JHOZANCNFSM5QFX7XIQ>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n> You are receiving this because you were mentioned.Message ID:\n> ***@***.***>\n>\n", "@deeplearner2022,\r\n\r\nAs suggested please refer this [link](https://github.com/tensorflow/tensorflow/issues/55128#issuecomment-1062547263) and also this is not a feature or bug for Tensorflow. Please feel free to close this issue. Thanks!\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55127, "title": "Stage data on gpu pinned memory during transfer", "body": "Stage data on gpu pinned memory during transfer\n", "comments": []}, {"number": 55126, "title": "Enhance summary for ResourceHandle to output shapes as well.", "body": "Enhance summary for ResourceHandle to output shapes as well.\n", "comments": []}]