[{"number": 47966, "title": "how to build  tensorflow-lite-gpu-x.x.x.aar", "body": "\r\n**System information**\r\n-  Linux Ubuntu 20.04:\r\n- Mobile device : android\r\n- TensorFlow installed from:source\r\n- TensorFlow version:2.4.1\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):Bazel 3.1.0\r\n\r\n**Describe the problem**\r\nBuild success with https://www.tensorflow.org/lite/guide/build_android but no gpu support\r\nAny information on 'how to build tensorflow-lite-gpu-x.x.x.aar'\r\n", "comments": ["@terryheo @thaink could you take a look?", "You can build it the same way you build tensorflow-lite.aar.\r\nJust replace `//tensorflow/lite/java:tensorflow-lite`with `//tensorflow/lite/java:tensorflow-lite-gpu`.\r\nEx:\r\n```\r\nbazel build -c opt --cxxopt=--std=c++14 \\\r\n    --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n    //tensorflow/lite/java:tensorflow-lite-gpu\r\n```\r\n", "cool , Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47966\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47966\">No</a>\n"]}, {"number": 47965, "title": "Multiple inputs api does NOT work correctly, concatenates first input and null to second", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Full code, system settings and errors below.  Tried this in many different formats and setups under Anaconda Jupyter.\r\n\r\nBy feeding the different inputs x, and x2, as the output, it can be seen that the inputs feed in cacatenated into the first input and not split between the two.\r\n\r\nmodel = keras.Model(inputs=[x, x2], outputs=x)\r\nmodel.predict([data,data1])\r\nWARNING:tensorflow:Model was constructed with shape (None, 2) for input Tensor(\"input_3:0\", shape=(None, 2), dtype=float32), but it was called on an input with incompatible shape (None, 4, 2).\r\n[[[ 2.  3.]\r\n  [ 4.  5.]\r\n  [ 6.  7.]\r\n  [ 8.  9.]]\r\n\r\n [[10. 11.]\r\n  [12. 13.]\r\n  [14. 15.]\r\n  [16. 17.]]]\r\n\r\nmodel = keras.Model(inputs=[x, x2], outputs=x2) \r\nmodel.predict([data,data1])\r\n  AssertionError: Could not compute output Tensor(\"input_2:0\", shape=(None, 2), dtype=float32)\r\n  puts out empty data sets. Full error at bottom.\r\n\r\n============ complete code ==========\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\n\r\nfrom datetime import date\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ndata = [[2,3],[4,5],[6,7],[8,9]]\r\ndata1 = [[10,11],[12,13],[14,15],[16,17]]\r\n\r\ntrain = (np.array(data) * 2).tolist()\r\n\r\nprint('data', data)\r\nprint('train',train)\r\nprint()\r\n\r\nx = layers.Input(shape=(2,),dtype=tf.float32)\r\nx2 = layers.Input(shape=(2,),dtype=tf.float32)\r\n# DOESN'T SEE X2 !!!!!!!!!! BOTH GO INTO X !!!!!!!!!!!!!!!!!!!!!!\r\n\r\nmodel = keras.Model(inputs=[x, x2], outputs=x)\r\n# switched to outputs = x2 for second testing\r\nmodel.compile() #loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\npred = model.predict([data,data1])\r\nprint(pred)\r\n\r\n============ system configuration =======\r\nprint('keras version:', keras.__version__)\r\nimport tensorpack.tfutils as u\r\nprint(u.collect_env_info())\r\nkeras version: 2.4.0\r\npython version: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\r\ntensorflow version: 2.3.0\r\nkeras version: 2.4.0\r\n\r\n-------------------  ----------------------------------------------------------------------------------------\r\nsys.platform         win32\r\nPython               3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\r\nTensorpack           v0.11-2-g88530083-dirty @C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorpack\r\nNumpy                1.20.1\r\nTensorFlow           2.3.0/unknown @C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\r\nTF Compiler Version  MSVC 192729111\r\nTF CUDA support      False\r\nTF MKL support       False\r\nTF XLA support       False\r\nFree RAM             21.47/32.00 GB\r\nCPU Count            16\r\ncv2                  4.0.1\r\nmsgpack              1.0.2\r\npython-prctl         False\r\n\r\n========== FULL ERROR for using x2 as output ========\r\nWARNING:tensorflow:Model was constructed with shape (None, 2) for input Tensor(\"input_1:0\", shape=(None, 2), dtype=float32), but it was called on an input with incompatible shape (None, 4, 2).\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-4-eaa004bedc40> in <module>\r\n     30 print(model.summary())\r\n     31 \r\n---> 32 pred = model.predict([data,data1])\r\n     33 print(pred)\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n    128       raise ValueError('{} is not supported in multi-worker mode.'.format(\r\n    129           method.__name__))\r\n--> 130     return method(self, *args, **kwargs)\r\n    131 \r\n    132   return tf_decorator.make_decorator(\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1597           for step in data_handler.steps():\r\n   1598             callbacks.on_predict_batch_begin(step)\r\n-> 1599             tmp_batch_outputs = predict_function(iterator)\r\n   1600             if data_handler.should_sync:\r\n   1601               context.async_wait()\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    821       # This is the first call of __call__, so we have to initialize.\r\n    822       initializers = []\r\n--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    824     finally:\r\n    825       # At this point we know that the initialization is complete (or less\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    695     self._concrete_stateful_fn = (\r\n--> 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    697             *args, **kwds))\r\n    698 \r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3063     arg_names = base_arg_names + missing_arg_names\r\n   3064     graph_function = ConcreteFunction(\r\n-> 3065         func_graph_module.func_graph_from_py_func(\r\n   3066             self._name,\r\n   3067             self._python_function,\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n~\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nAssertionError: in user code:\r\n\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\r\n        return step_function(self, iterator)\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\r\n        outputs = model.predict_step(data)\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\r\n        return self(x, training=False)\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:385 call\r\n        return self._run_internal_graph(\r\n    C:\\Users\\rkers\\Anaconda3\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:517 _run_internal_graph\r\n        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\r\n\r\n    AssertionError: Could not compute output Tensor(\"input_2:0\", shape=(None, 2), dtype=float32)\r\n", "@RichardKershner-v1 \r\n\r\nCan you share simple stand alone code such that we could replicate the error reported or share a colab gist with the code and error, also share tf version we see that the template has not been filled.\r\n\r\nYou may also refer to below issues and let us know:\r\n#34912 [link](https://stackoverflow.com/questions/61444983/keras-cnn-with-several-input-filters)", "Thought I had the code above.  It also might be a Keras and not a tensor flow issue.\r\nI did figure the fix for it, which doesn't seem like it should be the one that works.\r\n\r\nFirst, I had to name the inputs and build a dataset.  The weird key is that I had to reshape the data, as below.  Not intuitive.  I have the below code with an output for concatenate and with an output for average.\r\n\r\ndef input_gen():\r\n    a = np.array(data1).reshape(-1,1,2).tolist()\r\n    b = np.array(data2).reshape(-1,1,2).tolist()\r\n    return {'x1': a, 'x2': b}\r\ndataset = tf.data.Dataset.from_tensor_slices(input_gen ())\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n#data1 = [[[2,3]],[[4,5]],[[6,7]],[[8,9]]]  this is what new shape looks like that works\r\n#data2 = [[[10,11]],[[12,13]],[[14,15]],[[16,17]]]\r\ndata1 = [[2,3],[4,5],[6,7],[8,9]]\r\ndata2 = [[10,11],[12,13],[14,15],[16,17]]\r\n\r\ntrain = (np.array(data) * 2).tolist()\r\n\r\nx1 = layers.Input(shape=(2),dtype=tf.float32, name=\"x1\")\r\nx2 = layers.Input(shape=(2),dtype=tf.float32, name=\"x2\")\r\n\r\ny1 = layers.Concatenate()([x1, x2]) \r\ny2 = layers.Average()([x1, x2])\r\n\r\nmodel = keras.Model(inputs=[x1, x2], outputs=[y1,y2])\r\nmodel.compile()#loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\n# print(model.summary())\r\n\r\ndef input_gen():\r\n    a = np.array(data1).reshape(-1,1,2).tolist()\r\n    b = np.array(data2).reshape(-1,1,2).tolist()\r\n    return {'x1': a, 'x2': b}\r\ndataset = tf.data.Dataset.from_tensor_slices(input_gen ())\r\n\r\npred = model.predict(dataset)\r\n\r\nprint('pred')\r\nprint(pred)\r\n\r\n-- output as expected--\r\n-- a 4(batchsize) by 4 array concatenated of the 2 (4x4) inputs --\r\n-- and the average by axis = 1 , shape 4(batch size) by 2\r\ny1 is for concatenate, y2 is for average as below.\r\n[array([[ 2.,  3., 10., 11.],\r\n       [ 4.,  5., 12., 13.],\r\n       [ 6.,  7., 14., 15.],\r\n       [ 8.,  9., 16., 17.]], dtype=float32), array([[ 6.,  7.],\r\n       [ 8.,  9.],\r\n       [10., 11.],\r\n       [12., 13.]], dtype=float32)]", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47965\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47965\">No</a>\n"]}, {"number": 47964, "title": "CUDA Issue: Initialization of a toy model takes 5GB graphical memory", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nconda\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION = 2.3.0\r\ntf.version.GIT_VERSION = v2.3.0-rc2-23-gb36436b087\r\ntf.version.COMPILER_VERSION = 7.3.1 20180303\r\n- Python version:\r\n== check python ===================================================\r\npython version: 3.8.5\r\npython branch: \r\npython build version: ('default', 'Sep  4 2020 07:30:14')\r\npython compiler version: GCC 7.3.0\r\npython implementation: CPython\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n== compiler =====================================================\r\nc++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version:\r\nCUDA Version: 11.0\r\n- GPU model and memory:\r\n```python\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    pdb.set_trace()\r\n    super(MyModel, self).__init__()\r\n    pdb.set_trace()\r\n\r\n    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n    pdb.set_trace()\r\n    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n    pdb.set_trace()\r\n\r\n  def call(self, inputs):\r\n    x = self.dense1(inputs)\r\n    return self.dense2(x)\r\n```\r\nThis chosen model is tiny, but it still takes 5GB graphical memory when I run it.\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have a personal workstation with Nvidia RTX 2060s card. I tried to run my ml model using cuda while the graphical memory immediately ran out once the model is initiated. In order to eliminate the error from my model, I tried with the following tiny model, and the memory still ran out once it's initiated.\r\n\r\n**Describe the expected behavior**\r\nmodel should only takes a few MB graphical memory as it's indeed tiny.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport pdb\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    pdb.set_trace() # trace1\r\n    super(MyModel, self).__init__()\r\n    pdb.set_trace() # trace2\r\n\r\n    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n    pdb.set_trace()\r\n    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n    pdb.set_trace()\r\n\r\n  def call(self, inputs):\r\n    x = self.dense1(inputs)\r\n    return self.dense2(x)\r\n\r\nmodel = MyModel()\r\n```\r\nAccording to the nvidia-smi, the graphical memory increases from 2767MB to 7755MB after running the object initiation between trace1 and trace2.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n\r\n\r\n\r\n```bash\r\n$python runnet.py \r\n2021-03-21 20:30:55.223213: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nWorking dir: /home/shuqi/Documents/2step-glm\r\nmanual save dir: /home/shuqi/Documents/2step-glm/manualsave/1616355056\r\n2021-03-21 20:30:56.200025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-03-21 20:30:56.238946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.239299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:29:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-21 20:30:56.239314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-21 20:30:56.240455: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-03-21 20:30:56.241618: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-03-21 20:30:56.241779: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-03-21 20:30:56.242891: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-21 20:30:56.243485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-21 20:30:56.245888: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-21 20:30:56.246000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.246377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.246673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-03-21 20:30:56.246919: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-21 20:30:56.251829: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3600030000 Hz\r\n2021-03-21 20:30:56.252237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55872d90cd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-03-21 20:30:56.252250: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-03-21 20:30:56.339116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.339497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55872d909050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-03-21 20:30:56.339519: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060 SUPER, Compute Capability 7.5\r\n2021-03-21 20:30:56.339804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.340317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:29:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-21 20:30:56.340346: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-21 20:30:56.340379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-03-21 20:30:56.340396: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-03-21 20:30:56.340412: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-03-21 20:30:56.340428: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-21 20:30:56.340444: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-21 20:30:56.340460: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-21 20:30:56.340541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.341052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.341503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-03-21 20:30:56.341532: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-21 20:30:56.643133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-21 20:30:56.643167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2021-03-21 20:30:56.643176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2021-03-21 20:30:56.643404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.643752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-21 20:30:56.644045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4658 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:29:00.0, compute capability: 7.5)\r\n2021-03-21 20:30:56.645082: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.687899: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688103: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688291: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688487: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688718: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688740: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688791: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688863: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.688908: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.689016: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.689036: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.689082: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.689153: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:30:56.689196: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n> /home/shuqi/Documents/2step-glm/src/buildnet.py(345)__init__()\r\n-> self.conv2a = tf.keras.layers.Conv1D(FLAGS.conv1, FLAGS.conv1size, strides=1, padding='same',\r\n(Pdb) c\r\n> /home/shuqi/Documents/2step-glm/src/buildnet.py(349)__init__()\r\n-> self.maxpool2a = tf.keras.layers.MaxPool1D(FLAGS.nk1, strides=FLAGS.nstride1, padding='same')\r\n(Pdb) c\r\n> /home/shuqi/Documents/2step-glm/src/buildnet.py(353)__init__()\r\n-> self.conv2b = tf.keras.layers.Conv1D(FLAGS.conv2, FLAGS.conv2size, strides=1, padding='same',\r\n(Pdb) c\r\n> /home/shuqi/Documents/2step-glm/src/buildnet.py(357)__init__()\r\n-> self.maxpool2b = tf.keras.layers.MaxPool1D(FLAGS.nk2, strides=FLAGS.nstride2, padding='same')\r\n(Pdb) c\r\n> /home/shuqi/Documents/2step-glm/src/buildnet.py(363)__init__()\r\n-> self.flatten = tf.keras.layers.Flatten()\r\n(Pdb) c\r\n> /home/shuqi/Documents/2step-glm/src/buildnet.py(387)build_model()\r\n-> inputnet(np.random.rand(FLAGS.batch_size, 31, FLAGS.numconsframes))  # dummy call\r\n(Pdb) c\r\nWARNING:tensorflow:Layer conv2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n\r\n2021-03-21 20:31:02.226508: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.227279: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.227682: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.228007: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.228116: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.228343: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.228386: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.228489: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.228620: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.228799: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.229711: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.229888: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.229926: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.229993: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.230095: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.230188: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.230797: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op ExpandDims in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.230876: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.230913: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op ExpandDims in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.231055: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op Conv2D in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.231237: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-21 20:31:02.589000: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2021-03-21 20:31:02.590836: E tensorflow/stream_executor/cuda/cuda_dnn.cc:328] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"runnet.py\", line 95, in <module>\r\n    inputnet, rnn, cell = build_model(ncell, FLAGS) # thr=0.4 if no buffer\r\n  File \"/home/shuqi/Documents/2step-glm/src/buildnet.py\", line 387, in build_model\r\n    inputnet(np.random.rand(FLAGS.batch_size, 31, FLAGS.numconsframes))  # dummy call\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/shuqi/Documents/2step-glm/src/buildnet.py\", line 366, in call\r\n    x = self.conv2a(input_tensor)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 247, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 1011, in convolution_v2\r\n    return convolution_internal(\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 1141, in convolution_internal\r\n    return op(\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 574, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\", line 1881, in conv1d\r\n    result = gen_nn_ops.conv2d(\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 938, in conv2d\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/shuqi/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]\r\n2021-03-21 20:31:02.714717: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.714808: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.714833: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.714856: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2021-03-21 20:31:02.714883: I tensorflow/core/common_runtime/eager/execute.cc:611] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n```\r\n\r\n\r\n", "comments": ["@realwsq \r\nI tried running your code on colab , please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5fd44f0b6bdd9c11c8d75cce5a362ad0/untitled567.ipynb), could you let us know the value for pdb as prompted in the gist.", "@Saduf2019 Hi, thank you for your reply. Here is the result running the gist. I stepped over to each breakpoint.\r\n```\r\n2021-03-22 08:14:16.041630: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> /home/shuqi/Documents/2step-glm/test.py(9)__init__()\r\n-> super(MyModel, self).__init__()\r\n(Pdb) c\r\n2021-03-22 08:14:39.444495: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-03-22 08:14:39.485378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:39.485746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:29:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-22 08:14:39.485765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-22 08:14:39.517930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-03-22 08:14:39.530904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-03-22 08:14:39.533790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-03-22 08:14:39.553771: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-22 08:14:39.556535: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-22 08:14:39.591204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-22 08:14:39.591549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:39.592104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:39.592924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-03-22 08:14:39.593475: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-22 08:14:39.610149: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3600030000 Hz\r\n2021-03-22 08:14:39.610779: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55829230d670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-03-22 08:14:39.610898: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-03-22 08:14:39.723811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:39.724495: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558292379080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2021-03-22 08:14:39.724515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060 SUPER, Compute Capability 7.5\r\n2021-03-22 08:14:39.725297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:39.725703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:29:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-03-22 08:14:39.725770: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-22 08:14:39.725808: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-03-22 08:14:39.725821: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2021-03-22 08:14:39.725832: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2021-03-22 08:14:39.725843: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-22 08:14:39.725854: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-22 08:14:39.725865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-22 08:14:39.725927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:39.726336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:39.726629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2021-03-22 08:14:39.726967: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-22 08:14:40.481770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-22 08:14:40.481817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2021-03-22 08:14:40.481823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2021-03-22 08:14:40.482657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:40.483110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-03-22 08:14:40.483474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4341 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:29:00.0, compute capability: 7.5)\r\n> /home/shuqi/Documents/2step-glm/test.py(12)__init__()\r\n-> self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\r\n(Pdb) c\r\n> /home/shuqi/Documents/2step-glm/test.py(14)__init__()\r\n-> self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n(Pdb) c\r\n--Return--\r\n> /home/shuqi/Documents/2step-glm/test.py(14)__init__()->None\r\n-> self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\r\n(Pdb) c\r\n```", "TF by default tries to preallocate all memory on the device.  Does setting [`set_memory_growth`](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth) to `true` help?", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/a1c10a698e46a0438f64e70339c20b29/untitled216.ipynb)..Thanks !", "@realwsq Did you try `set_memory_growth` as suggested by @sanjoy . I ran it in colab and didn't notice high memory usage. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/54a58d8d598a847d0a45be64fe413af1/untitled216.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47964\">No</a>\n"]}, {"number": 47963, "title": "self-deleted", "body": "self-deleted", "comments": ["After doing more Google search, I realize the problem is that I didn't really understand what precision and accuracy really means.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47963\">No</a>\n"]}, {"number": 47962, "title": "[TFLM] Fix for broken generated Makefiles project for Person detection benchmark", "body": "As described in issue https://github.com/tensorflow/tensorflow/issues/47961\r\n\r\nSome headers missing from tensorflow/lite/micro/benchmarks/Makefile.inc\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47961, "title": "[TFLM] Person detection benchmark build broken when generating Makefile projects", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): Nightly, source\r\n- Tensorflow version (commit SHA if source): commit id 9711a668d8538ac2a3ebe93a90226b89683d0206\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nWhen generating a Makefiles project for the person detection benchmark build fails due to some missing headers.\r\nCommand used: make -f tensorflow/lite/micro/tools/make/Makefile  generate_person_detection_benchmark_make_project\r\n\r\nI will open a PR shortly with a fix.\r\n", "comments": ["@yair-ehrenwald \r\nWe see that you have raised a pr, this issue will close once the pr is merged.\r\nplease update the tf version in the template."]}, {"number": 47960, "title": "[TFLM] Added a platform specific micro_time.cc for CEVA-DSP BX1 and SP500", "body": "Added code calling into CEVA's clock so the micro profiler can be used", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> You might prefer using system_setup.cc to avoid having state in the micro_time functions. See sparkfun_edge and arduino as examples.\r\n> \r\n> You'll have to still keep empty micro_time.cc and/ or debug_log.cc depending on what all needs to be specialized for your target.\r\n\r\nDone.\r\nThe micro_time.cc isn't empty as the clock() function we use isn't the standard one.", "> > You might prefer using system_setup.cc to avoid having state in the micro_time functions. See sparkfun_edge and arduino as examples.\r\n> > You'll have to still keep empty micro_time.cc and/ or debug_log.cc depending on what all needs to be specialized for your target.\r\n> \r\n> Done.\r\n> The micro_time.cc isn't empty as the clock() function we use isn't the standard one.\r\n\r\nIf you prefer to have all the functions in system_setp.cc then you can keep micro_time.cc empty. There is no requirement that micro_time.h be implemented in micro_time.cc\r\n\r\nI prefer that since it gives me a single file with all the code (time, debug_log and init). But whatever you prefer. I have approved this PR and we can merge it as-is.", "> > > You might prefer using system_setup.cc to avoid having state in the micro_time functions. See sparkfun_edge and arduino as examples.\r\n> > > You'll have to still keep empty micro_time.cc and/ or debug_log.cc depending on what all needs to be specialized for your target.\r\n> > \r\n> > \r\n> > Done.\r\n> > The micro_time.cc isn't empty as the clock() function we use isn't the standard one.\r\n> \r\n> If you prefer to have all the functions in system_setp.cc then you can keep micro_time.cc empty. There is no requirement that micro_time.h be implemented in micro_time.cc\r\n> \r\n> I prefer that since it gives me a single file with all the code (time, debug_log and init). But whatever you prefer. I have approved this PR and we can merge it as-is.\r\n\r\nNP, moved everything into system_setup.cc", "> > > > You might prefer using system_setup.cc to avoid having state in the micro_time functions. See sparkfun_edge and arduino as examples.\r\n> > > > You'll have to still keep empty micro_time.cc and/ or debug_log.cc depending on what all needs to be specialized for your target.\r\n> > > \r\n> > > \r\n> > > Done.\r\n> > > The micro_time.cc isn't empty as the clock() function we use isn't the standard one.\r\n> > \r\n> > \r\n> > If you prefer to have all the functions in system_setp.cc then you can keep micro_time.cc empty. There is no requirement that micro_time.h be implemented in micro_time.cc\r\n> > I prefer that since it gives me a single file with all the code (time, debug_log and init). But whatever you prefer. I have approved this PR and we can merge it as-is.\r\n> \r\n> NP, moved everything into system_setup.cc\r\n\r\nApologies, this PR was already in the merge queue at the time of your latest update.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/871c66bd6fb833358aca6c51f5a4af1f147855b7 has all the changes (except for the consolidation in system_setup.cc).\r\n\r\nI'll close this PR. If you could make a new PR for the consolidation in system_setup.cc, that would be great."]}, {"number": 47959, "title": "ValueError: Calling `Model.fit` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.fit` with eager mode enabled.", "body": "\r\n**System information**\r\n- Windows 10\r\n- TensorFlow version 2.4.1\r\n- Keras 2.4.1\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: v11.2/11.2\r\n- GPU model and memory: MSI geforce rtx 3060 12GB\r\n\r\n**Describe the current behavior**\r\nThe program fails to start. \r\n\r\n**Describe the expected behavior**\r\nIt should work. \r\n\r\n**Standalone code to reproduce the issue**\r\nI have deep Q learning class: \r\n```\r\nclass DQNAgent:\r\n    def __init__(self):\r\n        self.model = self.create_model()\r\n        self.target_model = self.create_model()\r\n        self.target_model.set_weights(self.model.get_weights())\r\n\r\n        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\r\n\r\n        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\")\r\n\r\n        self.target_update_counter = 0\r\n        self.graph = tf.compat.v1.get_default_graph()\r\n\r\n        self.terminate = False\r\n        self.last_log_episode = 0\r\n        self.training_initialized = False\r\n\r\n    def create_model(self):\r\n        base_model = Xception(weights=None, include_top=False, input_shape=(IM_HEIGHT, IM_WIDTH, 3))\r\n\r\n        x = base_model.output\r\n        x = GlobalAveragePooling2D()(x)\r\n\r\n        predictions = Dense(3, activation='linear')(x)\r\n        model = Model(inputs=base_model.input, outputs=predictions)\r\n        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\r\n        return model\r\n\r\n    def update_replay_memory(self, transition):\r\n        # transition = (current_state, action, reward, new_state, done)\r\n        self.replay_memory.append(transition)\r\n\r\n    def train(self):\r\n        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\r\n            return\r\n\r\n        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\r\n\r\n        current_states = np.array([transition[0] for transition in minibatch])/255\r\n        with self.graph.as_default():\r\n            current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\r\n\r\n        new_current_states = np.array([transition[3] for transition in minibatch]) / 255\r\n        with self.graph.as_default():\r\n            future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\r\n\r\n        X = []\r\n        y = []\r\n\r\n        # we want to update q table only when there exists a future state. If a car crashed just save the last q values\r\n        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\r\n            if not done:\r\n                max_future_q = np.max(future_qs_list[index])\r\n                new_q = reward + DISCOUNT * max_future_q\r\n            else:\r\n                new_q = reward\r\n\r\n            current_qs = current_qs_list[index]\r\n            current_qs[action] = new_q\r\n\r\n            X.append(current_state)\r\n            y.append(current_qs)\r\n\r\n        log_this_step = False\r\n        if self.tensorboard.step > self.last_logged_episode:\r\n            log_this_step = True\r\n            self.last_logged_episode = self.tensorboard.step\r\n\r\n        with self.graph.as_default():\r\n            self.model.fit(np.array(X)/255, np.array(y), batch_size=TRAINING_BATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if log_this_step else None)\r\n\r\n        if log_this_step:\r\n            self.target_update_counter += 1\r\n\r\n        if self.target_update_counter > UPDATE_TARGET_EVERY:\r\n            self.target_model.set_weights(self.model.get_weights())\r\n            self.target_update_counter = 0\r\n\r\n    def get_qs(self, state):\r\n        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\r\n\r\n    def train_in_loop(self):\r\n        X = np.random.uniform(size=(1, IM_HEIGHT, IM_WIDTH, 3)).astype(np.float32)\r\n        y = np.random.uniform(size=(1, 3)).astype(np.float32)\r\n\r\n        with self.graph.as_default():\r\n            self.model.fit(X, y, verbose=False, batch_size=1)\r\n\r\n        self.training_initialized = True\r\n\r\n        while True:\r\n            if self.terminate:\r\n                return\r\n            self.train()\r\n            time.sleep(0.01)\r\n```\r\n\r\nWhen I run the program: \r\n```\r\n2021-03-21 18:50:05.877519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nUsing TensorFlow backend.\r\nWARNING:tensorflow:From C:/Users/Micha\u0142/Desktop/Magisterka/Project/Carla_0.9.11_env/sentex_carla/sentex_full_tut.py:291: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\r\n\r\n2021-03-21 18:50:07.803967: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-21 18:50:07.806461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-21 18:50:07.827618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 computeCapability: 8.6\r\ncoreClock: 1.837GHz coreCount: 28 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 335.32GiB/s\r\n2021-03-21 18:50:07.827846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-21 18:50:07.834034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-21 18:50:07.834164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-21 18:50:07.837381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-21 18:50:07.838887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-21 18:50:07.843668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-21 18:50:07.846965: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-21 18:50:07.848560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-21 18:50:07.848732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-21 18:50:08.633848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-21 18:50:08.634021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-21 18:50:08.634128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-21 18:50:08.634412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4915 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-03-21 18:50:08.635416: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-21 18:50:08.660884: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-21 18:50:08.661049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 computeCapability: 8.6\r\ncoreClock: 1.837GHz coreCount: 28 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 335.32GiB/s\r\n2021-03-21 18:50:08.661286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-21 18:50:08.661415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-21 18:50:08.661545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-21 18:50:08.661688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-21 18:50:08.662177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-21 18:50:08.662341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-21 18:50:08.662515: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-21 18:50:08.662726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-21 18:50:08.663120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-21 18:50:08.663570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 computeCapability: 8.6\r\ncoreClock: 1.837GHz coreCount: 28 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 335.32GiB/s\r\n2021-03-21 18:50:08.663900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-21 18:50:08.664065: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-21 18:50:08.664222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-21 18:50:08.664385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-21 18:50:08.664546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-21 18:50:08.664710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-21 18:50:08.664866: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-21 18:50:08.665026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-21 18:50:08.665225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-21 18:50:08.665395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-21 18:50:08.665567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-21 18:50:08.665683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-21 18:50:08.665901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4915 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-03-21 18:50:08.666224: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-21 18:50:10.410494: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\r\n2021-03-21 18:50:10.410594: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\r\n2021-03-21 18:50:10.410692: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\r\n2021-03-21 18:50:10.411657: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found\r\n2021-03-21 18:50:10.412499: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n2021-03-21 18:50:10.412642: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-03-21 18:50:10.412928: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\r\n2021-03-21 18:50:10.413062: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Micha\u0142\\AppData\\Local\\Programs\\Python\\Python37\\lib\\threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\Micha\u0142\\AppData\\Local\\Programs\\Python\\Python37\\lib\\threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"C:/Users/Micha\u0142/Desktop/Magisterka/Project/Carla_0.9.11_env/sentex_carla/sentex_full_tut.py\", line 268, in train_in_loop\r\n    self.model.fit(X, y, verbose=False, batch_size=1)\r\n  File \"C:\\Users\\Micha\u0142\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1031, in fit\r\n    version_utils.disallow_legacy_graph('Model', 'fit')\r\n  File \"C:\\Users\\Micha\u0142\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\version_utils.py\", line 122, in disallow_legacy_graph\r\n    raise ValueError(error_msg)\r\nValueError: Calling `Model.fit` in graph mode is not supported when the `Model` instance was constructed with eager mode enabled. Please construct your `Model` instance in graph mode or call `Model.fit` with eager mode enabled.\r\n```\r\n\r\nI have tried adding to model.compile an argument: , run_eagerly equals: True, None, False - the result was still the same.\r\n\r\nThe whole code is here: https://pastebin.pl/view/10e9e350\r\n\r\nDue to the fact that I am working with the Carla environment, it will be hard to reproduce exactly my setup.\r\nThe code is from: [sentex](https://pythonprogramming.net/reinforcement-learning-self-driving-autonomous-cars-carla-python/?completed=/reinforcement-learning-agent-self-driving-autonomous-cars-carla-python/)\r\nFor him that code worked perfectly. I guess that he was using a significantly older version of TensorFlow., so the error is most likely because I am using 2.4.1 version of TF.\r\n", "comments": ["@Michal-Kolomanski \r\nI ran the code shared by you and do not see any error as reported, please refer to  the [gist here](https://colab.research.google.com/gist/Saduf2019/c04182de0f0b2a826b46a8c9943bd135/untitled568.ipynb).", "> @Michal-Kolomanski\r\n> I ran the code shared by you and do not see any error as reported, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/c04182de0f0b2a826b46a8c9943bd135/untitled568.ipynb).\r\n\r\n\r\nI prepared a notebook with exactly the same error: [colab](https://colab.research.google.com/drive/1-B1KmaVtu5cJJgmS0h0rMADrx79vVNvD?usp=sharing). \r\n", "@Michal-Kolomanski \r\nI ran the colab but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4d6000997a868fe08867b634d98362d4/untitled572.ipynb).", "> @Michal-Kolomanski\r\n> I ran the colab but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4d6000997a868fe08867b634d98362d4/untitled572.ipynb).\r\n\r\nPlease install:\r\n`!pip install tensorflow==2.4.1`\r\n`!pip install keras==2.4.1`\r\nIt should solve your problem.\r\n", "I am able to replicate the issue on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/1e7bbddc6b2d34a06cf86ab326bee316/untitled567.ipynb)", "@Michal-Kolomanski I have looked at your code. There is a mix of graph mode and eager mode (and by default TF2.x is eager). Error is mainly due to mix of both modes. We don't support debugging of long code. I think this is more suited for Stackoveflow as there is a larger community who support this kind of questions.\r\n\r\nGitHub is mainly for bugs and performance related issues. Thanks!\r\n", "> @Michal-Kolomanski I have looked at your code. There is a mix of graph mode and eager mode (and by default TF2.x is eager). Error is mainly due to mix of both modes. We don't support debugging of long code. I think this is more suited for Stackoveflow as there is a larger community who support this kind of questions.\r\n> \r\n> GitHub is mainly for bugs and performance related issues. Thanks!\r\n\r\nNow I am 100% sure that the issue is due to using a newer version of TF. Thank you for the response. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47959\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47959\">No</a>\n"]}, {"number": 47957, "title": "Fixed NotImplementedError: Cannot convert a symbolic Tensor to a numpy array", "body": "Fixed #9706\r\n\r\nImplemented as mentioned in https://github.com/tensorflow/models/issues/9706#issuecomment-791113516 and https://github.com/tensorflow/models/issues/9706#issuecomment-792106149 .\r\n\r\nThis problem happens with tensorflow 2.4.1 and numpy 1.20.1 .\r\nMinimal code to reproduce the problem:\r\n```python\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras import layers\r\nmodel = Sequential()\r\nmodel.add(layers.LSTM(256, return_sequences=True))\r\nmodel.build((10,20,30))\r\nmodel.summary()\r\n```\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47957) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@petronny  Can you please check build failures. Thanks!", "I think the main error is:\r\n```\r\nERROR: testImplementsAttributeAssertsOnSideInput (__main__.FunctionTest)\r\ntestImplementsAttributeAssertsOnSideInput (__main__.FunctionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test.runfiles/org_tensorflow/tensorflow/python/ops/array_ops.py\", line 3219, in ones\r\n    tensor_shape.TensorShape(shape))\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test.runfiles/org_tensorflow/tensorflow/python/framework/tensor_shape.py\", line 762, in __init__\r\n    self._dims = [Dimension(d) for d in dims]\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test.runfiles/org_tensorflow/tensorflow/python/framework/tensor_shape.py\", line 762, in <listcomp>\r\n    self._dims = [Dimension(d) for d in dims]\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test.runfiles/org_tensorflow/tensorflow/python/framework/tensor_shape.py\", line 206, in __init__\r\n    .format(value, type(value))), None)\r\n  File \"<string>\", line 3, in raise_from\r\nTypeError: Dimension value must be integer or None or have an __index__ method, got value '1.0' with type '<class 'float'>'\r\n```\r\nI guess it shoule be 1 with type 'int' instead of 1.0 with type 'float'.\r\n\r\nIs there a way to easily run the testImplementsAttributeAssertsOnSideInput test on my local machine?\r\nThen I can try to dig this error out.", "`bazel test //path/to/the/test:test_target` (need to look in the `BUILD` files to get the test target name)", "@petronny  Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "I will take a look after 2 weeks.", "I've fixed that test by appending `.eval()` to convert the tensor to a number.", "I've seen the new CI errors.\r\nMost of them are timeout errors. How could I fix them?\r\n\r\nI'll take a look into the rest errors in a few days.", "@mihaimaruseac  Can you please assist on above comments from @petronny. Thanks!\r\n", "@mihaimaruseac, @petronny Any update on this PR? Please. Thanks!", "This seems to fail a lot of tests. Can you please take a look?", "So these are scenarios where this fails:\r\n\r\n```\r\nshape=[<tf.Tensor 'Placeholder:0' shape=() dtype=int32>, <tf.Tensor 'Placeholder_1:0' shape=() dtype=int32>]\r\n```", "> shape=[<tf.Tensor 'Placeholder:0' shape=() dtype=int32>, <tf.Tensor 'Placeholder_1:0' shape=() dtype=int32>]\r\n\r\nHi, I'm trying to reproduce this error with:\r\n```python\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nx = tf.compat.v1.placeholder(tf.int32, shape=())\r\ny = tf.compat.v1.placeholder(tf.int32, shape=())\r\n ```\r\nHowever, I can't create a tensor with shape=(x, y) even in the vanila tensorflow like:\r\n```python\r\nz = tf.compat.v1.placeholder(tf.int32, shape=(x, y))\r\n```\r\nIs there any hint on what I should do next?", "Hi, \u2026 not opening  a new PR since this one already works on the issue \u2026\r\nI believe the proper fix for the underlying problem is not trying to get a product, because ultimately in situations where the values remain symbolic, they cannot be known, but to properly handle the `NotImplementedError`.\r\nThe code path calling `_constant_if_small` is already properly expecting this to fail; it is merely that the changes in the `tf.Tensor` subclasses yielding a `NotImplementedError` since they got an `__array__` method implemented, which gets called by the NumPy functions, where before NumPy would just yield a `TypeError`.\r\nTherefore I think only the line \r\nhttps://github.com/tensorflow/tensorflow/blob/0494c90c74fce1093bdd19d1257fc2389d7484fd/tensorflow/python/ops/array_ops.py#L2899\r\nneeds to be changed to:\r\n```python\r\nexcept (TypeError, NotImplementedError):\r\n``` \r\nBest regards, Christian", "I've updated the patch to add the `NotImplementedError` mentioned by @csachs .", "Thanks, but I meant that the only required change for fixing the bugs this PR tries to address, is to change the `except` clause, without altering the `np.prod` line. I don't think the test failures will be resolved by this combined approach \u2026 I don't know for sure tho, since I did not run all tests properly due to time constraints (TF builds really take some time locally\u2026).", "I see. I've reverted the `reduce_product` changes.\r\n\r\n@mihaimaruseac Could you trigger another CI build?", "CI looks better now, I think, can someone take a look?", "Is this likely to get a stable backport?"]}, {"number": 47956, "title": "WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer", "body": "In my training setup, I've customized the `.fit` method by overriding the `train_step` function. \r\n\r\n```\r\nclass CustomFit(tf.keras.Model()):\r\n\tdef train_step():\r\n    \t# do some modification \r\n\r\n    def test_step():\r\n\t\t# no modification\r\n\r\nmodel = CustomFit()\r\nmodel.compile(loss, optimizer, metrics)\r\nmodel.fit()\r\n```\r\n\r\nTraining start at this point as it should be but a warning message appears, though until now I didn't find any other issue except this annoying warning message. \r\n\r\n> WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\r\n  opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\r\n\r\nIn my code, by myself, I didn't write anywhere this statement `opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)`. What is this all about? Do I need to pass optimizer like this way before setting in `.compile`? Like \r\n\r\n```\r\nopt = Adam()\r\nopt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\r\nmodel.compile(loss, metrics, optimizer=opt)\r\n```\r\nOr, I can just ignore this warning? When I should use it and not?", "comments": ["@innat \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/46589#issuecomment-774097322) and let us know after you upgrade.", "I saw, and read. What's the catch?", "Let me clarify. I didn't face any `ERROR`. The model runs normally but at the beginning, it's showing this warning message. So, I suspect there something I need to concern about on which `tf` is a warning. ", "@innat \r\nThe warning message does not effect the model performance, can you share your system information , tf version cuda version, the links shared was for you to hide the warning as suggested in the link and resolved by many users in the same issue.\r\n\r\nKindly use the appropriate cuda and tf versions as per the comment to avoid the warning.", "Hiding some non-essential log message makes sense. But this warning tells something very specifically. \r\n\r\n> WARNING:tensorflow:tf.keras.mixed_precision.experimental.LossScaleOptimizer is deprecated. Please use tf.keras.mixed_precision.LossScaleOptimizer instead. Note that the non-experimental LossScaleOptimizer does not take a DynamicLossScale but instead takes the dynamic configuration directly in the constructor. For example:\r\nopt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt)\r\n\r\nLike, somewhere in my code I used this `experimental.LossScaleOptimizer` and it warning me not to use the experimental version. So, I suspected somehow `tf` in the backend, use it for loss scaling. My `tf` version `2.4.0`, CUDA: `11.0`, cuDNN: `8.0.4`"]}, {"number": 47955, "title": "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow", "body": "I've a custom custom `tf.keras` data generator using `tf.keras.utils.Sequence`. Whenever I set `use_multiprocessing` as True, it gives a warning message at the very beginning. The custom data generator (in my case) read the image data, do some augmentation and return batch-wise samples. Nothing special, just a conventional approach. The full warning statement is \r\n\r\n\r\n> WARNING:tensorflow: multiprocessing can interact badly with TensorFlow, causing non-deterministic deadlocks. For high-performance data pipelines tf. data is recommended.\r\n\r\nJust about to start training but freeze everything anyway after showing this message. The issue is, **Is it BAD to use multiprocessing, or Is it NOT POSSIBLE to use?** \r\n\r\n\r\n", "comments": ["@innat \r\nPlease refer to this comment from the member of the keras community [here](https://github.com/tensorflow/tensorflow/issues/46316#issuecomment-760339753).\r\n\r\nAlso please provide with code for us to replicate the issue.", "I wanted to reproduce this issue on some public data set on the kaggle kernel. But surprisingly I couldn't reproduce this issue.\r\n\r\nIn my local machine (Windows), my CPU: Intel Core `i5 6600` (`3.3`GHz), `4` logical Core, and with `TF 2.4.1`, Cuda `11.0`, cuDNN `8.0.4` - GPU: RTIX 2070. \r\n\r\nIn my local experiment, the `use_multiprocessing` froze my training at the beginning but in the kaggle environment, it runs anyway. o_O\r\n\r\nHere is a kernel I've used. https://www.kaggle.com/ipythonx/tf-keras-issue-47955?scriptVersionId=57548428\r\n\r\nIn the notebook, there is a class name `TrainConfig` where multiprocessing is set `True` or `False` during experiment. ", "Additionally, this is the kaggle CPU spec: `Xeon Processors @2.3Ghz, 46MB Cache. The same code runs on here but not on windows pc with `Intel Core i5, @3.3GHz, 6MB Cache`. What do I need to look at here? \r\n\r\nSame code, same package versions but different behavior in both local and online (kaggle) environment. Could it be the cause of hardware now (CPU)? ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47955\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47955\">No</a>\n", "I am facing the same issue. I used tf.keras.utils.Sequence to create my custom data generator, and I tried all solutions I could find to make it work, but whenever I try with multiprocessing, I get that same warning and the training does not start. "]}, {"number": 47954, "title": "TextVectorization inconsistency depending on output_sequence_length", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`TextVectorization`'s behavior changes if `output_sequence_length` is set.\r\n\r\n**Describe the expected behavior**\r\nI would expend the behavior to remain the same (up to the effect of `output_sequence_length`, of course). However, same that runs perfectly in one case, causes a runtime error in the other. The reason is that the former accepts scalar inputs, whereas the latter requires input rank > 0.\r\n\r\nP. S.\r\nI can fix it, the question is whether we want to accept scalar inputs or not.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nds_train, ds_test = tfds.load(\"imdb_reviews\", split=[\"train\", \"test\"], as_supervised=True)\r\n\r\n\r\nencoder1 = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=10000)\r\nencoder2 = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=10000, output_sequence_length=200)\r\n\r\nencoder1.adapt(ds_train.map(lambda x, y: x))\r\nencoder2.adapt(ds_train.map(lambda x, y: x))\r\n\r\nfor x, y in ds_train:\r\n  print(x)\r\n  print(encoder1(x)) # This one works\r\n  print(encoder2(x)) # This one fails\r\n\r\n\r\n\r\ntf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\r\ntf.Tensor(\r\n[  11   14   34  412  384   18   90   28    1    8   33 1322 3560   42\r\n  487    1  191   24   85  152   19   11  217  316   28   65  240  214\r\n    8  489   54   65   85  112   96   22 5596   11   93  642  743   11\r\n   18    7   34  394 9522  170 2464  408    2   88 1216  137   66  144\r\n   51    2    1 7558   66  245   65 2870   16    1 2860    1    1 1426\r\n 5050    3   40    1 1579   17 3560   14  158   19    4 1216  891 8040\r\n    8    4   18   12   14 4059    5   99  146 1241   10  237  704   12\r\n   48   24   93   39   11 7339  152   39 1322    1   50  398   10   96\r\n 1155  851  141    9], shape=(116,), dtype=int64)\r\n\r\n---------------------------------------------------------------------------\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\n<ipython-input-3-fe1d367865bf> in <module>()\r\n      2   print(x)\r\n      3   print(encoder1(x))\r\n----> 4   print(encoder2(x))\r\n\r\n8 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: text_vectorization_1/strided_slice/\r\n\r\n```", "comments": ["[colab link](https://colab.research.google.com/gist/eli-osherovich/591f0e7a44b9b912fd9fcf52711a09dd/untitled1.ipynb)", "Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/acad508bce039cd9db9209532dd88d83/47954.ipynb). Thanks!", "Any response?", "/cc @nikitamaia ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47954\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47954\">No</a>\n"]}, {"number": 47953, "title": "[tf.data] break down and parameterize FromSparseTensorSlicesTest", "body": "This PR refactors the `FromSparseTensorSlicesTest` by splitting the test into multiple parameterized tests.\r\n\r\nAddresses point 6 in https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963.\r\ncc: @jsimsa ", "comments": []}, {"number": 47952, "title": "[tf.data] support all combinations for MultiDeviceIteratorTest", "body": "This PR extends the test combinations for multi-device iterator tests. Additionally, the `self.test_session` is replaced with `self.cached_session` as per deprecation info.\r\n\r\n```TEST LOG\r\n//tensorflow/python/data/kernel_tests:multi_device_iterator_test         PASSED in 7.4s\r\n```\r\nAlso, this PR addresses one of the test with TODO(b/121264236) as support for emulating multiple devices in TF 2 now exists.  Reference: point 5 of https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963\r\ncc: @jsimsa", "comments": ["@jsimsa okay sure. Also, I was hoping we can discuss the graduation of a few `tf.data.experimental` APIs to `tf.data` (as per our discussion) Let me know if you have an internal workflow before proceeding with such changes. Thanks!", "@kvignesh1420 let me create a document that summarizes the status of each of the experimental endpoints and share it with you. The steps towards (and feasibility of) graduating experimental APIs will be different for different APIs.", "@jsimsa That would be great. Thanks!", "Could you let me know an email address that I can use to share the document with you? Thanks.", "@jsimsa you can send the doc to k.vignesh1420@gmail.com", "@jsimsa thanks for sharing the doc. I was thinking of assigning a few of the `ready to be migrated` API's to myself (i.e, put my name beside those items one by one) and work on them. This will prevent conflicts in our commits. WDYT?", "sounds good, let's continue the discussion on the doc"]}, {"number": 47951, "title": "Cant install tflite_runtime while deploying on web application online", "body": "**System information**\r\n- OS Platform and Distribution (windows 10)\r\n\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: Uninstalled(As I cannot use such a huge library)\r\n- Python version:  3.8\r\n- Installed using virtualenv and pip3 on local\r\n[requirements.txt](https://github.com/tensorflow/tensorflow/files/6177015/requirements.txt)\r\n\r\n- GPU 8Gb memory:\r\n\r\nI am new to this but I will try to explain my situation.\r\nSo I am using tflite.interpreter to load the saved model(converted to tflite) using tflite. interpreter.\r\nEverything works fine on the local system but when I deploy my Django app to Heroku or azure. I cant install tflite_runtime from Requirements.py during git push to remote.\r\nI used this to install it on my local\r\npip3 install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\r\n\r\nHow do I include tflite_runtime in requirements.txt so that it automatically installs using pip while pushing to GIT?\r\n\r\n`remote: Collecting scipy==1.6.1 remote: Downloading scipy-1.6.1-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB) remote: Collecting six==1.15.0 remote: Downloading six-1.15.0-py2.py3-none-any.whl (10 kB) remote: Collecting sqlparse==0.4.1 remote: Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB) remote: ERROR: Could not find a version that satisfies the requirement tflite-runtime==2.5.0 (from -r /tmp/build_1599e83a/requirements.txt (line 22)) (from versions: none) remote: ERROR: No matching distribution found for tflite-runtime==2.5.0 (from -r /tmp/build_1599e83a/requirements.txt (line 22)) remote: ! Push rejected, failed to compile Python app. remote: remote: ! Push failed remote: Verifying deploy... remote: remote: ! Push rejected to predictivemaintenance2021. remote: To https://git.heroku.com/pred2021.git ! [remote rejected] master -> master (pre-receive hook declined)\r\n`\r\n", "comments": ["@terryheo could you take a look?", "https://stackoverflow.com/a/2477610/11843861 maybe you resolve the problem by adding an extra index in the requirements.txt.", "Thank you for your time,\r\nI tried using extra index (-i) in requirements.txt but it gave me an error As I have other libraries in my requirements.txt file and it is looking for these libraries as well in the URL provided.\r\nIs there anything else i should try? Please suggest\r\n\r\n", "@nikhilnaregal This issue looks irrelevant with the TensorFlow project. This issue is related to how to use the custom repo in the requirements.txt. I would close this because this is the TensorFlow forum and another post from stackoverflow looks like a better place to handle this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47951\">No</a>\n"]}, {"number": 47950, "title": "Filter out legacy checks in xtensa xa_nnlib", "body": "Update depthwise_conv test to catch future failures like this.\r\n\r\nTested with:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_music_detect_benchmark -j8", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47949, "title": "[TF2.3.0] Failed to Build libtensorflow_cc.so for C++ APIs with conda environment ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (CentOS Linux 7 (Core)):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.7.10\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.1.243 (cuda), 7.6.5(cuDNN)\r\n- GPU model and memory: v100, 32g\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'am tring to build TensorFlow\u2019s C++ interface by this document (https://deepmd.readthedocs.io/en/latest/install-tf.2.3.html).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFirst, I create environment by \r\n\r\n> conda install bazel=3.1.0 cudatoolkit=10.1.243 cudnn=7.6.0=cuda10.1_0 python=3.8.8\r\n> conda install -c conda-forge cudatoolkit-dev=10.1.243 openmpi\r\n> conda install numpy \r\n\r\nAfter configuration, I try to build by 'bazel build -c opt --verbose_failures //tensorflow:libtensorflow_cc.so' \r\nThe error is\r\n\r\n> ERROR: /mnt/gs18/scratch/users/gepei/tensorflow/tensorflow/cc/BUILD:629:1: Linking of rule '//tensorflow/cc:ops/functional_ops_gen_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n>   (cd /mnt/ufs18/home-183/gepei/.cache/bazel/_bazel_gepei/515900c424239e830302083d92d41b3a/execroot/org_tensorflow && \\\r\n>   exec env - \\\r\n>     LD_LIBRARY_PATH=/opt/software/binutils/2.30-GCCcore-7.3.0/lib:/opt/software/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0:/opt/software/GCCcore/7.3.0/lib64:/opt/software/GCCcore/7.3.0/lib \\\r\n>     PATH=/mnt/home/gepei/anaconda3/envs/tensorflow_GPU/bin:/opt/software/binutils/2.30-GCCcore-7.3.0/bin:/opt/software/GCCcore/7.3.0/bin:/mnt/home/gepei/anaconda3/condabin:/usr/lib64/qt-3.3/bin:/opt/software/core/lua/lua/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/hpcc/bin:/usr/lpp/mmfs/bin:/opt/ibutils/bin:/opt/puppetlabs/bin:/opt/dell/srvadmin/bin \\\r\n>     PWD=/proc/self/cwd \\\r\n>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/cc/ops/functional_ops_gen_cc-2.params)\r\n> Execution platform: @local_execution_config_platform//:platform\r\n> bazel-out/k8-opt-exec-50AE0418/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Sfunctional_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `std::allocator<long long>::allocator()'\r\n> collect2: error: ld returned 1 exit status\r\n> Target //tensorflow:libtensorflow_cc.so failed to build\r\n> INFO: Elapsed time: 1427.425s, Critical Path: 388.90s\r\n> INFO: 5225 processes: 5225 local.\r\n> FAILED: Build did NOT complete successfully\r\n\r\n \r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe configuration is here.\r\n> You have bazel 3.1.0- (@non-git) installed.\r\n> Please specify the location of python. [Default is /mnt/home/gepei/anaconda3/envs/tensorflow_GPU/bin/python3]: \r\n> \r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   /mnt/home/gepei/anaconda3/envs/tensorflow_GPU/lib/python3.8/site-packages\r\n> Please input the desired Python library path to use.  Default is [/mnt/home/gepei/anaconda3/envs/tensorflow_GPU/lib/python3.8/site-packages]\r\n> \r\n> Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\n> No OpenCL SYCL support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: \r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with TensorRT support? [y/N]: \r\n> No TensorRT support will be enabled for TensorFlow.\r\n> \r\n> Could not find any cuda.h matching version '' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n> of:\r\n>         '/lib'\r\n>         '/lib64'\r\n>         '/opt/dell/srvadmin/lib64'\r\n>         '/opt/dell/srvadmin/lib64/openmanage'\r\n>         '/opt/ibutils/lib64'\r\n>         '/opt/mellanox/hcoll/lib'\r\n>         '/opt/mellanox/mxm/lib'\r\n>         '/opt/mellanox/sharp/lib'\r\n>         '/usr'\r\n>         '/usr/lib64//bind9-export'\r\n>         '/usr/lib64/atlas'\r\n>         '/usr/lib64/dyninst'\r\n>         '/usr/lib64/mysql'\r\n>         '/usr/lib64/qt-3.3/lib'\r\n> Asking for detailed CUDA configuration...\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: \r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: \r\n> \r\n> \r\n> Please specify the locally installed NCCL version you want to use. [Leave empty to use http://github.com/nvidia/nccl]: \r\n> \r\n> \r\n> Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: /mnt/home/gepei/anaconda3/envs/tensorflow_GPU/pkgs/cuda-toolkit/,/mnt/home/gepei/anaconda3/envs/tensorflow_GPU/\r\n> \r\n> \r\n> Found CUDA 10.1 in:\r\n>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/pkgs/cuda-toolkit/lib64\r\n>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/pkgs/cuda-toolkit/include\r\n> Found cuDNN 7 in:\r\n>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/lib\r\n>     /mnt/ufs18/home-183/gepei/anaconda3/envs/tensorflow_GPU/include\r\n> \r\n> \r\n> Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\n> Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.0,7.0,7.0,7.0]: \r\n> \r\n> \r\n> Do you want to use clang as CUDA compiler? [y/N]: \r\n> nvcc will be used as CUDA compiler.\r\n> \r\n> Please specify which gcc should be used by nvcc as the host compiler. [Default is /opt/software/GCCcore/7.3.0/bin/gcc]: \r\n> \r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n> \r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\n> Not configuring the WORKSPACE for Android builds.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n> \t--config=mkl         \t# Build with MKL support.\r\n> \t--config=monolithic  \t# Config for mostly static monolithic build.\r\n> \t--config=ngraph      \t# Build with Intel nGraph support.\r\n> \t--config=numa        \t# Build with NUMA support.\r\n> \t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n> \t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n> \t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n> \t--config=nogcp       \t# Disable GCP support.\r\n> \t--config=nohdfs      \t# Disable HDFS support.\r\n> \t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\n> Configuration finished\r\n\r\nHere is the line 629 of /mnt/gs18/scratch/users/gepei/tensorflow/tensorflow/cc/BUILD \r\n```\r\ntf_gen_op_wrappers_cc(\r\n    name = \"functional_ops\",\r\n    include_internal_ops = 1,\r\n    op_lib_names = [\r\n        \"functional_ops\",\r\n    ],\r\n    pkg = \"//tensorflow/core\",\r\n    visibility = [\"//tensorflow:internal\"],\r\n)\r\n```\r\n\r\n\r\nCan someone help me with that? Thanks a lot.", "comments": ["Fixed by changing gcc from 7.3.0 to 7.5.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47949\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47949\">No</a>\n"]}, {"number": 47948, "title": "Use Recording memory APIs in the benchmark.", "body": "This allows us to see how the memory usage is affected with different kernel implementations.\r\n\r\nDebugging http://b/183232978\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> looks like we need to include the recording allocator in the BUILD file for the benchmarks.\r\n\r\nfixed."]}, {"number": 47946, "title": "pip and conda install tensorflow problems in anaconda environment", "body": "After creating an environment in anaconda, I try to install the tensorflow by pip and conda. \r\n\r\n\r\n**System information of my computer:** \r\n- OS Platform and Distribution **win10 64bit**\r\n- Python version: **3.8.8 64 bit**\r\n- pip version: **21.0.1**\r\n- Installed using virtualenv: **pip and conda (both have problems)**\r\n- CUDA/cuDNN version: **11.0 for CUDA/ 8.0.5 for cuDNN**\r\n\r\n**Describe the problem**\r\n\r\n**1. For the pip method**\r\n```\r\nconda create --name tf python=3.8\r\npip install tensorflow\r\n```\r\n**The results are as followings:**\r\n```\r\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/\r\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))': /simple/tensorflow/\r\nCould not fetch URL https://pypi.org/simple/tensorflow/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/tensorflow/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))) - skipping\r\nERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow\r\nCould not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1125)'))) - skipping\r\n```\r\n\r\n**2. For the conda method**\r\n```\r\nconda create --name tf python=3.8\r\nconda install tensorflow\r\n```\r\nIt successes. **But the tensorflow version is 2.3.0, not the recent version 2.4.0.** So that I cannot use GPU.\r\n", "comments": ["@RayGuo-C \r\nPlease refer to these two links and let us know: [link](https://github.com/pypa/pip/issues/5448), [link1](https://stackoverflow.com/questions/25981703/pip-install-fails-with-connection-error-ssl-certificate-verify-failed-certi)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47946\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47946\">No</a>\n"]}, {"number": 47945, "title": "TensorFlow-Keras Code Example ", "body": "In keras official [doc](https://keras.io/examples/), there are some nice code examples to get started on something. Last time I visited this amazing write-up about [object detection](https://keras.io/examples/vision/retinanet/). However, what I mostly found in the computer vision examples are kinda randomly placed code examples with different degrees of complexities. \r\n\r\nSo, I think it would be clear and convenient to **make them sort in some order**. In computer vision, there are plenty of subcategories like classification, detection, segmentation, etc. Additionally, there are some general code examples like augmentation, different training approaches like knowledge distillation, etc. Because of contribution, the content gets enrich from time to time, sooner or later some kind of organizing strategies will be required. ", "comments": []}, {"number": 47944, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, MAXIMUM, MAX_POOL_2D, MUL, PAD. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["The select tf option can enable tf.ExtractImagePatches op. See https://www.tensorflow.org/lite/guide/ops_select", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47944\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47944\">No</a>\n"]}, {"number": 47943, "title": "When in \"webgl\" backend and image width is odd, the second run of tf.conv2d() may be wrong.", "body": "\r\n**System information**\r\n- TensorFlow.js version: 3.2.0\r\n\r\n**Describe the current behavior**\r\n\r\nWhen tf.gtBackend() is \"webgl\".\r\n  - If the width of the input is even, the result of tf.conv2d() is correct.\r\n  - If the width of the input is odd, the result of first time tf.conv2d() is correct. But the second time result may be wrong.\r\n \r\nWhen tf.gtBackend() is \"wasm\" or \"cpu\".\r\n  - The above problem seems not existed.\r\n\r\n\r\n**Describe the expected behavior**\r\nNo matter which backend, what kinds image width (even or odd), how many times to run tf.conv2d(), the result should be same and corrrect.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/ColorfulCakeChen/e3c7e6ce1be9c6c0f5f1a6b209b9dd02\r\n(The code coulde be paste into any example console of Tensorflow.js API (https://js.tensorflow.org/api/3.2.0/) weg page to run.)\r\n", "comments": ["(Sorry, I think this should be post to project tfjs (not here). So I close it.)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47943\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47943\">No</a>\n"]}, {"number": 47942, "title": "Bug Issues Testing", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47942\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47942\">No</a>\n"]}, {"number": 47940, "title": "Make horizontal input fusion and LLVM IR emitter deterministic", "body": "", "comments": []}, {"number": 47939, "title": "Feature request: #47493", "body": "I do some code refactoring and add an additional code description on comment for this [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py#L557-L646) which addressing [this thread](https://github.com/tensorflow/tensorflow/issues/47493). Rest assured, there are no other API that will be affected by this PR. \r\n\r\nImplements #47493", "comments": ["@gbaned @deeb02, the gentlest of bumps on this. Can I have a few seconds of your time for looking on this? \ud83d\ude04 ", "This PR changes the semantic of the API in a backward incompatible way, without achieving anything important. The current API is valid and follows the original math from the original paper as far as I can tell."]}, {"number": 47938, "title": "`model.fit()` ends before all epochs are ran", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (in a Docker container)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: CUDA 11.0 with cuDNN 8.0.5.39\r\n- GPU model and memory: GeForce RTX 2080 Ti with 11019 MiB\r\n\r\n**Describe the current behavior**\r\nI have been trying to implement [FlowNet 2.0](https://arxiv.org/abs/1612.01925) using Tensorflow 2.0 as an exercise. I've managed to implement a single FlowNetS module, but have been unable to train it. My `model.fit()` function is ran with 480 epochs, with 2500 steps per epoch. For some reason, `model.fit()` always ends after 2 epochs.\r\n\r\n**Describe the expected behavior**\r\nThe code runs for all 480 epochs (this will take, by my estimation, 200 days as the dataset loading is slow).\r\n\r\n**Standalone code to reproduce the issue**\r\nThis issue probably cannot be reproduced in Colab due to the large dataset and long runtime.\r\n\r\nHere's some details of my implementation:\r\n- subclassed from `tf.keras.Model`, which has a custom implementation of `__init__()`, `call()` and `test_step()`. The only thing I changed in `test_step()` is the metrics that get returned by `test_step()`\r\n- A custom loss class and a custom metric class\r\n- Adam optimizer with PiecewiseConstantDecay schedule\r\n- A `tf.data.Dataset` that dynamically loads the images and flows as necessary, and passes the data to the network in batches of 8, with prefetching. I cannot load the full Flying Chairs dataset into memory, since it is 30 GB\r\n- Learning done with the standard calls of `model.compile()` and `model.fit()`. The model returns multiple outputs during training, so loss_weights are provided during compilation. The `fit()` command is as below, with 2 `ModelCheckpoint` callbacks to store the full model of the best and last checkpoint (using this [temporary workaround](https://github.com/tensorflow/tensorflow/issues/42741#issuecomment-706534711)):\r\n```\r\nhistory = net.fit(x=train_set, epochs=480, verbose=1, validation_data=val_set,\r\n                      steps_per_epoch=2500, callbacks=[best_checkpoint, last_checkpoint])\r\n```\r\n\r\nI'm not sure how I'm supposed to include the source code, since the project source code is split into multiple files across subdirectories. Should I compile all the relevant code into a single source file?\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe output I get is as follows:\r\n```\r\nroot@4d7382461e21:/workspace/flownet2# python3 main.py\r\n2021-03-19 15:54:31.910652: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar\r\ny libcudart.so.11.0\r\n2021-03-19 15:54:33.351618: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_device\r\ns not set\r\n2021-03-19 15:54:33.352532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar\r\ny libcuda.so.1\r\n2021-03-19 15:54:33.438470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2021-03-19 15:54:33.439233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2021-03-19 15:54:33.439303: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar\r\ny libcudart.so.11.0\r\n2021-03-19 15:54:33.439397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar\r\ny libcublas.so.11\r\n2021-03-19 15:54:33.439440: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar\r\ny libcublasLt.so.11\r\n2021-03-19 15:54:33.439484: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic librar\r\ny libcufft.so.10\r\n2021-03-19 15:54:33.439527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-19 15:54:33.443278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-19 15:54:33.444066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-19 15:54:33.444126: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 15:54:33.446050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2021-03-19 15:54:33.446391: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-19 15:54:33.447649: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-19 15:54:33.614936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:17:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2021-03-19 15:54:33.615494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:65:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2021-03-19 15:54:33.615560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 15:54:33.615574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 15:54:33.615609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-19 15:54:33.615623: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-19 15:54:33.615635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-19 15:54:33.615723: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-19 15:54:33.615762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-19 15:54:33.615778: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 15:54:33.617531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2021-03-19 15:54:33.617607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-19 15:54:34.312786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-19 15:54:34.312840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1\r\n2021-03-19 15:54:34.312848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N\r\n2021-03-19 15:54:34.312853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N\r\n2021-03-19 15:54:34.314939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10069 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:17:00.0, compute capability: 7.5)\r\n2021-03-19 15:54:34.316283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9825 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\n2021-03-19 15:54:34.754017: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-03-19 15:54:34.772736: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3299990000 Hz\r\nEpoch 1/480\r\n2021-03-19 15:55:54.178077: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-19 15:55:56.009098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-19 15:55:56.406221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2500/2500 [==============================] - 31685s 13s/step - loss: 965251.6302 - flow2_loss: 2211748.8726 - flow3_loss: 2211866.4072 - flow4_loss: 2213292.5525 - flow5_loss: 2219776.0292 - flow6_loss: 2219457.2435 - flow2_aae: 273742796.5962 - flow3_aae: 273851546.8446 - flow4_aae: 274638881.0683 - flow5_aae: 275303256.6630 - flow6_aae: 275404682.2226 - val_epe: 2226169.7500 - val_aae: 17670296.0000\r\n\r\nEpoch 00001: loss improved from inf to 964092.00000, saving model to /workspace/flownet2/milestone1/best/\r\n2021-03-20 00:42:41.302179: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n\r\nEpoch 00001: saving model to /workspace/flownet2/milestone1/last/\r\nEpoch 2/480\r\n2500/2500 [==============================] - 4285s 2s/step - loss: 919111.6756 - flow2_loss: 2113024.2584 - flow3_loss: 2112958.6253 - flow4_loss: 2113410.2004 - flow5_loss: 2113135.0545 - flow6_loss: 2112805.9978 - flow2_aae: 56491300.6014 - flow3_aae: 56474786.1364 - flow4_aae: 56593601.0892 - flow5_aae: 56521067.3974 - flow6_aae: 56437117.0523 - val_epe: 2226564.2500 - val_aae: 17701936.0000\r\n\r\nEpoch 00002: loss improved from 964092.00000 to 921045.31250, saving model to /workspace/flownet2/milestone1/best/\r\n\r\nEpoch 00002: saving model to /workspace/flownet2/milestone1/last/\r\n{'loss': [964092.0, 921045.3125], 'flow2_loss': [2194216.25, 2117453.25], 'flow3_loss': [2195233.75, 2117392.75], 'flow4_loss': [2199682.75, 2117850.75], 'flow5_loss': [2219717.75, 2117574.25], 'flow6_loss': [2217494.0, 2117253.5], 'flow2_aae': [545963968.0, 59821748.0], 'flow3_aae': [546967424.0, 59804596.0], 'flow4_aae': [550095296.0, 59930716.0], 'flow5_aae': [552018432.0, 59854408.0], 'flow6_aae': [551568128.0, 59766340.0], 'val_epe': [2226169.75, 2226564.25], 'val_aae': [17670296.0, 17701936.0]}\r\nroot@4d7382461e21:/workspace/flownet2#\r\n```", "comments": ["I found the problem. Turns out I needed to use the `repeat()` function on the dataset. This line of output was not visible in my screen because it got overwritten (I was using a tmux session):\r\n```\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1200000 batches). You may need to use the repeat() function when building your dataset.\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47938\">No</a>\n", "From what I can tell, this issue is related to #36539 where a dataset iterator is not recreated at every new epoch"]}, {"number": 47937, "title": "[ROCm] Enabling unit test fft_ops_test.py on ROCm", "body": "This PR is based on the original changes made by @ekuznetsov139 here - https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/1167\r\n\r\n\r\n----------------------\r\n\r\nFix for a rocFFT bug in real<-->complex transforms\r\n\r\nThe input buffer contents can be over-written by rocFFT for all of the real <---> complex transforms (R2C, C2R, D2Z, Z2D), even when transform explicitly provides a separate output buffer.\r\n\r\nSee following issues for more details\r\n\r\n* ROCmSoftwarePlatform/rocFFT#298\r\n* ROCmSoftwarePlatform#1150\r\n\r\nThis commit adds a change to copy the input buffer (for the above transforms) and use the input copy when calling the rocFFT api to do the transform\r\n\r\n\r\n--------------------\r\n\r\n\r\n Updating fft_ops_test.py for ROCm\r\n\r\n1. Some testcases were being skipped on the ROCm platform. Those tests are now passing and hence being enabled by this commit\r\n\r\n2. For testcases that test the irfft transform, the input needs to be massaged on the ROCm platform. The ROCm irfft transform expects its input to be in the form that is output by the rfft transform.\r\n\r\n---------------------\r\n\r\n\r\n/cc @cheshire @chsigg @ekuznetsov139 ", "comments": ["@cheshire @chsigg gentle ping", "@deven-amd  Can you please check @cheshire's comments and keep us posted ? Thanks!", "@gbaned I have addressed the feedback from @cheshire and rebased to tip.\r\n"]}, {"number": 47936, "title": "Add GPU support of AsString + StringToHashBucket", "body": "This PR adds the GPU support for patterns of AsString + StringToHashBucket ops. We propose a new fused TensorToHashBucket op and use it via grappler remapper.\r\n\r\n@nluehr @benbarsdell ", "comments": ["@ezhulenev can you please review the Grappler changes?", "@sanjoy: Any updates on this PR? It seems some internal checks are failed. Can you help check?", "> @sanjoy: Any updates on this PR? It seems some internal checks are failed. Can you help check?\r\n\r\nLooking now.", "@sanjoy Any news about this PR?", "> @sanjoy Any news about this PR?\r\n\r\nNot yet, I ran into some issues importing the integrating dependence internally."]}, {"number": 47935, "title": "Stubbed out memory allocation calls in audio library", "body": "The micro features library used by the micro_speech example makes direct calls to malloc and new. Since this is used by both Micro and TensorFlow itself, I've added stub calls that direct to either the standard library allocation functions on a desktop OS where a heap is available, or using a simple linear allocation scheme from a fixed pool on embedded systems. This is needed to unblock some library integrations, and is tracked in b/152436073.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@petewarden Can you please check @advaitjain's comments and keep us posted ? Thanks!", "@petewarden Any update on this PR? Please. Thanks!", "@vp-cad"]}, {"number": 47934, "title": "Process finished with exit code -1073740791 (0xC0000409) when training on GPU", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [copied code from tensorflow transfer learning documentation](https://www.tensorflow.org/guide/keras/transfer_learning#an_end-to-end_example_fine-tuning_an_image_classification_model_on_a_cats_vs_dogs_dataset)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 - 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): ``pip install tensorflow``\r\n- TensorFlow version (use command below): ``v2.4.0-rc4-71-g582c8d236cb 2.4.0``\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: cuda:  cuda_11.2.r11.2/ cuDNN: v8.1.1.33\r\n- GPU model and memory: Nvidia 1080 8GB\r\n\r\ntf_env script output:\r\n\r\n```\r\n\r\n== check python ===================================================\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nbash: c++: command not found\r\n\r\n== check pips ===================================================\r\nnumpy                  1.19.5\r\nprotobuf               3.14.0\r\ntensorflow             2.4.0\r\ntensorflow-estimator   2.4.0\r\n\r\n== check for virtualenv =========================================\r\n\r\n== tensorflow import ============================================\r\nbash: /c/Users/Faraz: No such file or directory\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Mar 19 23:41:09 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.89       Driver Version: 460.89       CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080   WDDM  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   36C    P8     9W /  N/A |    963MiB /  8192MiB |      1%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A       532    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A      1276    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A      2784    C+G   ...lPanel\\SystemSettings.exe    N/A      |\r\n|    0   N/A  N/A      4564    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\r\n|    0   N/A  N/A      7408    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A      7928    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n|    0   N/A  N/A      8532    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\r\n|    0   N/A  N/A      8732    C+G   ...artMenuExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A      9116    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\r\n|    0   N/A  N/A      9996    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\r\n|    0   N/A  N/A     11768    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A     11836    C+G   ...m Files (x86)\\SCM\\SCM.exe    N/A      |\r\n|    0   N/A  N/A     11936    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\r\n|    0   N/A  N/A     12800    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\r\n|    0   N/A  N/A     13164    C+G   ...a\\74.0.3911.160\\opera.exe    N/A      |\r\n|    0   N/A  N/A     14716    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\r\n|    0   N/A  N/A     15196    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.4.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\faraz khan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 8, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running model training on GPU, I get the following output:\r\n\r\n```\r\n2021-03-19 23:43:27.889416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-19 23:43:31.028195: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-19 23:43:31.029096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-03-19 23:43:31.052703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.771GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-03-19 23:43:31.053038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-19 23:43:31.080499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-19 23:43:31.080660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-19 23:43:31.097601: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-19 23:43:31.101390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-19 23:43:31.150363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-19 23:43:31.165564: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-19 23:43:31.166410: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-19 23:43:31.166617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-19 23:43:31.167034: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-19 23:43:31.167949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.771GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-03-19 23:43:31.168488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-03-19 23:43:31.168746: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-19 23:43:31.169257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-19 23:43:31.169475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-03-19 23:43:31.169697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-03-19 23:43:31.169887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-03-19 23:43:31.170027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-03-19 23:43:31.170168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-19 23:43:31.170350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-03-19 23:43:31.796539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-19 23:43:31.796758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-03-19 23:43:31.796853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-03-19 23:43:31.797098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6274 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2021-03-19 23:43:31.798090: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nNumber of training samples: 9305\r\nNumber of validation samples: 2326\r\nNumber of test samples: 2326\r\n2021-03-19 23:43:32.077770: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-03-19 23:43:33.184162: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\nEpoch 1/20\r\n2021-03-19 23:43:37.097103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-03-19 23:43:37.446836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-03-19 23:43:37.484678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n```\r\nThe code I am using is mentioned from the official documentation for transfer learning as linked above. But every time I get this error. \r\n\r\nAlso, no matter what batch size I use, the GPU memory always shoots up to almost maximum capacity as can be seen from screenshot below, but no training happens as it fails at ``epoch 1``.\r\n\r\n![Capture221](https://user-images.githubusercontent.com/33456896/111851982-4202ec00-890d-11eb-9cf8-70a1540d6675.JPG)\r\n\r\nWhen I re-run this code on cpu using the lines:\r\n\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n```\r\n\r\nThe training runs fine without any problems but runs on the cpu. :(\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be able to train on the GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nAs mentioned above, I'm using the transfer learning tensorflow example code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\n# tfds.disable_progress_bar()\r\n\r\ntrain_ds, validation_ds, test_ds = tfds.load(\r\n    \"cats_vs_dogs\",\r\n    # Reserve 10% for validation and 10% for test\r\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\r\n    as_supervised=True,  # Include labels\r\n)\r\n\r\nprint(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\r\nprint(\r\n    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\r\n)\r\nprint(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))\r\n\r\nplt.figure(figsize=(10, 10))\r\nfor i, (image, label) in enumerate(train_ds.take(9)):\r\n    ax = plt.subplot(3, 3, i + 1)\r\n    plt.imshow(image)\r\n    plt.title(int(label))\r\n    plt.axis(\"off\")\r\n\r\nsize = (150, 150)\r\n\r\ntrain_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\r\nvalidation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\r\ntest_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))\r\n\r\nbatch_size = 32\r\n\r\ntrain_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\r\nvalidation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\r\ntest_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)\r\n\r\ndata_augmentation = keras.Sequential(\r\n    [\r\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\r\n        layers.experimental.preprocessing.RandomRotation(0.1),\r\n    ]\r\n)\r\n\r\nfor images, labels in train_ds.take(1):\r\n    plt.figure(figsize=(10, 10))\r\n    first_image = images[0]\r\n    for i in range(9):\r\n        ax = plt.subplot(3, 3, i + 1)\r\n        augmented_image = data_augmentation(\r\n            tf.expand_dims(first_image, 0), training=True\r\n        )\r\n        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\r\n        plt.title(int(labels[0]))\r\n        plt.axis(\"off\")\r\n\r\n\r\nbase_model = keras.applications.Xception(\r\n    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\r\n    input_shape=(150, 150, 3),\r\n    include_top=False,\r\n)  # Do not include the ImageNet classifier at the top.\r\n\r\n# Freeze the base_model\r\nbase_model.trainable = False\r\n\r\n# Create new model on top\r\ninputs = keras.Input(shape=(150, 150, 3))\r\nx = data_augmentation(inputs)  # Apply random data augmentation\r\n\r\n# Pre-trained Xception weights requires that input be normalized\r\n# from (0, 255) to a range (-1., +1.), the normalization layer\r\n# does the following, outputs = (inputs - mean) / sqrt(var)\r\nnorm_layer = keras.layers.experimental.preprocessing.Normalization()\r\nmean = np.array([127.5] * 3)\r\nvar = mean ** 2\r\n# Scale inputs to [-1, +1]\r\nx = norm_layer(x)\r\nnorm_layer.set_weights([mean, var])\r\n\r\n# The base model contains batchnorm layers. We want to keep them in inference mode\r\n# when we unfreeze the base model for fine-tuning, so we make sure that the\r\n# base_model is running in inference mode here.\r\nx = base_model(x, training=False)\r\nx = keras.layers.GlobalAveragePooling2D()(x)\r\nx = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\r\noutputs = keras.layers.Dense(1)(x)\r\nmodel = keras.Model(inputs, outputs)\r\n\r\n# model.summary()\r\n\r\nmodel.compile(\r\n    optimizer=keras.optimizers.Adam(),\r\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\r\n    metrics=[keras.metrics.BinaryAccuracy()],\r\n)\r\n\r\nepochs = 20\r\nmodel.fit(train_ds, epochs=epochs, validation_data=validation_ds)\r\n\r\n```\r\n", "comments": ["This is actually an nvcc bug. we are also running into this and reported to nvidia.\r\nNvidia has fixed this issue in cuda 10.1, but reported that they will not patch 10.0 with the fix.\r\nSo uninstall your cuda and re-install with latest version of cuda", "> This is actually an nvcc bug. we are also running into this and reported to nvidia.\r\n> Nvidia has fixed this issue in cuda 10.1, but reported that they will not patch 10.0 with the fix.\r\n> So uninstall your cuda and re-install with latest version of cuda\r\n\r\nThanks but I'm already using 11.2 I'm not using 10.0", "@farazk86 \r\nCan you please try with 11.0 and let us know, with tf 2.4.", "> @farazk86\r\n> Can you please try with 11.0 and let us know, with tf 2.4.\r\n\r\nOK, so, I tried it now with CUDA 11.0, 11.1, 11.2.2\r\n\r\nIve tried with all above versions and still get the same error :(", "Right, so an update, I was finally able to train on GPU.\r\n\r\nBut I had to downgrade to ``CUDA 10.1``, ``cuDNN 7.6`` and ``tensorflow 2.3``. \r\n\r\nI had to downgrade to make this work but I would really appreciate a solution to this for ``tensorflow 2.4`` and the newer CUDA and cuDNN as I have other frameworks that depend on the latest versions :(\r\n\r\n", "@farazk86 \r\nI ran the code on 2.4 and it works fine,please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/10e31dc3cec9f27676126e56caf9003c/untitled569.ipynb).\r\nAs the issue is resolved for you can you please move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47934\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47934\">No</a>\n", "Any possible solutions?", "Problem was solved for me when I followed the instructions [in this link ](https://www.reddit.com/r/tensorflow/comments/jsalkw/rtx_3090_and_tensorflow_for_windows_10_step_by/)", "RTX 3090 and Tensorflow for Windows 10 - step by step\r\nI recently bought an RTX 3090 (upgrading from a GTX 1060) and needed my keras/tensorflow notebooks to work. There are some guides on this on the internet, but these were often skipping some steps or explanations, so I wanted to share a very simple, \"for dummies\" kind of step by step instruction with explanations on how I got it to work on my system. It is similar to this guide, but a bit different as I encountered some problems that was not mentioned in that guide. I am using a Ryzen 2700X and Windows 10 Home.\r\n\r\nSo, what you need to know beforehand: The NVIDIA 3000 Series GPUs (Ampere) require CUDA v11 and cuDNN v8 to work. The tensorflow versions on anaconda and pip on Windows (currently at max tensorflow 2.3) do not include a tensorflow built with CUDA v11. But you can use pip to install a nightly build of tensorflow (currently tensorflow 2.5) which built with CUDA v11. Apart from a tensorflow build with CUDA v11, you will also need the actual DLLs for CUDA v11 and cuDNN v8. Normally, you would just install these with anaconda with the packages cudatoolkit and cudnn, but while cudatoolkit is available with v11, for cudnn, at least for Windows, v8 is not available in anaconda. The workaround is to manually get these DLLs and set them in the system environment path (so that python/tensorflow can find and load them). So let's start:\r\n\r\n\r\n\r\nFirst, install anaconda if you haven't already. Open the anaconda prompt with admin rights.\r\n\r\nType conda create -n tf2 python=3.8 and hit enter to create a new anaconda environment with python 3.8 (the tensorflow nightly build needs python 3.8 or higher, that's why we are using python 3.8)\r\n\r\nType activate tf2 or conda activate tf2 and hit enter to enter that new environment.\r\n\r\nInstall the nightly tensorflow build with pip3 install tf-nightly-gpu\r\n\r\nInstall other packages that you might need. For me, it's conda install jupyter scikit-learn matplotlib pandas\r\n\r\nNow, download CUDA v11 from NVIDIA (https://developer.nvidia.com/cuda-downloads or https://developer.nvidia.com/cuda-toolkit-archive). Yeah, the file is pretty big with 3GB.\r\n\r\nAdditionally, apparently we also need a Microsoft Visual Studio version for C++ for the installer to run properly. Download the free Visual Studio Community Edition (https://visualstudio.microsoft.com/downloads/) and install the C++ components. For this, select \"Desktop development with C++\", select the first 6 options and install. This step is taken from the guide I mentioned earlier, so refer to it if you have trouble with this. For me, I already had Visual Studio with C++ in mind set up on my computer, so I could skip this step.\r\n\r\nNow, let's first execute the CUDA v11 installer. Execute it. You can do the express installation, but if you already have GeForce Experience installed, you can also choose the Custom option and deselect everything that you already have installed with a higher version. For me, I only needed the very first checkbox with the CUDA options, so that might be enough.\r\n\r\nWhat the CUDA v11 installer basically did was installing all the CUDA v11 DLLs, Headers, and stuff in the directory \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\" (the version may be different for you). What we will do next: Add the cuDNN DLLs, Headers, etc. in this directory as well and then add this directory to the system path. Ok, let's go.\r\n\r\nDownload cuDNN from NVIDIA (https://developer.nvidia.com/rdp/cudnn-download). This file is around 700MB. You need to register as a developer and answer some questions, but don't worry, it's free. When asked for an email, you can type in any email, since in the next page, you will get an option to login using google or facebook as an alternative (which you may or may not prefer). Once you downloaded the file, extract it. Going into the directory, you will see three folders \"bin\", \"include\", \"lib\". Comparing it with the CUDA v11 directory (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1), you'll notice that these directories are present there as well! So just copy the folders from cuDNN to the CUDA v11 directory. Windows will add the files into the existing folders.\r\n\r\nNow, let's add those directories to the system path. In windows, open start and search for \"This PC\". Rightclick and select \"Properties\" to open a Window called \"System\". On the left side at the bottom, select \"Advanced system settings\". Click \"Environment Variables...\" at the bottom. Here, in the lower half, in \"System variables\", find and open \"Path\". Here, click \"New\" to add a new directory to the system path. Do this every time for each of the following directories (as mentioned earlier, the version number may be different for you). Some of the directories may be already listed there, so feel free to skip them (there is no negative effect from double entries though, so don't worry too much):\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\libnvvp\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\extras\\CUPTI\\lib64\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\include\r\n\r\nNow, very important: Restart your system!\r\n\r\nNow, run your code to see if everything works. For me, it was through a jupyter notebook. A simple thing to do first is to import tensorflow and check the physical devices:import tensorflow as tftf.config.list_physical_devices()\r\n\r\nYour GPU may not show up. Take a close look at the output of the console (for me, it was the anaconda prompt with which I started up my jupyter notebook). There, you should see logs like tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll or a similar log stating that a certain DLL could not be loaded! In my case, everything loaded except the DLL \"cusolver64_10.dll\". So, I went to the CUDA v11 directory (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1), opened the \"bin\" folder (the DLLs are in there) to check if that DLL was there. Nope, it was not. Instead, there was \"cusolver64_11.dll\". So what I did was just copy that DLL and renamed the copy to \"cusolver64_10.dll\". Yeah, sounds dumb, but after that, everything worked.\r\n\r\nI hope this guide helps at least someone. Hopefully, we will get official versions of tensorflow 2.5 and the CUDA/cudnn packages soon so that these kind of workarounds will not be required anymore.\r\n\r\n\r\n\r\nEDIT: Somewhat obvious thing I noticed today: Had a video call running and was using the background removal of NVIDIA Broadcast. Tried to do some inference during the call and it was failing with error messages like Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED and tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.. Well, yeah, that was because NVIDIA Broadcast was running, which itself uses the machine learning capabilities of the GPU. After closing it, things worked normally again. Kind of obvious, but somewhat easy to overlook/forget.\r\n\r\n72 Comments\r\n\r\nShare\r\n\r\nSave\r\n\r\nHide\r\n\r\nReport\r\n"]}]