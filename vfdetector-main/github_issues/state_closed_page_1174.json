[{"number": 17972, "title": "BF:Fix for rendering documentation", "body": "Fixes #17971 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17971, "title": "SVD example not rendering properly in docs", "body": "\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution ( Linux Ubuntu 16.04)**:\r\n- **TensorFlow version (master)**:\r\n\r\n\r\n### Describe the problem\r\nTensorflow docs for `svd` example not rendering \r\n\r\n![image](https://user-images.githubusercontent.com/2484004/37867110-4a59f9d4-2f6a-11e8-9d46-e1f74b860622.png)\r\n", "comments": []}, {"number": 17970, "title": "Add kernels/random_op.cc to contrib/makefile", "body": "This fixes an iOS build issue reported in #17666.", "comments": []}, {"number": 17969, "title": "Estimator not running custom init_op when loading from checkpoint", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\ntf.estimator does not initialize all model variables in the following scenario:\r\nThe model function provides a custom Saver that restores a subset of the GLOBAL_VARIABLES,\r\nand a custom init_op that initializes the non-restored variables. \r\n\r\nIn `SessionManager.prepare_session` there is the following code:\r\n```\r\n    if not is_loaded_from_checkpoint:\r\n      if init_op is None and not init_fn and self._local_init_op is None:\r\n        raise RuntimeError(\"Model is not initialized and no init_op or \"\r\n                           \"init_fn or local_init_op was given\")\r\n      if init_op is not None:\r\n        sess.run(init_op, feed_dict=init_feed_dict)\r\n      if init_fn:\r\n        init_fn(sess)\r\n\r\n    local_init_success, msg = self._try_run_local_init_op(sess)\r\n    if not local_init_success:\r\n      raise RuntimeError(\r\n          \"Init operations did not make model ready for local_init.  \"\r\n          \"Init op: %s, init fn: %s, error: %s\" % (_maybe_name(init_op),                                               init_fn, msg))\r\n```\r\n\r\nThe session manager does not run the `init_op` when loading from a checkpoint. It probabily would make sense to try the `init_op` at least in cases where `local_init_success` is `False` before aborting with an error.\r\n\r\nIf you agree I can create a PR, otherwise it would make sense to update the documentation to explicitely state that init won't be run in these scenarios (that should probably go in https://www.tensorflow.org/api_docs/python/tf/train/Scaffold)", "comments": ["@ispirmustafa Can you comment on this?", "we expect that all variables except local (tf.contrib.framework.local_variable) ones should be exist in checkpoint. \r\n"]}, {"number": 17968, "title": "tensorflow process hangs with use of cudnn_rnn", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0.61/6.0.21\r\n- **GPU model and memory**: 1080 8GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI ran a RNN model with tensorflow.contrib.cudnn_rnn.CudnnLSTM. After some epochs, the process hangs and never respond, even with `ctrl + c`. I also observed the volatile gpu-util fixes at 100% after the hang. This was not a problem when I ran the RNN with tensorflow.contrib.rnn.LSTMCell\r\n\r\n### Source code / logs\r\ncall stacks of all Threads is here: \r\nhttps://pastebin.com/sNxC6fWC\r\n\r\n", "comments": ["@protoget : Could you take a look?\r\n\r\n@harryxu-yscz : Is there a short program you can share that reproduces the problem?", "@asimshankar Sure. It would be helpful to have a  minimum reproducible code snippet. Cudnn LSTM uses more GPU memory than non-platform-dependent TF LSTM.", "Unfortunate I am not able to provide a snippet that reproduces the problem. I hope the call stacks are helpful. \r\n\r\nStrangely, my friend ran exactly the same code on his machine and had no problem with it. Am I running into the hardware issue here https://github.com/tensorflow/tensorflow/issues/1947?\r\n\r\nThe LSTM is written as:\r\n```\r\n        import tensorflow.contrib as tc\r\n        # bi-lstm\r\n        lstm_fw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\r\n                                         num_units=hidden_size,\r\n                                         input_size=input_size,\r\n                                         # dropout = 1 - keep_prob\r\n                                         dropout=0.)\r\n        lstm_bw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\r\n                                         num_units=hidden_size,\r\n                                         input_size=input_size,\r\n                                         # dropout = 1 - keep_prob\r\n                                         dropout=0.)\r\n        param_fw = tf.get_variable(\"lstm_fw_params\",\r\n                                   initializer=tf.random_uniform([lstm_fw.params_size()],\r\n                                                                 -0.1, 0.1),\r\n                                   validate_shape=False)\r\n        param_bw = tf.get_variable(\"lstm_bw_params\",\r\n                                   initializer=tf.random_uniform([lstm_bw.params_size()],\r\n                                                                 -0.1, 0.1),\r\n                                   validate_shape=False)\r\n        c_fw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        c_bw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        h_fw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        h_bw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        with tf.variable_scope(\"fw\"):\r\n            out_fw, h_fw, c_fw = lstm_fw(inputs, h_fw, c_fw, param_fw)\r\n        with tf.variable_scope(\"bw\"):\r\n            inputs_bw = tf.reverse_sequence(\r\n                inputs, seq_lengths=length, seq_dim=0, batch_dim=1)\r\n            out_bw, h_bw, c_bw = lstm_bw(inputs_bw, h_bw, c_bw, param_bw)\r\n            out_bw = tf.reverse_sequence(\r\n                out_bw, seq_lengths=length, seq_dim=0, batch_dim=1)\r\n        output = tf.concat([out_fw, out_bw], axis=2)\r\n        output = tf.transpose(output, [1, 0, 2])\r\n        output_h = tf.concat([h_fw, h_bw], axis=0)\r\n        output_c = tf.concat([c_fw, c_bw], axis=0)\r\n        return output, (tc.rnn.LSTMStateTuple(h=output_h, c=output_c),)\r\n```", "Same issue on tf1.10, also on tf1.12, with [nvidia tensorflow image 18.12](https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.12.html#rel_18.12), or tensorflow official docker images(tf1.10-devel-py2, tf1.12-devel-py3).\r\nIt only happens on some runs. GPU utilization at 100% and the process is not responsive.", "Also happening with tf1.12.\r\n\r\nHappens randomly, with low prob.\r\nAWS P3 instances [happened on at least two different instances]. \r\nGPU at 100% \"utilization\", but temp/fan are low, and training is stuck.", "> Also happening with tf1.12.\r\n> \r\n> Happens randomly, with low prob.\r\n> AWS P3 instances [happened on at least two different instances].\r\n> GPU at 100% \"utilization\", but temp/fan are low, and training is stuck.\r\n\r\nThe same problem here. And the job can not be killed. Any solution or explanation?\r\nBut I found out that the same is with Pytorch, so it is not Tensorflow related ...", "@harryxu-yscz We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. refer [link1](https://stackoverflow.com/questions/50127488/tensorflow-use-model-trained-in-cudnnlstm-in-cpu) , **[link2](https://github.com/tensorflow/tensorflow/issues/1947)** Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17968\">No</a>\n"]}, {"number": 17967, "title": "line 1: 51405 Floating point exception when using keras", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI'm using keras at the server.\r\nWhen I use it, I got this message.\r\nWhat is the problem and how can I solve it.\r\n\r\n### Source code / logs\r\nline 1: 51405 Floating point exception\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 183 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17966, "title": "Fix incorrect rendering in community/documentation.md", "body": "This fix tries to fix the incorrect rendering in community/documentation.md.\r\nThe issue was caused by the escaping of \"```\". As could be seen\r\nin the docs, there are \"```\" (backticks) inside the code block and\r\nthe code block was surrended by \"```\" (backticks) as well. The\r\nresult is that backticks confused the inteprater of markdown and\r\nmessed the documentation. See screenshot:\r\n\r\n<img width=\"896\" alt=\"screen shot 2018-03-23 at 3 37 49 pm\" src=\"https://user-images.githubusercontent.com/6932348/37856160-34061ee8-2eb0-11e8-9bd6-863e965c58a7.png\">\r\n\r\n\r\nThis fix uses indent four spaces to wrap the code blocks so that\r\nbackticks inside could be rendered correctly.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17965, "title": "TensorArray does not work inside `else` clause of `tf.cond`", "body": "\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 'v1.5.0-0-g37aa430', '1.5.0'\r\n- **Python version**:  2.7\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\n    import tensorflow as tf\r\n    def body(v):\r\n        m = tf.constant([v, v])\r\n        ta = tf.TensorArray(dtype=tf.float32, size=1)\r\n        t = ta.write(0, m)\r\n        return t\r\n\r\n    cond = tf.constant(False)\r\n    t = tf.cond(cond, true_fn=lambda : body(2.0), false_fn=lambda : body(3.0))\r\n\r\n    with tf.Session() as ss:\r\n        print(ss.run(t.stack()))\r\n```\r\n\r\nThe code above leads to the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"./array_cond.py\", line 69, in <module>\r\n    print(ss.run(t.stack()))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value\r\n\r\nInterestingly if `False` is replace with `True` it works as expected.", "comments": ["Hi, @iganichev I think I know the correct solution. (credit to my friends.)\r\nYou should move the .stack inside the body:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef body(v):\r\n    m = tf.constant([v,v])\r\n    ta = tf.TensorArray(dtype=tf.float32, size=3, dynamic_size=False)\r\n    t = ta.write(0, m)\r\n    return t.stack()\r\n\r\ncond = tf.constant(False)\r\nt = tf.cond(cond, true_fn=lambda: body(2.0), false_fn=lambda: body(3.0))\r\nwith tf.Session() as ss:\r\n    print(ss.run(t))\r\n```\r\nUnfortunately, I cannot explain why. I will try to to do that and post later. I think I need check the doc of tf.cond\r\nIf anyone know how to explain, please also tell me. Thx!", "Hi, I think here is the problem:\r\nthe true_fn should accept a tensor object (e.g. t.stack() ). However, t is a tensorArray and directly return it will cause problem.\r\nThis issue should be be closed.", "Is there any update?", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17964, "title": "Fix windows gpu build", "body": "\u2026inary.\r\n\r\nPiperOrigin-RevId: 190137278", "comments": []}, {"number": 17963, "title": "Latest nngraph cannot build with Hexagon SDK 3.0", "body": "While building TensorFlow with HVX support, it failed on the step of building **nnlib** with Hexagon SDK 3.0.\r\n\r\nError message is \r\n\r\n> error: invalid output constraint '=v' in asm\r\n\r\nThis issue is caused by the new version of nnlib, downgrade to an older version fixes it.", "comments": ["Nagging Reviewer @case540: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 17962, "title": "Stopping back-prop when using tf.cond", "body": "I am trying to stop back-prop from happening through one branch of a tf.cond statement. I am doing two forward passes through a CNN and trying to only back-prop the one with lower TCE (train_cross_entropy). The code is as follows: \r\n```    \r\ndef f1(): \r\n        with tf.control_dependencies([tf.stop_gradient(train_cross_entropy2)]):\r\n            return train_cross_entropy1\r\ndef f2():\r\n        with tf.control_dependencies([tf.stop_gradient(train_cross_entropy1)]):\r\n            return train_cross_entropy2\r\ntrain_cross_entropy = tf.cond(train_cross_entropy1 < train_cross_entropy2, f1, f2) \r\n```\r\nThe speed of this approach is equivalent to writing: \r\n```\r\ntrain_cross_entropy = tf.add(train_cross_entropy1, train_cross_entropy2)\r\n```\r\nWhereas I would hope that the speed would be more similar to\r\n```\r\ntrain_cross_entropy = tf.add(train_cross_entropy1, tf.stop_gradient(train_cross_entropy2))\r\n```\r\nwhich happens to compute gradients about twice as fast.\r\nI opened up a stack overflow question (https://stackoverflow.com/questions/49436264/proper-use-of-tf-cond-in-cnn/49439351) but I haven't found any help and I am wondering if this is the expected behaviour? \r\nHow do I go about stopping gradients from being calculated backwards through both parts of the graph? \r\nThanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17961, "title": "CUDNN grouped convolutions + depthwise convolution", "body": "This pull request implements grouped convolutions backed by the [CUDNN 7](http://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/Chunk1484074616.html#rel_7) convolution groups feature.\r\n\r\nThe feature is exposed in the DNN support class and the Conv2d ops launchers, but no API / operations are created to instantiate grouped convolutions directly.\r\n\r\nThis pull request also implements dispatching the DepthwiseNativeConv2d (and the corresponding backpropagation operations) to these new grouped convolution kernels. This feature is gated behind a feature flag TF_DEPTHWISE_CONV_USE_GROUPED_CONV which is disabled by default. This increases performance slightly for training Mobilenet in single precision (about 5% on Volta), but also paves the way for float16 training (which is very slow with the current implementation).\r\n\r\nTodo: (need advice / directions):\r\n- add tests for the new DepthwiseNativeConv2d path (probably duplicate existing tests but will also need to set the environment variable - how to do this?)\r\n- the feature flag may not be completely satisfactory at the moment: users compiling on CUDNN 6 and setting the feature flag will face a somewhat cryptic error that \"no algorithms are found\" as the shapes will be incorrect for the conv2d kernels.", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi Wenda. Thank you very much for your contribution, and sorry for the delay. We are actively working on mapping depthwise convolution to cuDNN's grouped convolution, and it should become available in the coming days. Unfortunately, it has been taking longer than expected. Instead of using a feature flag, we implemented this as a separate, 'labeled' kernel.\r\n\r\nI will look into your change and see how it compares to our implementation. Thank you again!", "Thanks for your work! I am looking forward to see an official implementation of Depthwise and Grouped convolutions in tensorflow.", "cuDNN's grouped convolutions to perform depthwise convolution can now be enabled with [graph._kernel_label_map](https://github.com/tensorflow/tensorflow/blob/8cb0558da924e891aa1bb5d79a6c0c846301e4eb/tensorflow/python/framework/ops.py#L3311)({\"DepthwiseConv2dNative\": \"cudnn_grouped_convolution\"}).\r\n\r\nIn the future, we will automatically choose between TF's depthwise convolution and cuDNN's grouped convolution, whichever gives the better performance.", "@chsigg  is there some speed comparasion about DepthwiseConv2Native when used cudnn, how much improved?"]}, {"number": 17960, "title": "Fix tf.svd docs issue", "body": "This fix tries to fix tf.svd docs issue where \"````\" (four backticks)\r\nwere used instead of three backticks (See previous \"```python\" with three backpacks). That caused the docs to render incorrectly.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17959, "title": "Branch 190208069", "body": "", "comments": []}, {"number": 17958, "title": "Shuffling slows down iterator at consumption", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.1 (v1.4.1-9-gc646af1957)\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 6.4.0\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVidia GeForce GTX 1060 6GB\r\n- **Exact command to reproduce**: python3 minimal_shuffle_bug.py\r\n\r\n### Describe the problem\r\nAdding shuffling during the definition of a `tf.data.Dataset` Graph results in a progressive slowdown during the iterations that consume the data, up to the start of a new epoch (when restarting an iteration over the full dataset). I believe this is a bug, or if it's a normal state of things given internal memory usage in the dataset pipeline, then I'd like to request this to be documented in the documentation, e.g., in the [Input Pipeline Performance Guide](https://www.tensorflow.org/versions/master/performance/datasets_performance).\r\n\r\n### Source code / logs\r\nA minimal working code example follows at the end of this post. The code does the same experiment twice, once with and once without dataset shuffling. Each experiment consumes 3 epochs of a dataset, within which one can notice slower and slower iterations (when shuffling is enabled), with a speed increase at each new start of epoch.\r\nMy command-line output is the following:\r\n\r\n> $ python3 minimal_shuffle_bug.py\r\n\r\n>    === START TEST WITH SHUFFLING DISABLED === \r\n2018-03-23 18:15:47.136029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 4.91GiB\r\n2018-03-23 18:15:47.136056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\niteration 99: last 100 iterations took 2.96ms\r\niteration 199: last 100 iterations took 2.49ms\r\niteration 299: last 100 iterations took 2.47ms\r\niteration 399: last 100 iterations took 2.47ms\r\niteration 499: last 100 iterations took 2.33ms\r\niteration 599: last 100 iterations took 2.32ms\r\niteration 699: last 100 iterations took 2.06ms\r\niteration 799: last 100 iterations took 2.06ms\r\niteration 899: last 100 iterations took 2.06ms\r\niteration 999: last 100 iterations took 2.01ms\r\nStart new epoch\r\niteration 1099: last 100 iterations took 2.08ms\r\niteration 1199: last 100 iterations took 2.07ms\r\niteration 1299: last 100 iterations took 2.07ms\r\niteration 1399: last 100 iterations took 2.07ms\r\niteration 1499: last 100 iterations took 2.07ms\r\niteration 1599: last 100 iterations took 2.08ms\r\niteration 1699: last 100 iterations took 2.04ms\r\niteration 1799: last 100 iterations took 2.08ms\r\niteration 1899: last 100 iterations took 2.07ms\r\niteration 1999: last 100 iterations took 2.07ms\r\nStart new epoch\r\niteration 2099: last 100 iterations took 2.06ms\r\niteration 2199: last 100 iterations took 2.07ms\r\niteration 2299: last 100 iterations took 2.07ms\r\niteration 2399: last 100 iterations took 2.01ms\r\niteration 2499: last 100 iterations took 2.01ms\r\niteration 2599: last 100 iterations took 2.01ms\r\niteration 2699: last 100 iterations took 2.05ms\r\niteration 2799: last 100 iterations took 2.01ms\r\niteration 2899: last 100 iterations took 2.01ms\r\niteration 2999: last 100 iterations took 2.01ms\r\n\r\n>    === START TEST WITH SHUFFLING ENABLED === \r\n2018-03-23 18:15:53.605928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\niteration 99: last 100 iterations took 12.77ms\r\niteration 199: last 100 iterations took 15.31ms\r\niteration 299: last 100 iterations took 17.66ms\r\niteration 399: last 100 iterations took 19.23ms\r\niteration 499: last 100 iterations took 20.99ms\r\niteration 599: last 100 iterations took 24.01ms\r\niteration 699: last 100 iterations took 25.48ms\r\niteration 799: last 100 iterations took 27.70ms\r\niteration 899: last 100 iterations took 28.04ms\r\niteration 999: last 100 iterations took 16.38ms\r\nStart new epoch\r\niteration 1099: last 100 iterations took 13.95ms\r\niteration 1199: last 100 iterations took 16.74ms\r\niteration 1299: last 100 iterations took 16.82ms\r\niteration 1399: last 100 iterations took 20.15ms\r\niteration 1499: last 100 iterations took 22.53ms\r\niteration 1599: last 100 iterations took 23.19ms\r\niteration 1699: last 100 iterations took 25.38ms\r\niteration 1799: last 100 iterations took 25.96ms\r\niteration 1899: last 100 iterations took 26.20ms\r\niteration 1999: last 100 iterations took 18.47ms\r\nStart new epoch\r\niteration 2099: last 100 iterations took 12.99ms\r\niteration 2199: last 100 iterations took 14.26ms\r\niteration 2299: last 100 iterations took 17.93ms\r\niteration 2399: last 100 iterations took 18.95ms\r\niteration 2499: last 100 iterations took 20.89ms\r\niteration 2599: last 100 iterations took 23.01ms\r\niteration 2699: last 100 iterations took 25.17ms\r\niteration 2799: last 100 iterations took 27.40ms\r\niteration 2899: last 100 iterations took 26.44ms\r\niteration 2999: last 100 iterations took 15.94ms\r\n\r\nAnd the code to reproduce this experiment:\r\n```import tensorflow as tf\r\nimport time\r\n\r\nnum_data = 5000000\r\nnum_epoch = 3\r\nbatch_size = 5000\r\nnum_iters = num_data*num_epoch/batch_size\r\n\r\ndef test_shuffle(enable_shuffling):\r\n    # Define dataset\r\n    dataset = tf.data.Dataset.range(num_data)\r\n    if (enable_shuffling):\r\n        dataset = dataset.shuffle(num_data)\r\n    dataset = dataset.batch(batch_size)\r\n    iterator = dataset.make_initializable_iterator()\r\n    # Define next element op\r\n    x = iterator.get_next()\r\n\r\n    # Launch session\r\n    sess = tf.InteractiveSession()\r\n    sess.run(iterator.initializer)\r\n\r\n    # Consume iterator and time\r\n    for i in range(int(num_iters)):\r\n        try:\r\n            t1 = time.time()\r\n            res = sess.run(x)\r\n\r\n            if i%100 == 99:\r\n                t2 = time.time()\r\n                print ('iteration {0:d}: last 100 iterations took {1:0.2f}ms'.format(i,1000*(t2-t1)))\r\n        except tf.errors.OutOfRangeError:\r\n            sess.run(iterator.initializer)\r\n            print('Start new epoch')\r\n\r\nprint ('   === START TEST WITH SHUFFLING DISABLED === ')\r\ntest_shuffle(False)\r\nprint()\r\n\r\nprint ('   === START TEST WITH SHUFFLING ENABLED === ')\r\ntest_shuffle(True)\r\nprint()\r\n```\r\n\r\nNote: this bug report follows comments and discussion on bug report https://github.com/tensorflow/tensorflow/issues/11591\r\nI initially experienced this issue with the MNIST dataset (smaller dataset than the example attached, i.e., 60K iterations ;  but with more operations for dataset loading).\r\n\r\nFinal note: I also tried the variant in which I do not capture the `OutOfRangeError` followed by initializing the iterator, but rather include a line `dataset = dataset.repeat(num_epoch)` in the dataset creation Graph. The results are similar.\r\n\r\nThanks a lot.\r\n\r\nKenneth\r\n", "comments": ["@jsimsa I can reproduce this, and there seems to be a small but reproducible slowdown for the last few batches, but I don't see an obvious cause in the code. Perhaps it crept in with the fused-shuffle-and-repeat changes?", "@kvanhoey thank you for reporting this issue.\r\n\r\nCould you please use a more recent version to see if you can reproduce this issue? I have tried reproducing this with a nightly build and do not see nearly as sharp of a difference (~4ms without shuffling and ~8ms with shuffling), the extra 4ms come from the overhead of shuffle when creating a batch of 5000 elements.", "Hi,\r\n\r\nThanks for following up so fast.\r\nI just upgraded to TF 1.6 (all other elements unchanged, same machine and libraries). Results are indeed better (see output below): ~2ms vs. ~4.5ms. But more importantly, iterations occur in constant time: they do not slow down progressively within the epoch anymore.\r\nOnly the very last iteration of each epoch seems slower, which is probably for the reshuffling, thus is reasonable. Testing on my (more complex) MNIST pipeline shows the same constant-time behavior.\r\n\r\nSo IMO: problem solved. Thanks a lot for the help and quick feedback.\r\n\r\n>    === START TEST WITH SHUFFLING DISABLED === \r\n2018-03-27 14:15:01.349378: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-03-27 14:15:01.546811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 4.97GiB\r\n2018-03-27 14:15:01.546838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-03-27 14:15:01.720048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4731 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\niteration 99: last 100 iterations took 1.94ms\r\niteration 199: last 100 iterations took 1.83ms\r\niteration 299: last 100 iterations took 1.93ms\r\niteration 399: last 100 iterations took 1.54ms\r\niteration 499: last 100 iterations took 1.73ms\r\niteration 599: last 100 iterations took 1.67ms\r\niteration 699: last 100 iterations took 1.69ms\r\niteration 799: last 100 iterations took 1.90ms\r\niteration 899: last 100 iterations took 1.68ms\r\niteration 999: last 100 iterations took 1.71ms\r\nStart new epoch\r\niteration 1099: last 100 iterations took 1.68ms\r\niteration 1199: last 100 iterations took 1.68ms\r\niteration 1299: last 100 iterations took 1.69ms\r\niteration 1399: last 100 iterations took 2.45ms\r\niteration 1499: last 100 iterations took 1.92ms\r\niteration 1599: last 100 iterations took 1.91ms\r\niteration 1699: last 100 iterations took 1.92ms\r\niteration 1799: last 100 iterations took 2.12ms\r\niteration 1899: last 100 iterations took 2.12ms\r\niteration 1999: last 100 iterations took 2.33ms\r\nStart new epoch\r\niteration 2099: last 100 iterations took 1.90ms\r\niteration 2199: last 100 iterations took 1.87ms\r\niteration 2299: last 100 iterations took 1.94ms\r\niteration 2399: last 100 iterations took 2.19ms\r\niteration 2499: last 100 iterations took 2.15ms\r\niteration 2599: last 100 iterations took 1.81ms\r\niteration 2699: last 100 iterations took 1.85ms\r\niteration 2799: last 100 iterations took 1.75ms\r\niteration 2899: last 100 iterations took 2.09ms\r\niteration 2999: last 100 iterations took 1.82ms\r\n\r\n>    === START TEST WITH SHUFFLING ENABLED === \r\n2018-03-27 14:15:07.661802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\r\n2018-03-27 14:15:07.662148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4729 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\niteration 99: last 100 iterations took 4.15ms\r\niteration 199: last 100 iterations took 4.53ms\r\niteration 299: last 100 iterations took 4.72ms\r\niteration 399: last 100 iterations took 4.59ms\r\niteration 499: last 100 iterations took 5.01ms\r\niteration 599: last 100 iterations took 5.68ms\r\niteration 699: last 100 iterations took 6.49ms\r\niteration 799: last 100 iterations took 5.92ms\r\niteration 899: last 100 iterations took 5.43ms\r\niteration 999: last 100 iterations took 10.18ms\r\nStart new epoch\r\niteration 1099: last 100 iterations took 4.94ms\r\niteration 1199: last 100 iterations took 4.93ms\r\niteration 1299: last 100 iterations took 4.53ms\r\niteration 1399: last 100 iterations took 5.31ms\r\niteration 1499: last 100 iterations took 4.73ms\r\niteration 1599: last 100 iterations took 5.34ms\r\niteration 1699: last 100 iterations took 4.74ms\r\niteration 1799: last 100 iterations took 4.90ms\r\niteration 1899: last 100 iterations took 4.81ms\r\niteration 1999: last 100 iterations took 11.58ms\r\nStart new epoch\r\niteration 2099: last 100 iterations took 4.64ms\r\niteration 2199: last 100 iterations took 3.89ms\r\niteration 2299: last 100 iterations took 4.05ms\r\niteration 2399: last 100 iterations took 4.15ms\r\niteration 2499: last 100 iterations took 4.28ms\r\niteration 2599: last 100 iterations took 4.45ms\r\niteration 2699: last 100 iterations took 4.69ms\r\niteration 2799: last 100 iterations took 4.83ms\r\niteration 2899: last 100 iterations took 4.86ms\r\niteration 2999: last 100 iterations took 11.96ms\r\n\r\n", "@kvanhoey I am happy to hear that you no longer see the constant slowdown, the slow down on the last iteration is likely due to deallocation of the no longer needed state"]}, {"number": 17957, "title": "InvalidArgumentError when training with weight regularization with anaconda in win 10 (tensorflow-gpu 1.6.0)", "body": "Error message occurs when applying L2 loss weight decay with 1080ti gpu activated\r\nNo Error message occurs when applying weight decay with only cpu running\r\nThe error message always come out if training with all the Optimizer in anaconda, win10, gpu 1080ti, tensorflow 1.6.0. CUDA 9 CUDNN 7.0\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_ADAM_add1CNN.py\", line 89, in <module>\r\n    _ = sess.run([train_step_adam],feed_dict={img_in:X_mb, label_gt:y_gt})\r\n  File \"D:\\ProgramData\\Anaconda4\\envs\\tff\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"D:\\ProgramData\\Anaconda4\\envs\\tff\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"D:\\ProgramData\\Anaconda4\\envs\\tff\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"D:\\ProgramData\\Anaconda4\\envs\\tff\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n```\r\n\r\nHere is the example code that can duplicate the issue:\r\n\r\n```\r\n...\r\ndef optimize_adam(loss, learning_rate, LRdecaysteps, LRdecayrate):\r\n    decay_learning_rate = tf.train.exponential_decay(learning_rate, global_step, LRdecaysteps, LRdecayrate, staircase=True)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n        return tf.train.AdamOptimizer(decay_learning_rate).minimize(loss, global_step=global_step), decay_learning_rate\r\n\r\nregularization_loss = tf.add_n(tf.losses.get_regularization_losses())\r\ntotal_loss = total_loss + regularization_loss\r\ntrain_step_adam,decay_learning_rate = optimize_adam(total_loss,learning_rate,LRdecaysteps,LRdecayrate)\r\n...\r\n```\r\n\r\nit can be run with :\r\nsession_conf = tf.ConfigProto(\r\n    device_count={'CPU' : 1, 'GPU' : 0},\r\n    allow_soft_placement=True,\r\n    log_device_placement=False\r\n)\r\n\r\nBut it will come out this error message with:\r\nsession_conf = tf.ConfigProto(\r\n    device_count={'CPU' : 1, 'GPU' : 1},\r\n    allow_soft_placement=True,\r\n    log_device_placement=False\r\n)", "comments": ["I can run the same code without any error on ubuntu 16.04 with 1080ti cpu activated.", "It works after I rewrite the L2 regularizer of regularizer.py in tensorflow\\contrib\\layers\\python\\layers\r\nIt also means that nn.l2_loss(weights) cause errors under Anaconda & win10 environment with GPU acceleration. \r\n\r\n```\r\n #return standard_ops.multiply(my_scale, nn.l2_loss(weights), name=name)\r\n return standard_ops.multiply(\r\n          my_scale,\r\n          standard_ops.reduce_sum(math_ops.square(weights))/2,\r\n          name=name)\r\n```", "After testing the issue will be closed!!\r\nWill it be update to solve the compatibility ?\r\nAny help ?", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 53 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 68 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @bignamehyp: It has been 84 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is it still an issue for latest TF?", "The problem is solved now."]}, {"number": 17956, "title": "Operation missing on iOS despite being added to tf_op_files.txt and ops_to_register.h", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.3\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.7.0-rc1\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: bazel release 0.11.1-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI believe it's a bug. We're trying to run a TensorFlow model on an iOS app by performing the steps described in the [official](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#optimization) [docs](https://www.tensorflow.org/mobile/prepare_models). I've previously successfully built and ran this graph and its related libraries on Android.\r\n\r\nWe build the libraries like this:\r\n\r\n    tensorflow/contrib/makefile/build_all_ios.sh -a arm64 -g our_inference_graph.pb\r\n\r\nWe make sure the operations in question (`fft_ops.cc`) are built:\r\n\r\n1. `tensorflow/core/framework/ops_to_register.h` contains `|| isequal(op, \"RFFT\")`\r\n\r\n1. `android_extended_ops_group1` in `tensorflow/core/kernels/BUILD` contains `fft_ops.cc`\r\n\r\n1. `tensorflow/contrib/makefile/tf_op_files.txt` contains `tensorflow/core/kernels/fft_ops.cc`\r\n\r\n1. The fft object file is built as `tensorflow/contrib/makefile/gen/obj/fft_ops.o`\r\n\r\nHowever, we get the following error when loading the graph:\r\n\r\n    2018-03-23 13:04:06.618259: E tensorflow/core/framework/op_kernel.cc:1197] OpKernel ('op: \"RFFT\" device_type: \"CPU\"') for unknown op: RFFT\r\n    2018-03-23 13:04:06.618616: E <App path>/TensorFlow/Adapter/TensorFlowUtils.mm:152] Could not create TensorFlow Graph: Not found: Op type not registered 'RFFT' in binary running on Foolish. Make sure the Op and Kernel are registered in the binary running in this process.\r\n    2018-03-23 13:04:06.618827: F <App path>/TensorFlow/Adapter/TensorFlowProcessor.mm:73] Couldn't load model: Not found: Op type not registered 'RFFT' in binary running on Foolish. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\nHere's my question on SO (no repsonse): https://stackoverflow.com/questions/49452489/opkernel-op-rfft-device-type-cpu-for-unknown-op-rfft\r\n\r\nRelated issues: issues #15921 and #5518 suggest adding the operation to `tf_op_files.txt`, which we've done (see above).\r\n\r\n### Source code / logs\r\nHere's how we load the graph: https://gist.github.com/nerthase/22f54c040a195f87b7a9b241536be2a1\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "/CC @petewarden, can you take a look?", "The issue is still here with latest release, 1.7.0.", "Yes, I'm still having this issue. Maybe the tag `avaiting response` is no more valid? @naktinis ", "@petewarden is there anything else we could provide that would make it easier for you to look into this issue?", "Could someone please look into this issue?", "I don't know if this is still an issue for you but I think you also have to add tensorflow/core/ops/spectral_ops.cc to tf_op_files.txt because in this file the RFFT-op is registered", "We're deprecating the older iOS mainline TensorFlow build process in favor of TF Lite, so closing this issue."]}, {"number": 17955, "title": "Results inconsistent between each freeze graph", "body": "### Describe the problem\r\n\r\nI apologize if this is the wrong forum, but it may be a bug regarding certain operations. When freezing a graph and then running it elsewhere (mobile device), the output is of low quality compared to the inference on the server on my semantic segmentation model. It is basically a messy version of what would run on the server. It is executing successfully, but it appears as though something was not initialized prior to freezing, even though the method to load the model between the export script and inference scripts is nearly identical. \r\n\r\nThe exported model can be run on the same images over and over and produce the same results for a given set of images, as expected. \r\n\r\n**Here is the really strange part:**\r\nHowever, each time the model is frozen, using exactly the same script and same checkpoint, it creates a different output for a given set of images.\r\n\r\nI went ahead and posted to stackoverflow in case this is the wrong place.\r\n[https://stackoverflow.com/questions/49454430/tensorflow-results-inconsistent-between-each-freeze-graph](https://stackoverflow.com/questions/49454430/tensorflow-results-inconsistent-between-each-freeze-graph)\r\n\r\n### Source code / logs\r\n\r\n```\r\ndef main():\r\n    args = get_arguments()\r\n    \r\n    if args.dataset == 'cityscapes':\r\n        num_classes = cityscapes_class\r\n    else:\r\n        num_classes = ADE20k_class\r\n\r\n    shape = [320, 320]\r\n\r\n    x = tf.placeholder(dtype=tf.float32, shape=(shape[0], shape[1], 3), name=\"input\")\r\n    img_tf = preprocess(x)\r\n\r\n    model = model_config[args.model]\r\n    net = model({'data': img_tf}, num_classes=num_classes, filter_scale=args.filter_scale)\r\n\r\n    raw_output = net.layers['conv6_cls']\r\n    raw_output_up = tf.image.resize_bilinear(raw_output, size=shape, align_corners=True)\r\n    raw_output_maxed = tf.argmax(raw_output_up, axis=3, name=\"output\")\r\n        \r\n    # Init tf Session\r\n    config = tf.ConfigProto()\r\n    sess = tf.Session(config=config)\r\n    init = tf.global_variables_initializer()\r\n\r\n    sess.run(init)\r\n    \r\n    model_path = model_paths[args.model]\r\n    ckpt = tf.train.get_checkpoint_state(model_path)\r\n    if ckpt and ckpt.model_checkpoint_path:\r\n        input_checkpoint = ckpt.model_checkpoint_path\r\n        loader = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=True)\r\n        load(loader, sess, ckpt.model_checkpoint_path)     \r\n    else:\r\n        print('No checkpoint file found at %s.' % model_path)\r\n        exit()\r\n\r\n    print(\"Loaded Model\")\r\n\r\n    # We retrieve the protobuf graph definition\r\n    graph = tf.get_default_graph()\r\n    input_graph_def = graph.as_graph_def()\r\n\r\n    # We use a built-in TF helper to export variables to constants\r\n    output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess, # The session is used to retrieve the weights\r\n        input_graph_def, # The graph_def is used to retrieve the nodes\r\n        output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n    )\r\n\r\n    # Finally we serialize and dump the output graph to the filesystem\r\n    with tf.gfile.GFile(\"model/output_graph.pb\", \"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n```", "comments": ["I have determined this is my issue, and will update the stackoverflow post above with how to remedy it if anyone else encounters it.", "@joelteply it appears the stackoverflow post doesn't have an explanation - do you remember what your solution was?"]}, {"number": 17954, "title": "Fix the feature shape of maxout output", "body": "This PR is to fix the feature shape of maxout.\r\n\r\nIn tf.contrib.layers.maxout(), when the shape of \"inputs\" is not completely specified, the shape of its output will be completely unknown, such as [None, None, None] in the 3D case.\r\n\r\nSince \"num_units\" has specified the final number of features in the maxout axis, the output can leverage it to set its shape accordingly.", "comments": ["@imsheridan could you take a look at the errors? There are issues like this one:\r\n\r\n```\r\n=====================================================================\r\nERROR: test_fully_connected (__main__.MaxOutTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 4159, in test_fully_connected\r\n    graph = _layers.maxout(graph, num_units=10)\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers.py\", line 3132, in maxout\r\n    shape[self.axis] = self.num_units\r\nNameError: name 'self' is not defined\r\n```", "Sorry for the delay, I'll look into this tomorrow.", "@imsheridan -- are you still working on this?", "It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 49 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17953, "title": "Do not follow control edges in segmenter and in conversion and (#17936)", "body": "gracefully handle some failures\r\n\r\n(cherry picked from commit 5daa95eeeae66b21fc60e08bf0f7c35b3df517f6)\r\n(cherry picked from commit ee87a13583001dd9b19cb5272f85d227ad59297f)\r\n(cherry picked from commit 9a1e6b0e9ca25da050f5a1866235189e6db528ae)\r\n\r\nand squashed", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 17952, "title": "Feature request: in tf.train.MonitoredSession(), add operator that makes it possible to skip certain hooks", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Pro 64-bit (10.0, Build 16299)\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.6.0\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI would like an operator that makes it possible to skip a certain/multiple hooks whilst running a tf.train.MonitoredSession(). An example of how this could look is provided below.\r\n\r\nThe reason why I want this functionality, is that I sometimes want to permit a certain hook(s) from running when calling `mon_sess.run()`. One reason for this can be that the hook generates unnecessary errors when doing this particular `mon_sess.run()` call. Another one can be that I simply don't want the hook's functionality on this particular `mon_sess.run()` call.\r\n\r\n### Source code / logs\r\nExample code of how this operator could look:\r\n\r\n```\r\nwith tf.train.MonitoredTrainingSession(\r\n  hooks=[hook1, hook2, hook3, hook4]\r\n) as mon_sess:\r\n  mon_sess.partial_run(fetches=..., feed_dict=..., skip_hooks=[hook1, hook3])\r\n```\r\n\r\nIs this possible?\r\n", "comments": ["Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We decided not to support partial hook run since it will complicate the usage. \r\nYou may find `MonitoredSession.run_step_fn` useful. It let's you run with or without hooks since you access the raw session."]}, {"number": 17951, "title": "Feature request: Create a function that handles errors for tf.train.SessionRunHook()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Pro 64-bit (10.0, Build 16299)\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.6.0\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI would like an operation for tf.train.SessionRunHook() that executes when an error is thrown from the sess.run() (it should be able to act upon a single error/multiple errors (in a list)/all errors. The reason I would like this, is because there exists use cases where a hook action should be performed only when an error is raised.\r\n\r\nMy current example of this use case is the switch between training and validation loops. I am, in a tf.train.MonitoredTrainingSession() trying to run a training loop and a validation loop that switches between each other. In order to achieve this, I'm using a tf.data.Dataset.iterator that runs in a while loop until a tf.errors.OutOfRangeError is thrown; breaking the while loop and moving on to the next. When this exception is thrown, I would like to perform hook actions, such as saving epoch summaries, print logging on the terminal, etc. My current way of doing this is to write these actions manually in the exception clause, as shown below. It would, however, be cleaner and easier if these actions could be performed in the hooks themselves when an error is thrown.\r\n\r\n### Source code / logs\r\nExample code I am using currently:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom functools import partial\r\n\r\ndef create_reset_metric(metric, scope='reset_metrics', **metric_args):\r\n    with tf.variable_scope(scope) as scope:\r\n        metric_op, update_op = metric(**metric_args)\r\n        vars = tf.contrib.framework.get_variables(\r\n                scope, collection=tf.GraphKeys.LOCAL_VARIABLES\r\n            )\r\n        reset_op = tf.variables_initializer(vars)\r\n    return metric_op, update_op, reset_op\r\n\r\ndataset_train = tf.data.Dataset.range(100)\r\niterator_train = dataset_train.make_initializable_iterator()\r\nnext_elem_train = iterator_train.get_next()\r\nmean_batch_train, mean_update_train, mean_reset_train = create_reset_metric(\r\n                                                            metric=tf.metrics.mean,\r\n                                                            scope='reset_metrics_train',\r\n                                                            values=next_elem_train)\r\nsummary_train = tf.summary.scalar('train_summary', mean_update_train, collections=['train'])\r\n\r\ndataset_test = tf.data.Dataset.range(50)\r\niterator_test = dataset_test.make_initializable_iterator()\r\nnext_elem_test = iterator_test.get_next()\r\nmean_batch_test, mean_update_test, mean_reset_test = create_reset_metric(\r\n                                                            metric=tf.metrics.mean,\r\n                                                            scope='reset_metrics_test',\r\n                                                            values=next_elem_test)\r\nsummary_test = tf.summary.scalar('test_summary', mean_update_test, collections=['test'])\r\n\r\nmerged_train_summary_op = tf.summary.merge_all('train')\r\nmerged_test_summary_op = tf.summary.merge_all('test')\r\n\r\ndef step_fn(fetches, feed_dict, step_context):\r\n    return step_context.session.run(fetches=fetches, feed_dict=feed_dict)\r\n\r\nwith tf.summary.FileWriter('./tmp/train_test_switch') as writer:\r\n    with tf.train.MonitoredTrainingSession() as sess:\r\n        epoch_step = 0\r\n        while not sess.should_stop():\r\n            sess.run_step_fn(partial(step_fn, iterator_train.initializer, {}))\r\n            while True:\r\n                try:\r\n                    summary_train_, _ = sess.run([merged_train_summary_op, next_elem_train])\r\n                except tf.errors.OutOfRangeError:\r\n                    writer.add_summary(summary_train_, epoch_step)\r\n                    sess.run_step_fn(partial(step_fn, mean_reset_train, {}))\r\n                    break\r\n\r\n            sess.run_step_fn(partial(step_fn, iterator_test.initializer, {}))\r\n            while True:\r\n                try:\r\n                    summary_test_, _ = sess.run([merged_test_summary_op, next_elem_test])\r\n                except tf.errors.OutOfRangeError:\r\n                    writer.add_summary(summary_test_, epoch_step)\r\n                    sess.run_step_fn(partial(step_fn, mean_reset_test, {}))\r\n                    break\r\n            print(\"epoch_step:\", epoch_step)\r\n            epoch_step += 1\r\n\r\n```\r\n\r\nExample of how I want the `tf.train.MonitoredTrainingSession()` to look (the beginning is the same as before):\r\n\r\n```\r\n...\r\n\r\nwith tf.summary.FileWriter('./tmp/train_test_switch') as writer:\r\n    error_catching_hook = ErrorCatchingHook(...)\r\n    with tf.train.MonitoredTrainingSession(hooks=[error_catching_hook]) as sess:\r\n        epoch_step = 0\r\n        while not sess.should_stop():\r\n            sess.run_step_fn(partial(step_fn, iterator_train.initializer, {}))\r\n            error_catching_hook.is_training() # Set flag to call correct params when OutOfRangeError\r\n            while True:\r\n                try:\r\n                    summary_train_, _ = sess.run([merged_train_summary_op, next_elem_train])\r\n                except tf.errors.OutOfRangeError:\r\n                    # error_catching_hook calls writer.add_summary(...)\r\n                    # error_catching_hook calls mean_reset_train\r\n                    break\r\n\r\n            sess.run_step_fn(partial(step_fn, iterator_test.initializer, {}))\r\n            error_catching_hook.is_testing() # Set flag to call correct params when OutOfRangeError\r\n            while True:\r\n                try:\r\n                    summary_test_, _ = sess.run([merged_test_summary_op, next_elem_test])\r\n                except tf.errors.OutOfRangeError:\r\n                    # error_catching_hook calls writer.add_summary(...)\r\n                    # error_catching_hook calls mean_reset_test\r\n                    break\r\n            print(\"epoch_step:\", epoch_step)\r\n            epoch_step += 1\r\n```\r\n\r\nI realize that this might not be the best example of a use case for this, but I hope you get my point, and I'm sure there are other use cases for this. \r\n\r\nAfter having read my code above, I also hope that this could provide a solution for others trying to gather and summarize epoch-wise data. A common pattern of summarizing train/test data is, after all, to summarize an epoch average after each epoch. This feels like an unnecessarily painful implementation task today, and I hope that this request of mine can at least bring down the pain a little bit. Hopefully, this can lead to a tf.train.EndOfSetHook() or similar, that is only called when a dataset reaches its end!", "comments": ["@ispirmustafa to comment on feature request feasibility.", "handling errors with hooks may not have that straightforward/clear usage. Instead I would recommend using multiple sessions something like following:\r\n\r\nYou can run some ops at the end of training by using `end` function of `SessionRunHook`\r\n```\r\nwith MonitoredTrainingSession(...) as mon_sess:\r\n  while not mon_sess.should_stop():\r\n    mon_sess.run(train_op)\r\n\r\nwith MonitoredTrainingSession(...) as mon_sess:\r\n  while not mon_sess.should_stop():\r\n    mon_sess.run(eval_op)\r\n```\r\n\r\nPlease let us know this works or not", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "> handling errors with hooks may not have that straightforward/clear usage. Instead I would recommend using multiple sessions something like following:\r\n> \r\n> You can run some ops at the end of training by using `end` function of `SessionRunHook`\r\n> \r\n> ```\r\n> with MonitoredTrainingSession(...) as mon_sess:\r\n>   while not mon_sess.should_stop():\r\n>     mon_sess.run(train_op)\r\n> \r\n> with MonitoredTrainingSession(...) as mon_sess:\r\n>   while not mon_sess.should_stop():\r\n>     mon_sess.run(eval_op)\r\n> ```\r\n> Please let us know this works or not\r\n\r\nHi, \r\nI would like to use multiple sessions in order to evaluate a validation dataset during training. \r\nI created the evaluation monitored session as a nested session of the training monitored session:\r\n\r\n```\r\ntimer = tf.train.SecondOrStepTimer(...)\r\nwith MonitoredTrainingSession(...) as train_session:\r\n    \r\n  train_session.run(train_op)\r\n    \r\n  if timer.should_trigger_for_step(..):\r\n    \r\n    with MonitoredTrainingSession(...) as eval_session:\r\n      eval_session.run(eval_op)\r\n```\r\n\r\nThe two graphs share weights, but because I want to produce different summaries for train and evaluation, each monitored-training-session has its own scaffold object. For the training session I have an init function, but for the evaluation session I don't want the weights to be initialised. If I don't declare an init_fn in the scaffold object, I get this error:\r\n\r\n```\r\n  File \"/Users/adva/anaconda/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py\", line 287, in prepare_session\r\n    init_fn(sess)\r\n  File \"/Users/adva/anaconda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py\", line 163, in <lambda>\r\n    self._init_fn = lambda sess: init_fn(self, sess)\r\nTypeError: callback() takes 1 positional argument but 2 were given\r\n```\r\n\r\nHow can I use multiple sessions this way? \r\n\r\nThank you!\r\nAdva", "Hi @advaza ,\r\nFor your approach you don't need to have two session. You should be able to call eval_op on train_session. That will keep the weights same.", "Thanks @ispirmustafa! I figured it out eventually. The error I got was because I changed the code from slim supervisor to monitored training session, and I had to define the init_fn differently. \r\n\r\nI ended up doing:\r\n\r\n```\r\ntimer = tf.train.SecondOrStepTimer(...)\r\nwith MonitoredTrainingSession(...) as train_session:\r\n  \r\n  while not train_session.should_stop():\r\n  \r\n    _, global_step_value = train_session.run([train_op, global_step])\r\n  \r\n    if timer.should_trigger_for_step(step=global_step_value):\r\n  \r\n      timer.update_last_triggered_step(step=global_step_value)\r\n  \r\n      train_session.run(eval_iterator.initializer) # Initializable Iterator\r\n  \r\n      while True:\r\n        try:\r\n          train_session.run(eval_op)\r\n      \r\n        except tf.errors.OutOfRangeError:\r\n          break\r\n```\r\n\r\n "]}, {"number": 17950, "title": "`tf.keras.model_to_estimator` doesn't work well in evaluating", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nI use docker tensorflow/tensorflow:1.7.0-rc1-devel-gpu-py3\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.7.0-rc1\r\n\r\n- **Python version**: \r\n3.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA9.0\r\n\r\n- **GPU model and memory**:\r\n1080Ti(12GB)\r\n\r\n- **Exact command to reproduce**:\r\nsee my gist below\r\n\r\n### Describe the problem\r\nI used networks in `tf.keras.applications` and `tf.keras.model_to_estimator`. I noticed that training loss gets low but validation loss doesn't when I don't use pretrained model and train from scratch. I doubted overfitting so I tried evalutating on training dataset. And get large validation loss althogh traing loss gets low inspite of same dataset. I think parameters of BatchNormalization are not updated when use `model_to_estimator`. Isn't it a bug?\r\n![loss](https://user-images.githubusercontent.com/22191150/37830935-21c07af8-2ee7-11e8-9bf0-bcd2ad07eebf.png)\r\n\r\n### Source code / logs\r\nhttps://gist.github.com/dhgrs/781eb8bec824c63cc4b626bf04cd4446", "comments": ["Can you please try other builtin networks like resnet50 to see if you can reproduce.", "Thanks for response. I tried resnet50 but got an error and could not run. I also tried Xception but got similar error.\r\nsource code: https://gist.github.com/dhgrs/4552eb5f44c7d9956089b440265f9ab9\r\n\r\nHere is a part of log.(`model_to_estimator_debug.py` is the script's name)\r\n```\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \"model_to_estimator_debug.py\", line 36, in <module>\r\n    resnet_estimator.train(input_fn=train_input_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 352, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 888, in _train_model\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 384, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 795, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 518, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 981, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 986, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 675, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 437, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 214, in finalize\r\n    self._saver.build()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1302, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1339, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 790, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 502, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 449, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 847, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1030, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Key bn2a_branch1/beta not found in checkpoint\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\t [[Node: save/RestoreV2/_301 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_306_save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```", "Nagging Assignee @bignamehyp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "tf 1.8.0: encountered same issue with RestNet50, with or without ImageNet weights.", "Can you please provide detailed description of the problem and the exact steps to reproduce?\r\n", "This should be very easy to reproduce, example:\r\n```\r\nbase_model = applications.ResNet50(classes=len(CLASSES), include_top=False, input_shape=(img_width,img_height,3))\r\n# build a classifier model to put on top of the convolutional model\r\ntop_model = Sequential()\r\ntop_model.add(Flatten(input_shape=base_model.output_shape[1:]))\r\ntop_model.add(Dense(256, activation='relu',W_regularizer=regularizers.l2(weight_decay)))\r\ntop_model.add(Dropout(0.5))\r\ntop_model.add(Dense(len(CLASSES), activation='softmax'))\r\n\r\nmodel = Model(inputs=base_model.input, outputs=top_model(base_model.output))\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=optimizers.SGD(lr=args.lrate, momentum=momentum),\r\n              metrics=['accuracy'])\r\n```\r\nThen feed some data.", "We have similar problem with 1.8.0 on the validation set. Is it really related to batch normalization parameters? ", "Is it related to https://github.com/tensorflow/tensorflow/issues/16455?", "/cc @CasiaFan", "See also https://github.com/keras-team/keras/pull/9965#issuecomment-382857348 /cc @fchollet @yifeif", "we have encountered the same issue both with tf 1.8.0 and master branch", "@bignamehyp Can you remove  stat:awaiting response label?", "This needs to have `stat:awaiting tensorflower` label", "@ewilderj Can we try to target this before the near 1.9 cut? At one week from the cut can we label what could be in?", "+ @drpngx for attention", "Sorry, can you:\r\n\r\n* try with the latest version\r\n* summarize again what the behavior is, and what makes you think it's a bug\r\n", "@drpngx Sorry I switched to another activity I was on that 10 days ago but I had the same behavior of the graph at  https://github.com/tensorflow/tensorflow/issues/17950#issue-308024416. There was a gist at the end of the comment. Something wrong was happening at eval time. Is it something related to https://github.com/tensorflow/tensorflow/issues/16102?", "@drpngx See also https://github.com/tensorflow/tensorflow/issues/16455#issuecomment-373605644. And https://towardsdatascience.com/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73. Is `model_to_estimator` handling this stuffs correctly?", "@fchollet is that a known problem?", "In the neighborhood of this a new issue https://github.com/tensorflow/tensorflow/issues/19643", "Gently ping before 1.9 is out", "https://github.com/tensorflow/tensorflow/issues/19643#issuecomment-394604598", "https://github.com/keras-team/keras/issues/9214", "@drpngx If @fchollet is it too busy I remember that also @yifeif had some work into `model_to_estimator`. Does the converter need to handle `update_ops`?", "I meant in the automatic created model by `model_to_estimator` like https://github.com/tensorflow/tensorflow/issues/16455#issuecomment-373605644", "Is there any relation with https://github.com/tensorflow/tensorflow/issues/19903?", "/cc @martinwicke for https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/Y0BSYDRQ_BU", "@tanzhenyu Another fun problem with model_to_estimator. This one probably easier to fix than the one you're currently looking into.", "I have the feeling that model_to_estimator is not used internally by google teams. And this is one of the causes why it is so exposed to bugs and more or less corner cases.", "You are not wrong. It is new, and definitely not yet as robust as say, copying your Keras code into a model_fn. ", "@martinwicke Can we move it back to contrib? cause it is becoming a source of errors especially for beginners that don't write explicit model_fn. It is a frequent use case: write tf.keras model, use the converter and then go with the train_and_evaluate loops. If not please extend the tests coverage if it is not tested internally by regular use. Also this is one of the main bridge between the tf.keras API and estimators API but tf.Keras bugfix/review resources are lacking and @fchollet is generally very busy  and without substitutes.", "We can't move it back. We will fix it though.\n", "Still got error with latest tensorflow 1.9.0rc1 (rn50 model):\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key bn2a_branch1/beta not found in checkpoint\r\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_INT64, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\t [[Node: save/AssignVariableOp_496/_5172 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_2892_save/AssignVariableOp_496\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op u'save/RestoreV2'\r\n```\r\n\r\n", "@bignamehyp Why have you labeled this as WIP? Is there anybody that is actively working on this?", "Can someone just explain what is the perimeter of this ticket? Is it a recognized bug?", "Yes, this is a real bug, and we're working on it. A solid solution will take more time than I would like though.", "@martinwicke Can you explain the cause of this so that users can workaround with model_fn untill is solved?", "Sorry, I should have elaborated.\r\n\r\nI believe (from the symptoms) that not all updates that are required to be run in the training loop are run by the Estimator returned by `model_to_estimator`. Hence it only affects batchnorm, which is one of the very few layers that need updates.\r\n\r\nThe best workaround IMO is to move the `Model` definition into the `model_fn`. An example of this is here: https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py (this example could be further simplified by running `.compile` on the model instead of defining the optimizer in the `model_fn` directly).", "@martinwicke The example that you are pointing is not a model with BN. \r\nIt is `model_to_estimator` that creates the implicit `model_fn` for the estimator. So is `model_to_estimator` not using `update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)` or something similar when it is creating its automatic `model_fn`? Do we need to use this in our model_fn as a workaround untill it is fixed?", "Yes. The workaround *should* be something like this:\r\n\r\n```\r\n# Batch norm requires update_ops to be added as a train_op dependency.\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n  train_op = optimizer.minimize(loss, tf.train.get_global_step())\r\n```\r\n\r\n@tanzhenyu FYI", "There are currently a couple of issues with model_to_estimator. I have acknowledged and currently working on one of them. This is the next issue in the queue. I will update it ASAP.", "@martinwicke I understand the we have many bugs here. But sometimes a quick workaround instead of waiting 3 months could be useful and it probably not require too much time as fixing it expecially if it has a low internal priority in the stack (that it is not visibile by us with specific labels).\r\n\r\n@ewilderj Generally I really hope that we could improve this process with a fast triage pass and then a bugfix cause we are always in the middle of switching api going from high to low and back also if not required just waiting for a fix (i.e. see also the warm_start issues at https://github.com/tensorflow/tensorflow/issues/20057). This really happen with API that are not used daily in Brain or other Google TF teams (I'am not internal it is just a github behaviour reverse engineering).\r\n\r\n@tanzhenyu Thanks, ping us when you will have an update so that we will switch back from the workaround.", "@bhack I don't completely understand the context, but my recommendation is that you write a description of the general process problem to developers@tensorflow.org so we can collectively talk about it outside of the context of this particular issue. That will make it easier for me to understand and hopefully help. Thank you!", "@ewilderj Done", "Plese notify me if we are not going in target for TF 1.9.\r\nI will open a PR to notify this bug in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/guide/keras.md#estimators section before you will release the final 1.9.", "1.9 has been cut a while ago, no fix for this bug is in it I believe.\r\n\r\nBtw., I agree we could do a better job with labels etc. Communication is hard work. ", "@martinwicke I've explicitely asked for 1.9 as soon as @ewilderj announced the cut out on the google group. See https://github.com/tensorflow/tensorflow/issues/17950#issuecomment-391318178\r\n\r\nGoogle is heavly involved in Kubernetes and there it can handle labels without problems. \r\nSo handling labels in kubernetes style or start to use https://github.com/tensorflow/tensorflow/projects it is a good minimum starting point.\r\n\r\nHave you an internal ETA? Cause @ewilderj already annunced the 1.10 cut out date.", "Cause it is too late for a fix i prepared the advise PR for 1.9 at https://github.com/tensorflow/tensorflow/pull/20437.\r\nPlease support the review before we publish 1.9 release.", "After some investigations, I think the root cause might not be directly related to train ops not created as control dependencies of update ops, because with keras _make_train_function(), both train ops and update ops are included inside the train function. \r\nOn the other hand, I think the problem might be caused by the _clone_and_build_model function. What I tried locally on the function is that 1) if it doesn't take in features & labels, the cloned model is correct with the right update ops, 2) if it does take in features & labels, the cloned model is missing the update ops from batch norm layer.\r\nI'm not sure why the behavior is different. But it looks like quite a subtle bug. I will investigate it next.", "Is not that `_create_ordered_io` alter some naming or scope that will not match in `_make_train_function`?", "Hmm..not really, because what I'm looking at is the clone_model provided by keras. I doubt it's because when clone model is created by passing in input_tensor, the _feed_inputs is lost, making \"updates += self.get_updates_for(self._feed_inputs)\" not adding any update ops inside _make_train_function.\r\nOn the other hand, when clone model is created without input_tensor, it will create a placeholder on its own, thus fulfilling _feed_inputs.\r\nI will update when I have more info.", "@tanzhenyu Do you have a very minimal snippet of code to test with or without input_tensor?", "Yes, please see following, https://gist.github.com/tanzhenyu/22fadcfda66704199a5c5d4edf10c17e", "As I investigate more, here's what I think happens:\r\n1. without input tensor, keras will create an InputLayer with is_placeholder being True: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/input_layer.py#L115\r\n2. with input tensor, keras will create an InputLayer with is_placeholder being False:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/input_layer.py#L119\r\n3. in both scenarios when _init_graph_network is called, depending on if is_placeholder being True or False, the model will or will not add the layer inputs into its _feed_inputs list:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/network.py#L307\r\n4. later when _make_train_function is called to find all training ops and update ops, it will specifically look for conditional updates relevant to the _feed_inputs:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L602\r\n\r\nAnd since _feed_inputs is empty, the update ops from batch norm (or any other layer with updates) will be excluded, i.e., moving mean and moving variance in this case will not be updated.", "Ok it sounds plausible.", "I'm not very familiar with the design choices of \"is_placeholder\", so not quite sure of the right solution. I will see if I can come up with 1) a solution for this, 2) a temporary fix to make this happen from user side.", "This is probably a bug introduced when we made it possible to use tensor inputs directly instead of feeding. Instead of looking through all _feed_inputs to find relevant updates, we have to look through all inputs, whether is_placeholder is true or not. \r\n\r\nMedium term, we have to refactor this to not rely on this brittle double-accounting -- using functions to encode updates should help (cc @alextp)", "Now that 1.9 is released do you think that it will land on master before 1.10 cut on July 12th?\r\n\r\n", "All: We have submitted a code change to fix the batch_norm issue.\r\nHowever, that might not be the sole root cause for the problem identified here. In order to make Nasnet or Resnet to work properly using model_to_estimator, we have to keep deep diving. One apparent reason for this: with batch_norm not updating its moving mean and moving variance, I don't see why estimator should report different training and evaluation results. So it's best we keep this issue open until we fundamentally resolve the problem.\r\n\r\nThat being said, @bhack do you mind updating other github issues that is batch_norm related? I feel with the code change we could close those.", "@tanzhenyu Have you already any suspect on what code path generated the problem?  Is it something specific to fine-tuning? https://github.com/keras-team/keras/pull/9965#issuecomment-382801648", "In the finetune case of  models with batch norm (quite frequent here) I think that we are inerithing all the mess and limits of this \"upstream\" long thread: https://github.com/keras-team/keras/pull/9965#issuecomment-398183440\r\n\r\n@tanzhenyu @martinwicke  What do you think?", "Also from the same thread https://medium.com/@joeyearsley/the-batch-normalisation-in-keras-is-not-broken-358b5e46af54", "I do think that there were other issues introduced specifically by model_to_estimator, and hopefully those (at least the ones related to batch_norm) are resolved by @tanzhenyu's fix. \r\n\r\nAdditionally, we are certainly inheriting the issues mentioned there, and we should fix them. So if there are open issues on TF related to finetuning and batchnorm, we should close them with a fix once we have one.", "When I confirmed this bug [two months ago](https://github.com/tensorflow/tensorflow/issues/17950#issuecomment-389088438) the model had the input tensor and finetuning so on our side we was involved with the `feeds_inputs` issue but also with the batch_norm/fine-tuning.\r\nI don't think that we have a dedicated issue for `tf.keras` cause often it is not clear where people open the tickets (keras or tf.keras?). For sure we could have some impact in https://github.com/tensorflow/tensorflow/issues/19295 other that here.", "@tanzhenyu Have you ran again the original @dhgrs code snippet https://github.com/tensorflow/tensorflow/issues/17950#issue-308024416 at the head of this issue? Is it solved now?\r\nSo that we can isolate if it is just an API problem on how to (default) handle BatchNorm finetuning on layers freeze with Keras/High Level API.", "With latest changes and local testing, I think the issue should have been resolved. I posted my code snippet below. Would you mind check it again before closing this ticket?\r\nnasnet (with batch norm):\r\nhttps://gist.github.com/tanzhenyu/48d705ba49acb0ff166da2db986745ef\r\n\r\n@dhgrs, also note that in your code example mnist are 28 by 28, while nasnet mobile requires at least 32 by 32. I'm not sure how the code works without checking the size, that could be something mysterious; meanwhile rescale it by 1/255 is something I would do either)", "@tanzhenyu I've recovered our old model and it is working fine after the `feed_inputs` fix. \r\nI think that we can close this and handle BN/finetune API issues in https://github.com/tensorflow/tensorflow/issues/19295 https://github.com/tensorflow/tensorflow/issues/19903\r\n\r\nWhat do you think?", "@bhack SGTM. And thanks for the prompt response!\r\nI will take a look at those two when I have time (presumably this week, if not ping me)"]}, {"number": 17949, "title": "incorrect description of num_sampled parameter in tf.nn.nce_loss() function: num_sampled is number of negative examples per 1 positive example (NOT per batch)", "body": "Hi all,\r\n\r\nLooks like parameter `num_sampled` in  `tf.nn.nce_loss` function defines the number of negative examples per a positive example but not per a batch as described in tensorflow documentation (https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss) \r\n\r\n(see the next code)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# `_compute_sampled_logits` is invoked in nce_loss to generate negative sample and calculate logits\r\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\r\n\r\nembedding_size = 10\r\nwords_number = 300\r\nbatch_size = 3\r\nnum_sampled = 3\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    # Input data.\r\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\r\n    train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\r\n\r\n    with tf.device('/cpu:0'):\r\n        embeddings = tf.Variable(\r\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\r\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\r\n        nce_weights = tf.Variable(\r\n                tf.random_uniform([words_number, embedding_size], -4., 4.))\r\n        nce_biases = tf.Variable(tf.zeros([words_number]))\r\n        \r\n    logits, labels = _compute_sampled_logits(\r\n                       weights=nce_weights,\r\n                       biases=nce_biases,\r\n                       inputs=embed,\r\n                       labels=train_labels,\r\n                       num_true=1,\r\n                       num_sampled=num_sampled,\r\n                       num_classes=words_number,\r\n                       remove_accidental_hits = False)\r\n    init = tf.global_variables_initializer()\r\n\r\n\r\nsession = tf.InteractiveSession(graph=graph)\r\ninit.run(session=session)\r\n\r\nbatch_inputs = np.array([0,1,2], dtype=np.int32)\r\nbatch_labels = np.array([[3],[4],[5]], dtype=np.int32)\r\n\r\nfeed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\r\nlogits_val, labels_val = session.run([logits, labels], feed_dict=feed_dict)\r\n\r\nprint (\"logits_val = {}\".format(logits_val))\r\nprint (\"labels_val = {}\".format(labels_val))\r\n\r\n```\r\n\r\nAs a result, `_compute_sampled_logits` function generated `num_sampled` examples per **1 positive example**:\r\n```\r\nlogits_val = [[ -8.18727493   2.02518415  14.18676853   0.51900673]\r\n [ -8.97232056   5.60003376   4.52866602   3.68161726]\r\n [ -0.36226368  -5.84330416  -3.39891291   5.58423615]]\r\nlabels_val = [[ 1.  0.  0.  0.]\r\n [ 1.  0.  0.  0.]\r\n [ 1.  0.  0.  0.]]\r\n```", "comments": ["Maybe it's not a bug but very significant typo in the documentation", "From my limited understanding, I think the documentation is correct. The first logit in your output is computed from all true samples, not just from 1 true sample. The rest 3 logits are for the 3 random samples.", "@denkuzin thank you for pointing this out!\r\n@MarkDaoust would you PTAL? ", "Nagging Assignee @MarkDaoust: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi denkuzin@,\r\n\r\nThanks for the report. Here's a fix that I think clarifies what's actually happening here.\r\n"]}, {"number": 17948, "title": "Fix a minor tuple plus operation", "body": "This PR is to fix a minor bracket format for tuple plus operation.\r\n\r\nAs we can see [_conv definition](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L2167), the input arg \"filter_size\" is an int tuple.\r\n>   #Now the computation.\r\n>   kernel = vs.get_variable(\r\n>       \"kernel\", filter_size + (total_arg_size_depth, num_features), dtype=dtype)\r\n\r\nThus this PR is to fix above \"[]\" with \"()\" since plus operation should be on same data type tuple instead of list.", "comments": ["@imsheridan This change results in the following error in test //tensorflow/contrib/rnn:rnn_cell_test: TypeError: can only concatenate list (not \"tuple\") to list. Can you fix the code ?", "Thanks @benoitsteiner , subbmited another iteration to fix the error you pointed out. Could you please kindly help how to run tests and check tests results myself locally?", "Mind looking at the failing tests?", "@jhseu Fixed the failed test case, could u pls help re-run the gates?", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'll looked into the failure today.", "Nagging Assignee @benoitsteiner: It has been 19 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@imsheridan any luck with this? It's almost ready to merge!", "Oh busy on other stuff recent days, will reopen this PR when I got time to fix the failure tests."]}, {"number": 17947, "title": "Fix typo", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@gunan, can you merge this as for some reason I don't have permission to do it myself ?", "Releasse branches are locked down, only release owners have access to release branches."]}, {"number": 17946, "title": "Formatting issue in tf debugger documentation", "body": "![image](https://user-images.githubusercontent.com/35289454/37821263-29a87d1c-2ea9-11e8-90b5-7f3bb5f42d49.png)\r\nIn the [FAQ section](https://www.tensorflow.org/programmers_guide/debugger#frequently_asked_questions), there seem to be some markdown formatting problems.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@caisq I don't understand the markup details, but it looks like the source has a \"```python\" that's indented rather than flush-left around the place where things go wrong. PTAL. ", "@Conceptron  @cy89 thanks for reporting this issue. I believe this has to do with some subtle differences among the two markdown renderers that we have:\r\n1. The one on GitHub\r\n2. The one on the tensorflow.org website.\r\n\r\nThis file renders correctly on GitHub:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/debugger.md\r\n\r\ncc @MarkDaoust for any possible insight.", "The site source (https://www.tensorflow.org/versions/r1.3/programmers_guide/debugger#debugging_model_training_with_tfdbg) looks like: \r\n\r\n```\r\n<li>The constructors of <code>LocalCLIDebugWrapperSession</code> and <code>LocalCLIDebugHook</code>\r\n   provide a keyword argument, <code>dump_root</code>, to specify the path\r\n   to which tfdbg dumps the debug data. You can use it to let tfdbg dump the\r\n   debug data on a disk with larger free space. For example:</li>\r\n</ul>\r\n<p>``` python\r\n   # For LocalCLIDebugWrapperSession\r\n   sess = tf_debug.LocalCLIDebugWrapperSession(dump_root=\"/with/lots/of/space\")</p>\r\n<p># For LocalCLIDebugHook\r\n   hooks = [tf_debug.LocalCLIDebugHook(dump_root=\"/with/lots/of/space\")]\r\n   <code>``\r\n   Make sure that the directory pointed to by dump_root is empty or nonexistent.\r\n   tfdbg cleans up the dump directories before exiting.\r\n```\r\n\r\nAll other instances of code markdown on the guide are wrapped with `<pre class=\"prettyprint lang-python\"><code>python goes here\r\n</code></pre>`, but this one instance is not.\r\n\r\nTo fix the documentation on the tensorflow.org website, you should just need to replace the appropriate section with:\r\n\r\n```\r\n<li>The constructors of <code>LocalCLIDebugWrapperSession</code> and <code>LocalCLIDebugHook</code>\r\n   provide a keyword argument, <code>dump_root</code>, to specify the path\r\n   to which tfdbg dumps the debug data. You can use it to let tfdbg dump the\r\n   debug data on a disk with larger free space. For example:</li>\r\n</ul>\r\n<pre class=\"prettyprint lang-none\"><code>   # For LocalCLIDebugWrapperSession\r\n   sess = tf_debug.LocalCLIDebugWrapperSession(dump_root=\"/with/lots/of/space\")\r\n\r\n   # For LocalCLIDebugHook\r\n   hooks = [tf_debug.LocalCLIDebugHook(dump_root=\"/with/lots/of/space\")]\r\n</code></pre>\r\n   Make sure that the directory pointed to by dump_root is empty or nonexistent.\r\n   tfdbg cleans up the dump directories before exiting.\r\n```"]}, {"number": 17945, "title": "1.7 compiled version with MKL fails unexpectedly", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04.5 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.7\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.8.4\r\n- **CUDA/cuDNN version**:\r\nNone\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nI successfully built tensorflow with mkl, got it all installed and then tried to run a keras network (one that has run on the pre-built binary version).  It looks like it started hits 800% (8 cores), but then dies with an error.  The logs below show the error is not in keras but in tensorflow/mkl. Source code included, I believe you can run any data through the network.\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nlogs:\r\n\r\n```\r\n./tensorflow/core/util/mkl_util.h:1288] Non-OK-status: Status(error::Code::INVALID_ARGUMENT, \"Unsupported data format\") status: Invalid argument: Unsupported data format \r\n```\r\n\r\nsource code:\r\n\r\n```\r\nimport os\r\nos.environ[\"KMP_BLOCKTIME\"] = str(0)\r\nos.environ[\"KMP_SETTINGS\"] = str(1)\r\nos.environ[\"KMP_AFFINITY\"]= str('granularity=fine,verbose,compact,1,0')\r\n\r\nd = datetime.datetime.utcnow()\r\nunixtime = calendar.timegm(d.utctimetuple())\r\n\r\nwindow_size = 224\r\nnb_input_series = train_X.shape[1]\r\nnb_outputs = 1\r\nbatch_size = 10\r\n\r\nmodel = Sequential((\r\n    Conv1D(2048, 2, activation='relu',input_shape=(window_size, nb_input_series)),\r\n    Conv1D(2048, 2, activation='relu'),\r\n    MaxPooling1D(2),\r\n    Conv1D(2048, 4, activation='relu'),\r\n    Conv1D(2048, 4, activation='relu'),\r\n    MaxPooling1D(2),\r\n    Conv1D(2048, 4, activation='relu'),\r\n    Conv1D(2048, 4, activation='relu'),\r\n    MaxPooling1D(2),\r\n    Conv1D(2048, 4, activation='relu'),\r\n    Conv1D(2048, 4, activation='relu'),\r\n    MaxPooling1D(2),\r\n    Dropout(0.1),\r\n    Dense(4096, activation='relu'),\r\n    Dense(4096, activation='relu'),\r\n#     GlobalAveragePooling1D(),\r\n    Flatten(),\r\n    Dense(nb_outputs, activation='linear')\r\n    ))\r\n\r\n    \r\nmodel.compile(loss='mse', optimizer=Adam(lr=0.0001), metrics=['mae'])\r\n\r\nmodel.summary()\r\n```", "comments": ["We'll take a look at this at Intel too. Were you running a specific model or unit test when you found this?", "I also met this bug when I did a simple CNN network with keras and tensorflow 1.6. The code could run successfully with tensorflow 1.5.", "#MeToo \r\nCompiled TF 1.7 (Ubuntu 16.04) today with mkl, trying to run custom seq2seq with attention made out of tf.contib's seq2seq building blocks, got:\r\n`2018-04-06 21:52:38.076967: F ./tensorflow/core/util/mkl_util.h:1288] Non-OK-status: Status(error::Code::INVALID_ARGUMENT, \"Unsupported data format\") status: Invalid argument: Unsupported data format`\r\nThere is no even message about what OP caused failure. Seq2seq is batch-major if important.", "Same here, installed tf and keras from conda.\r\n\r\n```\r\n_________________________________________________________________\r\n4/6/2018 2:46:52 PMLayer (type)                 Output Shape              Param #\r\n4/6/2018 2:46:52 PM=================================================================\r\n4/6/2018 2:46:52 PMreshape_1 (Reshape)          (None, 16, 5)             0\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PMconv1d_1_1 (Conv1D)          (None, 16, 512)           5632\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PMactivation_1_1 (PReLU)       (None, 16, 512)           8192\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PMmax_pooling_1_1 (MaxPooling1 (None, 8, 512)            0\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PMdropout_1_1 (Dropout)        (None, 8, 512)            0\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PMflatten_1_1 (Flatten)        (None, 4096)              0\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PMdense_1_2 (Dense)            (None, 1)                 4097\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PMactivationn_1_2 (Activation) (None, 1)                 0\r\n4/6/2018 2:46:52 PM=================================================================\r\n4/6/2018 2:46:52 PMTotal params: 17,921\r\n4/6/2018 2:46:52 PMTrainable params: 17,921\r\n4/6/2018 2:46:52 PMNon-trainable params: 0\r\n4/6/2018 2:46:52 PM_________________________________________________________________\r\n4/6/2018 2:46:52 PM2018-04-06 21:46:52.907397: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n4/6/2018 2:46:52 PMoptimizer.train_evaluate_score_ts_model_cv: INFO     batch_size: 64, max_epochs: 4096\r\n4/6/2018 2:46:52 PMoptimizer.train_evaluate_score_ts_model_cv: INFO     Starting model fit..\r\n4/6/2018 2:46:53 PMtf_logging.vlog: WARNING  Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\r\n4/6/2018 2:46:53 PM2018-04-06 21:46:53.904191: F ./tensorflow/core/util/mkl_util.h:1287] Non-OK-status: Status(error::Code::INVALID_ARGUMENT, \"Unsupported data format\") status: Invalid argument: Unsupported data format\r\n```", "@claynerobison can you tell how we can help you debug this? Commands for verbose logging, etc?", "@jostheim  While I was trying to reproduce your issue, I found the source code you pasted is not executable (incomplete code). Could you please help paste the entire source code? Thank you!\r\n@Luonic @jaikumarm Would also appreciate if you can paste your code for us to reproduce. Thank you! \r\n", "I can't give out the data and the rest of the code is in a giant jupyter notebook that I frankly don't have time to clean, so I'll let someone else give out their code.", "@jostheim  Ok, no problem. Looks like it will be great if @cczhong11 could provide the simple CNN network as mentioned in the comment above (quote again here) \"I also met this bug when I did a simple CNN network with keras and tensorflow 1.6. The code could run successfully with tensorflow 1.5.\" ", "https://gist.github.com/cczhong11/b3ee85d81d9d07e6c47f4a1ec49c66e8\r\n\r\nThis code could get this problem. The dataset could get from the url in the code. I listed all the packages installed by Anaconda. It is a interesting bug. I could only reproduce this when I first installed tensorflow. You could use the following code to install packages on a new virtual machine.\r\n```\r\nwget http://repo.continuum.io/miniconda/Miniconda3-4.4.10-Linux-x86_64.sh -O ~/miniconda.sh\r\nbash ~/miniconda.sh -b -p $HOME/miniconda\r\nexport PATH=\"$HOME/miniconda/bin:$PATH\"\r\nsource $HOME/miniconda/bin/activate\r\nconda install tensorflow keras -y\r\n```\r\n\r\n\r\nAfter I tried to install the 1.5.0 tensorflow and installed 1.7.0 tensorflow again, I could not reproduce this error.", "my original code is part of a much larger project so am not able to post it. however am able to replicate the issue on my azure ubuntu vm with the code @cczhong11 posted.\r\n\r\n\r\n```\r\npython cnn_tf.py\r\n/home/jay/anaconda3/envs/keras_tf_py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\nWARNING:tensorflow:From /home/jay/anaconda3/envs/keras_tf_py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`NHWC` for data_format is deprecated, use `NWC` instead\r\nEpoch 1/3\r\n2018-05-01 21:31:05.834727: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-05-01 21:31:06.974462: F ./tensorflow/core/util/mkl_util.h:1287] Non-OK-status: Status(error::Code::INVALID_ARGUMENT, \"Unsupported data format\") status: Invalid argument: Unsupported data format\r\nAborted (core dumped)\r\n```\r\n\r\n```\r\ncat .keras/keras.json\r\n{\r\n    \"image_dim_ordering\": \"tf\",\r\n    \"epsilon\": 1e-07,\r\n    \"floatx\": \"float32\",\r\n    \"backend\": \"tensorflow\"\r\n}\r\n```\r\n\r\n```\r\nconda list\r\n# packages in environment at /home/jay/anaconda3/envs/keras_tf_py36:\r\n#\r\nabsl-py                   0.1.13                   py36_0\r\naiopg                     0.13.2                    <pip>\r\nalabaster                 0.7.10           py36h306e16b_0\r\nanaconda                  custom           py36hbbc8b67_0\r\nanaconda-client           1.6.14                   py36_0\r\nanaconda-navigator        1.8.3                    py36_0\r\nanaconda-project          0.8.2            py36h44fb852_0\r\nasn1crypto                0.24.0                   py36_0\r\nastor                     0.6.2                    py36_0\r\nastroid                   1.6.3                    py36_0\r\nastropy                   3.0.1            py36h3010b51_1\r\nasyncio                   3.4.3                     <pip>\r\nattrs                     17.4.0                   py36_0\r\nbabel                     2.5.3                    py36_0\r\nbackcall                  0.1.0                    py36_0\r\nbackports                 1.0              py36hfa02d7e_1\r\nbackports.shutil_get_terminal_size 1.0.0            py36hfea85ff_2\r\nbcrypt                    3.1.4            py36h621fe67_0\r\nbeautifulsoup4            4.6.0            py36h49b8c8c_1\r\nbinutils_impl_linux-64    2.28.1               had2808c_3\r\nbinutils_linux-64         7.2.0                        26\r\nbitarray                  0.8.1            py36h14c3975_1\r\nbkcharts                  0.2              py36h735825a_0\r\nblaze                     0.11.3           py36h4e06776_0\r\nbleach                    1.5.0                    py36_0\r\nbokeh                     0.12.15                  py36_0\r\nboto                      2.48.0           py36h6e4cd66_1\r\nbottleneck                1.2.1            py36haac1ea0_0\r\nbzip2                     1.0.6                h9a117a8_4\r\nca-certificates           2018.03.07                    0\r\ncairo                     1.14.12              h7636065_2\r\ncertifi                   2018.1.18                py36_0\r\ncffi                      1.11.5           py36h9745a5d_0\r\nchardet                   3.0.4            py36h0f667ec_1\r\nclick                     6.7              py36h5253387_0\r\ncloudpickle               0.5.2                    py36_1\r\nclyent                    1.2.2            py36h7e57e65_1\r\ncolorama                  0.3.9            py36h489cec4_0\r\ncontextlib2               0.5.5            py36h6c84a62_0\r\ncryptography              2.2.2            py36h14c3975_0\r\ncudatoolkit               9.0                  h13b8566_0\r\ncudnn                     7.1.2                 cuda9.0_0\r\ncurl                      7.59.0               h84994c4_0\r\ncycler                    0.10.0           py36h93f1223_0\r\ncython                    0.28.2           py36h14c3975_0\r\ncytoolz                   0.9.0.1          py36h14c3975_0\r\ndask                      0.17.2                   py36_0\r\ndask-core                 0.17.2                   py36_0\r\ndatashape                 0.5.4            py36h3ad6b5c_0\r\ndbus                      1.13.2               h714fa37_1\r\ndecorator                 4.3.0                    py36_0\r\ndistributed               1.21.6                   py36_0\r\ndocutils                  0.14             py36hb0f60f5_0\r\ndominate                  2.3.1                     <pip>\r\nentrypoints               0.2.3            py36h1aec115_2\r\net_xmlfile                1.0.1            py36hd6bccc3_0\r\nexpat                     2.2.5                he0dffb1_0\r\nfastcache                 1.0.2            py36h14c3975_2\r\nfilelock                  3.0.4                    py36_0\r\nflask                     0.12.2           py36hb24657c_0\r\nFlask-Bcrypt              0.7.1                     <pip>\r\nFlask-Bootstrap           3.3.7.1                   <pip>\r\nflask-cors                3.0.3            py36h2d857d3_0\r\nFlask-JSON                0.3.2                     <pip>\r\nFlask-Login               0.4.1                     <pip>\r\nflask-nav                 0.6                       <pip>\r\nFlask-SQLAlchemy          2.3.2                     <pip>\r\nFlask-WTF                 0.14.2                    <pip>\r\nfontconfig                2.12.6               h49f89f6_0\r\nfreetype                  2.8                  hab7d2ae_1\r\nfuture                    0.16.0                   py36_1\r\ngast                      0.2.0                    py36_0\r\ngcc_impl_linux-64         7.2.0                habb00fd_3\r\ngcc_linux-64              7.2.0                        26\r\nget_terminal_size         1.0.0                haa9412d_0\r\ngevent                    1.2.2            py36h2fe25dc_0\r\nglib                      2.56.1               h000015b_0\r\nglob2                     0.6              py36he249c77_0\r\ngmp                       6.1.2                h6c8ec71_1\r\ngmpy2                     2.0.8            py36hc8893dd_2\r\ngraphite2                 1.3.11               hf63cedd_1\r\ngreenlet                  0.4.13           py36h14c3975_0\r\ngrpcio                    1.10.0           py36hf484d3e_0\r\ngst-plugins-base          1.14.0               hbbd80ab_1\r\ngstreamer                 1.14.0               hb453b48_1\r\ngxx_impl_linux-64         7.2.0                hdf63c60_3\r\ngxx_linux-64              7.2.0                        26\r\nh5py                      2.7.1            py36h3585f63_0\r\nharfbuzz                  1.7.6                h5f0a787_1\r\nhdf5                      1.10.1               h9caa474_1\r\nheapdict                  1.0.0                    py36_2\r\nhtml5lib                  0.9999999                py36_0\r\nhyperopt                  0.1.1            py36h256ac0b_0    jaikumarm\r\nib-insync                 0.9.12                    <pip>\r\nibapi                     9.73.6                    <pip>\r\nIbPy2                     0.8.0                     <pip>\r\nicu                       58.2                 h9c2bf20_1\r\nidna                      2.6              py36h82fb2a8_1\r\nimageio                   2.3.0                    py36_0\r\nimagesize                 1.0.0                    py36_0\r\nintel-openmp              2018.0.0                      8\r\nipykernel                 4.8.2                    py36_0\r\nipython                   6.3.1                    py36_0\r\nipython_genutils          0.2.0            py36hb52b0d5_0\r\nipywidgets                7.2.1                    py36_0\r\nisort                     4.3.4                    py36_0\r\nitsdangerous              0.24             py36h93cc618_1\r\njbig                      2.1                  hdba287a_0\r\njdcal                     1.4                      py36_0\r\njedi                      0.12.0                   py36_0\r\njinja2                    2.10             py36ha16c418_0\r\njpeg                      9b                   h024ee3a_2\r\njsonschema                2.6.0            py36h006f8b5_0\r\njupyter                   1.0.0                    py36_4\r\njupyter_client            5.2.3                    py36_0\r\njupyter_console           5.2.0            py36he59e554_1\r\njupyter_core              4.4.0            py36h7c827e3_0\r\njupyterlab                0.31.12                  py36_0\r\njupyterlab_launcher       0.10.5                   py36_0\r\nkeras                     2.1.5                    py36_0\r\nkiwisolver                1.0.1            py36h764f252_0\r\nkrb5                      1.16                 h3f6afd0_6\r\nlazy-object-proxy         1.3.1            py36h10fcdad_0\r\nlibcurl                   7.59.0               h1ad7b7a_0\r\nlibedit                   3.1                  heed3624_0\r\nlibffi                    3.2.1                hd88cf55_4\r\nlibgcc-ng                 7.2.0                hdf63c60_3\r\nlibgfortran-ng            7.2.0                hdf63c60_3\r\nlibgpuarray               0.7.5                h14c3975_0\r\nlibpng                    1.6.34               hb9fc6fc_0\r\nlibpq                     10.3                 h1ad7b7a_0\r\nlibprotobuf               3.5.2                h6f1eeef_0\r\nlibsodium                 1.0.16               h1bed415_0\r\nlibssh2                   1.8.0                h9cfc8f7_4\r\nlibstdcxx-ng              7.2.0                hdf63c60_3\r\nlibtiff                   4.0.9                h28f6b97_0\r\nlibtool                   2.4.6                h544aabb_3\r\nlibxcb                    1.13                 h1bed415_1\r\nlibxml2                   2.9.8                hf84eae3_0\r\nlibxslt                   1.1.32               h1312cb7_0\r\nllvmlite                  0.22.0           py36ha27ea49_0\r\nlocket                    0.2.0            py36h787c0ad_1\r\nlxml                      4.2.1            py36h23eabaa_0\r\nlzo                       2.10                 h49e0be7_2\r\nmako                      1.0.7            py36h0727276_0\r\nmarkdown                  2.6.11                   py36_0\r\nmarkupsafe                1.0              py36hd9260cd_1\r\nmatplotlib                2.2.2            py36h0e671d2_1\r\nmccabe                    0.6.1            py36h5ad9710_1\r\nmistune                   0.8.3                    py36_0\r\nmkl                       2018.0.2                      1\r\nmkl-service               1.1.2            py36h17a0993_4\r\nmkl_fft                   1.0.1            py36h3010b51_0\r\nmkl_random                1.0.1            py36h629b387_0\r\nmore-itertools            4.1.0                    py36_0\r\nmpc                       1.0.3                hec55b23_5\r\nmpfr                      3.1.5                h11a74b3_2\r\nmpmath                    1.0.0            py36hfeacd6b_2\r\nmsgpack-python            0.5.6            py36h6bb024c_0\r\nmultipledispatch          0.5.0                    py36_0\r\nnavigator-updater         0.1.0            py36h14770f7_0\r\nnbconvert                 5.3.1            py36hb41ffb7_0\r\nnbformat                  4.4.0            py36h31c9010_0\r\nncurses                   6.0                  h9df7e31_2\r\nnetworkx                  2.1                      py36_0\r\nnltk                      3.2.5            py36h7532b22_0\r\nnose                      1.3.7            py36hcdf7029_2\r\nnotebook                  5.4.1                    py36_0\r\nnumba                     0.37.0          np114py36h5fcf951_0\r\nnumexpr                   2.6.4            py36hc4a3f9a_0\r\nnumpy                     1.14.2           py36hdbf6ddf_1\r\nnumpydoc                  0.8.0                    py36_0\r\nodo                       0.5.1            py36h90ed295_0\r\nolefile                   0.45.1                   py36_0\r\nopenpyxl                  2.5.2                    py36_0\r\nopenssl                   1.0.2o               h20670df_0\r\npackaging                 17.1                     py36_0\r\npandas                    0.22.0           py36hf484d3e_0\r\npandoc                    1.19.2.1             hea2e7c5_1\r\npandocfilters             1.4.2            py36ha6701b7_1\r\npango                     1.41.0               hd475d92_0\r\nparamiko                  2.4.1                    py36_0\r\nparso                     0.2.0                    py36_0\r\npartd                     0.3.8            py36h36fd896_0\r\npatchelf                  0.9                  hf79760b_2\r\npath.py                   11.0.1                   py36_0\r\npathlib2                  2.3.0            py36h49efa8e_0\r\npatsy                     0.5.0                    py36_0\r\npcre                      8.42                 h439df22_0\r\npep8                      1.7.1                    py36_0\r\npexpect                   4.5.0                    py36_0\r\npickleshare               0.7.4            py36h63277f8_0\r\npillow                    5.1.0            py36h3deb7b8_0\r\npip                       10.0.0                    <pip>\r\npip                       9.0.3                    py36_0\r\npixman                    0.34.0               hceecf20_3\r\npkginfo                   1.4.2                    py36_1\r\npluggy                    0.6.0            py36hb689045_0\r\nply                       3.11                     py36_0\r\nprompt_toolkit            1.0.15           py36h17d85b1_0\r\nprotobuf                  3.5.2            py36hf484d3e_0\r\npsutil                    5.4.5            py36h14c3975_0\r\npsycopg2                  2.7.4            py36hb7f436b_0\r\nptyprocess                0.5.2            py36h69acd42_0\r\npy                        1.5.3                    py36_0\r\npyasn1                    0.4.2            py36h4d1db45_0\r\npycodestyle               2.4.0                    py36_0\r\npycosat                   0.6.3            py36h0a5515d_0\r\npycparser                 2.18             py36hf9f622e_1\r\npycrypto                  2.6.1            py36h14c3975_7\r\npycurl                    7.43.0.1         py36hb7f436b_0\r\npyflakes                  1.6.0            py36h7bd6a15_0\r\npygments                  2.2.0            py36h0d3125c_0\r\npygpu                     0.7.5            py36h14c3975_0\r\npyiqfeed                  0.9                       <pip>\r\npylint                    1.8.4                    py36_0\r\npymongo                   3.4.0                    py36_0\r\npynacl                    1.2.1            py36h14c3975_0\r\npyodbc                    4.0.23           py36hf484d3e_0\r\npyopenssl                 17.5.0           py36h20ba746_0\r\npyparsing                 2.2.0            py36hee85983_1\r\npyqt                      5.9.2            py36h751905a_0\r\npysocks                   1.6.8                    py36_0\r\npytables                  3.4.2            py36h3b5282a_2\r\npytest                    3.5.0                    py36_0\r\npytest-arraydiff          0.2                      py36_0\r\npytest-astropy            0.2.1                    py36_0\r\npytest-doctestplus        0.1.2                    py36_0\r\npytest-openfiles          0.2.0                    py36_0\r\npytest-remotedata         0.2.0                    py36_0\r\npython                    3.6.5                hc3d631a_0\r\npython-dateutil           2.7.2                    py36_0\r\npytz                      2018.4                   py36_0\r\npywavelets                0.5.2            py36he602eb0_0\r\npyyaml                    3.12             py36hafb9ca4_1\r\npyzmq                     17.0.0           py36h14c3975_0\r\nqt                        5.9.5                h7e424d6_0\r\nqtawesome                 0.4.4            py36h609ed8c_0\r\nqtconsole                 4.3.1            py36h8f73b5b_0\r\nqtpy                      1.4.0                    py36_0\r\nreadline                  7.0                  ha6073c6_4\r\nrequests                  2.18.4           py36he2e5f8d_1\r\nrope                      0.10.7           py36h147e2ec_0\r\nruamel_yaml               0.15.35          py36h14c3975_1\r\nschedule                  0.5.0                     <pip>\r\nscikit-image              0.13.1           py36h14c3975_1\r\nscikit-learn              0.19.1           py36h7aa7ec6_0\r\nscipy                     1.0.1            py36hfc37229_0\r\nseaborn                   0.8.1            py36hfad7ec4_0\r\nsend2trash                1.5.0                    py36_0\r\nsetuptools                39.0.1                   py36_0\r\nsimplegeneric             0.8.1                    py36_2\r\nsingledispatch            3.4.0.3          py36h7a266c3_0\r\nsip                       4.19.8           py36hf484d3e_0\r\nsix                       1.11.0           py36h372c433_1\r\nsnowballstemmer           1.2.1            py36h6febd40_0\r\nsortedcollections         0.6.1                    py36_0\r\nsortedcontainers          1.5.9                    py36_0\r\nsphinx                    1.7.2                    py36_0\r\nsphinxcontrib             1.0              py36h6d0f590_1\r\nsphinxcontrib-websupport  1.0.1            py36hb5cb234_1\r\nspyder                    3.2.8                    py36_0\r\nsqlalchemy                1.2.6            py36h14c3975_0\r\nsqlite                    3.22.0               h1bed415_0\r\nstatsmodels               0.8.0            py36h8533d0b_0\r\nsympy                     1.1.1            py36hc6d1c1c_0\r\ntblib                     1.3.2            py36h34cf8b6_0\r\ntensorboard               1.6.0            py36hf484d3e_0\r\ntensorflow                1.6.0                         0\r\ntensorflow-base           1.6.0            py36hff88cb2_0\r\ntensorflow-tensorboard    1.5.1            py36hf484d3e_0\r\ntermcolor                 1.1.0                    py36_1\r\nterminado                 0.8.1                    py36_1\r\ntestpath                  0.3.1            py36h8cadb63_0\r\ntheano                    1.0.1            py36h6bb024c_0\r\ntk                        8.6.7                hc745277_3\r\ntoolz                     0.9.0                    py36_0\r\ntornado                   5.0.1                    py36_1\r\ntraitlets                 4.3.2            py36h674d592_0\r\ntyping                    3.6.4                    py36_0\r\nunicodecsv                0.14.1           py36ha668878_0\r\nunixodbc                  2.3.6                h1bed415_0\r\nurllib3                   1.22             py36hbe7ace6_0\r\nvisitor                   0.1.3                     <pip>\r\nwcwidth                   0.1.7            py36hdf4376a_0\r\nwebencodings              0.5.1            py36h800622e_1\r\nwerkzeug                  0.14.1                   py36_0\r\nwheel                     0.31.0                   py36_0\r\nwidgetsnbextension        3.2.1                    py36_0\r\nwrapt                     1.10.11          py36h28b7045_0\r\nWTForms                   2.1                       <pip>\r\nxlrd                      1.1.0            py36h1db9f0c_1\r\nxlsxwriter                1.0.4                    py36_0\r\nxlwt                      1.3.0            py36h7b00a1f_0\r\nxz                        5.2.3                h55aa19d_2\r\nyaml                      0.1.7                had09818_2\r\nzeromq                    4.2.5                h439df22_0\r\nzict                      0.1.3            py36h3a3bf81_0\r\nzlib                      1.2.11               ha838bed_2\r\n```\r\n", "Thank you @cczhong11 and @jaikumarm for providing the code. With these I would be able to work on reproducing the issue - I will report the progress here. Thank you for your patience!", "Thanks for opening this issue. I meet the same error message, with Tensorflow 1.7 (GPU) and keras 2. ", "I was seeing this same crash and wondered if it's related to the warning you see:\r\n```\r\nWARNING:tensorflow:From ~/anaconda/envs/data/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`NHWC` for data_format is deprecated, use `NWC` instead\r\n```\r\nFollowing that issue #17842 I installed tf-nightly and the crash goes away - I can successfully fit the model. Potentially this could just be a case of other dependencies switching too - I tried to use conda packages as much as common, but it's another data point if that helps!", "Thanks @jostheim for reporting and @cczhong11 and @jaikumarm for providing code for me to reproduce the issue. Please know that I have reproduced the errors reported in here. \r\nThe reason is most likely due to Conv1D ops that are not well supported by Intel MKL-DNN execution path. And it confirms with commonality of the failures associated with this thread: all were related to conv1D. While we are working on enabling Conv1D with Intel MKL-DNN, please use the Eigen only TF wheel for now). ", "Sorry for the delay. Still working on root-causing this issue. Please do not close yet. Thank you!", "@wei-v-wang any progress on your side? \r\nI see a similar error:\r\n\r\n2018-06-29 19:04:36.268561: F ./tensorflow/core/util/mkl_util.h:1288] Non-OK-status: Status(error::Code::INVALID_ARGUMENT, \"Unsupported data format\") status: Invalid argument: Unsupported data format", "Hi @mlazarew,  yes TF v1.8 still has the issue (latest tagged release as of now). But I have got some good news: this has been fixed internally, using @cczhong11 's cnn code above.  \r\n**We are upstreaming the changes, hopefully the fix can appear in v1.9 or v1.10 the latest.** \r\n\r\nBelow shows the log as a FYI:\r\nUsing TensorFlow backend.\r\nEpoch 1/3\r\n2018-06-29 13:41:45.164183: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2018-06-29 13:41:45.183727: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n - 55s - loss: 0.4636 - acc: 0.7518\r\nEpoch 2/3\r\n - 55s - loss: 0.1737 - acc: 0.9376\r\nEpoch 3/3\r\n - 53s - loss: 0.0580 - acc: 0.9832\r\nAccuracy: 86.28%\r\n\r\n\r\n", "Any progress here?", "Same problem here with Keras 2.2.x and Tensorflow 1.9.0, definitely due to the Conv1D layer.", "thanks @michetonu for the update. I will check v1.10 and also our internal version and see where the fix is going to land, I am guessing v1.11. ", "By removing mkl and everything that depends on it (should be handled by conda) and installing nomkl instead, I could prevent the error from occuring. I am not sure about any performance consequences since my dataset is rather small.\r\nSee here: https://github.com/conda/conda/issues/2032\r\nWorked for me.", "@carlaucho If you are running on CPU, we definitely would recommend taking advantage of MKLDNN. However, it seems the fix for this Conv1D issue is still internal and hopefully will make it to TF v1.11.0 ", "Is there any possibility that I can train my network and leave this issue aside? Like using an old version? Thanks a lot~\r\n\r\nMy computer: Macbook pro, CPU Intel Core i5\r\nKeras: 2.2.2\r\ntensorflow: 1.9.0", "I have the same problem:\r\nMy computer: Macbook pro, CPU Intel Core i5\r\nKeras: 2.2.2\r\ntensorflow: 1.9.0", "I was facing the same issue. Once I upgraded the tensorflow to 1.10, everything is working fine.", "Same problem with latest version:\r\n- Windows 10, Intel Core i5-6200U\r\n- tensorflow 1.10.0-mkl_py36hb361250_0 (using Miniconda3)\r\n- keras 2.2.2-0\r\n- python 3.6.6\r\n\r\n", "Update: the fix to this issue has been merged to master branch of github.com/tensorflow/tensorflow. \r\nE.g. I have personally verified this public tensorflow commit id to work at least: \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/36568a821a13379b79d74586ff20bdd3e0da102b  \r\n(sorry I cannot point out which exactly commit fixed this bug, but the above should already contain the fix). \r\n\r\nUsing TensorFlow backend.\r\nEpoch 1/3\r\n2018-09-18 15:13:11.177778: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\r\n2018-09-18 15:13:11.236624: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n - 62s - loss: 0.4231 - acc: 0.7823\r\nEpoch 2/3\r\n - 61s - loss: 0.1421 - acc: 0.9503\r\nEpoch 3/3\r\n - 60s - loss: 0.0408 - acc: 0.9895\r\nAccuracy: 88.38%\r\n\r\n\r\nThanks @cczhong11 once again for providing code for me to easily track the bug status. Enjoy the fix and thanks everyone for reporting! ", "@Chaoste  I suggest using latest TF commit, or wait till Anaconda releases TF v1.11 release. As the fix happened between v1.10 and v1.11. \r\n@Salomefu , @FoxerLee , @michetonu, @jostheim , @Luonic , @jaikumarm , @wenouyang, @barnybug, @mlazarew  please upgrade TF to latest  commit version if possible. Thanks for waiting for trying Intel optimized tensorflow (aka. TensorFlow MKL-DNN). \r\n\r\n@tasnim07 thanks for reporting a working version has come out!", "Nagging Assignee @rmlarsen: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe this issue is fixed. Suggest closing this issue. \r\nPlease re-open/comment if the errors are still present. ", "Closing as suggested. Feel free to reopen if the issue persists. Thanks all for the contribution!"]}, {"number": 17944, "title": "App getting crashed while using USB Camera", "body": "OS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow installed from: Git cloned\r\nTensorFlow version: N/A\r\nBazel version: Build label: 0.11.1\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: x86 8GB RAM\r\nExact command to reproduce: bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config android --cpu=x86_64 --fat_apk_cpu=x86_64\r\n\r\nI think i am getting camera id null. is it because tensorflow uses camera2? if yes then how do i fix this issue?\r\n\r\n```\r\n03-23 05:21:14.646  2282  2282 W /system/bin/hwservicemanager: getTransport: Cannot find entry android.hardware.configstore@1.0::ISurfaceFlingerConfigs/default in either framework or device manifest.\r\n03-23 05:21:14.647 19595 19613 I com.example.android.tflitecamerademo: android::hardware::configstore::V1_0::ISurfaceFlingerConfigs::hasWideColorDisplay retrieved: 0\r\n03-23 05:21:14.647 19595 19613 I OpenGLRenderer: Initialized EGL, version 1.4\r\n03-23 05:21:14.647 19595 19613 D OpenGLRenderer: Swap behavior 2\r\n03-23 05:21:14.647 19595 19595 I RenderThread: type=1400 audit(0.0:310): avc: denied { map } for path=2F69393135202864656C6574656429 dev=\"tmpfs\" ino=918453 scontext=u:r:untrusted_app_25:s0:c512,c768 tcontext=u:object_r:untrusted_app_25_tmpfs:s0:c512,c768 tclass=file permissive=1\r\n03-23 05:21:14.651 19595 19595 I CameraManagerGlobal: Connecting to camera service\r\n03-23 05:21:14.652 19595 19595 D AndroidRuntime: Shutting down VM\r\n--------- beginning of crash\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime: FATAL EXCEPTION: main\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19595\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime: java.lang.IllegalArgumentException: cameraId was null\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.hardware.camera2.CameraManager.openCameraForUid(CameraManager.java:457)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.hardware.camera2.CameraManager.openCamera(CameraManager.java:433)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at com.example.android.tflitecamerademo.Camera2BasicFragment.openCamera(Camera2BasicFragment.java:479)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at com.example.android.tflitecamerademo.Camera2BasicFragment.access$000(Camera2BasicFragment.java:69)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at com.example.android.tflitecamerademo.Camera2BasicFragment$1.onSurfaceTextureAvailable(Camera2BasicFragment.java:102)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.TextureView.getHardwareLayer(TextureView.java:390)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.TextureView.draw(TextureView.java:339)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18142)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18133)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:19195)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18142)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18133)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18133)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:19195)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18142)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18133)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18133)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:18920)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.drawChild(ViewGroup.java:4236)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4022)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.draw(View.java:19195)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at com.android.internal.policy.DecorView.draw(DecorView.java:788)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.View.updateDisplayListIfDirty(View.java:18142)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:669)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:675)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:783)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewRootImpl.draw(ViewRootImpl.java:2992)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2806)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2359)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1392)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6752)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:911)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.Choreographer.doCallbacks(Choreographer.java:723)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.Choreographer.doFrame(Choreographer.java:658)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:897)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:790)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.os.Looper.loop(Looper.java:164)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at android.app.ActivityThread.main(ActivityThread.java:6494)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at java.lang.reflect.Method.invoke(Native Method)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:438)\r\n03-23 05:21:14.652 19595 19595 E AndroidRuntime:        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:807)\r\n03-23 05:21:14.653 15691 16933 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity\r\n03-23 05:21:14.654 15691 16933 W ActivityManager:   Force finishing activity com.android.settings/.Settings\r\n03-23 05:21:14.655 15691 16933 I ActivityManager: Killing 16856:com.google.android.partnersetup/u0a23 (adj 906): empty #17\r\n03-23 05:21:14.655 15691 15712 W system_server: kill(-16856, 9) failed: No such process\r\n03-23 05:21:14.656  2503  2503 D CRASHLOG: sdcard_allowed : Current crashlog mode is NOMINAL MODE - SDCard storage disabled.\r\n03-23 05:21:14.659 15691 18236 I ActivityManager: START u0 {act=android.intent.action.MAIN cat=[android.intent.category.HOME] flg=0x10000100 cmp=com.android.support.car.lenspicker/.LensPickerTrampolineActivity} from uid 0\r\n03-23 05:21:14.660 19595 19595 I Process : Sending signal. PID: 19595 SIG: 9\r\n03-23 05:21:14.674 15691 15758 W InputDispatcher: channel '656c0d6 com.example.android.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity (server)' ~ Consumer closed input channel or an error occurred.  events=0x9\r\n03-23 05:21:14.674 15691 15758 E InputDispatcher: channel '656c0d6 com.example.android.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity (server)' ~ Channel is unrecoverably broken and will be disposed!\r\n03-23 05:21:14.674 15691 16917 I ActivityManager: Process com.example.android.tflitecamerademo (pid 19595) has died: vis  +99TOP\r\n03-23 05:21:14.674 15691 16480 I WindowManager: WIN DEATH: Window{656c0d6 u0 com.example.android.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}\r\n03-23 05:21:14.674 15691 16480 W InputDispatcher: Attempted to unregister already unregistered input channel '656c0d6 com.example.android.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity (server)'\r\n03-23 05:21:14.676 15691 15717 W ActivityManager: setHasOverlayUi called on unknown pid: 19595\r\n03-23 05:21:14.681 15691 18187 I ActivityManager: START u0 {act=android.intent.action.MAIN cat=[android.intent.category.LAUNCHER] flg=0x10000000 pkg=com.android.settings cmp=com.android.settings/.Settings} from uid 10048\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution: Ubuntu 14.04\r\nTensorFlow installed from: Git cloned\r\nTensorFlow version: N/A\r\nBazel version: Build label: 0.11.1\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory:  x86 8GB RAM\r\nExact command to reproduce: bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config android --cpu=x86_64 --fat_apk_cpu=x86_64", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @miaout17: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @miaout17: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @miaout17: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @miaout17: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @miaout17: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this still an issue ?", "Nagging Assignee @miaout17: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17943, "title": "MKLDNN: conv2d forward DNN primitive reuse enhancement", "body": "This enhancement enables MKL primitive reuse for conv2d forward operation to reduce the overload of \r\nrepetitive creations of primitives. It \r\n\r\n- Improves MKL op based training performance \r\n- Improves MKL op based inference performance, especially for small batch size including BS=1 case", "comments": ["@gzmkl I recommend reading through https://google.github.io/styleguide/cppguide.html.", "Hi Rasmus,\r\n\r\nI have done as much as I can based on valuable suggestions from you and your team.\r\n\r\nThe MKL DNN reuse thing is basically rewriting the original integration code. So there are lots of code changes\r\nby multiple peoples, some of whom did the TF contribution first-time.\r\n\r\nI will make sure your suggestion will be reflected in near future PRs (primitive reuse for conv bwd, batchnort, relu etc).\r\n\r\nBest Regards,\r\nGZ\r\n\r\nFrom: Rasmus Munk Larsen [mailto:notifications@github.com]\r\nSent: Tuesday, April 17, 2018 6:21 PM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Zhuang, Guozhong <guozhong.zhuang@intel.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] MKLDNN: conv2d forward DNN primitive reuse enhancement (#17943)\r\n\r\n\r\n@rmlarsen requested changes on this pull request.\r\n\r\n________________________________\r\n\r\nIn tensorflow/core/kernels/mkl_conv_ops.cc<https://github.com/tensorflow/tensorflow/pull/17943#discussion_r182284013>:\r\n\r\n> +\r\n\r\n+template <typename T>\r\n\r\n+class Conv2DFwd : public DnnOp {\r\n\r\n+ public:\r\n\r\n+  explicit Conv2DFwd(const ConvFwdDimensions& convFwdDims) {\r\n\r\n+    fwd_stream_.reset(new stream(stream::kind::eager));\r\n\r\n+    // create conv primitive\r\n\r\n+    if (conv_fwd_ == nullptr) {\r\n\r\n+      Setup(convFwdDims);\r\n\r\n+    }\r\n\r\n+  }\r\n\r\n+\r\n\r\n+  ~Conv2DFwd() {}\r\n\r\n+\r\n\r\n+  // Convolution forward execute with bias\r\n\r\n+  void Execute(T* src, T* w, T* b, T* dst) {\r\n\r\nCan you make these parameter names more readable as well?\r\n\r\n________________________________\r\n\r\nIn tensorflow/core/kernels/mkl_conv_ops.cc<https://github.com/tensorflow/tensorflow/pull/17943#discussion_r182284042>:\r\n\r\n> +    filter_mem_->set_data_handle(static_cast<void*>(w));\r\n\r\n+    bias_mem_->set_data_handle(static_cast<void*>(b));\r\n\r\n+    dst_mem_->set_data_handle(static_cast<void*>(dst));\r\n\r\n+    fwd_stream_->submit(fwd_primitives_);\r\n\r\n+\r\n\r\n+    // after exec, set data handle back\r\n\r\n+    src_mem_->set_data_handle(DummyData);\r\n\r\n+    filter_mem_->set_data_handle(DummyData);\r\n\r\n+    bias_mem_->set_data_handle(DummyData);\r\n\r\n+    dst_mem_->set_data_handle(DummyData);\r\n\r\n+\r\n\r\n+    return;\r\n\r\n+  }\r\n\r\n+\r\n\r\n+  // Convolution forward execute without bias\r\n\r\n+  void Execute(T* src, T* w, T* dst) {\r\n\r\neschew abbreviation\r\n\r\n________________________________\r\n\r\nIn tensorflow/core/kernels/mkl_conv_ops.cc<https://github.com/tensorflow/tensorflow/pull/17943#discussion_r182284106>:\r\n\r\n> +  }\r\n\r\n+\r\n\r\n+  // MKLDNN memory\r\n\r\n+  std::shared_ptr<mkldnn::memory> src_mem_;\r\n\r\n+  std::shared_ptr<mkldnn::memory> filter_mem_;\r\n\r\n+  std::shared_ptr<mkldnn::memory> bias_mem_;\r\n\r\n+  std::shared_ptr<mkldnn::memory> dst_mem_;\r\n\r\n+\r\n\r\n+  std::shared_ptr<mkldnn::stream> fwd_stream_;\r\n\r\n+  std::vector<mkldnn::primitive> fwd_primitives_;\r\n\r\n+\r\n\r\n+  // desc & prmitive desc\r\n\r\n+  std::shared_ptr<mkldnn::convolution_forward::desc> fwd_desc_;\r\n\r\n+\r\n\r\n+  // memory desc\r\n\r\n+  std::shared_ptr<mkldnn::memory::desc> src_md_;\r\n\r\ns/md/mem_desc/g ?\r\n\r\n________________________________\r\n\r\nIn tensorflow/core/kernels/mkl_conv_ops.cc<https://github.com/tensorflow/tensorflow/pull/17943#discussion_r182284191>:\r\n\r\n> -\r\n\r\n-          auto conv_prim_desc = convolution_forward::primitive_desc(conv_desc,\r\n\r\n-                                                                  cpu_engine);\r\n\r\n-          AllocateOutputTensor(context, conv_prim_desc, output_dims_mkl_order,\r\n\r\n-                               tf_fmt, &output_tensor);\r\n\r\n-          // Set data handle for output.\r\n\r\n-          output.SetUsrMemDataHandle(output_tensor);\r\n\r\n-\r\n\r\n-          Tensor* filter_out_tensor = nullptr;\r\n\r\n-          AllocateFilterOutputTensor(context, conv_prim_desc,\r\n\r\n-                TFShapeToMklDnnDims(filter_tf_shape),\r\n\r\n-                &filter_out_tensor);\r\n\r\n-          PrepareAndExecuteNet(conv_prim_desc, &src, &filter,\r\n\r\n-                              nullptr, &output, filter_out_tensor);\r\n\r\n+        ConvFwdDimensions convFwdDims(src_dims, filter_dims, NONE_DIMS,\r\n\r\n+            dst_dims_mkl_order, strides, dilations, padding_l, padding_r);\r\n\r\ns/l/left/ s/r/right/\r\n\r\n________________________________\r\n\r\nIn tensorflow/core/kernels/mkl_conv_ops.cc<https://github.com/tensorflow/tensorflow/pull/17943#discussion_r182284229>:\r\n\r\n>        }\r\n\r\n-    } catch (mkldnn::error& e) {\r\n\r\n+\r\n\r\n+      // allocate output tensors output_tensor and filter_out_tensor\r\n\r\n+      std::shared_ptr<mkldnn::convolution_forward::primitive_desc>\r\n\r\n+      conv_fwd_pd = conv2d_fwd->fwd_pd_;\r\n\r\nWhat is \"pd\"? write out.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/17943#pullrequestreview-113049566>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Ab3J2-AwHimWA_0vICHfNgGBY4X4nZLjks5tppTvgaJpZM4S4K7m>.\r\n", "@gzmkl Thank you for your patience. ", "Rasmus,\r\n\r\nThank you very much for all the suggestions and helps. Woe will track the one \u201cto-be-refactored\u201d item in mind and conform to the code style\r\nIn next PRs.\r\n\r\nThanks\r\nGZ\r\n\r\n\r\n\r\nFrom: Rasmus Munk Larsen [mailto:notifications@github.com]\r\nSent: Friday, April 20, 2018 10:40 AM\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Zhuang, Guozhong <guozhong.zhuang@intel.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] MKLDNN: conv2d forward DNN primitive reuse enhancement (#17943)\r\n\r\n\r\n@rmlarsen approved this pull request.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/17943#pullrequestreview-114065247>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Ab3J23vX6GqyLgjfNLoPPnpDsvEkVRDvks5tqh12gaJpZM4S4K7m>.\r\n"]}]