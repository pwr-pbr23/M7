[{"number": 41714, "title": "Add benchmarks", "body": "- Benchmarks for `_benchmark_defun_matmul_with_signature`\r\n- Add CPU benchmark for `_benchmark_defun_args_matmul`, analogous to GPU benchmark in line 686. Docstring copied directly; connection to b/156187905 not verified", "comments": ["@jonathanchu33  Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 41713, "title": "Allocate temp", "body": "Extended C API with TF_AllocateTemp. Added TF_AllocatorAttributes to tf_tensor.h to enable allocation of tensors on host. \r\n@annarev @bmzhao ", "comments": ["@dnguyen28061 can you please resolve conflicts ?", "> There is one more candidate for moving into c_api macros: TF_Bool.\r\n\r\nGood idea. I will make sure to do that when in StreamExecutor change."]}, {"number": 41712, "title": "Conv1DTranspose Dilation support - Might be a bug, IDK.", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose]()\r\n\r\n## Description of issue (what needs changing):\r\nConv1DTranspose - Dilation - Does not inform uses that dilation doesn't work for any value  of `Dilation>1` because it isn't implemented yet. \r\n### Clear description\r\nCurrently documentation says:\r\n_\"an integer, specifying the dilation rate to use for dilated convolution. Currently, specifying a dilation_rate value != 1 is incompatible with specifying a stride value != 1.\"_\r\n\r\nThis may not be implemented yet in the newest of nightly build, but with my tf-nightly==2.5.0dev20200629 build this didn't work. I fear updating to new nightly builds in case in breaks my research code which relies on nightly builds until Conv1DTranspose is released in a supported build.\r\n```\r\nInvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.\r\n\t [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\\Users\\[username]\\Desktop\\libs_python\\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nThis is with stride == 1.\r\n### Correct links\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py\r\n\r\n### Parameters defined\r\n\r\nYes, setting my dilation to 1 gets rid of the issue.\r\n\r\n### Returns defined\r\n\r\nNot necessary. (I'm not sure if you are asking if I have define returns in my code or if my code returns a defined value, or if the documentation claims to return something)\r\n\r\n### Raises listed and defined\r\n```\r\nInvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.\r\n\t [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\\Users\\[username]\\Desktop\\libs_python\\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\n### Usage example\r\n\r\nNightly build, so no.\r\n\r\n### Request visuals, if applicable\r\n No.\r\n\r\n### Submit a pull request?\r\nI will not be doing so.\r\n\r\n\r\n### Test Code\r\n\r\nNote 1: This is built with tf-nightly==2.5.0dev20200626 which was removed from the PyPi archive for unknown reasons.\r\n\r\nNote 2: model.fit must be called for the error to occur. Simpy constructing and compiling the network was not enough to reproduce the error.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as krs\r\nimport numpy as np\r\n\r\ntrain_data = np.random.uniform(-1,1,(20,20))\r\n\r\ninputs = krs.Input((20,1))\r\n\r\nx = inputs\r\n\r\nx = krs.layers.Conv1D(1,3,strides = 1,padding='same',dilation_rate=2,activation='relu')(x)\r\nx = krs.layers.Flatten()(x)\r\nx = krs.layers.Dense(10,activation='relu')(x)\r\nx = krs.layers.Dense(2,activation='relu')(x)\r\nx = krs.layers.Dense(10,activation='relu')(x)\r\nx = krs.layers.Dense(20,activation='relu')(x)\r\nx = krs.layers.Reshape(target_shape=(20,1))(x)\r\nx = krs.layers.Conv1DTranspose(1,3,strides=1,dilation_rate=2,padding='same',activation='relu',output_padding=0)(x)\r\noutput = krs.layers.Flatten()(x)\r\n\r\nmodel = krs.Model(inputs,output,name='test')\r\n\r\nmodel.compile(optimizer='adam',loss='MSE')\r\n\r\nmodel.summary()\r\n\r\nmodel.fit(train_data,train_data)\r\n```", "comments": ["@EnderWiggin14 \r\nCould you please update the issue template, we do not find any details.\r\nPlease provide with simple indented stand alone code such that we can replicate the issue faced or if possible please provide a colab gist with error faced.", "Well, this was originally labeled as a Documentation issue so that \"Details\" section wasn't part of the standard template. I will try to get a short example added to the issue.\r\n\r\nUpdate: I have added code that reproduces the error on my system.", "I ran the code on nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/c7953f7c83b7c403adadec092931076f/untitled284.ipynb) and on [2.3.0rc0 the dilation support error is seen as reported](https://colab.research.google.com/gist/Saduf2019/79a85b1424a878f63d7fa31b369c5f68/untitled296.ipynb). ", "It is very odd that the TF nightly build used in the GIST (tf-nightly==2.4.0dev20200724) doesn't have Conv1DTranspose() support. I have another system that's running a slightly older tf-nightly build (tf-nightly==2.3.0dev20200622) which was still available 2-3 weeks ago but appears to now be deleted. This is one of the two builds that I actively use as well, and it supports the Conv1DTranspose() method, but also lacks the dilation support.", "I can repro this with latest nightly build. See [gist](https://colab.research.google.com/gist/ymodak/a230402addcdda55e1cb87f7c1447ab7/untitled284.ipynb)\r\nWhen `strides = 1` and `dilation_rate = 1` in the `tf.keras.layers.Conv1DTranspose` layer the code executes successfully.\r\nHowever for dilation_rate > 1 it fails.\r\nPerhaps the documentation can be updated for the `dilation_rate` parameter. Thanks!", "Docs are updated now.  See commit [75801da](https://github.com/tensorflow/tensorflow/commit/75801da4cd321aabbf79e78da1e5de1a10ba4c2a#diff-aa6c341a4b212afc57b49be73e689dc2)\r\nThanks!", "Thanks! Is there any news as to when the dilation_rate > 1 will be supported?"]}, {"number": 41711, "title": "Allocate temp", "body": "", "comments": []}, {"number": 41710, "title": "`--config=c++17` option is incompatible with GCC build", "body": "**System information**\r\n- OS Platform and Distribution **Ubuntu Linux 18.04 LTS**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version: **master** (`e2204b27`)\r\n- Python version: **3.8.3**\r\n- Bazel version (if compiling from source): **3.1.0**\r\n- GCC/Compiler version (if compiling from source): **7.5.0-3ubuntu1~18.04**\r\n\r\n**Describe the problem**\r\n\r\nThe `--config=c++17` build configuration option adds compiler flags which are incompatible with GCC.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n./configure (Accept defaults)\r\n```\r\n\r\n```\r\nbazel build --config=c++17 //tensorflow:tensorflow_cc\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n```\r\nERROR: /home/andrew/.cache/bazel/_bazel_andrew/05f0c1fc0fd5d93d6530b840ab375f68/external/com_github_grpc_grpc/BUILD:487:1: C++ compilation of rule '@com_github_grpc_grpc//:gpr_base' failed (Exit 1)\r\ngcc: error: unrecognized command line option '-stdlib=libc++'\r\n```\r\n\r\nThe `c++17` configuration option adds the `-stdlib=libc++` flag to the C++ compiler, but this is a Clang flag, not GCC.\r\n\r\n", "comments": ["@planetmarshall \r\n\r\nCan you please try with C++11 and C++14 and see if the error still persists.Please, refer #39617, #23561 and see if it helps you.Thanks!", "Thanks for the response. I can build with C++11 and C++14. The issue is the incompatible compiler flag - I note @golden0080 made the same observation in #23561", "@planetmarshall \r\n\r\nPlease, close this thread if your issue was resolved.Thanks!", "Can you send a PR to remove the `libc++` setup? We now have a different config option for that.", "> @planetmarshall\r\n> \r\n> Please, close this thread if your issue was resolved.Thanks!\r\n\r\nWell, it's not really resolved. The bazel file contains a configuration option that can't possibly work. Perhaps it should be removed?", "That's why I suggested sending a PR to remove the flag.", "Made #42158 to try removing the flag. If that works, we can use `--config=c++17_gcc` to compile as if `--config=c++17` was passed in but without linking `libc++`. If you want, can you patch in the PR and test too?\r\n\r\nUnfortunately, we might not be able to change the existing config as there might be people already using the old config.", "#42158 landed, I think we can close this now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41710\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41710\">No</a>\n"]}, {"number": 41709, "title": "Linking cross compiled  libtensorlowlite.so for aarch64 causes reference errors: undefined reference to `fcntl64@GLIBC_2.28'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18\r\n- TensorFlow installed from (source or binary): pip\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- GCC/Compiler version (if compiling from source):  aarch64-linux-gnu-g++\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to cross compile a binary to make use of a cross compiled libtensorflowlite.so library. However upon compilation I get the error\r\n```\r\n/home/aarch64/libtensorflowlite.so: undefined reference to `fcntl64@GLIBC_2.28'\r\n```\r\n\r\nI've followed the exact same steps using the native compiler (x86_64) and had no issues linking. \r\n\r\nHere is how I build from source:\r\n```\r\nbazel build --config=elinux_aarch64 --config=monolithic --cxxopt=--std=c++14 --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so\r\n```\r\n\r\nOnce I have my shared library, I link it to my inference code via:\r\n```\r\naarch64-linux-gnu-g++ -std=c++11 hello_world.cpp -I/home/tensorflow/ -I/home/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include   -L/home/aarch64 -ltensorflowlite -lm -lpthread\r\n```\r\n\r\nHowever the above error occurs.\r\nI'm wondering if its due to the steps in which I'm building from source. Specifically the `--config=elinux_aarch64` flag that I add for this cross-compilation case, and didn't have when I compiled natively. Otherwise is there a way to build that missing symbol into the library statically in compilation? Is this a tensorflow build issue? \r\n\r\nFYI: my system itself is running `Ubuntu GLIBC 2.27-3ubuntu1.2`.", "comments": ["This is the toolchain of elinux_aarch64\r\nhttps://github.com/tensorflow/tensorflow/tree/master/third_party/toolchains/embedded/arm-linux\r\nYou'd better update glibc of the target system to 2.28 or higher.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41709\">No</a>\n"]}, {"number": 41708, "title": "Backport rel/ format to r2.2 branch", "body": "These files are moved from the existing branch files.", "comments": []}, {"number": 41707, "title": "Backport rel/ build layout to r2.1", "body": "Files copied from their current state on the 2.1 branch.", "comments": ["@Dynmi unfortunately, your approval has no merging power", "> @Dynmi unfortunately, your approval has no merging power\r\n\r\nOh, I want to work with tensorflow, how can I join you?", "@Dynmi: For contributing, see https://www.tensorflow.org/community/contribute", "> @Dynmi: For contributing, see https://www.tensorflow.org/community/contribute\n\nI see. Thanks. But my intention is to be one of members of tensorflow repository\ud83d\ude2c. How should I join you?", "Keep contributing and at one point you will be given approval powers.\r\n\r\nOr join the team at Google :)", "> Keep contributing and at one point you will be given approval powers.\r\n> \r\n> Or join the team at Google :)\r\n\r\nThanks for your reply.\r\nI will keep contributing at my best. Of course, if I have the opportunity, I will definitely apply to join the team at Google!  :) \r\n"]}, {"number": 41706, "title": "Backport new rel/ structure to r2.0 branch", "body": "These scripts are from the branch, not cloned from master.", "comments": []}, {"number": 41705, "title": "Remove extra libtensorflow.sh commands", "body": "r1.15 does not have any libtensorflow builds in it -- I accidentally copied\nthese.", "comments": []}, {"number": 41704, "title": "tf.ones produces zeros on GPU in an unclear scenario", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1, 7\r\n- GPU model and memory: 2080Ti 11GB + 1070 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI was using my GPUs for another task, with near full memory taken. \r\nWhen using aside ipython, `tf.ones(())` (with any shape/dtype, except tf.int32 working as intented) produced 0s (like tf.zeros). I repeated this multiple times, in multiple instance of ipython. When setting running on CPU, tf.ones correctly produced 1s. After clearing the GPU memory, the issue disappeared, and now I'm unable to reproduce (for instance by filling my GPUs memories).\r\n\r\n**Describe the expected behavior**\r\nOutput 1s, or an error\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nCannot provide one, I'm not able to reproduce again\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mathieuorhan \r\nCan you please share simple indented stand alone code such that we could replicate the issue faced or if possible share a colab gist with the issue reported.", "@Saduf2019 \r\nI could not reproduce the issue", "@mathieuorhan \r\nIn that case we will have to move this issue to closed status, please feel free to reopen or create new issue in case you find a stand alone code to replicate.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41704\">No</a>\n"]}, {"number": 41703, "title": "Tensorflow C abruptly hangs at absl::synchronization_internal::Waiter::Wait(absl::synchronization_internal::KernelTimeout)", "body": "Hi All,\r\n\r\nI'm using Tensorflow 1.15 C libraries for running Parallel Image Processing Computations in my cluster. The cluster consists of 8 worker nodes, each running 40 computation threads, where each of them makes use of tensorflow c libraries to process the incoming data.\r\n\r\nAll works fine, when suddenly the entire cluster gets abruptly hanged. On getting the GDB from my nodes, I'm seeing additional threads (apart from my pre-configured setup of 40 per node), which I'm assuming to be created by Tensorflow. The GDB shows that the threads have been abruptly blocked at **absl::synchronization_internal::Waiter::Wait(absl::synchronization_internal::KernelTimeout)** \r\n\r\nOn viewing the system-level logs, I also see one of the nodes having kernel bug log stating\r\n\r\n[598428.945633] BUG: kernel NULL pointer dereference, address: \r\n0000000000000038\r\n  ...\r\n  [598428.945749] Workqueue: cifsoplockd cifs_oplock_break [cifs]\r\n  [598428.945793] RIP: 0010:smb2_push_mandatory_locks+0xd6/0x5a0 [cifs]\r\n  ...\r\n  [598428.945834] Call Trace:\r\n  [598428.945870]  ? cifs_revalidate_mapping+0x45/0x90 [cifs]\r\n  [598428.945901]  cifs_oplock_break+0x13d/0x450 [cifs]\r\n  [598428.945909]  process_one_work+0x1db/0x380\r\n  [598428.945914]  worker_thread+0x4d/0x400\r\n  [598428.945921]  kthread+0x104/0x140\r\n  [598428.945925]  ? process_one_work+0x380/0x380\r\n  [598428.945931]  ? kthread_park+0x80/0x80\r\n  [598428.945937]  ret_from_fork+0x35/0x40\r\n\r\nAfter restarting the cluster, everything goes back to the way it was. But still, this issue keeps on occuring at random.\r\nCan someone help me to understand and resolve this issue? Thanks.\r\n\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SLES 12\r\n- TensorFlow installed from (source or binary): Downloaded C Binaries from official website\r\n- TensorFlow version (use command below): 1.15\r\n- GCC/Compiler version (if compiling from source): 6.4.0\r\n\r\n**Logs** \r\nSample GDB Thread Call Stack\r\n\r\nLine 222195: [23] Thread 98 (Thread 0x7ef1b0fb1700 (LWP 36711)):\r\n\tLine 222196: [23] #0  0x00007efc2bc9a9f9 in syscall () from /lib64/libc.so.6\r\n\tLine 222197: [23] No symbol table info available.\r\n\tLine 222198: [23] #1  0x00007efc365aadc2 in absl::synchronization_internal::Waiter::Wait(absl::synchronization_internal::KernelTimeout) () from /usr/local/lib/libtensorflow.so\r\n\tLine 222203: [24]         __local = {<std::__shared_ptr<std::thread::_Impl_base, (__gnu_cxx::_Lock_policy)2>> = {_M_ptr = <optimized out>, _M_refcount = {_M_pi = 0x7f46400a7a50}}, <No data fields>}[23] No symbol table info available.\r\n\tLine 222204: [23] #2  0x00007efc365aad02 in AbslInternalPerThreadSemWait () from /usr/local/lib/libtensorflow.so\r\n\tLine 222209: [23] No symbol table info available.\r\n\tLine 222210: [23] #3  0x00007efc365ac2fd in absl::Mutex::Block(absl::base_internal::PerThreadSynch*) () from /usr/local/lib/libtensorflow.so\r\n\tLine 222211: [23] No symbol table info available.\r\n\tLine 222212: [23] #4  0x00007efc365ad1ed in absl::Mutex::AwaitCommon(absl::Condition const&, absl::synchronization_internal::KernelTimeout) () from /usr/local/lib/libtensorflow.so\r\n\tLine 222219: [23] No symbol table info available.\r\n\tLine 222220: [23] #5  0x00007efc365ad26d in absl::Mutex::Await(absl::Condition const&) () from /usr/local/lib/libtensorflow.so\r\n\tLine 222223: [23] No symbol table info available.\r\n\tLine 222224: [23] #6  0x00007efc32120f44 in stream_executor::host::HostStream::WorkLoop() () from /usr/local/lib/libtensorflow.so\r\n\tLine 222240: [23] No symbol table info available.\r\n\tLine 222241: [23] #7  0x00007efc2c75a810 in std::execute_native_thread_routine_compat (__p=<optimized out>) at ../../../.././libstdc++-v3/src/c++11/thread.cc:110\r\n\tLine 222498: [23]         __t = <optimized out>\r\n\tLine 222499: [23]         __local = {<std::__shared_ptr<std::thread::_Impl_base, (__gnu_cxx::_Lock_policy)2>> = {_M_ptr = <optimized out>, _M_refcount = {_M_pi = 0x7ef24c0f3270}}, <No data fields>}[23] \r\n\tLine 222499: [23]         __local = {<std::__shared_ptr<std::thread::_Impl_base, (__gnu_cxx::_Lock_policy)2>> = {_M_ptr = <optimized out>, _M_refcount = {_M_pi = 0x7ef24c0f3270}}, <No data fields>}[23] \r\n\tLine 222500: [23] #8  0x00007efc2d05c74a in start_thread () from /lib64/libpthread.so.0\r\n\tLine 222501: [23] No symbol table info available.\r\n\tLine 222502: [23] #9  0x00007efc2bc9ef6d in clone () from /lib64/libc.so.6\r\n", "comments": ["TF has a bunch of internal thread pools. Those thread pools sit on a condition variable while waiting for work to be available, which is probably what you're seeing here.\r\n\r\nTo be able to debug this we need to find the one or two threads which are not idling on the threadpool (you can recognize them for having more stack frames) and find out what they are blocked on. Building TF with debug symbols or with thread sanitize will help a lot in diagnosing this.\r\n\r\nAlternatively you can give us a reproducing example. What is the smallest program you can write that hits this occasional deadlock?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41702, "title": "I cant install/import tensorflow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1904\r\n- TensorFlow version: Trying to install version 2.2\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: trying to install with pip\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Its a laptop, that has both an Intel(R) UHD Graphics 630 and a Nvidia GTX 1060\r\n\r\n\r\n**Describe the problem**\r\nI get an error when trying to pip install tensorflow, however, I assumed that it was because I am using python 3.8, so I tried doing a special install with: `python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl` and that installed it, however when running a script that simply imports tensorflow, I get the error: `No module named '_pywrap_tensorflow_internal'` I know that it is tensorflow 1.12, but when I uninstalled and tried to do a special install of 2.2, it didn't work. Sorry if I'm just being stupid here but I cant figure it out.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nran `python -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl` in the cmd and when I try to import tensorflow I get the error `No module named '_pywrap_tensorflow_internal'`.\r\n\r\n**Any other info / logs**\r\nThe entire traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LORI\\Desktop\\Python Projects\\Tensor Flow Project\\main.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\LORI\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n```\r\n", "comments": ["Something does not make sense. Why would you install a mac pip on windows?", "Sorry, when writing this issue I had pasted the wrong command, I didn't actually install the mac one, I had installed the windows one. Also if it helps, when I tried to also install TensorFlow 2.2 through `https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-2.2.0-cp38-cp38-win_amd64.whl` It gives the error that it is not a supported wheel on this platform. It also did the same thing when I tried the CPU only version", "What is the output of\r\n\r\n```\r\npip list\r\npip --version\r\npython -vv\r\npython -w\r\n```\r\n\r\nIn general you don't need to download the pip from `storage.googleapis.com`, a `pip install tensorflow` should be able to get it, assuming your Python is on 64 bits, your pip is up to date, you have the needed MSVC redistributable installed and that Python is installed from the Python website, not from Microsoft Marketplace (apparently, there is a bug with that)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41702\">No</a>\n"]}, {"number": 41701, "title": "Try to fix bce loss check", "body": "Fix #41361", "comments": ["Only Windows Bazel failed but it is not related to this PR.", "/cc @k-w-w", "Can we close this or are you still interested?", "@fchollet Just out of curiosity cause I spent a little bit of time on this more the one year ago what was the outcome?", "Also, what about the connect ticket https://github.com/tensorflow/tensorflow/issues/41361?"]}, {"number": 41700, "title": "Hadoop registration", "body": "@mihaimaruseac \r\nThis PR adds support for `Hadoop` filesystem.", "comments": []}, {"number": 41697, "title": "converter.inference_input_type = tf.int8 is been ignored", "body": "**System information**\r\n- Docker image tensorflow/tensorflow:2.2.0 \r\n- Same issue with Windows python 3 and tensorflow 2.2.0 installed via pip\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nEspecially the converter.inference_input_type and converter.inference_output_type is imporant.\r\n```\r\nk_model = tf.keras.models.load_model(model_path)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(k_model)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\ndef representative_data_gen():\r\n    for input_value in data_set:\r\n        yield [input_value.astype(np.float32).reshape((1, 10))]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntf_lite_model_quant = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\nThe output of the convertion is the same as without specifying the inference_input and output_type. \r\nThe tflite outputfiles with and without the specification of the inference input type are attached. \r\n[tflite_conv_test.zip](https://github.com/tensorflow/tensorflow/files/4971633/tflite_conv_test.zip)\r\n\r\n\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- I expect a model without the qunatize and the dequantize layer at the beginning and at the end. \r\n- The generated model has a infernece_input_type of float32 not the expected int8 \r\n\r\n**Any other info / logs**\r\n\r\nWith tensorflow 1.15 the inference_input was int8 of the generated model when specifing the inference input type. Also no quantization or deqauntization layer were putted in the generated model. ", "comments": ["@MeghnaNatraj ", "This feature (`inference_input_type` and `inference_output_type`) is available starting TF 2.3. Would you able to upgrade the TF version and try again? ", "Worked fine with TF 2.3. Thanks. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41697\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41697\">No</a>\n"]}, {"number": 41696, "title": "Not able to create libtensorflow-lite.a from build_ios_universal_lib.sh", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.1.1\r\n- Python version: Python 2.7.16 and Python 3.7.3\r\n- Bazel version (if compiling from source): bazel 3.4.1-homebrew\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI was able to create libtensorflow-lite.a on my mac for Tensorflow version 1.8.0 before.\r\nNow, I am trying to do the same for version 2.1.0 but it's giving the following error \r\n\r\n```\r\nclang: error: no such file or directory: 'i386'\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nclang: error: no such file or directory: 'i386'\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/flatbuffer_conversions.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/allocation.o] Error 1\r\nclang: clang: error: no such file or directory: 'i386'\r\nerror: no such file or directory: 'i386'\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nclang: error: no such file or directory: 'i386'\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/arena_planner.o] Error 1\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/error_reporter.o] Error 1\r\nclang: error: no such file or directory: 'i386'\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nclang: error: no such file or directory: 'i386'\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/op_resolver.o] Error 1\r\nclang: error: no such file or directory: 'i386'\r\nclang: warning: no such sysroot directory: '-arch' [-Wmissing-sysroot]\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/c/c_api_internal.o] Error 1\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/api/tensor_utils.o] Error 1\r\nmake: *** [/Users/myMac/TFLite_2_1/tensorflow/lite/tools/make/gen/ios_i386/obj/tensorflow/lite/core/subgraph.o] Error 1\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n1. Have downloaded the code from https://github.com/tensorflow/tensorflow/tree/r2.1\r\n2. First, run download_dependencies.sh script and then build_ios_universal_lib.sh.\r\n\r\n**Facing the same issue in ver 2.2. I have also tried for ver 2.3 but it showed warning \"This build script is deprecated...\"**", "comments": ["@SimonFernandes7 \r\nCould you please refer to [this link](https://stackoverflow.com/questions/28420982/clang-error-no-such-file-or-directory-project-sdwebimage-3-7-1-build-releas) and let us know if it helps.", "@Saduf2019 Thanks, but the error that I'm facing is while creating the libtensorflow-lite.a file and not while using that file in Xcode. The reference link that you have provided is an issue while accessing the .a file in Xcode. ", "Thanks, for your reference. I was able to solve the issue by running the below command on terminal.\r\n`sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer/`\r\n\r\nAs it was not able to locate SDK \"iphonesimulator\"", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41696\">No</a>\n"]}, {"number": 41694, "title": "tf.lite.TFLiteConverter crashes when converting Keras model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.5- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-37224-ga6cd18a133 2.4.0-dev20200722\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nIt throws exception\r\n![\u622a\u5716 2020-07-24 \u4e0b\u53483 37 41](https://user-images.githubusercontent.com/4080524/88370104-f6229600-cdc3-11ea-8d7b-4f839cb5ac04.jpg)\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should finish the conversion successfully\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Input(shape=(28, 28), name='input'),\r\n    tf.keras.layers.Bidirectional(\r\n        tf.keras.layers.LSTM(20, return_sequences=True)),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\n\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nSTEPS = 28\r\nINPUT_SIZE = 28\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n# model directory.\r\nMODEL_DIR = \"keras_lstm\"\r\n\r\n# converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n# converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('tflite_test.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```", "comments": ["Renjie, could you take a look? This behavior is reproducible at HEAD and the error is coming from the SplitMergedOperandsPass.", "The model has a unidirectional_sequence_lstm op, which has the same tensor result from tfl.fill op at both stateful input tensors.", "Was able to convert the model successfully with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/4269072fef0f59a28396ccefbe4ddef5/41694.ipynb). Facing issue on running the code with [TF v2.3.0-rc2](https://colab.research.google.com/gist/amahendrakar/f51edfffb536854f7a1fb26884dd892f/41694-2-3.ipynb#scrollTo=d8HyaCK3fdbL) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b54d7ab3e07b0c821eb6076bf260033a/41694-tf-nightly.ipynb). \r\n\r\nPlease find the attached gist. Thanks!\r\n\r\n", "Can you try to follow the example here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb\r\n\r\n```\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nSTEPS = 28\r\nINPUT_SIZE = 28\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))\r\n\r\n# model directory.\r\nMODEL_DIR = \"keras_lstm\"\r\nmodel.save(MODEL_DIR, save_format=\"tf\", signatures=concrete_func)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)\r\ntflite_model = converter.convert()\r\n```\r\n\r\nThanks a lot!\r\n\r\nCheers,", "@haifengkao One other way beside what Renjie mentioned.\r\nis setting the batch size to 1 for the input\r\nlike this\r\ntf.keras.layers.Input(shape=(28, 28), name='input', batch_size=1)\r\n\r\nSo the sample above will be\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Input(shape=(28, 28), name='input', batch_size=1),\r\n    tf.keras.layers.Bidirectional(\r\n        tf.keras.layers.LSTM(20, return_sequences=True)),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'],experimental_steps_per_execution=1)\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('/tmp/tflite_test.tflite', 'wb') as f:\r\n    f.write(tflite_model)", "@renjie-liu I already tried it 2 days ago, and it works as I stated in the issue https://github.com/tensorflow/tensorflow/issues/41653", "@karimnosseir setting batch size to 1 works as well.   ", "Great! Shall we close the bug then?\r\n\r\nThe reason is in 2.2 the lstm ops are not fused but now they are.", "@renjie-liu  Of course not. You figured out the root cause, but it still crashes. Is fused op supposed to throw exception?", "Sure, we can through an exception if the batch size is not set during the conversion.", "@renjie-liu True, but the exception message should be \"batch size not set\" instead of dumping a whole screen of numbers. \r\nIf this concept is foreign to you, you may check [defensive programming here](https://enterprisecraftsmanship.com/posts/defensive-programming/)", "Hi Jaesung,\r\n\r\ncan you throw a warning in the python layer?\r\n\r\nthanks", "I tried the code in colab with TF v2.5 & didn't face the issue reported,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/f68daf4dd3d26b7be396df7db891b308/untitled283.ipynb) ..Thanks!", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41694\">No</a>\n"]}, {"number": 41800, "title": "Converting MediaPipe Handpose model to TFLite model", "body": "HI team,\r\n\r\nI am developing an embedded IOT device and I wanted to use the Handpose model for hand-tracking and gesture recognition but I didnt yet find a guide for converting Mediapipe models to TFLite model. Could you please provide a guide if its possible.\r\n\r\nThanks\r\n", "comments": ["@akshayvernekar please follow up here to see if hub team has any guide.\r\ncc @gowthamkpr ", "@akshayvernekar,\r\nCan you please elaborate on requirement/usecase. Thanks! ", "I am developing  a speaker where our goal is to use hand gestures to control the playback as opposed to using voice or physical buttons . Our platform is Linux based where our primary programming interface is C/C++ so we plan to use the tflite C++ library to do the inference . Unfortunately I didn\u2019t find a tflite version of the handpose model (Which is currently hosted on npm) which I can use . I wanted to know if there is any way to generate a tflite model from the hosted npm model .", "@akshayvernekar,\r\nI have transferred this issue from TF Hub repository to TF Lite Repository, as the issue is related to Lite.  ", "@akshayvernekar Did you get a chance to look at [mediapipe/models](https://github.com/google/mediapipe/tree/master/mediapipe/models)?\r\nFor hand detection models see https://github.com/google/mediapipe/tree/master/mediapipe/models#hand-detection-and-tracking", "Thanks @ymodak this is exactly what I was looking for, the repository has the necessary .tflite files . ", "Glad I could help. Closing this issue now. Thanks!", "@ymodak link provided is void\r\nthe tflite model for hands was deleted from repo", "The same problem, model was deleted", "@TheBricktop I found this https://github.com/google/mediapipe/blob/master/mediapipe/modules/palm_detection/palm_detection.tflite", "@kinivi do you have any idea how to convert pbtx to tflite? I need it for gpu accelerated palm and hand landmark", "@TheBricktop I can mislead, but I think you can use tflite model with GPU acceleration just by checking one parameter in the interpretator. ", "Hand tflite https://github.com/google/mediapipe/blob/master/mediapipe/modules/hand_landmark/hand_landmark.tflite"]}, {"number": 41693, "title": "InaccessibleTensorError in custom Model using add_loss and build", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:\r\n3.7.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the following code, which uses `add_loss()` *and* creates a layer in the `build()` method, I get an `InaccessibleTensorError`:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass MyModel(keras.models.Model):\r\n    def build(self, batch_input_shape):\r\n        self.output_layer = keras.layers.Dense(1)\r\n        super().build(batch_input_shape)\r\n\r\n    def call(self, inputs, training=None):\r\n        self.add_loss(1.0)\r\n        return self.output_layer(inputs)\r\n\r\nmodel = MyModel()\r\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\r\n\r\nX = tf.random.uniform((100, 10))\r\ny = tf.random.uniform((100, 1))\r\nhistory = model.fit(X, y, epochs=2)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the model to be trained normally, without error.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee the code above. You can run it in this colab: https://colab.research.google.com/drive/1c_NBNJ2vKt412WSg1HiPUNYjVmf1--YL\r\n\r\n**Other info / logs**\r\n\r\nHere is the full stacktrace:\r\n\r\n```\r\nInaccessibleTensorError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:231 __call__\r\n        reg_loss = math_ops.add_n(regularization_losses)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3239 add_n\r\n        return gen_math_ops.add_n(inputs, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:420 add_n\r\n        \"AddN\", inputs=inputs, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:591 _create_op_internal\r\n        inp = self.capture(inp)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:641 capture\r\n        % (tensor, tensor.graph, self))\r\n\r\n    InaccessibleTensorError: The tensor 'Tensor(\"Const:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=139933898535488); accessed from: FuncGraph(name=train_function, id=139933898618920).\r\n```\r\n", "comments": ["After further testing, it seems that you cannot use `add_loss()` in a custom model where the `build()` method is defined:\r\n\r\n```python\r\nclass MyModel2(keras.models.Model):\r\n    def build(self, batch_input_shape):\r\n        super().build(batch_input_shape)\r\n\r\n    def call(self, inputs, training=None):\r\n        self.add_loss(1.0)\r\n        return tf.reduce_mean(inputs)\r\n\r\nmodel2 = MyModel2()\r\nmodel2.compile(loss=\"mse\", optimizer=\"nadam\")\r\nhistory = model2.fit(X, y, epochs=2)\r\n```\r\n\r\nIf you comment out the `build()` method, everything works fine, even though the `build()` method does nothing more than call `super().build()`. That's really odd...", "I am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/60cb24d362d9b04cadd5cc4f631c5cb1/untitled294.ipynb)", "Is there any progress on this issue? \r\nOr a suggested workaround in the meantime? \r\n\r\nThanks in advance! ", "Thanks for the issue!\r\n\r\nConstant Tensors can't be used for losses in Functional Models, because the graph used to build the model is different than the graph used to execute it, and the building graph will capture the constant. Instead if you need a constant loss, you can use a callable loss:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass MyModel(keras.models.Model):\r\n\r\n    def build(self, batch_input_shape):\r\n        self.output_layer = keras.layers.Dense(1)\r\n        super().build(batch_input_shape)\r\n\r\n    def call(self, inputs, training=None):\r\n        self.add_loss(lambda: 1.0)\r\n        return self.output_layer(inputs)\r\n\r\nmodel = MyModel()\r\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\r\n\r\nX = tf.random.uniform((100, 10))\r\ny = tf.random.uniform((100, 1))\r\nhistory = model.fit(X, y, epochs=2)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41693\">No</a>\n", "The issue is not with constant but with graph management. This code still is not working:  \r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass MyModel(keras.models.Model):\r\n\r\n    def build(self, batch_input_shape):\r\n        self.output_layer = keras.layers.Dense(1)\r\n        super().build(batch_input_shape)\r\n\r\n    def call(self, inputs, training=None):\r\n        self.add_loss(tf.reduce_mean(self.output_layer(inputs)))\r\n        return self.output_layer(inputs)\r\n\r\nmodel = MyModel()\r\nmodel.compile(loss=\"mse\", optimizer=\"nadam\")\r\n\r\nX = tf.random.uniform((100, 10))\r\ny = tf.random.uniform((100, 1))\r\nhistory = model.fit(X, y, epochs=2)\r\n```\r\nBut when line 2709 in keras\\engine\\base_layer.py (`with tf_utils.maybe_init_scope(self):`) is commented no errors are raised."]}, {"number": 41692, "title": "assert padding in {'same', 'valid', 'full'} AssertionError", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): tf-nightly\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using Conv1DTranspose and I set `padding` to `causal` which is only valid for Conv1D but it fails and gives the assertion above.\r\n\r\n**Describe the expected behavior**\r\n Unless I'm very much mistaken this assertion should be updated.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@kernelizd \r\n\r\nRequest you to fill issue template.\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "I tried with tf-nightly version 2.4.0-dev20200724 and it works. Can you please confirm?\r\n```python\r\ntf.keras.layers.Conv1DTranspose(filters = 2, kernel_size = 3, strides=1, padding='causal', output_padding=None)\r\n#Outputs:\r\n<tensorflow.python.keras.layers.convolutional.Conv1DTranspose at 0x7fc6bc89cf60>\r\n```", "@ymodak \r\n\r\nThank you for trying it out. Yes, I can confirm the issue is there.\r\nThe model can be constructed, but onc you pass something you get the error.\r\nI constructed the same layer as you did above, called it `m`, then I passed\r\n\r\n`m(np.random.randn(4, 100, 2))`\r\n\r\nAnd got the error in the title.\r\n\r\nVersion: '2.4.0-dev20200723'", "As per the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose) the `causal` arg is not accepted as valid input for Conv1DTranspose.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41692\">No</a>\n"]}, {"number": 41691, "title": "ImportError: cannot import name py_checkpoint_reader", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@yuyuyuyuyu1234,\r\nIn order to expedite the trouble-shooting process, please provide the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/36609#issuecomment-590000208) from a similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41690, "title": "SPLIT_V for tensorflow lite micro", "body": "Currently SPLIT_V is not supported in tensorflow lite micro as a builtin op code. My model uses SPLIT_V. I need help with either preventing tflite conversion from generating SPLIT_V or how can I add it as a builtin op?", "comments": ["@Abhipray \r\nCan you please share simple stand alone code in support of the issue faced with the error faced or if possible share colab gist with error and tf version.", "@Saduf2019  Here is a colab that has a model that generates a SPLIT_V op with tflite converter. https://colab.research.google.com/drive/1XzrvDvxOVizrY5AT23hokcJHdgUcRd4r?usp=sharing\r\n\r\nIt generates a tflite file that you can download and view with netron https://github.com/lutzroeder/netron. You will see that the model uses SPLIT_V op which is currently not supported in tf lite micro.", "@Abhipray I am noticing an error in the model building code itself. Can you please check [this gist](https://colab.research.google.com/gist/jvishnuvardhan/a467b8730595ecb44e45fa826550c093/missing_split_v.ipynb) and share updated code to reproduce your issue? Thanks!", "@jvishnuvardhan Sorry about that. It's fixed now: https://colab.research.google.com/drive/1XzrvDvxOVizrY5AT23hokcJHdgUcRd4r?usp=sharing", "Hi Pete,\r\n\r\nDo you mind taking a look?\r\n\r\nThanks!", "Pete, just to add to this, I was able to create my own SPLIT_V op and add to the resolver easily before May. Since then, changes have been made to how ops are added and read from the flatbuffer and I can't figure out the necessary steps. What would be super helpful to me immediately is if you can share a link to an example PR that adds a built-in op that I can emulate. \r\nThanks!"]}, {"number": 41689, "title": "tf.UnsortSegmentSum/Prod support dynamic shape.", "body": "Convert UnsortSegmentSum/Prod to mhlo.dynamic_broadcast_in_dim and mhlo.scatter, which can support dynamic case.", "comments": ["the source IR is :\r\n```\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 455 : i32}} {\r\n  func @main(%arg0: tensor<?x?xf32>, %arg1: tensor<?xi32>) -> tensor<4439x?xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"\", outputs = \"\"}} {\r\n    %outputs = \"tf.Const\"() {device = \"\", value = dense<4439> : tensor<i32>} : () -> tensor<i32>\r\n    %outputs_0 = \"tf.UnsortedSegmentSum\"(%arg0, %arg1, %outputs) {_XlaAlreadyClustered = true, _class = [\"\"], device = \"\"} : (tensor<?x?xf32>, tensor<?xi32>, tensor<i32>) -> tensor<4439x?xf32>\r\n    reuturn %outputs_0 : tensor<4439x?xf32>\r\n}\r\n```\r\n\r\nNow will be lowered to \r\n```\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 455 : i32}} {\r\n  func @main(%arg0: tensor<?x?xf32>, %arg1: tensor<?xi32>) -> tensor<4676x?xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"\", outputs = \"\"}} {\r\n    %0 = mhlo.constant dense<4676> : tensor<i32>\r\n    %1 = tensor_cast %0 : tensor<i32> to tensor<i32>\r\n    %2 = mhlo.constant dense<0.000000e+00> : tensor<f32>\r\n    %3 = \"mhlo.broadcast\"(%2) {broadcast_sizes = dense<[4676, -1]> : tensor<2xi64>} : (tensor<f32>) -> tensor<4676x?xf32>\r\n    %4 = \"mhlo.scatter\"(%3, %arg1, %arg0) ( {\r\n    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):  // no predecessors\r\n      %5 = mhlo.add %arg2, %arg3 : tensor<f32>\r\n      \"mhlo.return\"(%5) : (tensor<f32>) -> ()\r\n    }) {indices_are_sorted = false, scatter_dimension_numbers = {index_vector_dim = 1 : i64, inserted_window_dims = dense<0> : tensor<1xi64>, scatter_dims_to_operand_dims = dense<0> : tensor<1xi64>, update_window_dims = dense<1> : tensor<1xi64>}, unique_indices = false} : (tensor<4676x?xf32>, tensor<?xi32>, tensor<?x?xf32>) -> tensor<4676x?xf32>\r\n    return %4 : tensor<4676x?xf32>\r\n  }\r\n}\r\n```\r\n\r\nAnd notice this line:  `%3 = \"mhlo.broadcast\"(%2) {broadcast_sizes = dense<[4676, -1]> : tensor<2xi64>} : (tensor<f32>) -> tensor<4676x?xf32>` , the `mhlo.broadcast` output has a dynamic dim,  it can't be solved, because it need the `%arg0` in `tf.UnsortedSegmentSum`, but miss the information here.\r\n\r\nSo here use `mhlo.dynamic_broadcast_in_dim` instead.\r\n\r\nAfter the change, the new IR is as follows:\r\n\r\n```\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 455 : i32}} {\r\n  func @main(%arg0: tensor<?x?xf32>, %arg1: tensor<?xi32>) -> tensor<4439x?xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"\", outputs = \"\"}} {\r\n    %0 = mhlo.constant dense<4439> : tensor<i32>\r\n    %1 = tensor_cast %0 : tensor<i32> to tensor<i32>\r\n    %2 = mhlo.constant dense<0.000000e+00> : tensor<f32>\r\n    %c4439_i64 = constant 4439 : i64\r\n    %c1 = constant 1 : index\r\n    %3 = dim %arg0, %c1 : tensor<?x?xf32>\r\n    %4 = index_cast %3 : index to i64\r\n    %5 = tensor_from_elements(%c4439_i64, %4) : tensor<2xi64>\r\n    %6 = \"mhlo.dynamic_broadcast_in_dim\"(%2, %5) {broadcast_dimensions = dense<[]> : tensor<0xi64>} : (tensor<f32>, tensor<2xi64>) -> tensor<4439x?xf32>\r\n    %7 = \"mhlo.scatter\"(%6, %arg1, %arg0) ( {\r\n    ^bb0(%arg2: tensor<f32>, %arg3: tensor<f32>):  // no predecessors\r\n      %8 = mhlo.add %arg2, %arg3 : tensor<f32>\r\n      \"mhlo.return\"(%8) : (tensor<f32>) -> ()\r\n    }) {indices_are_sorted = false, scatter_dimension_numbers = {index_vector_dim = 1 : i64, inserted_window_dims = dense<0> : tensor<1xi64>, scatter_dims_to_operand_dims = dense<0> : tensor<1xi64>, update_window_dims = dense<1> : tensor<1xi64>}, unique_indices = false} : (tensor<4439x?xf32>, tensor<?xi32>, tensor<?x?xf32>) -> tensor<4439x?xf32>\r\n    return %7 : tensor<4439x?xf32>\r\n  }\r\n}\r\n```\r\n", "> Can you include a test?\r\n\r\ndoing...", "@joker-eph @sherhut mehdi, please help to review. thanks.", "Sorry I'm on leave at the moment. Friendly ping to @joker-eph and @sherhut to take another look. :)", "@joker-eph @sherhut Can you please take a look on this PR ? Thanks!", "@joker-eph @sherhut  Can you please take a look on this PR ? Thanks!", "@joker-eph @sherhut Can you please take a look on this PR ? Thanks!", "@joker-eph @sherhut Can you please take a look on this PR ? Thanks!", "@shanshanpt  Can you please check @jpienaar's comments and keep us posted ? Thanks!", "@shanshanpt Any update on this PR? Please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 41688, "title": "[INTEL MKL] Bug fix in MKlConcat", "body": "This PR fixes a bug in MklConcat.\r\n\r\nIn MklConcat, when input tensors are all in tf format, we need make sure output memory format is the same as input, and pass dst_md as one param of `concat::primitive_desc`. Otherwise primitive will choose default output format which may be different from what we need.", "comments": []}, {"number": 41687, "title": "Also copy ubuntu sanity build to new layout", "body": null, "comments": []}, {"number": 41686, "title": "'WeightSharedConvolutionalBoxPredictor' object has no attribute 'inputs'", "body": "Hello. I have updated tf1 to the second version. Downloaded ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8 and configured config. But I get the error\r\n`Traceback (most recent call last):\r\n  File \"model_main.py\", line 108, in <module>\r\n    tf.app.run()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main.py\", line 104, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 472, in train_and_evaluate\r\n    return executor.run()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 349, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1182, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1211, in _train_model_default\r\n    self.config)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1170, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\tensorflow1\\models\\research\\object_detection\\model_lib.py\", line 543, in model_fn\r\n    update_ops=detection_model.updates(),\r\n  File \"C:\\tensorflow1\\models\\research\\object_detection\\meta_architectures\\ssd_meta_arch.py\", line 1350, in updates\r\n    self._box_predictor.inputs))\r\nAttributeError: 'WeightSharedConvolutionalBoxPredictor' object has no attribute 'inputs'`\r\n\r\n`model {\r\n  ssd {\r\n    num_classes: 12\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 1024\r\n        width: 1024\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"ssd_resnet152_v1_fpn_keras\"\r\n      depth_multiplier: 1.0\r\n      min_depth: 16\r\n      conv_hyperparams {\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 0.00039999998989515007\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            mean: 0.0\r\n            stddev: 0.029999999329447746\r\n          }\r\n        }\r\n        activation: RELU_6\r\n        batch_norm {\r\n          decay: 0.996999979019165\r\n          scale: true\r\n          epsilon: 0.0010000000474974513\r\n        }\r\n      }\r\n      override_base_feature_extractor_hyperparams: true\r\n      fpn {\r\n        min_level: 3\r\n        max_level: 7\r\n      }\r\n    }\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n        use_matmul_gather: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    box_predictor {\r\n      weight_shared_convolutional_box_predictor {\r\n        conv_hyperparams {\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.00039999998989515007\r\n            }\r\n          }\r\n          initializer {\r\n            random_normal_initializer {\r\n              mean: 0.0\r\n              stddev: 0.009999999776482582\r\n            }\r\n          }\r\n          activation: RELU_6\r\n          batch_norm {\r\n            decay: 0.996999979019165\r\n            scale: true\r\n            epsilon: 0.0010000000474974513\r\n          }\r\n        }\r\n        depth: 256\r\n        num_layers_before_predictor: 4\r\n        kernel_size: 3\r\n        class_prediction_bias_init: -4.599999904632568\r\n      }\r\n    }\r\n    anchor_generator {\r\n      multiscale_anchor_generator {\r\n        min_level: 3\r\n        max_level: 7\r\n        anchor_scale: 4.0\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        scales_per_octave: 2\r\n      }\r\n    }\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 9.99999993922529e-09\r\n        iou_threshold: 0.6000000238418579\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n        use_static_shapes: false\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    loss {\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_loss {\r\n        weighted_sigmoid_focal {\r\n          gamma: 2.0\r\n          alpha: 0.25\r\n        }\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n    encode_background_as_zeros: true\r\n    normalize_loc_loss_by_codesize: true\r\n    inplace_batchnorm_update: true\r\n    freeze_batchnorm: false\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 1\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    random_crop_image {\r\n      min_object_covered: 0.0\r\n      min_aspect_ratio: 0.75\r\n      max_aspect_ratio: 3.0\r\n      min_area: 0.75\r\n      max_area: 1.0\r\n      overlap_thresh: 0.0\r\n    }\r\n  }\r\n  sync_replicas: true\r\n  optimizer {\r\n    momentum_optimizer {\r\n      learning_rate {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.03999999910593033\r\n          total_steps: 100000\r\n          warmup_learning_rate: 0.013333000242710114\r\n          warmup_steps: 2000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.8999999761581421\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  fine_tune_checkpoint: \"C:/tensorflow1/models/research/object_detection/ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8\\checkpoint\\ckpt-0\"\r\n  num_steps: 100000\r\n  startup_delay_steps: 0.0\r\n  replicas_to_aggregate: 8\r\n  max_number_of_boxes: 100\r\n  unpad_groundtruth_tensors: false\r\n  fine_tune_checkpoint_type: \"classification\"\r\n  use_bfloat16: true\r\n  fine_tune_checkpoint_version: V2\r\n}\r\ntrain_input_reader {\r\n  label_map_path: \"C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"C:/tensorflow1/models/research/object_detection/train.record\"\r\n  }\r\n}\r\neval_config {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: false\r\n}\r\neval_input_reader {\r\n  label_map_path: \"C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: \"C:/tensorflow1/models/research/object_detection/test.record\"\r\n  }\r\n}\r\n`\r\n\r\n`(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection>python model_main.py --logtostderr --train_dir=training/ --pipeline_config_path=ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8/pipeline.config --model_dir=training/`", "comments": ["When i use model_main_tf2 i have this\r\n`WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\nW0724 04:29:54.094718  1196 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.`", "solver, sorry", "i am getting same error whats the issue?\r\n", "@hashtriton Excuse me I'm getting similar error what was the issue and how you solve it?", "I got alot of errors while like these, most of them were because of I was not using compatible versions of all libraries/tools(all details are mentioned on their readme file)  followed this and got no errors onwards...", "@Mandar800 What is the readme file you are referring? Is this: https://github.com/tensorflow/models/blob/master/research/object_detection/README.md ?\r\nBecause I followed the instructions in above file but I still get the error"]}, {"number": 41685, "title": "image_dataset_from_directory(label_mode=None) dosen't work.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow version (use command below): tf-nightly-gpu \r\nv1.12.1-37350-g537f3ec52f \r\n2.4.0-dev20200723\r\n- Python version: 3.8\r\n- GPU model and memory: Nvidia GTX 1080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nUsing the code provided below results in:\r\n\r\n> Found 0 files belonging to 0 classes.\r\n> Using 0 files for training.\r\n\r\nAnd at the end of the output:\r\n> TypeError: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string.\r\n\r\n**Describe the expected behavior**\r\nAs described in the documentation, expected to return a tensor of shape=[n,h,w,c].\r\n\r\n> _If label_mode is None, it yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels), encoding images (see below for rules regarding num_channels)._\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\r\ntrain_data = image_dataset_from_directory(directory='/folder/with/images', label_mode=None)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n[Traceback.txt](https://github.com/tensorflow/tensorflow/files/4969755/Traceback.txt)", "comments": ["Ran into the same issue. I assume you have your files located in '/folder/with/images' - Does it behave the same if you put them in '/folder/with/images/foo'? EDIT: https://colab.research.google.com/drive/1NSnMNnrFrpaB4zQsdm7AQPzThRMpRBxi?usp=sharing", "No, it dosen't, but with that it returns: x = tensor of shape=[n,h,w,c]; y = label [foo].\r\nAnd I don't want this, I want just the 'x'.", "@QLaHPD,\r\nI was able to run the code without any issues and got the expected output for `x`. Please check [this gist](https://colab.research.google.com/gist/amahendrakar/9452f651f1c59bac66570941d6659a59/41685.ipynb) for reference. \r\n\r\nAlso, please take a look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) and make sure the images are in the right directory structure. Thanks!\r\n\r\n", "> EDIT: https://colab.research.google.com/drive/1NSnMNnrFrpaB4zQsdm7AQPzThRMpRBxi?usp=sharing\r\n\r\nInteresting, here it works like this, but when I use model.fit(train_data, train_data), returns:\r\n\r\n> ValueError: `y` argument is not supported when using dataset as input.\r\n\r\nSo I'm assuming its another type of error. If you can help me more, I'd be grateful, but thanks for helping out.\r\n\r\n", "> Interesting, here it works like this, but when I use model.fit(train_data, train_data), returns:\r\n> \r\n> > ValueError: `y` argument is not supported when using dataset as input.\r\n\r\n@QLaHPD,\r\nIn order to reproduce this error could you please provide the complete code and the dataset you are using, so that we can look into it. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41683, "title": "Git Pull doesn't work in building from source for GPU", "body": "**System information**\r\nUbuntu 20.04\r\nTensorFlow installed from \"docker pull tensorflow/tensorflow:devel-gpu\"\r\n- TensorFlow version: 2.4?\r\n- Python version: 3.8.2 (installed on system but what's running in the container I don't know\r\n- Not sure but believe it's Pip\r\n- Bazel version (if compiling from source): Don't know\r\n- GCC/Compiler version (if compiling from source): don't know\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 3  GeForce GTX 1070s with 8GB each\r\n\r\n\r\n\r\n**Describe the problem**\r\nIn following the instructions on the bottom of the page for compiling TensorFlow from source for GPU. I ran across a problem with the git pull. when I run the docker container I end up in the /tensorflow directory. But there's no .git file there. But there is one in the /tensorflow_src directory. Either you need to change the docker run command to end up in that directory or I suggest you add a \"cd ../tensorflow_src\" command before the \"git pull\" command. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ndocker pull tensorflow/tensorflow:devel-gpu\r\ndocker run --gpus all -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\r\n    tensorflow/tensorflow:devel-gpu bash\r\ngit pull  # within the container, download the latest source code\r\n\r\n**Any other info / logs**\r\nN/A", "comments": ["Could you please try with the latest version, if you are using docker for GPU, you can follow this link https://www.tensorflow.org/install/docker#gpu_support. \r\nFor building from source for GPU on Linux system, you can follow this link https://www.tensorflow.org/install/source#gpu_support_3 and perform the git pull within the container.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41683\">No</a>\n"]}, {"number": 41682, "title": "ImportError: dlopen(.../_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: virtualenv, conda\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): GCC 4.2.1\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: Radeon RX 580 8GB installed, but want CPU only version\r\n\r\n**Describe the problem**\r\nI am running into the described error approximately 45 minutes into the build of the pip package. I believe the specific problem is: ImportError: dlopen(.../_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE. Any help would be greatly appreciated, thanks!\r\n\r\nI am trying to build tensorflow from source as my CPU spec is old. My aim is to get a working version of tensorflow 2.x, CPU optimisation would be great but not necessary as I can utilise my GPU for tasks using PlaidML.\r\nMy CPU:\r\nIntel Xeon x5670 2.93Ghz\r\nMMX instructions\r\nSSE / Streaming SIMD Extensions\r\nSSE2 / Streaming SIMD Extensions 2\r\nSSE3 / Streaming SIMD Extensions 3\r\nSSSE3 / Supplemental Streaming SIMD Extensions 3\r\nSSE4 / SSE4.1 + SSE4.2 / Streaming SIMD Extensions 4\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n# Check whether script is executing in a VirtualEnv or Conda environment\r\nif [ -z \"$VIRTUAL_ENV\" ] && [ -z \"$CONDA_PREFIX\" ] ; then\r\n\techo \"VirtualEnv or Conda env is not activated\"\r\n\texit -1\r\nfi\r\n\r\n# Set the virtual environment path\r\nif ! [ -z \"$VIRTUAL_ENV\" ] ; then\r\n  VENV_PATH=$VIRTUAL_ENV\r\nelif ! [ -z \"$CONDA_PREFIX\" ] ; then\r\n  VENV_PATH=$CONDA_PREFIX\r\nfi\r\n\r\n# Set the bin and lib directories\r\nVENV_BIN=$VENV_PATH/bin\r\nVENV_LIB=$VENV_PATH/lib\r\n\r\n# bazel tf needs these env vars\r\nexport PYTHON_BIN_PATH=$VENV_BIN/python\r\nexport PYTHON_LIB_PATH=`ls -d $VENV_LIB/*/ | grep python`\r\n\r\n# Set the native architecture optimization flag, which is a default\r\nCOPT=\"--copt=-march=native\"\r\n\r\n# Determine the available features of your CPU\r\nraw_cpu_flags=`sysctl -a | grep machdep.cpu.features | cut -d \":\" -f 2 | tr '[:upper:]' '[:lower:]'`\r\n\r\n# Append each of your CPU's features to the list of optimization flags\r\nfor cpu_feature in $raw_cpu_flags\r\ndo\r\n\tcase \"$cpu_feature\" in\r\n\t\t\"sse4.1\" | \"sse4.2\" | \"ssse3\" | \"fma\" | \"cx16\" | \"popcnt\" | \"maes\")\r\n\t\t    COPT+=\" --copt=-m$cpu_feature\"\r\n\t\t;;\r\n\t\t\"avx1.0\")\r\n\t\t    COPT+=\" --copt=-mavx\"\r\n\t\t;;\r\n\t\t*)\r\n\t\t;;\r\n\tesac\r\ndone\r\nbazel clean\r\n\r\n# Run TensorFlow configuration (accept defaults unless you have a need)\r\n./configure\r\n\r\n# Accepts default options for configuration when prompted\r\n\r\n# Build the TensorFlow pip package\r\nbazel build -c opt $COPT -k //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n...\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (385 packages loaded, 31018 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/sandbox\r\nERROR: /Users/alexanderjenkins/tensorflow/tensorflow/BUILD:971:1: Couldn't build file tensorflow/_api/v2/v2.py: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE\r\n  Referenced from: /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/eager/context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: dlopen(/private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental13LoadDatasetOp11kReaderFuncE\r\n  Referenced from: /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Expected in: flat namespace\r\n in /private/var/tmp/_bazel_alexanderjenkins/b4dba3332782448704ff09db5b64eb10/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\r\n", "comments": ["@AlexanderLJenkins,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/39262#issuecomment-625569319) from a similar issue and let us know if it helps. Thanks!", "Hi @amahendrakar, thank you for your quick reply and help.\r\n\r\nI believe the issue is the same as in the comment you mentioned: 'the constant variables in the class are compiled as indirect symbols that cannot be found in flat namespace on macOS using AppleClang++'. This is causing the build to fail at the final step.\r\n\r\nI have attempted to solve this problem by moving relevant lines from the .h to .cc files (snapshot_dataset_op.cc/.h and io_ops.cc/.h). However, the **Symbol not found** error remains. The new error points for me to do the same in a different file (data_service_dataset_op.cc/.h). \r\n\r\nAll files are in the same directory: tensorflow/tensorflow/core/kernels/data/experimental/\r\n\r\nPlease may you let me know what files I need to change?\r\n\r\nThanks again!\r\n\r\nAlex", "Updating Xcode to the most recent supported version has fixed this issue for me. Check here to download previous versions: https://xcodereleases.com.\r\n\r\nAlex", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41682\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41682\">No</a>\n"]}]