[{"number": 31188, "title": "Cast `inputs` array to float64 in case of big endian architecture for normalization", "body": "These changes fixes https://github.com/tensorflow/tensorflow/issues/26135 ", "comments": ["Hi @martinwicke Gentle reminder to merge this PR.", "Can one of the admins verify this patch?", "@martinwicke Can you please review the comments and merge the PR?", "Hi @martinwicke \r\n\r\nPlease review the comments and let me know those are fine. We verified the test behavior on tag 2.0.0-rc1 too on s390x, the test is still failing there. ", "@kbhute-ibm Could you please address Ubuntu Sanity errors? Thanks!", "@gbaned Thanks for the notification. Done with fixing the Ubuntu Sanity errors.", "Hi @martinwicke @gbaned,\r\n\r\nAs 'contrib' folder is removed from TF 2.0 (commit: https://github.com/tensorflow/tensorflow/commit/ffc25308ce2be84240ec90502b38800bc5a4dd60) this PR has merge conflicts in master. Had few queries:\r\n* Are we completely skipping tests present inside ./contrib from master and TF 2.0?\r\n* Can this PR be merged for TF 1.x?\r\n\r\nThanks,\r\nKoumudini", "No tests for contrib will be run in TF 2.0. In fact, contrib has plainly ceased to exist outside the 1.x branches.\r\n\r\nCan you request a cherry-pick into the 1.15 branch?\r\n\r\n@goldiegadde FYI, this is a harmless cherrypick that we can merge into 1.15.", "Hi @martinwicke \r\nAs you mentioned in the comment https://github.com/tensorflow/tensorflow/pull/31188#issuecomment-533207777, no contrib tests would run in TF 2.0. However we could still see few tests inside ./contrib folder https://github.com/tensorflow/tensorflow/tree/v2.0.0/tensorflow/contrib.\r\n\r\nCould you please comment on this and update our knowledge?\r\n\r\nThanks in advance.", "We haven't deleted the contrib folder on the 2.0 branch because of build\ncomplexities. We are not running any of the tests in it though. All code\ninside should be considered dead.\n", "@martinwicke Thanks for the clarification. "]}, {"number": 31187, "title": "Failed to compile 'tensorflow/lite/experimental/ruy/pack_avx512.cc'", "body": "**System information**\r\n- Have I written custom code - YES, but not in the failing part\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master branch\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory:N/A\r\n\r\n```\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::{anonymous}::HalfPackFloatAvx512(const float*, const float*, int, int, int, float*, float*)':\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:343:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t0 = LoaduTwo(src_ptr0, src_ptr4);\r\n                                         ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:344:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t1 = LoaduTwo(src_ptr1, src_ptr5);\r\n                                         ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:345:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t2 = LoaduTwo(src_ptr2, src_ptr6);\r\n                                         ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:346:41: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t3 = LoaduTwo(src_ptr3, src_ptr7);\r\n                                         ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:363:9: error: '_mm256_storeu_epi32' was not declared in this scope\r\n         _mm256_storeu_epi32(packed_ptr + 0 * 16, _mm512_castsi512_si256(r0));\r\n         ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:363:9: note: suggested alternative: '_mm256_store_epi64'\r\n         _mm256_storeu_epi32(packed_ptr + 0 * 16, _mm512_castsi512_si256(r0));\r\n         ^~~~~~~~~~~~~~~~~~~\r\n         _mm256_store_epi64\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:382:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t0 = MaskLoaduTwo(row_mask, src_ptr0, src_ptr4);\r\n                                                       ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:383:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t1 = MaskLoaduTwo(row_mask, src_ptr1, src_ptr5);\r\n                                                       ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:384:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t2 = MaskLoaduTwo(row_mask, src_ptr2, src_ptr6);\r\n                                                       ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:385:55: error: cannot convert '__m512 {aka __vector(16) float}' to '__m512i {aka __vector(8) long long int}' in assignment\r\n         t3 = MaskLoaduTwo(row_mask, src_ptr3, src_ptr7);\r\n                                                       ^\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:402:9: error: '_mm256_storeu_epi32' was not declared in this scope\r\n         _mm256_storeu_epi32(trailing_buf + 0 * 16, _mm512_castsi512_si256(r0));\r\n         ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:402:9: note: suggested alternative: '_mm256_store_epi64'\r\n         _mm256_storeu_epi32(trailing_buf + 0 * 16, _mm512_castsi512_si256(r0));\r\n         ^~~~~~~~~~~~~~~~~~~\r\n         _mm256_store_epi64\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::Pack8bitAvx512(const int8_t*, int8_t, const int8_t*, int, int, int, int8_t*, int32_t*)':\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:465:3: error: 'memset' was not declared in this scope\r\n   memset(trailing_buf, 0, kTrailingBufSize * sizeof(std::int8_t));\r\n   ^~~~~~\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:465:3: note: suggested alternative: 'Offset'\r\n   memset(trailing_buf, 0, kTrailingBufSize * sizeof(std::int8_t));\r\n   ^~~~~~\r\n   Offset\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:500:5: error: 'memcpy' was not declared in this scope\r\n     memcpy(packed_ptr + Layout::kCols * non_trailing_rows, trailing_buf,\r\n     ^~~~~~\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:500:5: note: suggested alternative: '_m_empty'\r\n     memcpy(packed_ptr + Layout::kCols * non_trailing_rows, trailing_buf,\r\n     ^~~~~~\r\n     _m_empty\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc: In function 'void ruy::PackFloatAvx512(const float*, const float*, int, int, int, float*)':\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:516:5: error: 'memset' was not declared in this scope\r\n     memset(trailing_buf, 0, sizeof(trailing_buf));\r\n     ^~~~~~\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:516:5: note: suggested alternative: 'Offset'\r\n     memset(trailing_buf, 0, sizeof(trailing_buf));\r\n     ^~~~~~\r\n     Offset\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:524:5: error: 'memcpy' was not declared in this scope\r\n     memcpy(packed_ptr + 16 * non_trailing_rows, trailing_buf,\r\n     ^~~~~~\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:524:5: note: suggested alternative: '_m_empty'\r\n     memcpy(packed_ptr + 16 * non_trailing_rows, trailing_buf,\r\n```\r\n", "comments": ["@DavidNorman \r\nIn order to expedite the trouble-shooting process, please provide a full code snippet to reproduce the issue reported here. Thanks!", "the code is in the repo as it stands.  i expect that if you try to compile the code with the default options, using this compiler: `gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0`, then you will get the error.", "further info:  setting the `tflite_with_ruy_explicit_false` config variable does not disable compilation of the file `tensorflow/lite/experimental/ruy/kernel_avx512.cc`.", "more further info: we set the following arch flags : `'-march=skylake'`", "-> @jalexstark ", "I was having the same issue today compiling the v2 API on a skylake architecture system. \r\n\r\nIs there, (or can there please soon be), a compile option for completely disabling tf lite when building the pip wheel. When building on the average desktop/laptop tf lite being there makes sense for completeness. But when building from source code for a high end workstation or cluster node this entire portion of tensorflow is never going to be used and only serves to break the build. \r\n\r\nI have had similar issues with tf lite when building tensorflow on aarch64 platforms where again tf lite will never be used and is not needed. \r\n\r\nIf we need tf lite, then we'd take our model building code and pretrained weights and use a seperate script to convert on any machine, even a laptop. There is no need to have tf lite host capability on cluster nodes and embedded devices and should be able to be disabled during compilation.\r\n\r\nHoping for a quick fix to the main issue. :) Thanks", "I just tried to build the r2.0 branch and have exactly the same error.  I am running Ubuntu 18.04.  My architecture is skylake too (Intel 7820x CPU).  My GPU is an RTX 2080 Ti.  It's a desktop computer not a high end workstation.  I ran the following:\r\n./configure\r\nAccepted defaults initially\r\nWhen asked for CUDA support, typed \"y\"\r\nWhen asked for TensorRT support, typed \"y\"\r\nWhen asked for compute capabilities, typed 7.5\r\nAccepted defaults thereafter\r\nbazel build --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\nI get the exact same error.\r\nAlso hoping for a fix or the ability to exclude tf lite.  Would you even need tf lite on a desktop computer?\r\n:) Also hoping for a fix. Thanks\r\n", "I can confirm that this error does **not** occur when building r2.0 branch on my laptop with a Coffeelake Intel  i7-9750H CPU.  The laptop is set up exactly the same as my Skylake desktop (which has a build error).  They are both Ubuntu 18.04, same options in ./configure, and same bazel build command (see my previous post immediately above for details).  I was able to pip install the .whl created on my laptop successfully on both my Coffeelake laptop and my Skylake desktop.", "Would you be able to post the compiler errors? I want to sort this out promptly. Lots of fixes have gone in to get this working with various compilers. It is mostly C++ fragility (transitive includes) and uneven type checking.\r\n\r\nI apologize for the pain, and really appreciate your patience. I hope the longer-run code health, and the performance of the new GEMM kernels as we roll them out, will bring benefits.\r\n", "@jalexstark \r\nHi Alex,\r\nThe attached files represent (1) bazel --verbose_explanations and (2) the entire cleaned capture of the output to the terminal when the following command's were issued:\r\n`\r\nTERM=dump\r\nscript capture.txt\r\n\r\n./configure\r\n\r\nbazel build --explain=verbose_explanations.txt --verbose_explanations --verbose_failures --subcommands=pretty_print --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\ncol -b < capture.txt > cleaned-capture.txt\r\n\r\nexit\r\n`\r\n[verbose_explanations.txt](https://github.com/tensorflow/tensorflow/files/3496750/verbose_explanations.txt)\r\n\r\n[cleaned-capture.txt](https://github.com/tensorflow/tensorflow/files/3496713/cleaned-capture.txt)\r\n\r\ncleaned-capture.txt caused gedit on my machine to go black because it's a big file.  I was able to load it in to LibreOffice 6 and read it without difficulty.  It just takes about 30 seconds to load in to LibreOffice.\r\n\r\nThe build ended with the same error detailed on this page.  It was run on my Skylake Intel i7-7820x running Unbuntu 18.04 and bazel 0.26.0.  I accepted the defaults in ./configure except for selecting support for CUDA and TensorRT and specifying compute capability 7.5 (it has an NVIDIA RTX 2080 Ti).\r\nI hope this helps.\r\nCheers,\r\nDan", "@jalexstark \r\nHi,\r\nHave you had a chance to look at the files in the previous post?  Was that the information you were looking for or do you need me to run some other commands and get error info?\r\nAll the best,\r\nDan", "Thanks for the details.\r\n\r\nI will check to see if these can be fixed. They might have already.\r\n\r\nSome of the errors are \"incorrect\" in that the statements do not match Intel documentation. That might just be weak C++ errors.\r\n\r\nIn the light of the inability of the computer engineering community to get even roughly consistent compilation together, which makes it very difficult to reproduce issues, I disabled AVX enhancements entirely outside of Clang+Linux. When the code is being developed less intensely, we will look into opening up to GCC and Apple.\r\n\r\nThanks again for taking the time to get the logs: they really do help, and confirm that we have at least fixed the basic bugs.", "I did a git pull to update to recent bug fixes.\r\nThe build still fails around //tensorflow/lite/experimental/ruy\r\nI produced these output files for this new build using the same method as before:\r\n[cleaned-capture.txt](https://github.com/tensorflow/tensorflow/files/3507388/cleaned-capture.txt)\r\n[verbose_explanations.txt](https://github.com/tensorflow/tensorflow/files/3507389/verbose_explanations.txt)\r\nCheers,\r\nDan", "Would you check which code you have?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/pack_avx512.cc#L352\r\n\r\ncleaned-capture.txt has\r\n\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc: In function \u2018void ruy::{anonymous}::HalfPackFloatAvx512(const float*, const float*, int, int, int, float*, float*)\u2019:\r\ntensorflow/lite/experimental/ruy/pack_avx512.cc:352:42: error: cannot convert \u2018__m512 {aka __vector(16) float}\u2019 to \u2018__m512i {aka __vector(8) long long int}\u2019 for argument \u20181\u2019 to \u2018__m512i _mm512_unpacklo_epi32(__m512i, __m512i)\u2019\r\n\t r0 = _mm512_unpacklo_epi32(t0, t1);\r\n\r\n(If my browser is showing me the correct file)\r\n\r\nThis issue should have been fixed.", "I had the same issue building `r2.0` `--config=v2` with `-march=barcelona` from {commit 65e6355ad9d74359a46827f87e76fa311ebf7714 (HEAD -> r2.0, origin/r2.0)} inside the `latest-devel` Docker container (`Linux 8dd3006a4831 4.19.37-5+deb10u1rodete1-amd64 #1 SMP Debian 4.19.37-5+deb10u1rodete1 (2019-07-22 > 2018) x86_64 x86_64 x86_64 GNU/Linux`)\r\n\r\nLatest r2.0 \"commit b0fee96b1dd893fe3fea6b746cf1965c8ca9f114 (HEAD -> r2.0, origin/r2.0)\" does not work either.\r\n\r\nI'll try master now.\r\n\r\nOK, master does not have `--config=v2` ", "Doing a simple 'git pull' was not enough to update \"pack_avx512.cc\".  I deleted and cloned the repository again then did:\r\ngit checkout r2.0\r\n\r\nI get a different set of build errors:\r\n\r\n[cleaned-capture.txt](https://github.com/tensorflow/tensorflow/files/3512651/cleaned-capture.txt)\r\n[verbose_explanations.txt](https://github.com/tensorflow/tensorflow/files/3512653/verbose_explanations.txt)\r\n", "Thanks.  This seems to bring it to a case of\r\n\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc: In function \u2018void ruy::Kernel8bitAvx512(const ruy::KernelParams8bit<16, 16>&)\u2019:\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error: \u2018_mm512_loadu_epi8\u2019 was not declared in this scope\r\n\t const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);\r\n\t\t\t\t  ^~~~~~~~~~~~~~~~~\r\n\r\nThere are two possibilities:\r\n(a) Not compiling for Skylake architectures, or not using Clang.\r\n\r\nSee\r\nhttps://software.intel.com/sites/landingpage/IntrinsicsGuide/#expand=3373,3373,2422,2452,2455,2186,103,2186,2204,87,4008,3534,2197,2192,2201,5008,2201,6098,245,3171,2458,2201,3505,5205,4550,94,1548,1383,3533,3533,3505,2984,3021,3263,3518,3956,3992,4029,6042,1383,429,1192,3395&avx512techs=AVX512F,AVX512BW,AVX512CD,AVX512DQ,AVX512VL&techs=MMX,SSE,SSE2,SSE3,SSSE3,SSE4_1,SSE4_2,AVX,AVX2,FMA&text=_mm512_loadu_epi8\r\nNote the requirement for AVX512BW support.\r\n\r\nThis should not be enabled.  See\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/platform.h#L81\r\nand\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/platform.h#L94\r\n\r\n(b) Compiling for Skylake architecture and have Clang and under Linux. This error should not occur. If it does, look at what header file is pulled in here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/pack_avx512.cc#L28\r\nExplore the tree of header files, which are required to have AVX512BW guards on a Skylake-enabled build setup.\r\n\r\nIf your code is up to date, and the TFLite does what we expect, then you are hitting a bug in your setup. There is nothing the code can do if the immintrin.h file installed and used in a Skylake build does not have Skylake support.", "The code is up to date.\r\nI tried using clang 6 and clang 9 and got the same error:\r\n\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc: In function \u2018void ruy::Kernel8bitAvx512(const ruy::KernelParams8bit<16, 16>&)\u2019:\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error: \u2018_mm512_loadu_epi8\u2019 was not declared in this scope\r\n         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);\r\n                                  ^~~~~~~~~~~~~~~~~\r\nAVX512BW support appears to be in /usr/lib/gcc/x86_64-linux-gnu/7/include/immintrin.h\r\n[immintrin.h.txt](https://github.com/tensorflow/tensorflow/files/3517676/immintrin.h.txt)\r\n\r\nand also in /usr/lib/llvm-6.0/lib/clang/6.0.0/include/immintrin.h\r\n[immintrin.h.clang.txt](https://github.com/tensorflow/tensorflow/files/3517692/immintrin.h.clang.txt)\r\n", "Obviously I am flying almost blind here, and I appreciate the logs. Do feel free to chase down as best you can: don't feel you have to wait for suggestions.\r\n\r\nSo, I do see\r\nhttps://github.com/gcc-mirror/gcc/blob/master/gcc/config/i386/avx512bwintrin.h#L385\r\n\r\nIn one search, but I cannot tell where your headers files came from.\r\n\r\nThe above linked file does seem to suggest how a bug could have crept in. Notice how there are masked loadu_epi8 instructions, but not the as-it-were unmasked ones.  According to Intel (link provided earlier), _mm512_loadu_epi8 should be available for __AVX512BW__.\r\n\r\nI would suggest grepping for loadu_epi8 in your include directories to hunt down the function prototype. The inclusion and guarding should match the support in\r\nhttps://software.intel.com/sites/landingpage/IntrinsicsGuide/#expand=3373,3373,2422,2452,2455,2186,103,2186,2204,87,4008,3534,2197,2192,2201,5008,2201,6098,245,3171,2458,2201,3505,5205,4550,94,1548,1383,3533,3533,3505,2984,3021,3263,3518,3956,3992,4029,6042,1383,429,1192,3395,3396&avx512techs=AVX512F,AVX512BW,AVX512CD,AVX512DQ,AVX512VL&techs=MMX,SSE,SSE2,SSE3,SSSE3,SSE4_1,SSE4_2,AVX,AVX2,FMA&text=loadu_epi8\r\n", "There are only masked loadu_epi8 instructions on my system:\r\n\r\ngrep \"loadu_epi8\" /usr/lib/gcc/x86_64-linux-gnu/7/include/avx512bwintrin.h\r\n\r\n_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const *__P)\r\n_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)\r\n\r\nI did a grep of all include directories for gcc, clang 6 and clang 9.  Clang 9 includes an unmasked instruction.  Would I have to reconfigure my whole toolchain to use Clang 9 with bazel?  I've tried looking up how to do that and it seems beyond my skill set.  Or do I need to patch or update gcc?  Or can your code be updated to use masked instructions?\r\n\r\nCheers,\r\nDan", "The entire output of the grep commands on include directories for gcc, clang 9 and clang 6 in that order was:\r\n\r\n(tfgpu) daniel@linuxcorsair:~/tensorflow$ grep -r \"loadu_epi8\" /usr/lib/gcc/x86_64-linux-gnu/7/include/\r\n\r\n/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void cons\r\nt *__P)\r\n/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)\r\n/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *\r\n__P)\r\n/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512vlbwintrin.h:_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)\r\n/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512bwintrin.h:_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const\r\n*__P)\r\n/usr/lib/gcc/x86_64-linux-gnu/7/include/avx512bwintrin.h:_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)\r\n\r\n(tfgpu) daniel@linuxcorsair:~/tensorflow$ grep -r \"loadu_epi8\" /usr/lib/llvm-9/lib/clang/9.0.0/include/\r\n\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vbmi2intrin.h:_mm512_mask_expandloadu_epi8(__m512i __S, __mmask64 __U, voi\r\nd const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vbmi2intrin.h:_mm512_maskz_expandloadu_epi8(__mmask64 __U, void const *__P\r\n)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm_mask_expandloadu_epi8(__m128i __S, __mmask16 __U, void\r\n const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm_maskz_expandloadu_epi8(__mmask16 __U, void const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm256_mask_expandloadu_epi8(__m256i __S, __mmask32 __U, v\r\noid const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlvbmi2intrin.h:_mm256_maskz_expandloadu_epi8(__mmask32 __U, void const *_\r\n_P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm_loadu_epi8 (void const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  struct __loadu_epi8 {\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  return ((struct __loadu_epi8*)__P)->__v;\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *\r\n__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm256_loadu_epi8 (void const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  struct __loadu_epi8 {\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:  return ((struct __loadu_epi8*)__P)->__v;\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void cons\r\nt *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512vlbwintrin.h:_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:_mm512_loadu_epi8 (void const *__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:  struct __loadu_epi8 {\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:  return ((struct __loadu_epi8*)__P)->__v;\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const\r\n*__P)\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/avx512bwintrin.h:_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)\r\n\r\n(tfgpu) daniel@linuxcorsair:~/tensorflow$ grep -r \"loadu_epi8\" /usr/lib/llvm-6.0/lib/clang/6.0.0/include/\r\n\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vbmi2intrin.h:_mm512_mask_expandloadu_epi8(__m512i __S, __mmask64 __U, v\r\noid const *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vbmi2intrin.h:_mm512_maskz_expandloadu_epi8(__mmask64 __U, void const *_\r\n_P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm128_mask_expandloadu_epi8(__m128i __S, __mmask16 __U,\r\n void const *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm128_maskz_expandloadu_epi8(__mmask16 __U, void const\r\n*__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm256_mask_expandloadu_epi8(__m256i __S, __mmask32 __U,\r\n void const *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlvbmi2intrin.h:_mm256_maskz_expandloadu_epi8(__mmask32 __U, void const\r\n*__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const\r\n *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void co\r\nnst *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512vlbwintrin.h:_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512bwintrin.h:_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void cons\r\nt *__P)\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/avx512bwintrin.h:_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)\r\n\r\nI tried making replacing gcc with clang (sudo update-alternatives --install /usr/bin/gcc gcc /usr/lib/llvm-9/bin/clang 20) and also replacing g++ with clang++ and replacing ld with lld.  However that just causes the compiler to not recognize gcc options a few seconds in to the build.  I restored gcc, g++ and ld after that.  I gathered such a crude approach was insufficient to get bazel to properly use the clang toolchain.  Is there a docker image where the toolchain is set up to build with clang?\r\n\r\nShould I try building in:\r\n\r\ndocker pull tensorflow/tensorflow:devel-gpu-py3 \r\n\r\nCheers,\r\nDan", "I just tried building in the docker image tensorflow/tensorflow:devel-gpu-py3.\r\nSo the compiler setup should be standard.\r\n\r\nI ran ./configure and chose not to use clang as the CUDA compiler.\r\n\r\nbazel build --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nI got the same error message.\r\n\r\nI then did \"bazel clean\" \r\n\r\nI ran ./configure, and this time chose clang as CUDA compiler, electing to download a fresh release.\r\n\r\nbazel build --config=opt --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nSame error again.\r\n\r\nAll these trials are being done with the BIOS  set to factory defaults.  No overclocking or XMP.", "Various.\r\n\r\nI did notice that one of the earlier logs showed an error in a line in (I think it was) kernel.h that no longer existed / exists. But I am assuming that the problems with the recent runs are with entirely fresh checkouts.\r\n\r\nThe current version can only compile the code with the problematic intrinsics if under clang.\r\n\r\nThe grep output shows that you have the illogical version of intrinsics for\r\n/usr/lib/gcc/x86_64-linux-gnu/7/include/\r\n/usr/lib/llvm-6.0/lib/clang/6.0.0/include/\r\n\r\nbut working version for\r\n/usr/lib/llvm-9/lib/clang/9.0.0/include/\r\n\r\nPlease take a look at XXX/tensorflow/lite/experimental/ruy/platform.h\r\nYou can disable the problematic features at line 81 of\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/platform.h#L81\r\n\r\nIf you do manage to figure out a version-based control of the logic at line 81, please let me know. Now that I have some idea of what is out in the wild, I might be able to take a stab, but the illogical intrinsics were, as you can see, propagated for a while before they were fixed.\r\n\r\n\"Illogical\" is not meant to be critical or judgemental. Rather they are just internally nonsensical. The Intel chipsets added masked versions of the CPU instructions as superset enhancements. They are logical supersets, and so having them without the non-masked versions is illogical.", "Thanks again for the grepping, BTW. I will probably add a guard such as __clang_major__ >= 7", "I tried:\r\nCC=/usr/lib/llvm-9/bin/clang CXX=/usr/lib/llvm-9/bin/clang++ bazel build --config=opt --define=using_clang=true --define=using_cuda_clang=true --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThis built for longer and seemed to get past the error we're talking about here.  However, due to incompatibilities with clang and gcc (I think) it eventually failed to build with an unrelated error.\r\n\r\nYou can get a successful build if you use comments in:\r\n~/tensorflow/tensorflow/lite/experimental/ruy/platform.h\r\nComment out:\r\n`\r\n// TODO(b/138433137) Select AVX-512 at runtime rather than via compile options.\r\n\r\n// #if defined(__AVX512F__) && defined(__AVX512DQ__) && defined(__AVX512CD__) && \\\r\n    defined(__AVX512BW__) && defined(__AVX512VL__)\r\n\r\n// #define RUY_DONOTUSEDIRECTLY_AVX512 1\r\n\r\n// #else\r\n\r\n#define RUY_DONOTUSEDIRECTLY_AVX512 0\r\n\r\n// #endif~/tensorflow/tensorflow/lite/experimental/ruy/platform.h\r\n\r\n`\r\n\r\ni.e. Always: #define RUY_DONOTUSEDIRECTLY_AVX512 0\r\n\r\nthen the build completes successfully.\r\n\r\nI'm no C developer.  I fear that tensorflow's kernel_avx512.cc is trying to accomplish with AVX-512 on a Skylake is not possible with the compatible gcc (version 7).  This incompatibility (support for unmasked instructions) does not appear to have been updated on the latest source of gcc.  I don't think it is easy to compile all of Tensorflow with clang-9 as it is incompatible in other ways out of the box.  My limited knowledge suggests the only 3 solutions are:\r\n1) Re-writing gcc intrinsics support.  I submitting patches to the gcc git.\r\n2) Giving up on AVX-512 (#define RUY_DONOTUSEDIRECTLY_AVX512 0).\r\n3) Making a flag that you include in \"bazel build\" that would make the whole build compatible with clang-9.  This would require re-writing areas of the code where clang-9 causes errors because it is not a straight-out replacement for gcc.\r\n\r\n ", "When I restore platform.h to its original state and run:\r\n\r\nCC=/usr/lib/llvm-9/bin/clang CXX=/usr/lib/llvm-9/bin/clang++ bazel build --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\nI get the same build error we're talking about (in tensorflow/lite/experimental/ruy/pack_avx512.cc).\r\n", "Recent versions of platform.h have the following lines:\r\n\r\n#if RUY_PLATFORM(X86_ENHANCEMENTS) && RUY_PLATFORM(X86) &&                    \\\r\n    defined(__AVX512F__) && defined(__AVX512DQ__) && defined(__AVX512CD__) && \\\r\n    defined(__AVX512BW__) && defined(__AVX512VL__)\r\n#define RUY_DONOTUSEDIRECTLY_AVX512 1\r\n#else\r\n#define RUY_DONOTUSEDIRECTLY_AVX512 0\r\n#endif\r\n\r\nYour file looks different. Would you check out the source / confirm which version that you are using?", "Yes, I did a git clone and tried out building 25 hours ago.  Then I was using a different version of platform.h to the one available now.  I just now did a new fresh git clone and have the new verion of platform.h.\r\nI still get the same error at _mm512_storeu_epi32.  I did not use clang as cuda compiler in ./configure.  I built using:\r\nbazel build --config=opt --config=cuda --config=v2 --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\nCheers,\r\nDaniel\r\n", "Just a moment - now at _mm512_storeu_epi32()?\r\n\r\nThat is more different that it might at first appear. Which line?\r\n\r\nI think that cuda means clang. There is a recent change (may not have propagated yet) that disables for Clang < version 8.\r\n\r\nNonetheless, it would be useful to know the exact error. I have a way of working around the other mess, but that might not help for epi32. There _used_ to be an actual bug relating to the types in the call to that function, but it was fixed a while back. So it would be good to know if something else is not working together.\r\n", "Sorry I pasted the end of the traceback.  The start of the error message\nincluedes: \"\u2018_mm512_loadu_epi8\u2019 was not declared in this scope\" early on,\nwhich is the same error:  The whole error message is:\n\nERROR:\n/home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++\ncompilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed\n(Exit 1)\nIn file included from\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:\n./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line\ncomment [-Wcomment]\n #endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || \\\n         ^\nIn file included from external/gemmlowp/fixedpoint/fixedpoint.h:895:0,\n                 from ./tensorflow/lite/experimental/ruy/kernel.h:22,\n                 from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:\nexternal/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring\nattributes on template argument \u2018__m128i {aka __vector(2) long long int}\u2019\n[-Wignored-attributes]\n struct FixedPointRawTypeTraits<__m128i> {\n                                       ^\nIn file included from\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:\n./tensorflow/lite/experimental/ruy/kernel.h: In function \u2018void\nruy::MakeKernelParamsFloat(const ruy::PackedMatrix<float>&, const\nruy::PackedMatrix<float>&, const ruy::BasicSpec<float, float>&, int, int,\nint, int, ruy::Matrix<float>*, ruy::KernelParamsFloat<LhsCols, RhsCols>*)\u2019:\n./tensorflow/lite/experimental/ruy/kernel.h:456:53: warning: typedef \u2018using\nParams = struct ruy::KernelParamsFloat<LhsCols, RhsCols>\u2019 locally defined\nbut not used [-Wunused-local-typedefs]\n   using Params = KernelParamsFloat<LhsCols, RhsCols>;\n                                                     ^\ntensorflow/lite/experimental/ruy/kernel_avx512.cc: In function \u2018void\nruy::Kernel8bitAvx512(const ruy::KernelParams8bit<16, 16>&)\u2019:\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error:\n\u2018_mm512_loadu_epi8\u2019 was not declared in this scope\n         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);\n                                  ^~~~~~~~~~~~~~~~~\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: note: suggested\nalternative: \u2018_mm512_add_epi8\u2019\n         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);\n                                  ^~~~~~~~~~~~~~~~~\n                                  _mm512_add_epi8\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: error:\n\u2018_mm512_loadu_epi32\u2019 was not declared in this scope\n                                _mm512_loadu_epi32(&params.lhs_sums[row]));\n                                ^~~~~~~~~~~~~~~~~~\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: note: suggested\nalternative: \u2018_mm512_load_epi32\u2019\n                                _mm512_loadu_epi32(&params.lhs_sums[row]));\n                                ^~~~~~~~~~~~~~~~~~\n                                _mm512_load_epi32\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: error:\n\u2018_mm512_loadu_epi32\u2019 was not declared in this scope\n                                _mm512_loadu_epi32(&params.rhs_sums[col]));\n                                ^~~~~~~~~~~~~~~~~~\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: note: suggested\nalternative: \u2018_mm512_load_epi32\u2019\n                                _mm512_loadu_epi32(&params.rhs_sums[col]));\n                                ^~~~~~~~~~~~~~~~~~\n                                _mm512_load_epi32\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: error:\n\u2018_mm_storeu_epi8\u2019 was not declared in this scope\n             _mm_storeu_epi8(tmp_ptr,\n_mm512_cvtepi32_epi8(accum_data_v[j]));\n             ^~~~~~~~~~~~~~~\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: note: suggested\nalternative: \u2018_mm_store_epi64\u2019\n             _mm_storeu_epi8(tmp_ptr,\n_mm512_cvtepi32_epi8(accum_data_v[j]));\n             ^~~~~~~~~~~~~~~\n             _mm_store_epi64\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: error:\n\u2018_mm_storeu_epi8\u2019 was not declared in this scope\n             _mm_storeu_epi8(tmp_ptr,\n_mm512_cvtepi32_epi8(accum_data_v[j]));\n             ^~~~~~~~~~~~~~~\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: note: suggested\nalternative: \u2018_mm_store_epi64\u2019\n             _mm_storeu_epi8(tmp_ptr,\n_mm512_cvtepi32_epi8(accum_data_v[j]));\n             ^~~~~~~~~~~~~~~\n             _mm_store_epi64\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: error:\n\u2018_mm256_storeu_epi16\u2019 was not declared in this scope\n             _mm256_storeu_epi16(tmp_ptr,\n             ^~~~~~~~~~~~~~~~~~~\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: note: suggested\nalternative: \u2018_mm256_store_epi64\u2019\n             _mm256_storeu_epi16(tmp_ptr,\n             ^~~~~~~~~~~~~~~~~~~\n             _mm256_store_epi64\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: error:\n\u2018_mm512_storeu_epi32\u2019 was not declared in this scope\n             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);\n             ^~~~~~~~~~~~~~~~~~~\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: note: suggested\nalternative: \u2018_mm512_store_epi32\u2019\n             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);\n             ^~~~~~~~~~~~~~~~~~~\n             _mm512_store_epi32\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n\nOn Fri, 23 Aug 2019 at 07:13, Alex Stark <notifications@github.com> wrote:\n\n> Just a moment - now at _mm512_storeu_epi32()?\n>\n> That is more different that it might at first appear. Which line?\n>\n> I think that cuda means clang. There is a recent change (may not have\n> propagated yet) that disables for Clang < version 8.\n>\n> Nonetheless, it would be useful to know the exact error. I have a way of\n> working around the other mess, but that might not help for epi32. There\n> *used* to be an actual bug relating to the types in the call to that\n> function, but it was fixed a while back. So it would be good to know if\n> something else is not working together.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31187?email_source=notifications&email_token=AAB26QRVHWE2ITYP4SVJ773QF36OVA5CNFSM4IIDGWLKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD46MTLQ#issuecomment-524077486>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAB26QRPKC4LCGQ63LNKVLDQF36OVANCNFSM4IIDGWLA>\n> .\n>\n", "I'm still not understanding.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ruy/kernel.h\r\n\r\ndoes not have a line 613, so how can there be this error?\r\n\r\n\r\nERROR:\r\n/home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++\r\ncompilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed\r\n(Exit 1)\r\nIn file included from\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:\r\n./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line\r\ncomment [-Wcomment]\r\n #endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || \\\r\n         ^", "I just now did:\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ngit checkout r2.0\r\n\r\ntensorflow/tensorflow/lite/experimental/ruy/kernel.h\r\nhas a line 613:\r\n\r\n[kernel.h.txt](https://github.com/tensorflow/tensorflow/files/3536878/kernel.h.txt)\r\n\r\nI am on r2.0 not master:\r\n\r\n[https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/lite/experimental/ruy/kernel.h](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/lite/experimental/ruy/kernel.h)", "Master branch builds successfully.  Maybe you guys just need to use the code in master for /tensorflow/lite/experimental/ruy/kernel.h in r2.0.", "@dbonner glad to here compilation is possible, have you looked at the diff between branches for that file?\r\n\r\nThe work around I used to disable tf lite from building was to comment out the following lines in the pip package BUILD file (https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/tools/pip_package/BUILD#L64-L66) which lets branch r2.0 build successfully on Skylake at the loss of tf lite functionality which I did not need. ", "@JossWhittle The files between the 2 branches are completely different.  One is about 25Kb (r2.0) and the other is only 2-3Kb (master) from memory.\r\nYes I have a similar workaround (commenting out AVX512 support for tf lite but not disabling TF lite entirely) to get r2.0 to build.\r\n\r\n@jalexstark Hi, hadn't heard from you in a bit.  I'm sorry I got you thinking I was on the master branch.  My problem has always been r2.0 since that's what I'm interested in learning to code in.  All those files I dumped were for building r2.0.  I just tried again and it is still failing.  Are you able to help with the skylake building for r2.0?  I don't know if you can just port the way they have fixed this issue from master to r2.0?  Or should I start a new thread for the r2.0 issue since this thread was started by someone who was building master?\r\n\r\nThe error right now on r2.0  is (I think it's still the same):\r\nERROR: /home/daniel/tensorflow/tensorflow/lite/experimental/ruy/BUILD:271:1: C++ compilation of rule '//tensorflow/lite/experimental/ruy:kernel' failed (Exit 1)\r\nIn file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:\r\n./tensorflow/lite/experimental/ruy/kernel.h:613:9: warning: multi-line comment [-Wcomment]\r\n #endif  // (RUY_PLATFORM(NEON_64) || RUY_PLATFORM(NEON_32) || \\\r\n         ^\r\nIn file included from external/gemmlowp/fixedpoint/fixedpoint.h:895:0,\r\n                 from ./tensorflow/lite/experimental/ruy/kernel.h:22,\r\n                 from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:\r\nexternal/gemmlowp/fixedpoint/./fixedpoint_sse.h:43:39: warning: ignoring attributes on template argument \u2018__m128i {aka __vector(2) long long int}\u2019 [-Wignored-attributes]\r\n struct FixedPointRawTypeTraits<__m128i> {\r\n                                       ^\r\nIn file included from tensorflow/lite/experimental/ruy/kernel_avx512.cc:18:0:\r\n./tensorflow/lite/experimental/ruy/kernel.h: In function \u2018void ruy::MakeKernelParamsFloat(const ruy::PackedMatrix<float>&, const ruy::PackedMatrix<float>&, const ruy::BasicSpec<float, float>&, int, int, int, int, ruy::Matrix<float>*, ruy::KernelParamsFloat<LhsCols, RhsCols>*)\u2019:\r\n./tensorflow/lite/experimental/ruy/kernel.h:456:53: warning: typedef \u2018using Params = struct ruy::KernelParamsFloat<LhsCols, RhsCols>\u2019 locally defined but not used [-Wunused-local-typedefs]\r\n   using Params = KernelParamsFloat<LhsCols, RhsCols>;\r\n                                                     ^\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc: In function \u2018void ruy::Kernel8bitAvx512(const ruy::KernelParams8bit<16, 16>&)\u2019:\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: error: \u2018_mm512_loadu_epi8\u2019 was not declared in this scope\r\n         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);\r\n                                  ^~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:111:34: note: suggested alternative: \u2018_mm512_add_epi8\u2019\r\n         const __m512i lhs_data = _mm512_loadu_epi8(lhs_ptr);\r\n                                  ^~~~~~~~~~~~~~~~~\r\n                                  _mm512_add_epi8\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: error: \u2018_mm512_loadu_epi32\u2019 was not declared in this scope\r\n                                _mm512_loadu_epi32(&params.lhs_sums[row]));\r\n                                ^~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:161:32: note: suggested alternative: \u2018_mm512_load_epi32\u2019\r\n                                _mm512_loadu_epi32(&params.lhs_sums[row]));\r\n                                ^~~~~~~~~~~~~~~~~~\r\n                                _mm512_load_epi32\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: error: \u2018_mm512_loadu_epi32\u2019 was not declared in this scope\r\n                                _mm512_loadu_epi32(&params.rhs_sums[col]));\r\n                                ^~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:170:32: note: suggested alternative: \u2018_mm512_load_epi32\u2019\r\n                                _mm512_loadu_epi32(&params.rhs_sums[col]));\r\n                                ^~~~~~~~~~~~~~~~~~\r\n                                _mm512_load_epi32\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: error: \u2018_mm_storeu_epi8\u2019 was not declared in this scope\r\n             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));\r\n             ^~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:277:13: note: suggested alternative: \u2018_mm_store_epi64\u2019\r\n             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));\r\n             ^~~~~~~~~~~~~~~\r\n             _mm_store_epi64\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: error: \u2018_mm_storeu_epi8\u2019 was not declared in this scope\r\n             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));\r\n             ^~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:293:13: note: suggested alternative: \u2018_mm_store_epi64\u2019\r\n             _mm_storeu_epi8(tmp_ptr, _mm512_cvtepi32_epi8(accum_data_v[j]));\r\n             ^~~~~~~~~~~~~~~\r\n             _mm_store_epi64\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: error: \u2018_mm256_storeu_epi16\u2019 was not declared in this scope\r\n             _mm256_storeu_epi16(tmp_ptr,\r\n             ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:309:13: note: suggested alternative: \u2018_mm256_store_epi64\u2019\r\n             _mm256_storeu_epi16(tmp_ptr,\r\n             ^~~~~~~~~~~~~~~~~~~\r\n             _mm256_store_epi64\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: error: \u2018_mm512_storeu_epi32\u2019 was not declared in this scope\r\n             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);\r\n             ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/ruy/kernel_avx512.cc:326:13: note: suggested alternative: \u2018_mm512_store_epi32\u2019\r\n             _mm512_storeu_epi32(tmp_ptr, accum_data_v[j]);\r\n             ^~~~~~~~~~~~~~~~~~~\r\n             _mm512_store_epi32\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 94.519s, Critical Path: 13.68s\r\nINFO: 1041 processes: 1041 local.\r\nFAILED: Build did NOT complete successfully\r\n", "@jalexstark Hi, please indicate whether I should start a new post to get Skylake building fixed for r2.0?  Many thanks, Daniel.", "I would suggest going ahead, making clear that it is a case of updating r2.0 with existing fixes, so that it doesn't get triaged as a bug needing a development fix.\r\n\r\nThanks.", "OK thanks, I have submitted the issue [https://github.com/tensorflow/tensorflow/issues/32026](https://github.com/tensorflow/tensorflow/issues/32026)", "Vanilla code from r2.0 branch still doesn't work for me on Skylake.\r\n\r\n> i.e. Always: #define RUY_DONOTUSEDIRECTLY_AVX512 0\r\n\r\nBut this workaround helps.", "Should be fixed in r2.1.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31187\">No</a>\n"]}, {"number": 31186, "title": "Add FP16 precision support to external C API", "body": "Addressing issue #31185\r\n\r\nAdd TFL_InterpreterSetAllowFp16PrecisionForFp32 to allow FP16 precision.\r\n", "comments": ["This needs proper API design review first. Please file an issue on this instead, we'll then evaluate it internally."]}, {"number": 31185, "title": "Add FP16 precision support to TFLite external C API", "body": "**System information**\r\n- TensorFlow version (you are using):master repository\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdd FP16 precision support to TFLite external C API.\r\n\r\n\r\n**Will this change the current api? How?**\r\nThis change has no influence.\r\n\r\n**Who will benefit with this feature?**\r\nTFLite extenal C API users\r\n\r\n**Any Other info.**\r\nHere is [my patch](https://github.com/stakemura/tensorflow/commit/d36a995ccb053145162bb993d351086b3eaa10c0)", "comments": ["What exactly are you looking for in the API? At the moment, we only officially support FP16 weights, which aren't exposed to clients in any visible way.", "I'm looking for the function equivalent to [TFLite model benchmark allow_fp16 option](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L226).\r\nI think that some additional code is required for supporting GPU delegations.\r\nIf needed, I want to submit PR including the additional code.", "The relaxed precision is for internal calculations, not for any external inputs, and we don't have any immediate plans for exposing fp16 inputs externally.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31185\">No</a>\n"]}, {"number": 31184, "title": "Transfer learning trained by custom TF 2.0 training loop performs worse than keras fit ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 7.6.0\r\n- GPU model and memory: GTX1660Ti, 6 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI performed transfer learning on pretrained model with TF custom training loop and keras fit.\r\nBoth of the settings are the same but TF custom training loop performs worse than the keras fit . I have no idea what's the problem.\r\n\r\nI have asked the questions on StackOverflow but not got the answer what I want\r\nhttps://stackoverflow.com/questions/57268705/transfer-learning-with-pretrained-model-by-tf-gradienttape-cant-converge\r\n\r\n**Describe the expected behavior**\r\nThe loss and accuracy of model trained by tf.GradientTape should be similar to the one trained by keras fit with the same settings\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\ntry:\r\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\nexcept:\r\n    pass\r\n\r\ncifar10 = tf.keras.datasets.cifar10\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n\r\ndef process_data(img, lbl):\r\n    img = tf.image.resize(img, (96, 96))\r\n    img = (img-128) / 128\r\n    return img, lbl\r\ntrain_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(128)\r\ntest_data = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128)\r\ntrain_data = train_data.map(process_data)\r\ntest_data = test_data.map(process_data)\r\ntrain_data, test_data\r\n\r\n# load the pretrained model\r\nbase_model = keras.applications.MobileNetV2(input_shape=(96, 96, 3), include_top=False, pooling='avg')\r\nx = base_model.outputs[0]\r\noutputs = layers.Dense(10, activation=tf.nn.softmax)(x)\r\nmodel = keras.Model(inputs=base_model.inputs, outputs=outputs)\r\n\r\n# Trained with keras fit\r\nmodel.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\r\nhistory = model.fit(train_data, epochs=1)\r\n\r\n# The results are: loss: 0.4345 - accuracy: 0.8585\r\n\r\n# Trained with tf.GradientTape\r\noptimizer = keras.optimizers.Adam()\r\ntrain_loss = keras.metrics.Mean()\r\ntrain_acc = keras.metrics.SparseCategoricalAccuracy()\r\ndef train_step(data, labels):    \r\n    with tf.GradientTape() as gt:\r\n        pred = model(data)\r\n        loss = keras.losses.SparseCategoricalCrossentropy()(labels, pred)\r\n\r\n    grads = gt.gradient(loss, model.trainable_variables)\r\n\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n    train_loss(loss)\r\n    train_acc(labels, pred)\r\n\r\nmodel = keras.Model(inputs=base_model.inputs, outputs=outputs)\r\nfor xs, ys in train_data:\r\n    train_step(xs, ys)\r\n\r\nprint('train_loss = {:.3f}, train_acc = {:.3f}'.format(train_loss.result(), train_acc.result()))\r\n\r\n# The results are:  train_loss = 12.832, train_acc = 0.099\r\n```\r\n\r\n**Other info / logs**\r\nIf the model trained by tf.GradientTape with smaller learning rate 0.0001 (the default is 0.001), it works well, train_loss = 0.275, train_acc = 0.915 , but that's not the real solution what I expected, it's just a workaround.\r\n", "comments": ["I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1MnjoWQpt1grgjQAE1gDZaKu6AB7fgWBq) here.Thanks!", "When will the next version of TF with these bugs fixed be released ? Thanks", "I have tried TensorFlow 2.0 RC but it still doesn't work. Is it that the bug is not fix yet? or there is something wrong with the codes ? How should I apply transfer learning with tf.GradientTape successfully ? Thanks", "Hey, I got the same problem. \r\nSimply add a training=True argument in the model like\r\n\r\npred = model(input, training=True)\r\n\r\nHopefully, you could solve it", "@JiayuanSternLi Thanks for the support in resolving the issue. @hgffly [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ddea44f8549605a6ea9ff069981794ef/untitled737.ipynb) is the gist for your reference. With `training = True`, custom training is producing similar results as keras fit. The loss and accuracy are as follows\r\n\r\n```\r\nTrain for 391 steps\r\n391/391 [==============================] - 2116s 5s/step - loss: 0.4347 - accuracy: 0.8597\r\n\r\n```\r\n\r\nI think this is resolved. I am closing this issue. Please feel free to reopen the issue if it persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31184\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31184\">No</a>\n", "> Hey, I got the same problem.\r\n> Simply add a training=True argument in the model like\r\n> \r\n> pred = model(input, training=True)\r\n> \r\n> Hopefully, you could solve it\r\n@JiayuanSternLi  , thanks for help, it did work.\r\nBut I don't see document mentioning this, or maybe I miss it and I'll be so glad if you could tell me how to know it.\r\nSo I need to add training=True argument to the model whenever I trained with GradientTape or just the example of transfer learning I listed. Because I have tried training other model, it can converge without assigning training=True.  \r\nI just wonder it's a bug or it's because I don't know the behavior of the model training in TF2 well.\r\nThanks", "FYI regarding setting training=True: I made this [commit](https://github.com/tensorflow/docs/commit/7ad5e9598e66efeef8b3ed9fa53fa5ed023ddbed) that makes its need more obvious in tensorflow.org tutorials going forward. ", "> > Hey, I got the same problem.\r\n> > Simply add a training=True argument in the model like\r\n> > pred = model(input, training=True)\r\n> > Hopefully, you could solve it\r\n> > @JiayuanSternLi  , thanks for help, it did work.\r\n> > But I don't see document mentioning this, or maybe I miss it and I'll be so glad if you could tell me how to know it.\r\n> > So I need to add training=True argument to the model whenever I trained with GradientTape or just the example of transfer learning I listed. Because I have tried training other model, it can converge without assigning training=True.\r\n> > I just wonder it's a bug or it's because I don't know the behavior of the model training in TF2 well.\r\n> > Thanks\r\n\r\n\"Such an issue differs from models you trained\". It's true.\r\nIt is caused by the batch norm layers. Those models with BN layers requires the argument traning=True, but others don't", "> @JiayuanSternLi Thanks for the support in resolving the issue. @hgffly [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/ddea44f8549605a6ea9ff069981794ef/untitled737.ipynb) is the gist for your reference. With `training = True`, custom training is producing similar results as keras fit. The loss and accuracy are as follows\r\n> \r\n> ```\r\n> Train for 391 steps\r\n> 391/391 [==============================] - 2116s 5s/step - loss: 0.4347 - accuracy: 0.8597\r\n> ```\r\n> \r\n> I think this is resolved. I am closing this issue. Please feel free to reopen the issue if it persists again. Thanks!\r\n\r\n@jvishnuvardhan . I also run into the same problem when training a tf.keras model even though I add \"train=True\". The loss reduced sharply when calling model.fit, but the performance is far worse when I use tf.GradientTape. \r\n\r\nI created a [reproducible example](https://colab.research.google.com/drive/1aIRqi_x-YGAFtZCWL4gkOPkW_8sSlZ_N) on Colab. Could you have a look? It would take 2 minutes to reproduce the problem.    ", "@wmmxk Can you please open a new issue with more details on your issue, error trace and provide the colab (in the above post). It will help others who are facing similar issue like you and uses transformer model. Thanks!"]}, {"number": 31183, "title": "Prevent to decompress the speech command dataset every time", "body": "In `AudioProcessor.maybe_download_and_extract_dataset()`, it's only necessary to decompress the speech dataset once after downloading, and repeated decompression slows down the debug progress.", "comments": ["Can one of the admins verify this patch?", "@petewarden Can you please take a look on this PR? Thanks!"]}, {"number": 31182, "title": "TF2-gpu: _SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, ", "body": "System information\r\n\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nTensorFlow installed from binary:\r\nTensorFlow version (use command below):tf-nightly-gpu-2.0-preview 2.0.0.dev20190729\r\nPython version: 3.6.4\r\nCUDA/cuDNN version: CUDA-10.0/ Cudnn7.6.1\r\nGPU model and memory: 3 Titan XP \r\n\r\n**Describe the current behavior**\r\nRNN Masking of length cause crash\r\n**Code to reproduce the issue**\r\n```\r\n    mask = tf.sequence_mask(length, dtype=tf.bool)\r\n    mask = K.expand_dims(mask)\r\n    lstm = layers.LSTM(num_hidden[i], return_sequences=True,\r\n                          return_state=True)\r\n    inputs, _, _ = lstm(inputs, mask=mask)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n\r\nW0731 09:54:25.986333 139687813953280 network.py:896] Layer signature_rnn_0 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).\r\nW0731 09:54:25.987287 139687813953280 network.py:896] Layer signature_rnn_1 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).\r\nW0731 09:54:25.987648 139687813953280 network.py:896] Layer signature_rnn_2 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).\r\nW0731 09:54:35.721126 139687813953280 deprecation.py:323] From /usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:468: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nApply a constraint manually following the optimizer update step.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: ExpandDims:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 110, in <module>\r\n    main()\r\n  File \"run.py\", line 77, in main\r\n    train_model.fit(train_dataset, callbacks=callbacks)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 724, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 681, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 298, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\", line 836, in execution_function\r\n    return [out.numpy() for out in distributed_function(input_fn)]\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 451, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1765, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1089, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1167, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 471, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 75, in quick_execute\r\n    \"tensors, but found {}\".format(keras_symbolic_tensors))\r\ntensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=bool>]\r\n\r\n```\r\n", "comments": ["@ChuangLee ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31181, "title": "C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS + docker\r\n- All other messages: using the following docker image to build tensorflow from source:\r\n```\r\ntensorflow/tensorflow\r\ndevel-py3\r\n34334eb7f043\r\n14 hours ago\r\n1.82GB\r\n```\r\n\r\n**Describe the problem**\r\n```\r\nERROR: /tensorflow_src/tensorflow/core/BUILD:2954:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1810.589s, Critical Path: 798.42s\r\nINFO: 5095 processes: 5095 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nAnd I couldn't find the file ```///usr/share/doc/gcc-7/README.Bugs``` ...\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n", "comments": ["@tensorflow-jenkins ", "@jiarenyf \r\nJust to verify did you get chance to follow instructions from [TensorFlow ](https://www.tensorflow.org/install/source)website .Please, let us know. Thanks!", "@ravikyram Yes, I follow the instruction [here](https://www.tensorflow.org/install/source#cpu-only_2). \r\n\r\nAnd get the error:\r\n```\r\nERROR: /tensorflow_src/tensorflow/core/BUILD:2954:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 4)\r\n```\r\n", "@jiarenyf \r\nPlease, let us know which version of TensorFlow you are trying to install.Thanks!", "@ravikyram I am trying to install the master-version.", "@ravikyram In fact, I can build from source when using docker image on a ubuntu18.04 machine; when I change to a mac OS machine (using docker image), the error occurs.\r\n\r\n```\r\nERROR: /root/tf_cpu/tensorflow/core/BUILD:2856:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 4): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/23867514bb253e4e74c979f371ebd15c/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/host/bin/tensorflow/core/_objs/framework_internal_impl/batch_util.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/_objs/framework_internal_impl/batch_util.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/util/batch_util.cc -o bazel-out/host/bin/tensorflow/core/_objs/framework_internal_impl/batch_util.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 377.764s, Critical Path: 337.17s\r\nINFO: 84 processes: 84 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "I also try building from source in the docker image: ubuntu:16.04 or ubuntu18.04 on max OS according to the instruction [here](https://www.tensorflow.org/install/source#setup_for_linux_and_macos).\r\n\r\nIn both cases, the error is the same: \r\n```\r\nC++ compilation of rule '***' failed (Exit 4): gcc failed: error executing command \r\n```\r\n\r\nIn far, I have only sucessfully built `tf_cpu=r1.14` on ubuntu system with docker image ubuntu:18.04.\r\nI wonder why I cannot built from source in the docker image over mac OS ?", "...", "I hate tensorflow, so difficult to use and so many bugs and no one cares about or fixes the bugs ...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31181\">No</a>\n", "`gcc: internal compiler error: Killed (program cc1plus)` means the error comes from the compiler itself. Most likely you ran out of memory on your system and the compiler was killed by the oomkiller.\r\n\r\nWould have been much more instructive and helpful to provide a full log of the compilation process.", "@jiarenyf, this might be an issue with the amount of memory Docker for Mac makes available to the container process. Would you mind checking the [\"advanced\" tab](https://docs.docker.com/docker-for-mac#advanced) of the Docker for Mac Preferences screen? \r\n\r\nTF needs a lot of resources to compile, so the default resource allocation of 2GB might be causing the compiler to quit.", "Was hitting this issue with docker/Windows, 2GB is not enough.  From @angersson's comment, I bumped it to 32GB and no longer hit this error.", "I am having the exact same issue here. With Docker on Mac OS. I increased docker's memory to 4GB and still having the error. "]}, {"number": 31180, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXP, FILL, GATHER, GREATER_EQUAL, LEAKY_RELU, LOGISTIC, MAX_POOL_2D, MUL, PACK, REDUCE_MIN, REDUCE_PROD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SQUEEZE, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: NonMaxSuppressionV3, Round, Where", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@tranvangaohd1994 Provide the exact sequence of commands / steps that you executed before running into the problem and also provide the Tensorflow version. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi @gadagashwini. I do object detection using yolo-tiny. I want to post-processing into model, for example non_max_suppression. So I using Lambda layer like this :\r\n\r\n`\r\nmodel_Cal = Lambda(yolo_post_processing, output_shape=(None,6), name='yolo_post_processing',\r\n            arguments={'anchors': self.anchors, 'num_classes': 80})(self.yolo_model.output) \r\n\r\nself.yolo_model = Model(self.yolo_model.input, model_Cal)\r\n`\r\n\r\nIn function **yolo_post_processing**(), I do post_processing to choose best boxes. In this function I use **tf.image.non_max_suppression** or **tf.boolean_mask**\r\n\r\n`\r\nclass_boxes = tf.boolean_mask(boxes, mask[:, c])\r\n\r\nnms_index = tf.image.non_max_suppression(\r\n            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\r\n`\r\n\r\nI build model and run it on PC, It's OK. It's work very well. \r\nBut when I convert it to model TFLite to use in Android by this code : \r\n\r\n`\r\nfrom tensorflow.contrib import lite\r\n    \r\nconverter = lite.TFLiteConverter.from_keras_model_file('model_data/md_custom_op.h5', input_shapes = {'input_1':[1,416,416,3]})\r\n    \r\ntfmodel = converter.convert()\r\n`\r\nI got that error **Here is a list of operators for which you will need custom implementations: NonMaxSuppressionV3, Round, Where**.\r\n\r\nI understand that I need to custom operators what don't support by TFLite. But I can't search anyone do this work in python code.  \r\n\r\nIf they are,please sent me a example about **custom operators**\r\n", "> Hi @gadagashwini. I do object detection using yolo-tiny. I want to post-processing into model, for example non_max_suppression. So I using Lambda layer like this :\r\n> \r\n> `\r\n> model_Cal = Lambda(yolo_post_processing, output_shape=(None,6), name='yolo_post_processing',\r\n> arguments={'anchors': self.anchors, 'num_classes': 80})(self.yolo_model.output)\r\n> \r\n> self.yolo_model = Model(self.yolo_model.input, model_Cal)\r\n> `\r\n> \r\n> In function **yolo_post_processing**(), I do post_processing to choose best boxes. In this function I use **tf.image.non_max_suppression** or **tf.boolean_mask**\r\n> \r\n> `\r\n> class_boxes = tf.boolean_mask(boxes, mask[:, c])\r\n> \r\n> nms_index = tf.image.non_max_suppression(\r\n> class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\r\n> `\r\n> \r\n> I build model and run it on PC, It's OK. It's work very well.\r\n> But when I convert it to model TFLite to use in Android by this code :\r\n> \r\n> `\r\n> from tensorflow.contrib import lite\r\n> \r\n> converter = lite.TFLiteConverter.from_keras_model_file('model_data/md_custom_op.h5', input_shapes = {'input_1':[1,416,416,3]})\r\n> \r\n> tfmodel = converter.convert()\r\n> `\r\n> I got that error **Here is a list of operators for which you will need custom implementations: NonMaxSuppressionV3, Round, Where**.\r\n> \r\n> I understand that I need to custom operators what don't support by TFLite. But I can't search anyone do this work in python code.\r\n> \r\n> If they are,please sent me a example about **custom operators**\r\n\r\nhi,\r\n\r\nyou can add \"converter.allow_custom_ops=True\" before \"tflite_model = converter.convert()\". This is working for me.", "@tranvangaohd1994, Please let us know if the @extra159's workaround helps this issue. Thanks!", "Hi extra159, thanks your attention.\r\nIf I add \"converter.allow_custom_ops=True\", it would convert successfully. But Have you try to run model_tflite after convert?? It's going to error not found operator tf.Where, etc . You should try!!\r\nI had to divide into 2 model, one can convert to tflife (output is 3 feature map of CNN-yolo) , one is model_pb to run function tf.image.non_max_suppression() to choose best boxes. That is my solution. Thank you\r\n"]}, {"number": 31179, "title": "Make swish an inlinable function", "body": "The implementation of swish currently uses a Defun that specifies noinline=True for the purpose of forcing recomputation of the sigmoid(x) tensor, instead of incurring the memory overhead of keeping it alive from the forward pass.\r\n\r\nThis detail (unique within the codebase) is undesirable because non-inlined functions are difficult to deal with inside graph optimization passes. E.g., the layout optimizer in grappler is currently unable to optimize through a non-inlined function call.\r\n\r\nThis commit changes the implementation of swish to achieve the same force-recomputation effect using a control dependency instead of setting noinline=True, allowing the function to be inlined and fully optimized by the backend.\r\n\r\nAttn. @reedwm \r\ncc. @nluehr ", "comments": ["Please fix test errors"]}, {"number": 31178, "title": "Fix auto_mixed_precision for in-graph train loops", "body": "Models with an in-graph while loop around the training update were observed to produce incorrect results when auto_mixed_precision was enabled. This was due to Casts being inserted between (non-resource-) Variable read nodes and Enter nodes, which breaks the model behavior.\r\n\r\nThis commit fixes the issue by preventing Enter nodes from being converted to fp16 if they are fed from Variable read nodes.\r\n\r\nAlso adds a test that demonstrates the issue with an in-graph training loop.\r\n\r\nAttn. @reedwm \r\ncc. @nluehr ", "comments": ["It looks like the `//tensorflow/python:auto_mixed_precision_test_gpu` test for the \"Linux GPU\" check is failing.", "\"RuntimeError: Attempting to capture an EagerTensor without building a function.\"\r\nI don't know how to interpret that, and I can't reproduce the failure in my build. I'm not sure if this is because I'm running the tests differently, or because my branch is out of date. Any guesses as to which is more likely?", "I think this issue is occurring with TF2 only. Can you try running with TF 2?\r\n\r\nThis is probably a bug somewhere in TensorFlow. The `run_deprecated_v1` decorator only turns on graph mode, but still keeps the other TF 2 behavior I think. Somewhere an EagerTensor is probably being created despite graph mode being on.", "I pulled master branch and ran with \"bazel test --config=v2 ...\" but I still can't reproduce this test failure. Any suggestions?", "I will try debugging"]}, {"number": 31177, "title": "Fix BatchMatMul support in auto_mixed_precision", "body": "The use of CUDA_VERSION was not working because the file is not compiled as a CUDA source file. This commit changes it to use the cuda version property of the cluster devices instead.\r\n\r\nAlso adds a test to ensure BatchMatMul is correctly converted to fp16.\r\n\r\nAttn. @reedwm \r\ncc. @nluehr ", "comments": []}, {"number": 31176, "title": "Correct file paths in comments", "body": "A number of files in the `tensorflow/core/kernels/data/experimental` directory have out-of-date comments telling where to find the corresponding `REGISTER_OP` calls. This PR updates all the comments with the correct path.", "comments": []}, {"number": 31175, "title": "TF2.0 - memory leak caused by autograph retracing due to bound method argument", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 30\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.5.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nFactoring out a train step function into our custom model lead to a memory leak due to continuously retracing the full autograph epoch dataset loop.\r\nThe root cause was that bound instance methods are not identical `model.step is not model.step`.\r\n\r\n**Describe the expected behavior**\r\n\r\n*According to [Issue 36175: Identity of bound methods - Python tracker](https://bugs.python.org/issue36175) this is supposed to be expected behavior in Python.\r\nI merely want to clarify the autograph behaviour and document this for other tensorflow users.*\r\n\r\nI guess there might be reasons to use identity (`is`) comparison on arguments when deciding whether an autograph needs retracing, but if it were possible to use equality comparison (`==` ) for arguments that are methods this surprising behaviour could be avoided.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef run_epoch(dataset, step):\r\n    print('retrace')\r\n    for X in dataset:\r\n        step(X)\r\n\r\nclass Model:\r\n    def step(self, X):\r\n        return X * 2\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(list(range(128)))\r\nmodel = Model()\r\n\r\nfor i in range(20):\r\n    # leads to retrace of run_epoch due to non-identity model.step\r\n    # (i.e. `model.step is not model.step`)\r\n    run_epoch(dataset, model.step)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nPossible workarounds:\r\n- store the bound method in a variable and thus keep using the same instance\r\n  `step_fn = model.step`\r\n- declare the method as `@staticmethod` and pass the model instance explicitly as first parameter\r\n", "comments": ["I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1KpbCu6lgchU1A_zWcYc62m6Mxkpopz4l) here.Thanks!", "@kkimdev @alextp FYI\r\n\r\nThis is a surprising and subtle behavior. I believe the proper fix would be to be resilient to this case, by comparing functions using the equality operator `==` instead of `is`:\r\n\r\n```\r\n>>> model.step is model.step\r\nFalse\r\n>>> model.step == model.step\r\nTrue\r\n```\r\n\r\nIn the mean time, another workaround I would recommend is to use the model object as argument, instead of methods:\r\n\r\n```\r\nfor i in range(20):\r\n    # passing the entire `model` avoids the retrace.\r\n    run_epoch(dataset, model)\r\n```", "I concur that the best workaround is to just pass the model. I don't think relying on equality comparison instead of identity comparison for functions is ideal since equality comparison is more expensive and this would create a harder-to-understand mental model.", "> I concur that the best workaround is to just pass the model.\r\n\r\nWell, our actual model has a train and eval step functions, and this refactoring was the result of reusing the same code for metrics computation. Passing both the model and a `@staticmethod` function is indeed the chosen workaround.\r\n\r\nThe performance argument seems mood, how would argument comparison play a measurable role w.r.t. executing an autograph function.\r\n\r\n```py\r\nimport timeit, types, tensorflow as tf\r\n\r\nclass Model:\r\n  def step(self):\r\n    pass\r\n\r\nm = Model()\r\ntimeit.timeit('m.step is m.step', number=1_000_000, globals={'m': m})\r\ntimeit.timeit('m.step == m.step', number=1_000_000, globals={'m': m})\r\n# still fairly fast\r\ntimeit.timeit('var == var if type(var) == types.MethodType else var is var', number=1_000_000, globals={'var': m.step, 'types': types})\r\n\r\n@tf.function\r\ndef graph(model):\r\n  pass\r\n\r\ntimeit.timeit('graph(m)', number=1_000, globals={'graph': graph, 'm': m})\r\n```\r\n\r\nBut as mentioned I'm not positive that working around this specific Python detail in TF would be worthwhile, merely wanted to document it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31175\">No</a>\n"]}, {"number": 31174, "title": "TF 1.14 Keras model throws exception when inputs are not the deepest nodes", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Titan V\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nTensorFlow 1.14 introduced a bug in Keras models.  The specific change is the introduction of these lines:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/a5120db4d6917f943176ef3c5bb938064604c761/tensorflow/python/keras/engine/network.py#L849-L850\r\n\r\nThey make the assumption that a model's inputs will be deeper than any other node.  When that is not true, any nodes whose depth is equal to or greater than the inputs does not get evaluated, producing an exception.\r\n\r\nThe problem is demonstrated by the following script.  It creates a model where an input and a variable have the same depth.  In TensorFlow 1.13 this runs correctly, but in 1.14 it throws an exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 17, in <module>\r\n    print(model(model.inputs))\r\n  File \"/home/peastman/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/peastman/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 751, in call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\n  File \"/home/peastman/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 903, in _run_internal_graph\r\n    assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)\r\nAssertionError: Could not compute output Tensor(\"add/add:0\", shape=(?, 1), dtype=float32)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nCalling the model should return a Tensor, not throw an exception.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras.layers as layers\r\n\r\nclass Variable(layers.Layer):\r\n\r\n  def __init__(self, initial_value, **kwargs):\r\n    super(Variable, self).__init__(**kwargs)\r\n    self.var = tf.Variable(initial_value)\r\n\r\n  def call(self, inputs):\r\n    return self.var\r\n\r\nvar = Variable([1.0])([])\r\ninput = layers.Input(shape=(1,))\r\noutput = layers.Add()([input, var])\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\nprint(model(model.inputs))\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["After searching for a workaround, I've concluded this bug is more severe than I originally thought.  It doesn't actually matter where in the tree the `Variable` layer appears.  All that matters is that it takes no inputs, but is not itself an `Input` layer.  Any layer with those properties anywhere in a model produces the exception.", "This issue exists tf-nightly version 1.15.0-dev20190816", "@peastman You'll have to make a Layer that creates a Variable and adds it to input, it's not possible to have a Keras Layer that takes no inputs in the Functional API", "In what sense is it not possible?  It worked perfectly up through TF 1.13.", "Note that the sample script is just a minimal reproduction.  The real models where I'm encountering this error are much more complicated.  But generally speaking, it's very common for computation graphs to have branches that don't depend on any inputs, just on variables.", "@peastman Closing this issue as it is the intended behaviour.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31174\">No</a>\n"]}, {"number": 31172, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): no idea, likely binary\r\n- TensorFlow version: 2.0.0b1\r\n- Python version: Python 3.7.4\r\n- Installed using pip\r\n- CUDA/cuDNN version: using CPU\r\n- GPU model and memory: using CPU\r\n\r\n\r\n\r\nRunning the testing in the object detection API sample models (@ https://github.com/tensorflow/models) gives me an error stating `ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed. Failed to load the native TensorFlow runtime.`\r\n\r\nAfter creating an virtual environment with `venv`, I ran:\r\n\r\n`git clone https://github.com/tensorflow/models.git`\r\n\r\n`cd models/research`\r\n\r\n`protoc object_detection/protos/*.proto --python_out=.`\r\n\r\n`python object_detection/builders/model_builder_test.py`, which resulted in the following error:\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 20, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nAny ideas on how to fix this error? Thanks for the help ahead of time!", "comments": ["From the template it looks like you are installing **TensorFlow-GPU** (TF) prebuilt binaries.\n\n **TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.** \n\n* If you have above configuration and using _**Windows**_ platform -\n     * Try adding the CUDA, CUPTI, and cuDNNinstallation directories to the %PATH% environment variable. \n     * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n     * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n     * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n\nPlease let us know if this helps.", "I'm not using a GPU, so I thought I didn't need CUDA and cuDNN?", "From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries.\r\n\r\n**TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.**\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n* Try Google Colab to use TensorFlow.\r\n    * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.\r\n    * It has an added advantage since you can you easily switch to different hardware accelerators     \r\n      (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.\r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\nPlease let us know if this helps.", "Thanks @ymodak for the advice.\r\n\r\nI'm using this tutorial by sentdex by the way: https://www.youtube.com/watch?v=COlbP62-B-U\r\n\r\nI started by attempting to use Google Colab, but got an error with no solution I could find online:\r\n```\r\n/content/models/research\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 23, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/model_builder.py\", line 20, in <module>\r\n    from object_detection.builders import anchor_generator_builder\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/anchor_generator_builder.py\", line 22, in <module>\r\n    from object_detection.protos import anchor_generator_pb2\r\nImportError: cannot import name 'anchor_generator_pb2'\r\n```\r\n\r\nI also seemed to get the same error after running `pip install tensorflow==1.14`:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 20, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\nick desktop\\Desktop\\Projects\\MachineLearning\\ObjectDetection\\tensorflow-env-obj\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\nick desktop\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nso perhaps it's not a problem with TF >1.5?\r\n\r\nThanks for your help!", "You should be able to install TF version less than 1.6 perhaps TF 1.5 however that won't solve your current issue. \r\n- The reason being TF Object detection API requires TF 1.12 or above.\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md\r\n\r\n- You can try using google colab to execute object detection api however you need to setup the recommended environment.\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md", "I literally have the same problem, tried every combination of CUDA10.0/Python3.7/Python3.6/cuDNN4.2/cuDNN7.5/cuDNN7.6/tensorflow-gpu2.0.0b/tensorflow-gpu1.14.0 on win10\r\n\r\nHere it crashes upon import tensorflow already.\r\n\r\nRunning on a i7-860 + GTX 1060\r\n\r\nI eventually gave up on tensorflow-gpu and doing it in the browser now with tfjs.", "Sorry, I forgot to ask, are there any sample google colabs that run the object detection API? I followed all the instructions, here's my setup but still gives an error. https://colab.research.google.com/drive/1m6Cw38ldCMndeuIOwoMExd4EBxkDOkc4", "cc TheNewSound Sorry to hear about this. Does your CPU support AVX instructions sets?\r\n-*-*-*-*-\r\ncc nicholas-2\r\nSure, you can take a look at this;\r\nhttps://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb\r\nI will close this issue now that original query has been answered. Feel free to follow up if need be. We can reopen the issue if required. Thanks!", "> cc TheNewSound Sorry to hear about this. Does your CPU support AVX instructions sets?\r\n> -_-_-_-_-\r\n> cc nicholas-2\r\n> Sure, you can take a look at this;\r\n> https://github.com/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb\r\n> I will close this issue now that original query has been answered. Feel free to follow up if need be. We can reopen the issue if required. Thanks!\r\n\r\nI think it doesnt support AVX, could that be an issue?", "Yes that is true.\r\n\r\n> **TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.**\r\n> Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n\r\n"]}, {"number": 31171, "title": "404 Error during oxford-pets tutorial dataset loading", "body": "**System information**\r\n- OS Platform and Distribution (Windows 10):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version: GPU 2.0 beta\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: nvidia GTX 1050 4gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nproblematic tutorial: https://www.tensorflow.org/beta/tutorials/images/segmentation\r\nIn the segmentation tutorial, with the copied code from the tutorial, when loading oxford pet dataset from tensorflow_datasets I get:\r\ntensorflow_datasets.core.download.downloader.DownloadError: Failed to get url http://www.robots.ox.ac.uk/~vgg/data/pets/data\\images.tar.gz. HTTP code: 404.\r\n\r\nI can  go to the site with the web browser with no problems (and even download the dataset).\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nfrom tensorflow_examples.models.pix2pix import pix2pix\r\nimport tensorflow_datasets as tfds\r\ntfds.disable_progress_bar()\r\nfrom IPython.display import clear_output\r\nimport matplotlib.pyplot as plt\r\n#problematic line:\r\ndataset, info = tfds.load('oxford_iiit_pet:3.0.0', with_info=True)\r\n\r\n\r\n**Any other info / logs**\r\ntensorflow_datasets.core.download.downloader.DownloadError: Failed to get url http://www.robots.ox.ac.uk/~vgg/data/pets/data\\images.tar.gz. HTTP code: 404.\r\n\r\nAm i doing something wrong, or the site structure changed?", "comments": ["@janprz I tried executing the code on Colab but i didn't see any error message. It successfully downloaded the dataset. Please look at the below output.\r\n```\r\nDataset oxford_iiit_pet downloaded and prepared to /root/tensorflow_datasets/oxford_iiit_pet/3.0.0. Subsequent calls will reuse this data.\r\nW0731 08:24:33.067447 140505681938304 dataset_builder.py:397] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\r\n``` \r\nCould you please try once and let us know. Thanks!", "> @janprz I tried executing the code on Colab but i didn't see any error message. It successfully downloaded the dataset. Please look at the below output.\r\n> \r\n> ```\r\n> Dataset oxford_iiit_pet downloaded and prepared to /root/tensorflow_datasets/oxford_iiit_pet/3.0.0. Subsequent calls will reuse this data.\r\n> W0731 08:24:33.067447 140505681938304 dataset_builder.py:397] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\r\n> ```\r\n> \r\n> Could you please try once and let us know. Thanks!\r\n\r\nHello, I have tried again, but the problem still exists. I am connected to the internet so I have no idea why I am getting 404, because I haven't edited any url path or something.", "I have also tried connecting through the other internet provider with same effect.\r\nUsing Colab it worked, but why I can't do it locally on my pc? I want to work later with little or no internet on holidays.\r\nI have no problems with downloading other datasets like mnist.", "You can download the data from this link\r\nhttps://www.robots.ox.ac.uk/~vgg/data/pets/\r\nLater you can use ```tf.keras.utils.get_file``` to load the dataset.\r\nSee,\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file", "Thanks a lot for the answers,  I have already downloaded it myself manually.\r\nHowever it is interesting that only this dataset cannot be downloaded by me with tfds.load().\r\n\r\nSince the problem isn't resolved I shouldn't close the issue?", "Hi,\r\nI've got the exact same problem with a similar setup.\r\n\r\n    OS Platform and Distribution (Windows 10):\r\n    TensorFlow version: GPU 2.0 beta\r\n    Python version: 3.6\r\n    Installed using virtualenv? pip? conda?: pip\r\n    CUDA/cuDNN version: 10.0/7.6\r\n    GPU model and memory: nvidia GTX 1060 6gb\r\n\r\n", " Can you try with TF 2.0 nightly version?\r\n```!pip install tf-nightly-2.0-preview```", "> \r\n> \r\n> Can you try with TF 2.0 nightly version?\r\n> `!pip install tf-nightly-2.0-preview`\r\n\r\nThanks for your answer,\r\nbut I tried it and I still get the same error", "@rsepassi Can you please take a look?\r\nhttps://www.tensorflow.org/beta/tutorials/images/segmentation\r\nApparently windows users cannot download this particular dataset with,\r\n```python\r\ndataset, info = tfds.load('oxford_iiit_pet:3.0.0', with_info=True)\r\n```", "im having the same problem ", "I'm having the same problem I do these lines : \r\ndataset = tf.keras.utils.get_file(\"images.tar.gz\",\r\n\"file://C:/Users/Sylvain ARD/Desktop/test segmentation semantique/images.tar.gz\",extract=True)\r\nin place of : \r\ndataset, info = tfds.load('oxford_iiit_pet:3.0.0', with_info=True)\r\n\r\nbut how to get the info please ??\r\nthank you !\r\nbye", "Yes, it is still the issue. I have tried yesterday. No difference.\n\n\u015br., 28 sie 2019 o 14:39 Alfred Sorten Wolf <notifications@github.com>\nnapisa\u0142(a):\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31171?email_source=notifications&email_token=AJGJHH6AFZZKDASWHLVK7H3QGZWZ3A5CNFSM4IH72TN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5K6T5A#issuecomment-525724148>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJGJHH3PKKJQ2H7FQESS4BTQGZWZ3ANCNFSM4IH72TNQ>\n> .\n>\n", "Same here...\r\nI can download the dataset with `tf.keras.utils.get_file`, or else use another dataset from tfds, but then the structure is not the same as in the tutorial, which makes it harder to work on.\r\n\r\nIt works fine on colab, but it doesn't on windows.\r\n\r\nIt must be a classic Windows/Linux path incompatibility, since this is the error.  \r\n`Failed to get url http://www.robots.ox.ac.uk/~vgg/data/pets/data\\images.tar.gz. HTTP code: 404.`\r\n\r\nOne could probably edit the code to make it work, but for now I just want to learn how to segment with tensorflow (in order to use it on my own datasets), so I'll just stick to colab. I'm still interested if the issue is fixed!", "Hi, the problem is caused by the os.path.join function on Windows.\r\nOpen tensorflow_datasets / image / oxford_iiit_pet.py and replace these lines:\r\n84 replace with `url = _BASE_URL + \"/images.tar.gz\",`\r\n87 replace with `url = _BASE_URL + \"/annotations.tar.gz\",`\r\n\r\n91 replace with ` images_path_dir = dl_paths[\"images\"] + \"/images\"`\r\n92 replace with `annotations_path_dir = dl_paths[\"annotations\"] + \"/annotations\"`\r\n\r\nI hope it helps you", "> Hi, the problem is caused by the os.path.join function on Windows.\r\n> Open tensorflow_datasets / image / oxford_iiit_pet.py and replace these lines:\r\n> 84 replace with `url = _BASE_URL + \"/images.tar.gz\",`\r\n> 87 replace with `url = _BASE_URL + \"/annotations.tar.gz\",`\r\n> \r\n> 91 replace with ` images_path_dir = dl_paths[\"images\"] + \"/images\"`\r\n> 92 replace with `annotations_path_dir = dl_paths[\"annotations\"] + \"/annotations\"`\r\n> \r\n> I hope it helps you\r\n\r\nJust checked this solution on my windows machine. This will give an error:\r\n...\\tensorflow_datasets\\downloads\\robots.ox.ac.uk_vgg_pets_imageswMR1o1DWRq_DHWToagdXedb7P88RHpceK3WqG77VVwU.tar.gz.tmp.e6a1467c6d6942f8824208990307191b\\images.tar.gz, has wrong checksum.", "only replaced `84` and `87` lines in `tensorflow_datasets/image/oxford_iiit_pet.py` and dataset have been installed successfully. 91 and 92 lines are correct.\r\n```python\r\n    dl_paths = dl_manager.download_and_extract({\r\n        \"images\": tfds.download.Resource(\r\n            url= \"{}/images.tar.gz\".format(_BASE_URL),  # <-- this line changed\r\n            extract_method=tfds.download.ExtractMethod.TAR),\r\n        \"annotations\": tfds.download.Resource(\r\n            url=\"{}/annotations.tar.gz\".format(_BASE_URL), # <-- this line changed\r\n            extract_method=tfds.download.ExtractMethod.TAR)\r\n    })\r\n```\r\n", "thank you\nSylvain Ard\n0549507724\n0778380991\nsylvain.ard@gmail.com\nhttp://sylvain-ard.fr\nEntreprise individuelle SIRET : 80079243400022\nAppt 26 B\u00e2t A R\u00e9sidence Le Patio\n83 rue de la Bugellerie\n86000 Poitiers\n\n\nLe mar. 8 oct. 2019 \u00e0 22:04, Sergey Yanzin <notifications@github.com> a\n\u00e9crit :\n\n> only replaced 84 and 87 lines in\n> tensorflow_datasets/image/oxford_iiit_pet.py and dataset have been\n> installed successfully\n>\n>     dl_paths = dl_manager.download_and_extract({\n>         \"images\": tfds.download.Resource(\n>             url= \"{}/images.tar.gz\".format(_BASE_URL),  # <-- this line changed\n>             extract_method=tfds.download.ExtractMethod.TAR),\n>         \"annotations\": tfds.download.Resource(\n>             url=\"{}/annotations.tar.gz\".format(_BASE_URL), # <-- this line changed\n>             extract_method=tfds.download.ExtractMethod.TAR)\n>     })\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31171?email_source=notifications&email_token=AEWZCR54SZ7M2ROROGECS23QNTRVFA5CNFSM4IH72TN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAVM7LA#issuecomment-539676588>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEWZCRZGKDRWBWR4EF7K4UDQNTRVFANCNFSM4IH72TNQ>\n> .\n>\n", "Hi,\r\nhave you ever encountered problem of \"NonMatchingChecksumError: file has wrong chencksum\" when \r\ndownloading the oxford-iiit-pet dataset\uff1f I use tfds.load( ) ", "@janprz \r\nplease let us know if the issue still persist  ", "@janprz \r\nplease update on the above comment", "In fact I got this problem recently, my **System Information** as below:\r\n\r\n- mac OSX Catalina 10.15.3\r\n- python 3.7 with virtualenv\r\n- coding in Jupyter notebook\r\n- tensorflow 2.1.0\r\n- tensorflow_datasets 2.1.0\r\n\r\nAfter I run the code:\r\n```\r\ntfds.load('oxford_iiit_pet:3.1.0')\r\n```\r\nI got the raised error:\r\n```\r\n...images.tar.gz, has wrong checksum.\r\n```\r\nI ran the load several times and tried the method above mentioned, it still downloaded the file with different size from its claim. I checked the file size and compared it with the version downloaded from [official site](https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz). It looks like the ```tfds``` downloaded the incomplete one with the different size and sha 256 checksum, but it do looks normal and can be loaded by the code.\r\n\r\nThus there are two options to solve this:\r\n1. Modify and pass the download_config parameter.**(Recommended)**\r\n  ```python\r\n  builder = tfds.builder('oxford_iiit_pet:3.1.0')\r\n  info = builder.info\r\n  print(info)\r\n  # by setting register_checksums as True to pass the check\r\n  config = tfds.download.DownloadConfig(register_checksums = True)\r\n  builder.download_and_prepare(download_config=config)\r\n  dataset = builder.as_dataset()\r\n  ```\r\n2. Modify the ```tensorflow_datasets/url_checksums/oxford_iiit_pet.txt``` file\r\n  Replace the sha 256 checksums and the size numbers with the downloaded files. ", "For me, the problem is solved. Thank You!\n\n\u015br., 31 lip 2019 o 10:36 gadagashwini <notifications@github.com> napisa\u0142(a):\n\n> @janprz <https://github.com/janprz> I tried executing the code on Colab\n> but i didn't see any error message. It successfully downloaded the dataset.\n> Please look at the below output.\n>\n> Dataset oxford_iiit_pet downloaded and prepared to /root/tensorflow_datasets/oxford_iiit_pet/3.0.0. Subsequent calls will reuse this data.\n> W0731 08:24:33.067447 140505681938304 dataset_builder.py:397] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n>\n> Could you please try once and let us know. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31171?email_source=notifications&email_token=AJGJHH7IVRWHYTND6PMNVV3QCFFIVA5CNFSM4IH72TN2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3GQCMQ#issuecomment-516751666>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJGJHH5GM726JBXXCLXOH7TQCFFIVANCNFSM4IH72TNQ>\n> .\n>\n", "> ```python\r\n> builder\r\n> ```\r\n\r\n666", "> Hi,\r\n> have you ever encountered problem of \"NonMatchingChecksumError: file has wrong chencksum\" when\r\n> downloading the oxford-iiit-pet dataset\uff1f I use tfds.load( )\r\n\r\nYes. Same issue here", "@rsepassi \r\nAs confirmed by you that it is resolved for you, let us know if we may move this to closed status.", "Tensorflow updated this dataset to tfds nightly. \"pip install -q -U tfds-nightly\"\r\nUse this command. Now you're free to use", "error download_manager.py\r\n\r\n        if self._register_checksums:\r\n            self._record_sizes_checksums()\r\n        elif (dl_size, sha256) != self._sizes_checksums.get(resource.url, None):\r\n            raise NonMatchingChecksumError(resource.url, tmp_path)", "Downloading and preparing dataset oxford_iiit_pet/3.2.0 (download: 773.52 MiB, generated: 774.69 MiB, total: 1.51 GiB) to /root/tensorflow_datasets/oxford_iiit_pet/3.2.0...\r\nDl Completed...: 0%\r\n0/2 [00:32<?, ? url/s]\r\nDl Size...:\r\n0/0 [00:31<?, ? MiB/s]\r\nExtraction completed...:\r\n0/0 [00:31<?, ? file/s]\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nTimeoutError                              Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/urllib3/connection.py in _new_conn(self)\r\n    158             conn = connection.create_connection(\r\n--> 159                 (self._dns_host, self.port), self.timeout, **extra_kw)\r\n    160 \r\n\r\n38 frames\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNewConnectionError                        Traceback (most recent call last)\r\nNewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f343b58e470>: Failed to establish a new connection: [Errno 110] Connection timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nMaxRetryError                             Traceback (most recent call last)\r\nMaxRetryError: HTTPConnectionPool(host='www.robots.ox.ac.uk', port=80): Max retries exceeded with url: /~vgg/data/pets/data/annotations.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f343b58e470>: Failed to establish a new connection: [Errno 110] Connection timed out',))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConnectionError                           Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)\r\n    514                 raise SSLError(e, request=request)\r\n    515 \r\n--> 516             raise ConnectionError(e, request=request)\r\n    517 \r\n    518         except ClosedPoolError as e:\r\n\r\nConnectionError: HTTPConnectionPool(host='www.robots.ox.ac.uk', port=80): Max retries exceeded with url: /~vgg/data/pets/data/annotations.tar.gz (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f343b58e470>: Failed to establish a new connection: [Errno 110] Connection timed out',))", "@janprz Closing this issue since the issue is resolved  for you.\r\n\r\n@x5675602 Please  submit a new issue and we can track it seperately. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31171\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31171\">No</a>\n"]}, {"number": 31170, "title": "THIS IS A TEST CHERRYPICK. PLEASE DO NOT MERGE. Fix minor typo in error message in optimizer_v2.py.", "body": "PiperOrigin-RevId: 260734042", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31170) for more info**.\n\n<!-- need_sender_cla -->", "Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 31169, "title": "Tensorflow Keras load model", "body": "I have a keras model which I'm able to load with Keras (with Tensorflow Backend). Tensorflow Keras however is not able to load that model, which makes it impossible to use `TFLiteConverter.from_keras_model_file`\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04 64-bit\r\n- TensorFlow installed from (source or binary): binary (CPU)\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Keras Version: 2.2.4\r\n- Python version: 3.7.3\r\n\r\n**Describe the behavior**\r\nThe model works fine with Keras\r\n```python\r\nfrom keras.models import load_model\r\n\r\ninp = np.random.standard_normal([2, 1, 128, 128]).astype(np.float32)\r\nm29v2 = load_model(\"model_lcnn_29v2.h5\")\r\nprint(m29v2.predict(inp)[0].shape)\r\n```\r\nbut it does not work with Tensorflow Keras:\r\n```python\r\nfrom tensorflow.keras.models import load_model\r\n\r\ninp = np.random.standard_normal([2, 1, 128, 128]).astype(np.float32)\r\nm29v2 = load_model(\"model_lcnn_29v2.h5\")\r\nprint(m29v2.predict(inp)[0].shape)\r\n```\r\nI will get following exception:\r\n```\r\n    m29v2 = load_model(\"demo_data/model_lcnn_29v2.h5\")\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 212, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/layers/serialization.py\", line 89, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 192, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1131, in from_config\r\n    process_node(layer, node_data)\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1087, in process_node\r\n    layer(flat_input_tensors[0], **kwargs)\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 591, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1881, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/home/paul/anaconda3/envs/openvino/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\", line 1005, in build\r\n    raise ValueError('The last dimension of the inputs to `Dense` '\r\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\r\n```\r\n\r\nBtw other loading other keras models work with Tensorflow Keras but that one for some reason not.\r\n\r\n[Link to problematic model](https://drive.google.com/open?id=1dDEvk0dvjv4DveaI7IWT91_BSYa0fJ8l)", "comments": ["@paulbauriegel Can you please share the code snippet of the whole model so that we can get take a look at it. Thanks!", "@gowtham-kp I extracted you the code from the h5 model, I hope it helps. Depending on the import `import tensorflow.keras ...` vs `import keras ...` the code should work or not work\r\n\r\n<details>\r\n<summary>Code</summary>\r\n\r\n```python\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef target_layer(x,  axis,  start_i,  end_i):\r\n    slices = [slice(None, None)] * len(K.int_shape(x))\r\n    slices[axis] = slice(start_i, end_i)\r\n    return x[tuple(slices)]\r\n\r\n\r\nx0_1 = layers.Input(shape=(1, 128, 128), name='0')\r\nx62_pad = layers.ZeroPadding2D(name='62_pad', padding=((2, 2), (2, 2)), data_format='channels_first')(x0_1)\r\nx62 = layers.Conv2D(name='62', filters=96, kernel_size=(5, 5), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x62_pad)\r\nx63 = layers.Lambda(name='63', function=lambda x: target_layer(x, 1, 0, 48))(x62)\r\nx64 = layers.Lambda(name='64', function=lambda x: target_layer(x, 1, 48, 96))(x62)\r\nx65 = layers.Maximum(name='65')([x63, x64])\r\nx67 = layers.ZeroPadding2D(name='67', padding=((0, 0), (0, 0)), data_format='channels_first')(x65)\r\nx66_pad = layers.ZeroPadding2D(name='66_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x65)\r\nx68_pad = layers.ZeroPadding2D(name='68_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x67)\r\nx66 = layers.MaxPooling2D(name='66', padding='valid', strides=(2, 2), data_format='channels_first')(x66_pad)\r\nx68 = layers.AveragePooling2D(name='68', padding='valid', strides=(2, 2), data_format='channels_first')(x68_pad)\r\nx69 = layers.Add(name='69')([x66, x68])\r\nx70_pad = layers.ZeroPadding2D(name='70_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x69)\r\nx70 = layers.Conv2D(name='70', filters=96, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x70_pad)\r\nx71 = layers.Lambda(name='71', function=lambda x: target_layer(x, 1, 0, 48))(x70)\r\nx72 = layers.Lambda(name='72', function=lambda x: target_layer(x, 1, 48, 96))(x70)\r\nx73 = layers.Maximum(name='73')([x71, x72])\r\nx74_pad = layers.ZeroPadding2D(name='74_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x73)\r\nx74 = layers.Conv2D(name='74', filters=96, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x74_pad)\r\nx75 = layers.Lambda(name='75', function=lambda x: target_layer(x, 1, 0, 48))(x74)\r\nx76 = layers.Lambda(name='76', function=lambda x: target_layer(x, 1, 48, 96))(x74)\r\nx77 = layers.Maximum(name='77')([x75, x76])\r\nx78 = layers.Add(name='78')([x77, x69])\r\nx79 = layers.Conv2D(name='79', filters=96, kernel_size=(1, 1), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x78)\r\nx80 = layers.Lambda(name='80', function=lambda x: target_layer(x, 1, 0, 48))(x79)\r\nx81 = layers.Lambda(name='81', function=lambda x: target_layer(x, 1, 48, 96))(x79)\r\nx82 = layers.Maximum(name='82')([x80, x81])\r\nx83_pad = layers.ZeroPadding2D(name='83_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x82)\r\nx83 = layers.Conv2D(name='83', filters=192, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x83_pad)\r\nx84 = layers.Lambda(name='84', function=lambda x: target_layer(x, 1, 0, 96))(x83)\r\nx85 = layers.Lambda(name='85', function=lambda x: target_layer(x, 1, 96, 192))(x83)\r\nx86 = layers.Maximum(name='86')([x84, x85])\r\nx88 = layers.ZeroPadding2D(name='88', padding=((0, 0), (0, 0)), data_format='channels_first')(x86)\r\nx87_pad = layers.ZeroPadding2D(name='87_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x86)\r\nx89_pad = layers.ZeroPadding2D(name='89_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x88)\r\nx87 = layers.MaxPooling2D(name='87', padding='valid', strides=(2, 2), data_format='channels_first')(x87_pad)\r\nx89 = layers.AveragePooling2D(name='89', padding='valid', strides=(2, 2), data_format='channels_first')(x89_pad)\r\nx90 = layers.Add(name='90')([x87, x89])\r\nx91_pad = layers.ZeroPadding2D(name='91_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x90)\r\nx91 = layers.Conv2D(name='91', filters=192, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x91_pad)\r\nx92 = layers.Lambda(name='92', function=lambda x: target_layer(x, 1, 0, 96))(x91)\r\nx93 = layers.Lambda(name='93', function=lambda x: target_layer(x, 1, 96, 192))(x91)\r\nx94 = layers.Maximum(name='94')([x92, x93])\r\nx95_pad = layers.ZeroPadding2D(name='95_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x94)\r\nx95 = layers.Conv2D(name='95', filters=192, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x95_pad)\r\nx96 = layers.Lambda(name='96', function=lambda x: target_layer(x, 1, 0, 96))(x95)\r\nx97 = layers.Lambda(name='97', function=lambda x: target_layer(x, 1, 96, 192))(x95)\r\nx98 = layers.Maximum(name='98')([x96, x97])\r\nx99 = layers.Add(name='99')([x98, x90])\r\nx100_pad = layers.ZeroPadding2D(name='100_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x99)\r\nx100 = layers.Conv2D(name='100', filters=192, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x100_pad)\r\nx101 = layers.Lambda(name='101', function=lambda x: target_layer(x, 1, 0, 96))(x100)\r\nx102 = layers.Lambda(name='102', function=lambda x: target_layer(x, 1, 96, 192))(x100)\r\nx103 = layers.Maximum(name='103')([x101, x102])\r\nx104_pad = layers.ZeroPadding2D(name='104_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x103)\r\nx104 = layers.Conv2D(name='104', filters=192, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x104_pad)\r\nx105 = layers.Lambda(name='105', function=lambda x: target_layer(x, 1, 0, 96))(x104)\r\nx106 = layers.Lambda(name='106', function=lambda x: target_layer(x, 1, 96, 192))(x104)\r\nx107 = layers.Maximum(name='107')([x105, x106])\r\nx108 = layers.Add(name='108')([x107, x99])\r\nx109 = layers.Conv2D(name='109', filters=192, kernel_size=(1, 1), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x108)\r\nx110 = layers.Lambda(name='110', function=lambda x: target_layer(x, 1, 0, 96))(x109)\r\nx111 = layers.Lambda(name='111', function=lambda x: target_layer(x, 1, 96, 192))(x109)\r\nx112 = layers.Maximum(name='112')([x110, x111])\r\nx113_pad = layers.ZeroPadding2D(name='113_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x112)\r\nx113 = layers.Conv2D(name='113', filters=384, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x113_pad)\r\nx114 = layers.Lambda(name='114', function=lambda x: target_layer(x, 1, 0, 192))(x113)\r\nx115 = layers.Lambda(name='115', function=lambda x: target_layer(x, 1, 192, 384))(x113)\r\nx116 = layers.Maximum(name='116')([x114, x115])\r\nx118 = layers.ZeroPadding2D(name='118', padding=((0, 0), (0, 0)), data_format='channels_first')(x116)\r\nx117_pad = layers.ZeroPadding2D(name='117_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x116)\r\nx119_pad = layers.ZeroPadding2D(name='119_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x118)\r\nx117 = layers.MaxPooling2D(name='117', padding='valid', strides=(2, 2), data_format='channels_first')(x117_pad)\r\nx119 = layers.AveragePooling2D(name='119', padding='valid', strides=(2, 2), data_format='channels_first')(x119_pad)\r\nx120 = layers.Add(name='120')([x117, x119])\r\nx121_pad = layers.ZeroPadding2D(name='121_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x120)\r\nx121 = layers.Conv2D(name='121', filters=384, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x121_pad)\r\nx122 = layers.Lambda(name='122', function=lambda x: target_layer(x, 1, 0, 192))(x121)\r\nx123 = layers.Lambda(name='123', function=lambda x: target_layer(x, 1, 192, 384))(x121)\r\nx124 = layers.Maximum(name='124')([x122, x123])\r\nx125_pad = layers.ZeroPadding2D(name='125_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x124)\r\nx125 = layers.Conv2D(name='125', filters=384, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x125_pad)\r\nx126 = layers.Lambda(name='126', function=lambda x: target_layer(x, 1, 0, 192))(x125)\r\nx127 = layers.Lambda(name='127', function=lambda x: target_layer(x, 1, 192, 384))(x125)\r\nx128 = layers.Maximum(name='128')([x126, x127])\r\nx129 = layers.Add(name='129')([x128, x120])\r\nx130_pad = layers.ZeroPadding2D(name='130_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x129)\r\nx130 = layers.Conv2D(name='130', filters=384, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x130_pad)\r\nx131 = layers.Lambda(name='131', function=lambda x: target_layer(x, 1, 0, 192))(x130)\r\nx132 = layers.Lambda(name='132', function=lambda x: target_layer(x, 1, 192, 384))(x130)\r\nx133 = layers.Maximum(name='133')([x131, x132])\r\nx134_pad = layers.ZeroPadding2D(name='134_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x133)\r\nx134 = layers.Conv2D(name='134', filters=384, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x134_pad)\r\nx135 = layers.Lambda(name='135', function=lambda x: target_layer(x, 1, 0, 192))(x134)\r\nx136 = layers.Lambda(name='136', function=lambda x: target_layer(x, 1, 192, 384))(x134)\r\nx137 = layers.Maximum(name='137')([x135, x136])\r\nx138 = layers.Add(name='138')([x137, x129])\r\nx139_pad = layers.ZeroPadding2D(name='139_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x138)\r\nx139 = layers.Conv2D(name='139', filters=384, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x139_pad)\r\nx140 = layers.Lambda(name='140', function=lambda x: target_layer(x, 1, 0, 192))(x139)\r\nx141 = layers.Lambda(name='141', function=lambda x: target_layer(x, 1, 192, 384))(x139)\r\nx142 = layers.Maximum(name='142')([x140, x141])\r\nx143_pad = layers.ZeroPadding2D(name='143_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x142)\r\nx143 = layers.Conv2D(name='143', filters=384, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x143_pad)\r\nx144 = layers.Lambda(name='144', function=lambda x: target_layer(x, 1, 0, 192))(x143)\r\nx145 = layers.Lambda(name='145', function=lambda x: target_layer(x, 1, 192, 384))(x143)\r\nx146 = layers.Maximum(name='146')([x144, x145])\r\nx147 = layers.Add(name='147')([x146, x138])\r\nx148 = layers.Conv2D(name='148', filters=384, kernel_size=(1, 1), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x147)\r\nx149 = layers.Lambda(name='149', function=lambda x: target_layer(x, 1, 0, 192))(x148)\r\nx150 = layers.Lambda(name='150', function=lambda x: target_layer(x, 1, 192, 384))(x148)\r\nx151 = layers.Maximum(name='151')([x149, x150])\r\nx152_pad = layers.ZeroPadding2D(name='152_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x151)\r\nx152 = layers.Conv2D(name='152', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x152_pad)\r\nx153 = layers.Lambda(name='153', function=lambda x: target_layer(x, 1, 0, 128))(x152)\r\nx154 = layers.Lambda(name='154', function=lambda x: target_layer(x, 1, 128, 256))(x152)\r\nx155 = layers.Maximum(name='155')([x153, x154])\r\nx156_pad = layers.ZeroPadding2D(name='156_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x155)\r\nx156 = layers.Conv2D(name='156', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x156_pad)\r\nx157 = layers.Lambda(name='157', function=lambda x: target_layer(x, 1, 0, 128))(x156)\r\nx158 = layers.Lambda(name='158', function=lambda x: target_layer(x, 1, 128, 256))(x156)\r\nx159 = layers.Maximum(name='159')([x157, x158])\r\nx160_pad = layers.ZeroPadding2D(name='160_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x159)\r\nx160 = layers.Conv2D(name='160', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x160_pad)\r\nx161 = layers.Lambda(name='161', function=lambda x: target_layer(x, 1, 0, 128))(x160)\r\nx162 = layers.Lambda(name='162', function=lambda x: target_layer(x, 1, 128, 256))(x160)\r\nx163 = layers.Maximum(name='163')([x161, x162])\r\nx164 = layers.Add(name='164')([x163, x155])\r\nx165_pad = layers.ZeroPadding2D(name='165_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x164)\r\nx165 = layers.Conv2D(name='165', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x165_pad)\r\nx166 = layers.Lambda(name='166', function=lambda x: target_layer(x, 1, 0, 128))(x165)\r\nx167 = layers.Lambda(name='167', function=lambda x: target_layer(x, 1, 128, 256))(x165)\r\nx168 = layers.Maximum(name='168')([x166, x167])\r\nx169_pad = layers.ZeroPadding2D(name='169_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x168)\r\nx169 = layers.Conv2D(name='169', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x169_pad)\r\nx170 = layers.Lambda(name='170', function=lambda x: target_layer(x, 1, 0, 128))(x169)\r\nx171 = layers.Lambda(name='171', function=lambda x: target_layer(x, 1, 128, 256))(x169)\r\nx172 = layers.Maximum(name='172')([x170, x171])\r\nx173 = layers.Add(name='173')([x172, x164])\r\nx174_pad = layers.ZeroPadding2D(name='174_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x173)\r\nx174 = layers.Conv2D(name='174', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x174_pad)\r\nx175 = layers.Lambda(name='175', function=lambda x: target_layer(x, 1, 0, 128))(x174)\r\nx176 = layers.Lambda(name='176', function=lambda x: target_layer(x, 1, 128, 256))(x174)\r\nx177 = layers.Maximum(name='177')([x175, x176])\r\nx178_pad = layers.ZeroPadding2D(name='178_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x177)\r\nx178 = layers.Conv2D(name='178', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x178_pad)\r\nx179 = layers.Lambda(name='179', function=lambda x: target_layer(x, 1, 0, 128))(x178)\r\nx180 = layers.Lambda(name='180', function=lambda x: target_layer(x, 1, 128, 256))(x178)\r\nx181 = layers.Maximum(name='181')([x179, x180])\r\nx182 = layers.Add(name='182')([x181, x173])\r\nx183_pad = layers.ZeroPadding2D(name='183_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x182)\r\nx183 = layers.Conv2D(name='183', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x183_pad)\r\nx184 = layers.Lambda(name='184', function=lambda x: target_layer(x, 1, 0, 128))(x183)\r\nx185 = layers.Lambda(name='185', function=lambda x: target_layer(x, 1, 128, 256))(x183)\r\nx186 = layers.Maximum(name='186')([x184, x185])\r\nx187_pad = layers.ZeroPadding2D(name='187_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x186)\r\nx187 = layers.Conv2D(name='187', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x187_pad)\r\nx188 = layers.Lambda(name='188', function=lambda x: target_layer(x, 1, 0, 128))(x187)\r\nx189 = layers.Lambda(name='189', function=lambda x: target_layer(x, 1, 128, 256))(x187)\r\nx190 = layers.Maximum(name='190')([x188, x189])\r\nx191 = layers.Add(name='191')([x190, x182])\r\nx192 = layers.Conv2D(name='192', filters=256, kernel_size=(1, 1), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x191)\r\nx193 = layers.Lambda(name='193', function=lambda x: target_layer(x, 1, 0, 128))(x192)\r\nx194 = layers.Lambda(name='194', function=lambda x: target_layer(x, 1, 128, 256))(x192)\r\nx195 = layers.Maximum(name='195')([x193, x194])\r\nx196_pad = layers.ZeroPadding2D(name='196_pad', padding=((1, 1), (1, 1)), data_format='channels_first')(x195)\r\nx196 = layers.Conv2D(name='196', filters=256, kernel_size=(3, 3), padding='valid', data_format='channels_first', activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x196_pad)\r\nx197 = layers.Lambda(name='197', function=lambda x: target_layer(x, 1, 0, 128))(x196)\r\nx198 = layers.Lambda(name='198', function=lambda x: target_layer(x, 1, 128, 256))(x196)\r\nx199 = layers.Maximum(name='199')([x197, x198])\r\nx201 = layers.ZeroPadding2D(name='201', padding=((0, 0), (0, 0)), data_format='channels_first')(x199)\r\nx200_pad = layers.ZeroPadding2D(name='200_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x199)\r\nx202_pad = layers.ZeroPadding2D(name='202_pad', padding=((0, 0), (0, 0)), data_format='channels_first')(x201)\r\nx200 = layers.MaxPooling2D(name='200', padding='valid', strides=(2, 2), data_format='channels_first')(x200_pad)\r\nx202 = layers.AveragePooling2D(name='202', padding='valid', strides=(2, 2), data_format='channels_first')(x202_pad)\r\nx203 = layers.Add(name='203')([x200, x202])\r\nx211 = layers.Reshape(name='211', target_shape=(-1,))(x203)\r\nx212 = layers.Dense(name='212', units=256, activation='linear', kernel_initializer='zeros', bias_initializer='zeros')(x211)\r\nx214 = layers.Dense(name='214', units=80013, activation='linear', use_bias=False, kernel_initializer='zeros', bias_initializer='zeros')(x212)\r\n\r\nmodel_tf = Model(inputs=x0_1, outputs=x214)\r\n```\r\n</details>", "@There is a problem with the model itself @paulbauriegel. I might be wrong, but I noticed an issue with target_shape of the layer \"x211 = layers.Reshape(name='211', target_shape=(-1,))(x203)\". When I replaced -1 with any integer, there is no error at all.", "@gowtham-kp I agree with you to some extend. The `-1` which is supposed to infer the dimension is causing the problem. Using a Flatten layer here will probably do the same and work without problems. Still specifying the operation in that manner is generally valid right?", "Its valid but I have to investigate furthur and will get back to you on this @paulbauriegel. As of now as a work around can you flatten the layer and work on it. Thanks!", "Closing this issue as it has been answered. Please additional comments and we can open the issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31169\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31169\">No</a>\n", "So the answer is then,  Reshape (-1,) is just not possible?"]}, {"number": 31168, "title": "[INTEL MKL] Enable MatMul in eager mode", "body": "Enable MKL MatMul in eager mode.\r\n\r\nPending on the following PRs (that is, this PR should be merged after them):\r\nhttps://github.com/tensorflow/tensorflow/pull/30401   (MKL conv fwd in eager mode)\r\nhttps://github.com/tensorflow/tensorflow/pull/30402   (MKL conv bwd in eager mode)", "comments": ["Pinging @penpornk for review.", "(I meant, this PR seems to have just a few lines of changes when based on https://github.com/tensorflow/tensorflow/pull/30401.)", "@penpornk.  Yes, it depends on PR #30401. It is better waiting #30401 is merged. Thanks!", "Headsup: after PR https://github.com/tensorflow/tensorflow/pull/30402 being merged, \r\nthere will be some conflicts which I will resolve. ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31168) for more info**.\n\n<!-- need_author_cla -->", "FYI - the expected merge conflicts have been resolved", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31168) for more info**.\n\n<!-- need_author_consent -->", "Please ignore, Git merge messed things up. Created a new PR here https://github.com/tensorflow/tensorflow/pull/31311  I will try to close this PR.", "I'll close the PR for you."]}, {"number": 31167, "title": "[TF2.0] [Support] TPU training code Breaks after 2 steps of Training", "body": "I'm training a model on TPU, however the code execution breaks after exactly after 2 steps. I'm using tf.distribute.experimental.TPUStrategy for the training.\r\nAfter running exactly two steps *inside* experimental_run_v2(), it breaks.\r\nThe execution doesn't even come out of experimental_run_v2() (I'm logging every microstep (forward passing, loss calculation, applying gradients, etc) to keep a note of the steps).\r\n\r\nthe trainer code is located here: [https://github.com/captain-pool/GSOC/blob/90980a1bbfd9753fbba5e189a1329f84ba86b448/E3_Distill_ESRGAN/libs/train.py#L200-L245](https://github.com/captain-pool/GSOC/blob/90980a1bbfd9753fbba5e189a1329f84ba86b448/E3_Distill_ESRGAN/libs/train.py#L200-L245)\r\n\r\nStack Overflow Question: [https://stackoverflow.com/questions/57274370/tpu-training-code-breaks-after-2-steps-of-training](https://stackoverflow.com/questions/57274370/tpu-training-code-breaks-after-2-steps-of-training)\r\n\r\nHere's the error log:\r\n\r\n```\r\nI0730 14:25:03.909630 139951185348352 train.py:247] Starting Adversarial Training\r\nI0730 14:25:04.162598 139951185348352 train.py:259] Start Train\r\nI0730 14:25:09.262624 139951185348352 api.py:512] Student Fake\r\nI0730 14:25:34.609512 139951185348352 api.py:512] Teacher fake\r\nW0730 14:25:43.462079 139951185348352 deprecation.py:323] From /home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nI0730 14:25:43.486022 139951185348352 api.py:512] student_ra\r\nI0730 14:25:44.305178 139951185348352 api.py:512] teacher_ra\r\nI0730 14:25:44.322464 139951185348352 api.py:512] disc_loss\r\nI0730 14:25:44.415445 139951185348352 api.py:512] gen_loss\r\nI0730 14:25:45.656436 139951185348352 api.py:512] gen gradient\r\nI0730 14:25:46.245677 139951185348352 api.py:512] disc gradient\r\nW0730 14:25:51.213224 139951185348352 deprecation.py:323] From /home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/distribute/values.py:878: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nApply a constraint manually following the optimizer update step.\r\nI0730 14:25:52.171965 139951185348352 api.py:512] gen apply\r\nI0730 14:26:03.028684 139951185348352 api.py:512] disc apply\r\nI0730 14:26:10.171932 139951185348352 api.py:512] Student Fake\r\nI0730 14:26:10.637515 139951185348352 api.py:512] Teacher fake\r\nI0730 14:26:11.779316 139951185348352 api.py:512] student_ra\r\nI0730 14:26:12.462838 139951185348352 api.py:512] teacher_ra\r\nI0730 14:26:12.480808 139951185348352 api.py:512] disc_loss\r\nI0730 14:26:12.508683 139951185348352 api.py:512] gen_loss\r\nI0730 14:26:13.662986 139951185348352 api.py:512] gen gradient\r\nI0730 14:26:14.201171 139951185348352 api.py:512] disc gradient\r\nI0730 14:26:15.363607 139951185348352 api.py:512] gen apply\r\nI0730 14:26:16.708091 139951185348352 api.py:512] disc apply\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 129, in <module>\r\n    train_and_export(**vars(FLAGS))\r\n  File \"main.py\", line 89, in train_and_export\r\n    trainer.train_adversarial(student_generator)\r\n  File \"/home/rick/GSOC/E3_Distill_ESRGAN/libs/train.py\", line 260, in train_adversarial\r\n    train_step(image_lr, image_hr)\r\n  File \"/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 445, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1730, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 778, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 471, in call\r\n    ctx=ctx)\r\n  File \"/home/rick/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg\r\n\t [[{{node rrdb_student_conv2d_conv2d_rrdb_student_conv2d_kernel_139950763791080_handle_inputs_0}}]] [Op:__inference_train_step_44255]\r\n```\r\n\r\nCC: @srjoglekar246 , @vbardiovskyg ", "comments": ["Any insight on this issue even though you closed it out? Had the same issue today on the TPU v3-8 nightly with the tf-nightly-2.0 tensorflow build.  Maybe we're too early on TPU's in 2.0?", "Hey @samuelrusk, the code breaks because I was placing the Distributed Dataset on \"/job:worker\" but forgot to place the trainer code on it, instead I was placing it on the CPU. Moving the trainer code inside the scope of \"/job:worker\" fixed the issue.\r\nThe error message was not clear at all, so that was making it extremely difficult to Debug.", "@captain-pool Awesome, that worked for me! Thanks for the bit. Don't fully understand the mechanics of adding the \"/job:worker\" inside the scope but it seems to have fixed the issue."]}, {"number": 31166, "title": "Possible tf.matmul bug (wrong results) on tensorflow-gpu", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.6.7\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: cuda-10.0\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\ntf.matmul on tensorflow-gpu gave wrong results. Here is a simplified version of the code.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nr = [[ 1.0, 0.0],    [0.0, 1.0]]\r\n\r\nx, y = np.meshgrid(list(range(400)), list(range(400)))\r\ncoords = np.stack([x,y],-1).reshape((400,400,2,1))\r\ncoords = tf.convert_to_tensor(coords,dtype=tf.float32)\r\n\r\nr1 = tf.constant(r)\r\n\r\nnewCoords = tf.matmul(r1, coords)\r\n\r\nsess = tf.Session()\r\nret = sess.run(newCoords,feed_dict={r1:r})\r\n\r\nplt.matshow(ret[:,:,0,0])\r\nplt.show()\r\n```\r\nWhen I ran it on my tensorflow-gpu, here is the result:\r\n![bug](https://gist.githubusercontent.com/sWizad/3a25d6559ea3308e5cc2731519635c32/raw/2479341ed43fb8e69c94eda3a1362d5cbde7d2d7/Figure_1.png)\r\nLooks like it stops computing halfway through and gave the rest 0 as a result.\r\n\r\n**Describe the expected behavior**\r\nHere is the result with CPU:\r\n![CPU](https://user-images.githubusercontent.com/45821224/62137176-08631480-b2d5-11e9-8b8e-6348e8b206e7.png)\r\n\r\n**********************************************************\r\nBelow is my old post. Initially, I thought the problem was related to TFRecord, but seems like this problem occurs without even using tfrecord too. For completeness, I keep the old example code with tfrecord.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\ndef parser(serialized_example):\r\n      fs = tf.io.parse_single_example(\r\n          serialized_example,\r\n          features={ \"r\": tf.FixedLenFeature([4], tf.float32) })\r\n      fs[\"r\"] = tf.reshape(fs[\"r\"], [2, 2])\r\n      return fs\r\n\r\nr = [[ 1.0, 0.0],[0.0, 1.0]]\r\n\r\nwith tf.io.TFRecordWriter(\"cc.test\") as tfrecord_writer:\r\n    feature = {\"r\": tf.train.Feature(float_list=tf.train.FloatList(value=np.array(r).flatten() ))}\r\n    example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n    tfrecord_writer.write(example.SerializeToString())\r\ndataset = tf.data.TFRecordDataset([\"cc.test\"])\r\ndataset = dataset.map(parser).repeat().make_one_shot_iterator()\r\nfeatures = dataset.get_next()\r\n\r\nx, y = tf.meshgrid(list(range(400)), list(range(400)))\r\ncoords = tf.stack([x, y], -1)     #(h,w,2)\r\ncoords = tf.expand_dims(tf.cast(coords,tf.float32),-1) #(h,w,2,1)\r\n\r\nr1 = features[\"r\"]\r\nr2 = tf.constant(r)\r\n\r\nnewCoords = tf.matmul(r1, coords)\r\n\r\nsess = tf.Session()\r\nret = sess.run(newCoords[:,:,0,0])\r\nplt.matshow(ret)\r\nplt.show()\r\n```\r\nThe code will create \"cc.test\" file to save the variable ``r`` and load it as ``r1``. Then matmul ``r1`` with some big varibles.\r\n\r\n\r\n\r\n", "comments": ["I have tried on colab with TF version 1.14 and was able to reproduce the issue.Please, find the [gist ](https://colab.research.google.com/drive/1-PILZtGYy4N5yV_goNWGx6CyOe22gutg)here.Thanks!", "Replicated with Ubuntu 18.04, CUDA 10, tensorflow-gpu 1.14.0, and Python 2.7", "Replicated with \r\nUbuntu 16.04, CUDA 10, tensorflow-gpu 1.14.0, and Python 2.7\r\nUbuntu 16.04, CUDA 10, tensorflow-gpu 1.14.0, and Python 3.6", "I'm having the same issue with:\r\nUbuntu 18.04, CUDA 10, tensorflow-gpu 1.14.0, Python 3.6, \r\ncudnn 7.6.2, driver version 430.26.", "I get this same issue on tensorflow-gpu 2.0rc0 on Windows with python 3.7.2, CUDA 10.0.", "I've also run into the same issue; current workaround is to perform the matmul on cpu.\r\n\r\nUbuntu 18.04, CUDA 10.1, tensorflow-gpu 1.13.1, and Python 3.5.2", "@sWizad \r\n\r\nI tried in colab with TF GPU versions for 1.14, 1.15 and i am not seeing any issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/6c0d71133160fb0614bb28942c3fba04/untitled79.ipynb).Please, let us know whether we can close this issue.Thanks!", "Hello,\r\nI've run into the same issue on Ubuntu 18.04 CUDA 10.1 tensorflow-gpu version 2.0.0 and Python 3.7.4. Cudnn version is 7.6.2.\r\nBut in my case, if I use the version 1.14 of tensorflow the code above gives me the right answer.\r\n\r\nIt seems that in colab, there is no issue for the version 2.0 and for the 1.14.\r\nAny idea why is there that problem ?\r\n\r\n", "@ravikyram @humanpose1  This is fixed on Colab because CUDA 10.1 is used there. However, the issue still persists in the official TF 1.15 or TF 2 python packages, which are using CUDA 10.0.", "This is most likely a duplicate of issue #26969.", "@fxtentacle Can you please try with `TensorFlow 2.1.0-rc0` and let us know whether it resolved for you or not. Thanks!", "I have tried `tf-nightly-gpu` and I cannot reproduce the issue. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d8c15cfca53160749ccf84c1443a3bbb/untitled679.ipynb). Thanks.\r\nThe issue is there in `TF1.15.0` and I am not sure whether there will be any more changes TF1.x. Thanks!", "@jvishnuvardhan Yes, `tf-nightly-gpu` works because it uses `CUDA 10.1` instead of `CUDA 10.0`. To confirm, I ran inside your gist:\r\n```\r\nfrom tensorflow.python.platform import build_info as tf_build_info\r\nprint(tf_build_info.cuda_version_number)\r\n```\r\nwhich returns `10.1`.\r\n", "BTW I believe the bug we're seeing here is this one:\r\nhttps://docs.nvidia.com/cuda/archive/10.1/cuda-toolkit-release-notes/index.html#unique_1334956937\r\n> In earlier releases, cuBLAS GEMM calls might randomly crash when running multiple host threads sharing one cuBLAS handle despite adhering to recommended usage in Thread Safety. This bug affects cuBLAS in earlier CUDA releases, and is fixed in CUDA 10.1. \r\n\r\nSo this should be a very easy fix for `TF 1.15`, too: Just recompile with CUDA 10.1.", "@fxtentacle: Thank you for your analysis. As the fix is is part of CUDA 10.1, we recommend users to upgrade to a TF release with support for CUDA 10.1+. We do not plan to release a new 1.x based on CUDA 10.1.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31166\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31166\">No</a>\n"]}, {"number": 31165, "title": "[tf 2.0] use summary apis without occupying gpu memory", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0-dev20190725\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWith current tf 2.0 summary in a gpu-equipped machine, the process will occupy gpu memory as follows:\r\n```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tf.__version__\r\nOut[2]: '2.0.0-dev20190725'\r\n\r\nIn [3]: !nvidia-smi\r\nTue Jul 30 21:47:26 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\r\n| N/A   28C    P0    26W / 250W |      0MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\r\n| N/A   28C    P0    25W / 250W |      0MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\nIn [4]: writer = tf.summary.create_file_writer(\"./testdir\")\r\n...\r\ninfo be omitted to avoid clutter\r\n...\r\n\r\nIn [5]: with writer.as_default():\r\n   ...:     tf.summary.scalar(\"test\", 123, step=1)\r\n   ...:\r\n\r\nIn [6]: !nvidia-smi\r\nTue Jul 30 21:47:44 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\r\n| N/A   29C    P0    41W / 250W |  15460MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\r\n| N/A   29C    P0    36W / 250W |    418MiB / 16130MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     63963      C   ...2.0/envs/tf-nightly-2.0-0725/bin/python 15449MiB |\r\n|    1     63963      C   ...2.0/envs/tf-nightly-2.0-0725/bin/python   407MiB |\r\n+-------------------------------------\r\n```\r\n\r\nWe expect some apis which we can use to log metrics without gpu usage, such as `tf.Summary` and `tf.Summary.Value` in tf 1.x.\r\n\r\n**Will this change the current api? How?**\r\nNot sure.\r\n\r\n**Who will benefit with this feature?**\r\nWho wants to use tensorflow apis to log results in a separate process, which is expected to occupy no GPU memory.\r\n", "comments": ["Hi,\r\nIs there any chance you can actually place the operations on CPU? E.g.\r\n```\r\nwith tf.device('CPU:0'):\r\n    writer = tf.summary.create_file_writer(\"./testdir\")\r\n    # etc.\r\n```\r\n\r\nI have no idea whether this works, nor if simply creating the writer on device influences the way future operations are dealt with, which you would have to test unless a more knowledgeable person comes around :-)", "@pandrey-fr Thanks for your advice.\r\n```\r\nwith tf.device(\"CPU:0\"), writer.as_default():\r\n    tf.summary.scalar(\"test\", 123, step=1)\r\n```\r\nNow, it only consumes `407 MB` of GPU memory, which may be used to initialize something by gpu-enabled tensorflow version.\r\n\r\nHowever, for tf 1.x with gpu enabled, we can still use APIs such as `tf.Summary` to write logs without initializing GPU kernels. I still expect some similar APIs for tf 2.x.", "@llan-ml Can you please create an issue in [Tensorflow/tensorboard](https://github.com/tensorflow/tensorboard) repository and reference this issue there as this is closely related to tensorboard. Thanks!"]}, {"number": 31164, "title": "Add support for CTC for float64", "body": "Previous pull request with this commit #21822", "comments": ["Are you sure @rthadur? One more year of waiting from the same reviewer?", "@rthadur, as with #21822, @ebrevdo just ignores this PR", "Should I do something to fix the error with the failed build? Because I have no idea what to do with this specific error.", "I will try and fix it upon merge.\n\nOn Thu, Aug 15, 2019 at 12:40 AM aprimostka <notifications@github.com>\nwrote:\n\n> Should I do something to fix the error with the failed build? Because I\n> have no idea what to do with this specific error.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31164?email_source=notifications&email_token=AANWFG72LH6TRGMGDBEEUATQEUB5FA5CNFSM4IH4GVH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4LC5VY#issuecomment-521547479>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG6NMNHU23JDTO6FAGLQEUB5FANCNFSM4IH4GVHQ>\n> .\n>\n"]}, {"number": 31163, "title": "[MLIR] couldn't convert MLIR to GraphDef via tf-mlir-translate", "body": "`tf-mlir-translate` fails while converting MLIR to GraphDef.\r\n\r\n`$ uname -a`\r\n```\r\nLinux pc-ubuntu 4.15.0-54-generic #58-Ubuntu SMP Mon Jun 24 10:55:24 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\n`$ cd tensorflow/compiler/mlir`\r\n`$ cat add.pbtxt `\r\n```\r\nnode {\r\n  name: \"Add\"\r\n  op: \"Add\"\r\n  input: \"input0\"\r\n  input: \"input1\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"input0\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"input1\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n}\r\nversions {\r\n  producer: 27\r\n}\r\n```\r\n\r\n`$ bazel run tf-mlir-translate -- --graphdef-to-mlir --tf-input-arrays=input0,input1 --tf-input-data-types=DT_INT32,DT_INT32 --tf-input-shapes=10:10 --tf-output-arrays=Add ${PWD}/add.pbtxt -o ${PWD}/add.mlir`\r\n\r\n`$ cat add.mlir`\r\n```\r\n\r\n\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 27 : i32}} {\r\n  func @main(%arg0: tensor<10xi32>, %arg1: tensor<10xi32>) -> tensor<10xi32>\r\n  attributes  {tf.entry_function = {inputs = \"input0, input1\", outputs = \"Add\"}} {\r\n    %0 = tf_executor.graph {\r\n      %1:2 = tf_executor.island {\r\n        %4 = \"tf.Placeholder.input\"(%arg0) {device = \"\", dtype = \"tfdtype$DT_INT32\", name = \"input0\", shape = \"tfshape$dim { size: 10 }\"} : (tensor<10xi32>) -> tensor<10xi32>\r\n        tf_executor.yield %4 : tensor<10xi32>\r\n      } {device = \"\", dtype = \"tfdtype$DT_INT32\", name = \"input0\", shape = \"tfshape$dim { size: 10 }\"}\r\n      %2:2 = tf_executor.island {\r\n        %4 = \"tf.Placeholder.input\"(%arg1) {device = \"\", dtype = \"tfdtype$DT_INT32\", name = \"input1\", shape = \"tfshape$dim { size: 10 }\"} : (tensor<10xi32>) -> tensor<10xi32>\r\n        tf_executor.yield %4 : tensor<10xi32>\r\n      } {device = \"\", dtype = \"tfdtype$DT_INT32\", name = \"input1\", shape = \"tfshape$dim { size: 10 }\"}\r\n      %3:2 = tf_executor.island {\r\n        %4 = \"tf.Add\"(%1#0, %2#0) {T = \"tfdtype$DT_INT32\", device = \"\", name = \"Add\"} : (tensor<10xi32>, tensor<10xi32>) -> tensor<10xi32>\r\n        tf_executor.yield %4 : tensor<10xi32>\r\n      } {T = \"tfdtype$DT_INT32\", device = \"\", name = \"Add\"}\r\n      tf_executor.fetch %3#0 : tensor<10xi32>\r\n    }\r\n    return %0 : tensor<10xi32>\r\n  }\r\n}\r\n```\r\n`$ bazel run tf-mlir-translate -- --mlir-to-graphdef ${PWD}/add.mlir -o ${PWD}/converted_add.pbtxt`\r\n\r\n**output**\r\n```\r\n...\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Build completed successfully, 1 total action\r\n2019-07-30 15:38:00.617166: E tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate_registration.cc:82] Graph export failed: Failed precondition: op node 'tf_executor.graph' was not a TF op!\r\n```\r\n", "comments": ["CC +=  @joker-eph", "Sorry about this, it was fixed in 459f8cf615402f022de9ea1f9c1f033509db0076 but reverted in 459f8cf615402f022de9ea1f9c1f033509db0076 because it broke an ASAN test. The ASAN fix is under review and we should land everything today (hopefully).", "Thanks!", "The ASAN fix is landed as https://github.com/tensorflow/tensorflow/commit/417972d14641a9a4a3ac512a078465d6cfeaff30 now.", "Fixed in ff79f54b9c89789f03b09d5a9a5afd481adca6b0"]}, {"number": 31162, "title": "Can not use large dimension in Embedding layer on GPU(s).", "body": "I am using TF2.0 latest nightly build and I am trying to train LSTM model for text classification on very large dataset of 16455928 sentences. For embedding layer in the model, I have a vocab size of 366856 and I used 1000 as embedding dimension value in it, on which the 2 GPUs(Tesla T4 from Google) ran out of memory.\r\nSince I can not lower the size of vocabulary (maybe there is a way), so I used lower value for embedding dimension (100) on which the model starts training. Now my question is if there is a way I can use higher value of embedding dimension?. Maybe by putting set of layers of my model on different GPUs, if so then what is the way in TF2.0? Also, will using more number GPUs help? Thank you!", "comments": ["Hi @rishabhsahrawat,\r\n\r\nCould you be more precise as to when the issue occurs exactly, and what your data processing pipeline looks like (i.e. do you have the entire dataset loaded in memory, or do you iteratively load and discard pieces of it)?\r\n\r\nI would not be surprised if your issue was similar to the one I reported in #30952 (which has not been picked up as of now...), but maybe it is a simpler dataflow management issue. In the former case, you might want to try to run after disabling Eager execution (which is not a fix to the issue, but could be a work-around...).", "Hi @pandrey-fr , thank you for your questions. I am actually using `tf.data.Dataset` (reading a csv file using `make_csv_dataset`). After that I created an encoder using `tfds.features.text.TokenTextEncoder` which gives a vocabulary size of 366856 (unique words). This vocab size is then given to Embedding layer of my model along with the embedding size of 1000 which on running the model throws huge log of error but following lines were interesting for me\r\n\r\n> Resource exhausted:  OOM when allocating tensor with shape[366856, 1000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\r\nSo, if I lower the embedding size to <=100, then everything works without any error.\r\nI think my problem is also same like your mentioned issue. I hope they fix it soon. Anyways in the meantime, I will try running after disabling Eager execution. I checked the error log you have mentions, so if you change the values in the Embedding layer to lower values does it work for you with Eager execution and GPU enabled?", "> So, if I lower the embedding size to <=100, then everything works without any error.\r\n\r\nTo be clear, does this include the fitting process of the model? If so, then it could just be that the data is indeed too large for the amount of RAM on your GPU. Could you describe your GPU's specs (name, RAM)?\r\n\r\nYou could also try running your code without enabling GPU use (run `tf.config.experimental.set_visible_devices([], 'GPU') at the beginning of your code) and monitor how much RAM is being used to get a notion of how much memory your model and data require, and whether this is a stable amount throughout the training cycle or an increasing one (which would make it similar to my issue).\r\n\r\n> Maybe by putting set of layers of my model on different GPUs, if so then what is the way in TF2.0?\r\n\r\nI had not noticed this part of the question, but this could be a strategy. I believe you then need to use the `tf.device` context manager when defining your operations to force their placement on this or that GPU, e.g. something that would look like:\r\n```python\r\n# list available GPUS, make sure you have at least two\r\ngpus = tf.config.experimental.list_logical_devices('GPU')\r\nassert len(gpus) >= 2\r\n# place the embedding layer on the first GPU\r\nwith tf.device(gpus[0]):\r\n    embedded = tf.keras.layers.Embedding(input_dim, output_dim)(inputs)\r\n# place the rest of the model on the second GPU\r\nwith tf.device(gpus[1]):\r\n    output = some_layers_stack(embedded)\r\n```", "Yes, this happens after calling `model.fit`, if that was your question and my error log is also similar like the one you have mentioned in your issue. I am using 2 Tesla T4 GPUs each with 16 GB memory from Google.\r\nAs per your suggestion, I will run it on CPU and see the memory consumption. However, I want to share one information (not sure if this will be helpful). Earlier, I was trying to shuffle the dataset using `dataset.shuffle()`which fills up the buffer and memory of CPU before running for the first epoch. Eventually, this buffer filling up process uses all 16 GB RAM of my CPU and after this nothing runs. I even raised an issue [here](https://github.com/tensorflow/tensorflow/issues/30646#event-2516513806). Not sure if this is a normal behaviour.\r\n\r\nAt the end you have mentioned a sample code for placing model layers on different GPUs, I have a question regarding this. Do you think it will work similarly like `tf.distribute.MirroredStrategy()` from [here](https://www.tensorflow.org/beta/guide/distribute_strategy#using_tfdistributestrategy_with_keras)? Regardless, I will try this approach too. Thank you!", "> Earlier, I was trying to shuffle the dataset using dataset.shuffle()which fills up the buffer and memory of CPU before running for the first epoch. Eventually, this buffer filling up process uses all 16 GB RAM of my CPU and after this nothing runs.\r\n\r\nOkay, so basically your dataset (multiplied by embedding dimension) is huge, and the embedding vectors representing it cannot be all loaded in memory (either CPU or GPU) at the same time. For your model to run, you therefore need to keep the amount of data loaded at any given time under 16 GB (which is the limit on both your CPU and GPU, notwithstanding the idea of using multiple parallel devices).\r\n\r\nIf we make the (possibly strong) assumption that there is no actual TensorFlow issue, this should be achievable (with a large embedding dimension) by tweaking your `tf.data.Dataset` object - basically, you want to load a small part of the data (which makes up for a few batches), shuffle and padded-batch it, feed it to your model, discard it and go on doing the same with the next bit. Are you still using the dataset from the [tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches)? If so, I will try to write you a bit of code later on tonight.\r\n\r\nNow, once this will be dealt with, we should watch for any memory increase during training (despite the dataflow), which would indicate an issue similar to mine, but hopefully your problem only comes from loading too much data at once.\r\n\r\n> At the end you have mentioned a sample code for placing model layers on different GPUs, I have a question regarding this. Do you think it will work similarly like tf.distribute.MirroredStrategy() from here?\r\n\r\nFrom what I understand, `tf.distribute.MirroredStrategy()` creates a copy of your model on each available GPU, so as to run parallel fitting on various batches. If that is indeed the case, the amount of memory used on the first GPU should not decrease; instead, this strategy will boost training runtime by distributing the processing of training samples on parallel copies of the model (and still somehow gather unified weights update - I guess each would treat part of the batch, then gradients would be computed based on an aggregate of local losses, or maybe there is some rougher locally-computed updates aggregations scheme ; I have not looked into the details).\r\n\r\nWhat I was suggesting (and I have absolutely no idea whether that would work) was to put distinct parts of the architecture on the various devices. It might not be possible, or induce data transmission overheads that greatly slow computations (and possibly do not yield the memory gain I had in mind) - so, if you try experimenting in that direction, I would be glad to hear about the results!", "Yes, I am still using the same way for loading the dataset so padded_batch\netc. except shuffling since it doesn\u2019t work for me as I said.\nI will surely try out the way to divide the layers of the model and hope it\nwill work. I will keep you updated about that.\n\nThank you again for your suggestions and help.\n-Rishabh\n\nOn Tuesday, July 30, 2019, Paul Andrey <notifications@github.com> wrote:\n\n> Earlier, I was trying to shuffle the dataset using dataset.shuffle()which\n> fills up the buffer and memory of CPU before running for the first epoch.\n> Eventually, this buffer filling up process uses all 16 GB RAM of my CPU and\n> after this nothing runs.\n>\n> Okay, so basically your dataset (multiplied by embedding dimension) is\n> huge, and the embedding vectors representing it cannot be all loaded in\n> memory (either CPU or GPU) at the same time. For your model to run, you\n> therefore need to keep the amount of data loaded at any given time under 16\n> GB (which is the limit on both your CPU and GPU, notwithstanding the idea\n> of using multiple parallel devices).\n>\n> If we make the (possibly strong) assumption that there is no actual\n> TensorFlow issue, this should be achievable (with a large embedding\n> dimension) by tweaking your tf.data.Dataset object - basically, you want\n> to load a small part of the data (which makes up for a few batches),\n> shuffle and padded-batch it, feed it to your model, discard it and go on\n> doing the same with the next bit. Are you still using the dataset from the\n> tutorial\n> <https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches>?\n> If so, I will try to write you a bit of code later on tonight.\n>\n> Now, once this will be dealt with, we should watch for any memory increase\n> during training (despite the dataflow), which would indicate an issue\n> similar to mine, but hopefully your problem only comes from loading too\n> much data at once.\n>\n> At the end you have mentioned a sample code for placing model layers on\n> different GPUs, I have a question regarding this. Do you think it will work\n> similarly like tf.distribute.MirroredStrategy() from here?\n>\n> From what I understand, tf.distribute.MirroredStrategy() creates a copy\n> of your model on each available GPU, so as to run parallel fitting on\n> various batches. If that is indeed the case, the amount of memory used on\n> the first GPU should not decrease; instead, this strategy will boost\n> training runtime by distributing the processing of training samples on\n> parallel copies of the model (and still somehow gather unified weights\n> update - I guess each would treat part of the batch, then gradients would\n> be computed based on an aggregate of local losses, or maybe there is some\n> rougher locally-computed updates aggregations scheme ; I have not looked\n> into the details).\n>\n> What I was suggesting (and I have absolutely no idea whether that would\n> work) was to put distinct parts of the architecture on the various devices.\n> It might not be possible, or induce data transmission overheads that\n> greatly slow computations (and possibly do not yield the memory gain I had\n> in mind) - so, if you try experimenting in that direction, I would be glad\n> to hear about the results!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31162?email_source=notifications&email_token=ADZMMPDPP5VBX4VVGG66O2DQCBOIPA5CNFSM4IH3FFHKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3EL4SQ#issuecomment-516472394>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADZMMPBBTUPGO7Z6PYH6QL3QCBOIPANCNFSM4IH3FFHA>\n> .\n>\n\n\n-- \nSent from Gmail iPhone\n", "Hi Rishabh,\r\n\r\nLooking further at the tutorial (whose dataset is way smaller than yours, if I understand well) and doing a few tests on my own, I must say I am surprised by the issues you are encountering...\r\n\r\nNormally, setting a reasonable buffer_size argument when using shuffle should solve memory issues associated with that part of the data pipeline. As for the embeddings matrix, it is indeed rather big, but when I tried allocating a similarly-shaped one on my system (using alternatively the GPU's 4GB dedicated memory or the 16 GB of RAM I have at my disposal) it fitted with just a slight warning about its size (basically it takes a bit less than 2 GB of memory space, which is a lot for a single weights matrix but should be tractable given your config).\r\n\r\nIs there any chance you could share your dataset and code with me (optionally _via_ a private channel) so that I can have a look? I suspect there might be something wrong in the dataflow that would explain your going out of memory, but I could be overly optimistic.\r\n\r\nAt any rate, an alternative way of decreasing the embeddings' dimensionality would be to decrease the number of tokens in your vocabulary, _e.g._ using a WordPiece tokenizer to break uncommon tokens down to knows ones (including phonetic tokens if needed). This way you might end up with a matrix with (way) less rows, which might also be a good thing for your modeling (depending on the amount of rarely-used tokens in the dataset).\r\n\r\nBest,\r\nPaul", "Hi Paul, thank you for your response. My dataset size is 16455928 elements/sentences, so in order to make sure that shuffling is perfect I must use either the same or greater value as the size of my full dataset for buffer_size (mentioned [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle)) otherwise there might be some elements that the model might see never or more than once during an epoch.\r\nI am stunned that it fit for you and just using 2 GB of memory. Just to be clear this is how my Embedding layer looks like `tf.keras.layers.Embedding(input_dim = 366856, output_dim=1000, input_length = 4)` giving 366857000 parameters. This results in error. However, `tf.keras.layers.Embedding(input_dim = 366856, output_dim=100, input_length = 4)` giving 36685700 parameters works for me.\r\nI am sorry but I can not share the code or dataset, as it does not belong to me. :(\r\nYour suggestion for using WordPiece. After reading about it quickly I think it is more or less similar to stemming words i.e. playing, played etc. -> play. I am already applying Lemmatization and Stemming during preprocessing dataset. \r\n\r\nRegards,\r\nRishabh", "@rishabhsahrawat Will it be possible to create minimal reproducible code and share with us to move faster. Thanks!", "> My dataset size is 16455928 elements/sentences, so in order to make sure that shuffling is perfect I must use either the same or greater value as the size of my full dataset for buffer_size (mentioned here) otherwise there might be some elements that the model might see never or more than once during an epoch.\r\n\r\nThat is correct, however I guess an imperfect shuffling might be better than no shuffling at all, if the dataset comprises some order - but it might be already shuffled \"by nature\", or could probably be shuffled on disk outside of TensorFlow. I leave it up to you to see what suits best your usecase!\r\n\r\n> I am stunned that it fit for you and just using 2 GB of memory. Just to be clear this is how my Embedding layer looks like tf.keras.layers.Embedding(input_dim = 366856, output_dim=1000, input_length = 4) giving 366857000 parameters. This results in error. However, tf.keras.layers.Embedding(input_dim = 366856, output_dim=100, input_length = 4) giving 36685700 parameters works for me.\r\n\r\nI just tested again using your exact code; I agree on the number of parameters, but on RAM it still takes \"only\" 1.4 ~ 1.5 GB of RAM, at least after instantiation and feeding it a Tensor (to ensure the weights are indeed built)... But maybe there is something somewhere in your code that triggers the creation of multiple copies of the weights?\r\n\r\n> I am sorry but I can not share the code or dataset, as it does not belong to me. :(\r\n\r\nNo problem, that was to be expected.\r\n\r\n> Your suggestion for using WordPiece. After reading about it quickly I think it is more or less similar to stemming words\r\n\r\nThat is indeed partially similar, but there is also the breaking down to phonetics of uncommon tokens that, in my short experience, can greatly decrease vocabulary size - but again, this is merely an abstract suggestion and it is up to you to see whether it suits your needs given your data and application context :)\r\n\r\nTo conclude, it seems that you are encountering a memory issue, whose relying on the mere size of your model on a bug somewhere is still unsure...\r\n\r\n**Edit**: after writing the code below I tried running it and I _did_ get out of GPU memory during training (the first batch went well, the second triggered an allocation error).\r\n\r\n@gadagashwini I leave it up to Rishabh to provide with more details about the model he is using, notably as to the output dimensionality, but I guess a first approximation would be:\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Embedding(input_dim=366856, output_dim=1000, input_length=4),\r\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1000)),\r\n    tf.keras.layers.Dense(500, activation='relu'),\r\n    tf.keras.layers.Dense(64, activation='softmax')\r\n])\r\nmodel.compile('adam', 'sparse_categorical_crossentropy', ['sparse_categorical_accuracy'])\r\n\r\nmock_inputs = tf.random.uniform((64, 4), 1, 366856, tf.int64)\r\nmock_target = tf.random.uniform((64, 1), 0, 64, tf.int64)\r\n\r\nmodel.fit(mock_inputs, mock_target, batch_size=32, epochs=5)\r\n```", "Hi @gadagashwini , I can not share the dataset unfortunately but I can share the model architecture example. These example layers also, from [here](https://www.tensorflow.org/beta/tutorials/load_data/text#build_the_model) throws same errors about memory full, GPU exhausted etc. Updated layers look like this-\r\n```\r\nmodel = tf.keras.Sequential() \r\nmodel.add(tf.keras.layers.Embedding(input_dim = 366856, output_dim = 1000, input_length = 4))\r\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,dropout = 0.5, recurrent_dropout = 0.5,kernel_regularizer = keras.regularizers.l2(0.001))))\r\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(685731, activation='softmax'))\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(),\r\n\t          loss='sparse_categorical_crossentropy',\r\n\t          metrics=['accuracy'])\r\n````\r\nIf I use output_dim <=150 in Embedding layer, then training starts without any problems. This was all on 2 GPUs. \r\nNow I have access to 2 more GPUs (now 4 in total) and if I use the same model architecture the error has changed to this- \r\n\r\n```Traceback (most recent call last):\r\n  File \"model_with_tfsplit.py\", line 99, in <module>\r\n    history = model.fit(train_data, epochs=20, validation_steps = steps ,validation_data=test_data , callbacks = [callback_for_saving])\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 710, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 680, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 296, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\", line 836, in execution_function\r\n    return [out.numpy() for out in distributed_function(input_fn)]\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 417, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 360, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 1749, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2053, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 1939, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 795, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 310, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 785, in wrapper\r\n    raise e.ag_error_metadata.to_exception(type(e))\r\nRuntimeError: in converted code:\r\n    relative to /home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python:\r\n    keras/distribute/distributed_training_utils.py:825 distributed_function  *\r\n        outputs = strategy.experimental_run_v2(\r\n    distribute/distribute_lib.py:753 experimental_run_v2\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    keras/engine/training.py:976 train_on_batch  *\r\n        outputs = training_eager.train_on_batch(\r\n    keras/engine/training_eager.py:303 train_on_batch\r\n        output_loss_metrics=output_loss_metrics))\r\n    keras/engine/training_eager.py:251 _process_single_batch\r\n        model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n    keras/optimizer_v2/optimizer_v2.py:435 apply_gradients\r\n        self._create_slots(var_list)\r\n    keras/optimizer_v2/adam.py:148 _create_slots\r\n        self.add_slot(var, 'v')\r\n    keras/optimizer_v2/optimizer_v2.py:587 add_slot\r\n        initial_value=initial_value)\r\n    ops/variables.py:258 __call__\r\n        return cls._variable_v2_call(*args, **kwargs)\r\n    ops/variables.py:252 _variable_v2_call\r\n        shape=shape)\r\n    ops/variables.py:63 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    distribute/distribute_lib.py:1394 create_colocated_variable\r\n        return next_creator(*args, **kwargs)\r\n    ops/variables.py:63 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    distribute/shared_variable_creator.py:69 create_new_variable\r\n        v = next_creator(*args, **kwargs)\r\n    ops/variables.py:63 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    distribute/distribute_lib.py:1306 creator_with_resource_vars\r\n        return self._create_variable(*args, **kwargs)\r\n    distribute/mirrored_strategy.py:512 _create_variable\r\n        values.SyncOnReadVariable, *args, **kwargs)\r\n    distribute/distribute_lib.py:2272 create_mirrored_variable\r\n        value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n    distribute/mirrored_strategy.py:504 _real_mirrored_creator\r\n        v = next_creator(*args, **kwargs)\r\n    ops/variables.py:63 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    eager/def_function.py:348 variable_capturing_scope\r\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n    ops/variables.py:260 __call__\r\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    eager/def_function.py:140 __init__\r\n        initial_value() if init_from_fn else initial_value,\r\n    distribute/mirrored_strategy.py:463 initial_value_fn\r\n        return array_ops.identity(init_value)\r\n    util/dispatch.py:180 wrapper\r\n        return target(*args, **kwargs)\r\n    ops/array_ops.py:92 identity\r\n        copied = input._copy()  # pylint: disable=protected-access\r\n    framework/ops.py:921 _copy\r\n        new_tensor = self._copy_nograd(ctx, device_name)\r\n    framework/ops.py:914 _copy_nograd\r\n        new_tensor = self._copy_to_device(context=ctx._handle, device=device_name)\r\n\r\n    RuntimeError: Error copying tensor to device: . Dst tensor is not initialized. \r\n```\r\nWhen I use output_dim <= 150, it starts training without errors. \r\nNow, I just found out if I use output_dim = 200, the error is same like on 2 GPUs with output_dim = 1000. Seems like the error log is changing also. :(\r\nI also looked at CPU memory usage while training with output_diim = 150, the memory starts increasing and stops at 14.8 GB from 300 MB. It then increase very slowly as the batches are processed during the epoch. Maybe this behaviour is expected or the problem is like this opened [issue](https://github.com/tensorflow/tensorflow/issues/30952).", "Hi @pandrey-fr , I just tried your dummy model. As you mentioned, training on single GPU also triggered allocation memory problem one time during epoch 2  like yours and one time during epoch 5. The CPU RAM it takes near 2-3 GBs but as soon as the fit model fit function is executed, the RAM usage increase and stops near 14-15 GBs.\r\nI also then tried training it on multiple GPUs after adding `MirroredStrategy()` scope. On execution, it does use all my GPUs but then throws Assertion error as follows, even before starting first epoch. \r\n```Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 710, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 614, in fit\r\n    epochs=epochs)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2214, in _distribution_standardize_user_data\r\n    assert isinstance(x, dataset_ops.DatasetV2)\r\nAssertionError\r\n```\r\nThank you!", "Hi @rishabhsahrawat,\r\n\r\nThank you for the details and feedback ; let us hope the issue gets picked up by someone who can clarify what is going on, and especially disentangle expected behaviours from potential bugs.\r\n\r\nJust a side note, but as your model's output dimension if the same as the input embedding one (I guess you are doing some kind of language modelling task, _e.g._ predicting masked or next tokens in a sentence), you might want to re-use the embedding matrix as a kernel on your output layer (this is notably what is done in Transformer models - well, not in the Google tutorial, but in the paper and reference implementations) ; this greatly diminishes the number of trainable weights :)\r\n\r\n(note that you have to tweak a little to do that, either with a custom layer or the functional API, but it really is not much of a difficulty)", "> Just a side note, but as your model's output dimension if the same as the input embedding one (I guess you are doing some kind of language modelling task, _e.g._ predicting masked or next tokens in a sentence), you might want to re-use the embedding matrix as a kernel on your output layer\r\n\r\nI am not sure if I understood it correctly, but I think you mean creating something like a 'word2vec' model? If not, can you also share some helpful link to understand it better?\r\nYou are right, I am doing tokens prediction.\r\nThank you!", "**UPDATE**\r\n@ymodak @gadagashwini \r\nI just tested the same model with output_dim= 1000 and also 150 on my Macbook Pro with 16 GB RAM and it trains without any problem. Of course the training is slow since it is only CPU. Now I don't what to do. I think the problems are somewhere in `MirrorStrategy()` and also training on GPUs.", "> > Just a side note, but as your model's output dimension if the same as the input embedding one (I guess you are doing some kind of language modelling task, e.g. predicting masked or next tokens in a sentence), you might want to re-use the embedding matrix as a kernel on your output layer\r\n\r\n> I am not sure if I understood it correctly, but I think you mean creating something like a 'word2vec' model? If not, can you also share some helpful link to understand it better?\r\nYou are right, I am doing tokens prediction.\r\nThank you!\r\n\r\nWhat I mean is doing something like this (with a possibly more-general implementation):\r\n\r\n```python\r\nclass SharedKernelSoftmax(tf.keras.layers.Layer):\r\n    def __init__(self, kernel, bias_initializer='zeros'):\r\n        self.kernel = kernel\r\n        self.bias = self.add_weight(\r\n            name='bias', shape=(tf.shape(kernel)[1],), dtype=kernel.dtype,\r\n            initializer=bias_initializer\r\n        )\r\n\r\n    def call(self, inputs, **kwargs):\r\n        output = tf.keras.backend.dot(inputs, self.kernel) + self.bias\r\n        return tf.nn.softmax(output, axis=-1)\r\n```\r\n\r\nAnd using such a layer (instantiated by being passed a shared embedding matrix of the output vocabulary) instead of the final softmax in your model. That being said, I know see that your input and output layers have different dimensions (different vocabulary, I guess?), so this is actually not relevant here (it would be if your input and output vocabulary were the same, or if you used a sequence-to-sequence model where tokens in the output vocabulary are also embedded before being passed to a decoder model). Sorry!", "> I just tested the same model with output_dim= 1000 and also 150 on my Macbook Pro with 16 GB RAM and it trains without any problem. Of course the training is slow since it is only CPU. Now I don't what to do. I think the problems are somewhere in MirrorStrategy() and also training on GPUs.\r\n\r\nInteresting... Could you monitor RAM usage and share some indications as to the amount of memory used when training on CPU?", "Thank you for clarifying my confusion. As you have noticed the input and output layers do not have same dimensions. So it will not be useful, however since I am tokenizing the data so I am saving encodings to a token file which I can load later without tokenizing the whole dataset on each run.\r\n\r\nI am now training the model with output_dim 1000 on CPU and I can see most of the memory is taken but the unused memory is changing to sometimes 3 MB, 600 MB and sometimes even 3 GBs. It is still running the first epoch right now. The %CPU usage is going even upto 1000. I am using `top` command.", "@ymodak @gadagashwini I just found out if I train on 2 GPUs then it takes 1s per step to train but when I use all 4 GPUs it takes 2s per step. I think it should train faster on more GPUs. \r\nI am following [this](https://www.tensorflow.org/beta/guide/distribute_strategy#mirroredstrategy) for placing on particular devices.\r\nAlso, when I am training on first 2 GPUs(GPU:0 and GPU:1) and the other two (GPU:2 and GPU:3) are idle, now if I want to train on the idle 2 GPUs, it is throwing `ResourceExhaustedError: OOM when allocating tensor with shape[150,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill] name: Adam/bidirectional/forward_lstm/kernel/m/Initializer/zeros/\r\n`\r\nI can also share the full log of error if required.\r\nPlease help. ", "If you use distribution strategy, could you  try `tf.distribute.experimental.CentralStorageStrategy`?", "Hi @yuefengz , thank you for your suggestion. I tried this by implementing it in the same way as `MirroredStrategy` so like \r\n```\r\ncentral_strategy = tf.distribute.experimental.CentralStorageStrategy()\r\nwith central_strategy.scope():\r\n--model def & compilation--\r\n```\r\nRead about it from [here](https://www.tensorflow.org/beta/guide/distribute_strategy#centralstoragestrategy) too.\r\nUnfortunately, I am receiving following error, please correct me if my implementation is wrong.\r\n````\r\nTraceback (most recent call last):\r\n  File \"model_with_tfsplit.py\", line 82, in <module>\r\n    model.add(tf.keras.layers.Embedding(vocab_size, 150, input_length = 4))\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 175, in add\r\n    layer(x)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 687, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2005, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 296, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/layers/embeddings.py\", line 134, in build\r\n    constraint=self.embeddings_constraint)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2175, in __setattr__\r\n    if val.trainable:\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/variables.py\", line 475, in trainable\r\n    raise NotImplementedError\r\nNotImplementedError\r\n2019-08-06 08:11:01.889111: I tensorflow/core/common_runtime/eager/execute.cc:585] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\n````\r\nThe error log is huge, so sharing only the last part of it. If you require, I can share the full log.", "I have a question from all of you, @ymodak @gadagashwini @yuefengz , do you think if I use Tf1.14 I can achieve the task of training embedding layer with bigger dimension on multiple GPUs efficiently without any problems that I am facing in TF2.0?. If yes, then could you please share some helpful links for converting the code into TF 1.x. I am new in TF 1.x. Thank you!", "I have another question. I want to continue training after loading the lastly saved model for which I am defining `tf.keras.models.load_mode(my_model.h5)` inside `mirror.scope()`. Since I already compiled it during training earlier so I don't compile it again as also mentioned [here](https://keras.io/getting-started/faq/). On execution, I am receiving following error\r\n````\r\nFile \"model_with_tfsplit.py\", line 94, in <module>\r\n    model =tf.keras.models.load_model('TF_model_onfull_2_03.h5') # Loading for retraining\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 138, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 187, in load_model_from_hdf5\r\n    model._make_train_function()\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2015, in _make_train_function\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 500, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 391, in get_gradients\r\n    grads = gradients.gradients(loss, params)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/gradients_impl.py\", line 158, in gradients\r\n    unconnected_gradients)\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/gradients_util.py\", line 541, in _GradientsHelper\r\n    for x in xs\r\n  File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/distribute/values.py\", line 716, in handle\r\n    raise ValueError(\"`handle` is not available outside the replica context\"\r\nValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call\r\n`````\r\nEven if I choose a single GPU, it still ends up with the same error. If I do not use `mirror.scope()` then the training starts from beginning but after few steps it reaches the same accuracy and loss value like before the model was saved. Although not sure if this behaviour is normal or not.", "@rishabhsahrawat You might want to try `tf.compat.v1.disable_v2_behavior()` in TF2.0; basically, it will allow you to 1) see if the issues are caused by v2 behaviors (in which case you can then play with the finer mechanisms-disabling compatibility instructions in order to understand where the problem emerges from) and 2) see which parts of your code actively depend on v2 behaviors (which will thus raise exceptions and require some code changes).\r\n\r\nIn my limited experience, if you are using the keras API, you do not need to change that much things to run code compatible with TF1.14. The main changes are Eager being disabled (which disables a few things and requires stricter implementation of some others, notably as to type casting, which is somewhat more flexible with Eager enabled), and the necessity, when not using keras, to manually set up placeholders, session objects and initialization instructions; but the latter points are automated through keras backend, so you probably will not have to go through them.\r\n\r\nGood luck, and if you run into specific problems, do ask for support :-)", "Hi @ymodak any updates on the issue?. I am still struggling with it. ", "For the problem with `CentralStorageStrategy`, could you show a colab or something to reproduce the issue?", "With the latest nightly build, I am able to use larger dimension (max 1000) but not bigger. Also, after raising the issue I started building the model in Tf1.14 and it also has the same issue only so value upto 400 works but not higher.", "Hi Rishabh, \r\n\r\nGenerally, an embedding layer in Keras uses more RAM than you would expect. To get the RAM requirements do: no. tokens * no. dimensions * dtype * approx. 15. So for 400k tokens, 1k dimensions, 32 bit dtype you would need about 1.6 * 15 which is about 24 GB. This probably loads on the CPU because it is able to use your swapfile. If the weights were simply in a NumPy array it would only take up about 1.6 GB but Keras / TF is really greedy. I really wish it were less greedy because it would be so much faster to start up a model =(  \r\nMaybe PyTorch is better?", "@rishabhsahrawat Closing as this is not a bug, but feature request for supporting sharded embeddings in sync training. Please raise a feature request as its working intended. Thanks!.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31162\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31162\">No</a>\n", "> Hi @yuefengz , thank you for your suggestion. I tried this by implementing it in the same way as `MirroredStrategy` so like\r\n> \r\n> ```\r\n> central_strategy = tf.distribute.experimental.CentralStorageStrategy()\r\n> with central_strategy.scope():\r\n> --model def & compilation--\r\n> ```\r\n> \r\n> Read about it from [here](https://www.tensorflow.org/beta/guide/distribute_strategy#centralstoragestrategy) too.\r\n> Unfortunately, I am receiving following error, please correct me if my implementation is wrong.\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"model_with_tfsplit.py\", line 82, in <module>\r\n>     model.add(tf.keras.layers.Embedding(vocab_size, 150, input_length = 4))\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n>     result = method(self, *args, **kwargs)\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 175, in add\r\n>     layer(x)\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 687, in __call__\r\n>     self._maybe_build(inputs)\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2005, in _maybe_build\r\n>     self.build(input_shapes)\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\", line 296, in wrapper\r\n>     output_shape = fn(instance, input_shape)\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/layers/embeddings.py\", line 134, in build\r\n>     constraint=self.embeddings_constraint)\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2175, in __setattr__\r\n>     if val.trainable:\r\n>   File \"/home/rishabh/.local/lib/python2.7/site-packages/tensorflow_core/python/ops/variables.py\", line 475, in trainable\r\n>     raise NotImplementedError\r\n> NotImplementedError\r\n> 2019-08-06 08:11:01.889111: I tensorflow/core/common_runtime/eager/execute.cc:585] Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\r\n> ```\r\n> \r\n> The error log is huge, so sharing only the last part of it. If you require, I can share the full log.\r\n\r\nI have got the same error when using CentralStorageStrategy (2.4.1). It can be reproduced by changing the strategy in https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/parameter_server_training.ipynb.\r\n\r\nIt only appears when I used multiple gpus and embedding in my model. For single gpu, it works, and for models that do not have embedding, it also works.\r\n\r\n\r\nLog:\r\n```\r\nWARNING:tensorflow:Model was constructed with shape (None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.string, name='feature'), name='feature', description=\"created by layer 'feature'\"), but it was called on an input with incompatible shape (3,).\r\nINFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Error reported to Coordinator: \r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 228, in _call_for_each_replica\r\n    **merge_kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 667, in wrapper\r\n    return converted_call(f, args, kwargs, options=options)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 376, in converted_call\r\n    options=options)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 350, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options, False)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 478, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 683, in _distributed_apply\r\n    var, apply_grad_to_update_var, args=(grad,), group=False))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2494, in update\r\n    return self._update(var, fn, args, kwargs, group)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/parameter_server_strategy.py\", line 560, in _update\r\n    **self._select_single_value(kwargs))\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 667, in wrapper\r\n    return converted_call(f, args, kwargs, options=options)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 396, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 478, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 654, in apply_grad_to_update_var\r\n    grad.values, var, grad.indices, **apply_kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 1215, in _resource_apply_sparse_duplicate_indices\r\n    **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adam.py\", line 214, in _resource_apply_sparse\r\n    m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 1243, in _resource_scatter_add\r\n    return x.value()\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\", line 460, in value\r\n    raise NotImplementedError\r\nNotImplementedError\r\n\r\n\r\n```\r\n@yuefengz has it been fixed?\r\n", "@liyinhgqw @yuefengz do you fix it? I  got the same error changing MirroredStrategy to CentralStorageStrategy because of OOM, on python3.7, tensorflow2.4, linux."]}, {"number": 31161, "title": "Cannot convert a Tensorflow GraphDef model to tflite format", "body": "<h1> Description </h1>\r\n\r\nI have been trying to convert a custom fastai based model to tflite and tf-js for deployment purpose. I was able to convert my fastai based model to onnx format and then to tensorflow format. I was able to perform inference on tensorflow model. However, when i try to convert the same model to tflte and tf-js i get error using the tflite converter API.\r\nI get multiple errors in sequence.\r\n1. the converter always searches for saved_model.pb | saved_model.pbtxt file only\r\n![image](https://user-images.githubusercontent.com/16400126/62123641-fe95dd00-b2e5-11e9-9d12-dacc246024bc.png)\r\n1. This error is solved by saving the pb file by name of saved_model.pb However, i face the next error after this which is something related to not finding right tags in metagraph.\r\n![image](https://user-images.githubusercontent.com/16400126/62123741-37ce4d00-b2e6-11e9-82ed-2e1744dfe88d.png)\r\n\r\nI am not able to resolve this issue, I have tried to create a session and add variables an tags to my graph but to no success. \r\n![image](https://user-images.githubusercontent.com/16400126/62123900-90054f00-b2e6-11e9-9e56-ed4dd69f8070.png)\r\n\r\nPlease help with this.\r\n", "comments": ["@osheenn ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@oanush sorry for the delay, i am using windows 10, the tensorflow version is 1.13.1. I have converted a pytorch model to tensorflow using onnx but now i cant convert it into tf-js.\r\nLet me know what details you are looking for.\r\nThanks!"]}, {"number": 31160, "title": "same one line code but get different type.", "body": "```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# 2.0.0-beta1\r\nprint(tf.__version__)\r\n\r\n# [P1]Out result is: tensorflow.python.framework.ops.EagerTensor\r\nprint(type(tf.random.uniform([], maxval=1.0)))\r\n\r\ndef preprocess(n):\r\n    # [P2]Out result is: <class 'tensorflow.python.framework.ops.Tensor'>\r\n    print(type(tf.random.uniform([], maxval=1.0)))\r\n    return n\r\n\r\nds = tf.data.Dataset.from_tensor_slices(tf.cast([1], tf.float32))\\\r\n.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\r\n.repeat()\r\n\r\n```\r\nThere is one line same code '**type(tf.random.uniform([], maxval=1.0))**' in two positions [P1],[P2], but we got two different type, one is EagerTensor and the other is Tensor. \r\n\r\nSome people know why? Thanks for your reading.\r\n", "comments": ["Hi,\r\n\r\nI have an imprecise answer to your question, which should hopefully clarify things for you but might not be entirely accurate from a technical standpoint.\r\n\r\nIn version 2.0, Eager execution is enabled by default, hence any Tensor you create will be wrapped as an EagerTensor. This is what shows up at position [P1] in your code.\r\n\r\nThat being said, TensorFlow relies on building computation graphs, and when possible (basically, every time you use a \"behind-the-scenes\", possibly lower-level, mechanism) a single graph is being built which does not work in Eager mode and is maintained in the background to be reused on different Tensors (with similar specs). In your case, calling `tf.data.Dataset.map` on a function will trigger the conversion of this function (using Autograph and such) so as to define such a re-usable graph. Hence, in the body of the converted function (which is the one being actually called), processed Tensors are no longer Eager. This is also why you sometimes need to wrap some operations that cannot be converted with `tf.py_function`, so that they will be preserved \"as-is\" in spite of this behaviour (see an example in [this tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer)).", "By the way, no offence and I am happy to try to help you, but this is not an issue _per se_ and would be better asked on a questions-answering website such as StackOverflow rather than here ;-)"]}, {"number": 31159, "title": "gdr: Fix build error introduced by commit ea4cbee", "body": "Ping @dubey to review.", "comments": []}, {"number": 31158, "title": "R1.5", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31158) for more info**.\n\n<!-- need_sender_cla -->"]}]