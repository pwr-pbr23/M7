[{"number": 25084, "title": "Added release notes for 1.13 release", "body": "", "comments": ["@aselle, are the changes in #24666 getting into this release? If so, shouldn't the fix and compatibility with OpenSSL 1.1 be added to the release notes?", "Folks, could you please mention that we've added support of Apache Ignite File System in this release (see https://github.com/tensorflow/tensorflow/pull/22194 PR and [this documentation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ignite#distributed-file-system)).\r\n\r\nI think we could use language like \"_Ignite File System plugin added to contrib/ignite that allows to work with IGFS file system_\".", "@bemeurer, no  the branch was created in december, so that change is not going to be in 1.13.\r\n@dmitrievanthony, yes I'll add that.", "@dmitrievanthony, this seems to already be in the 1.12 release notes.\r\n```\r\n * Ignite Dataset added to contrib/ignite that allows to work with Apache\r\n    Ignite.\r\n```", "@aselle Is there any chance / process for that to be changed, and for the OpenSSL fixes to go into this version? The lack of it makes tensorflow unbuildable on environments with OpenSSL 1.1", "@aselle, I'm talking about another functionality. In 1.2 we have released Ignite **Dataset** (see https://github.com/tensorflow/tensorflow/pull/22210) that allows to read training data from Ignite Cache (like from tables in database). In 1.3 we are about to release Ignite **File System** (see https://github.com/tensorflow/tensorflow/pull/22194) that allows to use IGFS (file system on top of Apache Ignite) as a TensorFlow file system to make checkpoints and logging.\r\n\r\nI think we need to reflect that in release notes.", "Folks, please take my last comment into account before merge.", "@dmitrievanthony, I added a note under tf.contrib section", "Thanks, @aselle! I forgot to mention that in this release we also added ability to start/stop TensorFlow server from Java API (see https://github.com/tensorflow/tensorflow/pull/23022). Perhaps it's too small fix to mention it in release notes, so just to remind.", "If you want this mentioned please add it as a PR to the 1.13 branch in\ngithub. I'll put it into the final release notes.\n\nOn Wed, Jan 23, 2019 at 1:07 AM Anton Dmitriev <notifications@github.com>\nwrote:\n\n> Thanks, @aselle <https://github.com/aselle>! I forgot to mention that in\n> this release we also added ability to start/stop TensorFlow server from\n> Java API (see #23022 <https://github.com/tensorflow/tensorflow/pull/23022>).\n> Perhaps it's too small fix to mention it in release notes, so just to\n> remind.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/25084#issuecomment-456724021>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52pzW87wkjQvf-j1tQSo1ZmkLShisks5vGCY5gaJpZM4aLwjy>\n> .\n>\n"]}, {"number": 25083, "title": "Object Detection API Android demo performance issue", "body": "**System information**\r\n- Windows 10:\r\n- Android app testing on LG Nexus 5:\r\n- TensorFlow installed from source:\r\n- TensorFlow version: 1.12\r\n**Describe the problem**\r\nI have built Object Detection API Android demo and testing on LG Nexus 5. But it has following performance issue:\r\nAfter some object is detected e.g. \"person\", it is keeping to show bounding box and score for last detected object even if the camera is capturing scene with no objects e.g. black screen.  Screenshots are given below.\r\n**Any other info / logs**\r\nProject has been built using following configuration: \r\ndef nativeBuildSystem = 'none'\r\nTrained Model: SSD MobileNet\r\n<b>Screenshots</b>\r\nWith object:\r\n![screenshot_20190122-094725](https://user-images.githubusercontent.com/27653152/51506149-1cadec80-1e2e-11e9-9cf6-6582e5448cf3.png)\r\nWithout object:\r\n![screenshot_20190122-100945](https://user-images.githubusercontent.com/27653152/51506151-1fa8dd00-1e2e-11e9-9539-a6f5ddd699ce.png)\r\n\r\n\r\n", "comments": ["@anamozov Could you provide a code to reproduce the bug? More details will help in finding root-cause of the issue. Thanks!\r\n", "@jvishnuvardhan Thanks for reply. The code is the original Tensorflow Object Detection API Android demo code: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android](url)", "@anamozov You need to compile the \"libtensorflow_demo.so\" for your phone architecture, it is used for the tracking. First try to add this to your project: [https://github.com/miyosuda/TensorFlowAndroidDemo/tree/master/app/src/main/jniLibs](url).", "The issue is resolved by adding object tracking to the project, which is building the project using bazel.", "@jvishnuvardhan, @FranciscoSP  I'm also facing one issue in the tenserflow demo app. \r\n Issue : Showing lag on camera preview, happening only on google pixel device (Android OS: Pie), Could you please help me this?\r\n\r\n", "@salim124 can you explain the issue in more detail?"]}, {"number": 25082, "title": "Wish help: Does the tf.Session.run() function process each tensor one-by-one in a batch??", "body": "Hi,\r\n\r\nCurrently, I would like to use the tf.Session.run() function to train a CNN model. For this mission, the input data to the model is a batch (4D, [batch_size, height of image, width of image, depths]) contains 16 image blocks. My understanding is that the 16 blocks should be processed by the CNN model one-by-one, and then calculate the mean loss based on the 16 outputs to update the model parameters (one batch data has been processed).  \r\n\r\nMay I ask if the tf.session.run() will process the image block (a tensor) one-by-one?\r\n\r\nThank you in advance.", "comments": ["Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 25081, "title": "checkpoint.restore not working for tf.keras custom model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Mac OS High Sierra\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): use default tensorflow on Colab ( 1.12.0 ) I installed 1.12.0 binary for my Mac with pip install tensorflow\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Using CPU on Mac\r\n- GPU model and memory: GPU on Colab. Using CPU on Mac. \r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI followed [nmt_with_attention.ipynb](https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb) on Tensorflow Tutorial.\r\nSaved the weight by \r\n```\r\n# download fines\r\nfrom google.colab import files\r\n\r\nfiles.download( \"./training_checkpoints/checkpoint\" ) \r\nfiles.download( \"./training_checkpoints/ckpt-5.index\" ) \r\nfiles.download( \"./training_checkpoints/ckpt-5.data-00000-of-00001\" ) \r\n```\r\nThen I load the weight on my Mac. Source code are same. I only use \r\n```\r\ncheckpoint_dir = '<my local directory>'\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n                                 encoder=encoder,\r\n                                 decoder=decoder)\r\n\r\n# restoring the latest checkpoint in checkpoint_dir\r\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\r\n```\r\n\r\n**Describe the expected behavior**\r\ntranslate test function return same results but \r\n\r\nOn training model, I got \r\n```\r\nInput: <start> hace mucho frio aqui . <end>\r\nPredicted translation: it s very cold here . <end>\r\n```\r\n\r\nOn restored model, I got\r\n```\r\nInput: <start> hace mucho frio aqui . <end>\r\nPredicted translation: shame back look right work to get right work to get \r\n```\r\n\r\nLooks almost random.\r\nHow can I save and load weight of tf.keras custom model properly?\r\nOr does nightly solve the problem?\r\n\r\n**Code to reproduce the issue**\r\nI use Tensorflow Tutorial's code and just restore weight on different session.\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["I also tested on [text_generation tutorial](https://www.tensorflow.org/tutorials/sequences/text_generation).\r\n\r\nI tried tf.keras.Model subclassing.\r\n```\r\nclass Model(tf.keras.Model):\r\n  def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\r\n    super(Model, self).__init__()\r\n    self.rnn_units = rnn_units\r\n    self.batch_size = batch_size\r\n\r\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n\r\n    if tf.test.is_gpu_available():\r\n      self.gru = tf.keras.layers.CuDNNGRU(self.rnn_units, \r\n                                          return_sequences=True, \r\n                                          return_state=True, \r\n                                          recurrent_initializer='glorot_uniform')\r\n    else:\r\n      self.gru = tf.keras.layers.GRU(self.rnn_units, \r\n                                     return_sequences=True, \r\n                                     return_state=True, \r\n                                     recurrent_activation='sigmoid', \r\n                                     recurrent_initializer='glorot_uniform')\r\n\r\n    self.fc = tf.keras.layers.Dense(vocab_size)\r\n        \r\n  def call(self, x, hidden):\r\n    x = self.embedding(x)\r\n\r\n    # output shape == (batch_size, max_length, hidden_size) \r\n    # states shape == (batch_size, hidden_size)\r\n\r\n    # states variable to preserve the state of the model\r\n    # this will be used to pass at every step to the model while training\r\n    output, states = self.gru(x, initial_state=hidden)\r\n\r\n    # The dense layer will output predictions for every time_steps(max_length)\r\n    # output shape after the dense layer == (max_length * batch_size, vocab_size)\r\n    x = self.fc(output)\r\n\r\n    return x, states\r\n  \r\n  def initialize_hidden_state(self):\r\n    return tf.zeros((self.batch_size, self.rnn_units))\r\n```\r\nAnd trained the model with loop like...\r\n```\r\n# Training step\r\nEPOCHS = 5\r\n\r\nfor epoch in range(EPOCHS):\r\n    start = time.time()\r\n    \r\n    # initializing the hidden state at the start of every epoch\r\n    # initally hidden is None\r\n    # hidden = model.reset_states()\r\n    hidden_f = model.initialize_hidden_state()\r\n    \r\n    \r\n    for (batch_n, (inp, target)) in enumerate(dataset):\r\n          with tf.GradientTape() as tape:\r\n              # feeding the hidden state back into the model\r\n              # This is the interesting step\r\n              predictions, _ = model(inp, hidden_f)\r\n              loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\r\n              \r\n          grads = tape.gradient(loss, model.trainable_variables)\r\n          optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n          if batch_n % 100 == 0:\r\n              template = 'Epoch {} Batch {} Loss {:.4f}'\r\n              print(template.format(epoch+1, batch_n, loss))\r\n\r\n    # saving (checkpoint) the model every 5 epochs\r\n    if (epoch + 1) % 5 == 0:\r\n      model.save_weights(checkpoint_prefix.format(epoch=epoch))\r\n\r\n    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\r\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\r\n\r\nmodel.save_weights(checkpoint_prefix.format(epoch=epoch))\r\n```\r\n\r\nThen save, and load the weight on both Colab and my Mac environment\r\n\r\nColab results: \r\n```\r\nROMEO: I have are to my eage;\r\nAnd bring the resolting bre his par and sattering with liver\r\nAnd duked, must home thy state as For't\r\nWish boying wifed threal. Sup untalther bare\r\nTaked Harrion Prophee, badich his lark's\r\nOf race on Anoening to to't.\r\n\r\nCOMINIUS:\r\nThat, as thou may'st from two and hands\r\nOur grave frield these graviors sounds to wills.\r\n\r\nEETOLYEUS:\r\nAre you shall have you some interray\r\nAbove bid ense would not-wargent; and to your after to his\r\nhead, but breathes,' do you see. The king of our mustress\r\nHad make me while a brother's bravound with up,\r\nAge'llows her appeace of my pates.\r\n\r\nCAPULET:\r\nCome, come worthing, we promest our pressing meding\r\nfor borts, sin. I'll satite to his encas;\r\nWhich is night is ten, there not proy.\r\n\r\nLEONTES:\r\nThou be a man's time,\r\nTo speak to the hour and mistress, bray touch in die;\r\nIn her my heart, with way of him: out.\r\n\r\nRAgher:\r\nNay, therefore suxs my lord for our cause and papt back,\r\nWith nothing, gull'd Parria. Say, 'tis as common behave\r\nBy oue rive sight, as \r\n```\r\n\r\nMy Mac results: \r\n```\r\nROMEO: no is.\r\n\r\nSICANTIA:\r\nNo morrow, shall come the house at to make a foul will?\r\n\r\nPAULINA:\r\n'sis she's nowly leave of lat,\r\nBeing boy; now, go,--\r\nQUEEN MARGAREP:\r\nGod for the sen: to--\r\nDort many the world's likil, and not of you.\r\n\r\nCATUS:\r\nGood mind, Should, 'Tis tounders tranct opprains and dogh actornim's have.\r\n\r\nGLOUCESTER:\r\n\r\nCLARENCE:\r\n'Twas some pother in the child of York.\r\n\r\nLADY CAPULET:\r\nAs I thank me in the death, where then for all good lord\r\nAnd tlack you'll know; surman's basiness dogs, thou doth appared dishopitiee,\r\nA sighty call-bulling eecest such most lost can refromes;\r\nTenting orme strength up and grief abouts worshia,\r\nfeeds that I shall kindded's exe incention.\r\n\r\nMENENIUS:\r\nNo, so, I'll to your son, werries what\r\nfair's pard, in pasis a phoptery wiedds\r\nTo see thy heapher. Weak, but that's the maids:\r\nTherefore, Good me blood, good sim.--I\r\nAy, free, sir, we heart frowhs, therefore's elmous countend:\r\nBut then you are our seemina to the specken\r\nBest, which art us home no merry of themsent\r\nTo \r\n```\r\n\r\nColab tensorflow version: 1.13.0-dev20190122\r\n\r\nMy Mac tensorflow version: 1.13.0a-dev20181218\r\n\r\nSince this example always returns different results, it's a bit difficult to evaluate but I think even my Mac can generate okay results. So I think I can say the model loaded the weights successfully.\r\n\r\nSo my guess is tf-nightly ( 1.13.0 ) already fixed the bug...\r\nIf someone know about this, please let us know.\r\n\r\nThanks in advance.", "@yashk2810 Can you PTAL? Thanks\r\n", "The problem here is that you are using both CuDNNGRU and GRU. So when you train your model in Colab and on a GPU, CUDNNGRU is used. But when you load the weights on a CPU, GRU is getting used. In Tensorflow 1.x, the variables of these two GRU's are not interchangeable.\r\n\r\nBut this has been fixed in TF 2.0. But I should warn you that colab is not on CUDA 10 yet, so you won't be able to train with a GPU in TF 2.0. (cc: @craigcitro to confirm this).\r\n\r\nCan you please try out these examples:\r\n\r\n1. https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/nmt_with_attention.ipynb\r\n2. https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/text_generation.ipynb\r\n\r\nThanks", "@yashk2810 Actually, as it happens, CUDA 10 landed in Colab today. \ud83d\ude00 ", "That's awesome. Then you should be able to train your models on a GPU with TF 2.0. \r\n\r\n!pip install tf-nightly-gpu-2.0-preview", "Closing this issue since its resolved. Fee free to reopen if the issue still persists. Thanks!"]}, {"number": 25080, "title": "MirroredStrategy is not supported by ExponentialUpdateLossScaleManager and LossScaleOptimizer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\ntf-nightly-gpu==1.13.0.dev20181022\r\n- Python version:\r\n3.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nCuda 9.2\r\n- GPU model and memory:\r\n1x V100\r\n\r\n----------\r\n* TLDR: MirroredStrategy is not supported by [ExponentialUpdateLossScaleManager](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_manager.py#L104) and [LossScaleOptimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_optimizer.py#L28)\r\n\r\n* When attempting to use the LossScaleOptimizer with a MirroredStrategy, we get the following error: \r\n\r\n    * ```ValueError: Outputs of true_fn and false_fn must have the same type: int64, bool``` emanating from [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_optimizer.py#L158).\r\n    We are able to get around this line by changing:\r\n     ```is_overall_finite, true_apply_gradients_fn, gen_control_flow_ops.no_op)```\r\n    To:\r\n     ```is_overall_finite, true_apply_gradients_fn, lambda: tf.zeros([1], tf.int64))```\r\n\r\n* Note, this error does not occur if MirroredStrategy is not enabled, and training proceeds as normal.\r\nIf we make the above change, then we can run the LossScaleOptimizer +  FixedLossScaleManager with MirroredStrategy enabled.\r\n\r\n* However, if we attempt to use the ExponentialLossScaleManager  with MirroredStrategyEnabled instead of the FixedLossScaleManager we get the following:\r\n```ValueError: You must specify an aggregation method to update a MirroredVariable in Tower Context\".``` emanating from this line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/mixed_precision/python/loss_scale_manager.py#L170\r\n\r\n* This suggests that variables introduced and used by ExponentialLossScaleManager need to be aware of MirroredStrategy.  What exactly do we need to do to make sure ExponentialLossScaleManager variables can be handled by MirroredStrategy?  \r\n\r\n* Our suspicion is that we need to handle variables from ExponentialLossScaleManager in _create_slots(), _prepare(), _apply_dense(), and _apply_sparse() as suggested by this part of the code in Optimizer https://github.com/tensorflow/tensorflow/blob/f9b9cf52eb36a5d5e8bbf96fc1f2b6f584ce7867/tensorflow/python/training/optimizer.py#L552\r\nIn addition, [this comment](https://github.com/tensorflow/tensorflow/issues/23986#issuecomment-444391385) (which has the same error as we do) suggested that reimplementing the optimizer is the right solution.", "comments": ["For what it's worth, I ended hacking my way through this by adding aggregation flags to [loss_scale](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/mixed_precision/python/loss_scale_manager.py#L140), [num_good_steps](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/mixed_precision/python/loss_scale_manager.py#L145) and [num_bad_steps](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/mixed_precision/python/loss_scale_manager.py#L147)\r\n\r\nFor example\r\n```\r\n    agg_type = tf.VariableAggregation.ONLY_FIRST_TOWER\r\n    synch_type = variable_scope.VariableSynchronization.ON_READ\r\n    self._loss_scale = variable_scope.variable(\r\n        name=\"loss_scale\",\r\n        initial_value=ops.convert_to_tensor(init_loss_scale, dtypes.float32),\r\n        dtype=dtypes.float32, trainable=False, \r\n        synchronization=synch_type, aggregation=agg_type)\r\n```", "I'm not sure that is the right way to do things, but just wanted to flag.", "contrib.mixed_precison will be replaced by a solution better integrated into Keras. These optimizers haven't been updated for DistributionStrategy and given contrib won't exist in 2.0 they won't. \r\n\r\n@reedwm @azaks2 FYI"]}, {"number": 25079, "title": "24374 Fix tf.einsum so it computes the trace correctly", "body": "This is in relation to and fixes issue #24374.  tf.einsum is not.  Einsum is not properly calculating Trace.  This fix functions by properly identifying a trace call and utilizing math_ops.trace.  Previously It would receive an error\r\n\r\n`Subscript not supported: an axis appears more than once: i`\r\nor something similar.  Before that error, people with older versions were receiving the wrongly calculated value.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Could you fix the lint errors?\r\n```\r\ntensorflow/python/ops/special_math_ops.py:242: [C0301(line-too-long), ] Line too long (107/80)\r\ntensorflow/python/ops/special_math_ops.py:242: [C0326(bad-whitespace), ] Exactly one space required around comparison\r\ntensorflow/python/ops/special_math_ops.py:243: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 10\r\n```", "> Could you fix the lint errors?\r\n> \r\n> ```\r\n> tensorflow/python/ops/special_math_ops.py:242: [C0301(line-too-long), ] Line too long (107/80)\r\n> tensorflow/python/ops/special_math_ops.py:242: [C0326(bad-whitespace), ] Exactly one space required around comparison\r\n> tensorflow/python/ops/special_math_ops.py:243: [W0311(bad-indentation), ] Bad indentation. Found 12 spaces, expected 10\r\n> ```\r\n\r\nwill do", "@drpngx  Fixed the linting errors", "Did you test it? For some reason it seems to fail.\r\n```\r\n    _ = special_math_ops.einsum('ii->', x)\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 181, in __exit__\r\n    self._raiseFailure(\"{} not raised\".format(exc_name))\r\n  File \"/usr/lib/python3.4/unittest/case.py\", line 130, in _raiseFailure\r\n    raise self.test_case.failureException(msg)\r\nAssertionError: ValueError not raised\r\n```", "Im in the process of building it right now, it takes awhile on my computer so I pushed to ensure i cant get any further feedback while i await the build, sorry for the confusion.  I will look into it", "I found the but related to that test, I have pushed the code up but I am still in testing.  My machine is taking some time to build so I pushed it in case I can get any early feedback", "The tests have appeared to have passed on my end, I havent had time to dig into all the logs but their appears to be no issues", "The tests passed on my end @drpngx ", "So this is working on my end, I'll continue to tweak it to see if there are any better approaches while I await feedback.  ", "Thoughts?", "OK, let me run the tests again to see what's wrong.", "Did you test this test?\r\n\r\n```\r\nFAIL: test_simple (__main__.EinsumTest)\r\ntest_simple (__main__.EinsumTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/special_math_ops_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/special_math_ops_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/special_math_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/special_math_ops_test.py\", line 292, in test_simple\r\n    self.run_test(case)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/special_math_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/special_math_ops_test.py\", line 359, in run_test\r\n    self.assertLess(err, 1e-8)\r\nAssertionError: 36.89950854256984 not less than 1e-08\r\n```", "I believe I did?  Let me go back and check", "So not quite sure if I missed it or what but I was able to find and fix the error/bug.  The failing test was due to the added trace case('ijji').  The output is correct from the function, the issue is that the unit test does not correctly calculate the trace.  For now I removed the test and have left only one simple test case for trace('ii').  Let me know if that case is enough, otherwise, I can look into adding it back in and adding in code to ensure the unit test calculates the right value.  @drpngx", "Yes, it would be nice to have that case work as well. Let me know when it's ready. Thanks!", "@drpngx I fixed the test and added it back in.  Thanks for the patience.  Its tested on my side and should be ready", "Thank you for the quick turnaround! Kicking off the tests...", "Could you fix the lint errors and reupload?\r\n\r\n```\r\nFAIL: Found 6 non-whitelited pylint errors:\r\ntensorflow/python/ops/special_math_ops_test.py:355: [C0326(bad-whitespace), ] Exactly one space required around assignment\r\ntensorflow/python/ops/special_math_ops_test.py:356: [C0326(bad-whitespace), ] Exactly one space required around comparison\r\ntensorflow/python/ops/special_math_ops_test.py:357: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\ntensorflow/python/ops/special_math_ops_test.py:357: [C0326(bad-whitespace), ] Exactly one space required after assignment\r\ntensorflow/python/ops/special_math_ops_test.py:358: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\ntensorflow/python/ops/special_math_ops_test.py:360: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\r\n```", "Yeah, sorry will fix those shortly", "Linting issues fixed @drpngx", "Testing...", "Tests run and things are linted on my side, not sure what is off with the internal builds\r\n", "Yeah the errors are unrelated. @ymodak it's ready.", "Yay!  Thanks for the review!", "Woohoo!"]}, {"number": 25078, "title": "ERROR: /tmp/tensorflow-1.9.0/tensorflow/BUILD:541:1: Executing genrule //tensorflow:python_api_gen failed (Exit 1) ", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Alpine 3.8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source (https://github.com/tensorflow/tensorflow/archive/v${TENSORFLOW_VERSION}.tar.gz)\r\n- TensorFlow version: 1.9.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:  Bazel\r\n- Bazel version (if compiling from source): 0.17.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA. CPU only\r\n- GPU model and memory: NA. CPU only\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuild fails with error\r\nERROR: /tmp/tensorflow-1.9.0/tensorflow/BUILD:541:1: Executing genrule //tensorflow:python_api_gen failed (Exit 1) \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nPFA the docker\r\n[Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/2780053/Dockerfile.txt)\r\n[build.log](https://github.com/tensorflow/tensorflow/files/2780059/build.log)\r\n\r\n\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last): File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"/usr/lib/python3.6/imp.py\", line 243, in load_module return load_dynamic(name, filename, file) File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic return _load(spec) ImportError: Error relocating /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so: je_tsd_tls: initial-exec TLS resolves to dynamic definition in /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 27, in <module> from tensorflow.python.util import tf_decorator File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 49, in <module> from tensorflow.python import pywrap_tensorflow File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module> raise ImportError(msg) ImportError: Traceback (most recent call last): File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File \"/root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"/usr/lib/python3.6/imp.py\", line 243, in load_module return load_dynamic(name, filename, file) File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic return _load(spec) ImportError: Error relocating /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so: je_tsd_tls: initial-exec TLS resolves to dynamic definition in /root/.cache/bazel/_bazel_root/7fa049be34bffde882bfd5021ad00c35/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/python/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/install_sources#common_installation_problems for some common reasons and solutions. Include the entire stack trace above this error message when asking for help.\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@p2jindia Please check the [tested build configurations](https://www.tensorflow.org/install/source#linux). For Tensorflow 1.9.0, Bazel 0.11.0 vesion is supported.  Could you downgrade your Bazel version and let us know how it progresses. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "This looks like an alpine/musl issue:  https://bugs.freedesktop.org/show_bug.cgi?id=35268\r\nGetting rid of STATIC_TLS is the current workaround."]}, {"number": 25076, "title": "tf.Estimator as checkpointable", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (or github SHA if from source): 1.12\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nValueError: `Checkpoint` was expecting a checkpointable object (an object derived from `CheckpointableBase`), got <tensorflow.python.estimator.canned.dnn.DNNClassifier object at 0x11b370e48>. If you believe this object should be checkpointable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.\r\n\r\n```\r\n\r\nModel:\r\n```\r\nself.classifier = tf.estimator.DNNClassifier(\r\n            feature_columns=my_feature_columns,\r\n            # Two hidden layers of 10 nodes each.\r\n            hidden_units=[10, 10],\r\n            # The model must choose between 3 classes.\r\n            n_classes=3)\r\n```\r\nI want to save checkpoints on demand in different directories. I use external optimizer for hyperparameter search ray-tune, which requires implementation of checkpointing which is control by scheduler from ray-tune optimizations and does not interfere into tf.Estimator internal saving procedure.  tf.train.Checkpoint looks like the perfect choice to save checkpoints on demand, yet does not work with tf.estimotor.", "comments": ["Can you use [`Estimator.export_saved_model`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_saved_model)? That will also export the Estimator's graph, but the SavedModel includes a training checkpoint. You could also peek inside the Estimator's model_fn, where there should be a training checkpoint just sitting around (assuming `train` was already called).\r\n\r\nI think this would be a great thing to do once Estimator uses functions referencing variables which are accessible outside the Estimator. That is a long way off. (CC @karmel )", "Hi @allenlavoie. Yes, I could use Estimator.export_saved_model. I prefer to relay on checkpointing, to avoid additional performance bottleneck related to exporting the model.\r\nI implemented a toy example how to use checkpointing with ray.tune and the tf.Estimator [here](https://github.com/agniszczotka/PBT-TF.Estimator-with-Ray.Tune/blob/master/trainable_estimator_with_pbt.py), which relays on saving checkpoint at the end of tf.Estimator.train(). \r\n\r\nIndeed. tf.Estimator as chekpointable would be perfect.\r\n", "@agniszczotka \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25076\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25076\">No</a>\n"]}, {"number": 25075, "title": "[GPU][ROCm] Fix copy rules for ROCm path", "body": "Addressed one bazel change in upstream TensorFlow:\r\n\r\ncommit 5bb89f16124ebbcff38a7bd6ec404cfa54ec9b91\r\n\r\nThe logic for the commit was sound, but didn't taken much consideration on the layout of ROCm header files. Fixed it in this PR.\r\n\r\nFix incorrect targets for ROCm headers, and how ROCm headers are copied into\r\nexecroot.", "comments": ["Logs from all 4 failed targets suggest they are not related to the modification introduced in this PR.", "@gunan may I request your help to review this minor change to the build system? Newly implemented copy rules need `-L` on ROCm path as some files are symbolic links in a standard ROCm installation."]}, {"number": 25074, "title": "Allow non-standard CUDA SDK installation directory when using clang as CUDA compiler", "body": "This fixes the following error when using clang as the CUDA compiler with a non-standard CUDA SDK installation directory by setting cuda-path to CUDA_TOOLKIT_PATH (as outlined here: https://llvm.org/docs/CompileCudaWithLLVM.html):\r\n\r\n```\r\nERROR: /user_data/.tmp/cache.srbo/bazel/_bazel_srbo/96431eb3bb8da25a1741c1c1ac91e19b/external/nccl_archive/BUILD.bazel:33:1: C++ compilation of rule '@nccl_archive//:nccl' failed (Exit 1): clang failed: error executing command \r\n  (cd /user_data/.tmp/cache.srbo/bazel/_bazel_srbo/96431eb3bb8da25a1741c1c1ac91e19b/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CLANG_CUDA_COMPILER_PATH=/builds/clang/5.0.1/771843fbd0/bin/clang \\\r\n    CUDA_TOOLKIT_PATH=/builds/cuda/8.0.61/68e269425a \\\r\n    CUDNN_INSTALL_PATH=/builds/cudnn/7/4fdffde0c4 \\\r\n    LD_LIBRARY_PATH=/builds/gcc/4.9.4/12a8bec7dd/lib64:/builds/gcc/4.9.4/12a8bec7dd/lib:/builds/ncurses/5.9/7514ad76af/lib:/builds/readline/6.0/19c81cf560/lib:/builds/python/2.7.8/db3474c23a/lib:/builds/hdf5/1.8.14/22c15aa4cc/lib:/builds/intel_mkl/11.1.3/53d6c44ec1/lib:/builds/cuda/8.0.61/68e269425a/lib64:/builds/cuda/8.0.61/68e269425a/lib:/builds/cudnn/7/4fdffde0c4/lib64:/builds/llvm/5.0.1/8294c740c5/lib:/builds/clang/5.0.1/771843fbd0/lib \\\r\n    PATH=/builds/gcc/4.9.4/12a8bec7dd/bin:/builds/ncurses/5.9/7514ad76af/bin:/builds/python/2.7.8/db3474c23a/bin:/builds/python_setuptools/18.2/effab46b3c/bin:/builds/hdf5/1.8.14/22c15aa4cc/bin:/builds/python_cython/0.22.1/a0ebdb4b77/bin:/builds/python_numpy/1.9.2/dec6a1e459/bin:/builds/python_wheel/0.32.3/5ea8be18ae/bin:/builds/cuda/8.0.61/68e269425a/bin:/builds/openjdk/1.8.0/4a629ebe98/bin:/builds/bazel/0.18.0/08c43177ba/bin:/builds/llvm/5.0.1/8294c740c5/bin:/builds/clang/5.0.1/771843fbd0/bin:/builds/dneg_bobtools/1.5.0/ea7a1f49e0/bin:/builds/app_wrappers/27deea27ba/bin:/bin:/usr/bin:/usr/sbin:/sbin:/etc:/usr/etc:/usr/local/bin:/usr/bin/X11 \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHONPATH=/builds/python_sitecustomize/720267664e/lib/python:/builds/python/2.7.8/db3474c23a/lib/python2.7/site-packages:/builds/python_setuptools/18.2/effab46b3c/lib/python:/builds/python_enum34/1.1.6/ac652bf4db/lib/python:/builds/python_cython/0.22.1/a0ebdb4b77/lib/python:/builds/python_numpy/1.9.2/dec6a1e459/lib/python:/builds/python_six/1.11.0/46e57fd0ea/lib/python:/builds/python_h5py/2.5.0/a63bb4b8bc/lib/python:/builds/python_keras_applications/1.0.6/e8b5c55fe4/lib/python:/builds/python_keras_preprocessing/1.0.5/a82d888721/lib/python:/builds/python_mock/1.0.1/bc8f23773f/lib/python:/builds/python_wheel/0.32.3/5ea8be18ae/lib/python:/builds/clang/5.0.1/771843fbd0/lib/python:/builds/dneg_bobtools/1.5.0/ea7a1f49e0/lib/python:/builds/dneg_bobhelper/1.0.0/bf3f7e07dc/lib/python \\\r\n    PYTHON_BIN_PATH=/builds/python/2.7.8/db3474c23a/bin/python \\\r\n    PYTHON_LIB_PATH=/builds/python/2.7.8/db3474c23a/lib/python2.7/site-packages \\\r\n    TF_CUDA_CLANG=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NCCL_VERSION=1 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /builds/clang/5.0.1/771843fbd0/bin/clang -MD -MF bazel-out/k8-opt/bin/external/nccl_archive/_objs/nccl/core.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/nccl_archive/_objs/nccl/core.cu.pic.o' -iquote external/nccl_archive -iquote bazel-out/k8-opt/genfiles/external/nccl_archive -iquote bazel-out/k8-opt/bin/external/nccl_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -Ibazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/nccl -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-DCUDA_MAJOR=0' '-DCUDA_MINOR=0' '-DNCCL_MAJOR=0' '-DNCCL_MINOR=0' '-DNCCL_PATCH=0' -Iexternal/nccl_archive/src -O3 -x cuda '-DGOOGLE_CUDA=1' '--cuda-gpu-arch=sm_35' '--cuda-gpu-arch=sm_52' '--cuda-gpu-arch=sm_60' '--cuda-gpu-arch=sm_61' -c bazel-out/k8-opt/genfiles/external/nccl_archive/src/core.cu.cc -o bazel-out/k8-opt/bin/external/nccl_archive/_objs/nccl/core.cu.pic.o)\r\nclang: error: cannot find libdevice for sm_35. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice.\r\nclang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\nclang: error: cannot find libdevice for sm_52. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice.\r\nclang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\nclang: error: cannot find libdevice for sm_60. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice.\r\nclang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\nclang: error: cannot find libdevice for sm_61. Provide path to different CUDA installation via --cuda-path, or pass -nocudalib to build without linking with libdevice.\r\nclang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\nclang: error: cannot find CUDA installation.  Provide its path via --cuda-path, or pass -nocudainc to build without CUDA includes.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 74.398s, Critical Path: 16.08s\r\nINFO: 512 processes: 512 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nBuilt using:\r\n`bazel build --distdir=distdir --action_env=PYTHONPATH --distinct_host_configuration=false //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@SimonBoorer Can you please sign the cla to move ahead with this PR? Thanks!", "@SimonBoorer Gentle reminder to please sign the cla in order to move ahead with this PR. Thanks!", "closing this PR due to lack of activity."]}, {"number": 25073, "title": "fit gets slower on consecutive calls when model is recompiled", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10 Home\r\n\r\n- TensorFlow installed from (source or binary):\r\nusing pip3\r\n- TensorFlow version (use command below):\r\nreproduced on both\r\nb'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0\r\nb'v1.11.0-rc2-4-gc19e29306c' 1.11.0\r\n\r\n- Python version:\r\nPython 3.6.7 :: Anaconda, Inc.\r\n\r\n**Describe the current behavior**\r\nOver multiple iterations with a re-compile of model, fit() time continuously increases.  If the re-compile is commented out, fit() time remains constant.  I found a similar bug in the js project but not sure if it's related: https://github.com/tensorflow/tfjs/issues/448\r\n\r\n**Describe the expected behavior**\r\nExpect fit() time to remain constant even with multiple re-compiles\r\n\r\n**Code to reproduce the issue**\r\n`\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Activation, Flatten\r\nimport numpy as np\r\n\r\ninput1 = np.array([[0, 1, 1, 1, 0, 0, 0, 1, 1, 1]])\r\ndOutput1 = np.array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\r\n\r\nmodel = Sequential()\r\ninitializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=1)\r\nmodel.add(Dense(25, input_dim=10, kernel_initializer=initializer, bias_initializer='zeros'))\r\nmodel.add(Activation('relu'))\r\ninitializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=2)\r\nmodel.add(Dense(25, kernel_initializer=initializer, bias_initializer='zeros'))\r\nmodel.add(Activation('relu'))\r\ninitializer = tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=3)\r\nmodel.add(Dense(10, kernel_initializer=initializer, bias_initializer='zeros'))\r\nmodel.add(Activation('softmax'))\r\n\r\n#loss='categorical_crossentropy'\r\nsgd = tf.keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n\r\nfor i in range(100):\r\n    sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\r\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n    model.fit(input1, dOutput1, epochs=1, batch_size=1)\r\n    output1 = model.predict(input1)\r\n    output1\r\n`\r\n\r\n**Output**\r\n1/1 [==============================] - 0s 155ms/step - loss: 2.2998 - acc: 0.0000e+00\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 131ms/step - loss: 2.2906 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 183ms/step - loss: 2.2815 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 211ms/step - loss: 2.2724 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 235ms/step - loss: 2.2633 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 221ms/step - loss: 2.2543 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 367ms/step - loss: 2.2452 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 324ms/step - loss: 2.2362 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 269ms/step - loss: 2.2271 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 307ms/step - loss: 2.2181 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 316ms/step - loss: 2.2091 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 315ms/step - loss: 2.2002 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 346ms/step - loss: 2.1912 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 404ms/step - loss: 2.1822 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 380ms/step - loss: 2.1732 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 417ms/step - loss: 2.1643 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 421ms/step - loss: 2.1553 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 442ms/step - loss: 2.1464 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 465ms/step - loss: 2.1375 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 486ms/step - loss: 2.1286 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 0s 468ms/step - loss: 2.1197 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 509ms/step - loss: 2.1108 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 544ms/step - loss: 2.1020 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 676ms/step - loss: 2.0932 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 543ms/step - loss: 2.0844 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 584ms/step - loss: 2.0756 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 608ms/step - loss: 2.0668 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 643ms/step - loss: 2.0580 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 651ms/step - loss: 2.0493 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 660ms/step - loss: 2.0406 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 664ms/step - loss: 2.0319 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 702ms/step - loss: 2.0232 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 735ms/step - loss: 2.0145 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 709ms/step - loss: 2.0059 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 773ms/step - loss: 1.9973 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 901ms/step - loss: 1.9887 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 902ms/step - loss: 1.9801 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 948ms/step - loss: 1.9715 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 865ms/step - loss: 1.9629 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 839ms/step - loss: 1.9544 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 987ms/step - loss: 1.9459 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 1s/step - loss: 1.9374 - acc: 1.0000\r\nEpoch 1/1\r\n1/1 [==============================] - 1s 1s/step - loss: 1.9289 - acc: 1.0000\r\nEpoch 1/1", "comments": ["I've been doing some playing around trying (unsuccessfully) to rig a temporary solution.  I thought it might have to do something with garbage collection and experimented with the following:\r\n* force collection with gc.collect() - doesn't work\r\n* save network to disk, delete variable referencing the NN using %xdel or %reset_selective, call gc.collect(), reload saved network from disk - doesn't work\r\n* save network to disk, full reset of kernel with %reset, reload saved network from disk - doesn't work\r\n\r\nThe only thing that I've found so far to bring fit() time back to initial run-time levels is restarting the kernel.", "So after more searching I learned about tf.Session().  I'm able to save the model to disk, call tf.keras.backend.clear_session(), reload the model and continue without the creeping calculation time.  I have not been able to figure out how to clear the session without completely destroying the model (and hence destroying the training to that point) without saving to disk.  Any suggestions?\r\nPointers to a good tutorial/write up/explanation of Sessions and what exactly is going on in the interaction between the C++ and python code would also be greatly appreciated (and might help me figure out an answer to my above question).  I've found the rfcs but no good design docs that clue me in on what's going on.\r\nBased on what little more I now know, this is a leak in the underlying c++ code?  I'll let someone who knows more than me decide if this bug should be closed or addressed in code.\r\nAlso curious if this would be addressed by the proposal to move from sessions to functions in the rfcs.", "*Using tensorflow 1.12.0*\r\n\r\nThank you for the hint with tf.keras.backend.clear_session()\r\nThat saved me!\r\nBefore my model was getting slower and slower and the RAM utilization grew by about 1.5 MB/s.\r\n\r\nDoing a objgraph.show_growth(limit=3)\r\nI get the following;\r\n```\r\ntuple 10304508 + 104274\r\nlist 591883 +8521\r\ndict 44840 + 5958\r\n```\r\n\r\nDestroying the model every time is fine for me since I do a hyperparameter search where I need to rebuild the model anyway but I guess it may be a bit of a pain in other situations.", "@markste-in glad that helped!  On the note of hyper-parameter search, I'm actually updating my models learning rate fairly often and have to re-compile every time.  I'm wondering if you (or anyone) knows why that's part of the model compilation as opposed to just a constant that can be passed in for the call to fit()...", "I don't know why it's part of the compile process (probably historical reasons in Keras?). I just know that you need to recompile if you change either the loss function, the optimizer/lr or the metrics according to [StackOverflow](https://stackoverflow.com/questions/47995324/does-model-compile-initialize-all-the-weights-and-biases-in-keras-tensorflow/47996024)\r\n\r\nI guess you can circumvent this if you use this little [trick](https://stackoverflow.com/questions/41533489/how-to-initialise-only-optimizer-variables-in-tensorflow) with plain tensorflow:\r\n```\r\noptimizer = tf.train.SomeOptimizer(learning_rate)\r\nsession.run(tf.variables_initializer(optimizer.variables()))\r\n```\r\n\r\nThen you could just do\r\n```\r\ntrain = optimizer.minimize(SomeCost)\r\nsess.run(train, feed_dict....)\r\n```\r\n\r\nBut I never tried that since I create a new model every time anyway.\r\n", "@mshiffer @fchollet @markste-in\r\n\r\nMemory leak is also observed when model is re-compiled:\r\n\r\n**Codes to demostrate memory leak:**\r\n\r\n```\r\nimport os\r\nimport psutil\r\nfrom tensorflow import keras\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(1, input_shape=(1,)))\r\nprocess = psutil.Process(os.getpid())\r\nfor count in range(1000):\r\n    model.compile(optimizer='sgd', loss='mse')\r\n    if count % 100 == 0:\r\n        print('#{} : mem = {} Byte'.format(count, process.memory_info().rss))\r\n```\r\n\r\n**Output**\r\n\r\n```\r\n#0 : mem = 284536832 Byte\r\n#100 : mem = 419696640 Byte\r\n#200 : mem = 554618880 Byte\r\n#300 : mem = 690429952 Byte\r\n#400 : mem = 826294272 Byte\r\n#500 : mem = 962461696 Byte\r\n#600 : mem = 1097363456 Byte\r\n#700 : mem = 1234108416 Byte\r\n#800 : mem = 1369239552 Byte\r\n#900 : mem = 1504563200 Byte\r\n```\r\n\r\n", "> @mshiffer @fchollet @markste-in\r\n> \r\n> Memory leak is also observed when model is re-compiled:\r\n\r\nCan confirm:\r\nUsed a slightly modified version:\r\n```\r\nimport os\r\nimport psutil\r\nimport objgraph\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nprint('Tensorflow: ',tf.__version__)\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Dense(1, input_shape=(1,)))\r\nprocess = psutil.Process(os.getpid())\r\nfor count in range(1000):\r\n    model.compile(optimizer='sgd', loss='mse')\r\n    if count % 100 == 0:\r\n        print('#{} : mem = {} Byte'.format(count, process.memory_info().rss))\r\n        objgraph.show_growth(limit=3)\r\n```\r\nand get the following output:\r\n```\r\nTensorflow:  1.12.0\r\n#0 : mem = 180330496 Byte\r\nfunction    38783    +38783\r\ntuple       22278    +22278\r\ndict        18244    +18244\r\n#100 : mem = 316846080 Byte\r\ntuple   545785   +523507\r\nlist     68151    +57902\r\ndict     57345    +39101\r\n#200 : mem = 454078464 Byte\r\ntuple  1069285   +523500\r\nlist    126051    +57900\r\ndict     96445    +39100\r\n#300 : mem = 594722816 Byte\r\ntuple  1592782   +523497\r\nlist    183950    +57899\r\ndict    135545    +39100\r\n```\r\n", "@fchollet Is there any timeline for this, or if not any way I can help to get it fixed?  Even my workaround makes the computation obnoxious because I have to serialize, write to disk, read from disk and de-serialize for each iteration...  I'd be happy to help out if doing so would help get a solution out.", "@ymodak Same question.  :)  I don't know the process for fixes/updates, but if I can help out in anyway I'd love to.  ", "Similar issue with tensorflow.keras.model.fit memory leak (tf_nightly 1.15.0-dev20190628):\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/overfit_and_underfit.ipynb\r\n\r\nMemory leak crash with following code:\r\n\r\nbaseline_history = baseline_model.fit(train_data,\r\n                                      train_labels,\r\n                                      epochs=20,\r\n                                      batch_size=512,\r\n                                      validation_data=(test_data, test_labels),\r\n                                      verbose=2)\r\n\r\ntcmalloc: large alloc 2000003072 bytes == 0x1f80fc000 @ 0x7f3ea0f47887 0x7f3e9f83dbf9 0x7f3e9f83eacb 0x7f3e9f83eb84 0x7f3e9f83ef6c 0x7f3e71909d79 0x7f3e719991be 0x7f3e719316f7 0x7f3e7193228c 0x7f3e718f0ffa 0x7f3e718f117a 0x7f3e74711dce 0x7f3e744e46e2 0x4f858d 0x4f98c7 0x4f7a28 0x4f876d 0x4f98c7 0x4f6128 0x4f426e 0x5a1481 0x512a60 0x53ee21 0x57ec0c 0x4f88ba 0x4fa6c0 0x4f6128 0x4f7d60 0x4f876d 0x4f98c7 0x4f6128\r\n\r\nreplace\r\nfrom tensorflow import keras\r\n\r\nwith below code to use keras directly rather than tensorflow.keras, and downgrade numpy to 1.16.2:\r\nimport keras\r\nimport keras.backend\r\n!pip install numpy==1.16.2\r\nimport numpy as np\r\n\r\nThen the memory leak and crash is not happening.\r\n\r\nHave also tested the notebook locally on windows 10, the crash could be prevented with pagefile increased to 200G, the pagefile could grow up to 160G with this notebook.\r\n\r\n\r\n\r\n", "@mshiffer Recently there were some updates related to performance. \r\n\r\nI think this was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/4ea1bda75a261a759263b896aab3948b/untitled37.ipynb) is the gist for reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you. Thanks!", "I am closing this issue as this was resolved in recent `tf-nightly`. Please feel free to reopen if this was not resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25073\">No</a>\n"]}, {"number": 25072, "title": "ImportError: DLL load failed: The specified procedure could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro (version: 1803, build: 17134.523)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A (using a Dell G5 5587 laptop)\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version: 1.12.0 (tensorflow-gpu)\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip (via PyCharm installer)\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GeForce 1060 with Max-Q Design (version: 24.21.14.1131)\r\n\r\n\r\n\r\n**Describe the problem**\r\nGetting an error regarding the not finding the dll file. The file I presume it's referring to is `\u202aC:\\tools\\cuda\\bin\\cudnn64_7.dll`, which has been added to the path (well, the folder containing it has).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nDownloaded/installed prerequisites according to TensorFlow site\r\nCreated virtual environment (within PyCharm)\r\nInstalled TensorFlow (using the automatic pip options within PyCharm)\r\nEntered python from the terminal\r\n`import tensorflow`\r\nError/traceback below produced\r\nTrawled online to try to find solutions.... ending with writing this\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\b5027438\\PycharmProjects\\DAFNI-UO\\venv2\\lib\\site-packages\\tensorflow\\__init__.py\", l\r\nine 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\b5027438\\PycharmProjects\\DAFNI-UO\\venv2\\lib\\site-packages\\tensorflow\\python\\__init__\r\n.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\b5027438\\PycharmProjects\\DAFNI-UO\\venv2\\lib\\site-packages\\tensorflow\\core\\framework\\\r\ngraph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\b5027438\\PycharmProjects\\DAFNI-UO\\venv2\\lib\\site-packages\\google\\protobuf\\descriptor\r\n.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n```\r\n", "comments": ["I've resolved the error following the instructions listed here:\r\nhttps://stackoverflow.com/questions/52092810/tensorflow-error-dll-load-failed-the-specified-procedure-could-not-be-found/53111377#53111377\r\n\r\nWhilst it's working, I think this probably needs addressing in the installation, as it goes against what TensorFlow apparently requires (and therefore installs automatically)!", "@trodaway Thanks for the link. Can you please tell which solution worked for you specifically?\r\nInstalling tf 1.6 wheel or lowering your protobuf version to 3.6.0 ", "@ymodak It was the lowering of the protobuf version (to 3.6.0) that seemed to do the trick.", "Closing since its resolved. Thanks!", "The Fix is not working with tensor-flow 2.0, It require protobuf >= 3.6.1", "> I've resolved the error following the instructions listed here:\r\n> https://stackoverflow.com/questions/52092810/tensorflow-error-dll-load-failed-the-specified-procedure-could-not-be-found/53111377#53111377\r\n> \r\n> Whilst it's working, I think this probably needs addressing in the installation, as it goes against what TensorFlow apparently requires (and therefore installs automatically)!\r\n\r\nThank you helped a lot", "Had the same issue. I was trying to solve it for more than 3 hours and finally Bingo, as easy as the following:\r\nPip uninstall protobuf\r\npip install protobuf==3.6.0\r\n", "Mostafa-Nakhaei, it's working too by doing the same procedure as you did"]}, {"number": 25071, "title": "Error loading transfer learning resnet model from frozen graph ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 25070, "title": "Use my data up bug?", "body": "In the project,\r\n[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands](url)\r\n\r\nMy code is down:\r\n\r\n`if __name__ == '__main__':\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\r\n      '--data_dir',\r\n      type=str,\r\n      default='D:/python/speechtf/data/',\r\n      help=\"\"\"\\\r\n      Where to download the speech training data to.\r\n      \"\"\")\r\n  parser.add_argument(\r\n      '--background_volume',\r\n      type=float,\r\n      default=0.1,\r\n      help=\"\"\"\\\r\n      How loud the background noise should be, between 0 and 1.\r\n      \"\"\")\r\n  parser.add_argument(\r\n      '--background_frequency',\r\n      type=float,\r\n      default=0.8,\r\n      help=\"\"\"\\\r\n      How many of the training samples have background noise mixed in.\r\n      \"\"\")\r\n  parser.add_argument(\r\n      '--silence_percentage',\r\n      type=float,\r\n      default=10.0,\r\n      help=\"\"\"\\\r\n      How much of the training data should be silence.\r\n      \"\"\")\r\n  parser.add_argument(\r\n      '--unknown_percentage',\r\n      type=float,\r\n      default=10.0,\r\n      help=\"\"\"\\\r\n      How much of the training data should be unknown words.\r\n      \"\"\")\r\n  parser.add_argument(\r\n      '--time_shift_ms',\r\n      type=float,\r\n      default=0.0,\r\n      help=\"\"\"\\\r\n      Range to randomly shift the training audio by in time.\r\n      \"\"\")\r\n  parser.add_argument(\r\n      '--testing_percentage',\r\n      type=int,\r\n      default=10,\r\n      help='What percentage of wavs to use as a test set.')\r\n  parser.add_argument(\r\n      '--validation_percentage',\r\n      type=int,\r\n      default=10,\r\n      help='What percentage of wavs to use as a validation set.')\r\n  parser.add_argument(\r\n      '--sample_rate',\r\n      type=int,\r\n      default=16000,\r\n      help='Expected sample rate of the wavs',)\r\n  parser.add_argument(\r\n      '--clip_duration_ms',\r\n      type=int,\r\n      default=100,\r\n      help='Expected duration in milliseconds of the wavs',)\r\n  parser.add_argument(\r\n      '--window_size_ms',\r\n      type=float,\r\n      default=30.0,\r\n      help='How long each spectrogram timeslice is.',)\r\n  parser.add_argument(\r\n      '--window_stride_ms',\r\n      type=float,\r\n      default=10.0,\r\n      help='How far to move in time between spectogram timeslices.',)\r\n  parser.add_argument(\r\n      '--feature_bin_count',\r\n      type=int,\r\n      default=40,\r\n      help='How many bins to use for the MFCC fingerprint',\r\n  )\r\n  parser.add_argument(\r\n      '--how_many_training_steps',\r\n      type=str,\r\n      default='25000,5000',\r\n      help='How many training loops to run',)\r\n  parser.add_argument(\r\n      '--eval_step_interval',\r\n      type=int,\r\n      default=400,\r\n      help='How often to evaluate the training results.')\r\n  parser.add_argument(\r\n      '--learning_rate',\r\n      type=str,\r\n      default='0.001,0.0001',\r\n      help='How large a learning rate to use when training.')\r\n  parser.add_argument(\r\n      '--batch_size',\r\n      type=int,\r\n      default=100,\r\n      help='How many items to train with at once',)\r\n  parser.add_argument(\r\n      '--wanted_words',\r\n      type=str,\r\n      default='1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88',\r\n      help='Words to use (others will be added to an unknown label)',)\r\n  parser.add_argument(\r\n      '--train_dir',\r\n      type=str,\r\n      default='D:/python/speechtf/train_model/',\r\n      help='Directory to write event logs and checkpoint.')\r\n  parser.add_argument(\r\n      '--save_step_interval',\r\n      type=int,\r\n      default=100,\r\n      help='Save model checkpoint every save_steps.')\r\n  parser.add_argument(\r\n      '--start_checkpoint',\r\n      type=str,\r\n      default='',\r\n      help='If specified, restore this pretrained model before any training.')\r\n  parser.add_argument(\r\n      '--model_architecture',\r\n      type=str,\r\n      default='conv',\r\n      help='What model architecture to use')\r\n  parser.add_argument(\r\n      '--check_nans',\r\n      type=bool,\r\n      default=False,\r\n      help='Whether to check for invalid numbers during processing')\r\n  parser.add_argument(\r\n      '--quantize',\r\n      type=bool,\r\n      default=False,\r\n      help='Whether to train the model for eight-bit deployment')\r\n  parser.add_argument(\r\n      '--preprocess',\r\n      type=str,\r\n      default='mfcc',\r\n      help='Spectrogram processing mode. Can be \"mfcc\" or \"average\"')\r\n\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)`\r\n\r\n\r\n\r\n`Traceback (most recent call last):\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Data too short when trying to read string\r\n\t [[{{node DecodeWav}} = DecodeWav[desired_channels=1, desired_samples=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:/python/speechtf/train4.py\", line 404, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"D:/python/speechtf/train4.py\", line 70, in main\r\n    FLAGS.testing_percentage, model_settings)\r\n  File \"D:/python/speechtf\\input_data4.py\", line 176, in __init__\r\n    self.prepare_background_data()\r\n  File \"D:/python/speechtf\\input_data4.py\", line 334, in prepare_background_data\r\n    feed_dict={wav_filename_placeholder: wav_path}).audio.flatten()\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Data too short when trying to read string\r\n\t [[node DecodeWav (defined at D:/python/speechtf\\input_data4.py:328)  = DecodeWav[desired_channels=1, desired_samples=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]\r\n\r\nCaused by op 'DecodeWav', defined at:\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\python\\lib\\idlelib\\run.py\", line 144, in main\r\n    ret = method(*args, **kwargs)\r\n  File \"D:\\python\\lib\\idlelib\\run.py\", line 474, in runcode\r\n    exec(code, self.locals)\r\n  File \"D:/python/speechtf/train4.py\", line 404, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"D:/python/speechtf/train4.py\", line 70, in main\r\n    FLAGS.testing_percentage, model_settings)\r\n  File \"D:/python/speechtf\\input_data4.py\", line 176, in __init__\r\n    self.prepare_background_data()\r\n  File \"D:/python/speechtf\\input_data4.py\", line 328, in prepare_background_data\r\n    wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\gen_audio_ops.py\", line 222, in decode_wav\r\n    desired_samples=desired_samples, name=name)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"D:\\python\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Data too short when trying to read string\r\n\t [[node DecodeWav (defined at D:/python/speechtf\\input_data4.py:328)  = DecodeWav[desired_channels=1, desired_samples=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ReadFile)]]`\r\n", "comments": ["Basically one of your files either has 0kb(empty) or less samples than the others and needs to be padded. Check your data folder for 0kb files.\r\n[Try this answer on stackoverflow.](https://stackoverflow.com/questions/50665532/tensorflow-speech-command-errordata-too-short-when-trying-to-read-string-when)", "> Basically one of your files either has 0kb(empty) or less samples than the others and needs to be padded. Check your data folder for 0kb files.\r\n> [Try this answer on stackoverflow.](https://stackoverflow.com/questions/50665532/tensorflow-speech-command-errordata-too-short-when-trying-to-read-string-when)\r\n\r\nThanks at first,\r\nBut I find the question, the function can't Decode wav file correctly\r\nWhen I test the function load_wav_file in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/input_data.py](url)\r\nthe bug ups,my codes are as follows\r\n\r\n`import tensorflow as tf`\r\n`from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio`\r\n`from tensorflow.python.ops import io_ops`\r\n\r\n`file='/6jdsgfakjgfsdsajkdfhkfp_.wav'`\r\nthe file link is [http://blog.sciencenet.cn/blog-1966190-1158442.html](url)\r\nyou can copy it to your Web browser then enter ,and find the wav file.\r\n`data=load_wav_file(file)`\r\n\r\nAnd then the same bug ups, I don't know why, Could U help me?\r\n\r\n\r\n", "Dear,\r\nCan't decode the wav file correctly,who can convert it to normal wav file that the function can decode.\r\nAny advice or suggestion will be okey.\r\nThx", "I am closing the issue as it a support question. IN future, please post this kind of support questions at Stackoverflow. Thanks!"]}, {"number": 25069, "title": "Floating point exception when trying to build and compile my keras model  with xla support. ", "body": "SYSTEM INFO:\r\n\r\nUBUNTU 16.04\r\nTENSORFLOW 1.12.0 (compiled from source with XLA support)\r\nKERAS 2.2.4\r\nGPU Nvidia Geforce RTX 2080 Ti\r\n\r\nI am trying to build and compile a simple CNN with XLA but I get a floating point exception.\r\n\r\n# Here's the block of code\r\n\r\n`\r\n```\r\nimport keras.backend.tensorflow_backend as bck\r\nconfig = bck.tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = bck.tf.OptimizerOptions.ON_1\r\nbck.set_session(bck.tf.Session(config=config))\r\n\r\n\r\nmodel = Sequential()\r\n\r\nmodel.add(Conv2D(32, (3,3),padding=\"same\",input_shape=input_shape))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Conv2D(32, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.5))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(256))#64\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(34))#num_classes\r\nmodel.add(Activation('softmax'))\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='SGD',metrics=[\"accuracy\"])\r\n\r\n\r\nmodel.summary()\r\nmodel.get_config()\r\n\r\n\r\nfrom keras import callbacks\r\n\r\nfilename='model_train_new.csv'\r\ncsv_log=callbacks.CSVLogger(filename, separator=',', append=False)\r\nfilepath=\"save_xla/Best-weights-my_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5\"\r\n\r\n\r\ncheckpoint = callbacks.ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=False, mode='auto',period=1)\r\n\r\ncallbacks_list = [csv_log,checkpoint]\r\n\r\n\r\n\r\nhist = model.fit(x, y, batch_size=32,epochs=num_epoch, verbose=1,callbacks=callbacks_list)\r\n````\r\n\r\n\r\nI am getting the following Output for this\r\n\r\n2019-01-21 16:33:21.512092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-01-21 16:33:21.512662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 10.73GiB freeMemory: 10.33GiB\r\n2019-01-21 16:33:21.512678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-01-21 16:33:24.538253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-21 16:33:24.538298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-01-21 16:33:24.538312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-01-21 16:33:24.538695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9966 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d_1 (Conv2D)            (None, 128, 128, 32)      320       \r\n_________________________________________________________________\r\nactivation_1 (Activation)    (None, 128, 128, 32)      0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 126, 126, 32)      9248      \r\n_________________________________________________________________\r\nactivation_2 (Activation)    (None, 126, 126, 32)      0         \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 63, 63, 32)        0         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 63, 63, 32)        0         \r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 61, 61, 64)        18496     \r\n_________________________________________________________________\r\nactivation_3 (Activation)    (None, 61, 61, 64)        0         \r\n_________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 30, 30, 64)        0         \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 57600)             0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 256)               14745856  \r\n_________________________________________________________________\r\nactivation_4 (Activation)    (None, 256)               0         \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          (None, 256)               0         \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 34)                8738      \r\n_________________________________________________________________\r\nactivation_5 (Activation)    (None, 34)                0         \r\n=================================================================\r\nTotal params: 14,782,658\r\nTrainable params: 14,782,658\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2019-01-21 16:33:25.223019: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f6f20001130 executing computations on platform CUDA. Devices:\r\n**2019-01-21 16:33:25.223063: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nFloating point exception (core dumped)**\r\n\r\n\r\nwhen I remove this part `import keras.backend.tensorflow_backend as bck\r\nconfig = bck.tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = bck.tf.OptimizerOptions.ON_1\r\nbck.set_session(bck.tf.Session(config=config))`\r\n\r\n\r\neverything works fine.\r\n\r\n\r\n\r\n ", "comments": ["I run in the same issue when using XLA with the GeForce RTX 2080 TI. It is reproducible using example code from tensorflow. More concretely I tried running the follwing script: `tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py`\r\n\r\nHere is the console output when running with the 2080:\r\n```\r\nCUDA_VISIBLE_DEVICES=\"0\" python3 mnist_softmax_xla.py \r\nWARNING:tensorflow:From mnist_softmax_xla.py:35: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n2019-01-30 17:50:03.027620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.575\r\npciBusID: 0000:06:00.0\r\ntotalMemory: 10.73GiB freeMemory: 10.53GiB\r\n2019-01-30 17:50:03.027659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-01-30 17:50:03.297256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-30 17:50:03.297288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-01-30 17:50:03.297296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-01-30 17:50:03.297543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10169 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:06:00.0, compute capability: 7.5)\r\n2019-01-30 17:50:03.513752: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7ff6e40021c0 executing computations on platform CUDA. Devices:\r\n2019-01-30 17:50:03.513808: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nFloating point exception (core dumped)\r\n```\r\n\r\nHere is the console output when running with a 1080:\r\n```\r\nCUDA_VISIBLE_DEVICES=\"1\" python3 mnist_softmax_xla.py \r\nWARNING:tensorflow:From mnist_softmax_xla.py:35: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/zomeck/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n2019-01-30 17:47:56.221149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.81GiB\r\n2019-01-30 17:47:56.221186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-01-30 17:47:56.434690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-30 17:47:56.434722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-01-30 17:47:56.434729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-01-30 17:47:56.434924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7535 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2019-01-30 17:47:56.624224: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x1669cce0 executing computations on platform CUDA. Devices:\r\n2019-01-30 17:47:56.624275: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n2019-01-30 17:47:56.734490: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:402] *** WARNING *** You are using ptxas 10.0.145, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\n2019-01-30 17:47:58.106152: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.10.0 locally\r\n0.91859996\r\n```", "@jlebar Could you please take a look at this issue? Thanks!", "Judging from the warning output, it seems you are running with a old version of tensorflow. There was a bug in the detection logic if you are using an old ptxas which was fixed in https://github.com/tensorflow/tensorflow/commit/83ff640fa5026b8bd3cb9c2ceff9e99e8e03823a\r\nThis is how I know you are using an old version.\r\n\r\nCan you try to run this with tensorflow at head, or a nightly version?", "I also have this exact problem, using the newest release of TensorFlow. Without XLA, the network runs at similar speed on an RTX 2080 Ti as it does with XLA on a GTX 1080 Ti.", "The newest release means TF 1.12.0 like mentioned above? It is very much possible that those bugs have been fixed in the meantime. So could you please try with a version compiled from head?", "Yes, TF 1.12. I've just finished compiling TF 1.13rc2 (on branch r1.13), and with it, I can run 'tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py' without crashing. The incorrect warning because of ptxas is also gone. But I get a new warning:\r\n```\r\n2019-02-22 13:33:11.390128: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/nvptx_backend_lib.cc:134] Unknown compute capability (7, 5) .Defaulting to telling LLVM that we're compiling for sm_30\r\n```\r\nThe result of this is faster than without XLA, and it appears to work :) But it is not as fast as expected, maybe compiling for sm_30 is not optimal. This line is probably missing from 1.13rc2:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/nvptx_backend_lib.cc#L126, I'll try master next.", "Thanks for trying it out, and pointing out the new warning. We should try to get this line cherry-picked into the next release if it is not too late for that.", "On master, the warning is not present, as expected, but its performance on our internal graph of the result is slower than with 1.13rc2. (Still faster than without XLA though :)\r\nI have no idea why yet, or if this is a more general problem or just an artefact of my testing.", "@tfboyd re the cherry-pick of 83ff640.\r\n\r\nThe new warning about sm75 should also be fixed in master.\r\n\r\nmnist is such a small thing that we haven't been bothered by its performance in XLA one way or another.  If you have \"real\" models where XLA is slower, we'd definitely be interested in seeing those.\r\n\r\nI think otherwise this bug is closed?", "This bug isn't originally mine, so I can't say if it is fixed, but at least for me, 1.13rc2 works.\r\n\r\nOn the model I'm working on at the moment, XLA itself in master is still faster than not using it, but it is slower than XLA in 1.13rc2 was. But the model is private and I'm not allowed to disclose any details, so I'll try to debug why this is the case on my own, and open a separate ticket should I find out anything.", "So the only solution for this is to install TF 1.13 ?", "Yes.", "@akuegel \r\nAlright then I am closing this issue.\r\n\r\nAlso is there any wheel file availabe for TF 1.13 with XLA. Compiling 1.12 from source with XLA was a pain and I don't want to go through the same steps again. If anybody has a wheel file plz share!!", "I thought that TF version 1.12 already had XLA support linked in by default?\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v1.12.0\r\n(under major features and improvements)\r\nThe same should be true for every TF release after that.", "@akuegel \r\nOh okay I didn't knew that. actually I was working with an older version on a Jetson tx2 and then cross compiled from source for xla support. and then I wanted to use a newer version on my PC so I compiled 1.12 from source without even knowing that it had already xla support by default."]}, {"number": 25068, "title": "[XLA] Support unknown dimension.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source r1.13 w/ XLA\r\n- **TensorFlow version (use command below)**: r1.13\r\n- **Python version**: 3.6.7\r\n- **Bazel version (if compiling from source)**: 0.18.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc 7.3.0\r\n- **CUDA/cuDNN version**: 10.0\r\n- **GPU model and memory**: GTX 1080Ti\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have a graph that selects certain \"records\" from some Tensors, based on an optimization strategy for relevance of the records in the current context. Details don't really matter here. The essence is that the `tf.boolean_mask()` returns a Tensor with a shape containing one unknown dimension. The result is that XLA is compiling the graph for every resulting length of that unknown dimension again and again. I can see that this is the issue, as multiple invocations of this graph lead to timings of being either very fast (3 ms) or very slow (350ms). The longer you wait, the more subgraphs XLA has compiled for a specific length and the more I see the faster invocations (where it does not need to compile anything).\r\n\r\nIs there anything available or planned to tell XLA to not fix the unknown length for every invocation, such that the compilation happens only once, for a general length in that specific dimension?\r\nAre there other tricks I can try to circumvent this issue?\r\n\r\n### Source code / logs\r\n```python\r\ncenter = tf.placeholder(...)\r\ndiff = tf.subtract(data, center)\r\nmask = tf.less(tf.abs(diff), 1.0) # select near data first\r\nnear_data = tf.boolean_mask(diff, mask)\r\n# Now, perform actual computations on near_data\r\n# because we know we can skip doing the computations on the other data, as the result\r\n# will be 0, due to the large distance.\r\n# However, now, near_data has a shape like [None, 5, 5] for example\r\n# and XLA recompiles the graph for every length of the first dimension\r\n```\r\n", "comments": ["@jlebar ", "Hi, thank you for the bug report.\r\n\r\n> Is there anything available or planned to tell XLA to not fix the unknown length for every invocation, such that the compilation happens only once, for a general length in that specific dimension?\r\n\r\nNot today.  We are working on changing this, but at the moment the assumption that all shapes are known at compile time is baked very deeply into XLA.\r\n\r\n> Are there other tricks I can try to circumvent this issue?\r\n\r\nCould you clarify for me: Is the large number of compilations hurting overall?  Like, if you trained to convergence with XLA, would this be slower because we're compiling so many things?\r\n\r\nIf not, then I'm not sure there's a bug here.\r\n\r\nIf so I'd say this is a bug in the heuristic we use to decide when to compile with XLA (\"lazy compilation\").  Perhaps we should tweak the heuristic, which AIUI is very simple right now.  I'm thinking maybe we could e.g. do some sort of exponential backoff, where we're eager to compile the first few clusters we see and then less eager to do new ones.\r\n\r\n@sanjoy @thomasjoerg wdyt?", "> If so I'd say this is a bug in the heuristic we use to decide when to compile with XLA (\"lazy compilation\").\r\n\r\nAnother option is to avoid clustering ops like `tf.where` (which is what `tf.boolean_mask` boils down to IIUC) to avoid excessive recompilation.  This may be easier than it sounds since it does not need to be 100% correct -- we only need it for profitability and not correctness.", "> Perhaps we should tweak the heuristic\r\n\r\nBtw, an easy way to do this is to change (increase in this case) the `kDefaultCompilationThreshold` field: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/xla_compilation_cache.h#L190\r\n\r\nI can change this to be a flag settable from the `TF_XLA_FLAGS` environment variable if that will help.", "Is it as simple as not clustering `tf.where`?  ISTM we'd have to sort of poison everything whose shape \"inherits from\" that op.\r\n\r\nI agree it's easier because it's not correctness-critical, but if autoclustering makes something really slow, in a sense that *is* incorrect.\r\n\r\n> Btw, an easy way to do this is to change (increase in this case) the kDefaultCompilationThreshold field:\r\n\r\nWe can do that, I'm just not sure a fixed threshold is the right approach, because if you run the computation long enough, eventually you're going to start hitting whatever fixed threshold we set.  Which is why I reached for exponential backoff: Eagerly trigger the first few compilations, then become less eager for future ones?\r\n\r\nIt may make sense to do both of the above things.\r\n\r\nWDYT?", "> Is it as simple as not clustering tf.where? ISTM we'd have to sort of poison everything whose shape \"inherits from\" that op.\r\n\r\nYes, `tf.where` and everything that depends on it, up to a point (and we can imperfect heuristics to infer where this point is; e.g. we may say that *waves hands* we know that a GEMM of a tf.where usually contracts away the variably sized dimension so anything after the GEMM is fine; in some cases we may have even stronger guarantees, like a slice of a `tf.where`).\r\n\r\n> Eagerly trigger the first few compilations, then become less eager for future ones?\r\n\r\nI forgot to mention something pertinent:  we have a concept of a \"megamorphic\" cluster, which is a cluster with highly variable shapes that we refuse to compile.  If the extra compilation time is hurting performance _overall_ (i.e. slowing down time to convergence) then maybe we should be tweaking the `IsMegamorphic` heuristic.\r\n\r\nBut megamorphism is a binary property (a cluster is either megamorphic or it isn't), so I think exponential backoff will be an improvement.", "@jlebar \r\n> > Are there other tricks I can try to circumvent this issue?\r\n> \r\n> Could you clarify for me: Is the large number of compilations hurting overall? Like, if you trained to convergence with XLA, would this be slower because we're compiling so many things?\r\n\r\nYes, it hurts performance tremendously for my application. The number of resulting entries after the boolean_mask can be easily anywhere between 20 and 4000. This can take up to 3980*350 milliseconds of compiling, which is 23 minutes of just compiling the same code over and over again. I obviously disabled XLA for now, but if I remember correctly, having this much compiled code in memory began causing memory and performance problems.\r\n\r\nHowever, right now, I've been thinking about this for a couple days, and I believe I might get away with making sure I have, for example, multiples of 50 entries after the `boolean_mask`, by padding the Tensor to the next multiple of 50. This would reduce the number of compiled graphs by 50, if I make sure that the `boolean_mask` part is not compiled by using the `jit_scope` technique, and specifically not including that part in the `jit_scope`.\r\n\r\nHowever, the padding of the Tensors is a theoretically unnecessary and hacky, annoying step, causing unnecessary overhead (both in the padding-process and the now extra entries being processed).\r\n\r\n@sanjoy \r\n> Yes, `tf.where` and everything that depends on it,\r\n\r\nI'm not exactly following 100% with the technical details of XLA internals, but please note, that in my scenario, most of the calculations happen *after* the `boolean_mask` (or equivalently `tf.where`, as I learned here). So not clustering the part after the `tf.where`, would cause the critical part of my graph to be not fused, optimised, and compiled (if I understood this XLA principle correctly).\r\n", "> So not clustering the part after the `tf.where`, would cause the critical part of my graph to be not fused, optimised, and compiled (if I understood this XLA principle correctly).\r\n\r\nThat's true, but as a first step we want enabling XLA to do no harm.  Once we're at a point where enabling XLA is never a *regression* we can focus on increasing the set of situations where XLA is actually a net positive.  Given this framing:\r\n\r\n> The number of resulting entries after the boolean_mask can be easily anywhere between 20 and 4000. This can take up to 3980*350 milliseconds of compiling, which is 23 minutes of just compiling the same code over and over again.\r\n\r\nIt looks like our megamorphic cluster detection is not working as intended.  Is it possible for you to share the full model?", "> Is it possible for you to share the full model?\r\n\r\nI'm afraid not. The complete model is quite large, and the setup required to run it is quite complex. Plus, I don't know if my employer would like me to throw this on the internet just yet. But the essentials are in my short piece of code in the question. I could try to set up an [MCVE](https://stackoverflow.com/help/mcve).\r\n\r\nCan anyone shed some light on the feasibility or the plans for implementing XLA compilation for unknown tensor sizes?\r\n\r\n", "> Can anyone shed some light on the feasibility or the plans for implementing XLA compilation for unknown tensor sizes?\r\n\r\nI am afraid I cannot say much more than what I wrote in https://github.com/tensorflow/tensorflow/issues/25068#issuecomment-457653556.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25067, "title": "Latest commit in deprecation.py breaks R TensorFlow client ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 28\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): nightly (r1.13)\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\nHi,\r\n\r\nthe commit \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/b97727bc3c7a9216670f361b639a60ed516917e0#diff-fa5fb3b8d9f512ad269f7eb67903ec2b\r\n\r\nbreaks the R TensorFlow client which uses embedded Python from R (see https://github.com/rstudio/tfdatasets/issues/17)\r\n\r\nSpecifically, the line\r\n\r\n```\r\nframe = stack[-4 if outer else -3]\r\n```\r\n\r\ntriggers the error\r\n\r\n```\r\nError in py_call_impl(callable, dots$args, dots$keywords) : \r\n  IndexError: list index out of range\r\n\r\nDetailed traceback: \r\n  File \"/home/key/anaconda3/envs/tf-master-1215/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 314, in new_func\r\n    _call_location(), decorator_utils.get_qualified_name(func),\r\n  File \"/home/key/anaconda3/envs/tf-master-1215/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 103, in _call_location\r\n    frame = stack[-4 if outer else -3]\r\n```\r\n\r\non our side. For us, in one specific example, instead of a list with at least 4 items, `stack` contains a list of 2 tuples, each of which are of length 6.\r\n\r\nUnfortunately, this will error and stop execution every time a new deprecation warning is issued.\r\n\r\nCurrent workaround on our side is https://github.com/rstudio/tensorflow/pull/287, however this comes at the cost of totally disabling deprecation warnings.\r\n\r\nIs there anything you could do to make this change compatible?\r\n\r\nThanks,\r\nSigrid\r\n\r\n", "comments": ["@jtkeeling could you look at this? I think it's simply a matter of adding bounds checking before accessing the higher levels of the stack. ", "I'll take a look now", "I've sent a fix to Martin to review, so hopefully it should be submitted soon.", "Great, thanks so much for reacting this fast! Is there any chance this could still make it into 1.13?", "@aselle would be the decider on that. ", "Great, thanks a lot for the quick fix, I just tested with the nightly and it works!\r\n@aselle it would be great if you could answer reg. 1.13, we do have a temporary workaround but in case that's not needed we'd remove it :-)\r\nThanks!"]}, {"number": 25066, "title": "AdamWOptimizer doesn't work with MirroredStrategy and tf.get_variable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 9.0.176 / 7.4.1\r\n- GPU model and memory: TITAN X, 12189MiB\r\n\r\n**Describe the current behavior**\r\n`tf.contrib.opt.AdamWOptimizer` does not do weight decay when using `MirroredStrategy` along with `tf.get_variable`\r\n**Describe the expected behavior**\r\n`tf.contrib.opt.AdamWOptimizer` should always do weight decay, regardless of how variables are created.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nflags = tf.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_bool('use_get_variable', False, 'Whether to use tf.get_variable to create variables')\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    if FLAGS.use_get_variable:\r\n        x = tf.get_variable('x', initializer=[1.])\r\n    else:\r\n        x = tf.Variable([1.], name='x')\r\n    loss = tf.reduce_sum(x * x)  # the original loss is 14.\r\n\r\n    # here we let learning rate be 0 so that the updates come only from weight decay.\r\n    # after 1 step of weight decay, the final loss should be 0.81.\r\n    optimizer = tf.contrib.opt.AdamWOptimizer(learning_rate=0.0, weight_decay=0.1)\r\n\r\n    tvars = tf.trainable_variables()\r\n    tf.logging.info('All variables: {}'.format(tvars))\r\n    grads = tf.gradients(loss, tvars)\r\n    global_step = tf.train.get_or_create_global_step()\r\n    train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step, decay_var_list=tvars)\r\n    output_spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n    return output_spec\r\n\r\n\r\ndef input_fn(params):\r\n    return tf.data.Dataset.from_tensors([0.]).repeat()\r\n\r\n\r\ndef main(_):\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n\r\n    distribution = tf.contrib.distribute.MirroredStrategy()\r\n    run_config = tf.contrib.tpu.RunConfig(train_distribute=distribution)\r\n    estimator = tf.contrib.tpu.TPUEstimator(use_tpu=False, model_fn=model_fn, config=run_config)\r\n    # here max_steps=2 so that the final loss printed is after 1 step update.\r\n    estimator.train(input_fn=input_fn, max_steps=2)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n```\r\n\r\n**Other info / logs**\r\nI ran the above script with 0 or 1 GPU, both producing:\r\n`python test.py --use_get_variable=False`\r\n![image](https://user-images.githubusercontent.com/7380587/51468725-a1efbd80-1daa-11e9-9328-367769a8c19f.png)\r\n\r\n`python test.py --use_get_variable=True`\r\n![image](https://user-images.githubusercontent.com/7380587/51468786-be8bf580-1daa-11e9-8958-d30f1c3dcf19.png)\r\n", "comments": ["@ytdu , this is an excellent and targeted bug you have detected:\r\n**_tf.contrib.opt.AdamWOptimizer_ does not do weight decay when using _MirroredStrategy_ along with _tf.get_variable_**\r\n@karmel and @josh11b , please advise who can help with this bug. Thanks.", "I don't know about this specific issue, but in general we have not supported contrib libraries with tf.distribute.Strategy since they are going away with 2.0.", "In general I don't know what happens when you combine TPUEstimator with MirroredStrategy. I'd expect you to use one or the other but not both.", "Thanks for the reply. In fact I've gone through some of the related source code, so I know something about why this happens.\r\n\r\n`tf.contrib.opt.AdamWOptimizer` (more specifically, `tf.contrib.opt.DecoupledWeightDecayExtension`) maintains a `decay_var_list`, the elements in which are references of variables that are going to be decayed. `MirroredStrategy` somehow wraps all the variables created by `tf.get_variable` into `MirroredVariable`s, which breaks the connection between `decay_var_list` and the variables. Therefore `AdamWOptimizer` does not know these `MirroredVariable`s need decaying.\r\n\r\nIt would be great that this kind of `MirroredVariable` wrapper related bugs will be eliminated in 2.0", "I don't think this bug will automatically get fixed in 2.0. You're right that the decay_var_list is being stored as the list of mirrored variables, but later on when applying the op, it tries to apply it on the underlying op and that fails. So there needs to be some change in the decay optimizers to handle this. \r\n\r\nLike Josh said, we are not prioritizing bugs with contrib optimizers. But if you have a fix in mind, feel free to send a PR. \r\n\r\n@facaiy - you had expressed interest in the opt module, do you know if there is any progress on this optimizer/is that part of the plan? (https://github.com/tensorflow/community/pull/18/files#r218985470)", "tensorflow/addons is planning to migrate AdamWOptimizer: https://github.com/tensorflow/addons/issues/24 . If anyone is interested, welcome to make a contribution. cc @seanpmorgan ", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25066\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25066\">No</a>\n"]}, {"number": 25065, "title": "Removed twice declaration AttrTypeByName", "body": "The interface AttrTypeByName was declared twice in attr_builder.h.", "comments": ["Nagging Reviewer @mhong: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25064, "title": "Example missing for using the field `tensor_content` with Golang grpc client", "body": "**Describe the documentation issue**\r\n\r\nWith my python client i can call my model server properly with the code like this:\r\n```\r\nrequest.inputs[\"input\"].CopyFrom(\r\n    tf.contrib.util.make_tensor_proto([165, 60, 35],dtype=dtypes.int32)\r\n)\r\nresponse = stub.Predict(request,30.0)\r\n```\r\n\r\nAnd in python terminal i checked the output of the `make_tensor_proto`, i got something like this:\r\n```\r\n>>> tf.contrib.util.make_tensor_proto([165, 60, 35],dtype=dtypes.int32)\r\ndtype: DT_INT32\r\ntensor_shape {\r\n  dim {\r\n    size: 3\r\n  }\r\n}\r\ntensor_content: \"\\245\\000\\000\\000<\\000\\000\\000#\\000\\000\\000\"\r\n```\r\n**But I cannot make this work with Golang, tried to search with different terms cannot find one example how to fill the `tensor_content` field.**\r\n\r\nAccording to the documentations i got:\r\n```\r\n// Serialized raw tensor content from either Tensor::AsProtoTensorContent or\r\n// memcpy in tensorflow::grpc::EncodeTensorToByteBuffer. This representation\r\n// can be used for all tensor types. The purpose of this representation is to\r\n// reduce serialization overhead during RPC call by avoiding serialization of\r\n// many repeated small items.\r\nTensorContent []byte `protobuf:\"bytes,4,opt,name=tensor_content,json=tensorContent,proto3\" json:\"tensor_content,omitempty\"`\r\n```\r\n(do i need to implement the above mentioned `Tensor::AsProtoTensorContent` and `tensorflow::grpc::EncodeTensorToByteBuffer` by myself?)\r\n\r\nBut tried different ways like:\r\n```\r\npr          tf.PredictRequest\r\n// Construct the `pr` structure properly.\r\n// ...\r\npr.Inputs[\"input\"].TensorContent = []byte{165, 60, 35}\r\n```\r\netc cannot make it work, always resulted in error message:\r\n```\r\npanic: rpc error: code = Internal desc = transport: received the unexpected content-type \"text/plain; charset=utf-8\"\r\n```\r\n\r\n**Also if i want to send such a request in JSON, what should i put in the `tensor_content` field:**\r\n```\r\n{\r\n  \"model_spec\": {\r\n    \"name\": \"gender\",\r\n    \"version\": \"1\"\r\n  },\r\n  \"inputs\": {\r\n    \"input\": {\r\n      \"dtype\": 3,\r\n      \"tensor_shape\": {\r\n        \"dim\": [\r\n          {\r\n            \"size\": 3\r\n          }\r\n        ]\r\n      },\r\n      \"tensor_content\": \"WzEsMl0=\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nPls help to provide a proper example in the corresponding docs, thanks.", "comments": ["@lnshi , TensorFlow r1.12.0 does not seem to support your Golang interfaces for your planned _Tensor::AsProtoTensorContent_ and _tensorflow::grpc::EncodeTensorToByteBuffer_\r\nYou may have to implement them yourself and invoke appropriate TensorFlow APIs.", "It's somewhat nontrivial for dt.string; but for other data types it's relatively straightforward to implement a hacky version yourself.  Sadly I don't know golang so I can't translate into the language's syntax but basically:\r\n\r\nif you have a vector of contiguous memory, the tensor_content is literally just that memory copied through.  So for example if you have a vector of 4 integers, you'd just copy the underlying memory buffer into tensor_content.  Assume a row-major format.\r\n\r\nSame with all contiguous memory representations: floats, bools, ints.\r\n\r\nBTW for bools, i dont know if golang does this optimization but in c++, std::vector<bool> uses a bitset rep so it's not a direct copy in that one case.  you have to ensure bools are encoded as integers.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25063, "title": "Problem in installing tensorflow on window 8.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : **window 8.1, 64bit** ,,,intel Core i3-5015U Processor,,,,2.1 GHz Processor,,,,,,4 GB DDR3\r\n- TensorFlow installed from: **conda** \r\n- Python version: 3.5\r\n- Installed using virtualenv: conda\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: NVIDIA geforce 820M\r\n\r\n\r\n\r\n**Describe the problem** \r\n\r\n(py35) C:\\Users\\AAMIR SIDDIQUI>conda install tensorflow\r\nSolving environment: failed\r\n\r\nPackagesNotFoundError: The following packages are not available from current channels:\r\n\r\n  - tensorflow\r\n\r\nCurrent channels:\r\n\r\n  - https://conda.anaconda.org/conda-forge/win-32\r\n  - https://conda.anaconda.org/conda-forge/noarch\r\n  - https://repo.anaconda.com/pkgs/main/win-32\r\n  - https://repo.anaconda.com/pkgs/main/noarch\r\n  - https://repo.anaconda.com/pkgs/free/win-32\r\n  - https://repo.anaconda.com/pkgs/free/noarch\r\n  - https://repo.anaconda.com/pkgs/r/win-32\r\n  - https://repo.anaconda.com/pkgs/r/noarch\r\n  - https://repo.anaconda.com/pkgs/pro/win-32\r\n  - https://repo.anaconda.com/pkgs/pro/noarch\r\n  - https://repo.anaconda.com/pkgs/msys2/win-32\r\n  - https://repo.anaconda.com/pkgs/msys2/noarch\r\n\r\nTo search for alternate channels that may provide the conda package you're\r\nlooking for, navigate to\r\n\r\n    https://anaconda.org\r\n\r\nand use the search bar at the top of the page.\r\n\r\n**i tried this also but nothing worked**\r\n\r\n(py35) C:\\Users\\AAMIR SIDDIQUI>conda search tensorflow --channel conda-forge\r\nLoading channels: done\r\n\r\nPackagesNotFoundError: The following packages are not available from current cha\r\nnnels:\r\n\r\n  - tensorflow\r\n\r\nCurrent channels:\r\n", "comments": ["This seems to be an issue with anaconda configuration.Suggest another place to ask.", "@llAamirll , Please try the following steps to do a conda install on Windows for TensorFlow 1.10.0 ; There is no GPU support.\r\nCreate a conda environment called tensorflow:\r\n_$ conda create -n tensorflow python=3.5_\r\nA community maintained conda package is available. Use the conda install instructions shown here:\r\n_https://anaconda.org/conda-forge/tensorflow_\r\n", "I think it was resolved. I am closing the issue. Please open new ticket if you see similar issue again. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25062, "title": "AttributeError: module 'keras_applications' has no attribute 'set_keras_submodules'", "body": "### System information\r\n- **Linux Ubuntu 16.04**\r\n- **Firefly RK3399**:\r\n- **TensorFlow version: 1.12.0**:\r\n- **Python version: 3.5.2**:\r\n\r\n### Describe the problem\r\nI'm trying the objection_detection with opencv and tensorflow. The source code has been given at the last. When I python3 11.py, it comes error. \r\nThough it says \"AttributeError: module 'keras_applications' has no attribute 'set_keras_submodules'\", I find    'set_keras_submodules'exits in init.py of keras_applications. So it confuses me quite a long time. I try to update .pyc in pycache but it didn't work.\r\n```\r\nroot@firefly:/usr/local/lib/python3.5/dist-packages/keras_applications# ls -l\r\ntotal 200\r\n-rw-r--r-- 1 root staff  3116 Jan 18 07:29 __init__.py\r\ndrwxr-sr-x 2 root staff  4096 Jan 21 02:53 __pycache__\r\n-rw-r--r-- 1 root staff 13450 Jan 18 07:29 densenet.py\r\n-rw-r--r-- 1 root staff 12632 Jan 18 07:29 imagenet_utils.py\r\n-rw-r--r-- 1 root staff 14725 Jan 18 07:29 inception_resnet_v2.py\r\n-rw-r--r-- 1 root staff 14586 Jan 18 07:29 inception_v3.py\r\n-rw-r--r-- 1 root staff 20306 Jan 18 07:29 mobilenet.py\r\n-rw-r--r-- 1 root staff 21720 Jan 18 07:29 mobilenet_v2.py\r\n-rw-r--r-- 1 root staff 29931 Jan 18 07:29 nasnet.py\r\n-rw-r--r-- 1 root staff 11860 Jan 18 07:29 resnet50.py\r\n-rw-r--r-- 1 root staff  8504 Jan 18 07:29 vgg16.py\r\n-rw-r--r-- 1 root staff  8985 Jan 18 07:29 vgg19.py\r\n-rw-r--r-- 1 root staff 14008 Jan 18 07:29 xception.py\r\nroot@firefly:/usr/local/lib/python3.5/dist-packages/keras_applications# sudo vim __init__.py\r\n```\r\n'set_keras_submodules' exits\r\n```\r\ndef set_keras_submodules(backend=None,\r\n                         layers=None,\r\n                         models=None,\r\n                         utils=None,\r\n                         engine=None):\r\n    # Deprecated, will be removed in the future.\r\n    global _KERAS_BACKEND\r\n    global _KERAS_LAYERS\r\n    global _KERAS_MODELS\r\n    global _KERAS_UTILS\r\n    _KERAS_BACKEND = backend\r\n    _KERAS_LAYERS = layers\r\n    _KERAS_MODELS = models\r\n    _KERAS_UTILS = utils\r\n```\r\n```\r\nroot@firefly:/home/firefly/models/research/object_detection/models# python3 11.py\r\nTraceback (most recent call last):\r\n  File \"11.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 88, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import applications\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/applications/__init__.py\", line 37, in <module>\r\n    keras_applications.set_keras_submodules)[0]:\r\nAttributeError: module 'keras_applications' has no attribute 'set_keras_submodules'\r\n```\r\n### Source code / logs\r\n11.py\r\n```\r\nimport numpy as np\r\nimport os\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\nimport cv2\r\nimport time\r\nfrom collections import defaultdict\r\n\r\nsys.path.append(\"../..\")\r\n\r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import visualization_utils as vis_util\r\n\r\n\r\nMODEL_NAME = 'ssd_mobilenet_v1_coco_2018_01_28'\r\n\r\nPATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\r\n\r\nPATH_TO_LABELS = os.path.join('/home/firefly/models/research/object_detection/data', 'mscoco_label_map.pbtxt')\r\n\r\nmodel_path = \"/home/firefly/models/research/object_detection/models/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt\"\r\n\r\nstart = time.clock()\r\nNUM_CLASSES = 90\r\n\r\nend= time.clock()\r\nprint('load the model' ,(end -start))\r\n\r\n## Load a (frozen) Tensorflow model into memory.\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\nwith tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n    serialized_graph = fid.read()\r\n    od_graph_def.ParseFromString(serialized_graph)\r\n    tf.import_graph_def(od_graph_def, name='')\r\n\r\n## Loading label map\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\ncap = cv2.VideoCapture(1)\r\ncap.set(3,640)\r\ncap.set(4,480)\r\nwith detection_graph.as_default():\r\n    with tf.Session(graph=detection_graph) as sess:\r\n        writer = tf.summary.FileWriter(\"logs/\", sess.graph)\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        loader = tf.train.import_meta_graph(model_path + '.meta')\r\n        loader.restore(sess, model_path)\r\n        while(1):\r\n            start = time.clock()\r\n            ret, frame = cap.read()\r\n            if cv2.waitKey(1) & 0xFF == ord('q'):\r\n                break\r\n            image_np =frame\r\n\r\n            image_np_expanded = np.expand_dims(image_np, axis=0)\r\n            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n            scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\n            classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\n            (boxes, scores, classes, num_detections) = sess.run(\r\n                [boxes, scores, classes, num_detections],\r\n                feed_dict={image_tensor: image_np_expanded})\r\n\r\n            vis_util.visualize_boxes_and_labels_on_image_array(\r\n                image_np, np.squeeze(boxes),\r\n                np.squeeze(classes).astype(np.int32),\r\n                np.squeeze(scores),\r\n                category_index,\r\n                use_normalized_coordinates=True,\r\n                line_thickness=6)\r\n            end = time.clock()\r\n\r\n            print ('One frame detect take time:' ,end - start)\r\n\r\n            cv2.imshow(\"capture\", image_np)\r\n            print('after cv2 show')\r\n            cv2.waitKey(1)\r\ncap.release()\r\ncv2.destroyAllWindows()\r\n```\r\n", "comments": ["Same problem here, did you solve it?", "@sistemas3-mv I have solved it. You can try to replace a new location to run the code. \r\nAt first, I run the code at /home/firefly/models/research/object_detection/models. When I run the code, it went errors importing tensorflow. So I run it at /home or other place, and the errors was gone. \r\nThough I have solved the problem, but I didn't realize what the real problem is.\r\nBy the way, remember to move your model to the location you run the code.", "Same problem,replacing a new location can not solve it", "Upgrade keras to 2.2.4 solved my problem (py3.6). ", "Same problem.At this env, keras==2.2.2, Keras-Applications==1.0.8  and keras_preprocessing==1.1.0,please upgrade  Keras-Applications==1.0.4 and keras_preprocessing==1.0.2 can solved the problem", "same here, \r\ndowngrade keras_applications to 1.0.7 solve the issue for me"]}, {"number": 25061, "title": "Provide a better error output with incorrect input for tf.keras.layers.Dot", "body": "This fix tries to address the issue raised in #25049 where the error output\r\nwas not very clear when incorrect input was passed for tf.keras.layers.Dot.\r\nWhen tuple instead of list was passed, the following erorr shwon up:\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 187, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string or a number, not 'TensorShapeV1'\r\n```\r\n\r\nThis fix catch the TypeError so that correct error message could be returned:\r\n```\r\nValueError: A `Dot` layer should be called on a list of 2 inputs.\r\n```\r\n\r\nThis fix fixes #25049.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for sending a PR! I'm just some guy, but I don't think that think is the right fix. If some other TypeError occurred, this would lead to a different confusing error message. I think that either of the fixes outlined in the bug report would be preferable.\r\n\r\nMy other 2c: (1) This could use a test, and (2) this applies to other merge layers as well.", "@josharian If you trace through the call and some other TypeError occurs, then the failure here means `convert_shapes` type fails, so the `input_shape` is not as was expected. The error message is indicating that the input is not a `list` so it is fine.", "@yongtang can you please resolve conflicts ?", "@rthadur updated, thanks.", "@josharian i just replied on the original issue, can you let us know if you are still seeing the issue?\r\n@yongtang  as @josharian mentioned i think the right fix is to support the use case for all the Merge layers and can you also add a unit test for this?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 25060, "title": "Issue/24374: tf.einsum function to compute the trace.", "body": "Proposition for solving issue connected with tf.einsum function that is not computing the trace of a matrix properly.\r\nAdded if else procedure for proper computing the trace of random matrix.\r\nIf coefficient is doubled function would use previously defined trace function.\r\nOtherwise, reduce_sum function would be used.", "comments": []}, {"number": 25059, "title": "Issue/24374: tf.einsum function to compute the trace.", "body": "Proposition for solving issue connected with tf.einsum function that is not computing the trace of a tensor properly.\r\nAdded if else procedure for proper computing the trace of random tensor.\r\nIf coefficient is doubled function would use previously defined trace function.\r\nOtherwise, reduce_sum function would be used.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 25058, "title": "Failed to build for iOS project using Xcode 10.1: Undefined symbols for architecture x86_64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave 10.14.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.7.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.20.0-homebrew\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\nTarget: x86_64-apple-darwin18.2.0\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n- Xcode: Version 10.1 (10B61)\r\n\r\n**Describe the problem**\r\n\r\nError during compiling iOS app with TF static library. The steps are described below.\r\nAfter compiling TF library from source, importing it in a blank Xcode project I get the following error:\r\n\r\n`ld: symbol(s) not found for architecture x86_64`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Cloned the TF repo with: `git clone https://github.com/tensorflow/tensorflow -b r1.12`\r\n2. Configuring:\r\n```\r\nYou have bazel 0.20.0-homebrew installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /Library/Python/2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Library/Python/2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n\r\nNo Amazon AWS Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: N\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: N\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished \r\n```\r\n3. Updated makefile for iOS as mentioned in https://github.com/tensorflow/tensorflow/issues/18356 and modified `tensorflow/tensorflow/core/util/work_sharder.cc` file as mentioned by https://github.com/tensorflow/tensorflow/issues/18356#issuecomment-422373566\r\n\r\n4. After compiling (successfully) I obtain:\r\n\r\n- `tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a`\r\n- `tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a`\r\n- `tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf-lite.a`\r\n\r\n5. I created a new blank Xcode project and following https://stackoverflow.com/questions/37769904/how-to-compile-ios-example-in-tensorflow?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa I linked all the libraries\r\n\r\n6. The compile phase ends with errors show below\r\n\r\n**Any other info / logs**\r\n\r\n`Undefined symbols for architecture x86_64:\r\n  \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::at(unsigned long) const\", referenced from:\r\n      google::protobuf::io::Tokenizer::IsIdentifier(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libprotobuf.a(tokenizer.o)\r\n  \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__grow_by(unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long)\", referenced from:\r\n      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >& std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__append_forward_unsafe<std::__1::__wrap_iter<char const*> >(std::__1::__wrap_iter<char const*>, std::__1::__wrap_iter<char const*>) in libprotobuf-lite.a(strutil.o)\r\n      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >& std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::__append_forward_unsafe<char const*>(char const*, char const*) in libprotobuf.a(strtod.o)\r\n  \"std::__1::condition_variable::__do_timed_wait(std::__1::unique_lock<std::__1::mutex>&, std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l> > >)\", referenced from:\r\n      std::__1::cv_status std::__1::condition_variable::wait_until<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > >(std::__1::unique_lock<std::__1::mutex>&, std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > > const&) in nsync.a(nsync_semaphore_mutex.o)\r\n  \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::replace(unsigned long, unsigned long, char const*)\", referenced from:\r\n      google::protobuf::EnumDescriptor::DebugString(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, google::protobuf::DebugStringOptions const&) const in libprotobuf.a(descriptor.o)\r\n      google::protobuf::Descriptor::DebugString(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, google::protobuf::DebugStringOptions const&, bool) const in libprotobuf.a(descriptor.o)\r\n  \"std::__1::this_thread::sleep_for(std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l> > const&)\", referenced from:\r\n      nsync::nsync_time_sleep(timespec) in nsync.a(time_rep_timespec.o)\r\n  \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::put(char)\", referenced from:\r\n      tensorflow::StatsCalculator::GetShortSummary() const in libtensorflow-core.a(stats_calculator.o)\r\n      tensorflow::StatsCalculator::HeaderString(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const in libtensorflow-core.a(stats_calculator.o)\r\n      tensorflow::StatsCalculator::GetStatsByNodeType() const in libtensorflow-core.a(stats_calculator.o)\r\n      tensorflow::StatsCalculator::GetStatsByMetric(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::StatsCalculator::SortingMetric, int) const in libtensorflow-core.a(stats_calculator.o)\r\n      tensorflow::StatsCalculator::GetOutputString() const in libtensorflow-core.a(stats_calculator.o)\r\n      tensorflow::PrintOp::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(logging_ops.o)\r\n      tensorflow::PrintV2Op::Compute(tensorflow::OpKernelContext*) in libtensorflow-core.a(logging_ops.o)`\r\n\r\n      ...\r\n\r\n`  \"std::__1::basic_ostream<char, std::__1::char_traits<char> >::operator<<(long)\", referenced from:\r\n      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, long>(int const&, long const&, char const*) in libtensorflow-core.a(ctc_loss_calculator.o)\r\n      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, long>(int const&, long const&, char const*) in libtensorflow-core.a(ctc_decoder_ops.o)\r\n      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, long>(int const&, long const&, char const*) in libtensorflow-core.a(pad_op.o)\r\n\"operator new(unsigned long)\", referenced from:\r\n protobuf_tensorflow_2fcontrib_2fboosted_5ftrees_2fproto_2flearner_2eproto::protobuf_AssignDescriptors() in libtensorflow-core.a(learner.pb.o)    tensorflow::boosted_trees::learner::LearnerConfig::LearnerConfig(tensorflow::boosted_trees::learner::LearnerConfig const&) in libtensorflow-core.a(learner.pb.o)\r\n      tensorflow::boosted_trees::learner::TreeRegularizationConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::TreeRegularizationConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)\r\n      tensorflow::boosted_trees::learner::TreeConstraintsConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::TreeConstraintsConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)\r\n      tensorflow::boosted_trees::learner::LearningRateConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::LearningRateConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)\r\n      tensorflow::boosted_trees::learner::LearningRateFixedConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::LearningRateFixedConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)\r\n      tensorflow::boosted_trees::learner::LearningRateLineSearchConfig* google::protobuf::Arena::CreateMessage<tensorflow::boosted_trees::learner::LearningRateLineSearchConfig>(google::protobuf::Arena*) in libtensorflow-core.a(learner.pb.o)\r\n      ...\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)`\r\n", "comments": ["@andreapavan , can you please step back to bazel 0.15.0 and use Python 2.7 or 3.6 , try your build process for the project again and let us know. Thanks.", "@msymp Thanks for the reply. Python was 2.7.10. I updated my post. I installed Bazel 1.15.0 from binary installer, but I always the same problem. I think is a problem of linking (or missing linking) of some library. How can I check all the dependencies?\r\nI followed this guide for importing compiled files:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios", "@andreapavan , This is apparently a systems issue specifically to the iOS build environment. This does not seem to be a TensorFlow repo issue. Can you check into some iOS community boards for answers? Thanks.", "@andreapavan Could you try pip install tensorflow==1.10.1 or any other new version (update last few digits).", "You most probably have to link to C++ runtime to the project. The errors are about not being able to link c++ std lib classes. ", "There is still no idea to fix this issue, isnt it?", "I got into this trouble some days ago.\r\nPlease try my solution to work around this issue.\r\nhttps://github.com/violettomsk/xcodesdks", "this might work (https://github.com/facebook/react-native/issues/23390#issuecomment-463157141)", "Thank you @violettomsk!\r\n\r\nAdding  `-lstdc++` solved a similar issue I saw when compiling C++ program depend on tensorflow 2.1 on macbook.\r\n\r\nWithout `-lstdc++`, I saw the following error\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::compare(unsigned long, unsigned long, char const*, unsigned long) const\", referenced from:\r\n      bool std::__1::operator==<char, std::__1::char_traits<char>, std::__1::allocator<char> >(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, char const*) in example_trainer-cedf75.o\r\n  \"std::__1::__vector_base_common<true>::__throw_length_error() const\", referenced from:\r\n      std::__1::vector<std::__1::pair<tensorflow::Node*, int>, std::__1::allocator<std::__1::pair<tensorflow::Node*, int> > >::__vallocate(unsigned long) in example_trainer-cedf75.o\r\n```\r\n\r\nAfter adding `-lstdc++`, and edit {LD_LIBRARY_PATH}, compilation and linking worked fine!\r\n```\r\nclang tensorflow/cc/tutorials/example_trainer.cc \\\r\n  -I${root_dir}/include \\\r\n  -ltensorflow_cc \\\r\n  -ltensorflow_framework \\\r\n  -L${root_dir}/lib \\\r\n  -lstdc++ \\\r\n  -o a.out\r\n\r\nLD_LIBRARY_PATH=${LD_LIBRARY_PATH}:PATH_TO_MY_TENSORFLOW_LIB ./a.out\r\n```\r\n\r\nReally appreciate your suggestions!", "@andreapavan please confirm if your issue is resolved", "@andreapavan \r\nplease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25058\">No</a>\n"]}, {"number": 25057, "title": "tensorflow mirroredstrategy takes forever to start training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0/7.3\r\n- GPU model and memory: 1080ti\r\n\r\n**Describe the current behavior** \r\ntf.Estimator using mirrored strategy takes forever to start training. Details are provided here https://stackoverflow.com/questions/54125722/tensorflow-mirroredstrategy-takes-forever-to-initialize\r\nAfter asking the question I additionaly tried different versions of tf/cuda, no changes. Also I was finally able to see the start of the training, It took about 40 minutes! \r\n\r\n\r\n**Describe the expected behavior**\r\nThe same model starts training in a couple of minutes without distribute strategy\r\n\r\nWhen using much simpler model, like ResNet50 mirrored strategy also lags at startup compared to single GPU, but nevertheless training starts in a couple of minutes. What may be the problem with mirrored strategy? ", "comments": ["A little update. I upgraded to 1.13 rc0, built from source. Simple model(e.g. ResNet) still working like a charm on a single GPU. With MirroredStrategy it still start training in only about 2-3 minutes, with a substantial lag(40-80 sec) on training step 0, but then it goes ok, GPU utilization around 90%.\r\nBut with a large model, mentioned in the link above, training gets consistently terminated because of protobuf limit of 2GB.", "How many GPUs are you using? If your current problem is because of protobuf limit, there is no short-term fix for now.", "@yuefengz I'm trying to use 4. But even with 2 I'm encountering the same problem, though it takes less time. Do you mean that model from official tensorflow repo isn't compatible with estimator api, or MirrorredStrategy, or both?  Could you please point me in the right direction? ", "The stackoverflow link does not work anymore. Can you please provide the details here again? Which model are you trying to run? You mentioned it is from official tensorflow repo?", "I used this model https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py\r\nI've tried changing layer objects from contrib to Keras, or Layers API, and also used it as is.  \r\n\r\nrun_config = tf.estimator.RunConfig(train_distribute=tf.distribute.MirroredStrategy(),                                                                                             save_checkpoints_steps=1000,\r\n                                    keep_checkpoint_max=5\r\n                                    )\r\n\r\nmodel = tf.estimator.Estimator(model_fn=model_fn,\r\n                             config=run_config,\r\n\r\nWithout tf.distribute.MirroredStrategy() it runs normal, otherwise it just hangs before training starts. It doesn't throw any errors. I've tried using single batch, so data size isn't an issue. Please let me know if you need any additional information", "Can you show me your model_fn for the estimator? We recently ran into a similar issue because there was an unnecessary control dependency in the model_fn which was causing the graph to blow up. I am wondering if that is the same case in your model_fn.", "I've removed some non-relevant lines\r\n\r\n```\r\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\r\n  \"\"\"Builds the 35x35 resnet block.\"\"\"\r\n  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\r\n    with tf.variable_scope('Branch_0'):\r\n      tower_conv = tf.contrib.layers.conv2d(net, 32, 1, scope='Conv2d_1x1')\r\n    with tf.variable_scope('Branch_1'):\r\n      tower_conv1_0 = tf.contrib.layers.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\r\n      tower_conv1_1 = tf.contrib.layers.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\r\n    with tf.variable_scope('Branch_2'):\r\n      tower_conv2_0 = tf.contrib.layers.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\r\n      tower_conv2_1 = tf.contrib.layers.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\r\n      tower_conv2_2 = tf.contrib.layers.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\r\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\r\n    up = tf.contrib.layers.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope='Conv2d_1x1')\r\n    net += scale * up\r\n    if activation_fn:\r\n      net = activation_fn(net)\r\n  return net\r\n\r\n\r\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\r\n  \"\"\"Builds the 17x17 resnet block.\"\"\"\r\n  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\r\n    with tf.variable_scope('Branch_0'):\r\n      tower_conv = tf.contrib.layers.conv2d(net, 192, 1, scope='Conv2d_1x1')\r\n    with tf.variable_scope('Branch_1'):\r\n      tower_conv1_0 = tf.contrib.layers.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\r\n      tower_conv1_1 = tf.contrib.layers.conv2d(tower_conv1_0, 160, [1, 7], scope='Conv2d_0b_1x7')\r\n      tower_conv1_2 = tf.contrib.layers.conv2d(tower_conv1_1, 192, [7, 1], scope='Conv2d_0c_7x1')\r\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\r\n    up = tf.contrib.layers.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope='Conv2d_1x1')\r\n    net += scale * up\r\n    if activation_fn:\r\n      net = activation_fn(net)\r\n  return net\r\n\r\n\r\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\r\n  \"\"\"Builds the 8x8 resnet block.\"\"\"\r\n  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\r\n    with tf.variable_scope('Branch_0'):\r\n      tower_conv = tf.contrib.layers.conv2d(net, 192, 1, scope='Conv2d_1x1')\r\n    with tf.variable_scope('Branch_1'):\r\n      tower_conv1_0 = tf.contrib.layers.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\r\n      tower_conv1_1 = tf.contrib.layers.conv2d(tower_conv1_0, 224, [1, 3], scope='Conv2d_0b_1x3')\r\n      tower_conv1_2 = tf.contrib.layers.conv2d(tower_conv1_1, 256, [3, 1], scope='Conv2d_0c_3x1')\r\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\r\n    up = tf.contrib.layers.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None, activation_fn=None, scope='Conv2d_1x1')\r\n    net += scale * up\r\n    if activation_fn:\r\n      net = activation_fn(net)\r\n  return net\r\n\r\n\r\ndef inception_resnet_v2(inputs, is_training=True, dropout_keep_prob=0.8, reuse=None, scope='InceptionResnetV2'):\r\n  \"\"\"Creates the Inception Resnet V2 model.\r\n  Args:\r\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\r\n    num_classes: number of predicted classes.\r\n    is_training: whether is training or not.\r\n    dropout_keep_prob: float, the fraction to keep before final layer.\r\n    reuse: whether or not the network and its variables should be reused. To be\r\n      able to reuse 'scope' must be given.\r\n    scope: Optional variable_scope.\r\n  Returns:\r\n    logits: the logits outputs of the model.\r\n    end_points: the set of end_points from the inception model.\r\n  \"\"\"\r\n\r\n  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs], reuse=reuse):\r\n    with tf.contrib.framework.arg_scope([tf.contrib.layers.dropout], is_training=is_training):\r\n      with tf.contrib.framework.arg_scope([tf.contrib.layers.conv2d]):\r\n        # 149 x 149 x 32\r\n        net = tf.contrib.layers.conv2d(inputs, 32, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3')\r\n        # 147 x 147 x 32\r\n        net = tf.contrib.layers.conv2d(net, 32, 3, padding='VALID', scope='Conv2d_2a_3x3')\r\n        # 147 x 147 x 64\r\n        net = tf.contrib.layers.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\r\n        # 73 x 73 x 64\r\n        net = tf.contrib.layers.max_pool2d(net, 3, stride=2, padding='VALID', scope='MaxPool_3a_3x3')\r\n        # 73 x 73 x 80\r\n        net = tf.contrib.layers.conv2d(net, 80, 1, padding='VALID', scope='Conv2d_3b_1x1')\r\n        # 71 x 71 x 192\r\n        net = tf.contrib.layers.conv2d(net, 192, 3, padding='VALID', scope='Conv2d_4a_3x3')\r\n        # 35 x 35 x 192\r\n        net = tf.contrib.layers.max_pool2d(net, 3, stride=2, padding='VALID', scope='MaxPool_5a_3x3')\r\n\r\n        # 35 x 35 x 320\r\n        with tf.variable_scope('Mixed_5b'):\r\n          with tf.variable_scope('Branch_0'):\r\n            tower_conv = tf.contrib.layers.conv2d(net, 96, 1, scope='Conv2d_1x1')\r\n          with tf.variable_scope('Branch_1'):\r\n            tower_conv1_0 = tf.contrib.layers.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\r\n            tower_conv1_1 = tf.contrib.layers.conv2d(tower_conv1_0, 64, 5, scope='Conv2d_0b_5x5')\r\n          with tf.variable_scope('Branch_2'):\r\n            tower_conv2_0 = tf.contrib.layers.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\r\n            tower_conv2_1 = tf.contrib.layers.conv2d(tower_conv2_0, 96, 3, scope='Conv2d_0b_3x3')\r\n            tower_conv2_2 = tf.contrib.layers.conv2d(tower_conv2_1, 96, 3, scope='Conv2d_0c_3x3')\r\n          with tf.variable_scope('Branch_3'):\r\n            tower_pool = tf.contrib.layers.avg_pool2d(net, 3, stride=1, padding='SAME', scope='AvgPool_0a_3x3')\r\n            tower_pool_1 = tf.contrib.layers.conv2d(tower_pool, 64, 1, scope='Conv2d_0b_1x1')\r\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1])\r\n\r\n        net = slim.repeat(net, 10, block35, scale=0.17, activation_fn=activation_fn)\r\n\r\n\r\n        # 17 x 17 x 1024\r\n        with tf.variable_scope('Mixed_6a'):\r\n          with tf.variable_scope('Branch_0'):\r\n            tower_conv = tf.contrib.layers.conv2d(net, 384, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3')\r\n          with tf.variable_scope('Branch_1'):\r\n            tower_conv1_0 = tf.contrib.layers.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\r\n            tower_conv1_1 = tf.contrib.layers.conv2d(tower_conv1_0, 256, 3, scope='Conv2d_0b_3x3')\r\n            tower_conv1_2 = tf.contrib.layers.conv2d(tower_conv1_1, 384, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3')\r\n          with tf.variable_scope('Branch_2'):\r\n            tower_pool = tf.contrib.layers.max_pool2d(net, 3, stride=2, padding='VALID', scope='MaxPool_1a_3x3')\r\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])\r\n\r\n        net = slim.repeat(net, 20, block17, scale=0.10, activation_fn=activation_fn)\r\n\r\n\r\n        with tf.variable_scope('Mixed_7a'):\r\n          with tf.variable_scope('Branch_0'):\r\n            tower_conv = tf.contrib.layers.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\r\n            tower_conv_1 = tf.contrib.layers.conv2d(tower_conv, 384, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3')\r\n          with tf.variable_scope('Branch_1'):\r\n            tower_conv1 = tf.contrib.layers.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\r\n            tower_conv1_1 = tf.contrib.layers.conv2d(tower_conv1, 288, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3')\r\n          with tf.variable_scope('Branch_2'):\r\n            tower_conv2 = tf.contrib.layers.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\r\n            tower_conv2_1 = tf.contrib.layers.conv2d(tower_conv2, 288, 3, scope='Conv2d_0b_3x3')\r\n            tower_conv2_2 = tf.contrib.layers.conv2d(tower_conv2_1, 320, 3, stride=2, padding='VALID', scope='Conv2d_1a_3x3')\r\n          with tf.variable_scope('Branch_3'):\r\n            tower_pool = tf.contrib.layers.max_pool2d(net, 3, stride=2, padding='VALID', scope='MaxPool_1a_3x3')\r\n          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool])\r\n\r\n        net = tf.contrib.layers.repeat(net, 9, block8, scale=0.20)\r\n        net = block8(net, activation_fn=None)\r\n        print(net.shape)\r\n\r\n        net = tf.contrib.layers.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\r\n        print(net.shape)\r\n        #net = tf.contrib.layers.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\r\n        net = tf.contrib.layers.avg_pool2d(net, net.get_shape()[1:3], padding='VALID')\r\n        net = tf.layers.flatten(net)\r\n        net = tf.contrib.layers.dropout(net, dropout_keep_prob, is_training=is_training)\r\n\r\n        logits = tf.layers.dense(inputs=net, units=28, name='out')\r\n        predictions = {\r\n          'classes': tf.argmax(input=logits, axis=1),\r\n          'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n        }\r\n        \r\n        predictions = {\r\n          'classes': tf.cast(tf.greater(tf.sigmoid(logits), 0.3), tf.int64),\r\n          'probabilities': tf.sigmoid(logits, name='sigm_tensor'),\r\n        }\r\n        \r\n  return logits, predictions\r\n\r\n\r\ndef inception_resnet_v2_arg_scope(weight_decay=0.000005,\r\n                                  batch_norm_decay=0.9997,\r\n                                  batch_norm_epsilon=0.001):\r\n  \"\"\"Yields the scope with the default parameters for inception_resnet_v2.\r\n  Args:\r\n    weight_decay: the weight decay for weights variables.\r\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\r\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\r\n  Returns:\r\n    a arg_scope with the parameters needed for inception_resnet_v2.\r\n  \"\"\"\r\n  # Set weight_decay for weights in conv2d and fully_connected layers.\r\n  with tf.contrib.framework.arg_scope([tf.contrib.layers.conv2d],\r\n                                      weights_regularizer=tf.contrib.layers.l2_regularizer(weight_decay),\r\n                                      biases_regularizer=tf.contrib.layers.l2_regularizer(weight_decay)):\r\n\r\n    batch_norm_params = {'decay': batch_norm_decay, 'epsilon': batch_norm_epsilon}\r\n    # Set activation_fn and parameters for batch_norm.\r\n    with tf.contrib.framework.arg_scope([tf.contrib.layers.conv2d],\r\n                                        activation_fn=tf.nn.relu,\r\n                                        normalizer_fn=None,\r\n                                        normalizer_params=batch_norm_params) as scope:\r\n      return scope\r\n\r\ndef model_fn(features, labels, mode):\r\n  \r\n  if_train = (mode == tf.estimator.ModeKeys.TRAIN)\r\n  input_layer = tf.cast(tf.reshape(features, [-1, 299, 299, 3]), tf.float32)\r\n\r\n  with tf.variable_scope('InceptionResnetV2'):\r\n    logits = irnv2(input_layer)\r\n\r\n  predictions = tf.argmax(input=logits, axis=1)\r\n  probabilities = tf.nn.softmax(logits, name='softmax')\r\n\r\n  if mode == tf.estimator.ModeKeys.PREDICT:\r\n    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    \r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n      train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n    \r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n  return tf.estimator.EstimatorSpec(mode=mode, loss=loss)\r\n\r\n\r\nsession_config = tf.ConfigProto(allow_soft_placement=True)\r\ndistr = tf.distribute.MirroredStrategy()\r\nrun_config = tf.estimator.RunConfig(train_distribute=distr,\r\n                                    save_checkpoints_steps=1000,\r\n                                    keep_checkpoint_max=5\r\n                                    )\r\n\r\n\r\nirn = tf.estimator.Estimator(model_fn=model_fn,\r\n                             config=run_config,\r\n                             model_dir,\r\n                             )\r\n\r\nirn.train(input_fn=tr_input_fn)\r\n```", "Thanks. Can you try changing these couple lines in the model_fn:\r\noriginal:\r\n```\r\nwith tf.control_dependencies(update_ops):\r\n      train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n```\r\n\r\nsuggestion:\r\n```\r\nminimize_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\ntrain_op = tf.group(minimize_op, updates_op)\r\n```\r\n\r\nThis has been a pattern recommended by TF docs but we are trying to change it as it is not needed, and can cause unnecessary control dependencies. \r\n", "@guptapriya, Thank you for your help!  I'm sorry for delayed response, I was away. I changed \r\n`with tf.control_dependencies(update_ops):\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())`\r\nto \r\n`update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n minimize_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n train_op = tf.group(minimize_op, update_ops)` \r\nand it worked! However. it still takes about 3 minutes, according to tf logging, for training to start. Is it expected behavior? ", "That's great. I think 3 mins is in the expected range. Will close this ticket. thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25057\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25057\">No</a>\n"]}, {"number": 25056, "title": "Update Adam.py", "body": "Removing the variable **self._updated_lr**. Do we need this variable ? And where are we using it ?\r\n\r\n`\r\n    # Created in SparseApply if needed.\r\n    self._updated_lr = None\r\n`", "comments": ["@alextp I had made some wrong commits in gradient_descent.py so reverted back"]}, {"number": 25055, "title": "Update Adam.py", "body": "We don't think we need this variable. Where do you see the point of using this variable ?   \r\n\r\n # Created in SparseApply if needed.\r\n    self._updated_lr = None", "comments": []}, {"number": 25054, "title": "How install tensorflow by python 3.7.2 on windows 10?", "body": "**System information**\r\n-- Windows version: Windows 10 64 bit\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?: virtualenv 16.2; pip 18.1\r\n\r\n**Describe the problem**\r\nI follow the tensorflow installation guide to install tensorflow by cmd methods.\r\nWhen I input \"pip3 install --user --upgrade tensorflow\" on cmd, the cmd shows the error messages\r\nthat is:\r\n\"Collecting tensorflow\r\nCould not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow\"\r\n\r\nQ1: Is this problem about the python version problem?\r\nQ2: Does the Tensorflow support the Python 3.7.2 version?\r\n\r\nTensorflow Link: [https://www.tensorflow.org/install/pip](https://www.tensorflow.org/install/pip)\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n![image](https://user-images.githubusercontent.com/25798404/51440345-25ed6b00-1d01-11e9-8b0b-df3788807db8.png)\r\n\r\n![image](https://user-images.githubusercontent.com/25798404/51440384-7238ab00-1d01-11e9-97fb-f2989aa5bcf0.png)\r\n\r\n", "comments": ["Yes and yes, They are working currently on bringing sypport to python 3.7 on linux systems, but I have no word on Windows. You should create a new env and run \r\nConda install python=3.6 which will allow you to install tensorflow.", "> Yes and yes, They are working currently on bringing sypport to python 3.7 on linux systems, but I have no word on Windows. You should create a new env and run\r\n> Conda install python=3.6 which will allow you to install tensorflow.\r\n\r\n@MatthiasRathbun \r\nQ1. Could I create the virtualenv environment to install the tensorflow and pyhton 3.7.2?\r\nQ2. Would the tensorflow support with python 3.7.2 if I install the Conda environment?", "@MatthiasRathbun \r\nQ1. Could I create the virtualenv environment to install the tensorflow and python 3.7.2?\r\nI also try to use the virtualenv environment to install tensorflow. The virtualenv is based on the python 3.7.2, but it also has same as this problem. Is it also the same as the version problems?\r\n\r\n![image](https://user-images.githubusercontent.com/25798404/51456931-680ebf00-1d8a-11e9-8b3d-e409ff4d4e7b.png)\r\n\r\n", "> @MatthiasRathbun\r\n> Q1. Could I create the virtualenv environment to install the tensorflow and python 3.7.2?\r\n> I also try to use the virtualenv environment to install tensorflow. The virtualenv is based on the python 3.7.2, but it also has same as this problem. Is it also the same as the version problems?\r\n> \r\n> ![image](https://user-images.githubusercontent.com/25798404/51456931-680ebf00-1d8a-11e9-8b3d-e409ff4d4e7b.png)\r\n\r\nNo, the only reason for mentioning a new env was in case you needed python 3.7. Tensorflow does not support python 3.7 regardless, You would need to install python 3.6 to use tensorflow. That is done by \r\n`conda install python=3.6` which will install version 3.6.8.", "https://www.lfd.uci.edu/~gohlke/pythonlibs/ has unofficial binaries for Windows that support Python 3.7.\r\n\r\nWhen will Tensorflow support Python 3.7 officially?\r\n\r\nEdit: Found [this comment](https://github.com/tensorflow/tensorflow/issues/20517#issuecomment-452888264): \"We are trying to have it for 1.13.\"", "Closing this issue since @MatthiasRathbun is correct in his explanation. Currently TensorFlow does not officially support Python 3.7. Please switch python 3.6 or lower and try again. Thanks!", "Firstly, you should download and setup the anaconda via [anaconda](https://www.anaconda.com/distribution/#download-section) and then open anaconda prompt and then follow these steps via [TensorFlow](https://www.tensorflow.org/install/pip). Fourthly, you should use 'pip install --ignore-installed --upgrade tensorflow==1.14.0' to install TensorFlow 1.14 on Conda  Finally, you can use TensorFlow that is version 1.14 on python 3.7\r\n\r\nHave a Great Time.\r\n", "can anyone please tell me the  appropriate solution", "> \r\n> \r\n> can anyone please tell me the appropriate solution\r\n\r\nJust install Python3.6\r\nIt took me 3 hours for find that's simple solution"]}]