[{"number": 11240, "title": "tf.control_dependencies does not work with variable initializer for read_value?", "body": "Consider this code:\r\n\r\n```\r\ndef test_var_init():\r\n  v = tf.Variable(initial_value=2, trainable=False, name=\"test_var_init\")\r\n  with tf.control_dependencies([v.initializer]):\r\n    x = v.read_value()\r\n  assert x.eval() == 2\r\n```\r\n\r\nIt should always succeed. However, it stochastically fails (maybe in about 75% of the runs or so), with the error:\r\n```\r\nFailedPreconditionError: Attempting to use uninitialized value test_var_init\r\n         [[Node: test_var_init/_0 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_5_test_var_init\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](test_var_init)]]\r\n```\r\n\r\nTensorFlow v1.2.0-rc2-21-g12f033d, 1.2.0, installed via pip, on Ubuntu 16.04 Linux.\r\nIt only seems to happen with GPU.\r\n\r\nI think on TF 1.1 and earlier this worked, although maybe for some reason the probability to fail was much lower there and I didn't notice it.\r\n", "comments": ["@albertz checking configuration: this occurs on the CPU; no GPU required?", "@cy89 Good that you mention this, I would not have thought that this might be related to GPU. I just tested this with and without GPU and it only seems to fail with GPU. (Tested now with TF 1.2.1.)\r\n", "Sorry to be a little formulaic: could you please fill in the GPU-related bits of the issue template, for the usual reproduction reasons? \r\n\r\nPlease provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Ok, but as I wrote, this is anyway stochastic, and I doubt that it depends on the particular GPU model, and I wrote already about OS and TF version.\r\n\r\n- **CUDA/cuDNN version**: CUDA 8.0 and cuDNN 5.1\r\n- **GPU model and memory**: GeForce GTX 680, 4036MiB\r\n", "Can you try with resource variables (i.e. use tf.get_variable(..., use_resource=True) instead of tf.Variable?", "I tried and I don't get the error in that case. Is there any documentation about this? Also, even if this is a possible work-around, shouldn't it also work with the normal `tf.Variable`? Or why doesn't it work with `tf.Variable`?", "resource variables and variables have different backend implementations,\nand the default will be switched to resource variables soon. For now that's\nthe workaround.\n\nOn Tue, Jul 11, 2017 at 7:51 AM, Albert Zeyer <notifications@github.com>\nwrote:\n\n> I tried and I don't get the error in that case. Is there any documentation\n> about this? Also, even if this is a possible work-around, shouldn't it also\n> work with the normal tf.Variable? Or why doesn't it work with tf.Variable?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11240#issuecomment-314468873>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXtn6yVeStMDXLCD4c2Lxd8KX7Qeks5sM4v-gaJpZM4OMBPh>\n> .\n>\n\n\n\n-- \n - Alex\n", "Closing for now, please reopen if the workaround is not sufficient."]}, {"number": 11239, "title": "Variables in Dataset functions not (always) initialized", "body": "### System information\r\n- **OS**: MacOS 10.12.4\r\n- **TensorFlow**: stock cpu tensorflow 1.2 from pip\r\n- **Python**: Python 2.7.13\r\n\r\n### Describe the problem\r\nWhen using a Dataset function containing tf.Variables needing initialization it succeeds sometimes, but fails other times. I haven't managed to pin point it further using `sleep`s or `tf.control_dependencies`.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\ndef fn(x):\r\n    v = tf.Variable(5, dtype=tf.int64)\r\n    return x + v\r\n\r\ndataset = (tf.contrib.data.Dataset.range(10)\r\n    .map(fn)\r\n)\r\n\r\niterator = dataset.make_initializable_iterator()\r\nnext = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n\r\n    for i in range(3):\r\n        res = sess.run(next)\r\n        print(res)\r\n```\r\n\r\nMoving the `v` variable out of the `fn` scope seems to resolve the issue.\r\n\r\nError log when variable fails to initialize:\r\n```\r\n(tf) no00023794:distributed-cluster olanymoe$ python set_test.py \r\n<MapDataset shapes: (), types: tf.int64>\r\n2017-07-03 10:33:55.377579: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-03 10:33:55.377593: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-03 10:33:55.377597: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-03 10:33:55.377601: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-07-03 10:34:05.404642: W tensorflow/core/framework/op_kernel.cc:1158] Failed precondition: Attempting to use uninitialized value Variable\r\n\t [[Node: Variable/read = Identity[T=DT_INT64, _class=[\"loc:@Variable\"]](Variable)]]\r\nTraceback (most recent call last):\r\n  File \"set_test.py\", line 49, in <module>\r\n    res = sess.run(next)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable\r\n\t [[Node: Variable/read = Identity[T=DT_INT64, _class=[\"loc:@Variable\"]](Variable)]]\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n```", "comments": ["I reproduced the issue on Ubuntu.", "While moving it out of the function scope solves the issue of initializing and reading, it doesn't seem to solve the issue of mutating the variable from inside the function. e.g.:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.Variable(5, dtype=tf.int64)\r\ndef fn(x):\r\n    return x + v.assign(v + 1)\r\n\r\ndataset = (tf.contrib.data.Dataset.range(10)\r\n    .map(fn)\r\n)\r\n\r\niterator = dataset.make_initializable_iterator()\r\nnext = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(iterator.initializer)\r\n\r\n    for i in range(3):\r\n        res = sess.run(next)\r\n        print(res)\r\n\r\n```\r\n\r\ngives the error log:\r\n\r\n```\r\n(tf) no00023794:distributed-cluster olanymoe$ python test.py \r\nTraceback (most recent call last):\r\n  File \"test.py\", line 9, in <module>\r\n    .map(fn)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 873, in map\r\n    return MapDataset(self, map_func, num_threads, output_buffer_size)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 1551, in __init__\r\n    self._map_func.add_to_graph(ops.get_default_graph())\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/framework/function.py\", line 449, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/framework/function.py\", line 168, in _create_definition_if_needed\r\n    outputs = self._func(*inputs)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 1539, in tf_map_func\r\n    ret = map_func(nested_args)\r\n  File \"test.py\", line 5, in fn\r\n    with tf.control_dependencies([v.assign(v + 1)]):\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 524, in assign\r\n    return state_ops.assign(self._variable, value, use_locking=use_locking)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 274, in assign\r\n    validate_shape=validate_shape)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 45, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/contrib/data/python/framework/function.py\", line 80, in create_op\r\n    data_types, **kwargs)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/framework/function.py\", line 665, in create_op\r\n    **kwargs)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2576, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/olanymoe/anaconda2/envs/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1185, in __init__\r\n    input_types))\r\nTypeError: In op 'Assign', input types ([tf.int64, tf.int64]) are not compatible with expected types ([tf.int64_ref, tf.int64])\r\n```\r\n\r\nThis is also tested with tensorflow CPU version compiled against master branch (afa03e2fd91a2d4a7dc960)", "As a \"workaround\" / hack it's possible to split the dataset into two, and reinitialize the second dataset from the first on each session run:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nv = tf.Variable(5, dtype=tf.int64)\r\n\r\ndataset1 = tf.contrib.data.Dataset.range(10)\r\niterator1 = dataset1.make_one_shot_iterator()\r\n\r\n# Out of dataset function scope so mutations possible\r\nelements = iterator1.get_next() + v.assign(v + 1)\r\n\r\ndataset2 = tf.contrib.data.Dataset.from_tensors(elements)\r\niterator2 = dataset2.make_initializable_iterator()\r\n\r\n# Crazy hack .. reinitialize second iterator from first for each run\r\nwith tf.control_dependencies([iterator2.initializer]):\r\n    next = iterator2.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    for i in range(10):\r\n        res = sess.run(next)\r\n        print(res)\r\n```\r\n\r\nThe reason for wanting this functionality is making oversampling part of the dataset pipeline by keeping a running average of the input distribution (i.e. calculate statistics on the original data, but output oversampled datapoints at the end of the pipeline).", "I'm not following why you'd want to declare the variable inside the function (your second example where you move it outside the function scope makes more sense to me). @alextp Can you comment on the second error int64 vs ref_int64 types, and perhaps suggest a better usage pattern for variables here?", "Assigning tf.Variable variables inside functions is not currently supported.\r\n\r\nYou can get around it by using tf resource variables, which are accessible by using tf.get_variable(..., use_resource=True) instead of tf.Variable to construct your variables.\r\n\r\nI think declaring a resource variable outside the function will solve your problem.", "thanks @alextp. please ask this on stackoverflow if you need further assistance.", "### System information\r\n- Ubuntu 16.04\r\n- Python 3.5\r\n- TF 1.5.0-dev20171118\r\n- GPU: Titan Xp\r\n\r\n### Describe the problem\r\nI encountered a strange case.\r\n\r\nMy training was fine on GPU 2 but when I train my model on GPU 1, TF always throw the following warning.\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:1198] Failed precondition: GetNext() failed \r\nbecause the iterator has not been initialized. Ensure that you have run the initializer operation \r\nfor this iterator before getting the next element.\r\n```\r\n\r\nI used the same training script and I did run the initializer (I was using `tf.data.TFRecordDataset` with make `make_initializable_iterator`):\r\n```python\r\nwith sv.managed_session(config=sess_config) as sess:\r\n    sess.run(data.iterator.initializer)\r\n```\r\nThe most annoying thing is that I couldn't use Tensorboard to monitor the training (despite the warning, training proceeded). \r\n\r\nNot sure whether it is the place to report this bug-like behavior here...", "@alextp @mrry Hey fellas!  I ran into this issue as well.  My use case is simple: use `Dataset.map(PerBatchWhiten)` to whiten each batch; however, I'd like this function to read from and update exponential moving averages of mean/std seen across entire history.  Hence the need to assign variables and read them from within the map_func.\r\n\r\nI tried to follow Alex's suggestions of creating my mean/std resource variables outside, but still ran into some issues with properly depending on the EMA ops from within the map_func.  Some error snippet thrown from within the Map():\r\n```\r\nValueError: Operation name: \"ExponentialMovingAverage\"\r\nop: \"NoOp\"\r\ninput: \"^ExponentialMovingAverage/AssignMovingAvg/ReadVariableOp_1\"\r\ninput: \"^ExponentialMovingAverage/AssignMovingAvg_1/ReadVariableOp_1\"\r\ninput: \"^ExponentialMovingAverage/AssignMovingAvg_2/ReadVariableOp_1\"\r\ninput: \"^ExponentialMovingAverage/AssignMovingAvg_3/ReadVariableOp_1\"\r\n is not an element of this graph.\r\n```\r\nIs there a currently recommended way of using TF Dataset + Exponential moving averages?", "Hi @concretevitamin. It should be possible to make this work, by moving the assignment into the `map()` function. This is probably a question more suited for Stack Overflow, where it would also be useful to see a minimum non-working example and the stack trace that accompanies the `ValueError`."]}, {"number": 11237, "title": "remove some warning", "body": "remove some warning for master branch.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->"]}, {"number": 11236, "title": "Cannot use AdamOptimizer in C++ when running graph defined from Python", "body": "Running Ubuntu 14.04, with r1.2 Tensorflow\r\n\r\nI have defined a graph in Python\r\n```\r\nfeatures = 784\r\noutput_dim = 10\r\ngraph_name = \"graph.pb\"\r\ngraph_folder = \"/home/fran/Repositories/tensorflow/bazel-bin/tensorflow/test\"\r\ninput_node_name = \"input\"\r\noutput_node_name = \"output\"\r\n\r\nwith tf.Session() as session:\r\n    x = tf.placeholder(tf.float32, [None, features], name=\"input\")\r\n    W = tf.Variable(tf.zeros([features, output_dim]), dtype=tf.float32, name=\"Weight\")\r\n    b = tf.Variable(tf.zeros([output_dim]), dtype=tf.float32, name=\"bias\")\r\n    y_pred = tf.nn.softmax(tf.matmul(x, W)+b, name=output_node_name)\r\n\r\n    y_true = tf.placeholder(tf.float32, [None, 10], name=\"y_true\")\r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_pred), reduction_indices=[1]), name=\"loss\")\r\n    optimizer = tf.train.AdamOptimizer(0.00001)\r\n    train_step = optimizer.minimize(cross_entropy, name=\"train\")\r\n    init = tf.variables_initializer(tf.global_variables(), name='init_all_vars_op')\r\n    saver = tf.train.Saver()\r\n    tf.train.write_graph(session.graph_def, graph_folder, graph_name, as_text=False)\r\n```\r\n\r\nIf I run this graph in a C++ program, where I load the graph, initialize the variables and feed X and Y tensors to \"train\", I get the following error (works fine with other optimizers like GradientDescent or RMSPropOptimizer):\r\n```\r\nTraining.\r\nStep: 0; Loss: 9.25913\r\n2017-07-03 09:54:43.264264: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\r\n2017-07-03 09:54:43.264468: F tensorflow/driving_school/training.cc:102] Non-OK-status: session->Run(inputs, {}, {\"train\"}, nullptr) status: Invalid argument: NodeDef mentions attr 'use_nesterov' not in Op<name=ApplyAdam; signature=var:Ref(T), m:Ref(T), v:Ref(T), beta1_power:T, beta2_power:T, lr:T, beta1:T, beta2:T, epsilon:T, grad:T -> out:Ref(T); attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_UINT8, DT_UINT16, DT_INT16, DT_INT8, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF]; attr=use_locking:bool,default=false>; NodeDef: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\t [[Node: train/update_Weight/ApplyAdam = ApplyAdam[T=DT_FLOAT, _class=[\"loc:@Weight\"], use_locking=false, use_nesterov=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Weight, Weight/Adam, Weight/Adam_1, beta1_power/read, beta2_power/read, train/learning_rate, train/beta1, train/beta2, train/epsilon, gradients/MatMul_grad/tuple/control_dependency_1)]]\r\nAborted (core dumped)\r\n```\r\nIs this a bug?\r\n\r\nFor reference, the code I use to load the graph definition and train is the following:\r\n```\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nint main(int argc, char* argv[]) {\r\n\r\n    Session* session;\r\n    std::cout << \"Initializing Tensorflow session.\" << std::endl;\r\n    TF_CHECK_OK(NewSession(SessionOptions(), &session));\r\n\r\n    // Read in the protobuf graph we previously exported\r\n    // (The path is relative to the cwd. Keep this in mind\r\n    // when using `bazel run` since the cwd isn't where you call\r\n    // `bazel run` but from inside a temp folder.)\r\n    GraphDef graph_def;\r\n    std::cout << \"Reading protobuf graph.\" << std::endl;\r\n    TF_CHECK_OK(ReadBinaryProto(Env::Default(), \"graph.pb\", &graph_def));\r\n\r\n    std::cout << \"Loading graph in Tensorflow session.\" << std::endl;\r\n    TF_CHECK_OK(session->Create(graph_def));\r\n\r\n    int batch_size = 64;\r\n    int input_feature_len = 784;\r\n    int output_feature_len = 10;\r\n    int training_steps = 1000;\r\n    int save_interval = 100;\r\n    std::string save_path = \"/home/fran/Repositories/tensorflow/tensorflow/test/\";\r\n\r\n    std::cout << \"Initializing Tensorflow I/O tensors.\" << std::endl;\r\n    Tensor X(DT_FLOAT, TensorShape({batch_size, input_feature_len}));\r\n    Tensor Y(DT_FLOAT, TensorShape({batch_size, output_feature_len}));\r\n\r\n    // Initialize our variables for training\r\n    std::cout << \"Initializing Tensorflow variables.\" << std::endl;\r\n    TF_CHECK_OK(session->Run({}, {}, {\"init_all_vars_op\"}, nullptr));\r\n\r\n    auto _XTensor = X.matrix<float>();\r\n    auto _YTensor = Y.matrix<float>();\r\n    _XTensor.setRandom();\r\n    _YTensor.setRandom();\r\n\r\n    // Assuming graph contains single input X, with node name \"input\"\r\n    // and single ground truth placeholder, with node name \"y_true\"\r\n    std::vector<std::pair<string, Tensor>> inputs = {\r\n            { \"input\", X },\r\n            { \"y_true\", Y }\r\n    };\r\n\r\n    // for checkpoint generation\r\n    Tensor model_string(DT_STRING, TensorShape( { 1, 1 } ) );\r\n\r\n    // Initialize output pointer for loss metric.\r\n    // Assuming graph contains single loss, with node name \"loss\"\r\n    // and single training node, with node name \"train\"\r\n    std::cout << \"Training.\" << std::endl;\r\n    std::vector<Tensor> output;\r\n    for (int i=0; i<training_steps; ++i) {\r\n        TF_CHECK_OK(session->Run(inputs, {\"loss\"}, {}, &output)); // Get loss (for monitoring)\r\n        float loss = output[0].scalar<float>()(0);\r\n        std::cout << \"Step: \" << i << \"; Loss: \" << loss << std::endl;\r\n        TF_CHECK_OK(session->Run(inputs, {}, {\"train\"}, nullptr)); // Train\r\n        output.clear();\r\n\r\n        // to save checkpoint feed name of checkpoint file as saver_def.filename_tensor_name\r\n        // and fetch the value of saver_def.save_tensor_name, as per graph definition script.\r\n        if (i % save_interval == 0){\r\n            model_string.matrix< std::string >()( 0, 0 ) = save_path + \"model_checkpoint_step\" + std::to_string(i) + \".ckpt\";\r\n            TF_CHECK_OK(session->Run({{\"save/Const:0\", model_string}}, {}, {\"save/control_dependency\"}, nullptr));\r\n        };\r\n    };\r\n    std::cout << \"Training complete.\" << std::endl;\r\n    \r\n    std::cout << \"Closing Tensorflow session.\" << std::endl;\r\n    session->Close();\r\n    delete session;\r\n    return 0;\r\n}\r\n```\r\n", "comments": ["Maybe you used a different TF version for the C++ code? In [here](https://github.com/tensorflow/tensorflow/blob/0c58158903ea9a3af612ae428c6cb3dede4abf16/tensorflow/core/ops/training_ops.cc#L1162) you see that `use_nesterov` is part of the `ApplyAdam` op. That was introduced in commit 53cb26d05a5c2080d8022124178b1cc43a30ffe5, which is part of TF 1.2.0.\r\n", "@fferroni Can I please know how to create model_string.meta . The below command creates only  model_string.index and model_string.data-00000-of-00001 files. .meta file is needed to do prediction as described in https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c\r\n\r\n`TF_CHECK_OK(session->Run({{\"save/Const:0\", model_string}}, {}, {\"save/control_dependency\"}, nullptr));`\r\n\r\nIs there any other way to do prediction in c++ without help of .meta file ?", "@fferroni Is the problem solved? Is it the TF version that matters? Very interested in it."]}, {"number": 11235, "title": "quantify the mobilenet", "body": "I try to quantify the mobilenet(in the https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md ssd_mobilenet_v1_coco), the tensorflow I use is v1.2,\r\nbazel build tensorflow/tools/quantization:quantize_graph \\\r\n&& bazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n--input=bazel build tensorflow/tools/quantization:quantize_graph \\\r\n&& bazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n--input=ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb\r\n--output_node_names=\"  \" --print_nodes --output=/tmp/quantized_graph.pb \\\r\n--mode=eightbit --logtostderr\r\nbut I don't decide the out_node_names .", "comments": ["I don't have a working solution but I have come a little longer on this issue.\r\n\r\nYou can use this tool to summarize your graph:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#inspecting-graphs\r\n\r\nIt will give you suggestions for outputs:\r\nFound 4 possible outputs: (name=detection_boxes, op=Identity) (name=detection_scores, op=Identity) (name=detection_classes, op=Identity) (name=num_detections, op=Identity)\r\n\r\nSo you write:\r\n--output_node_names=\"detection_boxes,detection_scores,detection_classes,num_detections\".\r\n\r\nHowever, this will not work either and it might be related to the comment by mddrill 23may:\r\nhttps://github.com/tensorflow/tensorflow/issues/8347\r\n\r\nHe basically states that the output nodes in the MobileNet are op=Identity which are removed by quantize_graph.py. \r\n\r\nHis fix does not work for me since my quantized graph is the same size as before quantization. It seems like the script ends up doing nothing.\r\n\r\nI'm not sure if the summarize tool lists the wrong nodes for the output or if the quantization tool does not work properly on the op=Identity.\r\n\r\nBest\r\n", "@nisseb for output_node_name in output_node_names\r\nKeyError: 'detection_boxes'", "Yes, that is what I got as well, because of what I wrote after:\r\n\"However, this will not work either ...\"\r\n\r\nIf you run the inspecting graphs script you see that all output nodes are IdentityOp which are removed by the quantize graph script (I think (?)).\r\n\r\nMaybe try your luck with transform_graph:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#inspecting-graphs\r\nWhich unfortunately also has an issue posted:\r\nhttps://stackoverflow.com/questions/44832492/tensorflow-ssd-mobilenet-model-accuracy-drop-after-quantization-using-transform\r\n\r\nLet me know if you solve it!\r\n\r\nBest", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I have the same question and I do not know how to resolve it ,could give me some sugessions", "@wujiahongPKU I still solve it.", "@wm901115nwpu  please tell me the way to solve it,as detailed as possible,Thanks!", "I have quantized the ssd-mobilenet_v1 model success, but when I run on android demo, the model can not be loaded.  i have create a [Issue 2853 here](https://github.com/tensorflow/models/issues/2853)", "I have the same issue. Does anyone solve it?\r\nKeyError: 'detection_boxes'\r\n", "@shinya321 If you're using \"--mode=quantize\" on the latest master commit of tensorflow, I found that there is a bug in the code that produces a KeyError at https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/tools/quantization/quantize_graph.py#L490\r\n\r\nThat line should be changed from:\r\n```\r\nif self.already_visited[current_node.name]:\r\n```\r\nto:\r\n```\r\nif current_node.name in self.already_visited and self.already_visited[current_node.name]:\r\n```", "@alexcdot Thank you so much for replying me!\r\nI updated \"quantize_graph.py\" as you said, and try to quantize ssd-mobilenet v1 model as below:\r\n<pre>\r\npython quantize_graph_new.py --input=frozen_inference_graph.pb --output_node_names=\"detection_scores,detection_classes,num_detections\" --output=frozen_8bit_graph.pb --mode=eightbit\r\n</pre>\r\n\r\nNext I try to execute inference by using frozen graph \"frozen_8bit_graph.pb\",\r\nbut still it does not work.... like this.\u2193\u2193\r\n<pre>\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nc:\\users\\s120077.sei\\appdata\\local\\continuum\\anaconda3\\envs\\takubo\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    488           results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 489               graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    490           results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n\r\nInvalidArgumentError: Node 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1_eightbit_reshape___hat__FeatureExtractor/Assert/Assert': Control dependencies must come after regular dependencies\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-88c9617d43b8> in <module>()\r\n     15     serialized_graph = fid.read()\r\n     16     od_graph_def.ParseFromString(serialized_graph)\r\n---> 17     tf.import_graph_def(od_graph_def, name='')\r\n     18 \r\n     19 \r\n\r\nc:\\users\\s120077.sei\\appdata\\local\\continuum\\anaconda3\\envs\\takubo\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py in new_func(*args, **kwargs)\r\n    430                 'in a future version' if date is None else ('after %s' % date),\r\n    431                 instructions)\r\n--> 432       return func(*args, **kwargs)\r\n    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    434                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\nc:\\users\\s120077.sei\\appdata\\local\\continuum\\anaconda3\\envs\\takubo\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    491         except errors.InvalidArgumentError as e:\r\n    492           # Convert to ValueError for backwards compatibility.\r\n--> 493           raise ValueError(str(e))\r\n    494 \r\n    495       # Create _DefinedFunctions for any imported functions.\r\n\r\nValueError: Node 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1_eightbit_reshape___hat__FeatureExtractor/Assert/Assert': Control dependencies must come after regular dependencies\r\n\r\n</pre>\r\n\r\nI could execute inference program by using no quantized model.\r\nIf you know, please tell me why the program does not work.", "> @alexcdot Thank you so much for replying me!\r\n> I updated \"quantize_graph.py\" as you said, and try to quantize ssd-mobilenet v1 model as below:\r\n> \r\n> python quantize_graph_new.py --input=frozen_inference_graph.pb --output_node_names=\"detection_scores,detection_classes,num_detections\" --output=frozen_8bit_graph.pb --mode=eightbit\r\n> Next I try to execute inference by using frozen graph \"frozen_8bit_graph.pb\",\r\n> but still it does not work.... like this.\u2193\u2193\r\n> \r\n> ---------------------------------------------------------------------------\r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> c:\\users\\s120077.sei\\appdata\\local\\continuum\\anaconda3\\envs\\takubo\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n>     488           results = c_api.TF_GraphImportGraphDefWithResults(\r\n> --> 489               graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n>     490           results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n> \r\n> InvalidArgumentError: Node 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1_eightbit_reshape___hat__FeatureExtractor/Assert/Assert': Control dependencies must come after regular dependencies\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> ValueError                                Traceback (most recent call last)\r\n>  in ()\r\n>      15     serialized_graph = fid.read()\r\n>      16     od_graph_def.ParseFromString(serialized_graph)\r\n> ---> 17     tf.import_graph_def(od_graph_def, name='')\r\n>      18 \r\n>      19 \r\n> \r\n> c:\\users\\s120077.sei\\appdata\\local\\continuum\\anaconda3\\envs\\takubo\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py in new_func(*args, **kwargs)\r\n>     430                 'in a future version' if date is None else ('after %s' % date),\r\n>     431                 instructions)\r\n> --> 432       return func(*args, **kwargs)\r\n>     433     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n>     434                                        _add_deprecated_arg_notice_to_docstring(\r\n> \r\n> c:\\users\\s120077.sei\\appdata\\local\\continuum\\anaconda3\\envs\\takubo\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n>     491         except errors.InvalidArgumentError as e:\r\n>     492           # Convert to ValueError for backwards compatibility.\r\n> --> 493           raise ValueError(str(e))\r\n>     494 \r\n>     495       # Create _DefinedFunctions for any imported functions.\r\n> \r\n> ValueError: Node 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/BatchNorm/batchnorm/mul_1_eightbit_reshape___hat__FeatureExtractor/Assert/Assert': Control dependencies must come after regular dependencies\r\n> \r\n> I could execute inference program by using no quantized model.\r\n> If you know, please tell me why the program does not work.\r\n\r\ni meet same problem,appear error,when i run inference program use quanrtized pd file.\r\nValueError: Node 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/convolution_eightbit_reshape___hat__FeatureExtractor/Assert/Assert': Control dependencies must come after regular dependencies\r\n"]}, {"number": 11234, "title": "add a new config option sycl_nodouble for SYCL build", "body": "When TF is built with SYCL enabled, the SYCL device code is generated\r\nat build time. Currently, all the data types such as float and double\r\nare registered to generate the device code.\r\n\r\nThe SYCL device code is compiled into SPIR at build time, and then\r\npassed to OpenCL implemenation at runtime. Since double precision is\r\nan optional feature in the OpenCL spec, it is possible that an OpenCL\r\nimplemenation does not support double.\r\n\r\nTo make some platforms without double support work, this new config\r\noption disables double register for SYCL device code.\r\n\r\nThis patch just changes the cwise_add operation as an example, and\r\nother operations will be changed in future small patches one by one.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "That makes sense.\r\nIs there a place were the compilation flags are summarized?\r\nIf so, this new `SYCL_NO_DOUBLE` flag and the new config option `sycl_nodouble` should be documented there.", "yes, it would be better to summarize all the compilation flags in one place. Looks that the most like place for sycl_nodouble would be at https://www.tensorflow.org/install/install_sources. Once sycl build is introduced here, sycl_nodouble can also be introduced here. Anyway, i think it can be done in another patch.", "just curious why the sanity checks are still waiting for status to be reported after 2 days, how could I see the detail of the sanity check to see what is happening. thanks.", "Thanks @guoyejun !\r\n@keryell that is an excelent point! In the near future we will be adding something simillar for half ( as it requires an OpenCL extension and might be not supported on all OpenCL platforms ). Indeed https://www.tensorflow.org/install/install_sources seems like good place for that. \r\n\r\n@benoitsteiner / @gunan / @drpngx what do you think? Is this approach acceptable?", "@drpngx Please take another look?", "Jenkins, test this please.", "So, there are a lot more double sycl kernels in the code. We'll need the ifdef guard everywhere?\r\n\r\nWhat is the nature of the error? Is it a runtime error or a build error?", "yes, there will be many ifdef to disable double sycl kernels (within the scope of TENSORFLOW_USE_SYCL).\r\n\r\nIt is a runtime error.  At build time, sycl device compiler (/usr/local/computecpp/bin/compute++) compiles the sycl kernels into SPIR. While at runtime, the SPIR (including double operations) is passed to OpenCL implementation. There will be an issue if the OpenCL implementation does not support double, which is allowed by OpenCL spec.\r\n", "Another question, is SYCL_NO_DOUBLE used anywhere else but tensorflow?\n\nOn Jul 6, 2017 7:37 PM, \"Guo Yejun (\u90ed\u53f6\u519b)\" <notifications@github.com> wrote:\n\n> yes, there will be many ifdef to disable double sycl kernels (within the\n> scope of TENSORFLOW_USE_SYCL).\n>\n> It is a runtime error. At build time, sycl device compiler\n> (/usr/local/computecpp/bin/compute++) compiles the sycl kernels into\n> SPIR. While at runtime, the SPIR (including double operations) is passed to\n> OpenCL implementation. There will be an issue if the OpenCL implementation\n> does not support double, which is allowed by OpenCL spec.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11234#issuecomment-313571493>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbdpLXoslmzoTfdVFAI3Jypuh8NObks5sLZn1gaJpZM4OL40J>\n> .\n>\n", "SYCL_NO_DOUBLE is only used in tensorflow.\r\n(do you prefer another naming? thanks)", "A lot of the SYCL kernels (though not all of them) are registered using `TF_CALL_GPU_NUMBER_TYPES_NO_HALF` from [register_types.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/register_types.h). This could be changed to a more SYCL specific define (something like `TF_CALL_SYCL_NUMBER_TYPES`?), which can be changed according to whether we want to support double, half etc. This would minimise the changes needed when we want to change the types registered for SYCL.\r\nThis doesn't help for the cwise ops though.", "The convention seems to be using `TENSORFLOW_SYSCL_NO_DOUBLE` for defines passed from the outside (e.g. `TENSORFLOW_USE_VERBS`). For inside variables, I would recommend a single place that has the `#ifdef` switch and use a macro in the spirit of what @jwlawson suggested. Could you draft that?", "The SYCL kernels are registered with different methods. Some sycl kernels are registered with TF_CALL_float/double, TF_CALL_GPU_NUMBER_TYPES_NO_HALF and TF_CALL_NUMBER_TYPES_NO_INT32 defined in register_types.h. Some are registered with REGISTER* defined in cwise_ops_common.h. And some are registered specially with the macro defined within its own .cc file, for example, fill_functor.cc.  \r\n\r\nIt's hard to find only one place to handle all the cases (for example, as @jwlawson mentioned, cwise ops cannot benifit from register_types.h ). With spirit of what @jwlawson suggested, we can try to reduce the places of #ifdef as less as possible. \r\n\r\nFor all the cwise ops, we can use one #ifdef in cwise_ops_common.h to handle double, and future half float usage. I'll upload the patch. For other sycl kernel categories, we can use the same spirit to reduce the number of #ifdef.\r\n", "any other comments? thanks.", "Jenkins, test this please.\r\n\r\nLooking forward to the CL that does it on all operators."]}, {"number": 11233, "title": "About if tensorflow can use flask to mount in IIS, use like as an web API to use? ", "body": "I set up a tensorflow system, use flask grammar to set up . And in local computer, all run ok, no error.\r\nBut when I mount it in IIS, as Web API to use, It always show FASTCGI error. Does tensorflow  incompatiable with IIS?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11232, "title": "Enable MKL Support in TensorFlow for Java", "body": "# Description\r\n\r\nI'm trying to use TensorFlow for Java in a Dataflow pipeline. Currently everything appears to be working but since Dataflow only supports CPU instances, inference time seems to be quite slow. In my previous experiments I've seen that building TensorFlow from source with MKL support usually provides a very significant speed gain.\r\n\r\nSince I'm currently using TensorFlow for Java directly from Maven repository, I won't be able to get MKL support. Would it be possible to enable MKL support for TensorFlow in Java?", "comments": ["/CC: @rmlarsen ", "For now, you'll have to build the native library from source. Something along the lines of (see the [README](https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/java#building-from-source) for details):\r\n\r\n```\r\ngit checkout <release_branch>\r\n./configure # Make sure you enable MKL\r\nbazel build -c opt //tensorflow/java:libtensorflow_jni.so\r\n```\r\n\r\nIf the native library is available in the JVM path (e.g., `-Djava.library.path=...`), then it will be preferred over the native library packaged in Maven (see [`NativeLibrary.java`](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/java/src/main/java/org/tensorflow/NativeLibrary.java#L48))\r\n\r\nAs for including MKL support in the maven packages we release - I'm sure you're familiar with the effort it takes to maintain and support binaries for different platforms and configurations. MKL-enabled distributions is something we're looking into (CC @tfboyd). We're unlikely to provide MKL-enabled configurations just for Java. But if and when MKL-enabled binaries are distributed for other TensorFlow binaries, we're likely to do the same for Java.\r\n\r\nHope that answers your question.", "I am working on a post with data to help people with CPU only situations.  MKL can provide a lot of improvement for CNN style models but there are some edge cases and you will need to set some environment variables as well as use the desired data format NCHW (assuming image data).  I am a big fan and I realize there is a lot we do not know about MKL performance.  With not very many people running the MKL version there are issue that could exist.  I have everyone knows the TensorFlow in GitHub is the same we use internally at Google and we do a lot of CPU training.  This provides a huge testing bed.  For a few reasons we cannot use MKL internally and thus it will be a while before it gets the level of testing as AVX and AVX2 editions.\r\n\r\nMKL also provides the most gains for training and on larger systems, e.g. 22-44 cores.  What you can do that is much safer is make sure to compile with AVX or AVX2 support based on your underlying system.  That will also provide significant gains over the default build that uses SSE3.  \r\n\r\nFinally, while we are working on getting the MKL editions to build on OSX and possibly windows it currently is only running on Linux.  Some community members have compiled for OSX but the configs need cleaned up for everyone to use.  \r\n\r\nHappy to answer questions and share any info I have from my testing.  ", "@tfboyd \r\nI think I managed to compile from source with mkl on my ubuntu machine, but it turned out to be much slower then the maven build, (about 6 times slower with mkl).\r\ndoes it make sense to you, since I didn't optimize as you said?\r\n> you will need to set some environment variables as well as use the desired data format NCHW\r\n\r\nAnyway, can you refer me to your post that you said you were working on? or even the drafts (if you didn't publish it yet)\r\nThanks ahead!", "If you clone master, you should be able to build the java library with `--config=mkl` and it should just work  with the latest changes.\r\nAt the moment, this is only on linux, but macOS support is pending a few bugfixes on bazel.", "@harelfar2 \r\n\r\n6x slower comparing CPU to CPU is not something I saw in my testing even when almost purposely picking bad configs for MKL.  The Maven build should only be SSE 3.1 (or something like that) and in the simple Resnet/Inception/AlexNet conv tests MKL was faster even with the less than optimal data_format and env variables.  I have likely pasted this too many places but here is a big part of the post I am trying to get through the editing process.\r\n\r\n### TensorFlow with Intel\u00ae MKL DNN\r\n\r\nIntel\u00ae has added optimizations to TensorFlow for Intel\u00ae Xeon\u00ae and Intel\u00ae Xeon\r\nPhi\u2122 though the use of Intel\u00ae Math Kernel Library for Deep Neural Networks\r\n(Intel\u00ae MKL-DNN) optimized primitives. The optimizations also provide speedups\r\nfor the consumer line of processors, e.g. i5 and i7 Intel processors. The Intel\r\npublished paper\r\n[TensorFlow* Optimizations on Modern Intel\u00ae Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture)\r\ncontains additional details on the implementation.\r\n\r\n> Note: MKL was added as of TensorFlow 1.2 and currently only works on Linux. It\r\n> also does not work when also using `--config=cuda`.\r\n\r\nIn addition to providing significant performance improvements for training CNN\r\nbased models, compiling with the MKL creates a binary that is optimized for AVX\r\nand AVX2. The result is a single binary that is optimized and compatible with\r\nmost modern (post-2011) processors.\r\n\r\nTensorFlow can be compiled with the MKL optimizations using the following\r\ncommands that depending on the version of the TensorFlow source used.\r\n\r\nFor TensorFlow source versions after 1.3.0:\r\n\r\n```bash\r\n./configure\r\n# Pick the desired options\r\nbazel build --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\n\r\nFor TensorFlow versions 1.2.0 through 1.3.0:\r\n\r\n```bash\r\n./configure\r\nDo you wish to build TensorFlow with MKL support? [y/N] Y\r\nDo you wish to download MKL LIB from the web? [Y/n] Y\r\n# Select the defaults for the rest of the options.\r\n\r\nbazel build --config=mkl --copt=\"-DEIGEN_USE_VML\" -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\n\r\n#### Tuning MKL for the best performance\r\n\r\nThis section details the different configurations and environment variables that\r\ncan be used to tune the MKL to get optimal performance. Before tweaking various\r\nenvironment variables make sure the model is using the `NCHW` (`channels_first`)\r\n[data format](#data-formats). The MKL is optimized for `NCHW` and Intel is\r\nworking to get near performance parity when using `NHWC`.\r\n\r\nMKL uses the following environment variables to tune performance:\r\n\r\n*   KMP_BLOCKTIME - Sets the time, in milliseconds, that a thread should wait,\r\n    after completing the execution of a parallel region, before sleeping.\r\n*   KMP_AFFINITY - Enables the run-time library to bind threads to physical\r\n    processing units.\r\n*   KMP_SETTINGS - Enables (true) or disables (false) the printing of OpenMP*\r\n    run-time library environment variables during program execution.\r\n*   OMP_NUM_THREADS - Specifies the number of threads to use.\r\n\r\nMore details on the KMP variables are on\r\n[Intel's](https://software.intel.com/en-us/node/522775) site and the OMP\r\nvariables on\r\n[gnu.org](https://gcc.gnu.org/onlinedocs/libgomp/Environment-Variables.html)\r\n\r\nWhile there can be substantial gains from adjusting the environment variables,\r\nwhich is discussed below, the simplified advice is to set the\r\n`inter_op_parallelism_threads` equal to the number of physical CPUs and to set\r\nthe following environment variables:\r\n\r\n*   KMP_BLOCKTIME=0\r\n*   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\r\n\r\nExample setting MKL variables with command-line arguments:\r\n\r\n```bash\r\nKMP_BLOCKTIME=0 KMP_AFFINITY=granularity=fine,verbose,compact,1,0 \\\r\nKMP_SETTINGS=1 python your_python_script.py\r\n```\r\n\r\nExample setting MKL variables with python `os.environ`:\r\n\r\n```python\r\nos.environ[\"KMP_BLOCKTIME\"] = str(FLAGS.kmp_blocktime)\r\nos.environ[\"KMP_SETTINGS\"] = str(FLAGS.kmp_settings)\r\nos.environ[\"KMP_AFFINITY\"]= FLAGS.kmp_affinity\r\nif FLAGS.num_intra_threads > 0:\r\n  os.environ[\"OMP_NUM_THREADS\"]= str(FLAGS.num_intra_threads)\r\n\r\n```\r\n\r\nThere are models and hardware platforms that benefit from different settings.\r\nEach variable that impacts performance is discussed below.\r\n\r\n*   **KMP_BLOCKTIME**: The MKL default is 200ms, which was not optimal in our\r\n    testing. 0 (0ms) was a good default for CNN based models that were tested.\r\n    The best performance for AlexNex was achieved at 30ms and both GoogleNet and\r\n    VGG11 performed best set at 1ms.\r\n\r\n*   **KMP_AFFINITY**: The recommended setting is\r\n    `granularity=fine,verbose,compact,1,0`.\r\n\r\n*   **OMP_NUM_THREADS**: This defaults to the number of physical cores.\r\n    Adjusting this parameter beyond matching the number of cores can have an\r\n    impact when using Intel\u00ae Xeon Phi\u2122 (Knights Landing) for some models. See\r\n    [TensorFlow* Optimizations on Modern Intel\u00ae Architecture](https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture)\r\n    for optimal settings.\r\n\r\n*   **intra_op_parallelism_threads**: Setting this equal to the number of\r\n    physical cores is recommended. Setting the value to 0, which is the default\r\n    and will result in the value being set to the number of logical cores, is an\r\n    option to try for some architectures.  This value and `OMP_NUM_THREADS`\r\n    should be equal.\r\n\r\n*   **inter_op_parallelism_threads**: Setting this equal to the number of\r\n    sockets is recommended. Setting the value to 0, which is the default,\r\n    results in the value being set to the number of logical cores.\r\n\r\n### Comparing compiler optimizations\r\n\r\nCollected below are performance results running training and inference on\r\ndifferent types of CPUs on different platforms with various compiler\r\noptimizations.  The models used were ResNet-50\r\n([arXiv:1512.03385](https://arxiv.org/abs/1512.03385)) and\r\nInceptionV3 ([arXiv:1512.00567](https://arxiv.org/abs/1512.00567)).\r\n\r\nFor each test, when the MKL optimization was used the environment variable\r\nKMP_BLOCKTIME was set to 0 (0ms) and KMP_AFFINITY to\r\n`granularity=fine,verbose,compact,1,0`.\r\n\r\n#### Inference InceptionV3\r\n\r\n**Environment**\r\n\r\n*   Instance Type: AWS EC2 m4.xlarge\r\n*   CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\n**Batch Size: 1**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\r\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\r\n:              :             : (step time)  :               :               :\r\n| ------------ | ----------- | ------------ | ------------- | ------------- |\r\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |\r\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |\r\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |\r\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |\r\n\r\n**Batch Size: 32**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=inception3 --data_format=NCHW \\\r\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\r\n:              :             : (step time)   :               :               :\r\n| ------------ | ----------- | ------------- | ------------- | ------------- |\r\n| MKL          | NCHW        | 10.24         | 4             | 1             |\r\n:              :             : (3125ms)      :               :               :\r\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |\r\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |\r\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |\r\n\r\n#### Inference ResNet-50\r\n\r\n**Environment**\r\n\r\n*   Instance Type: AWS EC2 m4.xlarge\r\n*   CPU: Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz (Broadwell)\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\n**Batch Size: 1**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\r\n--batch_size=1 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec   | Intra threads | Inter Threads |\r\n:              :             : (step time)  :               :               :\r\n| ------------ | ----------- | ------------ | ------------- | ------------- |\r\n| AVX2         | NHWC        | 6.8 (147ms)  | 4             | 0             |\r\n| MKL          | NCHW        | 6.6 (151ms)  | 4             | 1             |\r\n| MKL          | NHWC        | 5.95 (168ms) | 4             | 1             |\r\n| AVX          | NHWC        | 4.7 (211ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.7 (370ms)  | 4             | 0             |\r\n\r\n**Batch Size: 32**\r\n\r\nCommand executed for the MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --forward_only=True --device=cpu --mkl=True \\\r\n--kmp_blocktime=0 --nodistortions --model=resnet50 --data_format=NCHW \\\r\n--batch_size=32 --num_inter_threads=1 --num_intra_threads=4 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\n| Optimization | Data Format | Images/Sec    | Intra threads | Inter Threads |\r\n:              :             : (step time)   :               :               :\r\n| ------------ | ----------- | ------------- | ------------- | ------------- |\r\n| MKL          | NCHW        | 10.24         | 4             | 1             |\r\n:              :             : (3125ms)      :               :               :\r\n| MKL          | NHWC        | 8.9 (3595ms)  | 4             | 1             |\r\n| AVX2         | NHWC        | 7.3 (4383ms)  | 4             | 0             |\r\n| AVX          | NHWC        | 5.1 (6275ms)  | 4             | 0             |\r\n| SSE3         | NHWC        | 2.8 (11428ms) | 4             | 0             |\r\n\r\n#### Training InceptionV3\r\n\r\n**Environment**\r\n\r\n*   Instance Type: Dedicated AWS EC2 r4.16xlarge (Broadwell)\r\n*   CPU: Intel Xeon E5-2686 v4 (Broadwell) Processors\r\n*   Dataset: ImageNet\r\n*   TensorFlow Version: 1.2.0 RC2\r\n*   Test Script: [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/blob/mkl_experiment/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)\r\n\r\nCommand executed for MKL test:\r\n\r\n```bash\r\npython tf_cnn_benchmarks.py --device=cpu --mkl=True --kmp_blocktime=0 \\\r\n--nodistortions --model=resnet50 --data_format=NCHW --batch_size=32 \\\r\n--num_inter_threads=2 --num_intra_threads=36 \\\r\n--data_dir=<path to ImageNet TFRecords>\r\n```\r\n\r\nOptimization | Data Format | Images/Sec | Intra threads | Inter Threads\r\n------------ | ----------- | ---------- | ------------- | -------------\r\nMKL          | NCHW        | 20.8       | 36            | 2\r\nAVX2         | NHWC        | 6.2        | 36            | 0\r\nAVX          | NHWC        | 5.7        | 36            | 0\r\nSSE3         | NHWC        | 4.3        | 36            | 0\r\n\r\nResNet and [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\r\nwere also run on this configuration but in an ad hoc manner. There were not\r\nenough runs executed to publish a coherent table of results. The incomplete\r\nresults strongly indicated the final result would be similar to the table above\r\nwith MKL providing significant 3x+ gains over AVX2.\r\n\r\n", "I have enabled MKL-DNN in the builds of the JavaCPP Presets for TensorFlow:\r\nhttps://github.com/bytedeco/javacpp-presets/tree/master/tensorflow\r\nWith those builds, it's possible to use both the official Java API and the C/C++ API from Java..."]}, {"number": 11231, "title": "[solved, solution is on the bottom] link libtensorflow-core.a to c++ cross comple project", "body": "Hi ~ all~\r\n\r\n   i followed:\r\n      https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/README.md\r\n  and successfully got ios, android, linux lib files in contrib\\makefile\\gen\\lib.\r\n\r\n  i'm developing a cross platform sdk lib which is written by c++.  my plan is using my sdk c++ code to call tensorflow lib. \r\n\r\n   here is my question: where are the h files according to the tensorflow lib file ? \r\n\r\n   commenly, a lib project's out put is not only a binary file , but also h files .\r\n", "comments": ["should i manually copy all h files in tensorflow/core  ?? \r\n ", "under linux , i copied libtensorflow-core.a to /usr/local/lib and tensorflow folder to /usr/local/include,\r\nthen i copied benchmark_model.cc and benchmark_model.h to my own project , \r\nand i -ltensorflow-core in my project, then run into a runtime issue:\r\n\r\n/root/61_ecg_sdk/src/mytensorflow/benchmark_model.cc:533: undefined reference to `tensorflow::TestReporter::TestReporter(std::string const&, std::string const&)'\r\n\r\n\r\npls tell me how to get complete header files according to the binary file of libtensorflow-core.a ?\r\n", "this link said the libtensorflow-core.a is incomplete..\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/7813", "\r\n\r\nis all header files of libtensorflow-core.a in tensorflow\\contrib\\makefile\\gen\\proto\\tensorflow ?", "i searched the compile trace log , found that only part of the objs are packed into libtensorflow-core.a.\r\n\r\ntensorflow/core/util/reporter.o is not packed to libtensorflow-core.a , which benchmark needed .", "found a link to test libtensorflow-core.a\r\nhttps://tebesu.github.io/posts/Training-a-TensorFlow-graph-in-C++-API", "this post helps me a lot:\r\nhttps://github.com/tensorflow/tensorflow/issues/3308", "there are really several important notations of tensorflow/contrib/makefile :\r\n\r\nthe header files of libtensorflow.a consists of generated h files in tensorflow/contrib/makefile/gen and in tensor source.\r\n\r\ntensor depends on protobuf to serial class objects to file. fortunely, protobuf lib file is also generated in gen folder.\r\n\r\nall platforms has different shell scripts to build tensor, but share the same Makefile , who is in tensorflow/contrib/makefile/ and determines which source file would be added to libtensorflow.a. \r\nDefaultly libtensorflow.a contains very few source files. we need change Makefile content to solve\r\nruntime issues. \r\n\r\nafter reading the Makefile carefully, we found that we just simply config tf_op_files.txt to add and delete source files in tensorflow/core/kernels.\r\n\r\n\r\n", "so when you have your own c++ project , \r\ni recommend you to compile tensor first, \r\nand then copy the entire tensorflow folder to your c++ project, \r\nin your c++ project , config -I    -L    -l  paths to point to the tensorflow folder.\r\n\r\ncompile errors always caused by -I -L path not found and easy to solve\r\nif there are runtime errors , modify  tensorflow/contrib/makefile/gen/Makefile to add sources to libtensorflow.a, then re-run your c++ project after  the new libtensorflow.a is built.\r\n\r\n\r\n", "Hi, @jakiechris \r\nthanks for your instructions. I did a Medium post about it to help others as well:\r\nhttps://medium.com/@arnaldog12/how-to-build-tensorflow-as-a-static-library-for-android-5c762dbdd5d4"]}, {"number": 11230, "title": "Update gradients_impl.py", "body": "Corrected documentation of hessians() function.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11229, "title": "32-bit build failure in tensorflow/contrib/tensor_forest/kernels/stats_ops.cc", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04.5 i386\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: Git revision 744120fd8a0c3be0a90cca5a971459894c90b859\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See log below\r\n\r\n### Describe the problem\r\nBuilding //tensorflow/contrib/tensor_forest:stats_ops_lib for a 32-bit architecture fails,\r\n\r\n### Source code / logs\r\ngit checkout 744120fd8a0c3be0a90cca5a971459894c90b859\r\nexport PYTHON_BIN_PATH=/usr/bin/python\r\nexport USE_DEFAULT_PYTHON_LIB_PATH=1\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\nexport TF_NEED_MKL=0\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_CUDA=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_MPI=0\r\nexport COMPUTE=:0\r\n./configure\r\nbazel build -c opt --verbose_failures //tensorflow/contrib/tensor_forest:stats_ops_lib\r\n\r\n(removed unimportant log output)\r\n\r\nERROR: /home/codeplay/tensorflow/tensorflow/contrib/tensor_forest/BUILD:277:1: C++ compilation of rule '//tensorflow/contrib/tensor_forest:stats_ops_lib' failed: gcc failed: error executing command \r\n  (cd /home/codeplay/.cache/bazel/_bazel_codeplay/552bd1ca300856cf615cf243a9219401/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/contrib/tensor_forest/_objs/stats_ops_lib/tensorflow/contrib/tensor_forest/kernels/stats_ops.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/contrib/tensor_forest/_objs/stats_ops_lib/tensorflow/contrib/tensor_forest/kernels/stats_ops.pic.o' -fPIC -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/contrib/tensor_forest/kernels/stats_ops.cc -o bazel-out/local-opt/bin/tensorflow/contrib/tensor_forest/_objs/stats_ops_lib/tensorflow/contrib/tensor_forest/kernels/stats_ops.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./tensorflow/contrib/tensor_forest/kernels/v4/leaf_model_operators.h:19:0,\r\n                 from ./tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resource.h:21,\r\n                 from tensorflow/contrib/tensor_forest/kernels/stats_ops.cc:18:\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h: In member function 'virtual float tensorflow::tensorforest::TensorInputTarget::GetTargetWeight(int) const':\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:70:47: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     return num_weights > 0 && example_index < num_weights\r\n                                               ^\r\nIn file included from ./tensorflow/contrib/tensor_forest/kernels/v4/split_collection_operators.h:20:0,\r\n                 from ./tensorflow/contrib/tensor_forest/kernels/v4/fertile-stats-resource.h:24,\r\n                 from tensorflow/contrib/tensor_forest/kernels/stats_ops.cc:19:\r\n./tensorflow/contrib/tensor_forest/kernels/v4/grow_stats.h: In member function 'bool tensorflow::tensorforest::GrowStats::IsInitialized() const':\r\n./tensorflow/contrib/tensor_forest/kernels/v4/grow_stats.h:80:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     return weight_sum_ > 0 || splits_.size() == num_splits_to_consider_;\r\n                                                 ^\r\ntensorflow/contrib/tensor_forest/kernels/stats_ops.cc: In function 'void tensorflow::tensorforest::UpdateStats(tensorflow::tensorforest::FertileStatsResource*, const std::unique_ptr<tensorflow::tensorforest::TensorDataSet>&, const tensorflow::Tensor&, const tensorflow::Tensor&, int, const std::vector<int>&, const std::vector<int>&, std::unordered_map<int, std::unique_ptr<tensorflow::mutex> >*, tensorflow::mutex*, tensorflow::int32, tensorflow::int32, std::unordered_set<int>*)':\r\ntensorflow/contrib/tensor_forest/kernels/stats_ops.cc:176:72: error: no matching function for call to 'tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 0, Eigen::MakePointer>*, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 0, Eigen::MakePointer>*, const tensorflow::Tensor&, int&)'\r\n   TensorInputTarget target(&labels, &weights, input_labels, num_targets);\r\n                                                                        ^\r\ntensorflow/contrib/tensor_forest/kernels/stats_ops.cc:176:72: note: candidates are:\r\nIn file included from ./tensorflow/contrib/tensor_forest/kernels/v4/leaf_model_operators.h:19:0,\r\n                 from ./tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resource.h:21,\r\n                 from tensorflow/contrib/tensor_forest/kernels/stats_ops.cc:18:\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:57:3: note: tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(const SingleDimStorageType*, const SingleDimStorageType*, const tensorflow::Tensor&, int)\r\n   TensorInputTarget(const SingleDimStorageType* t,\r\n   ^\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:57:3: note:   no known conversion for argument 1 from 'const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 0, Eigen::MakePointer>*' to 'const SingleDimStorageType* {aka const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 0>*}'\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note: tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(const tensorflow::tensorforest::TensorInputTarget&)\r\n class TensorInputTarget : public StoredInputTarget<SingleDimStorageType> {\r\n       ^\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note:   candidate expects 1 argument, 4 provided\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note: tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(tensorflow::tensorforest::TensorInputTarget&&)\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note:   candidate expects 1 argument, 4 provided\r\ntensorflow/contrib/tensor_forest/kernels/stats_ops.cc: In function 'void tensorflow::tensorforest::UpdateStatsCollated(tensorflow::tensorforest::FertileStatsResource*, tensorflow::tensorforest::DecisionTreeResource*, const std::unique_ptr<tensorflow::tensorforest::TensorDataSet>&, const tensorflow::Tensor&, const tensorflow::Tensor&, int, const std::unordered_map<int, std::vector<int> >&, const std::vector<int>&, tensorflow::mutex*, tensorflow::int32, tensorflow::int32, std::unordered_set<int>*)':\r\ntensorflow/contrib/tensor_forest/kernels/stats_ops.cc:226:72: error: no matching function for call to 'tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 0, Eigen::MakePointer>*, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 0, Eigen::MakePointer>*, const tensorflow::Tensor&, int&)'\r\n   TensorInputTarget target(&labels, &weights, input_labels, num_targets);\r\n                                                                        ^\r\ntensorflow/contrib/tensor_forest/kernels/stats_ops.cc:226:72: note: candidates are:\r\nIn file included from ./tensorflow/contrib/tensor_forest/kernels/v4/leaf_model_operators.h:19:0,\r\n                 from ./tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resource.h:21,\r\n                 from tensorflow/contrib/tensor_forest/kernels/stats_ops.cc:18:\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:57:3: note: tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(const SingleDimStorageType*, const SingleDimStorageType*, const tensorflow::Tensor&, int)\r\n   TensorInputTarget(const SingleDimStorageType* t,\r\n   ^\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:57:3: note:   no known conversion for argument 1 from 'const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 0, Eigen::MakePointer>*' to 'const SingleDimStorageType* {aka const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 0>*}'\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note: tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(const tensorflow::tensorforest::TensorInputTarget&)\r\n class TensorInputTarget : public StoredInputTarget<SingleDimStorageType> {\r\n       ^\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note:   candidate expects 1 argument, 4 provided\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note: tensorflow::tensorforest::TensorInputTarget::TensorInputTarget(tensorflow::tensorforest::TensorInputTarget&&)\r\n./tensorflow/contrib/tensor_forest/kernels/v4/input_target.h:55:7: note:   candidate expects 1 argument, 4 provided\r\nTarget //tensorflow/contrib/tensor_forest:stats_ops_lib failed to build", "comments": ["We do not really build and test on 32 bit desktop operating systems.\r\nPlease try to reach out to stackoverflow to see if someone from the community has already ran into and solved this problem."]}, {"number": 11228, "title": "GPU kernel for segment_sum?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7.12\r\n- **CUDA/cuDNN version**: 6.0\r\n- **GPU model and memory**: GTK1070/ 8105MB\r\n\r\n^^ I don't think system information is hugely relevant in this case, but writing them down anyways.\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThis is a feature request. I was running *tf.unsorted_segment_sum* on an unsorted list of segments, and thought that I might be able to get a performance gain if the segment ids are sorted (as that is usually the case).\r\nHowever, to my surprise, I discovered that there are no supported GPU kernels for vanilla segment_sum, forcing tensorflow to copy memory over to the CPU and thereby slowing down the operation greatly.\r\nWould it be possible to support an optimized GPU version of segment_sum that takes advantage of the fact that the segments are sorted?\r\n\r\n### Source code / logs\r\n\r\n(Device Placement)\r\nUnsortedSegmentSum: (UnsortedSegmentSum)/job:localhost/replica:0/task:0/gpu:0\r\nsoftmax/ops/SegmentSum: (SegmentSum)/job:localhost/replica:0/task:0/cpu:0\r\n\r\n(Profiling)\r\n[Unsorted Segment Sum] : Took 1.979 Seconds\r\n[Segment Sum] : Took 2.704 Seconds", "comments": ["I'm working on this one.", "@tjingrant How is the PR going?", "Hi, I got some feedback about a week ago but since then was caught up with some other things. I will likely be able to submit an update within 2 days. Sorry for the delay.", "It looks like this issue has been fixed in PR #11630. I will close this one. Thanks for the contribution!"]}, {"number": 11227, "title": "Configure script code reduction (no merge please)", "body": "Please don't merge yet.\r\n\r\nThe configure script has a lot of repetition. Adding functions can reduce the number of lines of code, help prevent errors, and make life easier.\r\n\r\nI noticed it was kind of long when editing another part.\r\n\r\nThis adds two functions: build_with and get_path\r\n```bash\r\nfunction build_with {\r\n  # don't change value if it is already set\r\n  if [[ \"$(eval echo \\\"\\$$1\\\")\" ]] ; then\r\n    return\r\n  fi\r\n  local needed=\"$1\"\r\n  local build_ask=\"$2\"\r\n  # default is 'enabled' or 'disabled'\r\n  local default=\"$3\"\r\n\r\n  if [[ \"$4\" ]] ; then\r\n    local build_ans_yes=\"$4\"\r\n    local build_ans_no=\"$5\"\r\n  else\r\n    local build_ans_yes=\"$build_ask support will be enabled for TensorFlow.\"\r\n    local build_ans_no=\"No $build_ans_yes\"\r\n    local build_ask=\"Do you wish to build TensorFlow with $build_ask support?\"\r\n  fi\r\n\r\n  if [[ \"$default\" == \"enabled\" ]] ; then\r\n    local build_ask=\"$build_ask [Y/n] \"\r\n  else\r\n    local build_ask=\"$build_ask [y/N] \"\r\n  fi\r\n\r\n  while true; do\r\n    read -p \"$build_ask\" user_input\r\n\r\n    case \"$user_input\" in\r\n      [Yy]* ) echo \"$build_ans_yes\"\r\n        eval $needed=1\r\n        break;;\r\n      [Nn]* ) echo \"$build_ans_no\"\r\n        eval $needed=0\r\n        break;;\r\n      \"\" ) if [[ \"$default\" == \"enabled\" ]] ; then\r\n          echo \"$build_ans_yes\"\r\n          eval $needed=1\r\n        else\r\n          echo \"$build_ans_no\"\r\n          eval $needed=0\r\n        fi\r\n        break;;\r\n      * ) echo \"Invalid selection: $user_input\";;\r\n    esac\r\n  done\r\n}\r\n```\r\n\r\n```bash\r\nfunction get_path {\r\n  if [[ \"$(eval echo \\\"\\$$1\\\")\" ]] ; then\r\n    return\r\n  fi\r\n  local sys_path=\"$1\"\r\n  local path_for=\"$2\"\r\n  # looking for a path for a bin or lib\r\n  local type_of_path=\"$3\"\r\n  # optional search for bin, array of paths for lib\r\n  local generic_path=\"$4\"\r\n  local default_path=\"\"\r\n\r\n  if [[ \"$type_of_path\" == \"bin\" ]] ; then\r\n    local default_path=\"$(which $path_for || true)\"\r\n    if [[ !\"$default_path\" ]] && [[ \"$generic_path\" ]] ; then\r\n      local default_path=\"$(which $generic_path || true)\"\r\n    fi\r\n    local ask_for_path=\"Please specify the location of $path_for. [Default is $default_path]:\"\r\n  else\r\n    echo \"Possible library path(s): \"\r\n    for x in \"${generic_path[@]}\" ; do\r\n      if [[ \"$x\" ]] && [[ ! \"$default_path\" ]] ; then\r\n        local default_path=\"$x\"\r\n      fi\r\n      echo \"    $x\"\r\n    done\r\n    local ask_for_path=\"Please input the desired $path_for library path to use. [Default is $default_path]:\"\r\n  fi\r\n\r\n  while true; do\r\n    read -p \"$ask_for_path \" $sys_path\r\n\r\n    if [[ ! \"$(eval echo \\$$sys_path)\" ]] ; then\r\n      if [[ -e \"$default_path\" ]] ; then\r\n        eval $sys_path=\"$default_path\"\r\n      elif [[ -e \"$generic_path\" ]] && [[ \"$type_of_path\" == \"bin\" ]] ; then\r\n        eval $sys_path=\"$default_path\"\r\n      else\r\n        echo \"ERROR! I cannot find a default path and one was not specified.\" 1>&2\r\n      fi\r\n    fi\r\n\r\n    if [[ -e \"$(eval echo \\$$sys_path)\" ]] ; then\r\n      if [[ \"$type_of_path\" == \"bin\" ]] && [[ ! -x \"$(eval echo \\$$sys_path)\" ]] ; then\r\n        echo \"ERROR! \\$$sys_path is not executable.\" 1>&2\r\n      else\r\n        break\r\n      fi\r\n    else\r\n      echo \"ERROR! $(eval echo \\$$sys_path) does not seem to be a valid path to $path_for.\" 1>&2\r\n    fi\r\n  done\r\n}\r\n```\r\n\r\nThose functions alone kill ~150 or so lines of code. I was going to do more reducing MKL, but I believe I rebased those changes away due to the discussion in  #11212\r\n\r\nIf possible, I'd like to run tests on this to see if there's any issue with cross compatibility. (I'm not sure what permissions are, and if I can run the tests myself?) I killed another 50 or so from the CUDA area, but I want to try it on a machine with CUDA before running it.\r\n\r\nThe selection below is just to see what versions you're supporting. TensorFlow is pretty new, and you're already using some more recent options in the script. I just wanted to test it to be sure.\r\n```bash\r\ndeclare -A array_test=( [one]=\"position_one\" [two]=\"position_two\" [three]=\"position_three\" )\r\narray_ans=\"\"\r\npositional_test=( one two three )\r\npositional_ans=\"position_oneposition_twoposition_three\"\r\n\r\nfor x in \"${positional_test[@]}\" ; do\r\n  array_ans+=\"$(eval echo \\${array_test[$x]})\"\r\ndone\r\n\r\nif [[ ! \"$array_ans\" == \"$positional_ans\" ]] ; then\r\n  echo \"dec arrays not supported\"\r\n  exit 1\r\nfi\r\n```\r\n\r\nIf that is successful, I can remove another 100 or so lines. It'll (probably) be more readable too, for instance you can just add something like:\r\n```bash\r\ndeclare -A GCP=( [tf_needs]=\"TF_NEED_GCP\" [name]=\"Google Cloud Platform\" [default]=\"disabled\" \\\r\n                                [write_opts]=\"write_gcp\" [enabled]=0 )\r\n```\r\nAnd have it iterate over fields instead of having an entire section of the script that has the same install process as TF_NEED_HDFS.", "comments": ["Can one of the admins verify this patch?", "This script is planned to be moved to python\r\n@yifeif How close are we to that?", "That makes a lot of sense.\r\n\r\nIf the Python script isn't far away, or even if the script is and this stuff will make it harder for you in the future, definitely close it. I did this because my build failed (locate not on system), and I just edited a few other things while I was looking at the file, and uploaded my changes. I figured I'd check to see if I should continue to do more on this file, or just leave it. If it's helpful, I can also fix it up a bit without spending too much time on additional things, but if not, feel free to kill it.", "@yifeif, any thoughts? Should we review this PR or close it in favor of the upcoming Python version of it?", "Thanks for the PR @hotchkiss87. Sorry I haven't gotten around to do the python conversion yet. If I still don't get to this next week, I'll review your PR.", "@yifeif I thought about it a bit, and it's probably a better use of our time if we worked on the python replacement instead. I have some work done (for other install scripts) via python, but it would unfortunately take a lot of work (a lot more work than a couple of quick changes to the current configure script) to implement for tesseract. The current configure scripts support a version of python that's much older than the scripts I run do.\r\n\r\nIf you're interested, I'd share any work done, but for me the most important part is what the earliest version of python you're targeting is.\r\n\r\n### What I feel uncomfortable committing to\r\nMy 'knee-jerk' reaction is typically just to do it all myself, but I have work, family, and some work for some local non-profits that I've already committed to for the next month, and that are depending on me to deliver. I hate being bored and commit to enough work so that there's no chance that this will happen, but I am quite committed to delivering for the non-profits (and the people that they serve) for the next month or so.\r\n\r\nI also hate inefficiency, and two people repeating the same work is painfully inefficient.\r\n\r\n### Thanks for your work!\r\nThanks for the project! I ended up going with my own SVM classifier for a project, but I'm sure I'll utilize this in the future.\r\n\r\n(oh no way I just checked git log and you work for Google? IDK if you'll want/need the partial python script(s) I have then... wow! tbh if you're really planning on just rewriting everything yourself, I can send you what I have. Worst case it's all useful and you... rewrite everything like you already planned to do, but hopefully you'll find something useful! I mostly just use a set of functions I've perfected over the years to handle this stuff for me. I'm younger than some of the old guys I have worked with, who did UNIX sysadmin work and the like in the 70s/80s, so I take a more modern approach, but some of the work I have may be useful! I'm more than happy to GPL/Apache it to you too. Should I email it in git or? I'd feel bad about posting non-mergeworthy code to github, but if you want a partial script, I can deliver.)"]}, {"number": 11226, "title": "Fix #11152: Adding import for pythons threading library", "body": "State in the `programmers_guide/threading_and_queues` example where the `threading.Thread` is coming from. This fixes #11152.\r\n\r\nThis is based on the comment by @ali01 in https://github.com/tensorflow/tensorflow/issues/11152#issuecomment-312393705.\r\n", "comments": ["Can one of the admins verify this patch?", "Note that we're moving to the interface in `contrib.dataset`.", "Jenkins, test this please.", "Ignoring unrelated `acosh` failure on windows."]}, {"number": 11225, "title": "Updating Java doc formatting issues", "body": "Updating Java doc formatting issues", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Ignoring flaky test (cosh)."]}, {"number": 11224, "title": "Update gradients_impl.py", "body": "Corrected documentation of hessians() function.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Sorry, this needs to be sent again `master`, not the release branch. "]}, {"number": 11223, "title": "tf.gather axis argument", "body": "It'd be nice to have an `axis` argument to `tf.gather`. This would bring it closer to the numpy equivalent, [np.take](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.take.html). \r\n\r\nThis would also pave the way to supporting Numpy [array indexing](https://docs.scipy.org/doc/numpy/user/basics.indexing.html#index-arrays), e.g. `t[:, [0, 2, 3], ::2]`.\r\n\r\nBased on discussion in #9236 and all the remedies provided on [this StackOverflow thread](https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension), there would be a lot of use for it.\r\n\r\nThe most common workaround of `tf.transpose(tf.gather(tf.transpose(...), ...), ...)` is super inefficient.\r\n ", "comments": ["There are 135 code references on GitHub for the query [tf.transpose(tf.gather(tf.transpose](https://github.com/search?utf8=%E2%9C%93&q=%22tf.transpose%28tf.gather%28tf.transpose%22&type=Code), though this includes a lot of repo clones (GitHub search is pretty mediocre).\r\n\r\nInternally at Google, there are 48 uses of the workaround.\r\n\r\nSee also: https://github.com/tensorflow/tensorflow/pull/9236#issuecomment-295812771\r\n\r\n", "Fixed in https://github.com/frankchn/tensorflow/commit/b1f9e2c89eb007cb4b9483d08dcace1e45e84164."]}, {"number": 11222, "title": "R1.2", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 11221, "title": "Using pd.Series instead of pd.Dataframe", "body": "Small change to use Series for x and y values for clarity and simplicity. Code executes equivalently afaict.", "comments": ["Can one of the admins verify this patch?", "as a matter of side-interest, I had made some changes to update my code compat with Estimators, but when I when to [merge it](https://github.com/tensorflow/tensorflow/pull/11220), it had already been updated lol.", "Jenkins, test this please.", "Sorry @drpngx I'm not too familiar with this code or with `pandas`. Maybe @roumposg would know? He made the last big change to the internal version of this.", "I am not familiar with Pandas in particular, but I did write this code and tested that it works.\r\n\r\nI cannot see what benefit this cl brings in. Yufeng, can you clarify?", "@yufengg any thoughts? Should I close this PR or leave it open for you to work it further?", "Looks like we can close this, since it doesn't have a clear benefit and we haven't heard from @yufengg.", "Oops sorry missed this -- There's a small perf benefit (since you're not constructing a DataSet and then making a Series out of it), but the main reason is for clarity, since it removes the mental step of \"Okay we make a Dataset, then select the index column 1 from it, which implicitly makes a Series\".\r\n\r\nInstead, we just make a Series explicitly. This makes the code more readable and consistent.", "LGTM, thanks for the explanation. But I don't think I have the power to approve your change. Perhaps the others can help.", "Can one of the admins verify this patch?", "@roumposg were you able to run this code and verify that it still works as intended?", "Yes, I ran it and verified it works as intended.\n\nOn Wed, Aug 2, 2017, 3:45 PM Neal Wu <notifications@github.com> wrote:\n\n> @roumposg <https://github.com/roumposg> were you able to run this code\n> and verify that it still works as intended?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11221#issuecomment-319819023>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG6Mj-AOPPBsWexyOP6hqIdmEOvoG7Zeks5sUPwHgaJpZM4OLk4H>\n> .\n>\n", "OK, great! Will approve. I checked out the two failing tests and they appear to be unrelated.", "@benoitsteiner could you help us merge?"]}, {"number": 11220, "title": "updated for Estimators and input_fn", "body": "", "comments": ["Can one of the admins verify this patch?", "made changes on my branch that it turns out were already done by the time I got around to merging... So it goes."]}, {"number": 11219, "title": "gridlstm", "body": "Hello,\r\n \r\n1- where can find an example for GRIDLSTM(2D) or multi_diagonal . i search in net but not found.\r\n2- known that tensorflow support from keras . how can insert a tensorflow layer in keras .\r\n\r\nTHANKS", "comments": ["I think it's the other way around; Keras may be implemented in TensorFlow. I haven't heard of TF in Keras yet. @jeansely ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11218, "title": "ValueError: Shape (1, 5) must have rank at least 3", "body": "CODE I AM TRYING TO RUN : \r\n\r\nfrom __future__ import print_function, division\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\nnum_epochs = 100\r\ntotal_series_length = 50000\r\ntruncated_backprop_length = 15\r\nstate_size = 4\r\nnum_classes = 2\r\necho_step = 3\r\nbatch_size = 5\r\nnum_batches = total_series_length//batch_size//truncated_backprop_length\r\n\r\ndef generateData():\r\n    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\r\n    y = np.roll(x, echo_step)\r\n    y[0:echo_step] = 0\r\n\r\n    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\r\n    y = y.reshape((batch_size, -1))\r\n\r\n    return (x, y)\r\n\r\nbatchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\r\nbatchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\r\n\r\ncell_state = tf.placeholder(tf.float32, [batch_size, state_size])\r\nhidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\r\ninit_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)\r\n\r\nW2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\r\nb2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\r\n\r\n# unstack columns\r\ninputs_series = tf.split(batchX_placeholder, truncated_backprop_length, 1)\r\nlabels_series = tf.unstack(batchY_placeholder, axis=1)\r\n\r\n# Forward passes\r\ncell = tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True)\r\nstates_series, current_state = tf.nn.dynamic_rnn(cell, inputs_series, initial_state = init_state)\r\n\r\nlogits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\r\npredictions_series = [tf.nn.softmax(logits) for logits in logits_series]\r\n\r\nlosses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels) for logits, labels in zip(logits_series,labels_series)]\r\ntotal_loss = tf.reduce_mean(losses)\r\n\r\ntrain_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\r\n\r\ndef plot(loss_list, predictions_series, batchX, batchY):\r\n    plt.subplot(2, 3, 1)\r\n    plt.cla()\r\n    plt.plot(loss_list)\r\n\r\n    for batch_series_idx in range(5):\r\n        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\r\n        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\r\n\r\n        plt.subplot(2, 3, batch_series_idx + 2)\r\n        plt.cla()\r\n        plt.axis([0, truncated_backprop_length, 0, 2])\r\n        left_offset = range(truncated_backprop_length)\r\n        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\r\n        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\r\n        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\r\n\r\n    plt.draw()\r\n    plt.pause(0.0001)\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.initialize_all_variables())\r\n    plt.ion()\r\n    plt.figure()\r\n    plt.show()\r\n    loss_list = []\r\n\r\n    for epoch_idx in range(num_epochs):\r\n        x,y = generateData()\r\n        _current_cell_state = np.zeros((batch_size, state_size))\r\n        _current_hidden_state = np.zeros((batch_size, state_size))\r\n\r\n        print(\"New data, epoch\", epoch_idx)\r\n\r\n        for batch_idx in range(num_batches):\r\n            start_idx = batch_idx * truncated_backprop_length\r\n            end_idx = start_idx + truncated_backprop_length\r\n\r\n            batchX = x[:,start_idx:end_idx]\r\n            batchY = y[:,start_idx:end_idx]\r\n\r\n            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\r\n                [total_loss, train_step, current_state, predictions_series],\r\n                feed_dict={\r\n                    batchX_placeholder: batchX,\r\n                    batchY_placeholder: batchY,\r\n                    cell_state: _current_cell_state,\r\n                    hidden_state: _current_hidden_state\r\n\r\n                })\r\n\r\n            _current_cell_state, _current_hidden_state = _current_state\r\n\r\n            loss_list.append(_total_loss)\r\n\r\n            if batch_idx%100 == 0:\r\n                print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\r\n                plot(loss_list, _predictions_series, batchX, batchY)\r\n\r\nplt.ioff()\r\nplt.show()\r\n\r\n\r\n\r\nBut I have the following error \r\nValueError: Shape (1, 5) must have rank at least 3\r\n", "comments": []}, {"number": 11217, "title": "tools.compatibility module is missing or not installed properly through pip", "body": "Running tf_upgrade.py gets ImportError:\r\n\r\n```\r\n$ python tf_upgrade.py \r\nTraceback (most recent call last):\r\n  File \"tf_upgrade.py\", line 23, in <module>\r\n    from tensorflow.tools.compatibility import ast_edits\r\nImportError: No module named compatibility\r\n\r\n```\r\nThere's no subdir named compatibility under tools in python packages dir. Those were taken from:\r\n\r\n```\r\n>>> import site; site.getsitepackages()\r\n['/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages'] \r\n```\r\n\r\n### System information\r\nUbuntu 16.04\r\nTensorflow 1.2.1, upgraded 'natively' via pip from a pre 1.0 version:\r\n$ sudo -H pip install --upgrade tensorflow-gpu\r\nPython version 2.7.12\r\n\r\n", "comments": ["Got exactly the same issue.", "A workaround is to replace the line `from tensorflow.tools.compatibility import ast_edits` with `import ast_edits` while making sure that the file [ast_edits.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/ast_edits.py) is in the same directory as the `tf_upgrade.py` script.", "Got the same issue, and the above workaround works.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@HumamAlwassel  Thanks. It works", "Any updates? Is it fixed now? I am also facing the same problem", "> A workaround is to replace the line `from tensorflow.tools.compatibility import ast_edits` with `import ast_edits` while making sure that the file [ast_edits.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/ast_edits.py) is in the same directory as the `tf_upgrade.py` script.\r\n\r\nThis line \"from tensorflow.tools.compatibility import ast_edits\" is in which file ?"]}, {"number": 11216, "title": "Add wget to be installed in docker images", "body": "same as #11215 but changes suggested for all docker files.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using scripts provided with tensorflow to download and build imagenet from scratch.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Nvidia GRID M6-8Q(8GB)\r\n- **Exact command to reproduce**: bazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n\r\n### Describe the problem\r\nI am trying to train inception v3 net for imagenet dataset using instructions at-\r\n[](https://github.com/tensorflow/models/tree/master/slim)\r\n[](https://github.com/tensorflow/models/blob/master/inception/README.md#getting-started)\r\n\r\nAfter setting the download path using following command - \r\n```\r\n# location of where to place the ImageNet data\r\nDATA_DIR=$HOME/imagenet-data\r\n```\r\nI ran the bazel command to build preprocessing script-\r\n```\r\n# build the preprocessing script.\r\ncd tensorflow-models/inception\r\nbazel build //inception:download_and_preprocess_imagenet\r\n```\r\nAnd then run it-\r\n```\r\n# run it\r\nbazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n```\r\nRunning above command fails saying it cannot find `wget` - \r\n\r\n> root@docker_container:/data/workspace/models/inception# bazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n> In order to download the imagenet data, you have to create an account with\r\n> image-net.org. This will get you a username and an access key. You can set the\r\n> IMAGENET_USERNAME and IMAGENET_ACCESS_KEY environment variables, or you can\r\n> enter the credentials here.\r\n> Username: <my username>\r\n> Access key: <my password>\r\n> Saving downloaded files to /data/imagenet-data/raw-data/\r\n> Downloading bounding box annotations.\r\n> bazel-bin/inception/download_and_preprocess_imagenet.runfiles/inception/inception/data/download_imagenet.sh: line 58: wget: command not found\r\n> bazel-bin/inception/download_and_preprocess_imagenet.runfiles/inception/inception/data/download_imagenet.sh: line 64: wget: command not found\r\n\r\nI have reported this as bug #11214 as well", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "It was me, nobody else!", "I used the same email address(or username) to commit!", "Please use #11215 instead."]}, {"number": 11215, "title": "Add 'wget' to be installed in docker image", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using scripts provided with tensorflow to download and build imagenet from scratch.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Nvidia GRID M6-8Q(8GB)\r\n- **Exact command to reproduce**: bazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n\r\n### Describe the problem\r\nI am trying to train inception v3 net for imagenet dataset using instructions at-\r\n[](https://github.com/tensorflow/models/tree/master/slim)\r\n[](https://github.com/tensorflow/models/blob/master/inception/README.md#getting-started)\r\n\r\nAfter setting the download path using following command - \r\n```\r\n# location of where to place the ImageNet data\r\nDATA_DIR=$HOME/imagenet-data\r\n```\r\nI ran the bazel command to build preprocessing script-\r\n```\r\n# build the preprocessing script.\r\ncd tensorflow-models/inception\r\nbazel build //inception:download_and_preprocess_imagenet\r\n```\r\nAnd then run it-\r\n```\r\n# run it\r\nbazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n```\r\nRunning above command fails saying it cannot find `wget` - \r\n\r\n> root@docker_container:/data/workspace/models/inception# bazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n> In order to download the imagenet data, you have to create an account with\r\n> image-net.org. This will get you a username and an access key. You can set the\r\n> IMAGENET_USERNAME and IMAGENET_ACCESS_KEY environment variables, or you can\r\n> enter the credentials here.\r\n> Username: <my username>\r\n> Access key: <my password>\r\n> Saving downloaded files to /data/imagenet-data/raw-data/\r\n> Downloading bounding box annotations.\r\n> bazel-bin/inception/download_and_preprocess_imagenet.runfiles/inception/inception/data/download_imagenet.sh: line 58: wget: command not found\r\n> bazel-bin/inception/download_and_preprocess_imagenet.runfiles/inception/inception/data/download_imagenet.sh: line 64: wget: command not found\r\n\r\nI have reported this as bug #11214 as well", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Ignoring flaky test."]}, {"number": 11214, "title": "Downloading and building imagenet from scratch says \"wget: command not found\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using scripts provided with tensorflow to download and build imagenet from scratch.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Nvidia GRID M6-8Q(8GB)\r\n- **Exact command to reproduce**: bazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n\r\n### Describe the problem\r\nI am trying to train inception v3 net for imagenet dataset using instructions at-\r\n[](https://github.com/tensorflow/models/tree/master/slim)\r\n[](https://github.com/tensorflow/models/blob/master/inception/README.md#getting-started)\r\n\r\nAfter setting the download path using following command - \r\n```\r\n# location of where to place the ImageNet data\r\nDATA_DIR=$HOME/imagenet-data\r\n```\r\nI ran the bazel command to build preprocessing script-\r\n```\r\n# build the preprocessing script.\r\ncd tensorflow-models/inception\r\nbazel build //inception:download_and_preprocess_imagenet\r\n```\r\nAnd then run it-\r\n```\r\n# run it\r\nbazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n```\r\nRunning above command fails saying it cannot find `wget` - \r\n\r\n> root@docker_container:/data/workspace/models/inception# bazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\r\n> In order to download the imagenet data, you have to create an account with\r\n> image-net.org. This will get you a username and an access key. You can set the\r\n> IMAGENET_USERNAME and IMAGENET_ACCESS_KEY environment variables, or you can\r\n> enter the credentials here.\r\n> Username: <my username>\r\n> Access key: <my password>\r\n> Saving downloaded files to /data/imagenet-data/raw-data/\r\n> Downloading bounding box annotations.\r\n> bazel-bin/inception/download_and_preprocess_imagenet.runfiles/inception/inception/data/download_imagenet.sh: line 58: wget: command not found\r\n> bazel-bin/inception/download_and_preprocess_imagenet.runfiles/inception/inception/data/download_imagenet.sh: line 64: wget: command not found\r\n\r\n", "comments": ["@pankajkgupta Does the recent commit fix the problem for you, and if so, can we close?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11213, "title": "Disable flaky cwise_ops_test due to broken acosh on windows.", "body": "", "comments": ["I think this started  6/28 and I see a few other tests failing on windows around the same time. \r\nCan take a look if we can fix the root cause.", "Thanks! The cosh was introduced this week. I can find the pointer tomorrow\n\nOn Jul 2, 2017 10:37 PM, \"Guenther Schmuelling\" <notifications@github.com>\nwrote:\n\nI think this started 6/28 and I see a few other tests failing on windows\naround the same time.\nCan take a look if we can fix the root cause.\n\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/11213#issuecomment-312553315>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbfaV0kSHynhvbdlP3V2SgxpaVRYOks5sKH4UgaJpZM4OLf-W>\n.\n", "I have root caused the problem to https://github.com/tensorflow/tensorflow/pull/10598\r\nHowever, as this was blocking most submissions, I have elected to disable the test.\r\nI am happy to quickly reenable it once we have a fix for acosh.\r\nAlso, @guschmue any input on the discussion about what maybe wrong with acosh is welcome!", "Ah, makes perfect sense."]}, {"number": 11212, "title": "Tidy up MKL configuration.", "body": "Also only use \"--config=mkl\" to build with MKL support.", "comments": ["@tfboyd Could you get reviewers from Intel to look at this PR? Also, if we have any documentation for mkl, it will need to be updated..\r\n\r\n@allenlavoie @jart Looks like my bazel configs were correct, but build_pip_package script had some hardcoded references to mkl libraries.", "@tfboyd, this PR blocks my further work on having MKL on windows and macos.\r\nHowever, this PR should be sufficient to build TF with both MKL support and cuda.", "windows cmake tests have an issue, but they are known issues due to a bug in `tf.acosh`", "/CC: @rmlarsen ", "One reason we had added MKL download in the config file was to enable the user to be able to point to  a local copy instead of downloading it. Is it possible to accomplish the same if the download is  through bazel? \r\n\r\nOther than this the changes look fine.", "@jart, how can we enable users to use their own copies of MKL with this setup?\r\n\r\n@agramesh1 Can you provide input on the license question @drpngx added on the mkl.BUILD file?", "```\r\nFAIL: mismatch in packaged licenses and external dependencies\r\nMissing the licenses for the following external dependencies:\r\n//third_party/mkl\r\n@mkl//\r\n```", "(Removing \"awaiting testing then merge\", since @jart might have some input)", "Resolved license issue. \r\nI will tidy things up further when adding macOS support by removing third_party/mkl directory.\r\n\r\n@agramesh1 looks like as @drpngx pointed out the license in MKL girhub repository is apache.\r\nYou might want to update the license there.", "@jart do you have any comments on BUILD files.", "Ping! Any objections to this PR?", "@gunan  I will get MKL team to update license on github repository.  Regarding the PR,  we want be sure there is a way for users to use their own copy of MKL. Awaiting response from @jart ", "Current PR removes straightforward support for working with your local copy of MKL.\r\nIf that is a vital feature you need, how about I add that with a subsequent PR?", "real_path() addition is sort of irrelevant to this change, but when experimenting I discovered a bug in build pip package. If I ran build_pip_package script with a relative path, the whl file just got lost. I wanted to fix that in this PR.\r\nWould you like me to move it to a new PR?", "I have prepared the local mkl configuration here:  https://github.com/gunan/tensorflow/tree/local_mkl\r\nOnce this PR is merged, I will create a new pull request.\r\n@agramesh1 the model I have will be this:\r\n-configure script won't ask anything to the users about MKL.\r\n-using `--config=mkl` will be sufficient to build a pip package with MKL support.\r\n-If you use the environment variable `TF_MKL_ROOT`, `--config=mkl` will use that mkl instead of downloading it from the internet.\r\nDoes the above sound good to you?", "@gunan Yes, that will work. Thanks.", "As discussed with @gunan, will submit internally to make this easier to merge. Closing."]}, {"number": 11211, "title": "Fix broken link for reader_base.proto", "body": "This fix fixes broken links for reader_base.proto in version_compat.md and version_semantics.md:\r\n\r\n`tensorflow/core/kernels/reader_base.proto -> tensorflow/core/framework/reader_base.proto`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "cc @MarkDaoust "]}, {"number": 11210, "title": "Tensorflow execution with HVX failed on Qualcomm 820 Board", "body": "@satok16 could you have a look at this error? \r\n\r\nI have built the libs and am using Intrinsys 820 Qualcomm board. Tried your troubleshooting procedure to sign as well and put the .so file in `/system/lib/rfsa/adsp/`. Running hexagon graph gives me the same error : \r\n\r\n```\r\nWARNING: linker: Warning: unable to normalize \"\"\r\nRunning main() from test_main.cc\r\nNote: Google Test filter = GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from GraphTransferer\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:445 Fuse and run inception v3 on hexagon with tf runtime\r\ntensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:72: Failure\r\nExpected: (version) >= (1), actual: 0 vs 1\r\nnative : hexagon_graph_execution_test.cc:123 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:129 header size = 54\r\nnative : hexagon_graph_execution_test.cc:131 image size = 40\r\nnative : hexagon_graph_execution_test.cc:133 width = 299\r\nnative : hexagon_graph_execution_test.cc:135 height = -299\r\nnative : hexagon_graph_execution_test.cc:457 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:464 Build fused graph\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : hexagon_graph_execution_test.cc:123 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:129 header size = 54\r\nnative : hexagon_graph_execution_test.cc:131 image size = 40\r\nnative : hexagon_graph_execution_test.cc:133 width = 299\r\nnative : hexagon_graph_execution_test.cc:135 height = -299\r\nnative : hexagon_graph_execution_test.cc:263 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:171 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:175 Copy data to tensor.\r\nnative : hexagon_graph_execution_test.cc:284 Run graph\r\nInit hexagon with max attributes (Controller version = 92)\r\nFailed to disable DSP DCVS: ffffffff\r\n\r\nFailed to append const node 65538\r\nFailed to append const node 65538\r\nFailed to append const node 65539\r\nFailed to append const node 65539\r\n\r\n```\r\n`...\r\n`\r\n```\r\nFailed to append const node 66640\r\nnative : hexagon_control_wrapper.cc:252 Setup graph completed\r\nPrepare failed! returned 0xffffffff\r\n\r\nDUMP HEXAGON LOG: \r\nExecute graph!\r\nExecution failed!\r\nexecute got err: -1\r\n\r\nExecution failed\r\nnative : hexagon_control_wrapper.cc:312 Check failed: output_tensor->TotalBytes() >= std::get<1>(output) 4032, 2152910848\r\nAborted \r\n\r\n```\r\nI have attached my adblogcat output as well here. Thanks.\r\n[logcat.txt](https://github.com/tensorflow/tensorflow/files/1117450/logcat.txt)\r\n\r\n", "comments": ["\"Expected: (version) >= (1), actual: 0 vs 1\" indicates that the signature of HVX binary is wrong.  Did you try troubleshooting in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx ?\r\n", "Thanks @satok16 for the quick reply. Yes. I did use elfsign tool in Qualcomm sdk and generated the .so. also rebooted the device and tested it again yet the same error generated.", "Can you put all command lines and several lines after that?", "Sorry for the delay @satok16. Here is the output of the testsig file in my 820 board: \r\n```\r\nadb shell ls /system/lib/rfsa/adsp/testsig*\r\n/system/lib/rfsa/adsp/testsig-0x***.so\r\n```\r\nthe error as you mentioned comes from : \r\n```\r\ntensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:72: Failure\r\nExpected: (version) >= (1), actual: 0 vs 1\r\n```\r\nwhich is from this Tensorflow wrapper: \r\n\r\n```\r\nstatic void CheckHexagonControllerVersion() {\r\n  HexagonControlWrapper hexagon_control_wrapper;\r\n  const int version = hexagon_control_wrapper.GetVersion();\r\n  ASSERT_GE(version, 1);\r\n  LOG(INFO) << \"Hexagon controller version is \" << version;\r\n}\r\n```\r\nCould you let me know what version mismatch I got here ?\r\n\r\n**I am using the prebuilt library with -p argument.**\r\n\r\nHere is the readelf of my **hexagon_graph_execution** file:\r\n```\r\nELF Header:\r\n  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 \r\n  Class:                             ELF32\r\n  Data:                              2's complement, little endian\r\n  Version:                           1 (current)\r\n  OS/ABI:                            UNIX - System V\r\n  ABI Version:                       0\r\n  Type:                              DYN (Shared object file)\r\n  Machine:                           ARM\r\n  Version:                           0x1\r\n  Entry point address:               0xc8c9c\r\n  Start of program headers:          52 (bytes into file)\r\n  Start of section headers:          39944896 (bytes into file)\r\n  Flags:                             0x5000000, Version5 EABI\r\n  Size of this header:               52 (bytes)\r\n  Size of program headers:           32 (bytes)\r\n  Number of program headers:         8\r\n  Size of section headers:           40 (bytes)\r\n  Number of section headers:         36\r\n  Section header string table index: 35\r\n\r\n```\r\n\r\nreadelf of **libhexagon_controller.so**: \r\n```\r\n\r\nELF Header:\r\n  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 \r\n  Class:                             ELF32\r\n  Data:                              2's complement, little endian\r\n  Version:                           1 (current)\r\n  OS/ABI:                            UNIX - System V\r\n  ABI Version:                       0\r\n  Type:                              DYN (Shared object file)\r\n  Machine:                           ARM\r\n  Version:                           0x1\r\n  Entry point address:               0x0\r\n  Start of program headers:          52 (bytes into file)\r\n  Start of section headers:          31664 (bytes into file)\r\n  Flags:                             0x5000000, Version5 EABI\r\n  Size of this header:               52 (bytes)\r\n  Size of program headers:           32 (bytes)\r\n  Number of program headers:         7\r\n  Size of section headers:           40 (bytes)\r\n  Number of section headers:         28\r\n  Section header string table index: 27\r\n```\r\n\r\nand also readelf of **libhexagon_nn_skel.so**: \r\n```\r\n\r\nELF Header:\r\n  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 \r\n  Class:                             ELF32\r\n  Data:                              2's complement, little endian\r\n  Version:                           1 (current)\r\n  OS/ABI:                            UNIX - System V\r\n  ABI Version:                       0\r\n  Type:                              DYN (Shared object file)\r\n  Machine:                           QUALCOMM DSP6 Processor\r\n  Version:                           0x1\r\n  Entry point address:               0x0\r\n  Start of program headers:          52 (bytes into file)\r\n  Start of section headers:          215200 (bytes into file)\r\n  Flags:                             0x60\r\n  Size of this header:               52 (bytes)\r\n  Size of program headers:           32 (bytes)\r\n  Number of program headers:         3\r\n  Size of section headers:           40 (bytes)\r\n  Number of section headers:         24\r\n  Section header string table index: 21\r\n```\r\n\r\nP.s: here is the full command line output:\r\n\r\n```\r\nWARNING: linker: Warning: unable to normalize \"\"\r\nRunning main() from test_main.cc\r\nNote: Google Test filter = GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\n[==========] Running 1 test from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 1 test from GraphTransferer\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:445 Fuse and run inception v3 on hexagon with tf runtime\r\ntensorflow/core/kernels/hexagon/hexagon_graph_execution_test.cc:72: Failure\r\nExpected: (version) >= (1), actual: 0 vs 1\r\nnative : hexagon_graph_execution_test.cc:123 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:129 header size = 54\r\nnative : hexagon_graph_execution_test.cc:131 image size = 40\r\nnative : hexagon_graph_execution_test.cc:133 width = 299\r\nnative : hexagon_graph_execution_test.cc:135 height = -299\r\nnative : hexagon_graph_execution_test.cc:457 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:464 Build fused graph\r\ncan't determine number of CPU cores: assuming 4\r\ncan't determine number of CPU cores: assuming 4\r\nnative : hexagon_graph_execution_test.cc:123 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:129 header size = 54\r\nnative : hexagon_graph_execution_test.cc:131 image size = 40\r\nnative : hexagon_graph_execution_test.cc:133 width = 299\r\nnative : hexagon_graph_execution_test.cc:135 height = -299\r\nnative : hexagon_graph_execution_test.cc:263 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:171 Ioading image finished.\r\nnative : hexagon_graph_execution_test.cc:175 Copy data to tensor.\r\nnative : hexagon_graph_execution_test.cc:284 Run graph\r\nInit hexagon with max attributes (Controller version = 92)\r\nFailed to disable DSP DCVS: ffffffff\r\n\r\nFailed to append const node 65538\r\nFailed to append const node 65538\r\nFailed to append const node 65539\r\nFailed to append const node 65539\r\nFailed to append const node 65540\r\nFailed to append const node 65540\r\nFailed to append const node 65541\r\nFailed to append const node 65541\r\nFailed to append const node 65542\r\nFailed to append const node 65542\r\nFailed to append const node 65543\r\nFailed to append const node 65543\r\nFailed to append const node 65544\r\nFailed to append const node 65544\r\nFailed to append const node 65545\r\nFailed to append const node 65545\r\nFailed to append const node 65546\r\nFailed to append const node 65546\r\nFailed to append const node 65547\r\nFailed to append const node 65547\r\nFailed to append const node 65548\r\nFailed to append const node 65548\r\nFailed to append const node 65549\r\nFailed to append const node 65549\r\nFailed to append const node 65550\r\nFailed to append const node 65550\r\nFailed to append const node 65551\r\nFailed to append const node 65551\r\nFailed to append const node 65552\r\nFailed to append const node 65552\r\nFailed to append const node 65553\r\nFailed to append const node 65553\r\nFailed to append const node 65554\r\nFailed to append const node 65554\r\nFailed to append const node 65555\r\nFailed to append const node 65555\r\nFailed to append const node 65556\r\nFailed to append const node 65556\r\nFailed to append const node 65557\r\nFailed to append const node 65557\r\nFailed to append const node 65558\r\nFailed to append const node 65558\r\nFailed to append const node 65559\r\nFailed to append const node 65559\r\nFailed to append const node 65560\r\nFailed to append const node 65560\r\nFailed to append const node 65561\r\nFailed to append const node 65561\r\nFailed to append const node 65562\r\nFailed to append const node 65562\r\nFailed to append const node 65563\r\nFailed to append const node 65563\r\nFailed to append const node 65564\r\nFailed to append const node 65564\r\nFailed to append const node 65565\r\nFailed to append const node 65565\r\nFailed to append const node 65566\r\nFailed to append const node 65566\r\nFailed to append const node 65567\r\nFailed to append const node 65567\r\nFailed to append const node 65568\r\nFailed to append const node 65568\r\nFailed to append const node 65569\r\nFailed to append const node 65569\r\nFailed to append const node 65570\r\nFailed to append const node 65570\r\nFailed to append const node 65571\r\nFailed to append const node 65571\r\nFailed to append const node 65572\r\nFailed to append const node 65572\r\nFailed to append const node 65573\r\nFailed to append const node 65573\r\nFailed to append const node 65574\r\nFailed to append const node 65574\r\nFailed to append const node 65575\r\nFailed to append const node 65575\r\nFailed to append const node 65576\r\nFailed to append const node 65576\r\nFailed to append const node 65577\r\nFailed to append const node 65577\r\nFailed to append const node 65578\r\nFailed to append const node 65578\r\nFailed to append const node 65579\r\nFailed to append const node 65579\r\nFailed to append const node 65580\r\nFailed to append const node 65580\r\nFailed to append const node 65581\r\nFailed to append const node 65581\r\nFailed to append const node 65582\r\nFailed to append const node 65582\r\nFailed to append const node 65583\r\nFailed to append const node 65583\r\nFailed to append const node 65584\r\nFailed to append const node 65584\r\nFailed to append const node 65585\r\nFailed to append const node 65585\r\nFailed to append const node 65586\r\nFailed to append const node 65586\r\nFailed to append const node 65587\r\nFailed to append const node 65587\r\nFailed to append const node 65588\r\nFailed to append const node 65588\r\nFailed to append const node 65589\r\nFailed to append const node 65589\r\nFailed to append const node 65590\r\nFailed to append const node 65590\r\nFailed to append const node 65591\r\nFailed to append const node 65591\r\nFailed to append const node 65592\r\nFailed to append const node 65592\r\nFailed to append const node 65593\r\nFailed to append const node 65593\r\nFailed to append const node 65594\r\nFailed to append const node 65594\r\nFailed to append const node 65595\r\nFailed to append const node 65595\r\nFailed to append const node 65596\r\nFailed to append const node 65596\r\nFailed to append const node 65597\r\nFailed to append const node 65597\r\nFailed to append const node 65598\r\nFailed to append const node 65598\r\nFailed to append const node 65599\r\nFailed to append const node 65599\r\nFailed to append const node 65600\r\nFailed to append const node 65600\r\nFailed to append const node 65601\r\nFailed to append const node 65601\r\nFailed to append const node 65602\r\nFailed to append const node 65602\r\nFailed to append const node 65603\r\nFailed to append const node 65603\r\nFailed to append const node 65604\r\nFailed to append const node 65604\r\nFailed to append const node 65605\r\nFailed to append const node 65605\r\nFailed to append const node 65606\r\nFailed to append const node 65606\r\nFailed to append const node 65607\r\nFailed to append const node 65607\r\nFailed to append const node 65608\r\nFailed to append const node 65608\r\nFailed to append const node 65609\r\nFailed to append const node 65609\r\nFailed to append const node 65610\r\nFailed to append const node 65610\r\nFailed to append const node 65611\r\nFailed to append const node 65611\r\nFailed to append const node 65612\r\nFailed to append const node 65612\r\nFailed to append const node 65613\r\nFailed to append const node 65613\r\nFailed to append const node 65614\r\nFailed to append const node 65614\r\nFailed to append const node 65615\r\nFailed to append const node 65615\r\nFailed to append const node 65616\r\nFailed to append const node 65616\r\nFailed to append const node 65617\r\nFailed to append const node 65617\r\nFailed to append const node 65618\r\nFailed to append const node 65618\r\nFailed to append const node 65619\r\nFailed to append const node 65619\r\nFailed to append const node 65620\r\nFailed to append const node 65620\r\nFailed to append const node 65621\r\nFailed to append const node 65621\r\nFailed to append const node 65622\r\nFailed to append const node 65622\r\nFailed to append const node 65623\r\nFailed to append const node 65623\r\nFailed to append const node 65624\r\nFailed to append const node 65624\r\nFailed to append const node 65625\r\nFailed to append const node 65625\r\nFailed to append const node 65626\r\nFailed to append const node 65626\r\nFailed to append const node 65627\r\nFailed to append const node 65627\r\nFailed to append const node 65628\r\nFailed to append const node 65628\r\nFailed to append const node 65629\r\nFailed to append const node 65629\r\nFailed to append const node 65630\r\nFailed to append const node 65630\r\nFailed to append const node 65631\r\nFailed to append const node 65631\r\nFailed to append const node 65632\r\nFailed to append const node 65632\r\nFailed to append const node 65633\r\nFailed to append const node 65633\r\nFailed to append const node 65634\r\nFailed to append const node 65634\r\nFailed to append const node 65635\r\nFailed to append const node 65635\r\nFailed to append const node 65636\r\nFailed to append const node 65636\r\nFailed to append const node 65637\r\nFailed to append const node 65637\r\nFailed to append const node 65638\r\nFailed to append const node 65638\r\nFailed to append const node 65639\r\nFailed to append const node 65639\r\nFailed to append const node 65640\r\nFailed to append const node 65640\r\nFailed to append const node 65641\r\nFailed to append const node 65641\r\nFailed to append const node 65642\r\nFailed to append const node 65642\r\nFailed to append const node 65643\r\nFailed to append const node 65643\r\nFailed to append const node 65644\r\nFailed to append const node 65644\r\nFailed to append const node 65645\r\nFailed to append const node 65645\r\nFailed to append const node 65646\r\nFailed to append const node 65646\r\nFailed to append const node 65647\r\nFailed to append const node 65647\r\nFailed to append const node 65648\r\nFailed to append const node 65648\r\nFailed to append const node 65649\r\nFailed to append const node 65649\r\nFailed to append const node 65650\r\nFailed to append const node 65650\r\nFailed to append const node 65651\r\nFailed to append const node 65651\r\nFailed to append const node 65652\r\nFailed to append const node 65652\r\nFailed to append const node 65653\r\nFailed to append const node 65653\r\nFailed to append const node 65654\r\nFailed to append const node 65654\r\nFailed to append const node 65655\r\nFailed to append const node 65655\r\nFailed to append const node 65656\r\nFailed to append const node 65656\r\nFailed to append const node 65657\r\nFailed to append const node 65657\r\nFailed to append const node 65658\r\nFailed to append const node 65658\r\nFailed to append const node 65659\r\nFailed to append const node 65659\r\nFailed to append const node 65660\r\nFailed to append const node 65660\r\nFailed to append const node 65661\r\nFailed to append const node 65661\r\nFailed to append const node 65662\r\nFailed to append const node 65662\r\nFailed to append const node 65663\r\nFailed to append const node 65663\r\nFailed to append const node 65664\r\nFailed to append const node 65664\r\nFailed to append const node 65665\r\nFailed to append const node 65665\r\nFailed to append const node 65666\r\nFailed to append const node 65666\r\nFailed to append const node 65667\r\nFailed to append const node 65667\r\nFailed to append const node 65668\r\nFailed to append const node 65668\r\nFailed to append const node 65669\r\nFailed to append const node 65669\r\nFailed to append const node 65670\r\nFailed to append const node 65670\r\nFailed to append const node 65671\r\nFailed to append const node 65671\r\nFailed to append const node 65672\r\nFailed to append const node 65672\r\nFailed to append const node 65673\r\nFailed to append const node 65673\r\nFailed to append const node 65674\r\nFailed to append const node 65674\r\nFailed to append const node 65675\r\nFailed to append const node 65675\r\nFailed to append const node 65676\r\nFailed to append const node 65676\r\nFailed to append const node 65677\r\nFailed to append const node 65677\r\nFailed to append const node 65678\r\nFailed to append const node 65678\r\nFailed to append const node 65679\r\nFailed to append const node 65679\r\nFailed to append const node 65680\r\nFailed to append const node 65680\r\nFailed to append const node 65681\r\nFailed to append const node 65681\r\nFailed to append const node 65682\r\nFailed to append const node 65682\r\nFailed to append const node 65683\r\nFailed to append const node 65683\r\nFailed to append const node 65684\r\nFailed to append const node 65684\r\nFailed to append const node 65685\r\nFailed to append const node 65685\r\nFailed to append const node 65686\r\nFailed to append const node 65686\r\nFailed to append const node 65687\r\nFailed to append const node 65687\r\nFailed to append const node 65688\r\nFailed to append const node 65688\r\nFailed to append const node 65689\r\nFailed to append const node 65689\r\nFailed to append const node 65690\r\nFailed to append const node 65690\r\nFailed to append const node 65691\r\nFailed to append const node 65691\r\nFailed to append const node 65692\r\nFailed to append const node 65692\r\nFailed to append const node 65693\r\nFailed to append const node 65693\r\nFailed to append const node 65694\r\nFailed to append const node 65694\r\nFailed to append const node 65695\r\nFailed to append const node 65695\r\nFailed to append const node 65696\r\nFailed to append const node 65696\r\nFailed to append const node 65697\r\nFailed to append const node 65697\r\nFailed to append const node 65698\r\nFailed to append const node 65698\r\nFailed to append const node 65699\r\nFailed to append const node 65699\r\nFailed to append const node 65700\r\nFailed to append const node 65700\r\nFailed to append const node 65701\r\nFailed to append const node 65701\r\nFailed to append const node 65702\r\nFailed to append const node 65702\r\nFailed to append const node 65703\r\nFailed to append const node 65703\r\nFailed to append const node 65704\r\nFailed to append const node 65704\r\nFailed to append const node 65705\r\nFailed to append const node 65705\r\nFailed to append const node 65706\r\nFailed to append const node 65706\r\nFailed to append const node 65707\r\nFailed to append const node 65707\r\nFailed to append const node 65708\r\nFailed to append const node 65708\r\nFailed to append const node 65709\r\nFailed to append const node 65709\r\nFailed to append const node 65710\r\nFailed to append const node 65710\r\nFailed to append const node 65711\r\nFailed to append const node 65711\r\nFailed to append const node 65712\r\nFailed to append const node 65712\r\nFailed to append const node 65713\r\nFailed to append const node 65713\r\nFailed to append const node 65714\r\nFailed to append const node 65714\r\nFailed to append const node 65715\r\nFailed to append const node 65715\r\nFailed to append const node 65716\r\nFailed to append const node 65716\r\nFailed to append const node 65717\r\nFailed to append const node 65717\r\nFailed to append const node 65718\r\nFailed to append const node 65718\r\nFailed to append const node 65719\r\nFailed to append const node 65719\r\nFailed to append const node 65720\r\nFailed to append const node 65720\r\nFailed to append const node 65721\r\nFailed to append const node 65721\r\nFailed to append const node 65722\r\nFailed to append const node 65722\r\nFailed to append const node 65723\r\nFailed to append const node 65723\r\nFailed to append const node 65724\r\nFailed to append const node 65724\r\nFailed to append const node 65725\r\nFailed to append const node 65725\r\nFailed to append const node 65726\r\nFailed to append const node 65726\r\nFailed to append const node 65727\r\nFailed to append const node 65727\r\nFailed to append const node 65728\r\nFailed to append const node 65728\r\nFailed to append const node 65729\r\nFailed to append const node 65729\r\nFailed to append const node 65730\r\nFailed to append const node 65730\r\nFailed to append const node 65731\r\nFailed to append const node 65731\r\nFailed to append const node 65732\r\nFailed to append const node 65732\r\nFailed to append const node 65733\r\nFailed to append const node 65733\r\nFailed to append const node 65734\r\nFailed to append const node 65734\r\nFailed to append const node 65735\r\nFailed to append const node 65735\r\nFailed to append const node 65736\r\nFailed to append const node 65736\r\nFailed to append const node 65737\r\nFailed to append const node 65737\r\nFailed to append const node 65738\r\nFailed to append const node 65738\r\nFailed to append const node 65739\r\nFailed to append const node 65739\r\nFailed to append const node 65740\r\nFailed to append const node 65740\r\nFailed to append const node 65741\r\nFailed to append const node 65741\r\nFailed to append const node 65742\r\nFailed to append const node 65742\r\nFailed to append const node 65743\r\nFailed to append const node 65743\r\nFailed to append const node 65744\r\nFailed to append const node 65744\r\nFailed to append const node 65745\r\nFailed to append const node 65745\r\nFailed to append const node 65746\r\nFailed to append const node 65746\r\nFailed to append const node 65747\r\nFailed to append const node 65747\r\nFailed to append const node 65748\r\nFailed to append const node 65748\r\nFailed to append const node 65749\r\nFailed to append const node 65749\r\nFailed to append const node 65750\r\nFailed to append const node 65750\r\nFailed to append const node 65751\r\nFailed to append const node 65751\r\nFailed to append const node 65752\r\nFailed to append const node 65752\r\nFailed to append const node 65753\r\nFailed to append const node 65753\r\nFailed to append const node 65754\r\nFailed to append const node 65754\r\nFailed to append const node 65755\r\nFailed to append const node 65755\r\nFailed to append const node 65756\r\nFailed to append const node 65756\r\nFailed to append const node 65757\r\nFailed to append const node 65757\r\nFailed to append const node 65758\r\nFailed to append const node 65758\r\nFailed to append const node 65759\r\nFailed to append const node 65759\r\nFailed to append const node 65760\r\nFailed to append const node 65760\r\nFailed to append const node 65761\r\nFailed to append const node 65761\r\nFailed to append const node 65762\r\nFailed to append const node 65762\r\nFailed to append const node 65763\r\nFailed to append const node 65763\r\nFailed to append const node 65764\r\nFailed to append const node 65764\r\nFailed to append const node 65765\r\nFailed to append const node 65765\r\nFailed to append const node 65766\r\nFailed to append const node 65766\r\nFailed to append const node 65767\r\nFailed to append const node 65767\r\nFailed to append const node 65768\r\nFailed to append const node 65768\r\nFailed to append const node 65769\r\nFailed to append const node 65769\r\nFailed to append const node 65770\r\nFailed to append const node 65770\r\nFailed to append const node 65771\r\nFailed to append const node 65771\r\nFailed to append const node 65772\r\nFailed to append const node 65772\r\nFailed to append const node 65773\r\nFailed to append const node 65773\r\nFailed to append const node 65774\r\nFailed to append const node 65774\r\nFailed to append const node 65775\r\nFailed to append const node 65775\r\nFailed to append const node 65776\r\nFailed to append const node 65776\r\nFailed to append const node 65777\r\nFailed to append const node 65777\r\nFailed to append const node 65778\r\nFailed to append const node 65778\r\nFailed to append const node 65779\r\nFailed to append const node 65779\r\nFailed to append const node 65780\r\nFailed to append const node 65780\r\nFailed to append const node 65781\r\nFailed to append const node 65781\r\nFailed to append const node 65782\r\nFailed to append const node 65782\r\nFailed to append const node 65783\r\nFailed to append const node 65783\r\nFailed to append const node 65784\r\nFailed to append const node 65784\r\nFailed to append const node 65785\r\nFailed to append const node 65785\r\nFailed to append const node 65786\r\nFailed to append const node 65786\r\nFailed to append const node 65787\r\nFailed to append const node 65787\r\nFailed to append const node 65788\r\nFailed to append const node 65788\r\nFailed to append const node 65789\r\nFailed to append const node 65789\r\nFailed to append const node 65790\r\nFailed to append const node 65790\r\nFailed to append const node 65791\r\nFailed to append const node 65791\r\nFailed to append const node 65792\r\nFailed to append const node 65792\r\nFailed to append const node 65793\r\nFailed to append const node 65793\r\nFailed to append const node 65794\r\nFailed to append const node 65794\r\nFailed to append const node 65795\r\nFailed to append const node 65795\r\nFailed to append const node 65796\r\nFailed to append const node 65796\r\nFailed to append const node 65797\r\nFailed to append const node 65797\r\nFailed to append const node 65798\r\nFailed to append const node 65798\r\nFailed to append const node 65799\r\nFailed to append const node 65799\r\nFailed to append const node 65800\r\nFailed to append const node 65800\r\nFailed to append const node 65801\r\nFailed to append const node 65801\r\nFailed to append const node 65802\r\nFailed to append const node 65802\r\nFailed to append const node 65803\r\nFailed to append const node 65803\r\nFailed to append const node 65804\r\nFailed to append const node 65804\r\nFailed to append const node 65805\r\nFailed to append const node 65805\r\nFailed to append const node 65806\r\nFailed to append const node 65806\r\nFailed to append const node 65807\r\nFailed to append const node 65807\r\nFailed to append const node 65808\r\nFailed to append const node 65808\r\nFailed to append const node 65809\r\nFailed to append const node 65809\r\nFailed to append const node 65810\r\nFailed to append const node 65810\r\nFailed to append const node 65811\r\nFailed to append const node 65811\r\nFailed to append const node 65812\r\nFailed to append const node 65812\r\nFailed to append const node 65813\r\nFailed to append const node 65813\r\nFailed to append const node 65814\r\nFailed to append const node 65814\r\nFailed to append const node 65815\r\nFailed to append const node 65815\r\nFailed to append const node 65816\r\nFailed to append const node 65816\r\nFailed to append const node 65817\r\nFailed to append const node 65817\r\nFailed to append const node 65818\r\nFailed to append const node 65818\r\nFailed to append const node 65819\r\nFailed to append const node 65819\r\nFailed to append const node 65820\r\nFailed to append const node 65820\r\nFailed to append const node 65821\r\nFailed to append const node 65821\r\nFailed to append const node 65822\r\nFailed to append const node 65822\r\nFailed to append const node 65823\r\nFailed to append const node 65823\r\nFailed to append const node 65824\r\nFailed to append const node 65824\r\nFailed to append const node 65825\r\nFailed to append const node 65825\r\nFailed to append const node 65826\r\nFailed to append const node 65826\r\nFailed to append const node 65827\r\nFailed to append const node 65827\r\nFailed to append const node 65828\r\nFailed to append const node 65828\r\nFailed to append const node 65829\r\nFailed to append const node 65829\r\nFailed to append const node 65830\r\nFailed to append const node 65830\r\nFailed to append const node 65831\r\nFailed to append const node 65831\r\nFailed to append const node 65832\r\nFailed to append const node 65832\r\nFailed to append const node 65833\r\nFailed to append const node 65833\r\nFailed to append const node 65834\r\nFailed to append const node 65834\r\nFailed to append const node 65835\r\nFailed to append const node 65835\r\nFailed to append const node 65836\r\nFailed to append const node 65836\r\nFailed to append const node 65837\r\nFailed to append const node 65837\r\nFailed to append const node 65838\r\nFailed to append const node 65838\r\nFailed to append const node 65839\r\nFailed to append const node 65839\r\nFailed to append const node 65840\r\nFailed to append const node 65840\r\nFailed to append const node 65841\r\nFailed to append const node 65841\r\nFailed to append const node 65842\r\nFailed to append const node 65842\r\nFailed to append const node 65843\r\nFailed to append const node 65843\r\nFailed to append const node 65844\r\nFailed to append const node 65844\r\nFailed to append const node 65845\r\nFailed to append const node 65845\r\nFailed to append const node 65846\r\nFailed to append const node 65846\r\nFailed to append const node 65847\r\nFailed to append const node 65847\r\nFailed to append const node 65848\r\nFailed to append const node 65848\r\nFailed to append const node 65849\r\nFailed to append const node 65849\r\nFailed to append const node 65850\r\nFailed to append const node 65850\r\nFailed to append const node 65851\r\nFailed to append const node 65851\r\nFailed to append const node 65852\r\nFailed to append const node 65852\r\nFailed to append const node 65853\r\nFailed to append const node 65853\r\nFailed to append const node 65854\r\nFailed to append const node 65854\r\nFailed to append const node 65855\r\nFailed to append const node 65855\r\nFailed to append const node 65856\r\nFailed to append const node 65856\r\nFailed to append const node 65857\r\nFailed to append const node 65857\r\nFailed to append const node 65858\r\nFailed to append const node 65858\r\nFailed to append const node 65859\r\nFailed to append const node 65859\r\nFailed to append const node 65860\r\nFailed to append const node 65860\r\nFailed to append const node 65861\r\nFailed to append const node 65861\r\nFailed to append const node 65862\r\nFailed to append const node 65862\r\nFailed to append const node 65863\r\nFailed to append const node 65863\r\nFailed to append const node 65864\r\nFailed to append const node 65864\r\nFailed to append const node 65865\r\nFailed to append const node 65865\r\nFailed to append const node 65866\r\nFailed to append const node 65866\r\nFailed to append const node 65867\r\nFailed to append const node 65867\r\nFailed to append const node 65868\r\nFailed to append const node 65868\r\nFailed to append const node 65869\r\nFailed to append const node 65869\r\nFailed to append const node 65870\r\nFailed to append const node 65870\r\nFailed to append const node 65871\r\nFailed to append const node 65871\r\nFailed to append const node 65872\r\nFailed to append const node 65872\r\nFailed to append const node 65873\r\nFailed to append const node 65873\r\nFailed to append const node 65874\r\nFailed to append const node 65874\r\nFailed to append const node 65875\r\nFailed to append const node 65875\r\nFailed to append const node 65876\r\nFailed to append const node 65876\r\nFailed to append const node 65877\r\nFailed to append const node 65877\r\nFailed to append const node 65878\r\nFailed to append const node 65878\r\nFailed to append const node 65879\r\nFailed to append const node 65879\r\nFailed to append const node 65880\r\nFailed to append const node 65880\r\nFailed to append const node 65881\r\nFailed to append const node 65881\r\nFailed to append const node 65882\r\nFailed to append const node 65882\r\nFailed to append const node 65883\r\nFailed to append const node 65883\r\nFailed to append const node 65884\r\nFailed to append const node 65884\r\nFailed to append const node 65885\r\nFailed to append const node 65885\r\nFailed to append const node 65886\r\nFailed to append const node 65886\r\nFailed to append const node 65887\r\nFailed to append const node 65887\r\nFailed to append const node 65888\r\nFailed to append const node 65888\r\nFailed to append const node 65889\r\nFailed to append const node 65889\r\nFailed to append const node 65890\r\nFailed to append const node 65890\r\nFailed to append const node 65891\r\nFailed to append const node 65891\r\nFailed to append const node 65892\r\nFailed to append const node 65892\r\nFailed to append const node 65893\r\nFailed to append const node 65893\r\nFailed to append const node 65894\r\nFailed to append const node 65894\r\nFailed to append const node 65895\r\nFailed to append const node 65895\r\nFailed to append const node 65896\r\nFailed to append const node 65896\r\nFailed to append const node 65897\r\nFailed to append const node 65897\r\nFailed to append const node 65898\r\nFailed to append const node 65898\r\nFailed to append const node 65899\r\nFailed to append const node 65899\r\nFailed to append const node 65900\r\nFailed to append const node 65900\r\nFailed to append const node 65901\r\nFailed to append const node 65901\r\nFailed to append const node 65902\r\nFailed to append const node 65902\r\nFailed to append const node 65903\r\nFailed to append const node 65903\r\nFailed to append const node 65904\r\nFailed to append const node 65904\r\nFailed to append const node 65905\r\nFailed to append const node 65905\r\nFailed to append const node 65906\r\nFailed to append const node 65906\r\nFailed to append const node 65907\r\nFailed to append const node 65907\r\nFailed to append const node 65908\r\nFailed to append const node 65908\r\nFailed to append const node 65909\r\nFailed to append const node 65909\r\nFailed to append const node 65910\r\nFailed to append const node 65910\r\nFailed to append const node 65911\r\nFailed to append const node 65911\r\nFailed to append const node 65912\r\nFailed to append const node 65912\r\nFailed to append const node 65913\r\nFailed to append const node 65913\r\nFailed to append const node 65914\r\nFailed to append const node 65914\r\nFailed to append const node 65915\r\nFailed to append const node 65915\r\nFailed to append const node 65916\r\nFailed to append const node 65916\r\nFailed to append const node 65917\r\nFailed to append const node 65917\r\nFailed to append const node 65918\r\nFailed to append const node 65918\r\nFailed to append const node 65919\r\nFailed to append const node 65919\r\nFailed to append const node 65920\r\nFailed to append const node 65920\r\nFailed to append const node 65921\r\nFailed to append const node 65921\r\nFailed to append const node 65922\r\nFailed to append const node 65922\r\nFailed to append const node 65923\r\nFailed to append const node 65923\r\nFailed to append const node 65924\r\nFailed to append const node 65924\r\nFailed to append const node 65925\r\nFailed to append const node 65925\r\nFailed to append const node 65926\r\nFailed to append const node 65926\r\nFailed to append const node 65927\r\nFailed to append const node 65927\r\nFailed to append const node 65928\r\nFailed to append const node 65928\r\nFailed to append const node 65929\r\nFailed to append const node 65929\r\nFailed to append const node 65930\r\nFailed to append const node 65930\r\nFailed to append const node 65931\r\nFailed to append const node 65931\r\nFailed to append const node 65932\r\nFailed to append const node 65932\r\nFailed to append const node 65933\r\nFailed to append const node 65933\r\nFailed to append const node 65934\r\nFailed to append const node 65934\r\nFailed to append const node 65935\r\nFailed to append const node 65935\r\nFailed to append const node 65936\r\nFailed to append const node 65936\r\nFailed to append const node 65937\r\nFailed to append const node 65937\r\nFailed to append const node 65938\r\nFailed to append const node 65938\r\nFailed to append const node 65939\r\nFailed to append const node 65939\r\nFailed to append const node 65940\r\nFailed to append const node 65940\r\nFailed to append const node 65941\r\nFailed to append const node 65941\r\nFailed to append const node 65942\r\nFailed to append const node 65942\r\nFailed to append const node 65943\r\nFailed to append const node 65943\r\nFailed to append const node 65944\r\nFailed to append const node 65944\r\nFailed to append const node 65945\r\nFailed to append const node 65945\r\nFailed to append const node 65946\r\nFailed to append const node 65946\r\nFailed to append const node 65947\r\nFailed to append const node 65947\r\nFailed to append const node 65948\r\nFailed to append const node 65948\r\nFailed to append const node 65949\r\nFailed to append const node 65949\r\nFailed to append const node 65950\r\nFailed to append const node 65950\r\nFailed to append const node 65951\r\nFailed to append const node 65951\r\nFailed to append const node 65952\r\nFailed to append const node 65952\r\nFailed to append const node 65953\r\nFailed to append const node 65953\r\nFailed to append const node 65954\r\nFailed to append const node 65954\r\nFailed to append const node 65955\r\nFailed to append const node 65955\r\nFailed to append const node 65956\r\nFailed to append const node 65956\r\nFailed to append const node 65957\r\nFailed to append const node 65957\r\nFailed to append const node 65958\r\nFailed to append const node 65958\r\nFailed to append const node 65959\r\nFailed to append const node 65959\r\nFailed to append const node 65960\r\nFailed to append const node 65960\r\nFailed to append const node 65961\r\nFailed to append const node 65961\r\nFailed to append const node 65962\r\nFailed to append const node 65962\r\nFailed to append const node 65963\r\nFailed to append const node 65963\r\nFailed to append const node 65964\r\nFailed to append const node 65964\r\nFailed to append const node 65965\r\nFailed to append const node 65965\r\nFailed to append const node 65966\r\nFailed to append const node 65966\r\nFailed to append const node 65967\r\nFailed to append const node 65967\r\nFailed to append const node 65968\r\nFailed to append const node 65968\r\nFailed to append const node 65969\r\nFailed to append const node 65969\r\nFailed to append const node 65970\r\nFailed to append const node 65970\r\nFailed to append const node 65971\r\nFailed to append const node 65971\r\nFailed to append const node 65972\r\nFailed to append const node 65972\r\nFailed to append const node 65973\r\nFailed to append const node 65973\r\nFailed to append const node 65974\r\nFailed to append const node 65974\r\nFailed to append const node 65975\r\nFailed to append const node 65975\r\nFailed to append const node 65976\r\nFailed to append const node 65976\r\nFailed to append const node 65977\r\nFailed to append const node 65977\r\nFailed to append const node 65978\r\nFailed to append const node 65978\r\nFailed to append const node 65979\r\nFailed to append const node 65979\r\nFailed to append const node 65980\r\nFailed to append const node 65980\r\nFailed to append const node 65981\r\nFailed to append const node 65981\r\nFailed to append const node 65982\r\nFailed to append const node 65982\r\nFailed to append const node 65983\r\nFailed to append const node 65983\r\nFailed to append const node 65984\r\nFailed to append const node 65984\r\nFailed to append const node 65985\r\nFailed to append const node 65985\r\nFailed to append const node 65986\r\nFailed to append const node 65986\r\nFailed to append const node 65987\r\nFailed to append const node 65987\r\nFailed to append const node 65988\r\nFailed to append const node 65988\r\nFailed to append const node 65989\r\nFailed to append const node 65989\r\nFailed to append const node 65990\r\nFailed to append const node 65990\r\nFailed to append const node 65991\r\nFailed to append const node 65991\r\nFailed to append const node 65992\r\nFailed to append const node 65992\r\nFailed to append const node 65993\r\nFailed to append const node 65993\r\nFailed to append const node 65994\r\nFailed to append const node 65994\r\nFailed to append const node 65995\r\nFailed to append const node 65995\r\nFailed to append const node 65996\r\nFailed to append const node 65996\r\nFailed to append const node 65997\r\nFailed to append const node 65997\r\nFailed to append const node 65998\r\nFailed to append const node 65998\r\nFailed to append const node 65999\r\nFailed to append const node 65999\r\nFailed to append const node 66000\r\nFailed to append const node 66000\r\nFailed to append const node 66001\r\nFailed to append const node 66001\r\nFailed to append const node 66002\r\nFailed to append const node 66002\r\nFailed to append const node 66003\r\nFailed to append const node 66003\r\nFailed to append const node 66004\r\nFailed to append const node 66004\r\nFailed to append const node 66005\r\nFailed to append const node 66005\r\nFailed to append const node 66006\r\nFailed to append const node 66006\r\nFailed to append const node 66007\r\nFailed to append const node 66007\r\nFailed to append const node 66008\r\nFailed to append const node 66008\r\nFailed to append const node 66009\r\nFailed to append const node 66009\r\nFailed to append const node 66010\r\nFailed to append const node 66010\r\nFailed to append const node 66011\r\nFailed to append const node 66011\r\nFailed to append const node 66012\r\nFailed to append const node 66012\r\nFailed to append const node 66013\r\nFailed to append const node 66013\r\nFailed to append const node 66014\r\nFailed to append const node 66014\r\nFailed to append const node 66015\r\nFailed to append const node 66015\r\nFailed to append const node 66016\r\nFailed to append const node 66016\r\nFailed to append const node 66017\r\nFailed to append const node 66017\r\nFailed to append const node 66018\r\nFailed to append const node 66018\r\nFailed to append const node 66019\r\nFailed to append const node 66019\r\nFailed to append const node 66020\r\nFailed to append const node 66020\r\nFailed to append const node 66021\r\nFailed to append const node 66021\r\nFailed to append const node 66022\r\nFailed to append const node 66022\r\nFailed to append const node 66023\r\nFailed to append const node 66023\r\nFailed to append const node 66024\r\nFailed to append const node 66024\r\nFailed to append const node 66025\r\nFailed to append const node 66025\r\nFailed to append const node 66026\r\nFailed to append const node 66026\r\nFailed to append const node 66027\r\nFailed to append const node 66027\r\nFailed to append const node 66028\r\nFailed to append const node 66028\r\nFailed to append const node 66029\r\nFailed to append const node 66029\r\nFailed to append const node 66030\r\nFailed to append const node 66030\r\nFailed to append const node 66031\r\nFailed to append const node 66031\r\nFailed to append const node 66032\r\nFailed to append const node 66032\r\nFailed to append const node 66033\r\nFailed to append const node 66033\r\nFailed to append const node 66034\r\nFailed to append const node 66034\r\nFailed to append const node 66035\r\nFailed to append const node 66035\r\nFailed to append const node 66036\r\nFailed to append const node 66036\r\nFailed to append const node 66037\r\nFailed to append const node 66037\r\nFailed to append const node 66038\r\nFailed to append const node 66038\r\nFailed to append const node 66039\r\nFailed to append const node 66039\r\nFailed to append const node 66040\r\nFailed to append const node 66040\r\nFailed to append const node 66041\r\nFailed to append const node 66041\r\nFailed to append const node 66042\r\nFailed to append const node 66042\r\nFailed to append const node 66043\r\nFailed to append const node 66043\r\nFailed to append const node 66044\r\nFailed to append const node 66044\r\nFailed to append const node 66045\r\nFailed to append const node 66045\r\nFailed to append const node 66046\r\nFailed to append const node 66046\r\nFailed to append const node 66047\r\nFailed to append const node 66047\r\nFailed to append const node 66048\r\nFailed to append const node 66048\r\nFailed to append const node 66049\r\nFailed to append const node 66049\r\nFailed to append const node 66050\r\nFailed to append const node 66050\r\nFailed to append const node 66051\r\nFailed to append const node 66051\r\nFailed to append const node 66052\r\nFailed to append const node 66052\r\nFailed to append const node 66053\r\nFailed to append const node 66053\r\nFailed to append const node 66054\r\nFailed to append const node 66054\r\nFailed to append const node 66055\r\nFailed to append const node 66055\r\nFailed to append const node 66056\r\nFailed to append const node 66056\r\nFailed to append const node 66057\r\nFailed to append const node 66057\r\nFailed to append const node 66058\r\nFailed to append const node 66058\r\nFailed to append const node 66059\r\nFailed to append const node 66059\r\nFailed to append const node 66060\r\nFailed to append const node 66060\r\nFailed to append const node 66061\r\nFailed to append const node 66061\r\nFailed to append const node 66062\r\nFailed to append const node 66062\r\nFailed to append const node 66063\r\nFailed to append const node 66063\r\nFailed to append const node 66064\r\nFailed to append const node 66064\r\nFailed to append const node 66065\r\nFailed to append const node 66065\r\nFailed to append const node 66066\r\nFailed to append const node 66066\r\nFailed to append const node 66067\r\nFailed to append const node 66067\r\nFailed to append const node 66068\r\nFailed to append const node 66068\r\nFailed to append const node 66069\r\nFailed to append const node 66069\r\nFailed to append const node 66070\r\nFailed to append const node 66070\r\nFailed to append const node 66071\r\nFailed to append const node 66071\r\nFailed to append const node 66072\r\nFailed to append const node 66072\r\nFailed to append const node 66073\r\nFailed to append const node 66073\r\nFailed to append const node 66074\r\nFailed to append const node 66074\r\nFailed to append const node 66075\r\nFailed to append const node 66075\r\nFailed to append const node 66076\r\nFailed to append const node 66076\r\nFailed to append const node 66077\r\nFailed to append const node 66077\r\nFailed to append const node 66078\r\nFailed to append const node 66078\r\nFailed to append const node 66079\r\nFailed to append const node 66079\r\nFailed to append const node 66080\r\nFailed to append const node 66080\r\nFailed to append const node 66081\r\nFailed to append const node 66081\r\nFailed to append const node 66082\r\nFailed to append const node 66082\r\nFailed to append const node 66083\r\nFailed to append const node 66083\r\nFailed to append const node 66084\r\nFailed to append const node 66084\r\nFailed to append const node 66085\r\nFailed to append const node 66085\r\nFailed to append const node 66086\r\nFailed to append const node 66086\r\nFailed to append const node 66087\r\nFailed to append const node 66087\r\nFailed to append const node 66088\r\nFailed to append const node 66088\r\nFailed to append const node 66089\r\nFailed to append const node 66089\r\nFailed to append const node 66090\r\nFailed to append const node 66090\r\nFailed to append const node 66091\r\nFailed to append const node 66091\r\nFailed to append const node 66092\r\nFailed to append const node 66092\r\nFailed to append const node 66093\r\nFailed to append const node 66093\r\nFailed to append const node 66094\r\nFailed to append const node 66094\r\nFailed to append const node 66095\r\nFailed to append const node 66095\r\nFailed to append const node 66096\r\nFailed to append const node 66096\r\nFailed to append const node 66097\r\nFailed to append const node 66097\r\nFailed to append const node 66098\r\nFailed to append const node 66098\r\nFailed to append const node 66099\r\nFailed to append const node 66099\r\nFailed to append const node 66100\r\nFailed to append const node 66100\r\nFailed to append const node 66101\r\nFailed to append const node 66101\r\nFailed to append const node 66102\r\nFailed to append const node 66102\r\nFailed to append const node 66103\r\nFailed to append const node 66103\r\nFailed to append const node 66104\r\nFailed to append const node 66104\r\nFailed to append const node 66105\r\nFailed to append const node 66105\r\nFailed to append const node 66106\r\nFailed to append const node 66106\r\nFailed to append const node 66107\r\nFailed to append const node 66107\r\nFailed to append const node 66108\r\nFailed to append const node 66108\r\nFailed to append const node 66109\r\nFailed to append const node 66109\r\nFailed to append const node 66110\r\nFailed to append const node 66110\r\nFailed to append const node 66111\r\nFailed to append const node 66111\r\nFailed to append const node 66112\r\nFailed to append const node 66112\r\nFailed to append const node 66636\r\nFailed to append const node 66636\r\nFailed to append const node 66637\r\nFailed to append const node 66637\r\nFailed to append const node 66638\r\nFailed to append const node 66638\r\nFailed to append const node 66181\r\nFailed to append const node 66181\r\nFailed to append const node 66219\r\nFailed to append const node 66219\r\nFailed to append const node 66257\r\nFailed to append const node 66257\r\nFailed to append const node 66280\r\nFailed to append const node 66280\r\nFailed to append const node 66333\r\nFailed to append const node 66333\r\nFailed to append const node 66386\r\nFailed to append const node 66386\r\nFailed to append const node 66439\r\nFailed to append const node 66439\r\nFailed to append const node 66492\r\nFailed to append const node 66492\r\nFailed to append const node 66525\r\nFailed to append const node 66525\r\nFailed to append const node 66573\r\nFailed to append const node 66573\r\nFailed to append const node 66621\r\nFailed to append const node 66621\r\nFailed to append const node 66639\r\nFailed to append const node 66639\r\nFailed to append const node 66113\r\nFailed to append const node 66113\r\nFailed to append const node 66114\r\nFailed to append const node 66114\r\nFailed to append const node 66116\r\nFailed to append const node 66116\r\nFailed to append const node 66115\r\nFailed to append const node 66115\r\nFailed to append const node 66117\r\nFailed to append const node 66117\r\nFailed to append const node 66118\r\nFailed to append const node 66118\r\nFailed to append const node 66119\r\nFailed to append const node 66119\r\nFailed to append const node 66120\r\nFailed to append const node 66120\r\nFailed to append const node 66121\r\nFailed to append const node 66121\r\nFailed to append const node 66122\r\nFailed to append const node 66122\r\nFailed to append const node 66123\r\nFailed to append const node 66123\r\nFailed to append const node 66124\r\nFailed to append const node 66124\r\nFailed to append const node 66125\r\nFailed to append const node 66125\r\nFailed to append const node 66126\r\nFailed to append const node 66126\r\nFailed to append const node 66127\r\nFailed to append const node 66127\r\nFailed to append const node 66128\r\nFailed to append const node 66128\r\nFailed to append const node 66129\r\nFailed to append const node 66129\r\nFailed to append const node 66130\r\nFailed to append const node 66130\r\nFailed to append const node 66131\r\nFailed to append const node 66131\r\nFailed to append const node 66132\r\nFailed to append const node 66132\r\nFailed to append const node 66133\r\nFailed to append const node 66133\r\nFailed to append const node 66134\r\nFailed to append const node 66134\r\nFailed to append const node 66135\r\nFailed to append const node 66135\r\nFailed to append const node 66136\r\nFailed to append const node 66136\r\nFailed to append const node 66137\r\nFailed to append const node 66137\r\nFailed to append const node 66138\r\nFailed to append const node 66138\r\nFailed to append const node 66139\r\nFailed to append const node 66139\r\nFailed to append const node 66140\r\nFailed to append const node 66140\r\nFailed to append const node 66141\r\nFailed to append const node 66141\r\nFailed to append const node 66142\r\nFailed to append const node 66142\r\nFailed to append const node 66143\r\nFailed to append const node 66143\r\nFailed to append const node 66144\r\nFailed to append const node 66144\r\nFailed to append const node 66176\r\nFailed to append const node 66176\r\nFailed to append const node 66177\r\nFailed to append const node 66177\r\nFailed to append const node 66178\r\nFailed to append const node 66178\r\nFailed to append const node 66179\r\nFailed to append const node 66179\r\nFailed to append const node 66180\r\nFailed to append const node 66180\r\nFailed to append const node 66166\r\nFailed to append const node 66166\r\nFailed to append const node 66167\r\nFailed to append const node 66167\r\nFailed to append const node 66168\r\nFailed to append const node 66168\r\nFailed to append const node 66169\r\nFailed to append const node 66169\r\nFailed to append const node 66170\r\nFailed to append const node 66170\r\nFailed to append const node 66171\r\nFailed to append const node 66171\r\nFailed to append const node 66172\r\nFailed to append const node 66172\r\nFailed to append const node 66173\r\nFailed to append const node 66173\r\nFailed to append const node 66174\r\nFailed to append const node 66174\r\nFailed to append const node 66175\r\nFailed to append const node 66175\r\nFailed to append const node 66151\r\nFailed to append const node 66151\r\nFailed to append const node 66152\r\nFailed to append const node 66152\r\nFailed to append const node 66153\r\nFailed to append const node 66153\r\nFailed to append const node 66154\r\nFailed to append const node 66154\r\nFailed to append const node 66155\r\nFailed to append const node 66155\r\nFailed to append const node 66156\r\nFailed to append const node 66156\r\nFailed to append const node 66157\r\nFailed to append const node 66157\r\nFailed to append const node 66158\r\nFailed to append const node 66158\r\nFailed to append const node 66159\r\nFailed to append const node 66159\r\nFailed to append const node 66160\r\nFailed to append const node 66160\r\nFailed to append const node 66161\r\nFailed to append const node 66161\r\nFailed to append const node 66162\r\nFailed to append const node 66162\r\nFailed to append const node 66163\r\nFailed to append const node 66163\r\nFailed to append const node 66164\r\nFailed to append const node 66164\r\nFailed to append const node 66165\r\nFailed to append const node 66165\r\nFailed to append const node 66145\r\nFailed to append const node 66145\r\nFailed to append const node 66146\r\nFailed to append const node 66146\r\nFailed to append const node 66147\r\nFailed to append const node 66147\r\nFailed to append const node 66148\r\nFailed to append const node 66148\r\nFailed to append const node 66149\r\nFailed to append const node 66149\r\nFailed to append const node 66150\r\nFailed to append const node 66150\r\nFailed to append const node 66182\r\nFailed to append const node 66182\r\nFailed to append const node 66214\r\nFailed to append const node 66214\r\nFailed to append const node 66215\r\nFailed to append const node 66215\r\nFailed to append const node 66216\r\nFailed to append const node 66216\r\nFailed to append const node 66217\r\nFailed to append const node 66217\r\nFailed to append const node 66218\r\nFailed to append const node 66218\r\nFailed to append const node 66204\r\nFailed to append const node 66204\r\nFailed to append const node 66205\r\nFailed to append const node 66205\r\nFailed to append const node 66206\r\nFailed to append const node 66206\r\nFailed to append const node 66207\r\nFailed to append const node 66207\r\nFailed to append const node 66208\r\nFailed to append const node 66208\r\nFailed to append const node 66209\r\nFailed to append const node 66209\r\nFailed to append const node 66210\r\nFailed to append const node 66210\r\nFailed to append const node 66211\r\nFailed to append const node 66211\r\nFailed to append const node 66212\r\nFailed to append const node 66212\r\nFailed to append const node 66213\r\nFailed to append const node 66213\r\nFailed to append const node 66189\r\nFailed to append const node 66189\r\nFailed to append const node 66190\r\nFailed to append const node 66190\r\nFailed to append const node 66191\r\nFailed to append const node 66191\r\nFailed to append const node 66192\r\nFailed to append const node 66192\r\nFailed to append const node 66193\r\nFailed to append const node 66193\r\nFailed to append const node 66194\r\nFailed to append const node 66194\r\nFailed to append const node 66195\r\nFailed to append const node 66195\r\nFailed to append const node 66196\r\nFailed to append const node 66196\r\nFailed to append const node 66197\r\nFailed to append const node 66197\r\nFailed to append const node 66198\r\nFailed to append const node 66198\r\nFailed to append const node 66199\r\nFailed to append const node 66199\r\nFailed to append const node 66200\r\nFailed to append const node 66200\r\nFailed to append const node 66201\r\nFailed to append const node 66201\r\nFailed to append const node 66202\r\nFailed to append const node 66202\r\nFailed to append const node 66203\r\nFailed to append const node 66203\r\nFailed to append const node 66183\r\nFailed to append const node 66183\r\nFailed to append const node 66184\r\nFailed to append const node 66184\r\nFailed to append const node 66185\r\nFailed to append const node 66185\r\nFailed to append const node 66186\r\nFailed to append const node 66186\r\nFailed to append const node 66187\r\nFailed to append const node 66187\r\nFailed to append const node 66188\r\nFailed to append const node 66188\r\nFailed to append const node 66220\r\nFailed to append const node 66220\r\nFailed to append const node 66252\r\nFailed to append const node 66252\r\nFailed to append const node 66253\r\nFailed to append const node 66253\r\nFailed to append const node 66254\r\nFailed to append const node 66254\r\nFailed to append const node 66255\r\nFailed to append const node 66255\r\nFailed to append const node 66256\r\nFailed to append const node 66256\r\nFailed to append const node 66242\r\nFailed to append const node 66242\r\nFailed to append const node 66243\r\nFailed to append const node 66243\r\nFailed to append const node 66244\r\nFailed to append const node 66244\r\nFailed to append const node 66245\r\nFailed to append const node 66245\r\nFailed to append const node 66246\r\nFailed to append const node 66246\r\nFailed to append const node 66247\r\nFailed to append const node 66247\r\nFailed to append const node 66248\r\nFailed to append const node 66248\r\nFailed to append const node 66249\r\nFailed to append const node 66249\r\nFailed to append const node 66250\r\nFailed to append const node 66250\r\nFailed to append const node 66251\r\nFailed to append const node 66251\r\nFailed to append const node 66227\r\nFailed to append const node 66227\r\nFailed to append const node 66228\r\nFailed to append const node 66228\r\nFailed to append const node 66229\r\nFailed to append const node 66229\r\nFailed to append const node 66230\r\nFailed to append const node 66230\r\nFailed to append const node 66231\r\nFailed to append const node 66231\r\nFailed to append const node 66232\r\nFailed to append const node 66232\r\nFailed to append const node 66233\r\nFailed to append const node 66233\r\nFailed to append const node 66234\r\nFailed to append const node 66234\r\nFailed to append const node 66235\r\nFailed to append const node 66235\r\nFailed to append const node 66236\r\nFailed to append const node 66236\r\nFailed to append const node 66237\r\nFailed to append const node 66237\r\nFailed to append const node 66238\r\nFailed to append const node 66238\r\nFailed to append const node 66239\r\nFailed to append const node 66239\r\nFailed to append const node 66240\r\nFailed to append const node 66240\r\nFailed to append const node 66241\r\nFailed to append const node 66241\r\nFailed to append const node 66221\r\nFailed to append const node 66221\r\nFailed to append const node 66222\r\nFailed to append const node 66222\r\nFailed to append const node 66223\r\nFailed to append const node 66223\r\nFailed to append const node 66224\r\nFailed to append const node 66224\r\nFailed to append const node 66225\r\nFailed to append const node 66225\r\nFailed to append const node 66226\r\nFailed to append const node 66226\r\nFailed to append const node 66258\r\nFailed to append const node 66258\r\nFailed to append const node 66275\r\nFailed to append const node 66275\r\nFailed to append const node 66276\r\nFailed to append const node 66276\r\nFailed to append const node 66277\r\nFailed to append const node 66277\r\nFailed to append const node 66278\r\nFailed to append const node 66278\r\nFailed to append const node 66279\r\nFailed to append const node 66279\r\nFailed to append const node 66260\r\nFailed to append const node 66260\r\nFailed to append const node 66261\r\nFailed to append const node 66261\r\nFailed to append const node 66262\r\nFailed to append const node 66262\r\nFailed to append const node 66263\r\nFailed to append const node 66263\r\nFailed to append const node 66264\r\nFailed to append const node 66264\r\nFailed to append const node 66265\r\nFailed to append const node 66265\r\nFailed to append const node 66266\r\nFailed to append const node 66266\r\nFailed to append const node 66267\r\nFailed to append const node 66267\r\nFailed to append const node 66268\r\nFailed to append const node 66268\r\nFailed to append const node 66269\r\nFailed to append const node 66269\r\nFailed to append const node 66270\r\nFailed to append const node 66270\r\nFailed to append const node 66271\r\nFailed to append const node 66271\r\nFailed to append const node 66272\r\nFailed to append const node 66272\r\nFailed to append const node 66273\r\nFailed to append const node 66273\r\nFailed to append const node 66274\r\nFailed to append const node 66274\r\nFailed to append const node 66259\r\nFailed to append const node 66259\r\nFailed to append const node 66281\r\nFailed to append const node 66281\r\nFailed to append const node 66328\r\nFailed to append const node 66328\r\nFailed to append const node 66329\r\nFailed to append const node 66329\r\nFailed to append const node 66330\r\nFailed to append const node 66330\r\nFailed to append const node 66331\r\nFailed to append const node 66331\r\nFailed to append const node 66332\r\nFailed to append const node 66332\r\nFailed to append const node 66313\r\nFailed to append const node 66313\r\nFailed to append const node 66314\r\nFailed to append const node 66314\r\nFailed to append const node 66315\r\nFailed to append const node 66315\r\nFailed to append const node 66316\r\nFailed to append const node 66316\r\nFailed to append const node 66317\r\nFailed to append const node 66317\r\nFailed to append const node 66318\r\nFailed to append const node 66318\r\nFailed to append const node 66319\r\nFailed to append const node 66319\r\nFailed to append const node 66320\r\nFailed to append const node 66320\r\nFailed to append const node 66321\r\nFailed to append const node 66321\r\nFailed to append const node 66322\r\nFailed to append const node 66322\r\nFailed to append const node 66323\r\nFailed to append const node 66323\r\nFailed to append const node 66324\r\nFailed to append const node 66324\r\nFailed to append const node 66325\r\nFailed to append const node 66325\r\nFailed to append const node 66326\r\nFailed to append const node 66326\r\nFailed to append const node 66327\r\nFailed to append const node 66327\r\nFailed to append const node 66288\r\nFailed to append const node 66288\r\nFailed to append const node 66289\r\nFailed to append const node 66289\r\nFailed to append const node 66290\r\nFailed to append const node 66290\r\nFailed to append const node 66291\r\nFailed to append const node 66291\r\nFailed to append const node 66292\r\nFailed to append const node 66292\r\nFailed to append const node 66293\r\nFailed to append const node 66293\r\nFailed to append const node 66294\r\nFailed to append const node 66294\r\nFailed to append const node 66295\r\nFailed to append const node 66295\r\nFailed to append const node 66296\r\nFailed to append const node 66296\r\nFailed to append const node 66297\r\nFailed to append const node 66297\r\nFailed to append const node 66298\r\nFailed to append const node 66298\r\nFailed to append const node 66299\r\nFailed to append const node 66299\r\nFailed to append const node 66300\r\nFailed to append const node 66300\r\nFailed to append const node 66301\r\nFailed to append const node 66301\r\nFailed to append const node 66302\r\nFailed to append const node 66302\r\nFailed to append const node 66303\r\nFailed to append const node 66303\r\nFailed to append const node 66304\r\nFailed to append const node 66304\r\nFailed to append const node 66305\r\nFailed to append const node 66305\r\nFailed to append const node 66306\r\nFailed to append const node 66306\r\nFailed to append const node 66307\r\nFailed to append const node 66307\r\nFailed to append const node 66308\r\nFailed to append const node 66308\r\nFailed to append const node 66309\r\nFailed to append const node 66309\r\nFailed to append const node 66310\r\nFailed to append const node 66310\r\nFailed to append const node 66311\r\nFailed to append const node 66311\r\nFailed to append const node 66312\r\nFailed to append const node 66312\r\nFailed to append const node 66282\r\nFailed to append const node 66282\r\nFailed to append const node 66283\r\nFailed to append const node 66283\r\nFailed to append const node 66284\r\nFailed to append const node 66284\r\nFailed to append const node 66285\r\nFailed to append const node 66285\r\nFailed to append const node 66286\r\nFailed to append const node 66286\r\nFailed to append const node 66287\r\nFailed to append const node 66287\r\nFailed to append const node 66334\r\nFailed to append const node 66334\r\nFailed to append const node 66381\r\nFailed to append const node 66381\r\nFailed to append const node 66382\r\nFailed to append const node 66382\r\nFailed to append const node 66383\r\nFailed to append const node 66383\r\nFailed to append const node 66384\r\nFailed to append const node 66384\r\nFailed to append const node 66385\r\nFailed to append const node 66385\r\nFailed to append const node 66366\r\nFailed to append const node 66366\r\nFailed to append const node 66367\r\nFailed to append const node 66367\r\nFailed to append const node 66368\r\nFailed to append const node 66368\r\nFailed to append const node 66369\r\nFailed to append const node 66369\r\nFailed to append const node 66370\r\nFailed to append const node 66370\r\nFailed to append const node 66371\r\nFailed to append const node 66371\r\nFailed to append const node 66372\r\nFailed to append const node 66372\r\nFailed to append const node 66373\r\nFailed to append const node 66373\r\nFailed to append const node 66374\r\nFailed to append const node 66374\r\nFailed to append const node 66375\r\nFailed to append const node 66375\r\nFailed to append const node 66376\r\nFailed to append const node 66376\r\nFailed to append const node 66377\r\nFailed to append const node 66377\r\nFailed to append const node 66378\r\nFailed to append const node 66378\r\nFailed to append const node 66379\r\nFailed to append const node 66379\r\nFailed to append const node 66380\r\nFailed to append const node 66380\r\nFailed to append const node 66341\r\nFailed to append const node 66341\r\nFailed to append const node 66342\r\nFailed to append const node 66342\r\nFailed to append const node 66343\r\nFailed to append const node 66343\r\nFailed to append const node 66344\r\nFailed to append const node 66344\r\nFailed to append const node 66345\r\nFailed to append const node 66345\r\nFailed to append const node 66346\r\nFailed to append const node 66346\r\nFailed to append const node 66347\r\nFailed to append const node 66347\r\nFailed to append const node 66348\r\nFailed to append const node 66348\r\nFailed to append const node 66349\r\nFailed to append const node 66349\r\nFailed to append const node 66350\r\nFailed to append const node 66350\r\nFailed to append const node 66351\r\nFailed to append const node 66351\r\nFailed to append const node 66352\r\nFailed to append const node 66352\r\nFailed to append const node 66353\r\nFailed to append const node 66353\r\nFailed to append const node 66354\r\nFailed to append const node 66354\r\nFailed to append const node 66355\r\nFailed to append const node 66355\r\nFailed to append const node 66356\r\nFailed to append const node 66356\r\nFailed to append const node 66357\r\nFailed to append const node 66357\r\nFailed to append const node 66358\r\nFailed to append const node 66358\r\nFailed to append const node 66359\r\nFailed to append const node 66359\r\nFailed to append const node 66360\r\nFailed to append const node 66360\r\nFailed to append const node 66361\r\nFailed to append const node 66361\r\nFailed to append const node 66362\r\nFailed to append const node 66362\r\nFailed to append const node 66363\r\nFailed to append const node 66363\r\nFailed to append const node 66364\r\nFailed to append const node 66364\r\nFailed to append const node 66365\r\nFailed to append const node 66365\r\nFailed to append const node 66335\r\nFailed to append const node 66335\r\nFailed to append const node 66336\r\nFailed to append const node 66336\r\nFailed to append const node 66337\r\nFailed to append const node 66337\r\nFailed to append const node 66338\r\nFailed to append const node 66338\r\nFailed to append const node 66339\r\nFailed to append const node 66339\r\nFailed to append const node 66340\r\nFailed to append const node 66340\r\nFailed to append const node 66387\r\nFailed to append const node 66387\r\nFailed to append const node 66434\r\nFailed to append const node 66434\r\nFailed to append const node 66435\r\nFailed to append const node 66435\r\nFailed to append const node 66436\r\nFailed to append const node 66436\r\nFailed to append const node 66437\r\nFailed to append const node 66437\r\nFailed to append const node 66438\r\nFailed to append const node 66438\r\nFailed to append const node 66419\r\nFailed to append const node 66419\r\nFailed to append const node 66420\r\nFailed to append const node 66420\r\nFailed to append const node 66421\r\nFailed to append const node 66421\r\nFailed to append const node 66422\r\nFailed to append const node 66422\r\nFailed to append const node 66423\r\nFailed to append const node 66423\r\nFailed to append const node 66424\r\nFailed to append const node 66424\r\nFailed to append const node 66425\r\nFailed to append const node 66425\r\nFailed to append const node 66426\r\nFailed to append const node 66426\r\nFailed to append const node 66427\r\nFailed to append const node 66427\r\nFailed to append const node 66428\r\nFailed to append const node 66428\r\nFailed to append const node 66429\r\nFailed to append const node 66429\r\nFailed to append const node 66430\r\nFailed to append const node 66430\r\nFailed to append const node 66431\r\nFailed to append const node 66431\r\nFailed to append const node 66432\r\nFailed to append const node 66432\r\nFailed to append const node 66433\r\nFailed to append const node 66433\r\nFailed to append const node 66394\r\nFailed to append const node 66394\r\nFailed to append const node 66395\r\nFailed to append const node 66395\r\nFailed to append const node 66396\r\nFailed to append const node 66396\r\nFailed to append const node 66397\r\nFailed to append const node 66397\r\nFailed to append const node 66398\r\nFailed to append const node 66398\r\nFailed to append const node 66399\r\nFailed to append const node 66399\r\nFailed to append const node 66400\r\nFailed to append const node 66400\r\nFailed to append const node 66401\r\nFailed to append const node 66401\r\nFailed to append const node 66402\r\nFailed to append const node 66402\r\nFailed to append const node 66403\r\nFailed to append const node 66403\r\nFailed to append const node 66404\r\nFailed to append const node 66404\r\nFailed to append const node 66405\r\nFailed to append const node 66405\r\nFailed to append const node 66406\r\nFailed to append const node 66406\r\nFailed to append const node 66407\r\nFailed to append const node 66407\r\nFailed to append const node 66408\r\nFailed to append const node 66408\r\nFailed to append const node 66409\r\nFailed to append const node 66409\r\nFailed to append const node 66410\r\nFailed to append const node 66410\r\nFailed to append const node 66411\r\nFailed to append const node 66411\r\nFailed to append const node 66412\r\nFailed to append const node 66412\r\nFailed to append const node 66413\r\nFailed to append const node 66413\r\nFailed to append const node 66414\r\nFailed to append const node 66414\r\nFailed to append const node 66415\r\nFailed to append const node 66415\r\nFailed to append const node 66416\r\nFailed to append const node 66416\r\nFailed to append const node 66417\r\nFailed to append const node 66417\r\nFailed to append const node 66418\r\nFailed to append const node 66418\r\nFailed to append const node 66388\r\nFailed to append const node 66388\r\nFailed to append const node 66389\r\nFailed to append const node 66389\r\nFailed to append const node 66390\r\nFailed to append const node 66390\r\nFailed to append const node 66391\r\nFailed to append const node 66391\r\nFailed to append const node 66392\r\nFailed to append const node 66392\r\nFailed to append const node 66393\r\nFailed to append const node 66393\r\nFailed to append const node 66440\r\nFailed to append const node 66440\r\nFailed to append const node 66487\r\nFailed to append const node 66487\r\nFailed to append const node 66488\r\nFailed to append const node 66488\r\nFailed to append const node 66489\r\nFailed to append const node 66489\r\nFailed to append const node 66490\r\nFailed to append const node 66490\r\nFailed to append const node 66491\r\nFailed to append const node 66491\r\nFailed to append const node 66472\r\nFailed to append const node 66472\r\nFailed to append const node 66473\r\nFailed to append const node 66473\r\nFailed to append const node 66474\r\nFailed to append const node 66474\r\nFailed to append const node 66475\r\nFailed to append const node 66475\r\nFailed to append const node 66476\r\nFailed to append const node 66476\r\nFailed to append const node 66477\r\nFailed to append const node 66477\r\nFailed to append const node 66478\r\nFailed to append const node 66478\r\nFailed to append const node 66479\r\nFailed to append const node 66479\r\nFailed to append const node 66480\r\nFailed to append const node 66480\r\nFailed to append const node 66481\r\nFailed to append const node 66481\r\nFailed to append const node 66482\r\nFailed to append const node 66482\r\nFailed to append const node 66483\r\nFailed to append const node 66483\r\nFailed to append const node 66484\r\nFailed to append const node 66484\r\nFailed to append const node 66485\r\nFailed to append const node 66485\r\nFailed to append const node 66486\r\nFailed to append const node 66486\r\nFailed to append const node 66447\r\nFailed to append const node 66447\r\nFailed to append const node 66448\r\nFailed to append const node 66448\r\nFailed to append const node 66449\r\nFailed to append const node 66449\r\nFailed to append const node 66450\r\nFailed to append const node 66450\r\nFailed to append const node 66451\r\nFailed to append const node 66451\r\nFailed to append const node 66452\r\nFailed to append const node 66452\r\nFailed to append const node 66453\r\nFailed to append const node 66453\r\nFailed to append const node 66454\r\nFailed to append const node 66454\r\nFailed to append const node 66455\r\nFailed to append const node 66455\r\nFailed to append const node 66456\r\nFailed to append const node 66456\r\nFailed to append const node 66457\r\nFailed to append const node 66457\r\nFailed to append const node 66458\r\nFailed to append const node 66458\r\nFailed to append const node 66459\r\nFailed to append const node 66459\r\nFailed to append const node 66460\r\nFailed to append const node 66460\r\nFailed to append const node 66461\r\nFailed to append const node 66461\r\nFailed to append const node 66462\r\nFailed to append const node 66462\r\nFailed to append const node 66463\r\nFailed to append const node 66463\r\nFailed to append const node 66464\r\nFailed to append const node 66464\r\nFailed to append const node 66465\r\nFailed to append const node 66465\r\nFailed to append const node 66466\r\nFailed to append const node 66466\r\nFailed to append const node 66467\r\nFailed to append const node 66467\r\nFailed to append const node 66468\r\nFailed to append const node 66468\r\nFailed to append const node 66469\r\nFailed to append const node 66469\r\nFailed to append const node 66470\r\nFailed to append const node 66470\r\nFailed to append const node 66471\r\nFailed to append const node 66471\r\nFailed to append const node 66441\r\nFailed to append const node 66441\r\nFailed to append const node 66442\r\nFailed to append const node 66442\r\nFailed to append const node 66443\r\nFailed to append const node 66443\r\nFailed to append const node 66444\r\nFailed to append const node 66444\r\nFailed to append const node 66445\r\nFailed to append const node 66445\r\nFailed to append const node 66446\r\nFailed to append const node 66446\r\nFailed to append const node 66493\r\nFailed to append const node 66493\r\nFailed to append const node 66515\r\nFailed to append const node 66515\r\nFailed to append const node 66516\r\nFailed to append const node 66516\r\nFailed to append const node 66517\r\nFailed to append const node 66517\r\nFailed to append const node 66518\r\nFailed to append const node 66518\r\nFailed to append const node 66519\r\nFailed to append const node 66519\r\nFailed to append const node 66520\r\nFailed to append const node 66520\r\nFailed to append const node 66521\r\nFailed to append const node 66521\r\nFailed to append const node 66522\r\nFailed to append const node 66522\r\nFailed to append const node 66523\r\nFailed to append const node 66523\r\nFailed to append const node 66524\r\nFailed to append const node 66524\r\nFailed to append const node 66495\r\nFailed to append const node 66495\r\nFailed to append const node 66496\r\nFailed to append const node 66496\r\nFailed to append const node 66497\r\nFailed to append const node 66497\r\nFailed to append const node 66498\r\nFailed to append const node 66498\r\nFailed to append const node 66499\r\nFailed to append const node 66499\r\nFailed to append const node 66500\r\nFailed to append const node 66500\r\nFailed to append const node 66501\r\nFailed to append const node 66501\r\nFailed to append const node 66502\r\nFailed to append const node 66502\r\nFailed to append const node 66503\r\nFailed to append const node 66503\r\nFailed to append const node 66504\r\nFailed to append const node 66504\r\nFailed to append const node 66505\r\nFailed to append const node 66505\r\nFailed to append const node 66506\r\nFailed to append const node 66506\r\nFailed to append const node 66507\r\nFailed to append const node 66507\r\nFailed to append const node 66508\r\nFailed to append const node 66508\r\nFailed to append const node 66509\r\nFailed to append const node 66509\r\nFailed to append const node 66510\r\nFailed to append const node 66510\r\nFailed to append const node 66511\r\nFailed to append const node 66511\r\nFailed to append const node 66512\r\nFailed to append const node 66512\r\nFailed to append const node 66513\r\nFailed to append const node 66513\r\nFailed to append const node 66514\r\nFailed to append const node 66514\r\nFailed to append const node 66494\r\nFailed to append const node 66494\r\nFailed to append const node 66526\r\nFailed to append const node 66526\r\nFailed to append const node 66568\r\nFailed to append const node 66568\r\nFailed to append const node 66569\r\nFailed to append const node 66569\r\nFailed to append const node 66570\r\nFailed to append const node 66570\r\nFailed to append const node 66571\r\nFailed to append const node 66571\r\nFailed to append const node 66572\r\nFailed to append const node 66572\r\nFailed to append const node 66553\r\nFailed to append const node 66553\r\nFailed to append const node 66554\r\nFailed to append const node 66554\r\nFailed to append const node 66555\r\nFailed to append const node 66555\r\nFailed to append const node 66556\r\nFailed to append const node 66556\r\nFailed to append const node 66557\r\nFailed to append const node 66557\r\nFailed to append const node 66563\r\nFailed to append const node 66563\r\nFailed to append const node 66564\r\nFailed to append const node 66564\r\nFailed to append const node 66565\r\nFailed to append const node 66565\r\nFailed to append const node 66566\r\nFailed to append const node 66566\r\nFailed to append const node 66567\r\nFailed to append const node 66567\r\nFailed to append const node 66558\r\nFailed to append const node 66558\r\nFailed to append const node 66559\r\nFailed to append const node 66559\r\nFailed to append const node 66560\r\nFailed to append const node 66560\r\nFailed to append const node 66561\r\nFailed to append const node 66561\r\nFailed to append const node 66562\r\nFailed to append const node 66562\r\nFailed to append const node 66533\r\nFailed to append const node 66533\r\nFailed to append const node 66534\r\nFailed to append const node 66534\r\nFailed to append const node 66535\r\nFailed to append const node 66535\r\nFailed to append const node 66536\r\nFailed to append const node 66536\r\nFailed to append const node 66537\r\nFailed to append const node 66537\r\nFailed to append const node 66538\r\nFailed to append const node 66538\r\nFailed to append const node 66539\r\nFailed to append const node 66539\r\nFailed to append const node 66540\r\nFailed to append const node 66540\r\nFailed to append const node 66541\r\nFailed to append const node 66541\r\nFailed to append const node 66542\r\nFailed to append const node 66542\r\nFailed to append const node 66548\r\nFailed to append const node 66548\r\nFailed to append const node 66549\r\nFailed to append const node 66549\r\nFailed to append const node 66550\r\nFailed to append const node 66550\r\nFailed to append const node 66551\r\nFailed to append const node 66551\r\nFailed to append const node 66552\r\nFailed to append const node 66552\r\nFailed to append const node 66543\r\nFailed to append const node 66543\r\nFailed to append const node 66544\r\nFailed to append const node 66544\r\nFailed to append const node 66545\r\nFailed to append const node 66545\r\nFailed to append const node 66546\r\nFailed to append const node 66546\r\nFailed to append const node 66547\r\nFailed to append const node 66547\r\nFailed to append const node 66527\r\nFailed to append const node 66527\r\nFailed to append const node 66528\r\nFailed to append const node 66528\r\nFailed to append const node 66529\r\nFailed to append const node 66529\r\nFailed to append const node 66530\r\nFailed to append const node 66530\r\nFailed to append const node 66531\r\nFailed to append const node 66531\r\nFailed to append const node 66532\r\nFailed to append const node 66532\r\nFailed to append const node 66574\r\nFailed to append const node 66574\r\nFailed to append const node 66616\r\nFailed to append const node 66616\r\nFailed to append const node 66617\r\nFailed to append const node 66617\r\nFailed to append const node 66618\r\nFailed to append const node 66618\r\nFailed to append const node 66619\r\nFailed to append const node 66619\r\nFailed to append const node 66620\r\nFailed to append const node 66620\r\nFailed to append const node 66601\r\nFailed to append const node 66601\r\nFailed to append const node 66602\r\nFailed to append const node 66602\r\nFailed to append const node 66603\r\nFailed to append const node 66603\r\nFailed to append const node 66604\r\nFailed to append const node 66604\r\nFailed to append const node 66605\r\nFailed to append const node 66605\r\nFailed to append const node 66611\r\nFailed to append const node 66611\r\nFailed to append const node 66612\r\nFailed to append const node 66612\r\nFailed to append const node 66613\r\nFailed to append const node 66613\r\nFailed to append const node 66614\r\nFailed to append const node 66614\r\nFailed to append const node 66615\r\nFailed to append const node 66615\r\nFailed to append const node 66606\r\nFailed to append const node 66606\r\nFailed to append const node 66607\r\nFailed to append const node 66607\r\nFailed to append const node 66608\r\nFailed to append const node 66608\r\nFailed to append const node 66609\r\nFailed to append const node 66609\r\nFailed to append const node 66610\r\nFailed to append const node 66610\r\nFailed to append const node 66581\r\nFailed to append const node 66581\r\nFailed to append const node 66582\r\nFailed to append const node 66582\r\nFailed to append const node 66583\r\nFailed to append const node 66583\r\nFailed to append const node 66584\r\nFailed to append const node 66584\r\nFailed to append const node 66585\r\nFailed to append const node 66585\r\nFailed to append const node 66586\r\nFailed to append const node 66586\r\nFailed to append const node 66587\r\nFailed to append const node 66587\r\nFailed to append const node 66588\r\nFailed to append const node 66588\r\nFailed to append const node 66589\r\nFailed to append const node 66589\r\nFailed to append const node 66590\r\nFailed to append const node 66590\r\nFailed to append const node 66596\r\nFailed to append const node 66596\r\nFailed to append const node 66597\r\nFailed to append const node 66597\r\nFailed to append const node 66598\r\nFailed to append const node 66598\r\nFailed to append const node 66599\r\nFailed to append const node 66599\r\nFailed to append const node 66600\r\nFailed to append const node 66600\r\nFailed to append const node 66591\r\nFailed to append const node 66591\r\nFailed to append const node 66592\r\nFailed to append const node 66592\r\nFailed to append const node 66593\r\nFailed to append const node 66593\r\nFailed to append const node 66594\r\nFailed to append const node 66594\r\nFailed to append const node 66595\r\nFailed to append const node 66595\r\nFailed to append const node 66575\r\nFailed to append const node 66575\r\nFailed to append const node 66576\r\nFailed to append const node 66576\r\nFailed to append const node 66577\r\nFailed to append const node 66577\r\nFailed to append const node 66578\r\nFailed to append const node 66578\r\nFailed to append const node 66579\r\nFailed to append const node 66579\r\nFailed to append const node 66580\r\nFailed to append const node 66580\r\nFailed to append const node 66622\r\nFailed to append const node 66622\r\nFailed to append const node 66623\r\nFailed to append const node 66623\r\nFailed to append const node 66624\r\nFailed to append const node 66624\r\nFailed to append const node 66625\r\nFailed to append const node 66625\r\nFailed to append const node 66626\r\nFailed to append const node 66626\r\nFailed to append const node 66628\r\nFailed to append const node 66628\r\nFailed to append const node 66627\r\nFailed to append const node 66627\r\nFailed to append const node 66629\r\nFailed to append const node 66629\r\nFailed to append const node 66630\r\nFailed to append const node 66630\r\nFailed to append const node 66631\r\nFailed to append const node 66631\r\nFailed to append const node 66632\r\nFailed to append const node 66632\r\nFailed to append const node 66633\r\nFailed to append const node 66633\r\nFailed to append const node 66634\r\nFailed to append const node 66634\r\nFailed to append const node 66635\r\nFailed to append const node 66635\r\nFailed to append const node 66640\r\nFailed to append const node 66640\r\nnative : hexagon_control_wrapper.cc:252 Setup graph completed\r\nPrepare failed! returned 0xffffffff\r\n\r\nDUMP HEXAGON LOG: \r\nExecute graph!\r\nExecution failed!\r\nexecute got err: -1\r\n\r\nExecution failed\r\nFailed to read data.\r\nnative : hexagon_graph_execution_test.cc:290 Output byte size = 4032\r\nnative : hexagon_graph_execution_test.cc:291 Output shape = [1,1008]\r\nnative : graph_transfer_utils.cc:46 === Dump ranking ===\r\nnative : graph_transfer_utils.cc:49 0: 1000, dumbbell, 0\r\nnative : graph_transfer_utils.cc:49 1: 999, carbonara, 0\r\nnative : graph_transfer_utils.cc:49 2: 998, stole, 0\r\nnative : graph_transfer_utils.cc:49 3: 997, rubber eraser, 0\r\nnative : graph_transfer_utils.cc:49 4: 996, coffee mug, 0\r\nnative : graph_transfer_utils.cc:49 5: 995, flagpole, 0\r\nnative : graph_transfer_utils.cc:49 6: 994, parallel bars, 0\r\nnative : graph_transfer_utils.cc:49 7: 993, cheeseburger, 0\r\nnative : graph_transfer_utils.cc:49 8: 992, bubble, 0\r\nnative : graph_transfer_utils.cc:49 9: 991, beaker, 0\r\nFinalize hexagon\r\n[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime (5996 ms)\r\n[----------] 1 test from GraphTransferer (5996 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 1 test from 1 test case ran. (5996 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\n\r\n 1 FAILED TEST\r\n\r\n\r\n``` \r\n", "@satok16 just wanted to update you that I rebuild from source (without using -p) and it seems to be working. Maybe the [linking](https://github.com/tensorflow/tensorflow/issues/11257) and the above issue had to do something with the mismatch of the libraries.", "Sure, thank you for the update!"]}]