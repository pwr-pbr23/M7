[{"number": 31946, "title": "Expose tf.keras.preprocessing.text.tokenizer_from_json to match keras API", "body": "Note: This fix is a a re-apply of #30062 \r\n\r\nPR #30062 fixed #30061 issue. However, due to an incompatibility issue it was reverted. Now since 2.0.0RC0 is released, this PR is re-applied to carry #30062 again.\r\n\r\nSee https://github.com/tensorflow/tensorflow/pull/30062#issuecomment-524014058 for related details.\r\n\r\n/cc @mihaimaruseac @mike-edmonds \r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["This still fails on Mac. I'll debug it mid September and get this fixed", "cool! ", "One work around is to add the line\r\ntokenizer_from_json = text.tokenizer_from_json\r\nin keras/preprocessing/text.py\r\n\r\nand do \r\nfrom keras.preprocessing.text import tokenizer_from_json\r\ninstead of\r\nfrom tensorflow.keras.preprocessing.text import tokenizer_from_json", "This if fixed now"]}, {"number": 31945, "title": "tf.custom_gradient does not handle variables correctly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf 1.13.1 and 1.13.2 tested\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0/7.3.1 and 7.4.2\r\n- GPU model and memory: Nvidia Quadro P2000\r\n\r\nWhen applying the custom gradient to a function which uses/creates variables, then it recognizes it through the gradient tape, however it does not find the `variables=None` argument in the gradient function, which results in an error.\r\n`TypeError: If using @custom_gradient with a function that uses variables, then grad_fn must accept a keyword argument 'variables'.`\r\n\r\nThe expected behavior would be of course, that the found variables are passed correctly to the underlying gradient function. In the following code snippet I expect the `print` statement to display `[<tf.Variable 'outside:0' shape=() dtype=float32>]`. In fact, I achieved this already by changing the custom_gradient functor at one line though I am not sure if this is wanted.\r\n\r\n```python\r\nimport tensorflow as tf\r\n# from custom_gradient import custom_gradient  # my corrected version\r\nfrom tensorflow import custom_gradient\r\n\r\n\r\ndef layer(t, name):\r\n    var = tf.Variable(1.0, dtype=tf.float32, use_resource=True, name=name)\r\n    return t * var\r\n\r\n\r\n@custom_gradient\r\ndef custom_gradient_layer(t):\r\n    result = layer(t, name='outside')\r\n\r\n    def grad(*grad_ys, variables=None):\r\n        assert variables is not None\r\n        print(variables)\r\n        grads = tf.gradients(\r\n            layer(t, name='inside'),\r\n            [t, *variables],\r\n            grad_ys=grad_ys,\r\n        )\r\n        grads = (grads[:1], grads[1:])\r\n        return grads\r\n\r\n    return result, grad\r\n\r\n\r\nvar = tf.Variable(0.0, dtype=tf.float32)\r\nresult = custom_gradient_layer(var)\r\ngrads = tf.gradients(result, var)[0]\r\n```\r\n\r\nThe line I changed is https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/ops/custom_gradient.py#L197-L198\r\nto\r\n```python\r\n  variables_in_signature = (\"variables\" in grad_argspec.args or\r\n                            grad_argspec.varkw or \"variables\" in grad_argspec.kwonlyargs)\r\n```\r\n", "comments": ["@DavidS3141, Thanks for finding the issue.\r\nI could able to reproduce the issue on Colab with Tensorflow 1.13.1 . Please take a look at gist [here](https://colab.research.google.com/drive/1lG5pJtuX7IDxxH-Sxt8SzqQ4S8e3A_Wk). Thanks!", "This looks like an improvement. Want to send a PR with your change and a unit test?", "I came across the same issue in TensorFlow 1.14. As a result, \r\n- `*grad_ys` cannot be used. It has to be replaced by a fixed number of arguments; \r\n- then the custom-gradient function must output a fixed number of tensors. \r\n\r\nHowever, in my case, the number of inputs and outputs to custom-gradient function should be changeable. \r\n\r\n**Workaround**\r\n\r\nAfter a deeper look at `tf_inspect.getfullargspec`, I got the below:\r\n```python\r\nfrom tensorflow.python.util.tf_inspect import getfullargspec\r\n\r\ndef grad_fn(*grad_ys, variables=None):\r\n    pass\r\n\r\ndef grad_fn2(grad_ys, variables=None):\r\n    pass\r\n\r\ndef grad_fn3(grad_ys, **variables):\r\n    pass\r\n\r\ndef grad_fn4(*grad_ys, **variables):\r\n    pass\r\n\r\n# The code below are used by TF to check variables\r\ngrad_argspec = getfullargspec(grad_fn)\r\ngrad_argspec2 = getfullargspec(grad_fn2)\r\ngrad_argspec3 = getfullargspec(grad_fn3)\r\ngrad_argspec4 = getfullargspec(grad_fn4)\r\nvariables_in_signature = (\"variables\" in grad_argspec.args or grad_argspec.varkw)\r\nvariables_in_signature2 = (\"variables\" in grad_argspec2.args or grad_argspec2.varkw)\r\nvariables_in_signature3 = (\"variables\" in grad_argspec3.args or grad_argspec3.varkw)\r\nvariables_in_signature4 = (\"variables\" in grad_argspec3.args or grad_argspec4.varkw)\r\n\r\n# note if \"not variables_in_signature\" is True, we'd have the TypeError\r\nprint(not variables_in_signature)  # True\r\nprint(not variables_in_signature2)  # False\r\nprint(not variables_in_signature3)  # False\r\nprint(not variables_in_signature4)  # False\r\n```\r\nThus, a workaround is to define:\r\n```python\r\ndef grad_fn(*grad_ys, **kwargs):\r\n    variables = kwargs.get('variables', None)\r\n```\r\nThis would result in a warning in case no variable is used, but no error at all time.\r\n```\r\n@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\r\n```\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31945\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "Was this bug ever fixed?\r\n\r\nI'm happy to do a PR if someone can tell me what unit test needs to be written.\r\n\r\nI'm having exact the same issue with custom gradients: my loss requires instantiation of a temp variable. The workaround above didn't work for me.\r\n\r\nWasn't able to fix locally and try it out because I can't do a local build due to some bazel issue similar to here, even though I've just downgraded to bazel 0.21.0: https://github.com/tensorflow/tensorflow/issues/21362", "Can you file a separate issue with instructions to reproduce your problem?\n\nOn Thu, Apr 16, 2020 at 1:42 AM Andre Zapico <notifications@github.com>\nwrote:\n\n> Was this bug ever fixed?\n>\n> I'm happy to do a PR if someone can tell me what unit test needs to be\n> written.\n>\n> I'm having exact the same issue with custom gradients: my loss requires\n> instantiation of a temp variable. The workaround above didn't work for me.\n>\n> Wasn't able to fix locally and try it out because I can't do a local build\n> due to some bazel issue similar to here, even though I've just downgraded\n> to bazel 0.21.0: #21362\n> <https://github.com/tensorflow/tensorflow/issues/21362>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31945#issuecomment-614504670>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHROGRRQHSPS7RWC5DTTRM3AI3ANCNFSM4IPF7DHA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Thanks, I've opened an issue with a minimal working example."]}, {"number": 31944, "title": "failed to bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windiws 2012 r2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): tried so many version\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: none \r\n- GPU model and memory: none\r\n\r\n**Describe the problem**\r\nHi, I followed the instruction on [Tensorflow](https://www.tensorflow.org/install/source_windows)\r\nI had a hard time when doing this command\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n(I also found others said: change to -c opt, but not working either)\r\nmost of the error said \r\n**FAILED: Build did NOT complete successfully**\r\n\r\nI have tried many different versions of Bazel, from 0.18.0 to 0.28.1\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nfollow the instruction on the website\r\n\r\n1. install python and tensorflow: which I already have\r\n2. install bazel and add to %path%\r\n3. install msys2 and add to %path%\r\n4. install visual c++ build tools\r\n5. python ./configure.py\r\n6. bazel build error\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nthanks a lot!!", "comments": ["![image](https://user-images.githubusercontent.com/51321052/63635678-97c2d400-c697-11e9-8395-224bca43207a.png)\r\n![image](https://user-images.githubusercontent.com/51321052/63635685-c476eb80-c697-11e9-9218-6a372b2f8125.png)\r\n", "[according to this](https://github.com/tensorflow/tensorflow/issues/30556#issuecomment-510220573)\r\nMy tensorflow version is 1.13.1, so I installed bazel 0.20.0\r\nand run `python ./configure.py`\r\n![image](https://user-images.githubusercontent.com/51321052/63635713-4b2bc880-c698-11e9-91c2-5d0ebec0f8fa.png)\r\nhow did this happen?\r\n\r\nafter I tried the latest version of bazel, frankly, it could run configure.py\r\nhowever, when I ran \r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package` \r\nor\r\n`bazel build --c opt //tensorflow/tools/pip_package:build_pip_package` \r\nthis error occured\r\n`bazel-out/x64_windows-opt/bin/external/com_google_protobuf/src: warning: directo\r\nry does not exist.\r\nbazel-out/x64_windows-opt/bin/external/com_google_protobuf/src: warning: directo\r\nry does not exist.\r\nERROR: C:/users/administrator/desktop/tensorflow/tensorflow-master/tensorflow/co\r\nre/BUILD:2301:1: output 'tensorflow/core/protobuf/conv_autotuning.pb.h' was not\r\ncreated\r\nERROR: C:/users/administrator/desktop/tensorflow/tensorflow-master/tensorflow/co\r\nre/BUILD:2301:1: output 'tensorflow/core/protobuf/conv_autotuning.pb.cc' was not\r\n created\r\nERROR: C:/users/administrator/desktop/tensorflow/tensorflow-master/tensorflow/co\r\nre/BUILD:2301:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 181.594s, Critical Path: 25.73s\r\nINFO: 509 processes: 509 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully`\r\n\r\nthis time hold a little bit longer, but still failed", "I think this is [now] the same error I reported a couple of weeks back here https://github.com/tensorflow/tensorflow/issues/30999. I've failed to see any evidence any version since 1.10 builds on windows, this is also the latest version that has a pre-compiled binary for windows.", "Thanks for the update. I'm closing this in favor of #30999 \r\n\r\nNote that TF does [provide pre-compiled binaries on Windows](https://www.tensorflow.org/install), which you can install with Pip.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31944\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31944\">No</a>\n"]}, {"number": 31943, "title": "[TF 2.0.0-rc0] Run model.evaluate() let  notebook crash (Chrome).", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Mac\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): Tensorflow 2.0.0-rc0\r\n- Python version: 3.6.5\r\n\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\nv2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0\r\n\r\n**Describe the current behavior**\r\nWhen I run `result = model.evaluate(x_train, y_train), my jupyter notebook crashed.\r\n\r\n**Describe the expected behavior**\r\nI will be run another cell in jupyter notebook, but I can't.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n \r\nI use sample code in Tensorflow 2.0.0 RC: Classify images\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["I found crash only happens in Chrome browser.\r\n\r\nIn Firefox, as below:\r\n\r\n![Screenshot from 2019-08-26 13-53-56](https://user-images.githubusercontent.com/20853096/63668267-315dc300-c809-11e9-812f-6b665c13ab47.png)\r\n", "@kaka-lin \r\n\r\nI tried in Google colab and Jupyter notebook with Google chrome and able to execute the code successfully.Can you please upgrade your google chrome version and see if the problem still persists.Thanks!", "Hi, @ravikyram:\r\n\r\nI upgrade my google chrome version: Version 76.0.3809.132 (Official Build) (64-bit)\r\nBut the problem is still persistent.\r\n\r\nI record a demo video, pls check it thanks.\r\n\r\nTF2 - beta1:\r\n\r\nhttps://drive.google.com/file/d/1P5s-ROhQJq4_a6WyrPEdCQqUGBarYLAf/view?usp=sharing\r\n\r\nTF2 - rc0:\r\n\r\nhttps://drive.google.com/file/d/1G7MFn53t-YMFgcFNbZXZk06P921-Rid0/view?usp=sharing\r\n\r\n\r\n", "I have the same problem.", "@kaka-lin \r\nAs per my understanding in Google chrome with TF2 - beta1 version you are able to execute the code successfully .But with Google chrome using TF2 - rc0: you are facing the issue. Am i correct?\r\nThanks!", "@ravikyram  \r\n\r\nYes! ", "I tried reproducing the issue using Google chrome in Jupyter notebook and was able to execute the code with TF2 - beta1.But i am facing the issue with TF2 - rc0.Please, find the screenshot below.Thanks!\r\n![error](https://user-images.githubusercontent.com/51902062/63920602-dc1aef00-ca5e-11e9-93bd-18c6dd457d30.png)\r\n", "@kaka-lin I think this was resolved in the recent TF versions. I checked with `TF2.0` and `tf-nightly` and I cannot reproduce the issue. \r\n\r\nI am closing the issue. Please feel free to reopen the issue if it persists for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31943\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31943\">No</a>\n"]}, {"number": 31942, "title": "win10 Creating a Virtual Environment", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):window10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):do not install \r\n- TensorFlow version:do not install \r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.3 \r\n- GPU model and memory: Geforce GTX 1060\r\n\r\n\r\n\r\n**Describe the problem**\r\nthe command do not run.\r\nC:\\>virtualenv --system-site-packages -p python3 ./venv\r\nThe path python3 (from --python=python3) does not exist\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAccording to tenserflow website install tenserflow with pip in windows10 system. Python3.6 was installed in my computer.\r\n\r\nC:\\>pip3 --version\r\npip 19.2.2 from c:\\program files\\python36\\lib\\site-packages\\pip (python 3.6)\r\n\r\nC:\\>virtualenv --version\r\n16.7.4\r\n\r\nC:\\>virtualenv --system-site-packages -p python3 ./venv\r\nThe path python3 (from --python=python3) does not exist\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@zouhanting,\r\nPlease follow the steps below to install Tensorflow using pip and by creating virtual env\r\n```\r\npip install virtualenv \r\nvirtualenv tensorflow     #virtual env name : tensorflow\r\ntensorflow\\Scripts\\activate\r\ntensorflow>>pip install tensorflow==1.14.0   #mention version of your interest\r\n\r\n```\r\nLet us know how it progresses. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31942\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31942\">No</a>\n"]}, {"number": 31941, "title": "r2.0-CherryPick", "body": "The fix will correct the generation of 'minimum runtime version' in TF Lite's flatbuffer model.", "comments": []}, {"number": 31940, "title": "Where is stop_if_no_increase_hook gone?", "body": "I use tensorflow.contrib.estimator.stop_if_no_increase_hook in my code to perform an early stopping, which works smoothly before. But today it throws out an error:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n\r\nhttps://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\nhttps://github.com/tensorflow/addons\r\nhttps://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\nTraceback (most recent call last):\r\nFile \".\\main.py\", line 215, in \r\nhook = tf.contrib.estimator.stop_if_no_increase_hook(\r\nAttributeError: module 'tensorflow.contrib.estimator' has no attribute 'stop_if_no_increase_hook'\r\n\r\nI noticed that estimator has been moved to tensorflow/estimator. But it doesn't support this method now. Would you tell me where to find it or other workaround to perform early stoppoing?", "comments": ["Should be here? `tf.estimator.experimental.stop_if_no_increase_hook`\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/experimental/stop_if_no_increase_hook", "@WindQAQ Thank you so much\uff01", "I am closing this issue since the query is been answered. Please, feel free to reopen if the issue is still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31940\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31940\">No</a>\n"]}, {"number": 31939, "title": "Pip install can't find tensorflow2.0.0rc0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope\r\n- TensorFlow installed from (source or binary): wheel file via pip\r\n- TensorFlow version: tensorflow==2.0.0rc0\r\n- Python version: python3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): not source\r\n- GCC/Compiler version (if compiling from source): not source\r\n- CUDA/cuDNN version: not GPU\r\n- GPU model and memory: not GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nWhen you pip install tensorflow==2.0.0rc0, you get an error message saying that pip can't find a matching version of the distribution that't given.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nStep 5/11 : RUN /usr/local/bin/algorithmia-build\r\n ---> Running in 229b8273366c\r\nCollecting algorithmia<2.0,>=1.0.0 (from -r requirements.txt (line 1))\r\n  Downloading https://files.pythonhosted.org/packages/12/ae/38a82aae155a42261621eaba0a878959a0af93bf2d70d0ba98048ddaa3d2/algorithmia-1.2.0-py2.py3-none-any.whl\r\nRequirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.12.0)\r\nCollecting pip==19.2.2 (from -r requirements.txt (line 3))\r\n  Downloading https://files.pythonhosted.org/packages/8d/07/f7d7ced2f97ca3098c16565efbe6b15fafcba53e8d9bdb431e09140514b0/pip-19.2.2-py2.py3-none-any.whl (1.4MB)\r\nCollecting tensorflow-gpu==2.0.0rc0 (from -r requirements.txt (line 4))\r\n\u001b[91m  Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0rc0 (from -r requirements.txt (line 4)) (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\n\u001b[0m\u001b[91mNo matching distribution found for tensorflow-gpu==2.0.0rc0 (from -r requirements.txt (line 4))\r\n\u001b[0m\u001b[91mYou are using pip version 18.1, however version 19.2.2 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n\u001b[0mRemoving intermediate container 229b8273366c\r\nThe command '/bin/sh -c /usr/local/bin/algorithmia-build' returned a non-zero code: 1\r\n```", "comments": ["Looks like `tensorflow2.0.0rc0` moved away from the `manylinux1` tag to the `manylinux2010` tag.\r\n\r\nThe `manylinux2010` tag is only supported w/ pip version `19` and above.\r\n\r\nTo fix this bug/issue, update pip via:\r\n\r\n```\r\npip install -U pip\r\n```\r\n\r\nFore more info: https://discuss.python.org/t/the-next-manylinux-specification/1043", "I'm using pip 19.2.2 and I get the same error.", "Run this command, this worked for me.\r\n\r\n`pip install --upgrade tensorflow-gpu==2.0.0-rc0`", "@besirkurtulmus,\r\nFor Tensorflow CPU version try,\r\n`pip install --upgrade tensorflow==2.0.0-rc0`\r\nLet us know if that helps. Thanks!", "After upgrade pip by `python -m pip install --upgrade pip --user`. I could run `pip install --upgrade tensorflow-gpu==2.0.0-rc0`", "Yes, newer versions of TensorFlow require modern pip", "> @besirkurtulmus,\r\n> For Tensorflow CPU version try,\r\n> `pip install --upgrade tensorflow==2.0.0-rc0`\r\n> Let us know if that helps. Thanks!\r\n\r\nThis now works. Did you guys make a fix?", "I am having this exact issue right now\r\nOS - Windows 7 64 bit\r\nPIP version 19.2.3\r\nPython version 3.7.4\r\n\r\n`pip install --upgrade tensorflow==2.0.0-rc0`\r\n`Collecting tensorflow==2.0.0-rc0`\r\n`  ERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0-rc0 (from versions: none)`\r\n`ERROR: No matching distribution found for tensorflow==2.0.0-rc0`", "Can you try `pip install --upgrade -v tensorflow==2.0.0rc0` and post the full log?", "https://pastebin.com/QKMjueDJ", "Just to be sure, can you also try `python3 -m pip install --upgrade pip && python3 -m pip install --upgrade -v tensorflow==2.0.0rc0` and post the full log there?\r\n\r\nI have a nagging feeling that `pip` is running from a different interpreter than the one you're installing TF from", "Powershell says `python3` isnt a command i can run...", "PS C:\\Users\\Seva> python --version\r\nPython 3.7.4\r\nPS C:\\Users\\Seva> python3\r\nThe term 'python3' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spe\r\nlling of the name, or if a path was included, verify that the path is correct and try again.\r\nAt line:1 char:8\r\n+ python3 <<<<\r\n    + CategoryInfo          : ObjectNotFound: (python3:String) [], CommandNotFoundException\r\n    + FullyQualifiedErrorId : CommandNotFoundException\r\n\r\nPS C:\\Users\\Seva>\r\n\r\nreally odd", "Having the exact same issue. I'm windows. ", "I figure it's because I only have py -3 available, so running `python` is equivalent to `python3`\r\nRegardless, I ran that command as `python -m pip install --upgrade pip && python -m pip install --upgrade -v tensorflow==2.0.0rc0` and got the same error, other than pip telling me it was already up to date", "Tried that too. ", "These all seem issues with the environment. I'd suggest looking at the detailed logs from `python -m pip install --upgrade -vvv tensorflow==2.0.0rc0` (3 `v`s, the maximum verbosity level).", "@charmscale, Did you get a chance to look @mihaimaruseac's comment. Thanks!", "\r\nI am also getting similar issue... \r\n![image](https://user-images.githubusercontent.com/49004578/64326664-a9c34180-cfca-11e9-8ffc-a678317fc67b.png)\r\n", "`python -m pip install --upgrade pip && python -m pip install --upgrade setuptools && python -m pip install --upgrade tensorflow`\r\n\r\nIf this still fails, add `-vvv` just before `tensorflow` and post the full log _as attachment_, not as image", "Thanks for suggestion, but it didn't worked. Please find below attached logs : - \r\nhttps://pastebin.com/wmHbFzqn ", "Can you post the output of `pip debug --verbose` please?", "@Indseta it's because you're using a 32 bits CPU and we're releasing for 64 bits CPUs", "But by system has 64 bit OS  & Processor\r\n![image](https://user-images.githubusercontent.com/49004578/64472489-85e53480-d15f-11e9-82c5-60d1f6ecbd12.png)\r\n", "> `python -m pip install --upgrade pip && python -m pip install --upgrade setuptools && python -m pip install --upgrade tensorflow`\r\n> \r\n> If this still fails, add `-vvv` just before `tensorflow` and post the full log _as attachment_, not as image\r\n\r\nhttps://pastebin.com/kE0WMwzA\r\n\r\n\r\n\r\n> pip debug --verbose\r\n\r\nhttps://pastebin.com/dUTzee3Z\r\n\r\nI am trying to install tensorflow in a virtualenvironment through anaconda prompt. I am a beginner so please suggest accordingly.\r\nMy system details are as follows:-\r\n![image](https://user-images.githubusercontent.com/11918843/64475277-51dd3400-d19e-11e9-8728-3c2ff31074ca.png)\r\n", "I think in both of these cases you installed a 32 bits Python. Please install a 64 bits one if your CPU is 64 bits.\r\n\r\nFor 32 bits support see #32315", "Thanks for your suggestion.\r\nWill same solution also work for libraries OpenCV and face_recognition ? \r\nBecause both libraries are having similar installation issues on my system or any other solution will work for them to get fixed. Thanks.", "Recommend StackOverflow for this, no idea if same solution would work there and cannot extrapolate.", "> Thanks for your suggestion.\r\n> Will same solution also work for libraries OpenCV and face_recognition ?\r\n> Because both libraries are having similar installation issues on my system or any other solution will work for them to get fixed. Thanks.\r\n\r\nIf you are having issues with face_recognition because of mtcnn module I suggest using my fix, it works for TensorFlow 2.0.\r\n\r\nhttps://github.com/reliefs/mtcnn-tensorflow2\r\n\r\n> Recommend StackOverflow for this, no idea if same solution would work there and cannot extrapolate.\r\n\r\nNo issues with converting these packages from 1.x to 2.0, the groundwork for fast deployment of packages which use old versions has been done really well. ", "Looks like original issue reported here is resolved. Please feel free to reopen if issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31939\">No</a>\n", "I reinstalled python and was able to run the two commands for upgrading pip and then installing tensorflow. ", "> I think in both of these cases you installed a 32 bits Python. Please install a 64 bits one if your CPU is 64 bits.\r\n> \r\n> For 32 bits support see #32315\r\n\r\nThis ans is right please check python version ", "I was also facing the same issue, essentially the reason was tensorflow wheel file is not available for 32 bit system. They also not planning to publish 32 bit version this year, as it takes lots of effort at their end and it was not in their planning.\r\n\r\nSo, I solved it by changing my installation of python 32 bit to python 64 bit.", "As mentioned: if you really have a 32bits CPU (unlikely as these are very old), see #32315 \r\n\r\nIf your CPU is 64 bits, please install a 64 bits Python.", "https://github.com/tensorflow/tensorflow/issues/32627", "On my windows machine, I was facing same issue while running _pip install tensorflow==2.0.0-rc1_ though I was on latest version of pip and on 64-bit distribution for python.\r\nOddly enough, I face this issue when I am running the pip install inside a virtual environment. If i deactivate from my virtual env. I do not get this issue.\r\n\r\nWill continue to investigate if I could find anything more.\r\n\r\nThanks\r\n", "@rsjain1978 Can you check if you have both 32 bits and 64 bits binaries in your system? It's very likely that somehow the virtualenv python is 32 bits.\r\n\r\nAlternatively, you can compare output of `pip debug --verbose` outside and inside the virtualenv. That should point out these differences very quickly.", "C:\\Users\\User\\Desktop\\number_recognition\\venv\\Scripts\\python.exe \"C:/Users/User/Desktop/number_recognition/number _recog.py\"\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/User/Desktop/number_recognition/number _recog.py\", line 6, in <module>\r\n    from keras.preprocessing.image import ImageDataGenerator\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\User\\Desktop\\number_recognition\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n"]}, {"number": 31938, "title": "tf.function decorator with GradientTape >10x slower than tf.keras.model.Models.fit()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, reused code from https://www.tensorflow.org/beta/guide/effective_tf2 with minor modifications \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (originally found on Ubuntu 18.04)\r\n- TensorFlow version (use command below): 2\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using @tf.function decorator with  tf.GradientTape to compute updates on a model, I find that it takes 110 seconds per epoch. When I used model.fit(dataset), it only takes ~7 seconds. I'm not sure why it takes so much longer using the custom train function. This is code copied over from the Effective TensorFlow 2 documentation on tensorflow.org. To make sure this not specific to my personal machine, I used google colab with GPU enabled under Notebook settings. \r\n\r\n**Describe the expected behavior**\r\nI would expect that using @tf.function decorator for the train function would compute the gradient update steps in a similar amount of time as model.fit(). I am wondering if the model is not being put onto GPU and is kept on CPU when trained this way.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n\r\nimport tensorflow as tf\r\nimport time\r\n\r\n\r\nclass MyModel(tf.keras.models.Model):\r\n\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\r\n        self.flatten = tf.keras.layers.Flatten()\r\n        self.d1 = tf.keras.layers.Dense(128, activation='relu')\r\n        self.d2 = tf.keras.layers.Dense(10, activation='softmax')\r\n        self.times_called = tf.Variable(0.0, trainable=False)\r\n\r\n    def call(self, x):\r\n        self.times_called.assign_add(1)\r\n        x = self.conv1(x)\r\n        x = self.flatten(x)\r\n        x = self.d1(x)\r\n        return self.d2(x)\r\n\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n@tf.function\r\ndef train_step(model, dataset):\r\n    for images, labels in dataset:\r\n        with tf.GradientTape() as tape:\r\n            preds = model(images)\r\n            loss = loss_object(labels, preds)\r\n        gradients = tape.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\r\n\r\n    model = MyModel()\r\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\r\n    model.fit(train_ds)\r\n\r\n    model2 = MyModel()\r\n    start = time.time()\r\n    train_step(model2, train_ds)\r\n    print(model.times_called)\r\n    print(time.time() - start)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n\r\nThis is the output I get when running this code on google colab:\r\n```\r\n1563/1563 [==============================] - 7s 4ms/step - loss: 1.4598\r\n```\r\nTime it takes to run with @tf.function \r\n```\r\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1563.0>\r\n110.49453139305115\r\n```\r\n\r\n", "comments": ["@tf.function should not be around the whole epoch: `for images, labels in dataset:`\r\nMove the loop outside the function and it will fix your issue", "Hi gaborchris,\r\nAs suggested by DEKHTIARJonathan change your code like this it will be faster than model.fit.\r\n\r\n\r\n```\r\n@tf.function\r\ndef train_step(model, image, labels):\r\n    with tf.GradientTape() as tape:\r\n        preds = model(images)\r\n        loss = loss_object(labels, preds)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```\r\n\r\n```\r\nfor images, labels in train_ds:\r\n    train_step(model2, images, labels)\r\n```\r\n```\r\n1563/1563 [==============================] - 7s 4ms/step - loss: 1.4690\r\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1563.0>\r\n5.4s\r\n```", "@gaborchris ,\r\nCan you please try the solution provided by @DEKHTIARJonathan and @akanyaani.Thanks!", "@akanyaani \r\nThank you, you're solution worked and it appears this is not a bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31938\">No</a>\n"]}, {"number": 31937, "title": "r2.0-Cherrypick:Makes `nest` able to flatten dictionary views (produced by dict.items\u2026", "body": "\u2026(), dict.values(), dict.keys() in Python3.)\r\n\r\nThis is done for all mapping views rather than just ordereddict views because:\r\n1. In python2 these all returned lists anyway so nest worked on them even if dictionary ordering wasn't guaranteed\r\n2. In python 3.6 dictionaries became insertion-ordered as an implementation detail, and as of python 3.7 this became a language feature: https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6\r\n\r\nSo, this would only pose a not-already-present randomization risk with nest.flatten for:\r\n- people using python3 with Custom mappings or built-in mappings that don't have order guarantees (I'm not sure if there are still built-in mappings that don't have order guarantees)\r\n- people using dict views with older python3 versions that are < 3.6\r\n\r\nNote: This cl makes nest.pack_sequence_as with views as structures return a list rather than a mapping view, because you cannot directly instantiate built-in mapping views.\r\nPiperOrigin-RevId: 264433983", "comments": ["@tomerk can you please take a look at the failing builds."]}, {"number": 31936, "title": "[Intel MKL] Revert \"Export the utils functions from C++ to Python with pybind11 i\u2026", "body": "The commit uses pywrap_utils which has a libiomp5 dependency and the LD_LIBRARY_PATH is not updated to reflect this dependency. Reverting it to fix the issue.\r\n\r\nThis reverts commit da3f7b14fff64c492a93305ccc98b70fafdd9dde.\r\n\r\n\"Export the utils functions from C++ to Python with pybind11 instead of swig. This is part of a larger effort to deprecate swig and eventually with modularization break pywrap_tensorflow into smaller components. It will also make exporting C++ ops to Python significantly easier. XLA and MLIR are using the pybind11 macros already. Please refer to https://github.com/tensorflow/community/blob/master/rfcs/20190208-pybind11.md for more information.\"", "comments": ["Trying to See if there is a solution to fix this dependency and open a smaller PR. Closing it for now.", "Thanks for checking for a forward fix @nammbash ", "@av8ramit   New PR https://github.com/tensorflow/tensorflow/pull/31955 has been submitted to fix the regressions"]}, {"number": 31935, "title": "Simple way to manage and release GPU memory in colab", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab Ubuntu 18.04.2 LTS (Bionic Beaver)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Google Colab has tensorflow preinstalled\r\n- TensorFlow version (use command below): tensorflow-gpu 1.14.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130\r\n- GPU model and memory: Google Colab GPU Tesla T4, Memory: 15079MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nFor my research work I need to build lots of different convolutional GAN models and train them.\r\nFor this I build temporary models inside functions and test them. Once the function is done executing the models are no longer needed.\r\nA simple example:\r\n```python\r\ndef test_model():\r\n   model = build_model()\r\n   inputs = ...\r\n   outputs = model(inputs)\r\n   with tf.Session() as sess:\r\n      sess.run(tf.global_variables_initializer())\r\n      results = sess.run(outputs)\r\n   # evaluate results\r\n\r\ntest_model() # call the test function, once this is over we never use the model again.\r\n```\r\nHowever currently the models persist (can be seen in %tensorboard) and continue to fill up\r\nGPU memory. Eventually I start getting warning about GPU memory usage and start getting OOM errors. At this point I can't build new models or train any existing ones.\r\n\r\nI have already tried lots of different suggestions on how to release GPU memory\r\nhttps://github.com/tensorflow/tensorflow/issues/1578\r\nhttps://github.com/tensorflow/tensorflow/issues/19731\r\nhttps://github.com/tensorflow/tensorflow/issues/17048\r\nand several stackoverflow suggestions to no effect.\r\n\r\nThis problem is specific to a Jupter notebook based workflow (such as on Google Colab).\r\nA workflow that uses python files will not encounter this issue since all the GPU memory is released automatically once the python interpreter finishes.\r\n\r\n**Describe the expected behavior**\r\n- Please implement or suggest a way to release GPU memory being used by unneeded models in Google Colab/Jupter notebooks.\r\n- It would be nice to be able to release memory being used by specific models (that are no longer necessary) rather than resetting the runtime every time I run out of memory (which is often).\r\n- Some way to build models so that the GPU memory they occupy gets automatically released when they go out of scope would also be appreciated.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@HarikrishnanBalagopal I was running 10 training loops on colab and running into OOM errors after the 8th iteration. Putting the following at the end of the loop helped.\r\n`tf.reset_default_graph()`", "@HarikrishnanBalagopal Did you try @jmwoloso Suggestion. In some cases, you could also use `tf.keras.backend.clear_session()` to release the session memory. Thanks!", "I am currently calling ` tf.reset_default_graph()` if I ever pollute the default graph and changed my code to:\r\n```python\r\ndef test_model():\r\n   g_1 = tf.Graph()\r\n   with g_1.as_default():\r\n      model = build_model()\r\n      inputs = ...\r\n      outputs = model(inputs)\r\n   with g_1.as_default(), tf.Session() as sess:\r\n      sess.run(tf.global_variables_initializer())\r\n      results = sess.run(outputs)\r\n      # evaluate results\r\n\r\ntest_model() # call the test function, once this is over we never use the model again.\r\n```\r\nwhich seems to work fine so far. The graph gets garbage collected and haven't had any OOM errors yet, so I am closing the issue."]}, {"number": 31934, "title": "Update API docs for ParallelInterleaveDataset", "body": "This pull request updates the API docs for the `ParallelInterleaveDataset` op. I added documentation for the inputs and arguments, added a reference to the Python API call that creates instances of this dataset, and added a note that there is a newer implementation under the name `ParallelInterleaveDatasetV2`.", "comments": []}, {"number": 31933, "title": "[r1.15 cherrypick] Add a note to ignore dlerror for CPU-only pip package", "body": "\u2026mically opening cudart fails.\r\n\r\nPiperOrigin-RevId: 264940821", "comments": []}, {"number": 31931, "title": "2019-08-23 nightlies missing `tf.summary` symbols", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n#### System information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `tf-nightly-2.0-preview==2.0.0.dev20190823`\r\n- TensorFlow version (use command below): `2.0.0-dev20190823`\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n#### Describe the current behavior\r\n\r\n```\r\n$ virtualenv -q -p python3.6 ./ve\r\n$ . ./ve/bin/activate\r\n(ve) $ pip install -q tf-nightly-2.0-preview==2.0.0.dev20190823\r\n(ve) $ python\r\nPython 3.6.7 (default, Oct 21 2018, 08:08:16) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0-dev20190823'\r\n>>> tf.summary.create_file_writer\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'create_file_writer'\r\n>>> tf.summary.write\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorboard.summary._tf.summary' has no attribute 'write'\r\n```\r\n\r\n#### Describe the expected behavior\r\n\r\nThese symbols should exist (they\u2019re documented, for one).\r\n\r\n#### Code to reproduce the issue\r\n\r\nSee above.\r\n\r\n#### Other info / logs\r\n\r\nNot due to a regression in TensorBoard; our nightlies did not ship today\r\nbecause our smoke test caught this.\r\n", "comments": ["cc @mihaimaruseac ; anything new here?\r\n", "Googlers: <http://b/139924752>\r\n", "This seems to be caused by bad interaction between virtual pip, lazy loading and relative imports. ", "A change to TensorBoard to pick up the new APIs should fix this:\r\n<https://github.com/tensorflow/tensorboard/pull/2593>\r\n\r\nI\u2019ll re-trigger today\u2019s `tb-nightly` upload, which should resolve the\r\nproblem for future installs.\r\n", "Fixed:\r\n\r\n```\r\n$ virtualenv -q -p python3.6 ./ve\r\n$ . ./ve/bin/activate\r\n(ve) $ pip install -q -I tf-nightly-2.0-preview==2.0.0.dev20190823\r\n(ve) $ pip freeze | grep -e tf-nightly -e tb-nightly\r\ntb-nightly==1.15.0a20190823\r\ntf-nightly-2.0-preview==2.0.0.dev20190823\r\n(ve) $ python\r\nPython 3.6.7 (default, Oct 21 2018, 08:08:16) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0-dev20190823'\r\n>>> tf.summary.create_file_writer\r\n<function create_file_writer_v2 at 0x7fac87b68bf8>\r\n>>> tf.summary.write\r\n<function write at 0x7fac87b6c1e0>\r\n```"]}, {"number": 31930, "title": "[TF 2.0] Different results of binary cross entropy loss of the same architecture ", "body": "**System information**\r\n- Tensorflow 2.0\r\n\r\n**Describe the current behavior**\r\nI have created model from tf.keras.models for binary classification problem. It is dense with sigmoid activation in the last layer and binary cross entropy loss. I got different loss values whether my last layer was:\r\n```last_layer = Dense(1, activation='sigmoid')(previous_layer)```\r\nor\r\n```\r\nlast_layer = Dense(1)(previous_layer)\r\nlast_layer = sigmoid(last_layer)\r\n```\r\n\r\nIf `loss=BinaryCrossentropy(from_logits=False)`:\r\n-In the first case BCE do apply sigmoid before computing loss.\r\n-In the second case BCE do not apply sigmoid before computing loss.\r\n\r\n**Describe the expected behavior**\r\nI believe that from user point of view it would consistent if both examples gives exactly the same output- there is no difference in architecture.\r\n\r\nIt would be better if sigmoid is applied, no matter what are the initialization arguments of BinaryCrossentropy.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.keras.activations import sigmoid\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.losses import BinaryCrossentropy\r\nimport numpy as np\r\n\r\n\r\nx_train = np.array([[1000]])\r\ny_train = [0]\r\n\r\ninp = Input(x_train.shape[1])\r\nout = Dense(1, trainable=False, use_bias=False)(inp)\r\nmodel = Model(inp, out)\r\nmodel.get_layer('dense').set_weights([np.array([[1]])])\r\nmodel.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=True))\r\n\r\nprint('\\nFrom logits=True, no activation')\r\nmodel.fit(x_train, y_train, epochs=1)\r\nprint('Prediction:')\r\nprint(model.predict(x_train))\r\n\r\n\r\ninp = Input(x_train.shape[1])\r\nout = Dense(1, trainable=False, use_bias=False)(inp)\r\nout = sigmoid(out)\r\nmodel = Model(inp, out)\r\nmodel.get_layer('dense_1').set_weights([np.array([[1]])])\r\nmodel.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=False))\r\n\r\nprint('\\nFrom logits=False, separate sigmoid')\r\nmodel.fit(x_train, y_train, epochs=1)\r\nprint('Prediction:')\r\nprint(model.predict(x_train))\r\n\r\n\r\ninp = Input(x_train.shape[1])\r\nout = Dense(1, trainable=False, use_bias=False, activation='sigmoid')(inp)\r\nmodel = Model(inp, out)\r\nmodel.get_layer('dense_2').set_weights([np.array([[1]])])\r\nmodel.compile(optimizer='adam', loss=BinaryCrossentropy(from_logits=False))\r\n\r\nprint('\\nFrom logits=False, sigmoid in dense')\r\nmodel.fit(x_train, y_train, epochs=1)\r\nprint('Prediction:')\r\nprint(model.predict(x_train))\r\n```\r\n\r\n\r\nOutput tf 2.0:\r\n```\r\nFrom logits=True, no activation\r\n1/1 [==============================] - 0s 17ms/sample - loss: 1000.0000\r\nPrediction:\r\n[[1000.]]\r\n\r\nFrom logits=False, separate sigmoid\r\nTrain on 1 samples\r\n1/1 [==============================] - 0s 15ms/sample - loss: 1000.0000\r\nPrediction:\r\n[[1.]]\r\n\r\nFrom logits=False, sigmoid in dense\r\nTrain on 1 samples\r\n1/1 [==============================] - 0s 17ms/sample - loss: 15.3332\r\nPrediction:\r\n[[1.]]\r\n```\r\n\r\noutput tf 1.14\r\n\r\n```\r\nFrom logits=True, no activation\r\n1/1 [==============================] - 0s 39ms/sample - loss: 1000.0000\r\nPrediction:\r\n[[1000.]]\r\n\r\nFrom logits=False, separate sigmoid\r\n1/1 [==============================] - 0s 44ms/sample - loss: 1000.0000\r\nPrediction:\r\n[[1.]]\r\n\r\nFrom logits=False, sigmoid in dense\r\n1/1 [==============================] - 0s 57ms/sample - loss: 1000.0000\r\nPrediction:\r\n[[1.]]\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jacekblaz Can you please provide the github gist of the code for TF1.14 and Tf2.0 as I am unable to reproduce it. Here's my [github gist](https://colab.research.google.com/gist/gowthamkpr/fa90e3907b8e1b8257320f1a7eca66f9/untitled.ipynb)", "@gowthamkpr \r\n[gist](https://colab.research.google.com/gist/jacekblaz/1b860965969dca734a7618c2009bf018/untitled.ipynb)", "Thank you for the issue, i will look into this. I believe that some change was made when `from_logits=False` and last layer is `sigmoid` to use the `logits` from the previous layer. I am suspecting there is some issue there.", "I could not repro this on the latest nightly, are you still seeing this?", "@pavithrasv Heres the [github gist](https://colab.sandbox.google.com/gist/gowthamkpr/6ed52d1b5bb68a69094239530af6390c/copy-of-untitled.ipynb#scrollTo=VJdNRa91YqTN) of the issue. I am encountering an error in TF 2.0 nightly but not in TF 2.0b1. ", "@gowthamkpr I believe you are encountering the issue in TF 2.0b1- there is different output for the same architecture in your gist. This issue is not about error ValueError caused by numpy. \r\n\r\n@pavithrasv I have tried to reproduce this error in current nightly as well as in 2.0.0-rc1 and in both it seems to be fixed- results are the same: 1000.0\r\n\r\n[2.0.0-rc1](https://colab.research.google.com/gist/jacekblaz/1b860965969dca734a7618c2009bf018/untitled.ipynb)\r\n\r\n[2.0-b1](https://colab.research.google.com/drive/1Rtc84ogFactv-gE89A2QXV-6pwDKlybg)\r\n\r\n[nightly](https://colab.research.google.com/drive/1BPrwX-ffEqjOzi-WrML-wCcfBftyf5aI)\r\n\r\nSo at least the same architecture written in different style provide the same results. However I am not sure if that result is correct. Logits has values between 0 and 1, maximum BinaryCrossentropy for those value is 15.424949. I believe BinaryCrossentropy(from_logits=False) should compute logits before applying BinaryCrossentropy itself. Therefore if input=1000, target=0 and BinaryCrossentropy(from_logits=False) the result should be max 15.424949. \r\n\r\nI think documentation, code, behavior and math behind it are inconsistent. Look at this [comment](https://github.com/tensorflow/tensorflow/blob/c6babdd8aae6d71d0b570216557d20983d6421e7/tensorflow/python/keras/backend.py#L4470) deeply hidden in backend", "Pinging in @pavithrasv and @karmel again - it looks like this might be a bug, rather than a feature request.", "This one deserve more attention guys, looks like a bug to me @dynamicwebpaige @pavithrasv and @karmel  \r\nI am unable to reproduce my results with image segmentation model when changing only the training loop style to work with tf.function \r\nworking style:\r\n    model.compile(optimizer=optimizer, loss=['categorical_crossentropy'])\r\n    model.train_on_batch(inputs, outputs])   \r\nbroken style:\r\n    categorical_loss = tf.keras.losses.CategoricalCrossentropy()\r\n    train_step() # which is tf.function\r\nin my case the loss of the broken style start from ~15.385 and in the working style from ~5\r\n\r\n\r\n\r\n", "@jacekblaz I found out what's happening here. We have two different functions - one that computes cross-entropy loss when we have the logits information, one that computes cross-entropy loss with probabilities. For the inputs you have provided label=0 and predictions=1000, the loss value is undefined. Each of these functions handle this undefined case differently and hence you see 1000 for one and 15.33 for the other.\r\n\r\n", "We recommend that you use logits as inputs always for numerical stability. All of the TF docs and examples will be updated to reflect the same."]}, {"number": 31929, "title": "[XLA] add a postpone fusion mechanism", "body": "[XLA] add a postpone fusion mechanism to fuse downcast convert into producer instead of consumer.\r\n\r\n@thomasjoerg ", "comments": ["@nouiz can you please resolve conflicts ?", "I rebased. The conflict was due to this commit that revert some changes: https://github.com/tensorflow/tensorflow/commit/834159282e4598e4fca53ff3e9f4b9bac10af784\r\n\r\nI needed to re-enable that change. I did it quickly in the latest commit. Any information on when we could have that re-added to the master as my PR depend on that.", "I rebased this PR to fix a new conflict.", "Update so that this does not look stale: the internal benchmark suite detected a regression and NVIDIA now has the HLO IR needed to reproduce this problem.", "Marking as \"needing changes\" to get this off my queue.", "@sanjoy gentle ping if you have any update on this ?", "@thomasjoerg @nouiz What is the current status here?  I know we discussed this PR over email, did we decide to go ahead with this design?", "The conclusion of the email discussion was to try to fuse manually. Tried that, but got other problems. Now working on something else. Thomas started some works that could make this approaches working again, but it causes a regression that prevent him from merging this now. So nothing is sure of what will happen except that it probably won't be done rapidly.\r\n\r\nWe can close this PR and reopen/open a new PR if we decide to continue this. ", "I do not plan to work on this shortly. So I'll close this PR."]}, {"number": 31928, "title": "[tf.estimator] Training with tf.estimator + tf.keras and tf.keras only yields inconsistent results", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS 10.14.6 (18G87)`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): `v1.14.0-rc1-22-gaf24dc91b5 1.14.0`\r\n- Python version: `3.7.3`\r\n- Bazel version (if compiling from source): **n/a**\r\n- GCC/Compiler version (if compiling from source): **n/a**\r\n- CUDA/cuDNN version: **n/a**\r\n- GPU model and memory: **none**\r\n\r\n**Describe the current behavior**\r\n\r\nFollowing the official guides ([[1]](https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models), [[2]](https://www.tensorflow.org/beta/guide/migration_guide#using_a_custom_model_fn)), I was training [PSENet](https://github.com/sdll/psenet/tree/master/psenet) for text detection. Even though train metrics did improve to almost perfect levels and the loss remained stable and low after a while, the inference I got was gibberish.\r\n\r\nPSENet works as follows: running the image through the feature pyramid network to obtain segmentation maps, it then applies a custom algorithm to extract bboxes. The images below show the segmentation maps, with yellow regions corresponding to the predicted text, and purple to everything else.\r\n\r\nAfter 186 attempts to make it work on the AI Platform and $400 of GSoC credits, I realized that the problem was deeper than the implementation details, and decided to overfit on a single sample, using the `tf.keras` implementation of FPN from [`segmentation_models`](https://github.com/qubvel/segmentation_models) by @qubvel. I have tweaked his implementation for PSENet and ran into the same problems with `tf.estimator`, so it seems that `tf.estimator` is indeed the culprit.\r\n\r\nFor this sample image\r\n\r\n![sample image](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/input.png)\r\n \r\nand one of the labels\r\n\r\n![sample label](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/label.png)\r\n\r\nafter 300 epochs, the same loss and optimizer, the predicted labels\r\n\r\n- with a pure `tf.keras` implementation:\r\n\r\n![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-pure-keras.png)\r\n\r\n- with the `tf.keras` model converted using `tf.keras.estimator.model_to_estimator`:\r\n\r\n![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-estimator-and-conversion.png)\r\n\r\n- with the `tf.keras` model used in the `tf.estimator` model function:\r\n\r\n![](https://raw.githubusercontent.com/sdll/tf.estimator-failing-example/master/docs/training-with-estimator-and-no-conversion.png)\r\n\r\nI have tried the `tf.keras`-in-`model_fn` setup on 10 000 images for 30-50 epochs, and the results are much worse than this, which is itself not perfect.\r\n\r\n**Describe the expected behavior**\r\n\r\n1. TensorFlow documentation should state clearly the preferred way to use `tf.keras` models inside `tf.estimator`, given the knowledge that `tf.estimator` is built on `tf.keras.layers` and thus the expectation that interops is seamless.\r\n\r\n2. The discrepancy between training with `tf.keras` and `tf.estimator` + `tf.keras` should be minimal or non-existent\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe minimal failing example with the code and data is [here](https://github.com/sdll/tf.estimator-failing-example).\r\n\r\n**Other info / logs**\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/25670 is a related issue. Similar findings are documented [here](https://stackoverflow.com/questions/54910215/tensorflow-estimator-fails-to-converge-on-model-converted-from-keras-when-using).\r\n\r\nI can also confirm that training with `tf.estimator` took longer than with pure `tf.keras`, in alignment with [other reports](https://stackoverflow.com/questions/56930892/tf-estimator-vs-tf-keras-speed-disparity) of this behavior.  \r\n\r\n\r\nThe model itself is sensible, and the following example shows that it does generalize well. For this input (not in the original data):\r\n\r\n![IMAGE 2019-08-23 17:13:30](https://user-images.githubusercontent.com/17913919/63598985-549a2f80-c5c9-11e9-9cfe-2db8e72694f9.jpg)\r\n\r\n- the output from [another implementation](https://github.com/liuheng92/tensorflow_PSENet) written in tf.slim is as follows:\r\n\r\n![IMAGE 2019-08-23 17:14:20](https://user-images.githubusercontent.com/17913919/63599060-72679480-c5c9-11e9-84e1-0b95a9f12edf.jpg)\r\n\r\n- the output from the [Pytorch implementation](https://github.com/whai362/PSENet) by the original authors is this:\r\n\r\n![IMAGE 2019-08-23 17:16:54](https://user-images.githubusercontent.com/17913919/63599211-cecab400-c5c9-11e9-95e7-e8b48a5cc2e3.jpg)", "comments": ["Seems that there are many batch normalization layers in segmentation model. Using `keras` model in `estimator` should take of the `update_ops`. Otherwise BN's mean and variance will not update.\r\n\r\n```python\r\nupdate_ops = model.get_updates_for(None) + model.get_updates_for(features)\r\nminimize_op = optimizer.minimize(\r\n      total_loss,\r\n      var_list=model.trainable_variables,\r\n      global_step=tf.compat.v1.train.get_or_create_global_step())\r\ntrain_op = tf.group(minimize_op, update_ops)\r\n```\r\nor in TF1.x style\r\n```python\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n....\r\n```\r\n\r\nhttps://www.tensorflow.org/beta/guide/migration_guide#using_a_custom_model_fn", "Nice, could this be added to the [official guide](https://www.tensorflow.org/guide/estimators#creating_estimators_from_keras_models) on estimators? What can explain the fact that `model_to_estimator` performs so poorly? That `train_and_evaluate` should almost always be used instead of `train`?", "@gowthamkpr, could you please elaborate on what is clear in this instance? I have used tf 1.14 not tf 2.0. Are you sure the sole problem is in not updating the ops? Do the same reasons explain the fact that tf.keras.estimator.model_to_estimator has not worked as expected as well?", "@sdll Can you please create a new issue of clearly referring to what the bug is. Thanks!", "@gowthamkpr, I would love to tell you what the source of the problem is. To help finding this out, I have made a minimum failing example where the model trained in keras works well, while the model trained with keras inside the model function and the keras model converted to the estimator do not. Unfortunately, I do not have enough knowledge of the tf 1.14 internals to say where the training/inference go awry. ", "Any updates on it?  I spent **lots** of time on tf.estimator and tf.keras, but seems they can't be used together perfectly. \ud83d\ude1e ", "@sdll \r\nIs this still an issue", "Sorry, I do not have enough bandwidth to run the minimum failing example myself to check this out.", "@sdll \r\nan you get the example down to the simplest possible reproducible code so that it is easy for us to pinpoint the issue.", "No, sorry, but if I had time I would go by the route of extracting the tf.keras code for FPN from https://github.com/qubvel/segmentation_models and then running the [example](https://github.com/sdll/tf.estimator-failing-example) again.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31928\">No</a>\n"]}, {"number": 31927, "title": "tf2 load model issue", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190818\r\n- Python version: 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen I try to load a model I obtain the following error:\r\n```\r\nValueError: Unable to save the object ListWrapper([ListWrapper([]), ListWrapper([])]) (a list wrapper constructed to track trackable TensorFlow objects). A list element was replaced (__setitem__, __setslice__), deleted (__delitem__, __delslice__), or moved (sort). In order to support restoration on object creation, tracking is exclusively for append-only data structures.\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would like load the saved model.\r\n\r\n**Code to reproduce the issue**\r\nGoogle Colab link: [https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp](https://colab.research.google.com/drive/1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp)\r\n\r\nHere a portion of source code:\r\n\r\n```\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.python.keras import Input, Model\r\nfrom tensorflow.python.keras.layers import Dense, Dropout\r\nfrom tensorflow.python.keras.optimizers import Adam\r\nfrom tensorflow.python.keras.models import load_model\r\nn = 200\r\ndf = pd.DataFrame(data={'a': [x for x in range(n)], 'b': [x for x in range(n+10,n+n+10)], 'labels': [int(x%2==0) for x in range(n)]})\r\ndf = df.astype({'b': str})\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  \r\n  labels = dataframe.pop('labels')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\ntrain, test = train_test_split(df, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\ntrain_ds = df_to_dataset(train)\r\nval_ds = df_to_dataset(val, shuffle=False)\r\ntest_ds = df_to_dataset(test, shuffle=False)\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\nfor c in df.columns:\r\n  if c == 'labels':\r\n    continue\r\n  elif c == 'b':\r\n    el = feature_column.categorical_column_with_vocabulary_list(c, df[c].unique(), default_value=-10)\r\n    el_one_hot = feature_column.indicator_column(el)\r\n    feature_columns.append(el_one_hot)\r\n    feature_layer_inputs[c] = tf.keras.Input(shape=(1,), name=c, dtype=tf.string)\r\n  elif c == 'a':\r\n    feature_columns.append(feature_column.numeric_column(c, default_value=-10))\r\n    feature_layer_inputs[c] = Input(shape=(1,), name=c)\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\nf_layer = feature_layer(feature_layer_inputs)\r\ninput = [v for v in feature_layer_inputs.values()]\r\nx = Dense(2048, activation='relu')(f_layer)\r\nx = Dropout(0.5)(x)\r\nout = Dense(1, activation='sigmoid')(x)\r\nmodel = Model(inputs=[input], outputs=out)\r\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])\r\nmodel.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)\r\nmodel.save('aaa.model')\r\nnew_model = load_model('aaa.model')\r\n```\r\n\r\n", "comments": ["Hi it is happening because you have put some part of model outside graph of your graph.\r\nentry point of you model is **input** so feature layer is outside .\r\n\r\nAn op outside of the function building code is being passed a \"Graph\" tensor.\r\n\r\n```\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\nf_layer = feature_layer(feature_layer_inputs)\r\n```", "@daniele-sartiano, did you get a chance to look at @akanyaani's comment. Thanks", "@akanyaani thank you for your comment.\r\nI tried to edit the code in this way:\r\n```\r\ntrain, test = train_test_split(df, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\ntrain_ds = df_to_dataset(train)\r\nval_ds = df_to_dataset(val, shuffle=False)\r\ntest_ds = df_to_dataset(test, shuffle=False)\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\nfor c in df.columns:\r\n  if c == 'labels':\r\n    continue\r\n  elif c == 'b':\r\n    el = feature_column.categorical_column_with_vocabulary_list(c, df[c].unique(), default_value=-10)\r\n    el_one_hot = feature_column.indicator_column(el)\r\n    feature_columns.append(el_one_hot)\r\n    feature_layer_inputs[c] = tf.keras.Input(shape=(1,), name=c, dtype=tf.string)\r\n  elif c == 'a':\r\n    feature_columns.append(feature_column.numeric_column(c, default_value=-10))\r\n    feature_layer_inputs[c] = Input(shape=(1,), name=c)\r\n\r\nx = tf.keras.layers.DenseFeatures(feature_columns)(feature_layer_inputs)\r\nx = Dense(2048, activation='relu')(x)\r\nx = Dropout(0.5)(x)\r\nout = Dense(1, activation='sigmoid')(x)\r\nmodel = Model(inputs=[feature_layer_inputs], outputs=out)\r\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['binary_accuracy', 'AUC'])\r\nmodel.fit(train_ds, validation_data=val_ds, epochs=10, verbose=0)\r\nmodel.save('aaa.model')\r\nfrom tensorflow.python.keras.models import load_model\r\nnew_model = load_model('aaa.model')\r\n```\r\n\r\nbut I obtain the same error.\r\nAny suggestion to solve this issue?", "Was able to reproduce the issue with Tensorflow 2.0.0.dev20190818. Please see the gist [here](https://colab.research.google.com/drive/1T4pKIz9d6zqVsoZerjaquTKVhx4-Px4P).Thanks! ", "Hello everyone gladly I found this issue.\r\nI am facing the same problem.\r\nI am using Python3 and Tensorflow 2.0.0 rc\r\n\r\nI was able to replicate the issue both with the mnist dataset and the heart dataset.\r\nI tried some model variations and I believe that the problem comes from the joint occurrence of feature columns and save model.\r\n\r\nWhen I get it right the above code uses the functional api for keras. With the sequential model this issue happens too.\r\n\r\nThe issue is also occurring when using h5 or json for saving and then loading the model.\r\nTo be precise the error for h5 is\r\nValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32>)\r\n\r\nIf there are any information which I can provide for solving the issue please post here.\r\n", "I have the same error too.", "The issue seems to be solved for me with tensorflow 2.0 rc2 \r\nThe change log states\r\n\r\n> Model saving changes\r\nmodel.save and tf.saved_model.save may now save to the TensorFlow SavedModel format. The model can be restored using tf.keras.models.load_model. HDF5 files are still supported, and may be used by specifying save_format=\"h5\" when \r\n\r\n> \r\n\r\n", "@daniele-sartiano Looks like this was resolved in `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/dec0d251573289e24d7e3aed4580818a/untitled1.ipynb). Everything runs without any issue when I use `tf-nightly`. Thanks!\r\n\r\nI am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31927\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31927\">No</a>\n", "After installing tf-nightly, still i am getting the same error message.\r\nI am trying to read the CSV file from hdfs using tf.data API.\r\n\r\nFollowing is my code sample,-\r\n(code is referred from: #https://www.tensorflow.org/tutorials/load_data/csv)\r\ntrain_file_paths = [\"hdfs://..../tr/titanic/train/train.csv\"]\r\ntest_file_paths = [\"hdfs://...../tr/titanic/eval/test.csv\"]\r\n\r\nLABEL_COLUMN = 'survived'\r\nLABELS = [0, 1]\r\nCOLUMNS = [\"survived\",\"sex\",\"age\",\"n_siblings_spouses\",\"parch\",\"fare\",\"class\",\"deck\",\"embark_town\",\"alone\"]\r\n\r\ndef get_dataset(file_path, **kwargs):\r\n  dataset = tf.data.experimental.make_csv_dataset(\r\n      file_path,\r\n      batch_size=rowsin_batch, # 10- Artificially small to make examples easier to show.\r\n      column_names=COLUMNS,\r\n      label_name=LABEL_COLUMN,\r\n      na_value=\"?\",\r\n      header=False,\r\n      num_epochs=1,\r\n      shuffle=False,\r\n      ignore_errors=True, \r\n      **kwargs)\r\n  return dataset\r\n\r\nraw_train_data = get_dataset(train_file_paths)\r\nraw_test_data = get_dataset(test_file_paths)\r\n\r\nclass PackNumericFeatures(object):\r\n  def __init__(self, names):\r\n    self.names = names\r\n\r\n  def __call__(self, features, labels):\r\n    numeric_freatures = [features.pop(name) for name in self.names]\r\n    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_freatures]\r\n    numeric_features = tf.stack(numeric_features, axis=-1)\r\n    features['numeric'] = numeric_features\r\n\r\n    return features, labels\r\n\r\nNUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']\r\n\r\npacked_train_data = raw_train_data.map( PackNumericFeatures(NUMERIC_FEATURES))\r\n\r\npacked_test_data = raw_test_data.map( PackNumericFeatures(NUMERIC_FEATURES))\r\n\r\nnumeric_column = tf.feature_column.numeric_column('numeric', shape=[len(NUMERIC_FEATURES)])\r\nnumeric_columns = [numeric_column]\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(numeric_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  feature_layer,\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(128, activation='relu'),\r\n  layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.compile(\r\n    loss='binary_crossentropy',\r\n    optimizer='adam',\r\n    metrics=['accuracy'])\r\n\t\r\nmodel.fit(packed_train_data, epochs=2)\r\n\r\nprint(model.summary())\r\n\r\ntest_data = packed_test_data\r\n\r\nprint(\"evaluating model..\")\r\ntest_loss, test_accuracy = model.evaluate(test_data)\r\n\r\nmodel.save('my_model.h5') \r\n\r\nprint(\"**********Printing new model created from saved model**********\")\r\nnew_model = tf.keras.models.load_model('my_model.h5')\r\n\r\nprint(\"Show the model architecture\")\r\nprint(new_model.summary())\r\n\r\nPlease let me know if i am doing it wrong way.", "> Hello everyone gladly I found this issue.\r\n> I am facing the same problem.\r\n> I am using Python3 and Tensorflow 2.0.0 rc\r\n> \r\n> I was able to replicate the issue both with the mnist dataset and the heart dataset.\r\n> I tried some model variations and I believe that the problem comes from the joint occurrence of feature columns and save model.\r\n> \r\n> When I get it right the above code uses the functional api for keras. With the sequential model this issue happens too.\r\n> \r\n> The issue is also occurring when using h5 or json for saving and then loading the model.\r\n> To be precise the error for h5 is\r\n> ValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32>)\r\n> \r\n> If there are any information which I can provide for solving the issue please post here.\r\n\r\nThis error still persists with today's `tf-nightly` (also tried 1.15.0 and 2.0.0).\r\n\r\nI have `DenseFeatures` as the first layer.\r\nI'm able to `evaluate`, `predict` and even `save` the model.\r\nBut on `load_model` I get:\r\n```python\r\nValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float32>)\r\n```\r\nAny suggestions?\r\n\r\nEdit: This only happens with a `Sequential` model.", "@eliadl Please open a new issue with details related to issue and a standalone code to reproduce the issue. Thanks!", "@eliadl Have you any solutions for the problem?", "@magiclevinho I ended up using the [Keras functional API](https://www.tensorflow.org/guide/keras/functional), rather than a Sequential model.\r\nBecause that way the issue doesn't occur.", "@eliadl Thank You for answering!\r\nI don't know if the following code does similar as you said, but it works for me!\r\nI copied the code from an another website, (i don t know if I m allowed to post links?) the user was posting is, so the credit goes for him/her: Egor B Eremeev\r\n`from __future__ import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n#!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ndataframe.head()\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nage = feature_column.numeric_column(\"age\")\r\n\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n  feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n\r\n# bucketized cols\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\n# indicator cols\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\nfeature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n\r\n# embedding cols\r\nthal_embedding = feature_column.embedding_column(thal, dimension=8)\r\nfeature_columns.append(thal_embedding)\r\n\r\n# crossed cols\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\ncrossed_feature = feature_column.indicator_column(crossed_feature)\r\nfeature_columns.append(crossed_feature)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\r\n\r\nx = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\nx = layers.Dense(64, activation='relu')(x)\r\n\r\nbaggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(train_ds)`", "> @magiclevinho I ended up using the [Keras functional API](https://www.tensorflow.org/guide/keras/functional), rather than a Sequential model.\r\n> Because that way the issue doesn't occur.\r\n\r\nFor my purposes it does not work to use the Keras functional API, isn't there another solution to this problem? Having to switch to a different API will not work for every situation", "I also have the same issue. I tired with Tf-nightly. And also tried switching to functional API. Both do not seem to work. ", "The newly added [experimental layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding) seems to be an attempt to solve the issue.\r\n\r\nIn the old versions, feature_column blocks are defined outside of the model, resulting in the issue. These new experimental layers are placed inside model definition. I haven't tried it yet, but it looks promising."]}, {"number": 31926, "title": "Proper way to install TensorFlow Docker image with GPU support on Debian 10 (Debian Buster)", "body": "**System information**\r\n- OS: Debian GNU/Linux 10 (buster) x86_64 \r\n- Kernel: 4.19.0-5-amd64 \r\n- CPU: Intel i7-6700 (8) @ 3.400GHz \r\n- GPU: Intel HD Graphics 530 \r\n- GPU: NVIDIA GeForce RTX 2070\r\n\r\n**Describe the problem**\r\nI followed the [Tensorflow documentation](https://www.tensorflow.org/install/docker) in order to install a TensorFlow Docker image with GPU support on Debian Buster. For verification, I execute\r\n```\r\ndocker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\r\n```\r\nas stated in the documentation. However, all I get is the following error message:\r\n```\r\nsvdhero@ml-box:~$ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\r\ndocker: Error response from daemon: Unknown runtime specified nvidia.\r\n```\r\nAlternatively, I also tried\r\n```\r\nsvdhero@ml-box:~$ docker run --rm nvidia/cuda nvidia-smi\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"exec: \\\"nvidia-smi\\\": executable file not found in $PATH\": unknown.\r\n```\r\nwithout any luck, as one can see.\r\n\r\nPreviously, I installed my NVIDIA drivers successfully via\r\n```\r\nsudo apt install nvidia-driver\r\n```\r\nas one can see here:\r\n```\r\nsvdhero@ml-box:~$ nvidia-smi \r\nFri Aug 23 13:01:51 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.74       Driver Version: 418.74       CUDA Version: N/A      |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2070    On   | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   39C    P8     3W / 175W |      0MiB /  7952MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nI also installed docker successfully, as one can see here:\r\n```\r\nsvdhero@ml-box:~$ docker --version\r\nDocker version 19.03.1, build 74b1e89\r\n\r\nsvdhero@ml-box:~$ docker run --rm hello-world\r\n\r\nHello from Docker!\r\nThis message shows that your installation appears to be working correctly.\r\n\r\nTo generate this message, Docker took the following steps:\r\n 1. The Docker client contacted the Docker daemon.\r\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\r\n    (amd64)\r\n 3. The Docker daemon created a new container from that image which runs the\r\n    executable that produces the output you are currently reading.\r\n 4. The Docker daemon streamed that output to the Docker client, which sent it\r\n    to your terminal.\r\n\r\nTo try something more ambitious, you can run an Ubuntu container with:\r\n $ docker run -it ubuntu bash\r\n\r\nShare images, automate workflows, and more with a free Docker ID:\r\n https://hub.docker.com/\r\n\r\nFor more examples and ideas, visit:\r\n https://docs.docker.com/get-started/\r\n\r\n```\r\nI have **not** installed any `nvidia-docker` or `nvidia-container-toolkit`, because the Tensorflow documentation clearly says:\r\n\r\n> Note: The latest version of Docker includes native support for GPUs and nvidia-docker is not necessary.\r\n\r\nThis is a brand-new Debian install with no legacy packages installed.\r\n\r\nSo what am I doing wrong? I do realize that this is not a Tensorflow problem, but is there anything missing in the Tensorflow documentation? I followed the documentation exactly.\r\n\r\n", "comments": ["Can anybody confirm the above behaviour?\r\n\r\nShould I install `nvidia-container-toolkit` via\r\n```bash\r\n$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\r\n$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\r\n$ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\r\n\r\n$ sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\r\n$ sudo systemctl restart docker\r\n```\r\nas described on https://github.com/NVIDIA/nvidia-docker or is this not necessary with the latest docker version?", "I have seen that @Ethyling has added Debian 10 support to the Nvidia Container Toolkit over at https://github.com/NVIDIA/nvidia-docker recently.\r\n\r\nMaybe he could point me in the right direction? Last time I tried to install `nvidia-container-toolkit` I got an `Unsupported distribution`.", "@svdHero ,\r\nCan you try installing [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) and install TF.?Thanks!", "Thanks for your help. I have run\r\n```\r\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\r\n\r\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\r\nsudo systemctl restart docker\r\n```\r\nnow. This time, I did not get an `Unsupported distribution` error. Maybe that's related to the work @Ethyling has done. The installation seems to be successful:\r\n```\r\nsvdhero@ml-box:~$ sudo apt search nvidia-container-toolkit\r\nSorting... Done\r\nFull Text Search... Done\r\nnvidia-container-toolkit/buster,now 1.0.3-1 amd64 [installed]\r\n  NVIDIA container runtime hook\r\n```\r\n\r\nHowever, still no difference when trying to run the docker image:\r\n```\r\nsvdhero@ml-box:~$ docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi\r\ndocker: Error response from daemon: Unknown runtime specified nvidia.\r\nSee 'docker run --help'.\r\n```\r\nand\r\n```\r\nsvdhero@ml-box:~$ docker run --rm nvidia/cuda nvidia-smi\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"exec: \\\"nvidia-smi\\\": executable file not found in $PATH\": unknown.\r\n```\r\n\r\nI tried restarting the machine, just to be sure, but that didn't help either. Any idea what's going on here?", "@svdHero ,\r\nCan you try using [cuda-toolkit](https://tensorflow.google.cn/install/gpu?hl=en#install_cuda_with_apt)?Thanks!", "@oanush You mean not using the Docker image?", "I also tried\r\n```\r\nsvdhero@ml-box:~$ docker run --gpus all nvidia/cuda nvidia-smi\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"process_linux.go:413: running prestart hook 0 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: exec command: [/usr/bin/nvidia-container-cli --load-kmods --debug=/var/log/nvidia-container-toolkit.log configure --ldconfig=@/sbin/ldconfig --device=all --compute --utility --require=cuda>=10.1 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411 --pid=1409 /var/lib/docker/overlay2/2e5623f4c6dc82d2230bcdeca765998e5a5acfce1a7cb66ace64494f67b73d35/merged]\\\\\\\\nnvidia-container-cli: initialization error: driver error: failed to process request\\\\\\\\n\\\\\\\"\\\"\": unknown.\r\nERRO[0000] error waiting for container: context canceled \r\n```\r\nDoes nobody have any suggestions what's wrong here? It's obvious that the Tensorflow documentation is missing something crucial, but still the only suggestion by @oanush is \"Do not use the docker image, but install CUDA manually instead.\". Why then offer a Docker image at all?", "Thanks for your providing your versions and sample attempts. I've created a PR to fix the confusing instructions here: https://github.com/tensorflow/docs/pull/979\r\n\r\nTo be clear, it sounds like the correct way to get this working would be to install `nvidia-container-toolkit` and use `docker --gpus all`, which is what you have done. Our CUDA TF images should then work for you once Docker knows how to use your GPUs.\r\n\r\nUnfortunately, I'm not sure what the problem is in your last comment; you may need to look for help via NVIDIA or Stack Overflow since that failure isn't related to TensorFlow. I've seen a similar error before from when I tried to use nvidia-docker with a CUDA version that didn't support my GPU and nvidia driver version, but it looks like your versions [are compatible](https://github.com/NVIDIA/nvidia-docker/wiki/CUDA#requirements). Could it be that `--gpus all` attempts to load your Integrated GPU and fails? Maybe you need to restrict the selection somehow.", "Thank you @angersson for your detailed answer and for your PR. I am sure the improved wording will help future CUDA newbies like myself.\r\n\r\nConcerning my problem, I don't think that my integrated GPU is the cause. I get the same error message with\r\n```\r\ndocker run --gpus '\"device=1\"' nvidia/cuda nvidia-smi\r\n```\r\nand\r\n```\r\ndocker run --gpus '\"device=2\"' nvidia/cuda nvidia-smi\r\n```\r\nAlso, when running Ubuntu 18.04 on the same machine, everything works perfectly now. It seems to be a Debian 10 problem (unfortunately Debian is my company's Linux default).\r\nI've also tried to get help via https://github.com/NVIDIA/nvidia-docker/issues/1056, without any luck so far.\r\nIf you still have an idea, I would appreciate your help. However, once the PR for the documentation is merged, feel free to close this issue. I will continue the troubleshooting over at nvidia-docker.\r\n\r\nThanks again for your help and effort.", "https://github.com/tensorflow/docs/pull/979 has been merged, so I'll close this issue. Hopefully nvidia-docker issue can be resolved as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31926\">No</a>\n"]}, {"number": 31925, "title": "[TF1.x][TPU] How to perform preprocessing steps for text classification for training on TPU?", "body": "I have been trying since 5 days to train a simple text classification model on TPUs. But because of lack of documentation it is very difficult. I just can not perform tokenization, encoding, padding without `tf.py_func`. Please add some examples for doing these steps for TPU devices so that dumb people like me can understand TF. Will be greatly thankful to everyone at Google.\r\nI am following this [tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches).", "comments": ["@jvishnuvardhan @MarkDaoust Hello, is there any update?. I am still struggling with the problem.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "We've since rewritten that tutorial. [This one](https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu) also trains a text model on TPU"]}, {"number": 31924, "title": "Use typedef struct", "body": "", "comments": ["This misses the declaration specs from `TFL_CAPI_EXPORT`. Also, why should we switch to `typedef`?", "struct is not symbol. it should be tagged for functions or global variables. `__declspec(dllexport)` can be used for C++ classes. But this is plain struct. This header file is used from C. So `struct Foo` must not be written without `struct`. typedef is always required for struct, enum for C.", "I thought it got used as `struct Foo` everywhere else in the code, but seems not, so :+1: for the PR.\r\n\r\nThough, can you please make it `typedef struct Foo {...} Foo;` instead of `typedef struct { ...} Foo;`?", "c_api_types.h defines `typedef struct {} Foo` already."]}, {"number": 31923, "title": " module 'tensorflow' has no attribute 'matvec'", "body": "I saw the file tensorflow/python/ops/math_ops.py  include matvec excample:\r\n\r\n```\r\n`# 2-D tensor `a`\r\n  # [[1, 2, 3],\r\n  #  [4, 5, 6]]\r\n  a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\r\n\r\n  # 1-D tensor `b`\r\n  # [7, 9, 11]\r\n  b = tf.constant([7, 9, 11], shape=[3])\r\n\r\n  # `a` * `b`\r\n  # [ 58,  64]\r\n  c = tf.matvec(a, b)`\r\n```\r\n\r\nwhen I code that.\r\n\r\nthat error heppend:\r\n`AttributeError: module 'tensorflow' has no attribute 'matvec'`", "comments": ["@CCHDjango ,\r\nCan you please try using `tf.linalg.matvec(a,b)`,it should work fine.Thanks!", "Added a PR #32051 for docstring fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31923\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31923\">No</a>\n", "AttributeError: module 'tensorflow.linalg' has no attribute 'matvec'.  Can anyone advise? \r\n"]}, {"number": 31922, "title": "Running MobileNet SSD Float Detector", "body": "Hello. \r\n\r\nI'm trying to run the model \"mobile_ssd_v2_float_coco.tflite\" in the [object detection app](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android). I have gotten the model to run on the GPU following the [tutorial](https://www.tensorflow.org/lite/performance/gpu) and adjusting image resolution and output accordingly. The output seems to follow the [old format](https://github.com/tensorflow/tensorflow/commit/f3785197b4de9466b48462f4f93b455c88dd622b#diff-c748d758eb53ac58d70a13e07e02c6db), where only outputLocations and outputClasses are produced. In the previous code it was also necessary to rescale the bounding boxes and there was a file \"box_priors\". I found the old file, it has 1917 entries, since this was the number of results produced. The new model on the other hand produces 2034 results. Is there a compatible \"box_priors\" file? Is this file even necessary?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Hello. \r\n\r\nThanks for your response. The problem is not related to a specific platform, but here it is:\r\nThe App is running on Android, Android Studio running on a MacBook. \r\n\r\nI'm using tflite, 1.14 binaries:\r\norg.tensorflow:tensorflow-lite-gpu:1.14.0", "Hey @thias15 , are you interested in the raw box encodings and scores (before Non-Max-Suppression) or the final results (much less in number, but post-processed with NMS)? \r\n\r\nSo the network you have consists only of the 'Feature Extractor' & 'Box Predictor' components, which produce the raw set of candidate objects. These would need to be post-processed with NMS.\r\n\r\nIf all you need is the final results, please follow the instructions on [this page](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) on any of the models from our [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md). This should yield a model which follows the Inputs/Outputs mentioned [here](https://www.tensorflow.org/lite/models/object_detection/overview#starter_model). ", "Hi:\r\n\r\nI have a follow up question. I am using the same setup using tflite-gpu 1.14 and get the raw data of 'Feature Extractor' & 'Box Predictor'. Can you please let me know how to go about post-processing that data using NMS and I believe we would need the anchors to use with NMS?\r\n\r\nThanks.", "One way would be to [use TFLite's post-processing op](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md). \r\nIf you want to do things yourself, look at our [detection_postprocess](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc) op.\r\n\r\nI am closing this issue now, since it has been stale for a while."]}, {"number": 31921, "title": "Failed importing _pywrap_tensorflow", "body": "Hi all, I have a problem.\r\n\r\nAfter I installed tensorflow on my windows 10 machine with python 3.7.4\r\nusing this pip command: `pip install --user --upgrade tensorflow`\r\n\r\nI want the CPU only version on a 64-bit version!\r\n\r\nI was trying to verify the installation by running this command:\r\n`python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\nBut that gives me the following error:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n>     fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n>     raise ImportError(_ERR_MSG.format(name), name=name)\r\n> ImportError: No module named '_pywrap_tensorflow'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n>     _pywrap_tensorflow = swig_import_helper()\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n>     import _pywrap_tensorflow\r\n> ModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"<string>\", line 1, in <module>\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n>     from tensorflow.python import *\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n>     raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n>     fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\imp.py\", line 296, in find_module\r\n>     raise ImportError(_ERR_MSG.format(name), name=name)\r\n> ImportError: No module named '_pywrap_tensorflow'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n>     from tensorflow.python import pywrap_tensorflow\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n>     _pywrap_tensorflow = swig_import_helper()\r\n>   File \"C:\\Users\\Alexander\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n>     import _pywrap_tensorflow\r\n> ModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n> \r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n> \r\n> for some common reasons and solutions.  Include the entire stack trace\r\n> above this error message when asking for help.\r\n \r\nI have already tried reinstalling: Microsoft Visual C++ 2015 Redistributable Update 3, according to this issue here: [https://github.com/tensorflow/tensorflow/issues/7529](https://github.com/tensorflow/tensorflow/issues/7529)\r\nBut that didn't work....\r\n\r\nI am not missing a the dll file called: MSVCP140.DLL!\r\n\r\nHow can I fix this?", "comments": ["TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nWhat is your CPU make/model?", "I have an Intel Core i7 2600K, but which command should I use to install version 1.5?\r\n\r\nThis command: `pip3 install tensorflow==1.5.0`\r\nreturns:\r\n\r\n> ERROR: Could not find a version that satisfies the requirement tensorflow==1.5.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.5.0", "@PanderBoy18,\r\nTry below command to install tensorflow 1.5\r\n`pip3 install tensorflow==1.5`\r\nLet us know how it progresses. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31921\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31921\">No</a>\n"]}, {"number": 31920, "title": "Improve documentation for Dataset shard", "body": "In this pull request, I am making the changes proposed by @aaudiber in the review of #31897 \r\n\r\nRelated issue: #31698 \r\n\r\nThanks @aaudiber for the review.", "comments": []}, {"number": 31919, "title": "tf.GradientTape().tape() is returning None on the generator loss", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: I am using Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Not applicable\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: `2.0.0-beta1`\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: [This Colab notebook](https://colab.research.google.com/drive/1iIGWaktKZfDmxpjl4XARsBinTHcgd2Xf)\r\n\r\n### Describe the problem\r\n\r\nI am trying to reproduce [this simplistic GANs example](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f) using TensorFlow 2.0. Everything is working as _expected except for the generator network_. Here's one forward and backward pass of the generator network:\r\n\r\n```python\r\nwith tf.GradientTape() as tape:\r\n  g_fake_data = G(gen_input)\r\n  dg_fake_decision = D(get_moments(g_fake_data.numpy().T).reshape((1,4)))\r\n  g_error = criterion(dg_fake_decision, np.ones((1,1)))\r\ntape.gradient(g_error, G.trainable_weights)\r\n```\r\nThe gradients are all coming as `None`. I have tried to do `tape.watch(G.trainable_weights)` as well but it does not help. My suspect is that the `GradientTape` context manager is unable to keep track of the appropriate dependencies to compute the gradients. I have tried to move the `dg_fake_decision = D(get_moments(g_fake_data.numpy().T).reshape((1,4)))` step to `with tape.stop_recording()` but it still does not help. \r\n\r\nAny suggestions on this would be very helpful. I hope this would be helpful for the community specifically for them who are willing to dig deep with automatic differentiation. \r\n", "comments": ["Hello, maybe official tutorial could help you.\r\nhttps://www.tensorflow.org/beta/tutorials/generative/dcgan", "Hi. I am aware of this tutorial but the network topologies are different here. ", "After a rough look at the colab, I find `get_moments()` is a numpy function, so you pass `g_fake_data.numpy().T` into it, which makes the graph unconnected. So I modify this function to TF version and the gradients are not longer `None`. Modified colab notebook is down below:\r\nhttps://colab.research.google.com/drive/1r9HZRlVepGU6ScXPZLOjwoLa7vSr5DCJ", "Thank you very much @WindQAQ. I will look into and post my further findings here :)"]}, {"number": 31918, "title": "Do not make invalid android config with only NDK/SDK configured", "body": "Fixes #31843\r\n\r\nCorrects indent of templates for both SDK and NDK to use 4 spaces.\r\nMakes `pass` statement only if both NDK and SDK are missing", "comments": ["There is surprising CI failure:\r\n\r\n```\r\n[0 / 26] [Prepa] BazelWorkspaceStatusAction stable-status.txt\r\nERROR: /tmpfs/tmp/bazel/external/com_google_protobuf/BUILD:106:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 32 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\ngcc: error: unrecognized command line option '-std=c++14'\r\n```"]}, {"number": 31917, "title": "Intermittent crash with CUDA_ERROR_LAUNCH_FAILED failure", "body": "### System information\r\n- **OS Platform and Distribution**:\r\non (Linux-based) ML-engine runtime version 1.12, which means it is using TensorFlow 1.12, with Python 3.6.\r\n- **CUDA/cuDNN version**: \r\nCUDA Version: 10.1, with cuDNN: libcudnn.so.7.4.2\r\n- **GPU model and memory**:\r\nTesla K80\r\n- **Exact command to reproduce**:\r\nrunning custom code on object segmentation task.\r\n\r\n### The problem\r\nThe problem is the training crash after reaching thousands of steps (few hours). The crash is intermittent. The same code and dataset when rerun is OK on another occasion. This problems happened quite frequently (>20x) in my testings. One observation is that this does not happen for earlier TensorFlow 1.8. Current workaround for me is to downgrade to TF 1.8. This could be a bug on the library or CUDA driver.  The same problem happened in workstation offline .\r\n\r\n### The Logs\r\n```\r\nInstructions for updating:\r\nUse standard file APIs to delete files with this prefix.\r\nINFO:tensorflow:Recording summary at step 1239.\r\nINFO:tensorflow:global step 1240: loss = 0.2247 (1.349 sec/step)\r\nINFO:tensorflow:global step 1260: loss = 0.2513 (1.189 sec/step)\r\nINFO:tensorflow:global step 1280: loss = 0.3016 (1.181 sec/step)\r\nINFO:tensorflow:Recording summary at step 1291.\r\nINFO:tensorflow:global step 1300: loss = 0.2227 (1.151 sec/step)\r\nINFO:tensorflow:global step 1320: loss = 0.2227 (1.139 sec/step)\r\nINFO:tensorflow:global step 1340: loss = 0.2224 (1.273 sec/step)\r\nINFO:tensorflow:Recording summary at step 1342.\r\nINFO:tensorflow:global step 1360: loss = 0.2220 (1.186 sec/step)\r\nINFO:tensorflow:global step 1380: loss = 0.2297 (1.157 sec/step)\r\nINFO:tensorflow:Recording summary at step 1393.\r\nINFO:tensorflow:global step 1400: loss = 0.2217 (1.160 sec/step)\r\nINFO:tensorflow:global step 1420: loss = 0.2213 (1.169 sec/step)\r\nINFO:tensorflow:global step 1440: loss = 0.2209 (1.134 sec/step)\r\nINFO:tensorflow:Recording summary at step 1443.\r\n2019-08-01 01:43:15.655519: E tensorflow/stream_executor/cuda/cuda_driver.cc:1131] failed to enqueue async memcpy from host to device: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; GPU dst: 0x71133f800; host src: 0x7ff986bfcc80; size: 131072=0x20000\r\n2019-08-01 01:43:15.655586: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-08-01 01:43:15.655630: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-08-01 01:43:15.655630: E tensorflow/stream_executor/cuda/cuda_driver.cc:1000] could not wait stream on event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-08-01 01:43:15.655644: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-08-01 01:43:15.655731: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e0fd7c0\r\n2019-08-01 01:43:15.655731: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e6241c0\r\n2019-08-01 01:43:15.655717: I tensorflow/stream_executor/stream.cc:5027] [stream=0x5c23080,impl=0x5c23120] did not memcpy host-to-device; source: 0x7ff97e62e9c0\r\n2019-08-01 01:43:15.655755: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n```\r\n\r\n### Additional information logs:\r\nThe ML team support kindly provides machine debug information as followings:\r\n```\r\n\"The VM logs of failed jobs all have the same error, like\r\n I 2019-07-25T17:00:20.282471416Z [37582.281240] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception:  MISSING_INLINE_DATA\\r\\n \r\nI 2019-07-25T17:00:20.282477291Z [37582.289153] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception: ESR 0x404600=0x80000002\\r\\n \r\nI 2019-07-25T17:00:20.282557397Z [37582.297688] NVRM: Xid (PCI:0000:00:04): 13, Graphics Exception: ChID 0017, Class 0000a1c0, Offset 000001b4, Data 00002000\\r\\n \"\r\n```\r\nFYI, I did not succeed to reproduce the error with  cuda-memcheck or cuda-gdb or CUDA_DEVICE_WAITS_ON_EXCEPTION=1.\r\nThis issue is probably similar to ##20356\r\n\r\n", "comments": ["Did you build TF from sources for cuda 10.1 support? Also what is your nvidia driver version?", "No, I did not build the TF from the source code. I supposed the TF is preinstalled (when running on the Google AI-Platform). \r\nThe nvidia driver version from running nvidia-smi:\r\n```\r\n nvcc: NVIDIA (R) Cuda compiler driver \r\n Copyright (c) 2005-2017 NVIDIA Corporation \r\n Built on Fri_Sep__1_21:08:03_CDT_2017 \r\n Cuda compilation tools, release 9.0, V9.0.176 \r\n Mon Aug  5 06:56:55 2019        \r\n +-----------------------------------------------------------------------------+ \r\n | NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     | \r\n |-------------------------------+----------------------+----------------------+ \r\n | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | \r\n | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | \r\n |===============================+======================+======================| \r\n |   0  Tesla K80           On   | 00000000:00:04.0 Off |                    0 | \r\n | N/A   37C    P0    79W / 149W |      0MiB / 11441MiB |      0%      Default | \r\n +-------------------------------+----------------------+----------------------+ \r\n                                                                                 \r\n +-----------------------------------------------------------------------------+ \r\n | Processes:                                                       GPU Memory | \r\n |  GPU       PID   Type   Process name                             Usage      | \r\n |=============================================================================| \r\n |  No running processes found                                                 | \r\n +-----------------------------------------------------------------------------+ \r\n -rwxr-xr-x 1 root root 302770160 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn.so \r\n -rwxr-xr-x 1 root root 302770160 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn.so.7 \r\n -rwxr-xr-x 1 root root 302770160 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn.so.7.3.1 \r\n -rwxr-xr-x 1 root root 314634338 Jul 27 11:37 /usr/local/cuda/lib64/libcudnn_static.a \r\n```\r\nTo add extra info: this happens also for TF 1.13.1 although the test was not as extensive as TF 1.12. The specs for machine with TF 1.13.1 from nvidia-smi is as follows:\r\n```\r\n nvcc: NVIDIA (R) Cuda compiler driver I  \r\n Copyright (c) 2005-2018 NVIDIA Corporation I  \r\n Built on Sat_Aug_25_21:08:01_CDT_2018 I  \r\n Cuda compilation tools, release 10.0, V10.0.130 I  \r\n Mon Aug  5 07:04:42 2019        I  \r\n +-----------------------------------------------------------------------------+ I  \r\n | NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     | I  \r\n |-------------------------------+----------------------+----------------------+ I  \r\n | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC | I  \r\n | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. | I  \r\n |===============================+======================+======================| I  \r\n |   0  Tesla K80           On   | 00000000:00:04.0 Off |                    0 | I  \r\n | N/A   38C    P0    78W / 149W |      0MiB / 11441MiB |      0%      Default | I  \r\n +-------------------------------+----------------------+----------------------+ I  \r\n                                                                                 I  \r\n +-----------------------------------------------------------------------------+ I  \r\n | Processes:                                                       GPU Memory | I  \r\n |  GPU       PID   Type   Process name                             Usage      | I  \r\n |=============================================================================| I  \r\n |  No running processes found                                                 | I  \r\n +-----------------------------------------------------------------------------+ I  \r\n -rwxr-xr-x 1 root root 349141232 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn.so I  \r\n -rwxr-xr-x 1 root root 349141232 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn.so.7 I  \r\n -rwxr-xr-x 1 root root 349141232 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn.so.7.4.2 I  \r\n -rwxr-xr-x 1 root root 346085818 Jul 27 11:38 /usr/local/cuda/lib64/libcudnn_static.a I  \r\n```\r\n", "@jurneo could you provide a simple repro (e.g. python scripts, etc) for the problem? Also, could you try with TF [1.15rc1](https://pypi.org/project/tensorflow-gpu/1.15.0rc1/) or [2.0rc1](https://pypi.org/project/tensorflow-gpu/2.0.0rc1/)?\r\n\r\nAlso @nluehr, are you familiar with the `Graphics Exception` error mentioned above?", "The \"Graphics Exception\" is most likely a symptom of the \"unspecified launch failure\" which has left the application in a bad state. An unspecified launch failure often results from a Segfault within a GPU kernel. It is an asynchronous error in that it gets generated by a kernel executing on the GPU but isn't flagged on the CPU until the next CPU/GPU sync point (in this case, the sync point appears to be an attempt to run a host-to-device memcpy).\r\nIn addition to testing with the latest TF versions, I would also recommend updating cudnn to the latest version, 7.6.3.", "@aaroey It is likely the driver issue on linux. To share additional recent finding, this problem does not occur on TF 1.12 on Windows version. I will try your suggestion for later TF versions.\r\nMeanwhile, related thread discussion is on issuetracker google 138398232, and the python script is similar to deeplab v3. \r\nI would close this issue with the workaround (by upgrading to latest version of TF with latest update GPU on drivers), but feel free to reopen.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31917\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31917\">No</a>\n"]}, {"number": 31916, "title": "[Intel MKL] Upgrading the checkerframework component to the version 2.10.0.", "body": "Most recent version is required in order to ship Intel Optimized TensorFlow community build.", "comments": []}]