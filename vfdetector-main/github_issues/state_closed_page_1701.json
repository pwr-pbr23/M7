[{"number": 1870, "title": "crosstool_wrapper_driver_is_not_gcc failed: python2: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory", "body": "### Environment info\n\nOperating System: Scientific Linux 6.5\n\nInstalled version of CUDA and cuDNN: \nCUDA - 7.5.18\nCuDNN - 4.0.7\n\nTensorflow installation attempt from source..\n\nERROR: /global/home/users/kmuriki/.cache/bazel/_bazel_kmuriki/7a46079e1611cbcdacd2bbe4113de14c/external/re2/BUILD:9:1: C++ compilation of rule '@re2//:re2' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 36 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\npython2: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory\n\npython2 on my machine is located here : /global/software/sl-6.x86_64/modules/langs/python/2.7.8/bin/python2 and I do have libpython2.7.so.1.0 located here: /global/software/sl-6.x86_64/modules/langs/python/2.7.8/lib/libpython2.7.so.1.0. \n\nLD_LIBRARY_PATH points to /global/software/sl-6.x86_64/modules/langs/python/2.7.8/lib\n\n$ python-config --ldflags\n-lpthread -ldl -lutil -lm -lpython2.7 -Xlinker -export-dynamic\n\nWhat else am I missing ?\n", "comments": ["I'm having the same issue in a cluster with CentOS 6.7, also installing from source, following the instructions from https://github.com/tensorflow/tensorflow/issues/110\n\n/home/p269371/.cache/bazel/_bazel_p269371/62515d408409b93db1ccb498556e2fe0/external/protobuf/BUILD:71:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 42 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 127.\n/software/software/Python/2.7.11-foss-2016a/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 0.811s, Critical Path: 0.08s\n\n$python-config --ldflags\n-lpython2.7 -ldl -lm -lpthread -lutil -lm -Xlinker -export-dynamic\n$python-config --includes\n-I/software/software/Python/2.7.11-foss-2016a/include/python2.7 -I/software/software/Python/2.7.11-foss-2016a/include/python2.7\n$python-config --libs\n-lpython2.7 -ldl -lm -lpthread -lutil -lm\n$python-config --prefix\n/software/software/Python/2.7.11-foss-2016a\n$python-config --exec-prefix\n/software/software/Python/2.7.11-foss-2016a\n\n$ ls /software/software/Python/2.7.11-foss-2016a/lib\nlibpython2.7.so  libpython2.7.so.1.0  pkgconfig  python2.7\n\n> from distutils import sysconfig\n> print sysconfig.get_config_vars('LIBPL')\n> ['/software/software/Python/2.7.11-foss-2016a/lib/python2.7/config']\n\n$ls /software/software/Python/2.7.11-foss-2016a/lib/python2.7/config\nconfig.c  config.c.in  install-sh  libpython2.7.a  Makefile  makesetup  python.o  Setup  Setup.config  Setup.local\n\nSomeone solved it by installing python elsewhere: https://github.com/tensorflow/tensorflow/issues/414, I'd like to avoid that.\n", "@kmuriki What location did you enter for python while running `./configure`?\n", "@kmuriki Gentle ping. Please let me know if this is still an issue.\n", "Manjunath,\n\nThanks for writing back to me. Its been couple of weeks since I tried this\nand did not get a chance yet to attempt it again. I'm busy with other\ndeadlines all this week. Hopefully I will have some free time later this\nweek or early next week. I will attempt building from scratch once again\nand write back to you.\n\nJust to answer your earlier question we do not install Python to the\nstandard /usr/local kind of folders in our infrastructure. We have it\ninstalled on a central storage server which is NFS mounted at a path like\n/global/software/sl-6.x86_64/modules/langs/python/2.7.8/bin. We update all\nthe appropriate env variables PATH, LD_LIBRARY_PATH, LIBRARY_PATH and\nPYTHONPATH.\n\nThanks,\nKrishna.\n\nOn Mon, Jun 13, 2016 at 7:43 AM, Manjunath Kudlur notifications@github.com\nwrote:\n\n> @kmuriki https://github.com/kmuriki Gentle ping. Please let me know if\n> this is still an issue.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1870#issuecomment-225601369,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ACQb9l8sN8HiQGuc0fgOzH1a2mlYnYQBks5qLWx7gaJpZM4IFGcQ\n> .\n", "Same behaviour as @mlopezantequera when trying to build on CentOS 6.7. My debug output is the same, expect for the actual installation directory of Python that is different in my case.\n\nIf that matters, I'm running Python 2.7.10 using environment modules, so LD_LIBRARY_PATH etc are all set up correctly. The actual Python binary and libs have been built with EasyBuild.\n", "I am having the same issue on CentOs 6.7. Now I am using Bazel version 0.3.0 to compile tensorflow 0.9.0. How to make bazel recognize LD_LIBRARY_PATH is the question. #110 includes step-wise directions including a couple of hacks but does not work for me. Still getting the libpython not found error.\n", "A polite reminder to all participants. Kindly do share the solution to this issue if you found one.\n", "I'm having the same issue.  I can get tensorflow to install from source without GPU support enabled.  But with GPU support, on CentOS 6.4, it can't find libpython.so.1.0.  Python is installed in a non-standard location.  Suggestions would be welcome.\n", "Same problem here.\n", "Did it happen because of upgrading from python2.6 to 2.7?\nI ran into similar problems when I use some tools after upgrading python.\nIf your python2.6 still in the system, please try to make the link for python2 back to python2.6.\nLike this: https://youtu.be/R8Z5K2u5mag\nI am not sure if this solution fits to you, but it solve my problem.\nGood luck!\n", "Closing because of inactivity."]}, {"number": 1869, "title": "Add QOL improvements for tf.one_hot()", "body": "This commit adds a Python wrapper for tf.one_hot(), and introduces two\nmain, backwards compatible, improvements, as mentioned in #1799:\n1. The parameters `on_value` and `off_value` now have default values 1 and 0\n   respectively, as 32-bit integers. Because the most common use case of\n   one_hot() is to create Tensors for one-hot encoding, it will save users\n   some headache to not have to manually type in these values by hand each\n   time it is used. The parameters can still be given different values as\n   before, and this does not any previous behavior.\n2. `indices` and `depth` no longer have to be cast by hand as `int64` and\n   `int32` values, respectively. Previously, one_hot() would complain if\n   these values weren't of the exact right type- it now casts the values to\n   the appropriate type on its own.\n\nPinging @ebrevdo for review.\n", "comments": ["Can one of the admins verify this patch?\n", "@ebrevdo: pull request is here at #1869. Will work on your comments and update when it's ready to look at again. Sorry for putting the tag in the original commit message.\n", "Can one of the admins verify this patch?\n", "I've been working on this on and off for a few days- the one thing that I'm getting hung up on is the goal functionality for string data types. Correct me if I'm wrong, but I'm not sure the base one_hot operation supports strings. Running the current implementation of one_hot, I get the following error:\n\n```\n>>> import tensorflow as tf\n>>> sess = tf.Session()\n>>> a = tf.one_hot([1,0,2],3,\"1\",\"0\")\n>>> sess.run(a)\n...\ntensorflow.python.framework.errors.InvalidArgumentError: No OpKernel was registered to support Op 'OneHot' with these attrs\n     [[Node: OneHot_1 = OneHot[T=DT_STRING, axis=-1](OneHot_1/indices, OneHot_1/depth, OneHot_1/on_value, OneHot_1/off_value)]]\n```\n\nI'll update the code that I have for review once I finish writing a few tests.\n", "Additionally, do you think it might be better to pass dtype along to convert_to_tensor instead of checking for a type error?\n\n `on_value = ops.convert_to_tensor(on_value, dtype=dtype)`\n\nSince dtype is already given a default value of tf.float32, we're always going to have to do that type check, which ends up with unintuitive errors. For example:\n\n`tf.one_hot([0,1,2], 3, on_value=1, off_value=-1)` \n\nthrows a type error because it's expecting float values for on_value and off_value.\n", "@ebrevdo See the latest changes- is this closer to what you're looking for?\n", "Sorry, I'm away for a while.  Assigning @girving to finish the review.\n", "Happy to finish the review, but can you squash the commits into one?\n", "Thanks @girving - I've made the changes suggested, and also made it possible for the user to specify only one of `on_value` or `off_value` if they choose to. Added more tests to go along with this as well.\n", "Thanks!  Taking a look now.\n", "@girving I made the changes! I'm going to hold off on squashing until it looks good to go- that way I can revert anything easily (if necessary).\n", "Looks good!  Squash and I'll off the tests.  Unfortunately I have to do an errand soon, so there may be a delay.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Squashed- thank you, good sir!\n", "Jenkins, test this please.\n", "I installed the latest Tensorflow whl file through pip and attempted to use tf.one_hot. Now I'm getting \n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: No OpKernel was registered to support Op 'OneHot' with these attrs\n     [[Node: OneHot = OneHot[T=DT_BOOL, axis=-1](ArgMax, OneHot/depth, OneHot/on_value, OneHot/off_value)]]\n```\n\nDid I do something wrong?\n", "Two questions:\n1. When you say the latest TensorFlow wheel, do you mean `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl`, or did you download the nightly build?\n2. What was the context for this error? i.e. what did you input to the function?\n", "I ran `pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl` inside of a virtualenv and\n\n```\nimport tensorflow as tf\nsess = tf.Session()\na = tf.one_hot([1,0,2],3,\"1\",\"0\")\nsess.run(a)\n```\n\nreproduces it\n", "Great, thanks! The quality of life changes aren't actually part of TensorFlow 0.8, so in order to test it out, you'll have to either build TensorFlow from source, or install it from the nightly binaries [available here](https://github.com/tensorflow/tensorflow#installation)\n\nThe other issue is that, unfortunately, string values aren't working properly in `one_hot` at this time. Try running this command instead and see if it works:\n\n```\na = tf.one_hot([1,0,2],3,1,0)\n```\n\nThere are a few bugs/headaches that need to be addressed as well- hopefully by the next release it'll be working super smoothly!\n", "Oh! Turns out, using integers totally fixes it. My initial use case before I ended up reducing down to the test case I posted used booleans for the on/off values. That also doesn't work, but that's totally fine, since I can get the same behavior with integers and casts.\nGreat, thanks so much!\n", "No problem! Hopefully by the next release you won't have to fiddle around with casts for non-numeric values. \n\nGlad it's working for you!\n"]}, {"number": 1868, "title": "Fix conv2d with kernel 1x1 stride > 1", "body": "Fixes #889 - seems the problem was an optimization to call directly into cublas when the kernel is 1x1. I suspect that was meant for 1x1 and stride 1, as it calculates the wrong thing with stride2. This adds 3 tests - two that demo the bug, one that I did while debugging but figured to submit too for extra coverage.\n\nI signed the CLA\n", "comments": ["Can one of the admins verify this patch?\n", "Very nice, and good catch there!  Just a few small comments, and then we can test / merge.\n", "Can one of the admins verify this patch?\n", "@vrv I updated the patch - that test seems to work.\n", "(@ry: tell me when I'm supposed to look at this again)\n", "alright then!  LGTM.  @tensorflow-jenkins: test this please\n", "Doh, pooling_ops_test also has a similar function for checking whether stride > ksize that you'd have to disable, though I'm now wondering whether pooling also supports stride > ksize the same way conv does.\n", "oops- sorry! ok I'll look at this and get back to you\n", "If pooling doesn't natively support it, it's still probably worth getting this CL in, but will require a bit more work, in that you'd have to retain the stride > ksize checking for pooling (essentially forking the python shape code that's currently shared between conv2d and pooling ops).  Let's hope Eigen / Cudnn already support this:)\n", "@vrv Updated pooling tests - it seems to work out of the box. I squashed the changes into one commit. \n", "This is great!  @tensorflow-jenkins: test this please\n"]}, {"number": 1867, "title": "Fix typo in Mechanics 101 tutorial to match recent code. ", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1866, "title": "Fix a typo in Mechanics 101 tutorial to match recent code.", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1865, "title": "Merge internal changes", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Jenkins, test this please.\n"]}, {"number": 1864, "title": "Moved mnist reading from input_data in tutorial to learn/datasets", "body": "", "comments": []}, {"number": 1863, "title": "Merge internal changes", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Jenkins, test this please\n", "Jenkins, test this please.\n"]}, {"number": 1862, "title": "Merge internal changes", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 1861, "title": "Bug: tf.scan loses shape information when available", "body": "Operating System: CentOS 6.7\nIf installed from sources, provide the commit hash: b4b276e\n\n`tf.scan` loses shape information when available, which in turn makes it necessary to pass around shape information that could be inferred from the `Tensors` themselves.\n\nExample where we should see shape `[3]` (after conversion to list), but where we instead get a shape of `<unknown>`:\n\n```\ndef fn(previous_output, current_input):\n    print(current_input.get_shape())\n    return current_input\n\nx = tf.constant([[1, 2, 3], [4, 5, 6]])\ninitializer = tf.constant([0, 0, 0])\n\ny = tf.scan(fn, x, initializer=initializer)\n\nwith tf.Session() as sess:\n    print(sess.run(y))\n\n# <unknown>\n# [[1 2 3]\n# [4 5 6]]\n```\n", "comments": ["Scan uses TensorArray internally.  We currently don't support shape inference for TensorArray operations.  \n", "Fixed in HEAD.\n", "For people who stumbled on this later, this is the commit which fixed this issue: https://github.com/tensorflow/tensorflow/commit/8baf2f346f45c85be31cda0264b035ecda9c21bc \n"]}, {"number": 1860, "title": "Link to active linux GPU nightly build (#1859)", "body": "The Linux python 2 build appears to have been moved to another jenkins job, and the link was stale. This link came from @vrv, in the linked issue (#1859).\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed the cla.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks!\n"]}, {"number": 1859, "title": "Python 2 linux GPU nightly link in readme is wrong", "body": "Hey! I wanted to try out the scan op, so I installed the latest. however, on doing so, I discovered that no nightly build has been attempted in jenkins for about 20 days. if this is simply because one has not been run, could someone at google hit build and then take a look at why it hasn't run?\n\nNote: I don't mean it's failed; I mean there are no builds in the list newer than that, failed or otherwise.\n\nIn the meantime, I'll install from source.\n### Environment info\n\nOperating System: Linux; Ubuntu/14.04\n\nInstalled version of CUDA and cuDNN: irrelevant\n\ninstalled from binary pip package: http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n### What are you about to try?\n1. building from source.\n", "comments": ["Does http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-cp27-none-linux_x86_64.whl work? \n\n(difference seems to be gpu-working vs. gpu-slave)\n", "oh huh! it's not that it isn't building, it's that the wrong one is linked from the readme.\n", "this is silly trivial. I'll make a pr.\n", "I think we keep moving the URL as we add more jenkins instances and forgetting to change the README :(  sorry about that\n"]}, {"number": 1858, "title": "Fix \"naming\" anchor in \"Adding an Op\" How-To", "body": "The file had a braces-style declared anchor, {#naming}, which wasn't\nbeing rendered or pre-processed properly. This changes it to an HTML `<a>`\ntag. This should also fix the broken link pointing to this section on\nline 874 in the same file.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!\n", "Can one of the admins verify this patch?\n", "Will go  out with the next website push.\n"]}, {"number": 1857, "title": "Update update_version.sh", "body": "", "comments": ["LGTM\n"]}, {"number": 1856, "title": "Incremental builds are very slow", "body": "I'm following the instructions from \"Setting up TensorFlow for Development\" in `tensorflow/g3doc/get_started/os_setup.md`\n\nIf I make a small change to `core/kernels/eigen_spatial_convolutions.h` and rebuild the pip package (`bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package`) the process takes **204 seconds**.\n", "comments": ["You picked the worst file to modify. It's included a lot and building it is pretty hard on the compiler. There's not all that much we can do about this, sorry. I would guess that those 204s do not include any unnecessary steps.\n", "Seems like build_pip_package can take a very long time by itself even for small incremental changes that don't change much in terms of binary code, especially for debug builds. The amount of data generated is substantial but it's not bound by disk bandwidth. The python build is running on a single thread for 3-4 minutes suggesting the wheel generation process is CPU bound and could benefit from parallelization over multiple threads."]}, {"number": 1855, "title": "R0.7", "body": "I need this code to learn tensorflow so please allow request\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Just to be clear, these are commits to 0.7 that you're trying to forward port to master?\n", "Ah. Well we should fix some of these properly then.\n"]}, {"number": 1854, "title": "fix ppa repositories in ci_build", "body": "Those repositories are just workaround for old versions of libraries on old ubuntu (trusty). Those ppas do not make sense on newer ubuntu or debian.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "I'm closing this one. I will reopen it against r0.8 branch.\n"]}, {"number": 1853, "title": "shuffle_batch gives ZeroDivisionError when computing capacity stat", "body": "```\r\nimport tensorflow as tf\r\nraw = tf.ones((2))\r\ntf.train.shuffle_batch([raw], batch_size=2, capacity=4, min_after_dequeue=4, seed=5)\r\n\r\n```\r\n\r\nthis fails with\r\n\r\n```\r\nZeroDivisionError                         Traceback (most recent call last)\r\n<ipython-input-11-86e4af7ca1ee> in <module>()\r\n      1 import tensorflow as tf\r\n      2 raw = tf.ones((2))\r\n----> 3 tf.train.shuffle_batch([raw], batch_size=2, capacity=4, min_after_dequeue=4, seed=5)\r\n\r\n/Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/training/input.py in shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)\r\n   1220       allow_smaller_final_batch=allow_smaller_final_batch,\r\n   1221       shared_name=shared_name,\r\n-> 1222       name=name)\r\n   1223 \r\n   1224 \r\n\r\n/Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/training/input.py in _shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, keep_input, num_threads, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)\r\n    779     full = (math_ops.cast(math_ops.maximum(0, queue.size() - min_after_dequeue),\r\n    780                           dtypes.float32) *\r\n--> 781             (1. / (capacity - min_after_dequeue)))\r\n    782     # Note that name contains a '/' at the end so we intentionally do not place\r\n    783     # a '/' after %s below.\r\n\r\n```\r\n\r\nUnlike TF tensors, Python doesn't handle float division by zero. One solution is to wrap \"capacity\" and \"min_after_dequeue\" into TF tensors so that you get \"inf\" as a result instead of RuntimeError\r\n", "comments": []}, {"number": 1852, "title": "skflow save sets absolute paths for checkpoint that break on production", "body": "### Environment info\n\nRunning on latest [nightly build](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-cp27-none-linux_x86_64.whl).\n### Steps to reproduce\n1. Save an estimator to a folder (let's call it classifier.model)\n2. Attempt to restore on another machine in different machine\n\nThis breaks since the checkpoint file is pointing to the place in the new machine.\n### What have you tried?\n1. Manually changing the path in the checkpoint file fixes the issue\n", "comments": ["@sherrym skflow currently uses Saver to save checkpoints - is there anyway to disable saving global path into `checkpoint` file and save local path instread?\n", "@nemo Actually looking [at this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L558) code, it seems like if you just specify relative path when you are saving (e.g. `est.save('my_model/')`) - it will save with relative path and should work. Can you tell if this fixes your problem?\n", "@ilblackdragon just tried it. Still seeing absolute paths in there - we had `./my_model` before and I tried `my_model` and `my_model/`, both of which still created a checkpoint file with absolute paths.\n", "@nemo , could you please cut and paste your code showing how you set the save_path? I have unit tests demonstrating that relative paths work correctly.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver_test.py#L659\n", "Here it is:\n\n```\nclassifier = learn.TensorFlowEstimator(model_fn=tanh_dnn, n_classes=len(categories) + 1,\n    steps=len(documents) * 10, learning_rate=0.2, batch_size=100)\n\nclassifier.fit(X_train, y_train)\nclassifier.save('categories.model/')\n```\n\nSince it was renamed from `skflow` to `learn` - I haven't tested the different path types other than the one you see there, which still causes the checkpoint file to have a full path.\n", "@ilblackdragon, Saver.save() takes a save_path as an argument. You just need to change TensorFlowEstimator's save() in estimators/base.py to pass in save_path.\n", "Closing due to lack of activity. Please reopen if there is still a problem.\n"]}, {"number": 1851, "title": "Update word2vec.py", "body": "Makes it consistent with line 516.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "@tensorflow-jenkins: test this please\n", "Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please (trying to get them all green)\n"]}, {"number": 1850, "title": "Surprising behaviour with distributed Tensorflow", "body": "Hello, I'm observing some surprising behaviour with distributed Tensorflow, in terms of performance. In short, for the exact same Graph, I get a very different performance based on the machine I connect my tf.Session to.\n\nIn my experiments, I have two machines A and B in a cluster. Machine A has a GPU with cuDNN. I declare a graph that executes the cifar10 gpu example exclusively on machine A. No operators (including the variables) are declared on machine B. I log the device placement and Tensorboard does not see any operators associated with anything other device than machine A. \n\nWhen I execute the graph, starting my session using `tf.Session(\"grpc://machineA:2222\", ...` results in significantly better performance than using `tf.Session(\"grpc://machineB:2222\", ...`. It is also not just a constant overhead, as I increase the batch size, the difference seems to scale linearly:\n\n| Batch Size | Connect to machine A | Connect to machine B |\n| --- | --- | --- |\n| 128 | 0.19s | 0.50s |\n| 256 | 0.38s | 1.00s |\n| 512 | 0.74s | 1.78s |\n| 1024 | 1.42s | 3.38s |\n\n(Averaged over 10 iterations not including the first two)\n\nWhen looking at the output of `top` on machine B, it does seem like python is doing some work when I connect to machine B as it averages around 30% CPU vs ~0% otherwise, but I cannot tell what that is/how to prevent it.\n\nI am working with Tensorflow installed from source with commit ed1a947d5b306d74af24821110dc2eb2f36c038a. \n", "comments": ["Hi Valentin :).\n\nSo it sounds like you have three processes: a client (on machine A or B? or a third machine?), and two TensorFlow servers (one on machine A and one on machine B). If you connect to machine A and all of the ops are on machine A then all of the master<->worker traffic will be local; whereas if you connect to machine B and all of the ops are on machine A then all of the master<->worker traffic will cross the network. (I'm not sure about the client<->master traffic, because you didn't mention where the client is running. You might want to try putting the client in the same process as the machine A master, as I would expect that to give the best performance. That's the configuration we typically use for distributed TF.)\n\nIn the latter case, it's possible that 30% of machine B is being consumed by proxying RPCs between the client and machine A. You might be seeing latency because the \"queue runners\" that prefetch input data run sequentially and now have two network round trips per record, and you're now \"I/O bound\" (not actually bound by the disk, but by the steps that have to run to populate the input queues). You could try adding prefetching threads to the input pipeline to see if that improves things. (I'm assuming you're using a version of [`cifar10_multi_gpu_train.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py), so no data is being fed or fetched in each training step.)\n\nThe input pipeline \"protocol\" is very chatty on the network, and we've been relying on most of the interactions happening in-process. Thanks for digging into this: it might give us good motivation to improve the batching etc.!\n", "Hi Derek! \n\nThanks! That makes a lot of sense. Regarding the client location, I had tried either on machine A or B with little or no change in performance. So I had always left it on machine B so that the instructions were given from 1-hop away from the computes, although the batching may be different between client<->master and master<->worker?\n\nI am still puzzled though because I just re-ran some experiments with a larger network (the second layer 64->2048 neurons) but I still get the same 2.5X ish drop in performance, even though the number of RPCs should be constant? \n\nHere were the results:\n\n| Batch Size | Connect to machine A | Connect to machine B |\n| --- | --- | --- |\n| 128 | 2.06s | 5.26s |\n| 256 | 3.76s | 10.17s |\n\nThe client was always on machine B again. It seemed like python was doing less work on `top`. I tried to change `num_preprocess_threads` in cifar10_input.py to 32 and 64 with no real change in performance. \n", "That's surprising. Are you fetching anything in the call to `sess.run()` that you're measuring?\n", "It's the `train_op` for the multi_gpu example. (The whole program is a slightly modified version of cifar10_multi_gpu_train.py) \n", "Hello, just to say that I still get the same behaviour after updating to 0.8. I also had a look at the network traffic with `tcpdump` but it seemed similar in both settings. Any ideas on what could be going on?\n", "I think what is going on is that when I connect the client to machine B, the operations of machine A don't get scheduled on the GPU. At least, I get the exact same behaviour when scheduling operations on the CPU. I wish I could make sure nothing is running on the GPU but `nvidia-smi` does not support my graphics card (GTX 780). But the behaviours do really match for multiple workloads.\n\nI am not sure how to see why that would happen. Again, tensorboard tells me the operations are on the GPU.\n", "Hmm, how are you determining that the operations are actually running on the GPU? The `cifar10_multi_gpu_train.py` example has `allow_soft_placement=True`, so there's a possibility that the code is being silently placed on the CPU instead of the GPU (not sure if that would account for the slowdown, as I'm not 100% familiar with the performance of that model, but it seems plausible). If you pass the flag `--log_device_placement=True`, what device does it show for the ops?\n", "Indeed passing `--log_device_placement=True` shows the operations are not scheduled on the gpu. I've attached the two logs depending on which machine I use as master. ran(Machine A) has a gpu but trothy(Machine B) doesn't. Operations that should get scheduled on the gpu are under name scope `ran_gpu`.\n\nAny idea on how I could use the gpu remotely without risking to break things with `allow_soft_placement=False`?\n\n[from_ran.txt](https://github.com/tensorflow/tensorflow/files/223680/from_ran.txt)\n[from_trothy.txt](https://github.com/tensorflow/tensorflow/files/223681/from_trothy.txt)\n", "Have you installed the GPU version on both machines? I think the placer uses information about the locally registered kernels to make placement decisions, so if you connect your session to a process that only has the CPU kernels, it won't use a GPU anywhere. (This is a bit embarrassing, and we should fix it....)\n", "Ah that makes sense! No it was only the CPU version. Will it work for me to install the GPU version without cuda on the machine?\n", "I believe it should... if not, let me know and we'll have to build a workaround!\n", "Didn't work :-( \n\n> bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n> WARNING: Output base '/auto/homes/vd241/.cache/bazel/_bazel_vd241/bd4752ce1a8928ERROR: /local/scratch/vd241/tensorflow/third_party/gpus/cuda/BUILD:187:1: declared output 'third_party/gpus/cuda/cuda.config' is a dangling symbolic link.\n> ERROR: /local/scratch/vd241/tensorflow/third_party/gpus/cuda/BUILD:187:1: not all outputs were created.\n> ERROR: /local/scratch/vd241/tensorflow/third_party/gpus/cuda/BUILD:187:1: declared output 'third_party/gpus/cuda/cuda.config' is a dangling symbolic link.\n> ERROR: /local/scratch/vd241/tensorflow/third_party/gpus/cuda/BUILD:187:1: not all outputs were created.\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\n> Use --verbose_failures to see the command lines of failed build steps.\n> INFO: Elapsed time: 37.604s, Critical Path: 6.99s\n", "But actually, this means it shouldn't really be a problem as long as, whenever I have GPUs, my client connects to a machine with a GPU.\n\nThank you so much for all your help!\n", "Hi, VDalibard\uff1a\r\nI'm a freshman on tensorflow. Now I'm trying to establish a distributed tensorflow on two Ubuntu servers. I've used the code on tersorflow.org and ran the following codes  \r\n`CUDA_VISIBLE_DEVICES='' python distribute.py --ps_hosts=10.105.36.225:2223 --worker_hosts=10.105.36.225:2224,10.105.32.251:2224 --job_name=ps --task_index=0` \r\nto setup the parameter server. And \r\n`CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=10.105.36.225:2223 --worker_hosts=10.105.36.225:2224,10.105.32.251:2224 --job_name=worker --task_index=0` \r\nto create a worker node and\r\n `--task_index=1`\r\n for the other node. Whereas when I ran the last two commands, the terminal told me that \r\n`wmfs-MacBook-Pro-2:TensorFlow Murphy$ CUDA_VISIBLE_DEVICES=0 python distribute.py --ps_hosts=10.105.36.225:2223 --worker_hosts=10.105.36.225:2223,10.105.32.251:2224 --job_name=worker --task_index=0\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {10.105.36.225:2223}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, 10.105.32.251:2224}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\r\nE0112 00:40:14.185396000 123145561804800 tcp_client_posix.c:191] failed to connect to 'ipv4:10.105.36.225:2223': timeout occurred\r\nE0112 00:40:14.185566000 140736234132416 tcp_client_posix.c:191] failed to connect to 'ipv4:10.105.32.251:2224': timeout occurred`\r\n\r\nSo I wanna know if I need to start some kind of service on machine B?"]}, {"number": 1849, "title": "Creating a FFmpeg library along with a test \u2026", "body": "\u2026 that will work whether or not FFmpeg is installed on the machine running the test. This also creates a set of integration tests that will be run during the nightly build.\nChange: 119061788\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Jenkins, test this please?\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Jenkins, test this please.\n", "Jenkins, test this please, again.\n", "Jenkins, test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 1848, "title": "tensorflow version not updating to 0.7.1 using pip", "body": "Hi,\n\nI recently updated to tensorflow version 0.7.1 using the command:\n\n`pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`\n\nIf I run `pip show tensorflow`, then it gives me that version of tensorflow as `0.6.0`. However, `tf.__version__` in python gives me `0.7.1` \n\nI am using pip to install a few other dependencies(like prettytensor), it looks for the latest  version of tensorflow. Since it fails to find the latest version using pip, it isnt updating my other dependencies.\n\nPlease let me know if there is a workaround for this. Thanks in advance!\n", "comments": ["Is it possible to uninstall tensorflow and then reinstall?\n"]}, {"number": 1847, "title": "Modified tests for gradients", "body": "While implementing gradients for some new functions, I forgot to propagate the `grad` argument in the definition of the derivative in [`tensorflow/python/ops/math_grad.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py). The wrong implementation passed all the tests because the derivative of the function is evaluated in isolation. \n\nMultiplying the TensorFlow operation by a constant `a` ensures that such errors are caught by the tests.  This PR implements the modified tests with `a = 1.1`.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 1846, "title": "the documentation of master branch on tensorflow.org is not updated", "body": "the documentation of master branch on tensorflow.org is not updated\nStill some old version of documents\n", "comments": ["Updated it just now.\n", "@martinwicke can we make it to automatically update everyday?\n", "We're looking into it.\n"]}, {"number": 1845, "title": "add a multi-gpu version link in tutorial", "body": "add a `multi-gpu version` link in the deep-cnn tutorial\n", "comments": ["Can one of the admins verify this patch?\n", "@martinwicke OK, updated to the anchor tag that is automatically generated\n"]}, {"number": 1844, "title": "tensorflow/google/protobuf/BUILD:520:1: no such package 'util/python'", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:   redhat, tensorflow (0.5.0), bazel (0.1.1)\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1.  I am compiling tensorflow (0.5.0)  ,  run this :\n   bazel build -c opt --spawn_strategy=standalone --genrule_strategy=standalone --jobs=20 //tensorflow/cc:tutorials_example_trainer\n2.  The compile error looks like: \n   Object of type 'Label' has no field \"workspace_root\"\n   3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nERROR: /opt/TensorFlow/tensorflow/tensorflow/core/BUILD:297:1: in _proto_gen rule //tensorflow/core:protos_all_cc_genproto: \nTraceback (most recent call last):\n        File \"/opt/TensorFlow/tensorflow/tensorflow/core/BUILD\", line 297\n                _proto_gen(name = 'protos_all_cc_genproto')\n        File \"/opt/TensorFlow/tensorflow/google/protobuf/protobuf.bzl\", line 54, in _proto_gen_impl\n                _GenDir(ctx)\n        File \"/opt/TensorFlow/tensorflow/google/protobuf/protobuf.bzl\", line 11, in _GenDir\n                ctx.label.workspace_root\nObject of type 'Label' has no field \"workspace_root\".\nERROR: Analysis of target '//tensorflow/cc:tutorials_example_trainer' failed; build aborted.\nINFO: Elapsed time: 16.462s\n", "comments": ["And when I run: \nbazel build -c opt --spawn_strategy=standalone --genrule_strategy=standalone --jobs=20 //tensorflow/tools/pip_package:build_pip_package\n\nThe error message is :\nERROR: /opt/TensorFlow/tensorflow/google/protobuf/BUILD:520:1: no such package 'util/python': BUILD file not found on package path and referenced by '//google/protobuf:internal/_api_implementation.so'.\nERROR: Loading failed; build aborted.\n", "Oh, I have found the root cause.\n  That was caused the \"protobuf\" was not correctly gotten by git..\n  The solution is running this: \"git submodule foreach --recursive\"\n"]}, {"number": 1843, "title": "Implement an efficient AssignMatMul() for general BLAS GEMM pattern", "body": "TensorFlow currently lacks efficient in-place matrix updates such as rank-1 update `A += U V'`. @rmlarsen recommends supporting these in-place updates through a single new `AssignMatMul()` method that wraps the underlying GEMM kernels in Eigen or cuBlas etc.\n", "comments": ["This hasn't gotten much attention, but I like it so I'm going to leave it open in the forlorn hope that someone will eventually do it.", "We are going to close this issue. Feel free to reopen it if you want to contribute and link the PR to it."]}, {"number": 1842, "title": "Fixed a typo. It should be x (not b).", "body": "I think it's should be x, not b.\n\n``` python\n# 'perm' is more useful for n-dimensional tensors, for n > 2\n# 'x' is   [[[1  2  3]\n#            [4  5  6]]\n#           [[7  8  9]\n#            [10 11 12]]]\n# Take the transpose of the matrices in dimension-0\ntf.transpose(b, perm=[0, 2, 1]) ==> [[[1  4]\n...\ntf.transpose(x, perm=[0, 2, 1]) ==> [[[1  4]\n```\n", "comments": ["Can one of the admins verify this patch?\n", "This documentation is auto-generated from [this line in `array_ops.py`](https://github.com/tensorflow/tensorflow/blob/fe9dafd1583da5ccc205eab776f86afcb00411d2/tensorflow/python/ops/array_ops.py#L564). Can you please make the change there instead? Thanks!\n", "Thanks, @mrry. Done!\n", "@tensorflow-jenkins, test this please.\n", "Jenkins, test this please.\n", "@martinwicke I just changed one line in the comment. I am not sure why it causes two failures: \n ci.tensorflow.org \u2014 FAILURE\n Linux CPU Tests \u2014 FAILURE\n\nDid I miss something?\n", "That looks like a tool failure. I'm pretty sure those timeouts are not your fault.\n"]}, {"number": 1841, "title": "ValueError: No gradients provided for any variable", "body": "I'm trying to optimize only a specific subset of my variables like so:\n\n``` python\nmy_train = tf.train.RMSPropOptimizer(0.001).minimize(my_loss, var_list=my_variables)\n```\n\nThis gives me an error message\n\n```\nValueError: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x7f30317a5f10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f303146fa10>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f3031522390>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f30314e1dd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f30314a5fd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x7f303146ae90>))\n```\n\nThere is no error if I leave out the `var_list`, but this obviously isn't what I want, and from the error message it isn't clear to me what the problem is.\n", "comments": ["Turns out that my_loss didn't depend on my_variables due to a bug in my code. I was able to find the bug with the help of tensorboard and adding lots of name scopes. I suggest improving the error message to indicate what the problem really is (objective doesn't depend on variables).\n"]}]