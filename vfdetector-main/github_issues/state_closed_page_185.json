[{"number": 49198, "title": "tflite_runtime pip package built using bazel is missing `metrics_portable` module", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS (running in docker built from `latest-devel`)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source `master` branch\r\n- TensorFlow version: 2.6.0 \r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n## Description \r\n* Standalone tflite_runtime pip package built using bazel ([reference](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package#normal-build-for-your-workstation)) is missing `metrics_portable` module that is imported in `tflite_runtimer/interpreter.py`\r\n* The missing module is only encountered when the package is built using bazel. `build_pip_package.sh` bash script and cmake script include the metrics_* modules that are missing from the bazel scripts (refer: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package.sh#L35-L36)\r\n\r\n## Steps to reproduce\r\n* Steps to reproduce the issue are detailed in this repo: https://github.com/avroshk/build-tflite-runtime\r\n\r\n## Error logs\r\n```\r\ntflite_runtime==2.6.0\r\nTraceback (most recent call last):\r\n  File \"test_interpreter.py\", line 4, in <module>\r\n    import tflite_runtime.interpreter as tflite\r\n  File \"/usr/local/lib/python3.6/dist-packages/tflite_runtime/interpreter.py\", line 42, in <module>\r\n    from tflite_runtime import metrics_portable as metrics\r\nImportError: cannot import name 'metrics_portable'\r\n```\r\n", "comments": ["Do you want to submit a PR?", "@yyoon could you take a look?", "It's unofficial, but I applied the patch and built it, and it seemed to work fine. TensorFlow v2.5.0\r\n- Python 3.7 aarch64\r\n- Python 3.7 armv7l\r\n- Python 3.8 aarch64\r\n- Python 3.8 armv7l\r\n\r\nhttps://github.com/PINTO0309/TensorflowLite-bin", "Addressed in PR #49199 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49198\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49198\">No</a>\n"]}, {"number": 49197, "title": "Update python package description to include python 3.9", "body": "Update python package description to include python 3.9", "comments": []}, {"number": 49196, "title": "Error in tf.keras.metrics.AUC", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`tf.keras.metrics.AUC(from_logits=True)` and `tf.keras.metrics.AUC(from_logits=False)` outputs `TypeError: __init__() got an unexpected keyword argument 'from_logits'`\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nReproducible error: https://colab.research.google.com/drive/1-ZMVqG7t54a2QO-Vh2zKDdyT2gKYB47u?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: __init__() got an unexpected keyword argument 'from_logits'\r\n```", "comments": ["Cause It is available from TF 2.5.0\r\n\r\nPlease upgrade your TF version", "p.s. in Colab `pip install --upgrade tensorflow`", "This is also mentioned in TF 2.5 release notes. See https://github.com/tensorflow/tensorflow/releases", "Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49196\">No</a>\n"]}, {"number": 49195, "title": "Divide samples into multiple samples in tf.data input pipeline", "body": "\r\n**System information**\r\n- TensorFlow version: 2.4.1\r\n- Are you willing to contribute it: No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n[Stack overflow link](https://stackoverflow.com/questions/67439611/chunk-tensorflow-dataset-records-into-multiple-records)\r\n\r\nI have an unbatched tensorflow dataset that looks like this:\r\n\r\n```python\r\nds = ...\r\nfor record in ds.take(3):\r\n    print('data shape={}'.format(record['data'].shape))\r\n\r\n-> data shape=(512, 512, 87)\r\n-> data shape=(512, 512, 277)\r\n-> data shape=(512, 512, 133)\r\n```\r\n\r\nI want to divide each sample into multiple samples by slicing along the final axis. For example, feed the data to my network in chunks of depth 5. In the example above, the tensor of shape (512, 512, 87) would be divided into 17 tensors of shape (512, 512, 5). The final 2 rows of the matrix (tensor[:, :, 85:87]) could be discarded.\r\n\r\n```python\r\nchunked_ds = ...\r\nfor record in chunked_ds.take(1):\r\n    print('chunked data shape={}'.format(record['data'].shape))\r\n\r\n-> chunked data shape=(512, 512, 5)\r\n```\r\n\r\nFrom looking at the tf.data API, there is no method that takes a single sample and returns multiple samples. Is it possible implement this in a pipeline?\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @OliverP-1qbit, it sounds like [Dataset.flat_map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map) is exactly what you're looking for. The `flat_map` function takes a single sample and transforms it into multiple samples represented as a Dataset."]}, {"number": 49194, "title": "Build libtensorflowlite.so with TensorFlow ops supported for Android Error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu20.0.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:tensorflow-nightly\r\n- Python version:3.8.5\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):3.7.2\r\n- GCC/Compiler version (if compiling from source):9.3.0\r\n- CUDA/cuDNN version:no\r\n- GPU model and memory:no\r\n\r\n\r\n\r\n**Describe the problem**\r\nI need Tensorflow OPS to run tflite model inference in C++.\r\nI have added \"//tensorflow/lite/delegates/flex:delegate\" in /tensorflow/lite/BUILD according to the document\r\ndeps = [\r\n        \":framework\",\r\n        \":tflite_exported_symbols.lds\",\r\n        \":tflite_version_script.lds\",\r\n        \"//tensorflow/lite/delegates/flex:delegate\",\r\n        \"//tensorflow/lite/kernels:builtin_ops_all_linked\",\r\n    ],\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\nbazel build -j 12 -c opt --config=android_arm --cxxopt=\"-std=c++14\" --config=monolithic --local_ram_resources=4096 //tensorflow/lite:tensorflowlite\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n**This is .tf_configure.bazelrc**\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-Wno-sign-compare\r\nbuild --action_env ANDROID_NDK_HOME=\"/home/liuchen/android-ndk-r18b\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"21\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"30.0.3\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"28\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/home/liuchen/Android/Sdk\"\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\n*******************************************************************************************\r\n**Terminal Log**\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/liuchen/tensorflow-nightly/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/liuchen/tensorflow-nightly/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/liuchen/tensorflow-nightly/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --action_env ANDROID_NDK_HOME=/home/liuchen/android-ndk-r18b --action_env ANDROID_NDK_API_LEVEL=21 --action_env ANDROID_BUILD_TOOLS_VERSION=30.0.3 --action_env ANDROID_SDK_API_LEVEL=28 --action_env ANDROID_SDK_HOME=/home/liuchen/Android/Sdk\r\nINFO: Found applicable config definition build:short_logs in file /home/liuchen/tensorflow-nightly/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/liuchen/tensorflow-nightly/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:android_arm in file /home/liuchen/tensorflow-nightly/.bazelrc: --config=android --cpu=armeabi-v7a --fat_apk_cpu=armeabi-v7a\r\nINFO: Found applicable config definition build:android in file /home/liuchen/tensorflow-nightly/.bazelrc: --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --define=with_xla_support=false\r\nINFO: Found applicable config definition build:monolithic in file /home/liuchen/tensorflow-nightly/.bazelrc: --define framework_shared_object=false\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/5e0ab05514761a40824142ccbd75dab5aaa6c0d8.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: /home/liuchen/.cache/bazel/_bazel_liuchen/e2427695e0c557633b86683d5e3a86ac/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/40548a2974f1aea06215272d9c2b47a14a24e556.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/liuchen/tensorflow-nightly/WORKSPACE:23:14: in <toplevel>\r\n  /home/liuchen/tensorflow-nightly/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /home/liuchen/.cache/bazel/_bazel_liuchen/e2427695e0c557633b86683d5e3a86ac/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/liuchen/.cache/bazel/_bazel_liuchen/e2427695e0c557633b86683d5e3a86ac/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/ruy/archive/d37128311b445e758136b8602d1bbd2a755e115d.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/lite:tensorflowlite (152 packages loaded, 8399 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/liuchen/tensorflow-nightly/tensorflow/lite/BUILD:914:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): clang failed: error executing command external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang -shared -o bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/libtensorflowlite.so -Wl,-whole-archive ... (remaining 558 argument(s) skipped)\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::GetDelegates(int) const'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates::GetDelegates(int) const'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'typeinfo for tflite::ops::builtin::BuiltinOpResolver'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'typeinfo for tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'typeinfo name for tflite::ops::builtin::BuiltinOpResolver'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'typeinfo name for tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'vtable for tflite::ops::builtin::BuiltinOpResolver'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.pic.lo(register.pic.o): multiple definition of 'vtable for tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates'\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o): previous definition here\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/lite:tensorflowlite failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1756.851s, Critical Path: 1286.77s\r\nINFO: 2312 processes: 94 internal, 2218 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["@thaink could you take a look?", "> @thaink\u4f60\u53ef\u4ee5\u770b\u770b\u5417\uff1f\r\nThank you, I took the same operation to compile, V.2.5.0, V2.5.0-rc3, switch to Bazel3.1.0 to compile V2.4.0 all succeeded, only tf-nightly failed\r\n", "@SisComplex Can you try building with NDK r19c instead of r18b?\r\nOr maybe use the docker approach maybe simpler https://www.tensorflow.org/lite/guide/build_android#set_up_build_environment_using_docker.", "> @SisComplex Can you try building with NDK r19c instead of r18b?\r\n> Or maybe use the docker approach maybe simpler https://www.tensorflow.org/lite/guide/build_android#set_up_build_environment_using_docker.\r\n\r\nI switched to NDK r19c and it still shows Linking of rule'//tensorflow/lite:libtensorflowlite.so' failed (Exit 1), the same as before", "That is weird since //tensorflow/lite:tensorflowlite does not implicitly depends on tensorflow/lite/kernels:builtin_ops\r\nCould make a fork of tensorflow and upload your change there? I would like to take a closer look.", "> That is weird since //tensorflow/lite:tensorflowlite does not implicitly depends on tensorflow/lite/kernels:builtin_ops\r\n> Could make a fork of tensorflow and upload your change there? I would like to take a closer look.\r\n\r\nI downloaded the zip from the nightly branch of tensorflow this morning. I only modified the /tensorflow/lite/BULID file and ran ./configure.py and /tensorflow/lite/tools/make/download_dependencies.sh. Below are BULID and .tf_configure.bazelrc\r\n[.tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/6491236/default.tf_configure.bazelrc.txt)\r\n[BUILD.txt](https://github.com/tensorflow/tensorflow/files/6491237/BUILD.txt)\r\nIn the same environment I can build 2.4.0, 2.5.0-rc3, 2.5.0\r\n", "Hi, Could you please try changing dependency from  `//tensorflow/lite/kernels:builtin_ops_all_linked` \r\n to `//tensorflow/lite/kernels:builtin_ops`.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49194\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49194\">No</a>\n"]}, {"number": 49193, "title": "Can't save checkpoint when using multi-cells RNN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: Python 3.7.6\r\n- CUDA/cuDNN version: 11.3\r\n- GPU model and memory: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] \r\n\r\n**Describe the current behavior**\r\nI'm using multi-cells RNN with the following code:\r\n```python\r\nself.rnn_cells = [tf.keras.layers.LSTMCell(self.cfg.hidden_units) for _ in range(self.cfg.depth)]\r\nself.encoder = tf.keras.layers.RNN(\r\n    self.rnn_cells, return_sequences=True, return_state=True)\r\n```\r\nAnd I'm using `tf.train.Checkpoint` and `tf.train.CheckpointManager` to save checkpoint of my model.\r\n\r\nHowever, `self.manager.save()` throws an error that is paste below:\r\n```\r\nValueError: Unable to save the object ListWrapper([<tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x7f24a2c08f90>, <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x7f24a2bc5810>, <tensorflow.python.keras.layers.recurrent_v2.LSTMCell object at 0x7f24a2bc5f10>, <tensorflow_addons.seq2seq.attention_wrapper.AttentionWrapper object at 0x7f24a3ee1090>]) (a list wrapper constructed to track trackable TensorFlow objects). A list element was replaced (__setitem__, __setslice__), deleted (__delitem__, __delslice__), or moved (sort). In order to support restoration on object creation, tracking is exclusively for append-only data structures.\r\n\r\nIf you don't need this list checkpointed, wrap it in a non-trackable object; it will be subsequently ignored.\r\n```\r\n**Describe the expected behavior**\r\n`self.manager.save()`  can save the model correctly.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): No, I don't know how to solve this problem\r\n", "comments": ["Can you share a very minimal standalone example or a Colab that we could copy, paste and run?", "Thanks for your reply, and I have found the mistake I made when trying to write a minimal standalone example.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49193\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49193\">No</a>\n"]}, {"number": 49192, "title": "Update the reference of FTRL optimizer", "body": "Updated the wording to be consistent across the documentation.", "comments": []}, {"number": 49191, "title": "the ceritificate for mirror.tensorflow.org is invalid", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 10.2/8.2\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhile building tensorflow the following link is used by bazel to download an extension for aws:\r\nhttps://mirror.tensorflow.org/github.com/aws/aws-sdk-cpp/archive\r\nThe Certificate under that link isn't for mirror.tensorflow.org but for *.storage.googleapis.com\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nnot applicable\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["#37284", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49191\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49191\">No</a>\n"]}, {"number": 49190, "title": "GPU does not work ", "body": "```\r\nmodel.compile(\r\n", "comments": ["Can you share a very very minimal standalone code example that we could copy, paste and run or a Colab to reproduce this?\r\nPlease fill also the ISSUE template as it was removed.\r\n\r\nThanks.", "Please minimize your code surface with a single minimal Colab example to reproduce this cause It will take time to just read and analyze long examples.", "Please use the default TF 2.4.1 version in Colab (don't execute ``!pip install --upgrade tensorflow``):\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.version.VERSION)\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndevice_lib.list_local_devices()\r\n```\r\nWth the new TF 2.5.0 version you will not have the GPU available in Colab (probably CUDA versions misalignment)\r\n\r\nYou can close this and subscribe and upvote https://github.com/googlecolab/colabtools/issues/2013\r\n\r\n", "Yes don't execute any `pip install tensorflow`just use the `import` directly.", "> Yes don't execute any `pip install tensorflow`just use the `import` directly.\r\n\r\nHow to import previous version of tf? Because I do need to use GPU in colab.", "The default Is already installed", "On your local setup `pip install tensorflow==2.4.1`", "Can we close this?", "Hello,\r\n\r\nI just installed tf 2.5.0 and also find that my GPU is suddenly not detected.  @bhack above wrote that could be caused by \"(probably CUDA versions misalignment).\"  Could we get more info on that misalignment?  Will a future update of tf 2.5.0 correct this misalignment?  \r\n\r\nMust I update or reconfigure the various CUDA-related NVIDIA packages I installed to correct the problem?  I have CUDA 11.3 installed now, and that worked for tf 2.4.1.\r\n\r\nIs re-installing tf 2.4.1 using pip the only current way to get my GPU back?\r\n\r\nThank you...", "TF 2.5.0 pip works with Cuda 11.2", "Thank you for responding.  \r\n\r\nHmm.  I have 11.3 installed and CuDNN 8.2.  What should i do?  Is there a way to downgrade CUDA to 11.2?\r\n\r\n```\r\n>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Mar_21_19:24:09_Pacific_Daylight_Time_2021\r\nCuda compilation tools, release 11.3, V11.3.58\r\nBuild cuda_11.3.r11.3/compiler.29745058_0\r\n```\r\n\r\nI just downgraded tf to 2.4.1 (and tensorflow-io) and my GPU is recognized again, thank heavens.\r\n\r\nI'm running tf in a conda environment, Windows 10 x64.  The env is running Python 3.8.10.  I used `pip install tensorflow`.\r\n\r\nIt seems odd that tf GPU 2.4.1 will work with CUDA 11.3 and tf GPU 2.5.0 will not.  Is that a bug? ", "I only know that there was an issue compiling with 11.3 from sources. Probably you can try to comment at https://github.com/tensorflow/tensorflow/pull/48803", "Ok, thanks for the info.", "@Chugua ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the Tensorflow version,complete code and the dataset you are using. Thanks!"]}, {"number": 49189, "title": "Problem of optimisation with Tensorflow", "body": "Hello,\r\n\r\nMy Tensorflow version is as below:\r\n\r\nv1.12.1-53126-gd2083b259d1 (tf.version.GIT_VERSION),  2.5.0-dev20210318 (f.version.VERSION).\r\n\r\nWhen I performed training using Gradient Tapes for deep GP models (https://github.com/FelixOpolka/Deep-Gaussian-Process), the code for one iteration of optimisation is as below:\r\n\r\nfor x_batch, y_batch in zip(bch_X, bch_Y):\r\n\twith tf.GradientTape(watch_accessed_variables=False) as tape:\r\n\t\ttape.watch(dgp.trainable_variables)\r\n\t\tobjective = -model.elbo((x_batch, y_batch))\r\n\t\tgradients = tape.gradient(objective, model.trainable_variables)\r\n\toptimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\telbos.append(-objective.numpy())\r\nWhen I do multiple iterations of optimisations, say 100 iterations, the optimiser may fail and crash at some iteration, with error being a matrix inversion problem. It means the parameters reach a space which would make the matrix non-invertible. However, optimisers if they can compute the objective at the start should not fail and crash when they try parameters that fail the objective (they should just keep the current objective and sample somewhere else, as what the Scipy optimiser does.\r\n\r\nJust wondered if there is any solution for this?\r\n\r\nMany thanks.\r\n", "comments": ["@XiaoyuHy  Please provide the simple standalone code/ colab link along with input data to reproduce the issue at our end.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49189\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49189\">No</a>\n"]}, {"number": 49188, "title": "TF Lite converter does not always identify the `hard_swish` activation", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.2.3\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): v2.4.0-49-g85c8b2a817f (2.4.1)\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\nN/A\r\n\r\n**Description**\r\nCurrently TensorFlow Lite Converter can identify (i.e., convert to a single op) a hard swish activation which is defined as:\r\n\r\n`x * tf.nn.relu6(x + np.float32(3)) * np.float32(1. / 6.)`\r\n\r\n(see\u00a0[here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_hardswish.cc)). Unfortunately, it is not able to identify a hard swish defined in a \"Keras\" fashion (e.g., like in the official MobileNet v3 implementation\u00a0[here](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L444-L445)):\r\n\r\n`keras.layers.Multiply()[keras.layers.ReLU(6.)(x + 3.) * (1. / 6.), x]`\r\n\r\nIt would be sensible if it was able to identify `hard_swish` in both cases.\r\n\r\n**Standalone code to reproduce the issue** \r\nPlease see a Colab Notebook [here](https://colab.research.google.com/drive/1gBwh-OQ0t86-ErJpml4cILeUGdEGmRpe?usp=sharing) for a more detailed presentation of the issue.\r\n\r\n**Any other info / logs**\r\n\r\nN/A\r\n", "comments": ["@thaink could you take a look?", "The formula look the same. Let me check why it is not detected.", "I don't know if this is useful information, but the following logic did not reproduce the problem.\r\n```\r\n$ python3 -c 'import tensorflow as tf; print(tf.__version__)'\r\n2.5.0\r\n```\r\n```python\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import Model, Input\r\n\r\nx = Input(shape=(128,128,3), batch_size=1, dtype=tf.float32, name='input')\r\ny = x * tf.nn.relu6(x + 3) * 0.16666667\r\n\r\nmodel1 = Model(inputs=x, outputs=y)\r\nmodel1.summary()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model1)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"test_1.tflite\", \"wb\").write(tflite_model)\r\n\r\n#######################################################################\r\n\r\nx = Input(shape=(128,128,3), batch_size=1, dtype=tf.float32, name='input')\r\ny = x * tf.nn.relu6(x + np.float32(3)) * np.float32(1. / 6.)\r\n\r\nmodel2 = Model(inputs=x, outputs=y)\r\nmodel2.summary()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model2)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"test_2.tflite\", \"wb\").write(tflite_model)\r\n```\r\n```\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput (InputLayer)              [(1, 128, 128, 3)]   0                                            \r\n__________________________________________________________________________________________________\r\ntf.__operators__.add (TFOpLambd (1, 128, 128, 3)     0           input[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf.nn.relu6 (TFOpLambda)        (1, 128, 128, 3)     0           tf.__operators__.add[0][0]       \r\n__________________________________________________________________________________________________\r\ntf.math.multiply (TFOpLambda)   (1, 128, 128, 3)     0           input[0][0]                      \r\n                                                                 tf.nn.relu6[0][0]                \r\n__________________________________________________________________________________________________\r\ntf.math.multiply_1 (TFOpLambda) (1, 128, 128, 3)     0           tf.math.multiply[0][0]           \r\n==================================================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n\r\n\r\nModel: \"model_1\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput (InputLayer)              [(1, 128, 128, 3)]   0                                            \r\n__________________________________________________________________________________________________\r\ntf.__operators__.add_1 (TFOpLam (1, 128, 128, 3)     0           input[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf.nn.relu6_1 (TFOpLambda)      (1, 128, 128, 3)     0           tf.__operators__.add_1[0][0]     \r\n__________________________________________________________________________________________________\r\ntf.math.multiply_2 (TFOpLambda) (1, 128, 128, 3)     0           input[0][0]                      \r\n                                                                 tf.nn.relu6_1[0][0]              \r\n__________________________________________________________________________________________________\r\ntf.math.multiply_3 (TFOpLambda) (1, 128, 128, 3)     0           tf.math.multiply_2[0][0]         \r\n==================================================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n```\r\n![Screenshot 2021-05-22 23:40:59](https://user-images.githubusercontent.com/33194443/119230450-3626e680-bb57-11eb-85eb-4f11e0f570e4.png)\r\n\r\n```python\r\nx = Input(shape=(128,128,3), batch_size=1, dtype=tf.float32, name='input')\r\ny = x * tf.nn.relu6(x + 3) * 0.16666666\r\n\r\nmodel3 = Model(inputs=x, outputs=y)\r\nmodel3.summary()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model3)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"test_3.tflite\", \"wb\").write(tflite_model)\r\n```\r\n```\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput (InputLayer)              [(1, 128, 128, 3)]   0                                            \r\n__________________________________________________________________________________________________\r\ntf.__operators__.add (TFOpLambd (1, 128, 128, 3)     0           input[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf.nn.relu6 (TFOpLambda)        (1, 128, 128, 3)     0           tf.__operators__.add[0][0]       \r\n__________________________________________________________________________________________________\r\ntf.math.multiply (TFOpLambda)   (1, 128, 128, 3)     0           input[0][0]                      \r\n                                                                 tf.nn.relu6[0][0]                \r\n__________________________________________________________________________________________________\r\ntf.math.multiply_1 (TFOpLambda) (1, 128, 128, 3)     0           tf.math.multiply[0][0]           \r\n==================================================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n```\r\n![Screenshot 2021-05-22 23:49:18](https://user-images.githubusercontent.com/33194443/119230705-5e631500-bb58-11eb-940d-86e55878132a.png)\r\n", "The fix will be available in the tomorrow's tf-nightly version."]}, {"number": 49187, "title": "AttributeError: 'MirroredStrategy' object has no attribute 'experimental_run_v2'", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (Linux Ubuntu 18.04):\r\n- TensorFlow installed from (pip3):\r\n- TensorFlow version (use command below):\r\n`pip3 install tensorflow-gpu==2.4.1`\r\n- Python version:3.8.0\r\n- CUDA/cuDNN version: cuda11.3 driver 465.19\r\n- GPU model and memory: eight 3090 cards\r\n\r\ncodes as the [URL](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/custom_training.ipynb)\r\nnever modified\r\n\r\nbugs\r\n```\r\n2021-05-14 18:00:34.332244: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\nop: \"TensorSliceDataset\"\r\ninput: \"Placeholder/_0\"\r\ninput: \"Placeholder/_1\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n      type: DT_UINT8\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 28\r\n        }\r\n        dim {\r\n          size: 28\r\n        }\r\n        dim {\r\n          size: 1\r\n        }\r\n      }\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2021-05-14 18:00:34.366102: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\nop: \"TensorSliceDataset\"\r\ninput: \"Placeholder/_0\"\r\ninput: \"Placeholder/_1\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n      type: DT_UINT8\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 28\r\n        }\r\n        dim {\r\n          size: 28\r\n        }\r\n        dim {\r\n          size: 1\r\n        }\r\n      }\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2021-05-14 18:00:35.243992: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-05-14 18:00:35.262293: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\r\nTraceback (most recent call last):\r\n  File \"/data/prod/xulm1/custom_training.py\", line 113, in <module>\r\n    total_loss += distributed_train_step(x)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in user code:\r\n\r\n    /data/prod/xulm1/custom_training.py:99 distributed_train_step  *\r\n        per_replica_losses = strategy.experimental_run_v2(train_step,\r\n\r\n    AttributeError: 'MirroredStrategy' object has no attribute 'experimental_run_v2'\r\n\r\n\r\n```\r\n\r\nhow to deal with this ?\r\nthx\r\n", "comments": ["I found that v2 wasn't the Attribute, as below\r\n```\r\n>>> strategy.ex\r\nstrategy.experimental_distribute_dataset(\r\nstrategy.experimental_distribute_datasets_from_function(\r\nstrategy.experimental_distribute_values_from_function(\r\nstrategy.experimental_local_results(\r\n**strategy.experimental_run(**\r\n```\r\nand i use the last \r\nbut got another bug\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/prod/xulm1/custom_training.py\", line 113, in <module>\r\n    total_loss += distributed_train_step(x)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    /data/prod/xulm1/custom_training.py:99 distributed_train_step  *\r\n        per_replica_losses = strategy.experimental_run(train_step,\r\n\r\n    TypeError: experimental_run() got an unexpected keyword argument 'args'\r\n\r\n```", "Hi @ucasiggcas ,I think that ```mirroredstrategy.experimental_run_v2```  has been deprecated since Tensorflow 2.2 .This can found from the release pages:\r\nhttps://github.com/tensorflow/tensorflow/blob/93360e5c3bb1c7f3d5de2267d564bc8c77dfe3de/RELEASE.md\r\nIt has been renamed to ```mirroredstrategy.run``` method as specified in this segment:\r\n\r\n```Deprecated experimental_run_v2 method for distribution strategies and renamed the method run as it is no longer experimental.```\r\n\r\n Raised a PR  #49219  for logging messages.", "On a side note for using gpus with TF 2.4 pre built binary compatible cuda version is 11.0 and cudnn 8.0", "ok\r\nthx ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49187\">No</a>\n"]}, {"number": 49186, "title": "How to convert a channel first Keras model to channel last model?", "body": "Hello,\r\n\r\nI have a pre-trained Keras model (in h5 format) where all the layers operate on channel first data format. I want to convert this model to operate on the channel last data format (the default data format). \r\nDoes TensorFlow support this out of the box? Any help or pointers on how to do this is much appreciated.\r\n\r\nFor some clarity, the current model summary looks like this:\r\n\r\n    Model: \"model\"\r\n    __________________________________________________________________________________________________\r\n    Layer (type)                    Output Shape         Param #     Connected to\r\n    ==================================================================================================\r\n    input0 (InputLayer)             [(None, 3, 240, 320) 0\r\n    __________________________________________________________________________________________________\r\n    259_pad (ZeroPadding2D)         (None, 3, 242, 322)  0           input0[0][0]\r\n    __________________________________________________________________________________________________\r\n    259 (Conv2D)                    (None, 16, 120, 160) 432         259_pad[0][0]\r\n    __________________________________________________________________________________________________\r\n    260 (BatchNormalization)        (None, 16, 120, 160) 64          259[0][0]\r\n    __________________________________________________________________________________________________\r\n    261 (Activation)                (None, 16, 120, 160) 0           260[0][0]\r\n    __________________________________________________________________________________________________\r\n    262_pad (ZeroPadding2D)         (None, 16, 122, 162) 0           261[0][0]\r\n    __________________________________________________________________________________________________\r\n    262 (DepthwiseConv2D)           (None, 16, 120, 160) 144         262_pad[0][0]\r\n\r\nAs you can see, it's in the channel first format. I want to convert each layer in the model to operate on channel last format. So the ideal model summary will be as follows:\r\n\r\n    Model: \"model\"\r\n    __________________________________________________________________________________________________\r\n    Layer (type)                    Output Shape         Param #     Connected to\r\n    ==================================================================================================\r\n    input0 (InputLayer)             [(None, 240, 320, 3) 0\r\n    __________________________________________________________________________________________________\r\n    259_pad (ZeroPadding2D)         (None, 242, 322, 3)  0           input0[0][0]\r\n    __________________________________________________________________________________________________\r\n    259 (Conv2D)                    (None, 120, 160, 16) 432         259_pad[0][0]\r\n    __________________________________________________________________________________________________\r\n    260 (BatchNormalization)        (None, 120, 160, 16) 64          259[0][0]\r\n    __________________________________________________________________________________________________\r\n    261 (Activation)                (None, 120, 160, 16) 0           260[0][0]\r\n    __________________________________________________________________________________________________\r\n    262_pad (ZeroPadding2D)         (None, 122, 162, 16) 0           261[0][0]\r\n    __________________________________________________________________________________________________\r\n    262 (DepthwiseConv2D)           (None, 120, 160, 16) 144         262_pad[0][0]\r\n\r\nThanks.\r\n\r\n", "comments": ["You may try https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_image_data_format\r\n```python\r\ntf.keras.backend.set_image_data_format('channels_last')\r\n```", "Thank you @ymodak  for your resposne.\r\n\r\nYes, it didn't work. The model is pretrained and is in channel first format. Setting `tf.keras.backend.set_image_data_format('channels_first')`  makes the default ordering to be channels_first and didn't affect the loaded model.", "Have you tried to resave your model as in https://github.com/tensorflow/models/issues/3167#issuecomment-371410527 ?", "Thank you for the pointer. \r\n\r\nI have only the trained model. From the comment (2nd part) @bhack  mentioned do I have to do \"network surgery\"?\r\n\r\n> 2 .The above solution works when the training files were available. In the case where I had only the output of tf.estimator.export_savedmodel, I had to do a network surgery. I made a clone of the graph, in channel_last format, loaded it in and assigned all variables, values from the trained model. This new graph runs fine on CPU.\r\n\r\nIf yes, can you share a sample code on how it is done on a Conv2D layer?\r\n\r\nAny help is really appreciated. Thank you.", "I think that without the training model it could be a nice tutorial for Keras-io. Probably you could open a request at https://github.com/keras-team/keras-io", "If you will track the request there please close this and leave a reference to the new ticket.", "I will open a request at keras-io, but I'm would be really grateful if you could add in an example on how to do this for just one layer (say, Conv2D).", "I don't know if you are doing this for performance or not but e.g. for your Conv2d request if you see the internals there are some constrains on the format in different places or implicit/internal conversions. \r\nE.g. Search `NHWC` ad `NCHW` in this source code:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc.\r\n ", "Thank you.  This is new info.\r\n\r\nDoes this [line](https://github.com/tensorflow/tensorflow/blob/9d461da4cb0af2f737bbfc68cca3f6445f1ceb60/tensorflow/core/kernels/conv_ops.cc#L86) - `CHECK(data_format == FORMAT_NHWC) << \"Generic conv implementation only \"` at `conv_ops.cc` mean that internally Conv2D operates in NHWC? \r\n\r\nSo, if I parse Conv2D layers in the model and set their channel_format as NHWC, will it give the same output? I was planning to transpose the weights as well while setting the channel format to NHWC.", "That was for  generic and e.g. also on CPU you can see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L246-L247", "I have raised an [issue](https://github.com/keras-team/keras-io/issues/467) at keras-io. Will wait for their response to convert the model.\r\nThank you for your help and pointers. Really helpful. "]}, {"number": 49185, "title": "Installing tensorflow-gpu and upgrade grpcio", "body": "I have never had this problem before today, but now when I run code I have these errors\r\n\r\n\r\n", "comments": ["Can you try with ```!pip install --upgrade tensorflow-gpu >> /dev/null```", "Are you trying to install this in a fresh python env?", "On colab: `pip install --upgrade tensorflow-gpu` will install tensorflow 2.5.0.", "Why you need to install grpcio explicitly? TF 2.5.0 already install `grpcio~=1.34.0`", "Ok but i think this is tracked at https://github.com/tensorflow/tensorflow/issues/49190 and you can close this.", "Yes but we cannot have duplicate tickets", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49185\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49185\">No</a>\n"]}, {"number": 49184, "title": "Is it possible to use Sparse Tensor in Distributed Tensorflow ? ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHey everyone,\r\n\r\nI have been trying to feed sparse tensor as an input into my distributed tensorflow model. The struggle is real at the moment, it doesn't seem to work -> all I get is this error message which makes me wonder is it even possible with tensorflow ?\r\n\r\n`TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"cond_1/Identity:0\", shape=(None, 3), dtype=int64, device=/job:worker/replica:0/task:0/device:CPU:0), values=Tensor(\"sequential/dense/Cast:0\", shape=(None,), dtype=float32, device=/job:worker/replica:0/task:0/device:CPU:0), dense_shape=Tensor(\"cond_1/Identity_2:0\", shape=(3,), dtype=int64, device=/job:worker/replica:0/task:0/device:CPU:0)). Consider casting elements to a supported type.`\r\n\r\nIndeed, I have been reading other issue that others faced when using sparse tensor and the workaround that had been mentioned was to cast it to dense during runtime. However, the the training speed is heavily effected by it, at least for me causing it run 1 hour and 30 minutes per epoch whereas running it locally its 4 minute per epoch (**mind blown**) \r\n\r\nBest regards and stay safe\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sat2000pts ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the Tensorflow version, complete code and the dataset you are using. Thanks!", "@sat2000pts \r\nfrom the error logs it does not appear to be an incompatibility issue between the sparse tensor and training pipeline, can you isolate a single layer, try feeding the sparse inputs to it and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49184\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49184\">No</a>\n"]}, {"number": 49183, "title": "BUG: `RESHAPE` op is not always compatible with the `resize_tensor_input` method of the TF Lite Interpreter", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.2.3\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): v2.4.0-49-g85c8b2a817f (2.4.1)\r\n\r\n**Provide the text output from tflite_convert**\r\nN/A\r\n\r\n**Description**\r\n`RESHAPE` op (as converted from `tf.reshape` + `tf.shape` functions) is not compatible with the `resize_tensor_input` method of the TF Lite `Interpreter` when the tflite model is obtained from the model with the known input shape (e.g., the known shape being `(1, 1, 16000)`). In this case the `allocate_tensors` method called after `resize_tensor_input` (e.g., with the new input shape being `(1, 1, 32000)`) results in the\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (32000 != 16000)Node number 0 (RESHAPE) failed to prepare.\r\n```\r\n\r\nOn the other hand, when the tflite model is obtained from the model with partially unknown input shape (e.g. `(1, 1, None)`) then `resize_tensor_input` works correctly.\r\n\r\n**Standalone code to reproduce the issue** \r\nPlease see a Colab Notebook [here](https://colab.research.google.com/drive/1WEQbsJeB3cqQKVfsfuQsXvyTfh3FeF9A?usp=sharing) for a more detailed presentation of the issue (the last cell in the notebook results in the `RuntimeError` mentioned above).\r\n\r\n**Any other info / logs**\r\nN/A\r\n", "comments": ["This is an intended behavior. If you want to resize input tensors without any issues, the corresponding TF model should have dynamic dimension support on the specific dimensions, which should be overided."]}, {"number": 49182, "title": "Exception ignored in: function CapturableResource.__del__ after upgrading to TF 2.5", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS X 11.2.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nAfter upgrading from TF 2.4.1 to TF 2.5.0 i see strange messages in console when run training/testing of my model. See logs below.\r\n\r\n**Describe the expected behavior**\r\nNo ignored exceptions should be printed\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): No\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1IpmIj3WI17j6JkfgGWH0YEpowja7m71Y?usp=sharing\r\n \r\n\r\n**Other info / logs** \r\nIn some cases i got only such logs\r\n```python\r\nException ignored in: <function CapturableResource.__del__ at 0x15ad1c790>\r\nTraceback (most recent call last):\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\r\n    self._destroy_resource()\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\r\n    filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3279, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\nAttributeError: 'NoneType' object has no attribute '__wrapped__'\r\n```\r\n\r\nBut in another, it prepended with another one:\r\n```python\r\n  Traceback (most recent call last):\r\n    File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 2560, in get_attr\r\n      pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n  tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'raggedconcat_assert_equal_3_assert_assertguard_placeholder_1' has no attr named '_class'.\r\n  \r\n  During handling of the above exception, another exception occurred:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\r\n      self._destroy_resource()\r\n  ...\r\n    File \"/Users/alex/.pyenv/versions/3.8.6/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\r\n      out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  AttributeError: 'NoneType' object has no attribute '__wrapped__'\r\n```", "comments": ["Can you share a very very minimal standalone code example that we could copy, paste and run or a Colab to reproduce this?", "Here is a sample code to reproduce issue\r\n[test.py.zip](https://github.com/tensorflow/tensorflow/files/6491933/test.py.zip)\r\n\r\nAlso i made a demo in colab https://colab.research.google.com/drive/1IpmIj3WI17j6JkfgGWH0YEpowja7m71Y?usp=sharing\r\nNote, that it should be runned from console, not from notebook\r\n\r\nAs far as i noticed, the issue gone if code from \"main()\" function is moved to the top level (drop \"def main\" line and remove tabs in body code)\r\n\r\n\r\n", "@shkarupa-alex \r\nI  reproduced the code shared with no errors in tf-nightly.please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/c4b41038bc3509513c5403ec6ca96b1f/exception-ignored-in-function-capturableresource-__del__.ipynb).Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@shkarupa-alex \r\n\r\nPlease let us know if the issue still persists.Thanks", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49182\">No</a>\n", "I am seeing this exception get printed as a warning quite frequently, and found this example of it also showing up in the Tensorflow docs:\r\nhttps://www.tensorflow.org/tfx/tutorials/transform/census?hl=el#put_it_all_together_2\r\n\r\n```\r\nException ignored in: <function CapturableResource.__del__ at 0x7ff6dd76f050>\r\nTraceback (most recent call last):\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 277, in __del__\r\n    self._destroy_resource()\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 924, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3022, in __call__\r\n    filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3289, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\nAttributeError: 'NoneType' object has no attribute '__wrapped__'\r\n```\r\n\r\nIs there a way to actually suppress this, if it should in fact be ignored?", "Ccing @robert-crowe. \r\n\r\nFacing a similar problem. ", "Any update on this?", "> Any update on this?\r\n\r\nNo issue at 2.6.0", "This is a side effect of how Evaluator expects to receive examples, and can be fixed by adding to the ModelSpec in the EvalConfig:\r\n```\r\nmodel_specs {\r\n  signature_name: \"serving_default\"   <--- ADD THIS\r\n  label_key: \"tips\"\r\n}\r\n```\r\nSo far, this has always fixed this problem for me."]}, {"number": 49181, "title": "Problems with using tf.data.Datasets with MirroredStrategy with eager execution disables", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.8\r\n\r\n\r\n**Describe the current behavior**\r\nWhen training a model that uses tf.data.Datasets with MirroredStrategy while eager execution is disabled, the function (under backend.GraphExecutionFunction) below does not know how to handle inputs that do not include numpy arrays. \r\n\r\n```\r\n  def __call__(self, inputs):\r\n    inputs = nest.flatten(inputs, expand_composites=True)\r\n\r\n    session = get_session(inputs)\r\n    feed_arrays = []\r\n    array_vals = []\r\n    feed_symbols = []\r\n    symbol_vals = []\r\n    for tensor, value in zip(self.inputs, inputs):\r\n      if value is None:\r\n        continue\r\n\r\n      if tensor_util.is_tensor(value):\r\n        # Case: feeding symbolic tensor.\r\n        feed_symbols.append(tensor)\r\n        symbol_vals.append(value)\r\n      else:\r\n        # Case: feeding Numpy array.\r\n        feed_arrays.append(tensor)\r\n        # We need to do array conversion and type casting at this level, since\r\n        # `callable_fn` only supports exact matches.\r\n        tensor_type = dtypes_module.as_dtype(tensor.dtype)\r\n        array_vals.append(np.asarray(value,\r\n                                     dtype=tensor_type.as_numpy_dtype))\r\n\r\n    if self.feed_dict:\r\n      for key in sorted(self.feed_dict.keys()):\r\n        array_vals.append(\r\n            np.asarray(self.feed_dict[key], dtype=key.dtype.base_dtype.name))\r\n\r\n    # Refresh callable if anything has changed.\r\n    if (self._callable_fn is None or feed_arrays != self._feed_arrays or\r\n        symbol_vals != self._symbol_vals or\r\n        feed_symbols != self._feed_symbols or self.fetches != self._fetches or\r\n        session != self._session):\r\n      self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\r\n\r\n    fetched = self._callable_fn(*array_vals,\r\n                                run_metadata=self.run_metadata)\r\n    self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n    output_structure = nest.pack_sequence_as(\r\n        self._outputs_structure,\r\n        fetched[:len(self.outputs)],\r\n        expand_composites=True)\r\n    # We need to evaluate any composite tensor objects that have been\r\n    # reconstructed in 'pack_sequence_as', since otherwise they'll be output as\r\n    # actual CompositeTensor objects instead of the value(s) contained in the\r\n    # CompositeTensors. E.g., if output_structure contains a SparseTensor, then\r\n    # this ensures that we return its value as a SparseTensorValue rather than\r\n    # a SparseTensor.\r\n    return nest.map_structure(self._eval_if_composite, output_structure)\r\n\r\n```\r\n\r\nIn particular, these lines fail when the inputs are only symbolic tensors with symbolic values.\r\n\r\n```\r\n    fetched = self._callable_fn(*array_vals,\r\n                                run_metadata=self.run_metadata)\r\n    self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n    output_structure = nest.pack_sequence_as(\r\n        self._outputs_structure,\r\n        fetched[:len(self.outputs)],\r\n        expand_composites=True)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nThis function should be able to run when there are only feed_symbols and symbols_vals. Unless I am missing something? Does this function need to have feed_arrays/array_vals? \r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49181\">No</a>\n"]}, {"number": 49179, "title": "TF2.5.0 pip package depends on keras-nightly", "body": "**Describe the problem**\r\n\r\n`pip install tensorflow==2.5.0` installs `keras-nightly`. Is that intended?\r\n\r\n**Any other info / logs**\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/tools/pip_package/setup.py#L105-L107", "comments": ["/cc @scottzhu @mihaimaruseac do you mind taking a look at this?", "It wasn't intended but given than there are no more `keras-nightly` releases with the same major.minor, this is effectively frozen.\r\n\r\nWe will patch in a future patch release once Keras has a new stable release, if possible. If not, we'll fix in 2.6", "@bersbersbers \r\nCould you  please check the above comment by **mihaimaruseac**  and please confirm if the issue still persist.Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49179\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49179\">No</a>\n", "There is a similar issue with TF2.8.0, which depends on `tf-estimator-nightly`. I won't reopen this issue since I believe it does not have any technical implications, but it surely looks weird to the user (and I wouldn't be surprised if `tf-estimator-nightly` interfered with a separately installed `tensorflow-estimator` package, which may be a different version.)", "That is correct, it is an issue on our release process that this keeps happening. Apologies for this.\r\n\r\nFortunately, I don't think there will be any issue in this case, but if we discover a bug that impact users we will do a patch release to fix. Estimator is deprecated and used only in maintenance mode, so there shouldn't be any breakages, but we're open to being wrong and fixing this in 2.8.1, if needed."]}, {"number": 49178, "title": "[determinism] Add non-sparse softmax/xent GPU-determinism", "body": "This PR adds and tests deterministic forward and backward operation of `tf.nn.softmax_cross_entropy_with_logits` when running on a GPU.\r\n\r\nNote that there are changes and enhancements to the existing tests that may be obscured by the restructuring of the test files.\r\n\r\nThanks to @reedwm for providing support and guidance on this PR, including looking into the arithmetic equivalence of the forward and backward operation of the python-level solution.\r\n\r\nNote that a naive implementation of softmax followed by cross-entropy is not as numerically stable as the version implemented here (and in the existing Eigen-based/C-level implementation) in which the log in the cross-entropy function is moved back into the softmax, changing it into a log-softmax and changing the cross-entropy function into a dot-product.  log-softmax does not demand as large dynamic ranges as softmax.\r\n\r\nNote that the following tests do not pass on this deterministic implementation (and have been disabled):\r\n\r\n  * Backprop to logits when there is only a single class (the forward path passes). See `testSingleClass`.\r\n  * Backprop to logits when labels are broadcast (the forward path passes). See `testLabelsBroadcast`.\r\n\r\nI have not yet been able to determine the reason for this, and I don't know if it's because the existing functionality is incorrect or if the new, deterministic functionality is incorrect. For the single class case, for example, it seems to me that the correct gradients should all be zero (which is what the new, deterministic implementation provides). It seems as though the above two use cases  (single class and broadcast labels) would rarely be used; it's not obvious to me what the applications of these use cases would be, and these functionalities are also not documented. I have added TODO comments for me to look into this more deeply. @reedwm, feel free to explore.\r\n\r\nUPDATE: After further investigation, it has been revealed that the gradients only mismatch between the nondeterministic and deterministic implementations when the labels vector is not a valid probability distribution, as required (but not enforced) by the API. See [this comment](https://github.com/tensorflow/tensorflow/pull/49178#discussion_r641061999) for more information.\r\n\r\nThis PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#softmax-xent).\r\n\r\ncc @sanjoy @nluehr ", "comments": ["Sorry for long periods between reviews, I had taken some time off.", "> Sorry for long periods between reviews, I had taken some time off.\r\n\r\nNo problem, @reedwm. I hope you had a relaxing and enjoyable break."]}, {"number": 49177, "title": "File system scheme '[local]' not implemented on TPU", "body": "Hello, I have a task to train a model with millions of images using TPU, so I plan to load local data batch by batch when training. The main function to load data is shown below.\r\n\r\n`def batch_processing(image):\r\n    image1 = tf.image.decode_jpeg(tf.io.read_file(image), 3)\r\n    return tf.image.resize(image1, [IMGSIZE, IMGSIZE])/255.\r\n\r\n\r\ndef data_loading(path, data, batch, buffer=1000, training=False):\r\n    img1, start1 = [], 0\r\n    \r\n    while start1 < len(data):\r\n        path1 = data['image_id'][start1:start1+buffer]\r\n        img1 += path1.apply(lambda x: path+'train/'+x[0]+'/'+x[1]+'/'+x[2]+'/'+x+'.png').tolist()\r\n        start1 += buffer\r\n\r\n    set1 = tf.data.Dataset.from_tensor_slices((img1))\r\n    set1 = set1.map(batch_processing, num_parallel_calls=AUTOTUNE)\r\n    set1 = set1.shuffle(buffer).batch(batch) if training else set1.batch(batch)\r\n    set1 = set1.prefetch(buffer_size=AUTOTUNE)\r\n    return tpu_strategy.experimental_distribute_datasets_from_function(lambda _: set1)`\r\n\r\nHowever, I have got an error \"File system scheme '[local]' not implemented\". Since this code works on GPU, I think such local file scheme isn't supported on TPU currently. Could you please tell me how to deal with this problem, when I have too many local images to load?\r\n\r\nThank you!\r\n", "comments": ["Instead of using something like tf.io use os or PIL\r\nhttps://stackoverflow.com/questions/62870656/file-system-scheme-local-not-implemented-in-google-colab-tpu", "> Instead of using something like tf.io use os or PIL\r\n> https://stackoverflow.com/questions/62870656/file-system-scheme-local-not-implemented-in-google-colab-tpu\r\n\r\nI see, thank you.", "See also https://github.com/google-research/language/issues/54"]}, {"number": 49176, "title": "[ROCm] Hipsparse version of Sparse Add Operation", "body": "Bringing hipsparseXcsrgeam to Tensorflow for sparse add operations on ROCm enabled devices. \r\n\r\nBuilds on https://github.com/tensorflow/tensorflow/pull/49108 to flesh out Hipsparse functionality for Tensorflow. ", "comments": ["@cheshire this is now ready for review. ", "@stevenireeves  Can you please resolve conflicts? Thanks!", "@stevenireeves  Can you please resolve conflicts? Thanks!", "@gbaned, some of the functions needed from https://github.com/tensorflow/tensorflow/pull/49108 were rolledback. These need to be addressed before this can move forward. ", "@gbaned looks like the cancellation of the rollback removed the conflict. It should be ready to proceed. "]}, {"number": 49175, "title": "Tensorflow GPU Instructions should be updated for TF 2.5 and CUDA 11.2", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/gpu#windows_setup\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/install/gpu.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\n[TensorFlow 2.5](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0) is built against CUDA 11.2 and cuDNN 8.1.0. However, the GPU instructions still refer to CUDA 11.0 and cuDNN 8.0.4. I believe that for the sake of clarity these should be updated to match TF 2.5. This includes both the installation commands for Linux and the path commands for Windows.\r\n\r\n### Submit a pull request?\r\n\r\nI'd be glad to submit a PR for the windows instructions, but I don't currently have access to a machine I can use to verify the Linux steps.\r\n\r\n<!--\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n--->", "comments": ["@TylerADavis \r\nThe GPU page has been updated with latest information on [test build configurations](https://www.tensorflow.org/install/source?hl=en), kindly move this to closed status as resolved.", "@Saduf2019 Which page is it that got updated? While I can see that the [GPU tested configurations](https://www.tensorflow.org/install/source?hl=en#gpu) reference CUDA 11.2 and cuDNN 8.1, the GPU install page still references CUDA 11.0 and cuDNN 8.0, both on [github](https://github.com/tensorflow/docs/blob/master/site/en/install/gpu.md) and the [live site](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_110)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue should not be closed yet. It has been partially addressed by tensorflow/docs#1900, but not fully addressed yet.", "Moving this issue to closed status as PR is merged.", "The PR https://github.com/tensorflow/docs/pull/1900 did not solve this issue.  The installation instructions still refer to out-of-date libraries.  Section https://www.tensorflow.org/install/gpu#install_cuda_with_apt instructions still refer to CUDA 11.0, not 11.2 as the first half of docs specify.\r\n\r\nThere is currently no description of which libnvinfer libraries to install and how to do that.  It needs to be added to the gpu.md page install_cudo_with_apt instructions."]}, {"number": 49174, "title": "Update Estimator version after estimator final release", "body": "Update Estimator version after estimator final release", "comments": []}, {"number": 49173, "title": "[Crash fix] Fix cudaMallocAsync crashes.", "body": "The first commit fixes #48869. The second commit fixes another follow up crashes when using TF_GPU_ALLOCATOR=cuda_malloc_async.\r\n\r\nThe 2 fixes are:\r\n- The Allocator API have the statistics optional. But the function [BaseGPUDeviceFactory::CreateGPUDevice()](https://github.com/tensorflow/tensorflow/blob/aa08697dcb98135ee39ba00ee08b5c1a28cfde61/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1403) check that they are available.\r\n- Bad handling of the ptr to the compute stream. It is a ptr to the compute stream that is passed, not the compute stream itself.\r\n@sanjoy @chsigg", "comments": ["I updated this PR. Now the new configuration options is experimental.\r\nI added what got lost in my rebase the use of that new configuration option.\r\nI also updated the test to verify that the right allocator got used. If you have another idea how to do this, tell me.", "Just checking if things are good on your side. It was approved 6 days ago and didn't get in yet.", "@nouiz Could you please help resolve the merge conflict? Thank you!", "Rebased.", "@nouiz Thank you! It seems there are still conflicting files though, namely:\r\n```\r\ntensorflow/core/protobuf/config.proto\r\ntensorflow/tools/api/golden/v1/tensorflow.-g-p-u-options.pbtxt\r\n```\r\nCould you please help resolve the conflicts again? Sorry for the inconvenience!\r\n", "I fixed the new conflict.\r\nI also found a bug in the changes that caused the first merge conflict. I added the fix as it made my tests fail in CUDA 11.3.", "Just to be sure, I see many \"Internal CI build failed\". Do they block this PR? What is their cause? The windows error is that bazel didn't found the strategy \"sandboxed\". That have been reverted to my knowledge.", "> Just to be sure, I see many \"Internal CI build failed\". Do they block this PR? What is their cause? The windows error is that bazel didn't found the strategy \"sandboxed\". That have been reverted to my knowledge.\r\n\r\nThe problem was an unused variable. @penpornk has adjusted your patch internally, then that patch needed approval internally, now it is theoretically ready to land (if the tests pass).", "Thanks for the update.", "In some of the refactoring of cudaMallocAsync, when there is an OOM a deadlock was added.\r\nHere is a commit that fix it: https://github.com/tensorflow/tensorflow/commit/4cab1820acd4f9fa031bc59884e4bc461d33c3fa\r\n\r\nI wasn't sure if you wanted to include it in this PR or not. I can make another PR with the trivial fix if you want. Just tell me how you want to handle it.", "> In some of the refactoring of cudaMallocAsync, when there is an OOM a deadlock was added.\r\n> Here is a commit that fix it: [4cab182](https://github.com/tensorflow/tensorflow/commit/4cab1820acd4f9fa031bc59884e4bc461d33c3fa)\r\n> \r\n> I wasn't sure if you wanted to include it in this PR or not. I can make another PR with the trivial fix if you want. Just tell me how you want to handle it.\r\n\r\nI think landing that separately is fine. As far as I can tell, the problem is a missing BUILD dependency for the new header include in the gpu_device_test. I added this as review comment on the internal change, hopefully that is the only remaining thing needed to fix to get it landed.", "Any update? I think TF2.6 cutoff is soon. It would be great to get it included so people can start to use it manually.\r\nIs there anything I can do to help?", "@nouiz Apologies for the delay! A couple of tests failed the heapcheck (memory leaks). (You'll have to build them and run the binary with pprof to reproduce. The problem is these tests seem to have problems building in my open source environment right now -- so I'm not sure if you'll be able to build them.)\r\n```\r\n$ bazel build --config=cuda --config=opt  \\\r\n   //tensorflow/core/common_runtime/gpu:gpu_device_test\r\n$ bazel build --config=cuda --config=opt \\\r\n   //tensorflow/core/common_runtime/gpu:gpu_device_test_gpu\r\n```\r\n\r\nAny idea where the leak could come from? I'm sorry I haven't had time to take a closer look at what the PR is doing yet. I'll try to do that today. Will try to get this into 2.6.", "I looked at the code. But I do not understand what could leak.\r\nHere there is a different cast: https://github.com/tensorflow/tensorflow/pull/49173/files#diff-d54c8000677cfd5b7bbc09963559b66949e9738b944210ee215dff19730ee296R88\r\n\r\nI do not know pprof. Is there any options I must specify? I'll try to reproduce the error. Just in case I can help.", "@nouiz Thank you very much for offering to help! :)\r\nI think I've fixed the leaks. (I added the code that delete `gpu_bfc_allocator` back. But not the suballocator.) \r\n```c++\r\n      delete gpu_bfc_allocator;\r\n      gpu_bfc_allocator = nullptr;\r\n```\r\n\r\nThe merge is now stuck because of some (seemingly unrelated) infra issues instead. I'll keep you posted.\r\n\r\nJust for reference, here is how to use `pprof` to check for memory leaks (paths are based on Ubuntu 18.04 -- may be different on different OSes):\r\n```\r\n# Install pprof and TCMalloc\r\n$ sudo apt install libgoogle-perftools-dev google-perftools\r\n$ export PPROF_PATH=/usr/bin/google-pprof\r\n\r\n# Build the test\r\n$ bazel build --config=cuda --config=opt //tensorflow/core/common_runtime/gpu:gpu_device_test\r\n\r\n# Run the test with TCMalloc and pprof's heapcheck enabled.\r\n$ HEAPCHECK=normal LD_PRELOAD=/usr/lib/libtcmalloc.so TF2_BEHAVIOR=1 \\\r\n    bazel-bin/tensorflow/core/common_runtime/gpu/gpu_device_test\r\n```\r\n\r\n", "It is now merged. Thanks for the help.\r\n\r\nFor pprof last instruction \r\n```HEAPCHECK=normal LD_PRELOAD=/usr/lib/libtcmalloc.so TF2_BEHAVIOR=1 \\\r\n    bazel-bin/tensorflow/core/common_runtime/gpu/gpu_device_test```\r\n\r\nI do not see a call to pprof. Shoudn't it be:\r\n```\r\nHEAPCHECK=normal LD_PRELOAD=/usr/lib/libtcmalloc.so TF2_BEHAVIOR=1 \\\r\n    $PPROF_PATH bazel-bin/tensorflow/core/common_runtime/gpu/gpu_device_test\r\n```\r\nOr something like that?", "Thank you very much again for your patience!\r\n\r\n> I do not see a call to pprof. Shoudn't it be:\r\n> ```\r\n> HEAPCHECK=normal LD_PRELOAD=/usr/lib/libtcmalloc.so TF2_BEHAVIOR=1\r\n> $PPROF_PATH bazel-bin/tensorflow/core/common_runtime/gpu/gpu_device_test\r\n> ```\r\n> Or something like that?\r\n\r\nGreat question. [Heap checker](https://gperftools.github.io/gperftools/heap_checker.html) is part of TCMalloc. So we only need to link the binary to TCMalloc and turn on heap checker with the environment variable `HEAPCHECK`. It uses `pprof` underneath, that's why we need to install and set the path to `pprof`. :)"]}, {"number": 49172, "title": "Error importing from Keras models - AttributeError: module 'tensorflow.compat.v2.__internal__' has no attribute 'tf2'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- Keras version: 2.4.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe following error is raised when importing `Model` from keras.models \r\n\r\n```bash\r\ndosma/models/oaiunet2d.py:14: in <module>\r\n    from keras.models import Model\r\n/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/keras/__init__.py:20: in <module>\r\n    from . import initializers\r\n/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/keras/initializers/__init__.py:124: in <module>\r\n    populate_deserializable_objects()\r\n/opt/hostedtoolcache/Python/3.7.10/x64/lib/python3.7/site-packages/keras/initializers/__init__.py:49: in populate_deserializable_objects\r\n    LOCAL.GENERATED_WITH_V2 = tf.__internal__.tf2.enabled()\r\nE   AttributeError: module 'tensorflow.compat.v2.__internal__' has no attribute 'tf2'\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo error should be thrown\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): if this requires a fix, yes\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom keras.models import Model\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Please import keras from Tensorflow:\r\n```\r\nimport tensorflow as tf\r\n```\r\nAnd use `tf.keras`\r\n", "Great, thanks! I noticed the separate tf/keras imports worked up until tensorflow=2.4.1 - it seems as though the recent tensorflow 2.5.0 release caused this issue. Would it be useful to add this to the list of incompatibilities between the `keras` and `tensorflow` packages ?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49172\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49172\">No</a>\n"]}, {"number": 49171, "title": "Fixing CMake release build for Win32 target", "body": "- Issue #47166 raises the concern about a common build failure when building tensorflowlite_c.dll with cmake and msvc;\r\n- The issue is not related to tensorflow source code, instead it is probably related to a compiler (MSVC) issue when handling /O2 code optimization;\r\n- The proposal is to update CMakeLists.txt so it can recognize a MSVC x86 build and update the compiler optimization flags: from /O2 to /O1;", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49171) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@terryheo could you review this?", "Hi @terryheo \r\n\r\nI'm sorry for my delay but was no longer working on that project so I had to bring up a new Windows VM to check the changes from my side. Apparently it is all good, the build stills working, not sure why the Community CI Pipeline failed for the last commits though, could not understand the issue from Jenkins logs. Could you help me please?\r\n\r\nLike you requested, I applied the changes on the main CMakeLists (lite/CMakeLists) and it fixed the C++ tensorflow-lite library build, as far as I can tell. I still confused about the C API though, I was usually only build this library (lite/c/CMakeLists) and that's why I changed the c/CMakeLists file in the begging.  I added the SYSTEM_NAME check as peer your suggestion, but still thinking that it should be on both CMakeLists, since people can need to build one or the other. \r\n\r\nAny thoughts on that? ", "Updating my PR with master code fixed the CI build issue.\r\n@terryheo could you please review it when you have some time?\r\n\r\nCheers"]}, {"number": 49170, "title": "Include limits to prevent CI failure for tflite-micro.", "body": "This change is upstreaming the fix made with https://github.com/tensorflow/tflite-micro/pull/71/commits/5bd86af7fa8519c463eae03c72733aa479f00e5f\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49169, "title": "Tensorflow2 - \u201ctf.data.experimental.make_csv_dataset\u201d doesn't work with \u201ctf.keras.preprocessing.timeseries_dataset_from_array\u201d", "body": "Hello! I am trying to get TensorFlow to read +100 CSV files that ***don't*** fit in memory (+1GB size each). The files contain time-series data (EEG signals), with the labels in the first column. From the TensorFlow documentation it seems like I should be able to use the *tf.data* API to load my data off-disk.\r\n\r\nFor the sake of simplicity and reproducibility, let's consider the following \"*sample_data.csv*\" dataset:\r\n\r\n| Label    | Feature 1 | Feature 2 | \r\n| -------- | --------- | --------- |\r\n| Apple    | 1         | 2         |\r\n| Banana   | 3         | 4         |\r\n| Coconut  | 5         | 6         |\r\n| Durian   | 7         | 8         |\r\n| Eggplant | 9         | 10        |\r\n| Fruit    | 11        | 12        |\r\n\r\nI've tried using [tf.data.experimental.make_csv_dataset][1] to load the CSV files into *tf.data.Dataset* objects, and then [tf.keras.preprocessing.timeseries_dataset_from_array][2] to process the data into sliding windows with overlap. For the dataset above, I would do:\r\n\r\n    import tensorflow as tf\r\n\r\n    input_data = tf.data.experimental.make_csv_dataset(\r\n        'sample_data.csv',\r\n        batch_size=1,\r\n        column_names=['Label', 'Feature 1', 'Feature 2']\r\n        label_name='Label',\r\n        num_epochs=1,\r\n        shuffle=False\r\n    )\r\n\r\nWhich we can check works correctly by looking at the output from `list(input_data.as_numpy_iterator())`:\r\n```\r\n[319:]   [(OrderedDict([('Feature 1', array([1])), ('Feature 2', array([2]))]),\r\n          array([b'Apple'], dtype=object)),\r\n         (OrderedDict([('Feature 1', array([3])), ('Feature 2', array([4]))]),\r\n          array([b'Banana'], dtype=object)),\r\n         (OrderedDict([('Feature 1', array([5])), ('Feature 2', array([6]))]),\r\n          array([b'Coconut'], dtype=object)),\r\n         (OrderedDict([('Feature 1', array([7])), ('Feature 2', array([8]))]),\r\n          array([b'Durian'], dtype=object)),\r\n         (OrderedDict([('Feature 1', array([9])), ('Feature 2', array([10]))]),\r\n          array([b'Eggplant'], dtype=object)),\r\n         (OrderedDict([('Feature 1', array([11])), ('Feature 2', array([12]))]),\r\n          array([b'Fruit'], dtype=object))]\r\n```\r\n\r\nWe can then feed `input_data` to the next function:\r\n\r\n    my_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n        input_data,\r\n        targets=None,\r\n        sequence_length=3,\r\n        sequence_stride=2,\r\n        sampling_rate=1,  \r\n        batch_size=1,\r\n        shuffle=False\r\n    )\r\n\r\nWhich unfortunately **throws this error**:\r\n\r\n    [342]:  ~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py in __len__(self)\r\n            452       raise TypeError(\"dataset length is infinite.\")\r\n            453     if length.numpy() == UNKNOWN:\r\n        --> 454       raise TypeError(\"dataset length is unknown.\")\r\n            455     return length\r\n            456 \r\n    \r\n    TypeError: dataset length is unknown.\r\n\r\nI also tried using `my_dataset = input_data.window(3, shift=2)` (see the [tf.data.Dataset.window][3] documentation) and it didn't throw an error, but it seems to be returning an **empty dataset**? See \"*_VariantDataset shapes: (None,)*\" in the output:\r\n\r\n    list(input_data.window(3, shift=2))\r\n\r\n    [344]:\r\n    [(OrderedDict([('Feature 1',\r\n                    <_VariantDataset shapes: (None,), types: tf.int32>),\r\n                   ('Feature 2',\r\n                    <_VariantDataset shapes: (None,), types: tf.int32>)]),\r\n      <_VariantDataset shapes: (None,), types: tf.string>),\r\n     (OrderedDict([('Feature 1',\r\n                    <_VariantDataset shapes: (None,), types: tf.int32>),\r\n                   ('Feature 2',\r\n                    <_VariantDataset shapes: (None,), types: tf.int32>)]),\r\n      <_VariantDataset shapes: (None,), types: tf.string>),\r\n     (OrderedDict([('Feature 1',\r\n                    <_VariantDataset shapes: (None,), types: tf.int32>),\r\n                   ('Feature 2',\r\n                    <_VariantDataset shapes: (None,), types: tf.int32>)]),\r\n      <_VariantDataset shapes: (None,), types: tf.string>)]\r\n\r\nIf I load the \"*sample_data.csv*\" in memory using pandas and then feed the *timeseries_dataset_from_array* function a numpy array instead, it works correctly.\r\n\r\nAny ideas on how to solve this? **What's the best method to input overlapping windows from off-memory time-series data into TensorFlow**?\r\n\r\nThank you!\r\n\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset\r\n  [2]: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array\r\n  [3]: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window", "comments": ["@alexispomares ,\r\n\r\nIn order to reproduce the issue reported here, could you please provide the tensorflow version,complete code and the dataset or colab link you are using . Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49168, "title": " Could not compute output KerasTensor", "body": "I am trying to train a generator model, for which I am combining the generator and discriminator models.\r\nThe code:\r\n```\r\n  import tensorflow as tf\r\n  import os\r\n  import numpy as np\r\n  import PIL\r\n  from PIL import Image\r\n  from tqdm import tqdm\r\n  \r\n  class Localization(tf.keras.layers.Layer):\r\n      def __init__(self):\r\n          super(Localization, self).__init__()\r\n          self.bpool1 = tf.keras.layers.MaxPool2D()\r\n          self.bpool2 = tf.keras.layers.MaxPool2D()\r\n          self.bpool3 = tf.keras.layers.MaxPool2D()\r\n          self.bpool4 = tf.keras.layers.MaxPool2D()\r\n          \r\n          self.mpool1 = tf.keras.layers.MaxPool2D()\r\n          self.mpool2 = tf.keras.layers.MaxPool2D()\r\n          self.mpool3 = tf.keras.layers.MaxPool2D()\r\n          self.mpool4 = tf.keras.layers.MaxPool2D()\r\n  \r\n          self.bconv1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n          self.bconv2 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n          self.bconv3 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n          self.bconv4 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n          self.bconv5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n          self.bconv6 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n          self.bconv7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n          self.bconv8 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n  \r\n          self.mconv1 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n          self.mconv2 = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')\r\n          self.mconv3 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n          self.mconv4 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu')\r\n          self.mconv5 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n          self.mconv6 = tf.keras.layers.Conv2D(128, (3, 3), padding='same', activation='relu')\r\n          self.mconv7 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n          self.mconv8 = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')\r\n          \r\n          self.concatenate1 = tf.keras.layers.concatenate\r\n          self.concatenate2 = tf.keras.layers.concatenate\r\n  \r\n          self.flatten = tf.keras.layers.Flatten()\r\n          self.fc0 = tf.keras.layers.Dense(100, activation='relu')\r\n          self.fc1 = tf.keras.layers.Dense(20, activation='relu')\r\n          self.fc2 = tf.keras.layers.Dense(6, activation=None, bias_initializer=tf.keras.initializers.constant([1.0, 0.0, 0.0, 0.0, 1.0, 0.0]), kernel_initializer='zeros')\r\n  \r\n      def build(self, input_shape):\r\n          print(\"Building Localization Network with input shape:\", input_shape)\r\n  \r\n      def compute_output_shape(self, input_shape):\r\n          return [None, 6]\r\n  \r\n      def call(self, inputs):\r\n          mask, fg, bg = inputs\r\n          xm = self.concatenate1([fg, mask])\r\n          xm = self.mconv1(xm)\r\n          xm = self.mconv2(xm)\r\n          xm = self.mpool1(xm)\r\n  \r\n          xm = self.mconv3(xm)\r\n          xm = self.mconv4(xm)\r\n          xm = self.mpool2(xm)\r\n  \r\n          xm = self.mconv5(xm)\r\n          xm = self.mconv6(xm)\r\n          xm = self.mpool3(xm)\r\n  \r\n          xm = self.mconv7(xm)\r\n          xm = self.mconv8(xm)\r\n          xm = self.mpool4(xm)\r\n  \r\n  \r\n          xbg = self.bconv1(bg)\r\n          xbg = self.bconv2(xbg)\r\n          xbg = self.bpool1(xbg)\r\n          \r\n          xbg = self.bconv3(xbg)\r\n          xbg = self.bconv4(xbg)\r\n          xbg = self.bpool2(xbg)\r\n          \r\n          xbg = self.bconv5(xbg)\r\n          xbg = self.bconv6(xbg)\r\n          xbg = self.bpool3(xbg)\r\n          \r\n          xbg = self.bconv7(xbg)\r\n          xbg = self.bconv8(xbg)\r\n          xbg = self.bpool4(xbg)\r\n  \r\n          x = self.concatenate2((xbg, xm))\r\n          x = self.flatten(x)\r\n          x = self.fc0(x)\r\n          x = self.fc1(x)\r\n          theta = self.fc2(x)\r\n          theta = tf.keras.layers.Reshape((2, 3))(theta)\r\n          return theta\r\n  \r\n  class BilinearInterpolation(tf.keras.layers.Layer):\r\n      def __init__(self, height=320, width=320):\r\n          super(BilinearInterpolation, self).__init__()\r\n          self.height = height\r\n          self.width = width\r\n  \r\n      def compute_output_shape(self, input_shape):\r\n          return [None, self.height, self.width, 1]\r\n  \r\n      def get_config(self):\r\n          return {\r\n              'height': self.height,\r\n              'width': self.width,\r\n          }\r\n      \r\n      def build(self, input_shape):\r\n          print(\"Building Bilinear Interpolation Layer with input shape:\", input_shape)\r\n  \r\n      def advance_indexing(self, inputs, x, y):\r\n          '''\r\n          Numpy like advance indexing is not supported in tensorflow, hence, this function is a hack around the same method\r\n          '''        \r\n          shape = tf.shape(inputs)\r\n          batch_size, _, _ = shape[0], shape[1], shape[2]\r\n          \r\n          batch_idx = tf.range(0, batch_size)\r\n          batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\r\n          b = tf.tile(batch_idx, (1, self.height, self.width))\r\n          indices = tf.stack([b, y, x], 3)\r\n          return tf.gather_nd(inputs, indices)\r\n  \r\n      def call(self, inputs):\r\n          images, theta = inputs\r\n          homogenous_coordinates = self.grid_generator(batch=tf.shape(images)[0])\r\n          return self.interpolate(images, homogenous_coordinates, theta)\r\n  \r\n      def grid_generator(self, batch):\r\n          x = tf.linspace(-1, 1, self.width)\r\n          y = tf.linspace(-1, 1, self.height)\r\n              \r\n          xx, yy = tf.meshgrid(x, y)\r\n          xx = tf.reshape(xx, (-1,))\r\n          yy = tf.reshape(yy, (-1,))\r\n          homogenous_coordinates = tf.stack([xx, yy, tf.ones_like(xx)])\r\n          homogenous_coordinates = tf.expand_dims(homogenous_coordinates, axis=0)\r\n          homogenous_coordinates = tf.tile(homogenous_coordinates, [batch, 1, 1])\r\n          homogenous_coordinates = tf.cast(homogenous_coordinates, dtype=tf.float32)\r\n          return homogenous_coordinates\r\n      \r\n      def interpolate(self, images, homogenous_coordinates, theta):\r\n  \r\n          with tf.name_scope(\"Transformation\"):\r\n              transformed = tf.matmul(theta, homogenous_coordinates)\r\n              transformed = tf.transpose(transformed, perm=[0, 2, 1])\r\n              transformed = tf.reshape(transformed, [-1, self.height, self.width, 2])\r\n                  \r\n              x_transformed = transformed[:, :, :, 0]\r\n              y_transformed = transformed[:, :, :, 1]\r\n                  \r\n              x = ((x_transformed + 1.) * tf.cast(self.width, dtype=tf.float32)) * 0.5\r\n              y = ((y_transformed + 1.) * tf.cast(self.height, dtype=tf.float32)) * 0.5\r\n  \r\n          with tf.name_scope(\"VariableCasting\"):\r\n              x0 = tf.cast(tf.math.floor(x), dtype=tf.int32)\r\n              x1 = x0 + 1\r\n              y0 = tf.cast(tf.math.floor(y), dtype=tf.int32)\r\n              y1 = y0 + 1\r\n  \r\n              x0 = tf.clip_by_value(x0, 0, self.width-1)\r\n              x1 = tf.clip_by_value(x1, 0, self.width-1)\r\n              y0 = tf.clip_by_value(y0, 0, self.height-1)\r\n              y1 = tf.clip_by_value(y1, 0, self.height-1)\r\n              x = tf.clip_by_value(x, 0, tf.cast(self.width, dtype=tf.float32)-1.0)\r\n              y = tf.clip_by_value(y, 0, tf.cast(self.height, dtype=tf.float32)-1)\r\n  \r\n          with tf.name_scope(\"AdvanceIndexing\"):\r\n              Ia = self.advance_indexing(images, x0, y0)\r\n              Ib = self.advance_indexing(images, x0, y1)\r\n              Ic = self.advance_indexing(images, x1, y0)\r\n              Id = self.advance_indexing(images, x1, y1)\r\n  \r\n          with tf.name_scope(\"Interpolation\"):\r\n              x0 = tf.cast(x0, dtype=tf.float32)\r\n              x1 = tf.cast(x1, dtype=tf.float32)\r\n              y0 = tf.cast(y0, dtype=tf.float32)\r\n              y1 = tf.cast(y1, dtype=tf.float32)\r\n                              \r\n              wa = (x1-x) * (y1-y)\r\n              wb = (x1-x) * (y-y0)\r\n              wc = (x-x0) * (y1-y)\r\n              wd = (x-x0) * (y-y0)\r\n  \r\n              wa = tf.expand_dims(wa, axis=3)\r\n              wb = tf.expand_dims(wb, axis=3)\r\n              wc = tf.expand_dims(wc, axis=3)\r\n              wd = tf.expand_dims(wd, axis=3)\r\n                          \r\n          return tf.math.add_n([wa*Ia + wb*Ib + wc*Ic + wd*Id])\r\n  \r\n  class Composition(tf.keras.layers.Layer):\r\n      def __init__(self):\r\n          super(Composition, self).__init__()\r\n      def build(self, input_shape):\r\n          print(\"Building Composition Network with input shape:\", input_shape)\r\n  \r\n      def compute_output_shape(self, input_shape):\r\n          return input_shape\r\n  \r\n      def call(self, inputs):\r\n          mask, fg, bg = inputs\r\n          multiples = tf.constant([1, 1, 1, 3], tf.int32)\r\n          mask_mod = tf.tile(mask, multiples)\r\n          bg_mod = tf.keras.layers.Multiply()([bg, 1-mask_mod])\r\n          fg_mod = tf.keras.layers.Multiply()([fg, mask_mod])\r\n          composite_image = tf.keras.layers.Add()([bg_mod, fg_mod])\r\n          \r\n          return composite_image\r\n  \r\n  def gen_model(input_shape):\r\n      mask = tf.keras.layers.Input(shape=(input_shape[0], input_shape[1], 1))\r\n      fg = tf.keras.layers.Input(shape=(input_shape[0], input_shape[1], input_shape[2]))\r\n      bg = tf.keras.layers.Input(shape=(input_shape[0], input_shape[1], input_shape[2]))\r\n      theta = Localization()([mask, fg, bg])\r\n      xm = BilinearInterpolation(height=input_shape[0], width=input_shape[1])([mask, theta])\r\n      xfg = BilinearInterpolation(height=input_shape[0], width=input_shape[1])([fg, theta])\r\n      composite = Composition()([xm, xfg, bg])\r\n  \r\n      return tf.keras.models.Model(inputs=[mask, fg, bg], outputs=composite, name='generator')\r\n  \r\n  def disc_model(input_shape):\r\n    inp = tf.keras.layers.Input(shape=input_shape) \r\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),padding='same')(inp)\r\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n  \r\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n    \r\n    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n    \r\n    x = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n    \r\n    x = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),padding='same')(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    x = tf.keras.layers.MaxPool2D()(x)\r\n    \r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(25)(x)\r\n    x = tf.keras.layers.LeakyReLU()(x)\r\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n    return tf.keras.models.Model(inputs=inp, outputs=out, name='discriminator')\r\n  \r\n  \r\n  def stgan2(disc, gen):\r\n    disc.trainable = False\r\n    model = tf.keras.models.Sequential()\r\n    model.add(gen)\r\n    model.add(disc)\r\n  \r\n    model.compile(loss='binary_crossentropy', optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics = ['accuracy'])\r\n    return model\r\n  \r\n  gen = gen_model(input_shape=(420, 640, 3))\r\n  \r\n  dis = disc_model(input_shape=(420, 640, 3))\r\n  \r\n  gan = stgan2(dis, gen)\r\n```\r\n\r\nThe error:\r\nAssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 420, 640, 3), dtype=tf.float32, name=None), name='composition/add/add:0', description=\"created by layer 'composition'\")\r\n\r\nColab [link](https://colab.research.google.com/drive/1oMbH2Ka1hNcVZgyzf9Uahx9Snufv_aTv?usp=sharing)\r\n\r\n", "comments": ["`model.add` requires a layer. See the doc https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=nightly#add", "@bhack \r\nThanks, I replaced it tf.keras.Model module, and it's working.", "Can you close this?"]}]