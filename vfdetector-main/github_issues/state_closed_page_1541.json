[{"number": 6694, "title": "Why cannot I import the tpprof module?", "body": "From [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tfprof/python/tools/tfprof/tfprof_logger.py#L30) I get to know that the new added profile tool is imported, but it seems strange: 1) it cannot be imported from the interactive interface; 2) and when mnist is run it raise the same error saying `ImportError: cannot import name 'tfprof_log_pb2'`. What happened for the newly added feature?", "comments": ["It is because I run the tf in anaconda which installed the oldest tensorflow and then cannot find the module. "]}, {"number": 6693, "title": "Misnamed libcuda.dylib. (CUDA 8.0, macOS 10.12)", "body": "### Misnamed `libcuda.dylib` in prebuilt binary. (CUDA 8.0, macOS 10.12).\r\nTensorflow fails to load `libcuda.1.dylib`, a file which does not exist for CUDA 8.0 on macOS. The filename is `libcuda.dylib`.\r\n\r\nResolved by `sudo ln -s /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`.\r\n\r\nPreferable solution is a change to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L82.\r\n\r\nI would change `FormatLibraryFileName(\"cuda\", \"1\")` to `FormatLibraryFileName(\"cuda\")`, but I'm not sure on which OS and CUDA versions this is the correct naming scheme. Possibly a check is needed, `if ( stoi(GetCudaVersion()) >= 8 ) {...}` and maybe `if (__APPLE__) {...}`. \r\n\r\n### Related issue\r\nhttps://github.com/tensorflow/tensorflow/issues/2278\r\nSolution provided [here](https://github.com/tensorflow/tensorflow/issues/2278#issuecomment-244828047) by @eagleflo, @martinianodl.\r\n### Environment info\r\nmacOS 10.12.2\r\nCUDA Toolkit 8.0\r\n```\r\nls -l /usr/local/cuda/lib/libcud*\r\nlrwxr-xr-x  1 root  wheel     33 Jan  6 14:34 /usr/local/cuda/lib/libcuda.1.dylib -> /usr/local/cuda/lib/libcuda.dylib\r\n-rwxr-xr-x@ 1 root  wheel  13504 Nov  3 20:39 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x@ 1 root  wheel     45 Nov  3 20:40 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x@ 1 root  wheel     50 Nov  3 20:40 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x@ 1 root  wheel     46 Nov  3 20:40 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x@ 1 root  wheel     49 Nov  3 20:40 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\nlrwxr-xr-x  1 root  wheel     47 Jan  6 13:19 /usr/local/cuda/lib/libcudnn.5.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5.dylib\r\nlrwxr-xr-x  1 root  wheel     45 Jan  6 13:19 /usr/local/cuda/lib/libcudnn.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn.dylib\r\nlrwxr-xr-x  1 root  wheel     48 Jan  6 13:19 /usr/local/cuda/lib/libcudnn_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudnn_static.a\r\n\r\n```\r\nInstalled as a virtualenv with the prebuilt binary.\r\n\r\nTF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl\r\n```\r\n>>> tf.__version__\r\n'0.12.1'\r\n```\r\n\r\n", "comments": ["I guess it's a known issue. A bit late, I found this already covered in the documentation: https://www.tensorflow.org/get_started/os_setup#mac_os_x_segmentation_fault_when_import_tensorflow\r\n\r\n> This is due to the fact that by default, cuda creates libcuda.dylib, but tensorflow tries to load libcuda.1.dylib. This can be resolved by create a symbolic link:\r\n> `ln -sf /usr/local/cuda/lib/libcuda.dylib /usr/local/cuda/lib/libcuda.1.dylib`\r\n\r\nI would submit a PR for a permanent fix, but maybe it breaks something on other systems. For now the fix is in the documentation.", "@gunan, can we accept this patch, or is there some reason it can't work reliably in every case?", "@caisq what do you think?", "The fix is now merged. 1.0.0-rc1 will have the fix. Closing the issue."]}, {"number": 6692, "title": "Training only a subset of synaptic weights", "body": "It is often necessary to train only a subest of synaptic weights. One should be able to specify an arbitrary list of individual weights that will be updated/kept fixed by the gradient decent optimization algorithm and by other training algorithms. \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThere are no solutions for training only a subset of individual synaptic weights. One is only able to define an entire tensor as not trainable, but it is not possible to define individual weights within a tensor as trainable / non-trainable.\r\n\r\n\r\n", "comments": ["You can pass a `var_list` to the [`minimize`](https://www.tensorflow.org/api_docs/python/train/optimizers#Optimizer.minimize) call of the optimizer.", "@WI-KIWI  does @tillahoffmann's suggestion satisfy your need?", "If I understand @WI-KIWI may be asking if you can apply a binary masking matrix or vector to some parameters at each step. I am also interested in functionality that would allow me to pass either a list of indices to zero out weights or a list of indices to keep active at each step.", "Hi all,\r\nFirst of all, thanks for your quick responses.\r\n\r\n@aselle, \r\nThe suggestion of @thilahoffman works only with entire sensors, not with individual locations within a sensor. The latter is what I need.\r\n\r\n@szb0 yes, an application of a binary mask would be the way to go. This makes it two of us who need this feature.", "Sorry if I am not understanding, but can you just multiply by a mask as in the [dropout](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1886) op?", "I wouldn\u2019t know what to multiply by a mask, so that the gradient descent would only train a needed subset of synapses (not tensors, but only certain elements within tensors). \r\n\r\nOf course, it is easy to define a control tensor of 1s and 0s indicating fixed and trainable weights. However, I don\u2019t know how to make the gradient descent function use the control tensor.", "Basically a way to easily apply binary masks to the gradients for each parameter matrix/vector. In particular a nice to have would be a way that allows for a sparse representation of the masks (non-zero or zero indices). I think this is already possible to do if you make a modification between the optimizers compute_gradients and apply_gradients call. Maybe just by defining placeholders for the indices at that will be zeroed out at each step and applying these masks to the gradients before applying them. ", "You could do something like [this](https://gist.github.com/tillahoffmann/c04dd6ca9259ef68f743e44c74e3d095).", "@tillahoffmann thank you so much. I followed your suggestion and it worked! I appreciated your help!\r\n\r\nAlso many thanks to all here, especially to @szb0\r\n\r\nJust WOW!  \r\n\r\nCheers", "@WI-KIWI Based on my cursory glance of this issue, it seems what @tillahoffmann suggested was able to solve your problem. In that case, a new feature might not be necessary. Therefore I'm going to close this issue. If you disagree, and still feel that a feature should be added, let me know and I'll re-open this issue."]}, {"number": 6691, "title": "Feature Request: provide env vars to control resource usage (allow_growth, threads etc)", "body": "We provide GPGPU access on a cluster and to let the scheduler (HTCondor) assign the resources. Tt would be great to be able to set\r\n- the number of theads to use\r\n- the allow_growth options\r\n- and the per_process_gpu_memory_fraction option\r\nvia environment variables e.g. TF_NUM_THREADS=4, TF_ALLOW_GROW=1 etc.\r\n\r\n", "comments": ["@mrry do you have any opinions on this.", "I think you can write a function that reads your own environment variables to populate and return a tf.ConfigProto() that sets these options -- I don't think it belongs in the TF core.  The explicit proto makes reproduction much easier.", "I agree with @vrv. (I should get that printed on a t-shirt.)", "So essentially, someone who admins a cluster or uses this software has to be sure to force that code to run rather than letting users run arbitrary code that doesn't call their code. Presumably such a hook could be done somewhere else, even as extreme as LD_PRELOAD.\r\n", "@vrv sure I can, but the point is that the students and others do not have to do this. And to prevent the misuse of the cluster.\r\nPlease reopen this issue!!", "BTW, you can modify global setup by changing session.py\r\n\r\nIE, `inspect.getsourcefile(tf.Session)` to find where it's defined, and then modify `__init__` to something like below\r\n\r\n```\r\n   if config is not None:\r\n      if not isinstance(config, config_pb2.ConfigProto):\r\n        raise TypeError('config must be a tf.ConfigProto, but got %s'\r\n                        % type(config))\r\n      self._config = config\r\n      self._add_shapes = config.graph_options.infer_shapes\r\n    else:\r\n      self._config = config_pb2.ConfigProto(intra_op_parallelism_threads=24)\r\n      self._add_shapes = False\r\n```\r\n\r\nNow all sessions default to using 24 threads for the Eigen threadpool"]}, {"number": 6690, "title": "Calculation Delta, Memoization, Update Propagation and Laziness", "body": "If you just update a few element in the data structure then having to do calculations which are not affected by the change is a waste.  Also if you read only or consume un affected values then the update and calculations themselves are a waste. Also eagerly calculating this would be a waste since I might only read the result after few updates. In addition you can avoid calculating invariant items. Perhaps XLA can get Lightweight Modular Staging (LMS) functionality to do this type of analysis and optimisation.\r\n\r\nAlso see: https://github.com/tensorflow/tensorflow/issues/6690\r\n\r\n> For example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window.\r\n\r\nAlso other potential optimisations (will require some research):\r\n\r\n - If I have a matrix inverse and I update the original matrix, what would be the optimal calculation to arrive at an updated inverse matrix\r\n - Similarly for matrix multiplication and other operations.", "comments": ["Closing even though it's an interesting idea, see rationale in #6476."]}, {"number": 6689, "title": "Adding support for Big Endian in decode_raw_op_test", "body": "Adding support for Big Endian in decode_raw_op_test", "comments": ["Can one of the admins verify this patch?", "Decode_raw_op test fails on big endian with an error \"Unimplemented support for little_endian=true\".\r\nAs per [comments](https://github.com/tensorflow/tensorflow/pull/5450/#issuecomment-270173816), we have made changes in decode_raw and created a new PR for ongoing discussion on this. \r\n\r\n", "I took a quick look at this PR and my only concern is that there doesn't seem to be any handling of the little_endian_ attribute in the OP code. What if little_endian_ is false (indicating that the data is little endian) but the system is different (say big endian). I think that case isn't perhaps handled. If the type of the data and system match then I think this code should do the job. \r\n\r\nAdding Andrew as well.", "@rohan100jain  @drpngx  Other solution we could think is to add check for little endian platform. \r\nBelow is the patch: \r\n\r\n```\r\n--- a/tensorflow/core/kernels/decode_raw_op.cc\r\n+++ b/tensorflow/core/kernels/decode_raw_op.cc\r\n@@ -69,6 +69,15 @@ class DecodeRawOp : public OpKernel {\r\n         context, context->allocate_output(\"output\", out_shape, &output_tensor));\r\n     auto out = output_tensor->flat_inner_dims<T>();\r\n     DCHECK_EQ(flat_in.size(), out.dimensions()[0]);\r\n+\r\n+    if (::tensorflow::port::kLittleEndian) {\r\n      OP_REQUIRES(\r\n        context,\r\n        little_endian_ == ::tensorflow::port::kLittleEndian || sizeof(T) == 1,\r\n        errors::Unimplemented(\"Unimplemented support for little_endian=\",\r\n                              little_endian_ ? \"true\" : \"false\"));\r\n+    }\r\n   // Endianness matches, so just copy each string byte-for-byte.\r\n ```", "Patch look strange, there's a sizeof = 1 that's not used because of the if conditional?", "@drpngx \r\nDecode_raw_op test was failing on big endian due to following assert:\r\n\r\n```\r\nOP_REQUIRES(\r\n        context,\r\n        little_endian_ == ::tensorflow::port::kLittleEndian || sizeof(T) == 1,\r\n        errors::Unimplemented(\"Unimplemented support for little_endian=\",\r\n                              little_endian_ ? \"true\" : \"false\"));\r\n```\r\n\r\nWith our PR, we tried to remove the above assert assuming that type of data and the system match. \r\nHowever, as @rohan100jain commented,  we could think of above patch. Through if condition, assert will be skipped only on big endian and would not hamper original functionality. \r\n\r\nCan you think of better way to handle the above scenario?", "The test basically says that it's not implemented. Maybe it's stale. Does it pass all tests?", "@drpngx All tests are passing at our end with above mentioned patch. \r\nShould we modify PR so that you could trigger a build?\r\n", "Jenkins, test this please.", "Not sure if @aselle has more context. The test was written to mean that anything other than little endian doesn't work, but I guess if it works for you, then I am for removing the check altogether, if it also has a reasonable chance of working with middle endian.", "@aselle @drpngx  Please let me know your comments. ", "Sorry for the delay. Thinking about this, it's the right thing to delete the test since you have tested it and it's obsolete. The case we don't cover is middle endian but I don't think we need to worry about this now."]}, {"number": 6688, "title": "Outer product based Conv filters consume disproportionately high memory", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone\r\n### Environment info\r\nOperating System:\r\n\r\nUbuntu 14.04\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nNone, I'm using the CPU version of TF. \r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.10.0rc0\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThe minimum reproducible example is in this [github gist](https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232). The issue is of a disproportionately large memory usage. \r\n\r\nI modified the cifar10 example which ships with tensorflow to use outer-product of 3 vectors as the weights of the convolutional layers. This change can be seen in this [part of the code](https://gist.github.com/sahiliitm/bed9511d15ba0c45adf3c33210d59232#file-cifar10-py-L91). \r\n\r\nFor simplicity, i have removed all parameter training operations and even loss computations. The current model only computes logits (forward pass) again and again. \r\nThe unmodified code (can be setting the `use_outerp` flag to `False`) uses approximately 11.4 GB RAM\r\nwhereas the modified code (with outer product of vectors used as the convolutional weight tensor) uses a disproportionately high 17 GB RAM. \r\n\r\nAny idea why this is the case? \r\nMy intuition as to why this might happen is that maybe the outer product operations are being executed _every single time that the conv filter is needed_  instead of being executed exactly once in every forward pass. Is this really the case? Is there a way to fix this? \r\n\r\nSteps to reproduce:\r\n1. To run the default version of the code (low memory footprint):\r\npython train.py --use_outerp='False'\r\n\r\n2. To run the modified version of the code (high memory footprint):\r\npython train.py --use_outerp='True'\r\n\r\n### What other attempted solutions have you tried?\r\nI'm not sure what the problem is, so haven't tried anything.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["This question is best asked on StackOverflow first. You might make sure you are using tcmalloc, since the default allocator is prone to fragmentation.", "Using tcmalloc didn't help. I don't think this is because of memory fragmentation. Anyway, I've asked a [question](http://stackoverflow.com/questions/41517145/outer-product-based-conv-filters-consume-disproportionately-high-memory) on SO. Thanks!"]}, {"number": 6687, "title": "Cannot run a distributed training example with tensorflow v0.12.1", "body": "Hi,\r\n\r\nI was trying to run a distributed tensorflow [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py\r\n) in the official repository with v0.12.1 (current latest release).\r\nI can run asynchronous version without problems, but when I turned on `sync_replicas` tag, some errors occurred.\r\n(Please check the following logs in details)\r\n\r\nThis example code can be ran successfully with v0.12.0, so I guess there might be some modification from 0.12.0 to 0.12.1?\r\nCould someone check if that's the case? Thanks.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI haven't found others reported this issue as tensorflow v0.12.1 just released,\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 7.5, cuDNN 5.1\r\n\r\n```\r\n> ls -l /usr/local/cuda/lib/libcud*\r\n-rw-r--r-- 1 root root 189170 Oct 25 22:51 /usr/local/cuda/lib/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\r\nlrwxrwxrwx 1 root root     19 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\r\n-rwxr-xr-x 1 root root 311596 Oct 25 22:51 /usr/local/cuda/lib/libcudart.so.7.5.18\r\n-rw-r--r-- 1 root root 558020 Oct 25 22:51 /usr/local/cuda/lib/libcudart_static.a\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n4d924e796368163eff11a8151e8505715345f58d (Release 0.12.1)\r\n2. The output of `bazel version`\r\n```\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI simply used this example [mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py) in tensorflow repository.\r\n\r\n### Logs or other output that would be helpful\r\n\r\nFirst I launched the parameter server\r\n```\r\nexport CUDA_VISIBLE_DEVICES=0; python mnist_replica.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"ps\" --task_index=0 --num_gpus=1 --sync_replicas=True\r\n```\r\nand then launched the worker with `sync_replicas=True` tag\r\n```\r\nexport CUDA_VISIBLE_DEVICES=1; python mnist_replica.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"worker\" --task_index=0 --num_gpus=2 --train_steps=100 --sync_replicas=True\r\n```\r\n\r\nbut I got some errors here\r\n\r\n```\r\n...\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:2b:00.0)\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> localhost:50000}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:50001, 1 -> localhost:50002}\r\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:50002\r\nTraceback (most recent call last):\r\n  File \"mnist_replica.py\", line 281, in <module>\r\n    tf.app.run()\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"mnist_replica.py\", line 186, in main\r\n    train_step = opt.minimize(cross_entropy, global_step=global_step)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 279, in minimize\r\n    name=name)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 751, in apply_gradients\r\n    array_ops.reshape(self._replica_id, (1,)),\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2448, in reshape\r\n    name=name)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 503, in apply_op\r\n    as_ref=input_arg.is_ref).dtype.name\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```", "comments": ["Hm, can't reproduce...any idea what could be different about your setup? Here's what I tried\r\n\r\n```\r\nsource activate tf12.2\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl\r\npip install -I --upgrade setuptools\r\npip install --upgrade $TF_BINARY_URL\r\n\r\ngit fetch\r\ngit tag -l\r\ngit checkout tags/0.12.1 -b 0.12.1\r\nfind . -name \"mnist_replica*\"\r\n\r\ncd ~/tensorflow.git/tensorflow/tensorflow/tools/dist_test/python\r\n# python mnist_replica.py\r\nsource activate tf12.2\r\nexport CUDA_VISIBLE_DEVICES=1; python mnist_replica.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"ps\" --task_index=0 --num_gpus=1 --sync_replicas=True\r\n\r\ncd ~/tensorflow.git/tensorflow/tensorflow/tools/dist_test/python\r\nsource activate tf12.2\r\nexport CUDA_VISIBLE_DEVICES=2; python mnist_replica.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"worker\" --task_index=0 --num_gpus=2 --train_steps=100 --sync_replicas=True\r\n\r\n```\r\n\r\nat the end it finishes with\r\n```\r\n\r\n1483755958.593979: Worker 0: training step 94 done (global step: 92)\r\n1483755958.599673: Worker 0: training step 95 done (global step: 93)\r\n1483755958.604453: Worker 0: training step 96 done (global step: 94)\r\n1483755958.609369: Worker 0: training step 97 done (global step: 95)\r\n1483755958.614092: Worker 0: training step 98 done (global step: 96)\r\n1483755958.619020: Worker 0: training step 99 done (global step: 97)\r\n1483755958.624015: Worker 0: training step 100 done (global step: 98)\r\n1483755958.629047: Worker 0: training step 101 done (global step: 99)\r\n1483755958.633775: Worker 0: training step 102 done (global step: 100)\r\nTraining ends @ 1483755958.633824\r\nTraining elapsed time: 0.946386 s\r\nAfter 100 training step(s), validation cross entropy = 1463.93\r\n```", "Thanks for your quick reply.\r\nEarlier I used the 0.12.1 version with gpu support which was built by myself (since my cuda version is 7.5).\r\n\r\nTo make the situation simple, I tried to use the pre-built CPU version by the following instructions\r\n```\r\nsource vir-tf/bin/activate\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp27-none-linux_x86_64.whl\r\npip install --ignore-installed --upgrade $TF_BINARY_URL\r\n```\r\nand then ran\r\n```\r\nexport CUDA_VISIBLE_DEVICES=0; python mnist_replica.py --ps_hosts=\"localhost:50000\" --worker_hosts=\"localhost:50001\" --job_name=\"worker\" --task_index=0 --num_gpus=1 --train_steps=100 --sync_replicas=True\r\n```\r\nUnfortunately, the same error occurred.\r\n```\r\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:50001\r\nTraceback (most recent call last):\r\n  File \"mnist_replica.py\", line 281, in <module>\r\n    tf.app.run()\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"mnist_replica.py\", line 186, in main\r\n    train_step = opt.minimize(cross_entropy, global_step=global_step)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 279, in minimize\r\n    name=name)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 751, in apply_gradients\r\n    array_ops.reshape(self._replica_id, (1,)),\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2448, in reshape\r\n    name=name)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 503, in apply_op\r\n    as_ref=input_arg.is_ref).dtype.name\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```\r\nHowever, when I did the same things on another machine, it turns out to behave normal...\r\n\r\nNow I have no ideas why it failed on a specific machine...\r\nI also found that someone faced similar errors https://github.com/tensorflow/tensorflow/issues/3208 but under different situations.\r\nWhat do you think of the possible reasons that cause the errors?\r\nThanks.", "@sherrym there seems to be some issue that turns up in a variety of circumstances e.g. here and #3208 which results in this unhelpful error message. Any thoughts on making it easier to debug?", "My reading of the error message is that `self._replica_id` was `None`, so the code executed `array_ops.reshape(None, ...)` which gives this error. @jmchen-g -- do you have any idea why `replica_id` would be none in `training/sync_replicas_optimizer.py:751` ?", "Generally I think this would be more human-readable if `None` checking occurred few levels up, at `op_def_lib.apply_op`, ie [here](https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/python/framework/op_def_library.py#L289). Since `None` is a Python-only thing, this could be a check for None at the top level of `apply_op` which prints which value was none and which op.\r\n", "I'm facing the same issue when trying to use the same example from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py).\r\n\r\nAsynchronous works but setting sync_replicas to True results in the same error like this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"mnist_replica.py\", line 293, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"mnist_replica.py\", line 195, in main\r\n    train_step = opt.minimize(cross_entropy, global_step=global_step)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 279, in minimize\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py\", line 751, in apply_gradients\r\n    array_ops.reshape(self._replica_id, (1,)),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2448, in reshape\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 503, in apply_op\r\n    as_ref=input_arg.is_ref).dtype.name\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 360, in make_tensor_proto\r\n    raise ValueError(\"None values not supported.\")\r\nValueError: None values not supported.\r\n```\r\n\r\nI am running in CPU only mode on Ubuntu 16.04 with tensorflow 0.12.1 and Python 2.7.12. I'm facing the same issue with Python3 as well as on Ubuntu 14.04.", "Looks like in v 1.0 this issue was fixed?\r\nClosing this issue.", "Having this issue in 1.3.0..."]}, {"number": 6686, "title": "Adding support for Linux s390x in sparse_split_op", "body": "Changing the data type of split_dim to resolve sparse_split_op_test failure on Big Endian System.", "comments": ["Can one of the admins verify this patch?", "The split_dim in sparse_split_op.cc is int32 type which causes the column wise tests(where split_dim should be 1) to fail on Big Endian system as the \"context->input(0).scalar<int>()();\" returns 0 all the time.\r\n\r\nCould you review the change please?", "Jenkins, test this please", "Jenkins, test this please.", "Could we add a test for this?", "The int64 data type change in this Op file is related to the `//tensorflow/python/kernel_tests:sparse_split_op_test`.  With above change test case is passing on big endian as well as little endian machine. @drpngx , Are you referring to any other test here?", "I guess I'm looking for something that would test that specifically, on little endian, if possible.", "Hmm, thinking about it, it's hard to create a test for this."]}, {"number": 6685, "title": "Eliminate ':' in summary name of contrib.learn.estimators.", "body": "When I run the contrib.learn.DNNRegressor or contrib.learn.DNNClassifier model, I've got the log messages like below.\r\n\r\n```\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\r\nINFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\r\n```\r\n\r\nTensorboard show me that the characters ':' in these summary name were replaced by '_'.\r\n\r\nI think this change doesn't cause any big incompatibility issues.", "comments": ["Can one of the admins verify this patch?", "I think this looks fine to me. Adding Martin for a second look and mostly to make sure that this doesn't cause issues with existing running jobs etc.", "I think this is safe to do -- Tensorboard labels may change, but I don't think those are programmatically accessed anywhere.", "Jenkins, test this please", "Jenkins, test this please."]}, {"number": 6684, "title": "TensorFlow API compatibility test.", "body": "", "comments": []}, {"number": 6683, "title": "Tensorflow Model with CTC loss having save and restore problem", "body": "I am using tensorflow 0.12 without GPU support. I was testing it with various models. My template structure is\r\n```\r\n#Load some data from file\r\ngraph=tf.Graph()\r\nwith graph.as_default():\r\n     #Build Network\r\n     #saver=tf.train.Saver()\r\nwith tf.Session(graph=graph) as session:\r\n     if(sys.argv[1]==\"load\"):\r\n          saver.restore(session,\"weight_last\")\r\n     else:\r\n           initop=tf.global_variables_initializer()\r\n           session.run(initop)\r\n    #Continue Training\r\n```\r\nNow, I am facing a strange issue. When I am creating a MLP or RNN with this structure with a categorical cross entropy loss model this saving and restoring is working perfectly, i.e. after restore the loss is showing exact value that was showed during last save. But unfortunately when the network is loaded with CTC loss then after restoring the model is starting almost a new training. I am not sure what is going wrong? Any help shall be highly appreciated.", "comments": ["Do you have a simple repro you could share?", "@michaelisard \r\nI am extremely sorry for my delayed response. My input is taken from a H5 file which contains features extracted from online handwriting data sample. at every time step I have 16 features. Every time I read from this file the order of data is shuffled.\r\nHere is the part where I am creating the graph.\r\n```\r\ngraph=tf.Graph()\r\nwith graph.as_default():\r\n    print(\"Graph Creation\")\r\n    x=tf.placeholder(tf.float32,[None,ms,nb_features],name=\"x\")\r\n\r\n    y=tf.sparse_placeholder(tf.int32,name=\"y\")\r\n    seq_len=tf.placeholder(tf.int32,[None],name=\"seq_len\")\r\n    \r\n\r\n    f_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\r\n    f_stack=tf.nn.rnn_cell.MultiRNNCell([f_cell]*nb_layers)\r\n    \r\n    b_cell=tf.nn.rnn_cell.LSTMCell(nb_hidden,state_is_tuple=True)\r\n    b_stack=tf.nn.rnn_cell.MultiRNNCell([b_cell]*nb_layers)\r\n    \r\n    outputs,_=tf.nn.bidirectional_dynamic_rnn(f_stack,b_stack,x,sequence_length=seq_len,dtype=tf.float32)\r\n\r\n    merge=tf.concat(2, outputs,name=\"merge\")\r\n\r\n    shape = tf.shape(x)\r\n    batch_s,maxtimesteps=shape[0],shape[1]\r\n\r\n    output_reshape = tf.reshape(merge, [-1, nb_hidden*2])#batch*timesteps,nb_hidden\r\n   \r\n    W = tf.Variable(tf.truncated_normal([nb_hidden*2,nb_classes],stddev=0.1),name=\"W1\")\r\n\r\n    b = tf.Variable(tf.constant(0., shape=[nb_classes]),name=\"b1\")\r\n\r\n    logits = tf.add(tf.matmul(output_reshape, W) , b,name=\"logits\") #818622,52\r\n      \r\n    logits_reshape = tf.transpose(tf.reshape(logits, [batch_s, -1, nb_classes]),[1,0,2],name=\"logits_reshape\")#534,1533,52\r\n\r\n    loss =tf.nn.ctc_loss(logits_reshape, y, seq_len,time_major=True)\r\n    cost = tf.reduce_mean(loss,name=\"cost\")\r\n\r\n    optimizer = tf.train.RMSPropOptimizer(lr).minimize(cost)\r\n\r\n    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits_reshape, seq_len)\r\n\r\n    actual_ed=tf.edit_distance(tf.cast(decoded[0], tf.int32),y,normalize=False)\r\n    ler = tf.reduce_sum(actual_ed,name=\"ler\")\r\n    saver=tf.train.Saver()\r\n    bestsaver=tf.train.Saver()\r\n    print(\"Network Ready\")\r\n```\r\nJust after this, I am running the training and saving it\r\n```\r\nwith tf.Session(graph=graph,config=tf.ConfigProto(log_device_placement=False)) as session:    \r\n    #saver = tf.train.Saver()\r\n    if(sys.argv[1]==\"load\"):\r\n        saver.restore(session, \"Weights/model_last\")\r\n        print(\"Previous weights loaded\")\r\n    else:\r\n        init_op = tf.global_variables_initializer()\r\n        session.run(init_op)\r\n        print(\"New Weights Initialized\")\r\n    \r\n    best=0\r\n    acctest=0\r\n    bestloss=10000\r\n    #testfeed = {x:test_inpx[0],y:test_inp_sparse_y[0],seq_len:test_inpseqlen[0]}\r\n    testcases=[0,5,17,39,60]\r\n    true=[]\r\n    for tr in range(len(testcases)):\r\n        true1=label_from_sparse(test_inp_sparse_y[0],testcases[tr])\r\n        true.append(true1)    \r\n    print(\"Actual \",true)\r\n    for e in range(nb_epochs):\r\n        f=open(logfilename,\"a\")\r\n        totalloss=0\r\n        totalacc=0\r\n        starttime=time.time()\r\n\r\n        for b in range(trainbatch):\r\n            p=b+1\r\n            print(\"Reading Batch \",p,\"/\",trainbatch,end=\"\\r\")\r\n            feed = {x:inpx[b],y:inp_sparse_y[b],seq_len:inpseqlen[b]}\r\n            batchloss,batchacc, _ = session.run([cost,ler,optimizer], feed)\r\n            \r\n            totalloss=totalloss+batchloss\r\n            totalacc=totalacc+batchacc\r\n        avgloss=totalloss/trainbatch\r\n        avgacc=1-(totalacc/nctr)\r\n        if(avgloss<bestloss):\r\n            bestloss=avgloss\r\n            print(\"Network Improvement\")\r\n            saver.save(session, \"Weights/model_last\")\r\n        \r\n        testloss=0\r\n        testacc=0\r\n        for t in range(testbatch):\r\n            testfeed = {x:test_inpx[t],y:test_inp_sparse_y[t],seq_len:test_inpseqlen[t]}\r\n            outcome,testbatchloss,testbatchacc=session.run([decoded[0],cost,ler],testfeed)\r\n            if(t==0):\r\n                first_batch_outcome=outcome\r\n            testloss=testloss+testbatchloss\r\n            testacc=testacc+testbatchacc\r\n        \r\n        testfile=open(\"Results.txt\",\"w\")\r\n        testfile.write(\"Epoch \"+str(e)+\"\\n\")\r\n        for tc in range(len(testcases)):\r\n            predicted=label_from_sparse(first_batch_outcome,testcases[tc])\r\n            testfile.write(str(true[tc])+\" As \"+str(predicted)+\"\\n\")\r\n        testfile.close()\r\n        \r\n        testloss=testloss/testbatch\r\n        testacc=1-(testacc/ncts)\r\n        \r\n        endtime=time.time()        \r\n        if(testacc>best):\r\n            best=testacc\r\n            print(\"Test Result Improvement\")\r\n            bestsaver.save(session, \"BestWeights/model_best\")\r\n        timetaken=endtime-starttime\r\n        msg=\"Epoch \"+str(e)+\"(\"+str(timetaken)+ \" sec ) Training: Cost is \"+str(avgloss)+\" Accuracy \"+str(avgacc)+\" Testing: Loss \"+str(testloss)+\" Accuracy \"+str(testacc)+\"\\n\"\r\n        print(msg)\r\n        f.write(msg)\r\n        f.close()\r\n```\r\nNow, whenever I am loading the model from last save or best save, It is not showing any sign of previous training. Seems to be starting from some scratch. I also tried `import_meta_graph()` without any success. But the same strategy is working absolutely fine with a RNN model which is tested against the well known IRIS data set (hence a classification problem).\r\n```\r\nmygraph=tf.Graph()\r\n\r\nwith mygraph.as_default():\r\n    x=tf.placeholder(tf.float32,[None,4,1])\r\n    y=tf.placeholder(tf.float32,[None,3])\r\n    \r\n    rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\r\n    rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\r\n    \r\n    shape=tf.shape(rnnop)\r\n    batch=shape[0]\r\n    op=shape[1]*shape[2]\r\n    rnnrs=tf.reshape(rnnop,[batch,op],name=\"rnnrs\")\r\n    \r\n    w1=tf.Variable(tf.truncated_normal([12,5]),name=\"w1\")\r\n    b1=tf.Variable(tf.truncated_normal([5]),name=\"b1\")\r\n    \r\n    layer1=tf.add(tf.matmul(rnnrs,w1),b1)\r\n    layer1_op=tf.nn.tanh(layer1)\r\n    \r\n    w2=tf.Variable(tf.truncated_normal([5,6]),name=\"w2\")\r\n    b2=tf.Variable(tf.truncated_normal([6]),name=\"b2\")\r\n    \r\n    layer2=tf.add(tf.matmul(layer1_op,w2),b2)\r\n    layer2_op=tf.nn.tanh(layer2)\r\n    \r\n    w3=tf.Variable(tf.truncated_normal([6,3]),name=\"w3\")\r\n    b3=tf.Variable(tf.truncated_normal([3]),name=\"b3\")\r\n    \r\n    layer3=tf.add(tf.matmul(layer2_op,w3),b3)\r\n    prediction=layer3\r\n    \r\n    correct=tf.equal(tf.arg_max(prediction,1),tf.arg_max(y,1))\r\n    acc=tf.reduce_mean(tf.cast(correct,tf.float32))\r\n    #loss=tf.reduce_mean(tf.reduce_sum(tf.square(tf.sub(y,prediction))))\r\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))\r\n    optimizer=tf.train.MomentumOptimizer(0.01,0.9).minimize(loss)\r\n    saver=tf.train.Saver()\r\n    print(\"Network Ready\")\r\n\r\nwith tf.Session(graph=mygraph) as session:\r\n    feedx,feedy=loadiris(\"/media/parthosarothi/OHWR/Dataset/iris.csv\")\r\n    print(\"Data 0 x=\",feedx[0],\" y=\",feedy[0])\r\n    feed={x:feedx,y:feedy}\r\n    if(sys.argv[1]==\"load\"):\r\n        saver.restore(session,\"Weights/last\")\r\n        print(\"Previous Weights Loaded\")\r\n    else:\r\n        initop=tf.global_variables_initializer()\r\n        session.run(initop)\r\n        print(\"New Weights Loaded\")\r\n    for e in range(100):\r\n        l,_,p,a=session.run([loss,optimizer,prediction,acc],feed)\r\n        print(\"Loss is \",l,\" P \",p[0],\" y \",feedy[0],\" A \",a)\r\n        saver.save(session,\"Weights/last\")\r\n```\r\nI am completely in dark. Your concern is highly appreciated.", "Thanks for reaching out @xisnu. I'm having a hard time determining if this information indicates a bug in TensorFlow. You might want to try [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community support, since this issue tracker is for bugs and feature requests. If you can help us understand better why a bug exists, let me know and I'll re-open this issue.", "I also can confirm the issue:\r\nWhenever we are using the \r\n\r\n        rnncell=tf.nn.rnn_cell.BasicLSTMCell(3,state_is_tuple=True)\r\n        rnnop,_=tf.nn.dynamic_rnn(rnncell,x,dtype=tf.float32)#None,3,8\r\n    \r\nThe tf.train.Saver or alternative does not save the weights of the LSTM cells properlly.\r\nThis also seems to be the case of using\r\n\r\n        #Base cell\r\n        base_lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) \r\n\r\n        # Add dropout to the cell\r\n        drop = tf.contrib.rnn.DropoutWrapper(base_lstm, output_keep_prob = keep_prob)\r\n\r\n        # Stack up multiple LSTM layers, for deep learning\r\n        lstms_cells = tf.contrib.rnn.MultiRNNCell([drop]*2)\r\n\r\n        # Getting an initial state of all zeros\r\n        initial_state = lstms_cells.zero_state(batch_size, tf.float32)\r\n        initial_state = tf.identity(initial_state, name=\"initial_state\")\r\n\r\n            \r\n        rnn_out, final_state = tf.nn.dynamic_rnn(lstms_cells, inputs, dtype=tf.float32)\r\n\r\n\r\nSo right now it seems we have no reliable way to save and load LSTM based on `tf.contrib.rnn.BasicLSTMCell`", "Re-opening for triage again.", "It seems the issue is saving and restoring layers from  `tf.contrib.rnn`", "Thank you @igorbb  for confirming the issue. I updated Tensorflow but no change. Even I tried sucha model in Keras. It shows the same problem. I guess the problem is really in Tensorflow backend.", "Can you print the list of global variables before calling saver.save in one\ngraph, and after calling saver.save in the other graph?\n\nOn Jun 7, 2017 11:19 PM, \"Parthosarothi Mukherjee\" <notifications@github.com>\nwrote:\n\n> Thank you @igorbb <https://github.com/igorbb> for confirming the issue. I\n> updated Tensorflow but no change. Even I tried sucha model in Keras. It\n> shows the same problem. I guess the problem is really in Tensorflow backend.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-307009699>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyrodHMknQ5lpL1sJGBBMoI_Ce7yks5sB5JcgaJpZM4LceEB>\n> .\n>\n", "Yes\r\n\r\n**Before trainig**\r\n```\r\nwith tf.Session(graph=train_graph) as sess:\r\n    list_before_train = [v.name for v in tf.global_variables()]\r\npprint.pprint(list_before_train)    \r\n```\r\n\r\n> \r\n> ['EMBEDDING/Variable:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0',\r\n>  'OUT/W:0',\r\n>  'OUT/B:0',\r\n>  'beta1_power:0',\r\n>  'beta2_power:0',\r\n>  'EMBEDDING/Variable/Adam:0',\r\n>  'EMBEDDING/Variable/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam_1:0',\r\n>  'OUT/W/Adam:0',\r\n>  'OUT/W/Adam_1:0',\r\n>  'OUT/B/Adam:0',\r\n>  'OUT/B/Adam_1:0']\r\n> \r\n\r\n**After Training**\r\n```\r\nwith tf.Session(graph=train_graph) as sess:\r\n    list_after_train = [v.name for v in tf.global_variables()]\r\npprint.pprint(list_after_train)    \r\n\r\n```\r\n\r\n> ['EMBEDDING/Variable:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0',\r\n>  'OUT/W:0',\r\n>  'OUT/B:0',\r\n>  'beta1_power:0',\r\n>  'beta2_power:0',\r\n>  'EMBEDDING/Variable/Adam:0',\r\n>  'EMBEDDING/Variable/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam_1:0',\r\n>  'OUT/W/Adam:0',\r\n>  'OUT/W/Adam_1:0',\r\n>  'OUT/B/Adam:0',\r\n>  'OUT/B/Adam_1:0']\r\n\r\n**After load**\r\n *This means we reset the jupyter kernel (python env)\r\n```\r\ntf.reset_default_graph()\r\ntest_acc =[]\r\nwith tf.Session() as sess:\r\n    saver = tf.train.import_meta_graph(ckpt+'.meta')\r\n    saver.restore(sess, ckpt)\r\n    list_after_reset = [v.name for v in tf.global_variables()]\r\n```\r\n\r\n> \r\n> ['EMBEDDING/Variable:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0',\r\n>  'OUT/W:0',\r\n>  'OUT/B:0',\r\n>  'beta1_power:0',\r\n>  'beta2_power:0',\r\n>  'EMBEDDING/Variable/Adam:0',\r\n>  'EMBEDDING/Variable/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights/Adam_1:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam:0',\r\n>  'LSTM_ENS/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases/Adam_1:0',\r\n>  'OUT/W/Adam:0',\r\n>  'OUT/W/Adam_1:0',\r\n>  'OUT/B/Adam:0',\r\n>  'OUT/B/Adam_1:0']", "If we reset the python env: \r\nWe do **not** have the same accuracy for the  loaded model.\r\n\r\nIf we do not reset the python env:\r\nWe keep the same accuracy .", "It could be you're loading the wrong checkpoint?  Are you using [tf.train.latest_checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/latest_checkpoint)?", "Yes it what defines the ckpt variable. I forgot to include it in the snippet\n\n\nOn Jun 8, 2017 18:52, \"ebrevdo\" <notifications@github.com> wrote:\n\n> It could be you're loading the wrong checkpoint? Are you using\n> tf.train.latest_checkpoint\n> <https://www.tensorflow.org/api_docs/python/tf/train/latest_checkpoint>?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-307162338>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABB1Rs-k8DmeHP96MWBhBZ5dRR40YhMvks5sCCbOgaJpZM4LceEB>\n> .\n>\n", "A little stumped right now.  If you import the meta graph, and then\nre-export it, do the re-exported metagraph / graphdef change in the\nre-exported version?\n\nOn Thu, Jun 8, 2017 at 9:59 AM, igorbb <notifications@github.com> wrote:\n\n> Yes it what defines the ckpt variable. I forgot to include it in the\n> snippet\n>\n>\n> On Jun 8, 2017 18:52, \"ebrevdo\" <notifications@github.com> wrote:\n>\n> > It could be you're loading the wrong checkpoint? Are you using\n> > tf.train.latest_checkpoint\n> > <https://www.tensorflow.org/api_docs/python/tf/train/latest_checkpoint>?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 6683#issuecomment-307162338>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/ABB1Rs-\n> k8DmeHP96MWBhBZ5dRR40YhMvks5sCCbOgaJpZM4LceEB>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-307164390>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim54JBSYHbMaiPWFxWr9o0QBdwoR9ks5sCChqgaJpZM4LceEB>\n> .\n>\n", "I can try tomorrow when I am back at the office. \r\n\r\nBut for now I can provide more info : \r\n1 - I have checked if I was loading the righ .meta and .ckpt\r\n2 - Even tried by cleaning the checkpoints folder and starting from zero.  The files were definitely being created/updated.\r\n\r\nI found some questions regarding this on stack overflow, But there was no consensus other than saver should work. \r\nYet I haven't found anyone stating that it works flawless with a  python environment reset.\r\n\r\nOther than that I tried starting a new graph, load the meta and restored  session: If we reset the python kernel, no dice, some weight matrix is not loaded and accuracy goes down.\r\n\r\nBut if I do it **without** a kernel restart there is no problem.. The new graph keeps the accuracy and behaves like before.", "I ensure the same. I also tried 1 and 2 and they were fine. By the way, I am not using any separate virtual environment for python. So are there anything I can do with with \"restart\" ?", "hi xisnu, i do face the same issue after restore, the model train is happening from start instead the last check point. by any chance u had progress towards this issue ? i followed the recommendations in stack overflow but ntohing helps..", "@amadupu Sorry to say, no progress in this context. I believe the RNN weights are not saving properly as pointed out by @igorbb ", "thanks for the update, I've raised a new issue enclosing references (including this one) in the following request\r\nhttps://github.com/tensorflow/tensorflow/issues/10816\r\n\r\nhope someone has an answer to it", "I found the root cause. my input data vectors are modeled differently across each execution due to some issue in the data modeling part. Otherwise, no issue in tensor flow graph after restoration found. After fixing the input data model, I could able to retrain from the last checkpoint properly. I suggest cross checking the input data model for its consistency across multiple executions", "Good point. Though I have not checked my input data this way but I am quite sure that at every new training phase it is picking up a shuffled order of the same input. But should this matter? If I have a set of input X for training or testing then for any permutation of X shouldn't I get nearly same error/accuracy? Or I am misinterpreting your explanation @amadupu ", "I ran into the same issue when I reloaded a pretrained rnn model to generate a sequence. Now I have no idea about this problem.I haved tried many ways to check my code, I wonder whether some params in rnn are not stored.", "I have the same problem concerning the saving of rnn cells. I use a simple tf.nn.rnn_cell.BasicRNNCell for testing. Whenever I train my network stop the program and restart it to for example generate a sequence or further training (starting with the latest training state) it seems like the model was never trained before.\r\n\r\nBUT when I train the model, save it and in the same run train or generate a text by restoring the model before executing the task it works! That is crazy!\r\n", "I got it right when I use python2 instead.", "undoubtedly something is wrong with saving recurrent layer weights in tensorflow and python3. I have seen the same issue as @Hautzy reported.", "I train and store the model in python2, and restore it using python3, it got terrible result. But when I restore the model using python2, the result is good. Train and store in python3, I got awful result. ", "@vrv this looks like a serious issue in RNN layer store/restore with Saver in python3 (the behavior does not  appear in python2; see @pickou).  I won't be around to continue debugging for 2 weeks.  Anyone who can try to look at this with Saver experience?\r\n\r\n@pickou can you send us the metagraph & checkpoint for the model trained in python2?", "Yes. @pickou I got the same experience. In python2 everything is just fine. But python3 is making a mess of saving and restoring. I guess something is wrong in reading checkpoint values.", "@sherrym do you know who might know more about this? (Feel free to re-assign to someone who is more familiar with python saving semantics).", "@ebrevdo I use a model of multiplicative lstm instead of basiclstm. Do you still want the metagraph & checkpoint in python2?\r\n@xisnu I guess the problem lies not only  in reading checkpoint values, but also in storing. I have checked the values stored in python3, everything is right. I shall check the values stored in python2, and see the difference between the two.", "Yes please. it's ok if the lstm is not the one we provided, so long as we\ncan see the difference between py2 and py3.  Can you send us a link to the\nmultiplicative lstm code?\n\nOn Jul 25, 2017 8:24 PM, \"Junchao Yan\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> I use a model of multiplicative\n> lstm instead of basiclstm. Do you still want the metagraph & checkpoint in\n> python2?\n> @xisnu <https://github.com/xisnu> I guess the problem lies not only in\n> reading checkpoint values, but also in storing. I have checked the values\n> stored in python3, everything is right. I shall check the values stored in\n> python2, and see the difference between the two.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-317959979>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxQXsFsygxajmFVQnCUz-tCv7RDnks5sRtubgaJpZM4LceEB>\n> .\n>\n", "@ebrevdo Here is my code. https://github.com/pickou/multiplicative-lstm", "I can confirm the problem still exists in TF version 1.2.0. Saving and restoring model with LSTMCell and dynamic_rnn in Python 3 gives random weights. After switching to Python 2 all works fine.\r\n\r\nSolving it for TensorFlow will also make it work for Keras (using TF as backend). Actually I've rewritten my model from Keras to TF after having problems with Keras model.save(). Turns out that it's TF that is the problem.\r\n\r\nDoes anyone know how to make it work in Python 3?", "Has the issue been fixed in tensorflow 1.3?", "same problem under tensorflow 1.3, python 3.6 when restoring from tf.contrib.rnn", "Is everyone having this issue using Python 3.6?  What operating systme are you using?", "I am using Python 3.6 with tensorflow_gpu 1.3 in Ubuntu 14 TSI, the same issue happens. By switching to Python 2.7, restoring model works.", "There could be a problem in our swig wrapping w/ TF 3.6.\n\nOn Mon, Nov 6, 2017 at 12:29 PM, Sixun Ouyang <notifications@github.com>\nwrote:\n\n> I am using Python 3.6 with tensorflow_gpu 1.3 in Ubuntu 14 TSI, the same\n> issue happens. By switching to Python 2.7, restoring model works.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-342276131>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim26N4ae0LYx-hOyUUTu1Z0YYHfOeks5sz2wOgaJpZM4LceEB>\n> .\n>\n", "I am using Python 3.6 / TF1.4, having issue with same problem.. Ubuntu 16.04", "Tested with Python 2.7 / TF1.4 was same", "Is this in Windows or Linux or...?\n\nOn Mon, Nov 6, 2017, 11:56 PM Hwechul Cho (Derrick) <\nnotifications@github.com> wrote:\n\n> Tested with Python 2.7 / TF1.4 was same\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-342403227>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_g_hyPVvcqT1wq7GiEKgYsXNo8Wks5s0A00gaJpZM4LceEB>\n> .\n>\n", "I use Ubuntu. I think this problem comes with initial state when using dynamic rnn. If we use just `dtype` from `dynamic_rnn(..., dtype=tf.float32)` (which means not using user-defined initial state), `tf.Saver()` doens't save dynamic rnn last state.", "I used win7, Python3.6/TF 1.4m, which has this problem, I am trying Ubuntu, Python2.7 and TF1.4, hope this can be ok.\r\n\r\n@flrngel I used a user defined initial state in dynamic rnn, but have the problem \r\ninit_s = rnn_cell.zero_state(batch_size=BATCH_SIZE, dtype=tf.float32)  \r\n    outputs, final_s = tf.nn.dynamic_rnn(\r\n        rnn_cell,  \r\n        data,\r\n        initial_state=init_s,  \r\n        time_major=False, \r\n    )", "@gaofeitongji yeah, I'm having same trouble after I changed rnn user defined initial variable.. I'm in debugging all variables now..", "@flrngel yeah, @MagicTroy said it was ok in python 2,  it does not work for you ?", "Last test was with tensorflow 0.12 and python 2.7. Everything was fine.", "I found my problem and the problem was shuffling before making dictionary.. \ud83d\ude22 ", "@flrngel what was that? can you explain more detail?", "Everything was fine, Python2.7, TF1.4.", "@pickou can you provide anna.txt and any other files required to run? ", "ok, I'll push a complete one @ebrevdo .", "Thanks for asking the question and contribution guys. I'm having the same problem saving rnn LSTM layers with TF using python3. Restoring saved TF RnnLSTM using python3 is giving me random weights as well. Just wondering has this been fixed? ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "We have yet to track down the cause of this bug.\n\nOn Wed, Dec 20, 2017, 11:35 AM Alfred <notifications@github.com> wrote:\n\n> It has been 14 days with no activity and this issue has an assignee.Please\n> update the label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-353159221>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwHKd7WDXMPYhlU7e1am5SGzEZL8ks5tCWAfgaJpZM4LceEB>\n> .\n>\n", "Thank you - really appreciate the response\n\nOn Thu, 21 Dec 2017 at 3:14 PM, ebrevdo <notifications@github.com> wrote:\n\n> We have yet to track down the cause of this bug.\n>\n> On Wed, Dec 20, 2017, 11:35 AM Alfred <notifications@github.com> wrote:\n>\n> > It has been 14 days with no activity and this issue has an\n> assignee.Please\n> > update the label and/or status accordingly.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-353159221\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/ABtimwHKd7WDXMPYhlU7e1am5SGzEZL8ks5tCWAfgaJpZM4LceEB\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-353275691>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACHU99ru2I8ODzOZlmyjpyA88hgPYHjvks5tCgVogaJpZM4LceEB>\n> .\n>\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "ok but im not sure how\n\nOn Fri, Jan 5, 2018 at 3:28 AM, Alfred <notifications@github.com> wrote:\n\n> It has been 14 days with no activity and this issue has an assignee.Please\n> update the label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-355372898>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACHU9zPjAYRFb37izr4gTeRzMpnXBQ5Zks5tHSZvgaJpZM4LceEB>\n> .\n>\n", "Glad I found this issue. Having the same problem. Will definitely use python2.7 for LSTM now. Really hope you can fix this! Thanks!", "thanks! There seems to be a new update for tensorflow. Is the new update\nfixed for python3?\n\nOn Mon, Jan 8, 2018 at 1:56 AM, nrocketmann <notifications@github.com>\nwrote:\n\n> Glad I found this issue. Having the same problem. Will definitely use\n> python2.7 for LSTM now. Really hope you can fix this! Thanks!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-355839871>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACHU9wWy81UkWreprmw9Lp-ve5zm7dK2ks5tIQVGgaJpZM4LceEB>\n> .\n>\n", "@ebrevdo do we have a reproducible example for this problem? Thanks!", "@rajatmonga I believe the code in https://github.com/pickou/multiplicative-lstm should reproduce the problem.  I tried running it the other day but the training phase didn't end, and i wasn't sure what parameters to tweak to ensure it finished quickly enough that i could try loading a corrupt checkpoint.  @pickou does training need to run for as long as it does?", "@ebrevdo set the epoch to small value, 2 or 1. set hidden units to small value, maybe 64 or 128\u3002set lr to 0.01 or higher.", "Glad I've found this issue. I noticed strange loss growth and almost immediate fall back after I stoppped and run training again from last checkpoint.\r\n\r\nI can't switch from Python36-64 to python2 as my development machine has Windows, and there are no tensorflow windows python2 support.\r\n\r\n**UPD:**\r\nI dumped `tf.global_variables()` right after saving after training, and dumped them just after loading from that checkpoint, and got same result, so **I don't think it's a bug in Saver.**\r\n\r\nStill no idea why I got these loss peaks in the beginning of training. Also noticed there are to such peaks sometimes on almost trained models with low LSTM layer count, idk.", "Yes I have the same issue as well\n\nUnfortunately, the loss growth is still there even when I run it with\npython2\n\nOn Mon, 22 Jan 2018 at 6:02 PM, gloriouskilka <notifications@github.com>\nwrote:\n\n> Glad I've found this issue. I noticed strange loss growth and almost\n> immediate fall back after I stoppped and run training again from last\n> checkpoint.\n>\n> I can't switch to python2 as my development machine has Windows, and there\n> are no tensorflow windows python2 support.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6683#issuecomment-359375519>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACHU96lJuC_Hal89PEcgnGKZ1EywIEsPks5tNFyegaJpZM4LceEB>\n> .\n>\n", "same problem with you ! now i fix it by saving the vocab once and load it later traing, thank you your reminder! @flrngel", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@pickou  : As @NLPpupil and @flrngel point out, I don't believe there is a problem with the TensorFlow checkpointing here, but rather it's the use of `set` for the generating the vocabulary in your program that is the underlying cause of the problem.\r\n\r\nSpecifically, between `python train_gentext.py` and `python load_gentext.py`, the vocabulary changes because each time [`preprocess()`](https://github.com/pickou/multiplicative-lstm/blob/5c5c1ffceb34b541d7dbfe96f6813abdb49086ad/mlstm.py#L31) is invoked, it will return a different mapping from letters to integers due to its use of `set`, hence you see messed up results.\r\n\r\nI quickly verified this by adding a:\r\n```python\r\nprint(int_to_vocab)\r\n```\r\nin `preprocess()` and observed different mappings when invoking `train_gentext.py` and `load_gentext.py`.\r\n\r\nFurthermore, I verified that the variables are showing no corruption by printing the hashes of the variable contents before saving and after restoring using something like this:\r\n\r\n```python\r\ndef print_var_hashes(sess):\r\n  import hashlib\r\n  for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):                                                                              \r\n    val = sess.run(v.value())\r\n    h = hashlib.md5()                                                                                                                        \r\n    h.update(val.data)                                                                                                                       \r\n    print('%-50s %s' % (v.name, h.hexdigest()))\r\n```\r\n\r\nSo I think @pickou 's example isn't quite right.\r\n\r\n@xisnu @igorbb @bartgras  : Are you seeing this after moving to TF 1.0+? Could you provide a means to reproduce the problem?", "@asimshankar This might be the problem in my given example,I'll fix this and see if any error occurs after Spring Festival.", "@asimshankar I have fix this error, and no error occurs using TF 1.4 . Thanks", "Closing this out since it seems the issue does not exist in stable versions of TensorFlow (version 1.0 and above).\r\n\r\nPlease feel free to comment or open a new issue if I'm mistaken.\r\n\r\nThanks!", "I have solved this problem.\r\nActually, you restore your model like this:\r\n\r\nsess.run(tf.global_variables_initializer())\r\nsaver = tf.train.import_meta_graph(\"./tmp/model.ckpt.meta\")\r\nsave_path = saver.restore(sess, \"./tmp/model.ckpt\")\r\n\r\nDon't place your global_variables_initializer after your restoration, because all the variables will be re-initialized again, therefore it will retrain the model freshly. @xisnu \r\n", "Great!  Maybe we need to add a warning to saver docstring saying don't reinitialize variables after restore.", "> I have solved this problem.\r\n> Actually, you restore your model like this:\r\n> \r\n> sess.run(tf.global_variables_initializer())\r\n> saver = tf.train.import_meta_graph(\"./tmp/model.ckpt.meta\")\r\n> save_path = saver.restore(sess, \"./tmp/model.ckpt\")\r\n> \r\n> Don't place your global_variables_initializer after your restoration, because all the variables will be re-initialized again, therefore it will retrain the model freshly. @xisnu\r\n\r\nThis gives me all the variables twice when adding:\r\n`print(tf.global_variables())`"]}, {"number": 6682, "title": "Weird behavior of tf.tensordot when shapes are partly known", "body": "I frequently use ```tf.tensordot```, for its flexibility on axes.\r\nHowever, as I multiply two tensors whose shapes are partly known, it cannot infer the shape of the result tensor, i.e. shape is `<unknown>`, while other alternative functions that do the same operation infer shapes well.\r\n\r\nFor example, let `a` and `b` be\r\n```\r\na = tf.placeholder('float32', shape=[None, 100])\r\nb = tf.placeholder('float32', shape=[100, 300])\r\n```\r\n\r\nWe can validate that the below functions infer shapes well.\r\n```\r\nresult_matmul = tf.matmul(a, b)\r\nresult_matmul.get_shape().as_list()  # [None, 300]\r\n\r\nresult_einsum = tf.einsum('ij,jk->ik', a, b)\r\nresult_einsum.get_shape().as_list()  # [None, 300]\r\n```\r\n\r\nHowever, when `tf.tensordot` is used, the weird result occurs:\r\n```\r\nresult_tensordot = tf.tensordot(a, b, axes=[[1], [0]])\r\nresult_tensordot.get_shape()  # TensorShape(None)\r\nresult_tensordot.get_shape().as_list()  # Error\r\n```\r\nThus, I have to call `set_shape(shape)` function explicitly to make the output be able to be used for further operations.\r\n\r\nThere also exists an issue that it still uses `concat` function, where the warning that it would be deprecated after 2016-12-14 occurs, instead of `concat_v2`.", "comments": ["@rmlarsen here's a customer for partial shape inference in tensordot. Any plans on working on it, or want to throw it over to contributions welcome?", "Despite having a TODO for myself in the code, I'll add contributions welcome, in case somebody can beat me to it. It is not on the top of my list.", "Okay. I will look forward it if there's a point I can contribute.\r\nBy the way, is it somewhat \"uncommon\" to use `tensordot` function?\r\nI was a user of \"reshape (or transpose) - matmul - reshape (or transpose)\" approach previously, and am finding methods for doing this boring series of operations.\r\nI think `einsum` function is an attractive alternative, indeed more powerful alternative. A single question that makes me hesitate to move to `einsum` is: is there no performance issue?", "I've just submitted a pull request (#8452) that should address this."]}, {"number": 6681, "title": "Adding support for Linux s390x", "body": "Adding support for Linux s390x for TensorFlow", "comments": ["Can one of the admins verify this patch?", "Thank you for recreating the PR.\r\nCLA looks good.\r\nJenkins, test this please.", "Windows cmake test failure might be an issue on jenkins executor.\r\nJenkins, test this please.", "https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/704/console\r\n\r\nThis time it failed for real. Could you take a look?", "The error is '__BYTE_ORDER__' and '__ORDER_BIG_ENDIAN__' are undeclared identifiers. However @guschmue had already committed the fix for BYTE_ORDER on windows through [PR](https://github.com/tensorflow/tensorflow/pull/6102). \r\n@gushmue could you please have a look?", "Do I need to explicitly include `tensorflow/core/platform/cpu_info.h` in the files where I am using the BYTE_ORDER flag?", "To me it looks like you definitely need to include `tensorflow/core/platform/cpu_info.h`. You also need to add a BUILD dependency for the library which exposes this header file.", "Sounds good, will merge once the build is restored.", "09:39:59          c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\bfloat16.cc(23): error C2065: '__BYTE_ORDER__': undeclared identifier [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_core_framework.vcxproj]\r\n09:39:59          c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\bfloat16.cc(23): error C2065: '__ORDER_BIG_ENDIAN__': undeclared identifier [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_core_framework.vcxproj]\r\n09:39:59          c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\bfloat16.cc(37): error C2065: '__BYTE_ORDER__': undeclared identifier [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_core_framework.vcxproj]\r\n09:39:59          c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\bfloat16.cc(37): error C2065: '__ORDER_BIG_ENDIAN__': undeclared identifier [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_core_framework.vcxproj]\r\n", "@drpngx, I have made changes as per your feedback. \r\n@gunan, `cpu_info.h` has been included in all respective files and as a BUILD dependency. \r\nCould you request a build to verify?", "Jenkins, test this please.", "Please put the `cpu_info` alphabetically sorted in `tensorflow/core/BUILD`", "Jenkins, test this please."]}, {"number": 6680, "title": "Tensorflow(GPU) build needs to be reconfigured, everytime I reboot my ubuntu", "body": "If a just configure tensorflow using ./configure and set CUDA support to YES, tensorflow is successfully built. I can make some change in source code and rebuild using command:\r\n\r\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nBuild after I reboot my ubuntu, and try to build tensorflow using above command, it requires me to reconfigure tensorflow, and rebuild, which takes such a long time. error message is as follows:\r\n\r\nERROR: /home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/wolfson/.cache/bazel/_bazel_wolfson/41eaf6c788f09c81cffb135517d04fa2/external/local_config_cuda/crosstool/BUILD.", "comments": ["@davidzchen @damienmg \r\nDo you think this is a similar issue to #4848", "I this this is related to #4705 \r\n\r\nMy fix is \r\n1, Add export TF_NEED_CUDA=1 to ~/.bashrc\r\n2, Source ~/.bashrc\r\n3, configure tensorflow\r\n4, bazel build", "@xmbrst could we add this step to the docs?", "This may be an issue with setup; reassigning to @martinwicke .", "@damienmg I think the action_env should set the variable, not only make it available, right? So writing the bazel.rc should not let this happen.\r\n\r\nIs this still an issue?", "@martinwicke yes `--action_env FOO` make `FOO` available to action but `--action_env FOO=BAR` makes set `FOO` to `BAR` in all action as well as for configuration rules (roughly equivalent to `LAST_FOO=$FOO; export FOO=BAR; bazel build --action_env FOO; export FOO=$LAST_FOO`)", "Ok. I strongly believe that this issue is fixed. Please comment to reopen if this is an issue in 1.1 or at head."]}, {"number": 6679, "title": "Throw uninitialized local variable error to reduce confusion.", "body": "I reference this [post](http://stackoverflow.com/questions/41488276/cannot-get-a-simple-tfrecord-reader-to-work/41489484#41489484).\r\n\r\nIf one does not call `tf.local_variables_initializer()` when using `tf.train.string_input_producer` with `num_epochs` set as a local variable, it will throw an `OutOfRangeError` which might mislead people into thinking that the error lies with `tf.train.string_input_producer` instead of initializing the local variable.\r\n\r\nThrowing an uninitialized local variable error would be very helpful in this case.\r\n\r\n@mrry ", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I doubt we'll fix this bug. The current recommendation is to use `tf.data`, which does not rely on variables to be initialized, and hence doesn't suffer from this problem."]}, {"number": 6678, "title": "Installation from Source Error: \"Server finished RPC without an explicit exit code\"", "body": "### Environment info\r\nOperating System: Ubuntu 14.04\r\nGPU: NVIDIA Titan X\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`) f64c288ffd20819e264b80a4977b2ca8f33bfd98 (should be the latest one...)\r\n2. The output of `bazel version` 0.4.3\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nWhen running \"`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\", it reported \"`Server finished RPC without an explicit exit code`\" after a few minutes' running. The last few lines looks as follows:\r\n\r\n```\r\n...\r\n[2,244 / 3,310] Linking tensorflow/core/libarray_ops_op_lib.pic.lo\r\nServer finished RPC without an explicit exit code\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nI managed to install tensorflow via pip installation, and everything is running correctly (ex. Tensorflow demo).\r\n\r\nDoes anyone has any suggestions? Thanks in advance! :-)\r\n", "comments": ["Have you tried increasing the memory available?", "@michaelisard Hi Michael, thanks for your reply!\r\nYes, memory matters. I tried \"`bazel build -c opt --config=cuda --local_resources 8096,.5,1.0 //tensorflow/tools/pip_package:build_pip_package`\", and it managed to build. But it took 3 hours to build... Do you have any suggestion to speed up? \r\nMy machine has 16G RAM and 12G GPU memory. Thanks!", "I'm glad that solved the immediate problem! I would ask on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) for advice on speeding up the build; I'm sorry I don't have a lot of personal experience."]}, {"number": 6677, "title": "Tensorboard not showing data", "body": "I have installed the newest version of tensorflow. I was trying to write my code to the tensorboard, but was not able.  \r\nThis is what i got as a response .\r\n![image](https://cloud.githubusercontent.com/assets/14088328/21702101/ded3524a-d3a9-11e6-9334-30f960adaf8c.png)\r\n\r\nI found out was the problem was. The tensorboard program was not able to find the log files, this was because of it was saved in a pathname that uses a non \"english\" letter, in this particular instance the letter \"\u00c5\". So maybe you cant fix this bug\r\n\r\nBest regards \r\n", "comments": ["Can you please reopen this at https://github.com/tensorflow/tensorboard/issues with a reproduction? Perhaps a script we can run to generate unreadable data. "]}, {"number": 6676, "title": "fix documentation", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6675, "title": "Creating an auxiliary CUDA Stream for an op", "body": "I am writing an op which (sadly) requires using multiple CUDA streams to be efficient. Specifically, I need to have one auxiliary CUDA stream on whatever device the op is being executed; without this stream, the operations in this op will get entangled with operations in the rest of the graph, because the op _has_ to occasionally wait for its GPU kernels to complete before it can do things on CPU.\r\n\r\nIf I need to have an auxiliary CUDA stream besides the primary TensorFlow stream, one limited *just*  to the invocation of this op, how can I do so?\r\n\r\n(To clarify, the CUDA stream should be long-lived across multiple invocations of the op, probably, just that it shouldn't the same stream as used by the rest of the graph)", "comments": ["Figured it out!", "What was the solution? I am needing to use cuBLAS in a custom op but suspect I cannot use the default stream."]}, {"number": 6674, "title": "Generic swig rules", "body": "Looking into integrating Swig into our Bazel build all I could find was Tensorflows usage.\r\nIs there any reason parts like `_py_wrap_cc` have not been extracted to a more general script?\r\nWould tensorflow be willing to use such a thirdparty script should it get extracted/provided (into another github project perhaps)?", "comments": ["TensorFlow might be planning to transition to [PyClif](https://pypi.python.org/pypi/pyclif/0) at some point so not sure how much interest there is to refactor existing SWIG usages. CC'ing @jart for opinion since she last touched those build rules", "Can you at least give a rough overview of `pyclif`? Is it parsing headers same as `swig`.", "@gpshead for comment on PyClif. My understanding is that it's quite different from SWIG.", "Our current SWIG build code is not in a state where it's generalized enough for consumption outside the TensorFlow project. I've actually been thinking about cleaning up that code at some point or another, so I can break up our massive swig shared object into a bunch of tiny shared objects. But I'll hold off on that considering what I've read here.\r\n\r\n@gpshead TensorFlow would be a big customer for the PyClif project that would help it succeed. If PyClif wants to start a [bazelbuild](github.com/bazelbuild/)/rules_pyclif project with general purpose skylark rules that behave identically both internally and externally, and then migrate the TensorFlow project to use it, then I am willing to do all the necessary code reviews on both. I have experience setting up the [rules_closure](https://github.com/bazelbuild/rules_closure) project.", "@jart I am currently looking into a more general _rules_swig_ project. Prototyping the rules, I somewhat get the feeling I should wait until 0.6 so I can use generalized providers. Also I am not in a hurry.", "What are \"generalized providers\"?\r\n\r\nIf you're interesting in making a rules_swig project for Bazel, I would recommend having a conversation with @lberki and @ulfjack beforehand. Internally at Google we have a solution for this called py_wrap_cc. But it wasn't open sourced. Probably because Swig was a hard problem to solve and required peculiarities that wouldn't make sense outside of Google.", "Closing due to inactivity. Perhaps consider filing this issue against the Bazel project instead. Feel free to borrow any code from our repository that you like.", "We are actively working on open sourcing `CLIF` in the next few months. I suspect you should wait until we've released the tool before you start using it within the TF project. Providing `py_clif_cc` Bazel macros may not be part of that release given the current state of Bazel when it comes to Python support.", "@gpshead will it support Windows out of the gate?", "Likely, but not yet tested so I'm not promising anything until then. :)", "Some recent activity on this subject:\r\n\r\nhttps://stackoverflow.com/questions/51716049/recommended-approach-for-building-codebases-which-require-python-bindings-to-c\r\n\r\nhttps://github.com/bazelbuild/bazel/commit/d1d613c9e93ce69e3880728350d1cff959f6845d\r\n"]}, {"number": 6673, "title": "Enable sandboxing for Android builds to prevent asset double-inclusion", "body": "See https://github.com/bazelbuild/bazel/issues/2334 for context.", "comments": ["Jenkins, test this please."]}, {"number": 6672, "title": "Merge master into r1.0 to fetch all the fixes.", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Merge PR, merging all changes within the same repo, CLA ok to ignore."]}, {"number": 6671, "title": "WIP: In MatchFilenamesOnceTest.test, call get_temp_dir() only once", "body": "Also make sure files opened for writing are closed properly.", "comments": ["Win cmake py test running at: http://ci.tensorflow.org/job/experimental-cais-win-cmake-py/DEVICE=cpu/6/console\r\nTake 2:\r\nhttp://ci.tensorflow.org/job/experimental-cais-win-cmake-py/DEVICE=cpu/8/console", "This is a nice to have change, but closing it for now."]}, {"number": 6670, "title": "How to read Android Demo Detection Model Priors.pb file ?", "body": "Hello ! \r\n\r\nI would like to look into the assets file : multibox_location_priors.pb which contains boxpriors locations.\r\nI want to use them in a iPythonNotebook to test the model with offline images but I don't know how to access properly to the values for all locations.\r\nIs there any easy way to parse the file and get the locations values like it is possible to do with graph.pb file using these lines \r\n\r\n```\r\nwith tf.gfile.FastGFile(\"graph.pb\", 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n```\r\n\r\nThanks in advance for your help\r\nAlex", "comments": ["@AlexandreBriot The easiest thing to do would be to add tensorflow/examples/android/proto/box_coder.proto to CORE_PROTO_SRCS in tensorflow/core/BUILD (you may need to copy the file), and then you should be able to rebuild TF with support for reading it. It's not there already simply because it's a very model-specific proto only currently used for the demo.", "@andrewharp Hello Andrew and thank you for your precious help !\r\nI followed your instructions and here is where I am.\r\n\r\n- I modified  CORE_PROTO_SRCS in tensorflow/core/BUILD adding \"framework/box_coder.proto\",\r\n\r\n- I copied box_coder.proto in the corresponding core/framework/ directory \r\n\r\n- I rebuild tensorflow from source but get some errors related to proto version 2 vs 3\r\n\r\n- I modified box_coder.proto by doing these changes \r\n\r\n   * Change `syntax = \"proto2\";` to` syntax = \"proto3\";`\r\n   * Change` package org_tensorflow_demo; `to `package tensorflow;`\r\n   * Removing all optional and all default value \r\n           ex : `optional float mean = 1 [default = 0.0];` becomes `float mean = 1;`\r\n   * Suppressing all `; `after message section \r\n\r\n   Is that correct ? Or is there some inexact/unappropriate changes ?\r\n \r\n\r\n- Now I am able to import this freshly built tensorflow in my iPythonNotebook\r\n  The new proto definition is supposed to decode my box-priors.pb file\r\n  I want to do so using the lines as I can do for GraphDef but I don't know which Object I should create to replace GraphDef one and then loading data with ParseFromString method ...\r\n\r\n```\r\nwith tf.gfile.FastGFile(\"box-priors.pb\", 'rb') as f:\r\n    priors = tf._?Object?_()\r\n    priors.ParseFromString(f.read())\r\n```\r\n\r\nCould you give me your advice on that part ?\r\nThanks in advance for your help \r\nAlex\r\n\r\n\r\n\r\n\r\n", "@AlexandreBriot We'll probably update the code soon to just load from a txt file, as separately supporting a  proto build in the Android example adds disproportionate overhead.\r\n\r\nIn the meantime, I've exported the entire box prior proto to text, which you should be able to easily load into your Python code:\r\n\r\nhttps://gist.github.com/andrewharp/136b07af1f221e9c95b383889f9911e0\r\n\r\nedit: leaving this issue open to track until the discussed conversion is complete", "@andrewharp Thanks for your answer !\r\n\r\n", "@andrewharp  I have a trained spam detection model trained using keras and have got the .pb I am trying it to implement it into a mobile app. Can you please guide me to the right documentation please. It will be of great help."]}, {"number": 6669, "title": "Python3 pickle treat tf.gfile.GFile wrong", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: macOS Sierra 10.12.2\r\n\r\nInstalled version of CUDA and cuDNN: None\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): None\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n4d924e796368163eff11a8151e8505715345f58d\r\n\r\n2. The output of `bazel version` \r\n0.4.3-homebrew\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pickle\r\n\r\nf = tf.gfile.GFile(\"data_batch_1\")\r\npickle.load(f, encoding='latin1')\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nI tried to manually read all data into a variable: `s = f.read()` and unpickle it: `pickle.loads(s, encoding='latin')`. It works well.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n`TypeError: a bytes-like object is required, not 'str'`\r\n\r\nI don't know why this happened. I checked the file_io code, and it returns byte object when calling `read`. So maybe this is something about pickle, any ideas?", "comments": ["This kind of usage question is best asked on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow). Github issues are for bug reports and installation issues.", "@michaelisard I believe it's a bug about how we implement `file_io`'s `readline()`. It would return `str`. \r\nBut according to the python3 pickle document:\r\n\r\n> The argument file must have two methods, a read() method that takes an integer argument, and a readline() method that requires no arguments. **Both methods should return bytes.** Thus file can be an on-disk file opened for binary reading, an io.BytesIO object, or any other custom object that meets this interface.\r\n\r\nAny thoughts?", "@sherrym what do you think?", "I can confirm this bug in Ubuntu 16.04, python 3.5.\r\nI fixed this bug replacing **tf.gfile.GFile(<something>)** with **open(<something>)**", "@saxenasaurabh @rohan100jain @sherrym : Any updates?", "@orlov-alexander-dd I think that's a workaround since you use python open file API instead of the c++ implementation.", "This should be fixed by now... As long as the file is opened in binary mode it should work.", "@rohan100jain Will try, thanks!"]}, {"number": 6668, "title": "bad zlib link in core/BUILD", "body": "even though https://github.com/tensorflow/tensorflow/issues/6594 fixed the link to zlib in workspace.bzl, I'm getting another error looking for zlib at a bad link during the bazel build. Here's the error: \r\n```\r\nERROR: /opt/tensorflow/models/syntaxnet/tensorflow/tensorflow/core/BUILD:911:1: no such package '@zlib_archive//': Error downloading from http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive/zlib-1.2.8.tar.gz: 404 Not Found: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\r\n<html><head>\r\n<title>404 Not Found</title>\r\n</head><body>\r\n<h1>Not Found</h1>\r\n<p>The requested URL /zlib-1.2.8.tar.gz was not found on this server.</p>\r\n<p>Additionally, a 404 Not Found\r\nerror was encountered while trying to use an ErrorDocument to handle the request.</p>\r\n</body></html>\r\n and referenced by '//tensorflow/core:lib_internal'.\r\n____Loading package: @local_config_cc//\r\nERROR: /opt/tensorflow/models/syntaxnet/tensorflow/tensorflow/core/BUILD:911:1: no such package '@zlib_archive//': Error downloading from http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive: Error downloading http://zlib.net/zlib-1.2.8.tar.gz to /root/.cache/bazel/_bazel_root/9f6232783b216d8ac3748a33750481a1/external/zlib_archive/zlib-1.2.8.tar.gz: 404 Not Found: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\r\n<html><head>\r\n<title>404 Not Found</title>\r\n</head><body>\r\n<h1>Not Found</h1>\r\n<p>The requested URL /zlib-1.2.8.tar.gz was not found on this server.</p>\r\n<p>Additionally, a 404 Not Found\r\nerror was encountered while trying to use an ErrorDocument to handle the request.</p>\r\n</body></html>\r\n```", "comments": ["sorry the title is a bad description: the line in zlib is line 1136 and doesn't directly contain the link, not sure how the link gets called", "Have you tried the patch in #6594 to replace it with 1.2.10?", "I was trying that, but I believe the fix has been merged to master correct? I'm probably just doing something stupid *facepalm*", "We've merged to our latest stable branch (0.12) and master, so there's not much we can do here.  I suspect your code / environment / git is synced to an earlier commit -- try updating the branch and the problem should go away.  Hope that helps!", "sorry my fault, it was a submodule that wasn't updated. thanks for responding!", "How did you update that submodule? @bgrayburn ", "@harshsp31 been awhile since I did this, but should just need to go to head of master in tensorflow/models, then I probably did something silly like `cd` into the tensorflow submodule folder and pull, but you should probably do `git submodule update --recursive --remote` in the tensorflow/models folder to update any and all submodules.", "@bgrayburn Thank you for helping me out. I found the fix :)"]}, {"number": 6667, "title": "Branch 143639671", "body": "", "comments": []}, {"number": 6666, "title": "tf.train.Saver does not restore input queue", "body": "**example.py**\r\n```python\r\nimport tensorflow as tf\r\n\r\nqueue = tf.train.string_input_producer(['data.txt'])\r\n\r\nreader = tf.TextLineReader()\r\n_, line = reader.read(queue)\r\n\r\nglobal_step = tf.Variable(0, trainable=False, name='global_step')\r\nglobal_step_op = tf.assign_add(global_step, 1)\r\n\r\nwith tf.Session() as sess:\r\n    saver = tf.train.Saver()\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n    latest_checkpoint = tf.train.latest_checkpoint('/tmp')\r\n    if latest_checkpoint:\r\n        saver.restore(sess, latest_checkpoint)\r\n        print('restored from', latest_checkpoint)\r\n    else:\r\n        sess.run([\r\n            tf.local_variables_initializer(),\r\n            tf.global_variables_initializer(),\r\n        ])\r\n\r\n    for i in range(5):\r\n        value, step = sess.run([line, global_step_op])\r\n\r\n        print(value, step)\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n\r\n    saver.save(sess, '/tmp/model')\r\n```\r\n\r\n**data.txt**\r\n```txt\r\nline 1\r\nline 2\r\nline 3\r\nline 4\r\nline 5\r\nline 6\r\n```\r\n\r\nWill output on first run `python3 example.py`:\r\n> b'line 1' 1\r\n> b'line 2' 2\r\n> b'line 3' 3\r\n\r\nWill output on second run `python3 example.py`:\r\n> restored from /tmp/model\r\n> b'line 1' 4\r\n> b'line 2' 5\r\n> b'line 3' 6\r\n\r\nbut expected:\r\n> restored from /tmp/model\r\n> b'line 4' 4\r\n> b'line 5' 5\r\n> b'line 6' 6", "comments": ["I think that's working as intended -- Saver only restores parameters, and queues don't have those. Recreating Queue op resets it to the beginning.", "In my case it caused a systematic error in my hyperparameter evaluation however it should be easy to work a way around (e.g. always complete one epoch).", "If someone finds this. It should also be possible to use `reader.restore_state()` for this purpose. [See API docs](https://www.tensorflow.org/api_docs/python/io_ops/readers#TFRecordReader.restore_state)", "@bodokaiser did you try the solution of using reader.restore_state()? Did your Queue save correctly? I'm having huge problems with my model suddenly seeming overtrained when I save/reload it. Looks like it's seeing old data. ", "@yaroslavvb How would you suggest saving a Queue if the full dataset is not used at the point of saving? In that case, we need to keep the state of the Queue so it doesn't see the same old data again on restoring. If we save a .meta and restore the saver as in saver.restore(sess, tf.train.latest_checkpoint(\"./model/\")), will that save the Queue state? \r\nAlternatively, if I use the method from http://ischlag.github.io/2016/06/19/tensorflow-input-pipeline-example/ of a slice_input_producer and tf.train.batch instead of a RandomShuffleQueue, do these have savable parameters that save the queue's place in the data so they can be safely reloaded with a Checkpoint? "]}, {"number": 6665, "title": "Changes in Interactive Usage Section ", "body": "Changed x from Variable to constant and commented the x.initializer.run()", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "What is the rationale for this change?", "I think having Variable there shows how to create variables. So it adds value.\r\nI also verified that it works on 0.12 as is right now.\r\n\r\nSo I will close this PR. If you really think we should make this change, please reopen or create a new PR with more explanation."]}]