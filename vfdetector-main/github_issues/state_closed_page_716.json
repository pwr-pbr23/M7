[{"number": 32099, "title": "tf2.0.0-rc0: tensor shape check failed when using tf.function in tf.distribute.Strategy scope.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7\r\n- TensorFlow installed from (source or binary): binary(docker image: 2.0.0rc0-gpu-py3-jupyter)\r\n- TensorFlow version (use command below): 2.0.0-rc0\r\n- Python version:3.6.8\r\n- CUDA/cuDNN version: V10.0.130\r\n- GPU model and memory: P40\r\n\r\n**Describe the current behavior**\r\nI followed the document for writing custom training loop([ref](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/distribute/Strategy#in_short)), and caught an ValueError exception:\r\n> ValueError: Input tensor 'Const_1:0' enters the loop with shape (), but has shape (None, 110, 110, 1) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\r\n\r\nIf I removed 'tf.function' decorator, the code worked fine with a warning:\r\n> WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\r\n\r\n**Describe the expected behavior**\r\nIt should work, and run faster than eager mode.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\ndef get_net():\r\n    net = tf.keras.Sequential()\r\n    net.add(tf.keras.layers.Conv2D(filters=10,\r\n                                   kernel_size=(3, 3)))\r\n    net.add(tf.keras.layers.Dense(1))\r\n    return net\r\n\r\ndata = tf.random.normal(shape=(1280, 112, 112, 3))\r\nlabel = tf.random.normal(shape=(1280, ))\r\nmulti_db = tf.data.Dataset.from_tensor_slices((data, label)).batch(80)\r\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(multi_db)\r\n\r\nwith mirrored_strategy.scope():\r\n    net = get_net()\r\n    @tf.function\r\n    def replica_fn(input):\r\n        d, l = input\r\n        return net(d)\r\n\r\n    @tf.function\r\n    def distribute_train_epoch(dataset):\r\n        total_result = 0\r\n        for x in dataset:\r\n            per_replica_result = mirrored_strategy.experimental_run_v2(replica_fn, args=(x,))\r\n            total_result = mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_result, axis=None)\r\n        return total_result\r\n\r\n    for _ in range(100):\r\n        f = distribute_train_epoch(dist_dataset)\r\n```", "comments": ["I could reproduce the issue with Tensorflow 2.0.0-rc0. Please take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/58c3a0dd2d43c4cfd56ef5ffdd222a3f/untitled115.ipynb). Thanks!", "There is one serious TF issue here: the error message is somewhat confusing. If instead of `Const_1:0` it said the actual name of the tensor whose shape is changing (`total_result`) it'd be clear that there's a workaround; at the end of the loop doing `total_result = tf.reduce_sum(total_result)` since you seem to want `total_result` to be a scalar and `mirrored_strategy.reduce` sums across the replicas, not across the dimensions of the tensor.", "Yeah we should improve the error message. I checked that the error message is better for regular for loops / for loops over dataset without using distribution strategy. Only when using the for loop over distributed dataset do we run into the issue where the error message has the name of the loop variable wrong. \r\n\r\nBTW, this is the workaround Alex suggested above in code in case it helps (the tf.reduce_sum should happen within the loop, not outside):\r\n```\r\n@tf.function\r\n    def distribute_train_epoch(dataset):\r\n        total_result = 0.0\r\n        for x in dataset:\r\n            per_replica_result = mirrored_strategy.experimental_run_v2(replica_fn, args=(x,))\r\n            total_result = tf.math.reduce_sum(mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_result, axis=None))\r\n        return total_result\r\n```\r\n", "I tried to run the code on colab  with TF 2.5 and got a different error,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/3dc74f43690066028236059bb2694c12/untitled115.ipynb#scrollTo=yqEiBzYxKRUY) ..Thanks!", "This bug should be fixed in the latest TF version. Note that the repro script needs some corrections:\r\n\r\n```\r\n    ...\r\n        total_result = 0.0\r\n    ...\r\n            per_replica_result = mirrored_strategy.run(replica_fn, args=(x,))\r\n```\r\n\r\nWith those corrections, the error message is cleaner:\r\n\r\n```\r\nValueError: 'total_result' has shape () before the loop, but shape (None, 110, 110, 1) after one iteration. Use tf.autograph.experimental.set_loop_options to set shape invariants.\r\n```\r\n\r\nNote that total_result will still need to be initialized properly, but hopefully the error message is now clearer in what needs to be done.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32099\">No</a>\n"]}, {"number": 32098, "title": "Unable to create TF-TRT model using docker image tensorflow/tensorflow:nightly-gpu-py3", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes, my model is custom.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1 Cuda Drivers\r\n- GPU model and memory: T4, 12GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run the following code from the latest python3 gpu build, nothing happens.  When I run it from the nightly build, I'm able to get a model with 1 TRT Engine node.\r\n\r\n```\r\n\tconverter = trt.TrtGraphConverter(\r\n\t\tinput_saved_model_dir = str(saved_model_dir),\r\n\t\tmax_batch_size = batch_size,\r\n\t\tprecision_mode = precision )\r\n\tconverter.convert()\r\n\tconverter.save(output_saved_model_dir = str(output_saved_model_dir))\r\n```\r\n\r\nIs there an issue with the latest gpu build?  \r\n\r\nThanks,\r\n\r\n**Describe the expected behavior**\r\n\r\nShould be able to create a TRT model using the latest docker build.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@jtressle Sorry for the delay. Could you try using TF1.15.0rc1 that was released recently and let us know if the issue persists. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32098\">No</a>\n"]}, {"number": 32097, "title": "model.save in TF2 forces me to use saved_model.save() but then fails", "body": "Calling model.save(\"path/model\") from within my training loop which is inside strategy scope. I get this error:\r\n\r\n> AssertionError: tf.saved_model.save is not supported inside a traced @tf.function. Move the call to the outer eagerly-executed context.\r\n\r\nBut I'm in TF2, I'm NOT calling it inside a @tf.function. Even running tf.executing_eagerly() returns True when I stop the execution just before the save() call. \r\n\r\nHow do I work around this?\r\nI can't use .h5 saving (it says nested classes not supported)\r\nI also noticed a warning earlier in the script that appeared around the same time as this error, it says:\r\n\r\n> AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.func_graph.FuncGraph'> objects\r\n\r\nprobably related to my attempt trying to manually set_input_shape for my model (following the instructions of an error message that said I cannot otherwise save my model).\r\n\r\nThe code I am using is just the tensorflow 2 tutorial for VAE plus distribute strategy. The model class is about the same used by the tutorial: https://www.tensorflow.org/beta/tutorials/generative/cvae", "comments": ["More details here about other things I've tried: https://stackoverflow.com/questions/57719398/unable-to-save-model-with-tensorflow-2-0-0-beta1\r\n\r\nCurrently I find no way to save my VAE model in TF 2-beta (and RC)", "@kristofgiber ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Here is a minimal working example (that is, it works if you set save_model = False and it reproduces the described error when save_model = True). \r\n\r\nIt may seem unnecessary in this example to use a subclassed model but I have lots of custom functions added to it in my original code that I need it for. \r\n\r\nLooks like the error is reproducible even after removing distribute strategy scope so the strategy scope wasn't related. \r\n\r\nAlso notice that I have no @tf.function at all in this script yet the error complains about it:\r\n`AssertionError: tf.saved_model.save is not supported inside a traced @tf.function. Move the call to the outer eagerly-executed context.`\r\n\r\nCode:\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nsave_model = True\r\n\r\nlearning_rate = 1e-4\r\nBATCH_SIZE = 100\r\nTEST_BATCH_SIZE = 10\r\ncolor_channels = 1\r\nimsize = 28\r\n\r\n(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_images = train_images[:5000, ::]\r\ntest_images = train_images[:1000, ::]\r\ntrain_images = train_images.reshape(-1, imsize, imsize, 1).astype('float32')\r\ntest_images = test_images.reshape(-1, imsize, imsize, 1).astype('float32')\r\ntrain_images /= 255.\r\ntest_images /= 255.\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)\r\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_images).batch(TEST_BATCH_SIZE)\r\n\r\nclass AE(tf.keras.Model):\r\n    def __init__(self):\r\n        super(AE, self).__init__()\r\n        self.network = tf.keras.Sequential([\r\n            tf.keras.layers.InputLayer(input_shape=(imsize, imsize, color_channels)),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(50),\r\n            tf.keras.layers.Dense(imsize**2 * color_channels),\r\n            tf.keras.layers.Reshape(target_shape=(imsize, imsize, color_channels)),\r\n        ])\r\n    def decode(self, input):\r\n        logits = self.network(input)\r\n        return logits\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate)\r\nmodel = AE()\r\n\r\ndef compute_loss(data):\r\n    logits = model.decode(data)\r\n    loss = tf.reduce_mean(tf.losses.mean_squared_error(logits, data))\r\n    return loss\r\n\r\ndef train_step(data):\r\n    with tf.GradientTape() as tape:\r\n        loss = compute_loss(data)\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    return loss, 0\r\n\r\ndef test_step(data):\r\n    loss = compute_loss(data)\r\n    return loss\r\n\r\ninput_shape_set = False\r\nepoch = 0\r\nepochs = 20\r\nfor epoch in range(epochs):\r\n    for train_x in train_dataset:\r\n        train_step(train_x)\r\n    if epoch % 1 == 0:\r\n        loss = 0.0\r\n        num_batches = 0\r\n        for test_x in test_dataset:\r\n            loss += test_step(test_x)\r\n            num_batches += 1\r\n        loss /= num_batches\r\n        print(\"Epoch: {}, Loss: {}\".format(epoch, loss))\r\n\r\n        if save_model:\r\n            print(\"Saving model...\")\r\n            if not input_shape_set:\r\n                # Note: Why set input shape manually and why here:\r\n                # 1. If I do not set input shape manually: ValueError: Model <main.CVAE object at 0x7f1cac2e7c50> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).\r\n                # 2. If I set input shape manually BEFORE the first actual train step, I get: RuntimeError: Attempting to capture an EagerTensor without building a function.\r\n                model._set_inputs(train_dataset.__iter__().next())\r\n                input_shape_set = True\r\n            # Note: Why choose tf format: model.save('MNIST/Models/model.h5') will return NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using save_weights.\r\n            model.save('MNIST/Models/model', save_format='tf')\r\n\r\n```", "I have tried the same minimal reproduction example in tensorflow-gpu 2.0.0-rc0 and the error was more  revealing than what the beta version gave me. The error in RC says: \r\n\r\n> NotImplementedError: When subclassing the `Model` class, you should implement a `call` method.\r\n\r\nThis got me read through https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models where I found examples of how to do subclassing in TF2 in a way that allows saving. I was able to resolve the error and have the model saved by replacing my 'decode' method by 'call' in the above example (although this will be more complicated with my actual code where I had various methods defined for the class). This solved the error both in beta and in rc. Strangely, the training (or the saving) got also much faster in rc.\r\n\r\nSo I have two propositions as a conclusion:\r\n\r\n1. proposition: Fix the error in TF 2.0.0-beta to provide the same clue as TF 2.0.0-RC because the current error message in beta seems fishy (no relation to the context, no hint at the solution, probably a bug?). But RC provides a great deal of more info.\r\n\r\n2. The [VAE tutorial](https://www.tensorflow.org/beta/tutorials/generative/cvae) should also reflect the callable object-oriented concept and should already be written with the call method added and the various methods re-assigned to keras.layer classes, etc, to play nicely with the rest of the keras functional API that will allow saving and many more convenience functions. In fact, it would be better to replace the current CVA tutorial code with the VAE code described in the [custom layers documentation](https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models ). I built my code upon the VAE tutorial assuming that it shows the best possible way to start and will work with the default tf and keras methods such as saving. ", "Please find the replicated gist of colab for [TF version-2.0-rc0](https://colab.research.google.com/gist/oanush/404c8c09df7e69fcf01a00749b4f1da3/untitled0.ipynb).TThanks!", "@kristofgiber The \"NotImplementedError: When subclassing the Model class, you should implement a call method.\" error has been in Keras for a while now, or are you talking about another error message in the first proposition?\r\n\r\n@yashk2810 Can the VAE tutorial be updated to follow the Keras subclassing conventions?", "@kristofgiber Based on the error, I changed two lines in your code. I don't see any issue and the model was saved successfully.\r\n\r\nLine 1: from `def decode(self, input):` to `def call(self, input):` \r\n\r\nLine 2: from `logits = model.decode(data)` to `logits = model(data)`.  \r\n\r\nPlease check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/15ba2ea0afe019d505ae8f9205fe9a5f/untitled0.ipynb). Thanks!\r\n\r\nPlease close this issue if this was resolved for you. Thanks!", "Thanks, yes I stated the same in my 2nd comment (ie, that replacing the decode by call fixes the issue) and I think I only reported this as a documentation / error message issue which was only present with beta anyway. Thank you for the reply.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32097\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32097\">No</a>\n"]}, {"number": 32096, "title": "Backport https://github.com/tensorflow/addons/pull/306 into master", "body": "This is fixing the conditional in `tf.contrib.seq2seq.beam_search_decoder`'s `_check_static_batch_beam_maybe`, as reported in https://github.com/tensorflow/addons/issues/289 .", "comments": ["If you want this in tf1.15, consider also submitting this patch to the r1.15 branch.", "I suppose it's probably best to do that PR from upstream master (to upstream r1.15) instead of from my fork -- so that it really only needs to be reviewed this one time? :)", "Yes, let's get them in master first and then on r1.15.\r\n\r\nNote: the `r1.15` cherry-pick windows closes soon.", "Looks like we missed the window; since contrib is no longer in master.  I suggest just pushing this to the tf 1.15 branch manually.  You'll still only need to have one PR, it just won't be to master...", "The window for 1.15 cherry-picks has also closed, unfortunately.", "Even for bugfixes?\n\nOn Fri, Sep 27, 2019 at 11:05 AM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> The window for 1.15 cherry-picks has also closed, unfortunately.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32096?email_source=notifications&email_token=AANWFG64HPPBJHCWOLZA4KLQLZDNXA5CNFSM4ISHFPR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD7ZVOXA#issuecomment-536041308>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG4XUWO2ZJAWVTQG6A3QLZDNXANCNFSM4ISHFPRQ>\n> .\n>\n", "Unless we do a patch release for security vulnerabilities, no more changes are accepted into the branch as the release jobs for final are almost triggered."]}, {"number": 32095, "title": "To revert commit 7520b333d66a87574be8b910c284c61404e5ca03 (Update Eigen...) ", "body": "The commit 7520b333d66a87574be8b910c284c61404e5ca03 (Update Eigen to https://bitbucket.org/eigen/eigen/commits/7747e7809bd38fca...). breaks the build on Intel branch for architectures with AVX2. The errors shown is .\r\n\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:3747:1: Couldn't build file tensorflow/core/kernels/_objs/cast_op/cast_op_impl_double.o: C++ compilation of rule '//tensorflow/core/kernels:cast_op' failed (Exit 1): gcc failed: error executing command\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:3747:1: Couldn't build file tensorflow/core/kernels/_objs/cast_op/cast_op_impl_uint32.o: C++ compilation of rule \r\nERROR: /....../tensorflow/core/kernels/BUILD:3747:1: C++ compilation of rule '//tensorflow/core/kernels:cast_op' failed (Exit 1)\r\n     :\r\n\r\n", "comments": ["Hi @rmlarsen @penpornk  the upgrade to Eigen is breaking TF when compiled for AVX2 and AVX512. i.e using march=native on skylake or broadwell. ", "Requesting a review from @rmlarsen since he made the upgrade.", "I am investigating this.", "I have reproduced the bug locally. Working on a fix.", "Identified the issue. Fix upcoming.", "Fixed in: https://bitbucket.org/eigen/eigen/commits/a8ac6bd99362c13390d8c9dfeb5f707cc832ac6f working on TF update.", "TF update in testing.", "Fix submitted.", "> Fix submitted.\r\n\r\n@rmlarsen thanks for the quick response.\r\n", "With fix from Rasmus \r\n(https://github.com/tensorflow/tensorflow/commit/9f49088c42a6260df7f270a899f20bc475efad18)\r\n this PR is no longer needed!\r\n"]}, {"number": 32094, "title": "r2.0 cherry-pick request: [Intel MKL] Upgrade MKL-DNN to 0.20.3 ", "body": "This fixes a critical bug that affects convergence of 3D-GANs, U-Net, V-NET, and most other 3D models.\r\n\r\nOriginal PR: #31906 (merged in master).", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32094) for more info**.\n\n<!-- need_author_consent -->", "@agramesh1 Could you please post \"@googlebot I consent.\" to resolve the CLA issue? Thank you!", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32094) for more info**.\n\n<!-- ok -->"]}, {"number": 32093, "title": "Feature request: linspace can accept tensors for start and stop", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, [`tf.linspace`](https://www.tensorflow.org/api_docs/python/tf/linspace) accepts only single numbers (0-D tensor) for start and stop. Numpy recently added support for ndarrays for the `start` and `stop` parameters (since 1.16, see [np.linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) ). It would be very useful (especially when writing vectorized implementations) to have the possibility to call this function for multi dimensional tensors with behavior matching the one in Numpy.\r\n\r\n**Will this change the current api? How?** \r\n\r\nIt will be a backward-compatible change. Only the behaviour of the function will be changed (for certain sets of inputs) and the dimension checking at the start of the function.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nMostly when vectorizing different algorithms. One possible application is when doing a body pose estimation postprocessing pairwise connections per limb, we have all possible combinations of points in a multi dimensional tensor and we want to be able to call `tf.linspace` for the Monte Carlo sampling directly on the tensors to avoid slow iterative solutions.\r\n\r\n**Any Other info.**\r\nI have already implemented a function locally, that behaves in the same way. I have not checked if there would be problems in integration though.", "comments": ["So how should we proceed, do you want me to contribute this or you prefer a tensorflower to work on it?", "@hristo-vrigazov As you have implemented the function locally, You could create a [Pull-Request](https://github.com/tensorflow/tensorflow/pulls) to add the feature. Thanks! ", "Awesome, I will see where is the exact point to integrate the code, will add tests and update here when I am done.", "@hristo-vrigazov When you create a PR, post a link here so that we can close this issue when your PR approves/merges. Thanks!", "Hi @hristo-vrigazov, I am also interested in  this feature. Could we work together?", "@Leslie-Fang Sure, you can send me a message on [Gitter](https://gitter.im) and we can have a discussion there. I will show you what I have done so far and what is left (I think), while I am waiting for the build to finish. I am in Sofia, Bulgaria time, so I think we have quite a lot of overlap (assuming you are in Shanghai).\r\n", "Hello @jvishnuvardhan , I think I am ready with the implementation, I have opened a pull request. Not sure who should I add for reviewers though, since I do have some doubts (it's explained in the pull request).", "@hristo-vrigazov It seems you substitute the cpp implementation with a python implementation.\r\nPlease refer to this snippet of code of cpp implementation\r\nhttps://github.com/tensorflow/tensorflow/blob/ff8147d950d8dbc9866edb9e039280fe603698dc/tensorflow/core/kernels/sequence_ops.cc#L35-L78\r\n ", "@Leslie-Fang \r\nYes, this is the original implementation, but it assumes scalar `start` and `stop`. The idea is to support tensors, so I am internally doing it using `range` and elementwise multiplication - in a similar way that `numpy` is doing it: https://github.com/numpy/numpy/blob/v1.17.0/numpy/core/function_base.py#L37-L179\r\nThe only difference is that `numpy` is operating on the axis 0 and then does transpose, while I am operating directly on the given axis to avoid one additional transpose.", "@hristo-vrigazov Thanks, So numpy also implement this feature with python instead of cpp. ", "@Leslie-Fang Yep, I think so :) The unit tests from before still pass - so it looks like I have not broken anything for the legacy case. Let's see when the reviewers can comment on this ", "@hristo-vrigazov @Leslie-Fang It is possible to pass a 1-d tensor as well in C++. Just need to remove the requirement of `IsLegacyScalar` line. The shape constraint in `tensorflow/core/ops//math_ops.cc` will also need to be removed as well.", "@yongtang Thanks for the information. I guess the question is do we want to, since this operation can already be expressed by other operations? We would have to reuse some of the Python operations in C++ to implement this, which I thought would not be as easy as in Python, where they are already generated and implemented (I may be wrong here). I feel like this version is very easy to maintain, since it closely follows the Numpy one and it should not be much slower in terms of performance compared to the kernel one, but if TF developers want to keep it as a C++ kernel, then maybe I can decline this PR and this can be implemented it in the kernel by someone else.", "@hristo-vrigazov I don't have a preference over python or c++ for this case. I think python would be fine as long as there is not a big hit on performance.", "It's a nice to have feature, what's the status of this feature?", "The pull request is currently being reviewed.", "The pull request was recently into master, so this one is ready! Awesome!", "@hristo-vrigazov \r\n\r\nPlease close this thread if it solves your question. Thanks!\r\n"]}, {"number": 32092, "title": "[ROCm] Testcase updates to get ROCm Community Supported Builds passing again", "body": "Recent changes have broken a couple of tests that get run as part of the ROCm Community Supported Build, leading to the failure of the ROCm Nightly CSB.\r\n\r\nThis PR modifies those testcases (adding no_rocm tag to one, skipping subtests on the other) to make tests pass on the ROCm platform to get the ROCm Nightly CSB passing again (see commit messages for details)\r\n\r\n------------------------------------------------------------------\r\n\r\n\r\n@whchung @chsigg \r\n\r\n", "comments": ["@rthadur I am unable to access the logs for the CI run failures above.\r\n\r\nAll but the \"Ubuntu Sanity\" failure are unlikely to be caused by changes in this PR.  I have been trying to take a peek at the failure log for \"Ubuntu SAnity\" for a couple of hours now with no success (connection times out)."]}, {"number": 32091, "title": "[lite] how to build example label_image for ios (library+binary)?", "body": "[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.md\r\n](url)\r\nthis project provide an example for ubuntu+macos+android(32+64), only lack ios(library+binary).", "comments": ["@laohur label_image is a command line program. You probably want to start from the iOS [classification example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios) first.", "> \r\n> \r\n> @laohur label_image is a command line program. You probably want to start from the iOS [classification example](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/ios) first.\r\n\r\nthat means that you cannot build ios(lib) project from connand line ?\r\n\r\nand can i share one c++ lib source between ios and android?", "@laohur Nope, it's possible to run command line program on iOS, but it's complicated. And surely you can share C++ source code.", "Here is my code:\r\nbuild\r\n> cc_binary(\r\n    name = \"libProphet.so\",\r\n    srcs = [\"Prophet.cc\",],\r\n    copts = tflite_copts(),\r\n    linkopts = tflite_linkopts() + select({\r\n        \"//tensorflow:android\": [\r\n            \"-pie\",  # Android 5.0 and later supports only PIE\r\n            \"-lm\",  # some builtin ops, e.g., tanh, need -lm\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/lite:framework\",\r\n        \"//tensorflow/lite/c:c_api_internal\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite:builtin_op_data\",\r\n        \"//tensorflow/lite/schema:schema_fbs\",\r\n    ],\r\n    linkshared = True,\r\n)\r\nProphet.cc:\r\n> extern \"C\"{\r\nint init(const char* modelBuffer,int bufferSize){\r\n std::unique_ptr<tflite::FlatBufferModel> model=tflite::FlatBufferModel::VerifyAndBuildFromBuffer(_modelBuffer,modelSize);\r\nreturn 0;\r\n}\r\nint predict(float* example, int size ){\r\n    return -1;\r\n}\r\n}\r\n\r\nI just want to get a lib of android & ios.\r\n\" https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/BUILD \" is easy to build android library.  How about test this example?\r\nAnd what is the  elegant way to build ios library ?", "it quiet different between android and xcode. thanks!"]}, {"number": 32089, "title": "TF2.0 RC tf.keras.model.Evaluate has display bug", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. This is the code from https://www.tensorflow.org/beta/tutorials/quickstart/beginner\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab and Ubuntu 18.04\r\n- TensorFlow version (use command below): pip install tensorflow==2.0.0-rc0\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run model.evaluate with verbose=True, I get too many \"=======\" signs. I noticed this on Ubuntu 18.04 when I upgraded from TF2.0-beta to TF2.0.0-rc0. This is a mild annoyance when I am running models with soft-warp enabled on the command line.\r\n\r\n**Describe the expected behavior**\r\n\r\nThere should only be this many equal signs:\r\n[==============================]\r\nNot\r\n[===========================............................................\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n``` from __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n# Install TensorFlow\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=1)\r\n\r\nmodel.evaluate(x_test, y_test)\r\n```\r\n\r\n**Other info / logs**\r\nThis issue may be due to the denominator of the number of evaluated samples completed. It should be 10000/10000, not 10000/1.\r\nInstead of 10000/1 [====\r\n", "comments": ["Was able to reproduce the issue. Please find the github gist [here](https://colab.research.google.com/gist/gowthamkpr/a34035feed7b076e62a8fd51467394a9/untitled116.ipynb).  ", "@gaborchris This was resolved in `tf-nightly`. Please take a look at the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/9cf040a428c59f8efbd1e17f70d51a34/untitled116.ipynb). Thanks!\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if the issue persists for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32089\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32089\">No</a>\n"]}, {"number": 32088, "title": "Toolchain changes for ppc64le and manylinux", "body": "Toolchain changes to use /dt7 paths in the build. While I would have liked to be\r\nable to use the existing cc-compiler-local toolchain, I needed different\r\nbuiltin_include_directories to replace the x86 paths with powerpc64le paths.\r\n\r\nUsing the same tool chain for both CPU and GPU builds. Open to suggestions on\r\na better way to accomplish the same thing.", "comments": ["Generally, the preconfigs need to be generated and not manually adapted. I'm curious how any of this actually works for you, as I don't think we have a toolchain for powerpc on the images these configs are for?\r\nCan you elaborate on what you're trying to do? Thanks!", "Sorry, I should have added linked to the other two pull request that help explain this.\r\n\r\nI'm trying to create manylinux 2014 compliant whl files for ppc64le the same way TensorFlow is creating manylinux 2010 compliant whl files for x86_64.\r\n\r\nI took the manylinux 2010 docker files as a template and made a manylinux 2014 docker files for ppc64le. That is pull request https://github.com/tensorflow/tensorflow/pull/32022. \r\n\r\nThat PR only contains ppc64le specific changes, because the only CUDA 10.0 container NVIDIA provides for ppc64le is based on Ubuntu 18.04, I had to make a few changes to common scripts to support Ubuntu 18.04. I put those changes in a separate PR:  https://github.com/tensorflow/tensorflow/pull/32021/files for ease of review.\r\n\r\nWith those two PRs I have built a manylinux 2014 image for ppc64le with the /dt7 and /dt8 directories. These toolchain changes allow the code to build.\r\n\r\nI'll recreate the error without this change in a follow on post", "If the preference is I don't use the \"preconfig\" directory, I could create a new directory \"third_party/toolchains/ppc64le\" and add this new toolchain there. Would that be better?", "This is a sample error on why I need to add a ppc64le specific builtin_include_directories:\r\n\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/external/highwayhash/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@highwayhash//:sip_hash':\r\nthis rule is missing dependency declarations for the following files included by 'external/highwayhash/highwayhash/sip_hash.cc':\r\n  '/dt7/usr/lib/gcc/powerpc64le-unknown-linux-gnu/7/include/stddef.h'\r\n  '/dt7/usr/lib/gcc/powerpc64le-unknown-linux-gnu/7/include/stdint.h'\r\n  '/dt7/usr/lib/gcc/powerpc64le-unknown-linux-gnu/7/include/stdarg.h'\r\n```\r\n\r\nBut there are other errors I also had to work around:\r\n\r\n```\r\nERROR: /root/tensorflow/third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010/BUILD:45:1: in cc_toolchain_suite rule //third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010:toolchain: cc_toolchain_suite '//third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010:toolchain' does not contain a toolchain for cpu 'ppc'\r\n```\r\n\r\n```\r\nINFO: ToolchainResolution:     Toolchain constraint @bazel_tools//platforms:cpu has value @bazel_tools//platforms:x86_64, which does not match value @bazel_tools//platforms:ppc from the target platform @bazel_tools//platforms:target_platform\r\n```\r\n", "Are you trying to use this with RBE or do you just build inside the image?\r\nIf the latter, why can't you run ./configure.py and get a toolchain generated for you?", "@r4nt ,\r\n\r\nI'm not using remote build, I'm just building inside of the image. When I run ./configure.py, especially in the CPU build case, how does it determine to use /dt7/usr/bin/gcc instead of the default /usr/bin/gcc? Does it use whatever is first in the path?\r\n\r\nI noticed these docker images do nothing with the PATH variable, but my build script could do that. Thank you.", "ok, I tested and found out that build does work if I `export PATH=/dt7/usr/bin:$PATH` without any toolchain changes. Closing this PR."]}, {"number": 32087, "title": "[2.0.0-rc0] Using dataset makes layers.GRU not use cuDNN backend in graph mode", "body": "I'm experiencing some weird behaviour when compiling a graph containing a GRU cuDNN operation and a dataset using `2.0.0-rc0`.\r\n\r\nIn eager execution, both executing the GRU and executing the GRU in a dataset loop performs similarly, doing 10 iterations in ~300ms.\r\n\r\nWhen compiling to graph, using `tf.function` the time jumps to 3s for repeatedly exeuting the GRU, and to 30s when wrapped in a dataset iterator. Even when not using any data provided by the dataset (see example).\r\n\r\nLooking at the trace it seems like the second case (graph + tf.data iterator: 100x slowdown) is due to it no longer using the cuDNN implementation, as the fused_matmul, sigmoid, etc operations are visible in the trace. In the first case (graph, no tf.data iterator: 10x slowdown) I'm not sure what causes the performance regression.\r\n\r\nAttached is the profile traces generated by the script below: \r\n\r\n[gru_cudnn_dataset_graph_profiles.zip](https://github.com/tensorflow/tensorflow/files/3556137/gru_cudnn_dataset_graph_profiles.zip)\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.eager import profiler\r\n\r\nprint(tf.version.VERSION, tf.version.GIT_VERSION)\r\n\r\nds = tf.data.experimental.Counter()\r\n\r\nrnn = tf.keras.layers.GRU(1024)\r\ninp = tf.random.uniform((128,100,512))\r\n\r\n@tf.function\r\ndef f():\r\n    start = tf.timestamp()\r\n    for i in tf.range(10):\r\n        rnn(inp)\r\n        tf.print(tf.timestamp() - start)\r\n        start = tf.timestamp()\r\n\r\nwith profiler.Profiler('test/graph'):\r\n    print('graph')\r\n    f()\r\n\r\ndef f():\r\n    start = tf.timestamp()\r\n    for i in tf.range(10):\r\n        rnn(inp)\r\n        tf.print(tf.timestamp() - start)\r\n        start = tf.timestamp()\r\n\r\nwith profiler.Profiler('test/eager'):\r\n    print('eager')\r\n    f()\r\n\r\ndef f():\r\n    start = tf.timestamp()\r\n    for i in ds.take(10):\r\n        rnn(inp)\r\n        tf.print(tf.timestamp() - start)\r\n        start = tf.timestamp()\r\n\r\nwith profiler.Profiler('test/dataset_eager'):\r\n    print('dataset eager')\r\n    f()\r\n\r\n@tf.function\r\ndef f():\r\n    start = tf.timestamp()\r\n    for i in ds.take(10):\r\n        rnn(inp)\r\n        tf.print(tf.timestamp() - start)\r\n        start = tf.timestamp()\r\n\r\nwith profiler.Profiler('test/dataset_graph'):\r\n    print('dataset graph')\r\n    f()\r\n```\r\n```\r\n2.0.0-rc0 v2.0.0-beta1-5101-gc75bb66\r\ngraph\r\n0.0048191547393798828\r\n0.35352301597595215\r\n0.36730408668518066\r\n0.27707195281982422\r\n0.36916518211364746\r\n0.370150089263916\r\n0.31262898445129395\r\n0.28979992866516113\r\n0.300137996673584\r\n0.33440613746643066\r\neager\r\n0.010959863662719727\r\n0.011440038681030273\r\n0.010359048843383789\r\n0.011663913726806641\r\n0.010751008987426758\r\n0.00952005386352539\r\n0.011648893356323242\r\n0.010584115982055664\r\n0.010535001754760742\r\n0.010977983474731445\r\ndataset eager\r\n0.016921043395996094\r\n0.01053309440612793\r\n0.0098860263824462891\r\n0.010875940322875977\r\n0.0098860263824462891\r\n0.0096909999847412109\r\n0.0097639560699462891\r\n0.00999903678894043\r\n0.0095009803771972656\r\n0.0097169876098632812\r\ndataset graph\r\n0.014286041259765625\r\n3.5808620452880859\r\n3.4670181274414062\r\n3.5227558612823486\r\n3.4991629123687744\r\n3.4808690547943115\r\n3.4846518039703369\r\n3.5459158420562744\r\n3.4938459396362305\r\n3.4940009117126465\r\n```", "comments": ["This is because for some reason @tf.function puts the work on CPU, not GPU.  If you change the last function to the following, you will observe a similar performance.\r\n\r\n```python\r\n@tf.function\r\ndef f():\r\n    start = tf.timestamp()\r\n    for i in ds.take(10):\r\n        with tf.device('/device:GPU:0'):\r\n            rnn(inp)\r\n        tf.print(tf.timestamp() - start)\r\n        start = tf.timestamp()\r\n```\r\n\r\nThis issue will be tracked by https://github.com/tensorflow/tensorflow/issues/32138 "]}, {"number": 32086, "title": "Ragged tensor with start and end indices", "body": "**System information**\r\n- TensorFlow version (you are using):\r\nnightly\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRagged tensors are currently defined by specifying the boundaries between subtensors in the ragged dimension. If the elements at the end/beginning of consecutive subtensors in the ragged dimension are shared, defining both the start and end of each such subtensor could reduce the total size of the ragged tensor dramatically.\r\n\r\nTo make the implementation more straightforward, both starts and ends could be required to increase monotonically .\r\n\r\n**Will this change the current api? How?**\r\nNo. This would only require adding a construction function to the API.\r\n\r\n**Who will benefit with this feature?**\r\nThis feature could be used to compress overlapping tensors with overlapping sequences by a large margin. This is useful for training data containing words, tokens, transactions etc.\r\nFurthermore, this would allow slicing the underlying data tensor while using gaps, potentially reducing the number of necessary copies.", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32084, "title": "Fix TestApp compile errors", "body": "## Problem\r\nWe cannot build `TestApp` now because of compile errors.\r\n\r\n## Screenshots\r\n\r\n<img width=\"545\" alt=\"Screen Shot 2019-08-29 at 22 50 41\" src=\"https://user-images.githubusercontent.com/5785300/63946590-5162e000-cab0-11e9-9978-1efe95bea734.png\">\r\n\r\n<img width=\"545\" alt=\"Screen Shot 2019-08-29 at 22 50 55\" src=\"https://user-images.githubusercontent.com/5785300/63946592-5162e000-cab0-11e9-8c1d-fce12f8dee75.png\">", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32084) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32084) for more info**.\n\n<!-- ok -->", "@miaout17 Can you please take a look on this PR? Thanks!", "@yyoon can you review?", "In fact, I just tried building the `TestApp` from master branch, and it's actually building fine for me.\r\n@ken0nek Can you explain more about the exact steps you took, and what compile errors you saw?\r\n\r\nI used the typical CocoaPods flow, by running `pod install` and then opening the `TestApp.xcworkspace` (NOTE: not the `.xcodeproj` file), and building the `TestApp` scheme from Xcode.", "@ken0nek Can you please check @yyoon comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@ken0nek Any update on this PR? Please. Thanks!", "@gbaned Sorry for the late response. I checked this issue on master and there was no error any more. I will close this issue. Thank you for the review!"]}, {"number": 32083, "title": "fixed possiblity of integer flow", "body": "The current code can lead to integer overflow and weird results\r\nAs also discussed in below issue on some points it is giving some unexpected results.\r\nThis PR is to fix this problem\r\n#32045 :issue number\r\n", "comments": ["@mihaimaruseac i think we can check overflow and give revelant warning , please have an review", "~~All of these values are signed integers with a well defined behavior on overflow. As such, caller code should check and handle it, instead of here.~~ (this is wrong, __signed__ overflow is UB)\r\n\r\nClosing PR due to all mentioned issues."]}, {"number": 32081, "title": "Distributed training not working properly: Waiting for model to be ready. Ready_for_local_init_op: Variables not initialized: global_step", "body": "Hello,\r\nI am using tfhub elmo embedding for NER model training.\r\nTraining works fine for a single machine.\r\n\r\nWhen I am training to do training on GCP ML engine with 1 master node, 2 parameter server, 3 worker node.\r\n\r\nTraining started on master node,\r\nBut on all three worker node, I am getting this logs continuously for 1 hours,\r\nlooks like training is not getting started on worker node.\r\n\r\nI am using these versions which currently supported by Google cloud ML Engine:\r\nPython: 3.5\r\nTensorflow: 1.13.1\r\n\r\n**ML engine resource usage:**\r\n\r\n```\r\ntrainingInput:\r\n    scaleTier: CUSTOM\r\n    masterType: large_model\r\n    workerType: standard\r\n    parameterServerType: standard\r\n    workerCount: 3\r\n    parameterServerCount: 2\r\n```\r\n\r\n`Waiting for model to be ready. Ready_for_local_init_op: Variables not initialized: global_step, module/bilm/char_embed, module/bilm/CNN/W_cnn_0, module/bilm/CNN/b_cnn_0, module/bilm/CNN/W_cnn_1, module/bilm/CNN/b_cnn_1, module/bilm/CNN/W_cnn_2, module/bilm/CNN/b_cnn_2, module/bilm/CNN/W_cnn_3, module/bilm/CNN/b_cnn_3, module/bilm/CNN/W_cnn_4, module/bilm/CNN/b_cnn_4, module/bilm/CNN/W_cnn_5, module/bilm/CNN/b_cnn_5, module/bilm/CNN/W_cnn_6, module/bilm/CNN/b_cnn_6, module/bilm/CNN_high_0/W_carry, module/bilm/CNN_high_0/b_carry, module/bilm/CNN_high_0/W_transform, module/bilm/CNN_high_0/b_transform, module/bilm/CNN_high_1/W_carry, module/bilm/CNN_high_1/b_carry, module/bilm/CNN_high_1/W_transform, module/bilm/CNN_high_1/b_transform, module/bilm/CNN_proj/W_proj, module/bilm/CNN_proj/b_proj, module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel, module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias, module/bilm/RNN_0/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel, module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel, module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias, module/bilm/RNN_0/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/bias, module/bilm/RNN_1/RNN/MultiRNNCell/Cell0/rnn/lstm_cell/projection/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/kernel, module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/bias, module/bilm/RNN_1/RNN/MultiRNNCell/Cell1/rnn/lstm_cell/projection/kernel, module/aggregation/weights, module/aggregation/scaling, lstm_fused_cell/kernel, lstm_fused_cell/bias, lstm_fused_cell_1/kernel, lstm_fused_cell_1/bias, dense/kernel, dense/bias, crf, beta1_power, beta2_power, lstm_fused_cell/kernel/Adam, lstm_fused_cell/kernel/Adam_1, lstm_fused_cell/bias/Adam, lstm_fused_cell/bias/Adam_1, lstm_fused_cell_1/kernel/Adam, lstm_fused_cell_1/kernel/Adam_1, lstm_fused_cell_1/bias/Adam, lstm_fused_cell_1/bias/Adam_1, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, crf/Adam, crf/Adam_1, ready: None`", "comments": ["Request you to provide standalone code to reproduce the issue. Thanks!", "Here is the code:\r\n\r\n**dataset function:**\r\n\r\n`def dataset_fn(words, tags, params=None, shuffle_and_repeat=False):\r\n    params = params if params is not None else {}\r\n    shapes = (([None], ()), [None])\r\n    types = ((tf.string, tf.int32), tf.string)\r\n    defaults = (('<pad>', 0), 'O')\r\n\r\n    dataset = tf.data.Dataset.from_generator(\r\n        functools.partial(generator_fn, words, tags),\r\n        output_shapes=shapes, output_types=types)\r\n\r\n    if shuffle_and_repeat:\r\n        dataset = dataset.shuffle(params['buffer']).repeat(params['epochs'])\r\n\r\n    dataset = (dataset\r\n               .padded_batch(params.get('batch_size', 20), shapes, defaults)\r\n               .prefetch(1))\r\n    return dataset`\r\n\r\n\r\n`def model_fn(features, labels, mode, params):\r\n    # Read vocabs and inputs\r\n    dropout = params['dropout']\r\n\r\n    # Input tensor.\r\n    if (mode == tf.estimator.ModeKeys.PREDICT):\r\n        words, nwords = features['words'], features['nwords']\r\n    else:\r\n        words, nwords = features\r\n\r\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n    \r\n\r\n    with tf.gfile.GFile(params['vocab_tags_path'], 'r') as f:\r\n        indices = [idx for idx, tag in enumerate(f) if tag.strip() != 'O']\r\n        num_tags = len(indices) + 1\r\n\r\n    embeddings = ElmoEmbedding(words)\r\n\r\n    embeddings = tf.layers.dropout(embeddings, rate=dropout, training=training)\r\n    # print(\"---------embeddings-------------\")\r\n    # print(embeddings, (mode == tf.estimator.ModeKeys.PREDICT))\r\n    # print(features)\r\n    # LSTM\r\n    t = tf.transpose(embeddings, perm=[1, 0, 2])\r\n    # print(t.shape)\r\n    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\r\n    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(params['lstm_size'])\r\n    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\r\n    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords)\r\n    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords)\r\n    output = tf.concat([output_fw, output_bw], axis=-1)\r\n    output = tf.transpose(output, perm=[1, 0, 2])\r\n    output = tf.layers.dropout(output, rate=dropout, training=training)\r\n\r\n    # CRF\r\n    logits = tf.layers.dense(output, num_tags)\r\n    crf_params = tf.get_variable(\"crf\", [num_tags, num_tags], dtype=tf.float32)\r\n    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, nwords)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        # Predictions\r\n        reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\r\n            params['vocab_tags_path'])\r\n        pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\r\n        predictions = {\r\n            # 'pred_ids': pred_ids,\r\n            'tags': pred_strings,\r\n            'score': _\r\n        }\r\n\r\n        predictions[\"key\"] = tf.identity(features[\"key\"])\r\n\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n    else:\r\n        # Loss\r\n        vocab_tags = tf.contrib.lookup.index_table_from_file(params['vocab_tags_path'])\r\n        tags = vocab_tags.lookup(labels)\r\n        log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\r\n            logits, tags, nwords, crf_params)\r\n        loss = tf.reduce_mean(-log_likelihood)\r\n\r\n        # Metrics\r\n        weights = tf.sequence_mask(nwords)\r\n\r\n        accuracy_op = tf.metrics.accuracy(tags, pred_ids, weights)\r\n        precision_op = precision(tags, pred_ids, num_tags, indices, weights)\r\n        recall_op = recall(tags, pred_ids, num_tags, indices, weights)\r\n        f1_op = f1(tags, pred_ids, num_tags, indices, weights)\r\n\r\n        metrics = {\r\n            \"eval_accuracy\": accuracy_op,\r\n            \"eval_precision\": precision_op,\r\n            \"eval_recall\": recall_op,\r\n            \"eval_f1\": f1_op\r\n        }\r\n\r\n        # for metric_name, op in metrics.items():\r\n        #     tf.summary.scalar(metric_name, op[1])\r\n\r\n        if mode == tf.estimator.ModeKeys.EVAL:\r\n            return tf.estimator.EstimatorSpec(\r\n                mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n        elif mode == tf.estimator.ModeKeys.TRAIN:\r\n\r\n            train_tensors_log = {\r\n                'accuracy': accuracy_op[1],\r\n                'loss': loss,\r\n                'global_step': tf.train.get_or_create_global_step()\r\n            }\r\n\r\n            logging_hook = tf.train.LoggingTensorHook(tensors=train_tensors_log, every_n_iter=5)\r\n\r\n            training_accuracy = tf.summary.scalar(\"training_accuracy\", accuracy_op[1])\r\n            training_loss = tf.summary.scalar(\"training_loss\", loss)\r\n            training_precision = tf.summary.scalar(\"training_precision\", precision_op[1])\r\n            training_recall = tf.summary.scalar(\"training_recall\", recall_op[1])\r\n            training_f1 = tf.summary.scalar(\"training_f1\", f1_op[1])\r\n\r\n            summary_hook = tf.train.SummarySaverHook(save_steps=2, output_dir=params[\"checkpoint_path\"],\r\n                                                     summary_op=tf.summary.merge(\r\n                                                         [training_accuracy, training_loss, training_precision,\r\n                                                          training_recall, training_f1]))\r\n\r\n            train_op = tf.train.AdamOptimizer().minimize(\r\n                loss, global_step=tf.train.get_or_create_global_step())\r\n            return tf.estimator.EstimatorSpec(\r\n                mode, loss=loss, train_op=train_op, training_hooks=[logging_hook, summary_hook])`\r\n\r\n`strategy = tf.contrib.distribute.ParameterServerStrategy()\r\n\r\n    def train_inpf(input_context=None):\r\n        train_dataset = dataset_fn(params[\"train_words_path\"], params[\"train_tags_path\"],\r\n                                   params, shuffle_and_repeat=True)\r\n        # if input_context:\r\n        #     train_dataset = train_dataset.shard(input_context.num_input_pipelines,\r\n        #                                         input_context.input_pipeline_id)\r\n        return train_dataset\r\n\r\n    def eval_inpf(input_context=None):\r\n        eval_dataset = dataset_fn(params[\"test_words_path\"], params[\"test_tags_path\"])\r\n        # if input_context:\r\n        #     eval_dataset = eval_dataset.shard(input_context.num_input_pipelines,\r\n        #                                       input_context.input_pipeline_id)\r\n        return eval_dataset\r\n\r\n    # with strategy.scope():\r\n    #     iterator = strategy.make_input_fn_iterator(\r\n    #         train_inpf)\r\n    #     replica_results = strategy.extended.call_for_each_replica(\r\n    #         replica_fn, iterator.get_next())\r\n\r\n    cfg = tf.estimator.RunConfig(save_checkpoints_steps=5, train_distribute=strategy)  # train_distribute=strategy, eval_distribute=strategy\r\n\r\n    estimator = tf.estimator.Estimator(model_fn, params[\"checkpoint_path\"], cfg, params)\r\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\r\n\r\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\r\n        estimator, 'f1', 500, min_steps=8000, run_every_secs=120)\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_inpf, max_steps=100000)  # hooks=[hook], max_steps=2000\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_inpf, throttle_secs=1200)  # , throttle_secs=120\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)`", "ML Engine currently only supports estimator distributed training but not through tf.distribute. \r\nThe team is working on enabling that support soon. In the meantime you can add  this snippet to the beginning of your program:\r\n  import json, os\r\n  tf_config_json = json.loads(os.environ.get('TF_CONFIG', '{}'))\r\n  if tf_config_json and tf_config_json['cluster'].get('master', None):\r\n    tf_config_json['cluster']['chief'] = tf_config_json['cluster'].pop('master')\r\n    if tf_config_json['task']['type'] == 'master':\r\n      tf_config_json['task']['type'] = 'chief'\r\n    os.environ['TF_CONFIG'] = json.dumps(tf_config_json)", "@sshrdp meet same problem. What 's the meaning of \"estimator distributed training but not through tf.distribute\" ? ", "If you don't specify any distribution strategy and just run an estimator with parameter servers, you will not run into this issue. ", "If you're providing your own container, you need to use set your job's trainingInput.useChiefInTfConfig to true.\r\nSee this for more details: https://cloud.google.com/ai-platform/training/docs/distributed-training-details#chief-versus-master", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32081\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32081\">No</a>\n"]}, {"number": 32080, "title": "[r2.0:Cherrypick] Remove tensor input shape from function signature.", "body": "This commit solves a performance issue with `tf.function` and variable input shapes. See #29075.", "comments": ["Will this get in for 2.0.0rc1?", "guillaumekln@: Yes I believe so. I have pinged the branch owner to make sure of it.", "Please submit a cherry-pick to the 2.0 branch and add me as reviewer. Then\nit'll get in :-)\n\nOn Thu, Sep 5, 2019 at 9:40 AM Guillaume Klein <notifications@github.com>\nwrote:\n\n> Will this get in for 2.0.0rc1?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32080?email_source=notifications&email_token=AAABHROCAU2D2J2NYD364CDQIEY7TA5CNFSM4ISACJU2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD57YOIA#issuecomment-528451360>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLSZ2UTRBPH3MDGU5TQIEY7TANCNFSM4ISACJUQ>\n> .\n>\n\n\n-- \n - Alex\n", "This PR is already targeting the `r2.0` branch. Is there something else to do?", "Merging as all tests succeeded", "Then it will be in the next RC.\n\nOn Thu, Sep 5, 2019 at 9:50 AM Guillaume Klein <notifications@github.com>\nwrote:\n\n> This PR is already targeting the r2.0 branch. Is there something else to\n> do?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/32080?email_source=notifications&email_token=AAABHRJWTMYFSYOLOH3DVB3QIE2ENA5CNFSM4ISACJU2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD57ZPNI#issuecomment-528455605>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRIVE7SLGV4OARFQ4CDQIE2ENANCNFSM4ISACJUQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 32079, "title": "why the strideconv don't have same value with standconv?", "body": "Examples:\r\nconv1 = slim.conv2d(inputs, 16, [5, 5], stride=1, padding='SAME',scope='conv1')\r\nconv2 = slim.conv2d(inputs, 16, [5,5], stride=2,padding='SAME', scope='conv2')\r\n\r\nthe size of inputs is [1, 320, 320, 8]\r\nthe weight and bias of the conv1 and conv2 are the same.\r\nbut the outputs of the conv1 and conv2 are every different. \r\ncan anyone give me a explaination?\r\n\r\nI think the result may be like:\r\nconv1_out = [1 2 3 4 5 6 7 8 9 10\r\n                      11 12 13 14 15 16 17 18 19 20\r\n                      21 22 23 24 25 26 27 28 29 30\r\n                       31 32 33 34 35 36 37 38 39 40]\r\nthe conv2_out should be:\r\n[1 3 5 7 9\r\n21 23 25 27 29]\r\n\r\nactually, the result are very different.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["two of the value like this:\r\n![Uploading tf_stride1.jpg\u2026]()\r\n![tf_stride2](https://user-images.githubusercontent.com/9530892/63929247-ac321280-ca83-11e9-9550-a2c5fcb5f364.jpg)\r\n", "![tf_stride1](https://user-images.githubusercontent.com/9530892/63929350-da175700-ca83-11e9-9e62-b55016bc5330.jpg)\r\n", "@GuideWsp, Please provide tensorflow version and complete code snippet to reproduce the issue. Thanks!", "I test it with a deblur model, i had meet the same problem in SRN code.\r\nhttps://github.com/jiangsutx/SRN-Deblur/blob/master/models/model.py#L96\r\nfirstly, I save the results conv1_4 and conv2_1 with stride=2;\r\nthen, I save the results conv1_4 and conv2_1 with stride=1.\r\n\r\nfinaly, I found the same problem as my deblur model.\r\n\r\nif you can't reproduce it, you can concat me with email: wagnshupeng728@126.com, then i will provide the deblur model.\r\n", "\u4ee5SRN\u6a21\u578b\u4e3a\u4f8b\uff0c\u6e90\u7801\u89c1\uff1ahttps://github.com/jiangsutx/SRN-Deblur/\r\n\r\n\u6ce8\uff1atensorflow\u7248\u672c\u4e3a1.17.0\uff0c\r\n\r\n\u66fe\u5c1d\u8bd5\u5c06\u5176model\u4e2d\u7684conv1_4\u4e0econv2_1\u5bfc\u51fa\u3002\u8be5\u6a21\u578b\u4e2d\u9ed8\u8ba4conv2_1\u7684stride=2\uff0c\u3002\r\n\r\n\u624b\u52a8\u5c06\u5176\u6539\u4e3astride=1\uff0c\u4f46\u6a21\u578b\u52a0\u8f7d\u7b49\u5747\u4e0d\u53d8\u4ee5\u4fdd\u8bc1conv2_1\u7684\u53c2\u6570\u662f\u5b8c\u5168\u4e00\u81f4\u7684\u3002\r\n\r\n\u5c06\u4e24\u79cd\u65b9\u5f0f\u7684\u7ed3\u679c\u4fdd\u5b58\u4e0b\u6765\u53d1\u73b0\uff1a\r\n\r\nconv1_4\u7ed3\u679c\u662f\u76f8\u540c\u7684\uff0c\u800c\u4e24\u6b21conv2_1\u7684\u7ed3\u679c\u5b8c\u5168\u4e0d\u540c\u3002", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32079\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32079\">No</a>\n"]}, {"number": 32078, "title": "feature_column_v2", "body": "HI Support Team \r\n\r\nThere is a bug. \r\n\r\n\r\nFile \"/home/anaconda3/envs/zml/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 86, in <module>\r\n    from tensorflow_estimator.python.estimator.tpu import _tpu_estimator_embedding\r\n  File \"/home/anaconda3/envs/zml/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/tpu/_tpu_estimator_embedding.py\", line 33, in <module>\r\n    from tensorflow.python.tpu import feature_column_v2 as tpu_fc_v2\r\nImportError: cannot import name 'feature_column_v2'\r\n\r\nwhile in the folder only feature_column is available. \r\n\r\nThe installed version is 1.14 via PIP. \r\n\r\nThe file is available in your git, but doesnot gets installed via PIP. \r\n\r\nPlease let me know when the error / issue is resolved. \r\n\r\nGreetings. \r\n\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32078\">No</a>\n"]}, {"number": 32077, "title": "The batch_size argument must not be specified when using dataset as an input & Batch size: 32 is not divisible by num_workers: 3", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version :3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: TITAN Xp, 3 x 12G\r\n\r\n---\r\n\r\ni'm using `MirroredStrategy()` to train on 3 GPUs, and use a tf `Dataset` as the model inputs. when calling `model.fit()`, if i specify `batch_size`, it shows `The batch_size argument must not be specified when using dataset as an input.` and if i don't specify `batch_size`, it shows `Batch size: 32 is not divisible by num_workers: 3 [Op:ExperimentalRebatchDataset]`.\r\n\r\ni know that by calling `fit()`, `batch_size` is 32 by default, but when using dataset, i did call `batch()` and i can't change `batch_size` in `fit()`.\r\n\r\ni thought i could use 1, 2, 4, etc. GPUs that can devide 32, but why 3 GPUs is not working.\r\n\r\n```python\r\nstrategy = MirroredStrategy()\r\n    with strategy.scope():\r\n        model = LSTMModel(embs=que_embs, num_ans=len(ans_vocab.word_index) + 1)\r\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n        model(tf.zeros(shape=(config.batch_size, config.pad_max_len), dtype=tf.int32))\r\n        model.summary()\r\n\r\ndataset = Dataset.from_tensor_slices((inputs, labels))\r\ndataset = dataset.shuffle(buffer_size=4).repeat(3).batch(8)\r\nhistory = model.fit(dataset, epochs=config.num_epochs,\r\n                               # batch_size=config.batch_size,\r\n                               validation_data=(dataset_val['questions'], dataset_val['answers']),\r\n                               callbacks=create_callbacks())\r\n```\r\n", "comments": ["@Xie-Fangyuan,\r\nIn order to expedite the trouble-shooting process, please provide a minimal complete code to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32077\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32077\">No</a>\n", "This problem is not fixed.\r\n\r\n[mnist.ipynb.txt](https://github.com/tensorflow/tensorflow/files/3638873/mnist.ipynb.txt)\r\n\r\n(Tensorflow 2.0)", "@jtiscione,\r\nCan you please post a new issue by providing the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new/choose)?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!\r\n\r\n", "you need to put `batch_size=None`"]}, {"number": 32076, "title": "TF2.0rc0: experimental_distribute_datasets_from_function() does not respect end of iteration", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.0.0-beta1-5101-gc75bb66 2.0.0-rc0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 CUDA, 7.6.3.30-1+cuda10.0 cuDNN\r\n- GPU model and memory: 3x TitanX Pascal 12Gb\r\n\r\n**Describe the current behavior**\r\nNote: code worked fine with Tensorflow 2.0 Beta1. Changes introduced in RC0 caused this.\r\n\r\nI've created a function, that reads large file and return batches of needed size, considering the number of replicas. At the end of the file it return None (end of iteration). This function is then submitted to `tf.data.Dataset.from_generator()`:\r\n```\r\ndef dataset_fn(input_context):\r\n  bs = input_context.get_per_replica_batch_size(self._config.train.batch_size)\r\n  ds = tf.data.Dataset.from_generator(lambda: parallel_map_and_batch(data_file, bs))\r\n  ds = ds.shard(input_context.num_input_pipelines, input_context.input_pipeline_id)\r\n  return ds\r\n```\r\n\r\n`tf.distribute.MirroredStrategy.experimental_distribute_datasets_from_function(dataset_fn)` is used to get the dataset and create iterator from it.\r\n\r\n`tf.distribute.MirroredStrategy.experimental_run_v2(..., next(iterator))` is used to call a model on each replica.\r\n\r\nI've printed a batch size for each call of the model and I see something like:\r\n```\r\nReplica0: batch size 128\r\nReplica1: batch size 128\r\nReplica2: batch size 128\r\n...\r\nReplica0: batch size 107  (this was a last batch)\r\nReplica1: batch size 0  (generator returned None)\r\nReplica2: batch size 0\r\n```\r\n\r\nWhich causes `Check failed: work_element_count > 0 (0 vs. 0)` inside the model call because of the empty batch.\r\n\r\n**Describe the expected behavior**\r\n\r\nReplicas shouldn't peek empty batches from dried out iterator. Beta1 used to throw an exception in this case.\r\n\r\nP.S. Or `experimental_distribute_datasets_from_function` expects generator to always return number of batches dividable by number of replicas?", "comments": ["Fix for this problem is pretty simple (a bit of work, of course).\r\n\r\nLet your generator know about the global batch size and the number of replicas, then force it to output `num_replicas` number of batches on yield, like:\r\n```\r\nglobal_batch = read_batch(global_batch_size)\r\nfor replica in range(num_replicas):\r\n  replica_batch = global_batch[replica_start:replica_end]\r\n  yield replica_batch\r\n```", "And even with this fix I'm still sometimes getting a error:\r\n```\r\ntensorflow.python.framework.errors_impl.CancelledError: 3 root error(s) found.\r\n  (0) Cancelled:  Operation was cancelled\r\n         [[{{node cond_5/else/_71/IteratorGetNext}}]]\r\n  (1) Out of range:  End of sequence\r\n         [[{{node cond_3/else/_37/IteratorGetNext}}]]\r\n         [[Identity_4/_144]]\r\n  (2) Out of range:  End of sequence\r\n         [[{{node cond_3/else/_37/IteratorGetNext}}]]\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_mgpu_run_eval_step_graph_6335388]\r\n```\r\n\r\nI'm not really sure `next(iterator)` is supposed to work under `@tf.function` as I've read somewhere.", "@tridemax ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!\r\n", "Cannot reproduce it on a small sample - StopIteration is properly thrown. I'll try to figure out what causes this in my large model."]}, {"number": 32075, "title": "[TF2.0rc]TFRecord file size bigger than original csv file consisting text data.", "body": "I am trying transform a csv file consisting two columns: short_description, label. Description consists sentences from length 1 to 4 and label is just an integer number. The csv file has a size of 740 MBs however on converting it to TFRecord the size of the new file increased to 1.2GB, which I think is not correct since TFRecord files are supposed to have lesser size.\r\nFor `short_description` (column 1), I am using BytesList and for `label` (coulmn2) I am using Int64List as was shown here in this [tutorial](https://www.tensorflow.org/beta/tutorials/load_data/tf_records). Just to be clear, the data read from the new TFRecord file is fine i.e. no problem. It is just the size of the file. Now I do not understand what the problem is. Please help. Thank you!", "comments": ["Request you to provide standalone code and sample data to reproduce the issue. Thanks!", "Thank you @ravikyram . Here is the [link](https://drive.google.com/file/d/15bd5zrp-2wSsmHmi9u2SdYJITQYQ2eXv/view?usp=sharing) to the small part of big csv file (428.9KB). Following is the script used for transforming it into TFRecord, giving the tfrecord file of size 710KB.\r\n````\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\ndataset = tf.data.experimental.make_csv_dataset('random.csv', batch_size = 1,select_columns = ['short_description','Label'], label_name = 'Label', num_epochs = 1 ,shuffle = True)# (many examples, many labels) pairs\r\n\r\n# creating tfrecord\r\ndef serialize_example(string_input, integer_input):# Serializing every element\r\n\tstring_input = string_input.numpy()\r\n\tstring_list = tf.train.BytesList(value=string_input)\r\n\tfeature_strings = tf.train.Feature(bytes_list=string_list)\r\n\tint_list = tf.train.Int64List(value = [integer_input])\r\n\tfeature_int = tf.train.Feature(int64_list = int_list)\r\n\r\n\t#make Feature value as DICT\r\n\telements_dict = {\r\n\t  'strings': feature_strings,\r\n\t  'Ints': feature_int\r\n\t}\r\n\tfeatures_dict = tf.train.Features(feature=elements_dict)\r\n\texample = tf.train.Example(features=features_dict)\r\n\treturn example.SerializeToString()\r\n\r\ndef tf_serialize_example(f0, f1):\r\n\twith tf.device('CPU:0'):\r\n\t\ttf_string = tf.py_function(serialize_example, inp = [f0['short_description'], tf.reshape(f1, [])], Tout = tf.string)\r\n\treturn tf_string\r\nserialized_dataset = dataset.map(tf_serialize_example)# to serialize every element in the dataset\r\n\r\nfilename = 'random1.tfrecord'# name of the tfrecord file\r\nwriter = tf.data.experimental.TFRecordWriter(filename)# writing the tfrecord file\r\nwriter.write(serialized_dataset)\r\n````\r\nI used Python 2 and TF2.0rc.", "I have tried on Jupyter notebook with TF2.0rc0 and was able to reproduce the issue.Thanks!", "@ravikyram, could you try reproducing the behavior with TF 1.X, I suspect that this is not specific to TF 2.0.\r\n\r\n@rishabhsahrawat what leads you to believe that an uncompressed TFRecord file should be smaller than the equivalent csv file? Unlike csv, TFRecord format duplicates the feature key in each record, so I would expect the uncompressed TFRecord file to actually be bigger.", "Hi, according to the docs I got the understanding that TFRecord usually\ntakes less space. If that is not true then it must be written in the docs.\nSo, all the cases when it will take more space and less space.\nAlso, as you are saying that the resulted TFRecord is uncompressed file\nthen how does one get the compressed file with smaller size and can that be\nread or does it need to be first uncompressed and read? Thank you for your\nvaluable time.\n\n\nOn Wednesday, September 4, 2019, Jiri Simsa <notifications@github.com>\nwrote:\n\n> @ravikyram <https://github.com/ravikyram>, could you try reproducing the\n> behavior with TF 1.X, I suspect that this is not specific to TF 2.0.\n>\n> @rishabhsahrawat <https://github.com/rishabhsahrawat> what leads you to\n> believe that an uncompressed TFRecord file should be smaller than the\n> equivalent csv file? Unlike csv, TFRecord format duplicates the feature key\n> in each record, so I would expect the uncompressed TFRecord file to\n> actually be bigger.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32075?email_source=notifications&email_token=ADZMMPHL4FHVUXBOIYA5WPTQH7SV7A5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD54HXVQ#issuecomment-527989718>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADZMMPFXBNI3TLBHFAPNC23QH7SV7ANCNFSM4IR62YAA>\n> .\n>\n\n\n-- \nSent from Gmail iPhone\n", "Could you point me to the documentation that suggests that TFRecord should take less space then a CSV file?\r\n\r\nTFRecordWriter supports the same compression types as [TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) --  no compression, `GZIP`, or `ZLIB` -- specified through the `compression_type` argument.", "I read about it here (\nhttps://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564),\nsecond paragraph. Although I could not find something similar on TF docs.\nAs you mentioned earlier the size is expected to be bigger than the csv, so\ndo you think it is still beneficial to use TFRecords data for very large\nfiles?  Thank you!\n\nOn Thursday, September 5, 2019, Jiri Simsa <notifications@github.com> wrote:\n\n> Could you point me to the documentation that suggests that TFRecord should\n> take less space then a CSV file?\n>\n> TFRecordWriter supports the same compression types as TFRecordDataset\n> <https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset> --\n> no compression, GZIP, or ZLIB -- specified through the compression_type\n> argument.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32075?email_source=notifications&email_token=ADZMMPC3ZERQWBJB5EJ6R4DQIAZ6JA5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD55G5UI#issuecomment-528117457>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADZMMPHKBWWZZO3GBGZFFJDQIAZ6JANCNFSM4IR62YAA>\n> .\n>\n\n\n-- \nSent from Gmail iPhone\n", "If you compress the file, I would expect it to be smaller. Additional benefits of using TFRecord for large dataset is that you can process the data in a streaming fashion (e.g. with tf.data) as opposed to loading the entire dataset into memory which it might not fit.", "Okay, I understand. So, if I store the compressed TFRecord file then can I\nread it using tf.data.tfrecord as it is or do I have to uncompress it?\n>Processing the data in streaming fashion\nI think that will be similar to when loading a csv file (e.g. with\ntf.make_csv_dataset function) and processing it with tf.data, except maybe\nin terms of reading speed of the data. Please correct me if I wrong.\nBefore trying loading data from TFRECORD, I used tf.data to load my csv\nfile. If I want to use .shuffle function on this then it fills up the\nshuffle buffer before every epoch even though I have set\n\u2018shuffle_before_iteration=False\u2019 and this buffering also eats out my\ndevice\u2019s RAM which eventually run out of memory. I have raised an issue\nsince months but still waiting for the solution. Thank you for your\nvaluable time and inputs.\n\nOn Thursday, September 5, 2019, Jiri Simsa <notifications@github.com> wrote:\n\n> If you compress the file, I would expect it to be smaller. Additional\n> benefits of using TFRecord for large dataset is that you can process the\n> data in a streaming fashion (e.g. with tf.data) as opposed to loading the\n> entire dataset into memory which it might not fit.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32075?email_source=notifications&email_token=ADZMMPEUWOJ67PYIF5N3WQTQIFVFXA5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6AVRFQ#issuecomment-528570518>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADZMMPFTIW4TRJWFTWVQYHLQIFVFXANCNFSM4IR62YAA>\n> .\n>\n\n\n-- \nSent from Gmail iPhone\n", "You simply need to specify the `compression_type` argument of `TFRecordDataset`.\r\n\r\nCSV format is amenable to streaming processing but other formats might not be.\r\n\r\nAs for your `shuffle_before_iteration` issue, I am not sure which API are referring. Neither [CsvDataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset) nor [make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset) have that argument. Did you file an issue for this? If so, can you please share a link to it here? Thanks.", "Sorry for the confusion, I meant the \u2018tf.data.Dataset.shuffle\u2019  has a\nparameter \u2018reshuffle_each_iteration\u2019 even after setting it to \u2018False\u2019 it\nstill shuffles the dataset before every epoch and fills up the buffer which\nat some point leads to OOM error. This error sometimes also comes during\nthe first time it fills up the buffer or before the beginning of second\nepoch. Here is the issue that I have raised.\nhttps://github.com/tensorflow/tensorflow/issues/30646#event-2516513806\nThank you again!\n\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle\n\n\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle\n\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#shuffle\n\nOn Friday, September 6, 2019, Jiri Simsa <notifications@github.com> wrote:\n\n> You simply need to specify the compression_type argument of\n> TFRecordDataset.\n>\n> CSV format is amenable to streaming processing but other formats might not\n> be.\n>\n> As for your shuffle_before_iteration issue, I am not sure which API are\n> referring. Neither CsvDataset\n> <https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset>\n> nor make_csv_dataset\n> <https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset>\n> have that argument. Did you file an issue for this? If so, can you please\n> share a link to it here? Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32075?email_source=notifications&email_token=ADZMMPCHT5HDCTUJOIONGF3QIKK7ZA5CNFSM4IR62YAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6DSQOY#issuecomment-528951355>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADZMMPFVNWUKIT5VABCBSEDQIKK7ZANCNFSM4IR62YAA>\n> .\n>\n\n\n-- \nSent from Gmail iPhone\n", "`reshuffle_each_iteration` does not control whether elements should only be shuffled every epoch (or only the first epoch). What it controls is whether the order in which elements are shuffled is different (or same) every epoch.\r\n\r\nThe error on #30646 does not seem related to tf.data as not allocating memory on GPU. I suggest you create a new issue for your problem with minimal reproducible example and reference it here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32075\">No</a>\n", "HI @jsimsa , [here](https://github.com/tensorflow/tensorflow/issues/32376) is the new issue. Please let me know if you have any comments or suggestions. Thank you very much!\r\n-Rishabh Sahrawat"]}, {"number": 32074, "title": "TFLite build for rpi broken", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Debian):\r\n- TensorFlow installed from (source or binary): github\r\n- TensorFlow version: head\r\n- Installed using virtualenv? pip? conda?: docker tensorflow/tensorflow:nightly-devel\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n\r\nFollowed https://www.tensorflow.org/lite/guide/build_rpi#cross-compile_for_raspberry_pi\r\n\r\n```\r\n$ git clone https://github.com/tensorflow/tensorflow\r\n$ cd tensorflow/\r\n$ ./tensorflow/lite/tools/make/download_dependencies.sh\r\n$ ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n\r\n```\r\n\r\nEventually this fails with:\r\n\r\n```\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/root/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/roo\r\nt/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/ -I/root/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/root/tensorflow/tensorflow/lit\r\ne/tools/make/downloads/absl -I/root/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/root/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/root/tensorflow/tensorflow/lite/tools/make/downl\r\noads/farmhash/src -I/root/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include \\                                                                                            \r\n-o /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model \\                                                                                                                                \r\n /root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/benchmark-lib.a -Wl,--no-export-dynamic -Wl,--exclude-libs,ALL -Wl,--gc-sections -Wl,--as-needed -lstdc++ -lpthread -lm -ldl                       \r\n/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/benchmark-lib.a(benchmark_tflite_model.o): In function `tflite::benchmark::BenchmarkTfLiteModel::LogParams()':                                      \r\nbenchmark_tflite_model.cc:(.text+0x1164): undefined reference to `tflite::nnapi::GetStringDeviceNamesList[abi:cxx11]()'                                                                                            \r\ncollect2: error: ld returned 1 exit status                                                                                                                                                                         \r\ntensorflow/lite/tools/make/Makefile:292: recipe for target '/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model' failed                                                                 \r\nmake: *** [/root/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/benchmark_model] Error 1 \r\n```\r\n\r\n", "comments": ["`tflite::nnapi::GetStringDeviceNamesList` is in a relatively new file `tensorflow/lite/nnapi/nnapi_util.cc`. Either add this `nnapi_util.cc` to appropriate place in `tensorflow/lite/tools/make/Makefile` or add `#if defined(__ANDROID__)` around related code in ` tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc` \r\n", "Hello @freedomtan,\r\nI am facing the same **ERROR** but while Build TensorFlow Lite for **ARM64** boards not for Raspberry Pi, However for  Raspberry Pi it works fine.\r\n", "I've verified that there is no build issue on RPI3 and ARM64 now. @eslambakr please let me know if you still have the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32074\">No</a>\n"]}, {"number": 32073, "title": "build error tensorflow lite for ARM64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- TensorFlow version (or github SHA if from source):master\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n\r\n**problem:**\r\ni want to build the label_image,and i change the makefile,add label_image like minimal:\r\n```\r\nMINIMAL_SRCS := \\\r\n\ttensorflow/lite/examples/minimal/minimal.cc\r\nLABEL_IMAGE_SRCS := \\\r\n\ttensorflow/lite/examples/label_image/label_image.cc \\\r\n\ttensorflow/lite/examples/label_image/bitmap_helpers.cc\r\n```\r\n\r\n```\r\n$(MINIMAL_SRCS) \\\r\n$(LABEL_IMAGE_SRCS)\r\n```\r\n\r\n```\r\nALL_SRCS := \\\r\n\t$(MINIMAL_SRCS) \\\r\n        $(LABEL_IMAGE_SRCS) \\\r\n```\r\n\r\n```\r\nMINIMAL_BINARY := $(BINDIR)minimal\r\nLABEL_IMAGE_BINARY := $(BINDIR)label_image\r\n```\r\n\r\n```\r\nMINIMAL_OBJS := $(addprefix $(OBJDIR), \\\r\n$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(MINIMAL_SRCS))))\r\nLABEL_IMAGE_OBJS := $(addprefix $(OBJDIR), \\\r\n$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(LABEL_IMAGE_SRCS))))\r\n```\r\n\r\n`all: $(LIB_PATH)  $(MINIMAL_BINARY) $(BENCHMARK_BINARY) $(LABEL_IMAGE_BINARY)`\r\n\r\n```\r\n$(MINIMAL_BINARY): $(MINIMAL_OBJS) $(LIB_PATH)\r\n\t@mkdir -p $(dir $@)\r\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) \\\r\n\t-o $(MINIMAL_BINARY) $(MINIMAL_OBJS) \\\r\n\t$(LIBFLAGS) $(LIB_PATH) $(LDFLAGS) $(LIBS)\r\n\r\nminimal: $(MINIMAL_BINARY)\r\n\r\n$(LABEL_IMAGE_BINARY): $(LABEL_IMAGE_OBJS) $(LIB_PATH)\r\n\t@mkdir -p $(dir $@)\r\n\t$(CXX) $(CXXFLAGS) $(INCLUDES) \\\r\n\t-o $(LABEL_IMAGE_BINARY) $(LABEL_IMAGE_OBJS) \\\r\n\t$(LIBFLAGS) $(LIB_PATH) $(LDFLAGS) $(LIBS)\r\n\r\nlabel_image: $(LABEL_IMAGE_BINARY)\r\n```\r\ni got a build error\uff1alabel_image.cc\r\nundefined reference\uff1a\"tflite::evaluation::CreateGPUDelegate(tflite::FlateBufferModel)\"\r\nundefined reference\uff1a\"tflite::evaluation::CreateNNAPIDelegate()\"", "comments": ["They are in `tensorflow/lite/tools/evaluation/utils.cc`. Either add it to `$LABEL_IMAGE_SRCS` or replace them with `Interpreter::TfLiteDelegatePtr(nullptr, [](TfLiteDelegate*) {});`", "> They are in `tensorflow/lite/tools/evaluation/utils.cc`. Either add it to `$LABEL_IMAGE_SRCS` or replace them with `Interpreter::TfLiteDelegatePtr(nullptr, [](TfLiteDelegate*) {});`\r\n\r\nthanks for you reply ! i add the path to $LABEL_IMAGE_SRCS, this is very helpful to me and successfully solved the problem.", "I am closing the issue since the query is been resolved. Please, feel free to reopen if the issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32073\">No</a>\n"]}, {"number": 32072, "title": "iOS GestureClassification (Type 'Interpreter' has no member 'Options')", "body": "When I build this project, I got the error\r\nType 'Interpreter' has no member 'Options'\r\nMaybe changed class name recently, but the change is not complete\r\nThe file name is ModelDataHandler.swift and the line is 99\r\n\r\nThis is error message\r\n![\u87a2\u5e55\u5feb\u7167 2019-08-29 \u4e0b\u53481 53 17](https://user-images.githubusercontent.com/32124047/63915118-bf36e980-ca67-11e9-8c32-6562128c73f5.png)\r\n\r\nAnd I try this to fix\r\n![\u87a2\u5e55\u5feb\u7167 2019-08-29 \u4e0b\u53481 57 12](https://user-images.githubusercontent.com/32124047/63915166-dd044e80-ca67-11e9-9381-7360a7685c5a.png)\r\n\r\n", "comments": ["@t104360088,\r\nPlease provide the information asked in the [Template](https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md). Thanks!\r\n\r\n", "@gadagashwini this commit https://github.com/tensorflow/examples/commit/751f4648e3917178b0e67454422477fe5d81d611#diff-7e5803a55f12fc06d7347d59eb7d0f99 \r\nShould have a corresponding change to Interpreter which is here https://github.com/tensorflow/tensorflow/blob/c3a9eb7c28107574872c77237670cc174ef45d9f/tensorflow/lite/experimental/swift/Sources/Interpreter.swift", "**System information**\r\n- OS Platform and Distribution : macOS 10.14\r\n- TensorFlow installed from (source or binary): google colab\r\n- TensorFlow version (or github SHA if from source): 1.13.1", "@t104360088, Please provide the google colab gist to expedite the trouble-shooting process. Thanks!\r\n", "@gadagashwini , I think this issue is not related to ml model.\r\n\r\nhttps://gist.github.com/t104360088/d9a0904b1fdde0b326692c994d1028f6\r\n\r\n", "I believe this was fixed by 53afd4e200942a4b05b8f4d2982cb5e7c8c0a44d. Feel free to re-open or file a new bug if this is still an issue."]}, {"number": 32071, "title": "integer overflow possibility fix", "body": "The current code can lead to integer overflow and weird results\r\nCreated issue: https://github.com/tensorflow/tensorflow/issues/32045\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32071) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!) ", "@iur-kvasniuk thank you for your contribution, please sign CLA.", "> @iur-kvasniuk thank you for your contribution, please sign CLA.\r\n\r\n@gbaned, I have signed Google Individual CLA Aug 28, 2019 23:12 PDT", "@googlebot I signed it!", "username was changed to avoid dublication", "@googlebot I signed it!", "Closing this since it is a duplicate of PR#[32083](https://github.com/tensorflow/tensorflow/pull/32083). "]}, {"number": 32070, "title": "OP_REQUIRES failed at save_restore_v2_ops.cc:109 : Not found: Failed to create a NewWriteableFile", "body": "my os is win10:\r\nwhen i use tf.estimator.BestExporter, i find a bug.\r\n\r\nin saver.py \r\n_SHARDED_SUFFIX = \"_temp_%s/part\" % uuid.uuid4().hex\r\npath ='aaa\\bbb\\_temp_xxx/part....' \r\n\r\ni think to change code:\r\n    _SHARDED_SUFFIX_temp = \"_temp_%s\" % uuid.uuid4().hex\r\n    _SHARDED_SUFFIX = os.path.join(_SHARDED_SUFFIX_temp, 'part')\r\n\r\n\r\n", "comments": []}, {"number": 32069, "title": "Export Inference TensorFlow model Option(DEFAULT/API_MODEL)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (1.8/2.0):\r\n- Are you willing to contribute it (Yes):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWhen exporting(freeze graph) tensorflow model(graph) only for inferencing on mobile/Embedded/IoT using [Tflite](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite)/[MNN](https://github.com/alibaba/MNN) etc lite inference framework, it is difficult to convert/implement the freezed graph efficiently sometimes. For example, when using RNN/GRU/LSTM(static or dynamic) or control flow api(operations), tensorflow will unroll the api to lower operations that make converting/implementing/optimizing on other inference framework difficult, and need much efforts for optimizing the unrolled graph.\r\nI wonder whether the following recommendation is acceptable:\r\n> Tensorflow provide two options when exporting(freeze graph), one is the *DEFAULT* which like the original process, the other is *API_MODEL* which do not lower the api, keep the api-operations in the freezed graph(pb). \r\n This method like [convert-RNN-tflite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/convert/rnn.md), but I think the above method is better.\r\n\r\n**Will this change the current api? How?**\r\nYes, add one option(DEFAULT/API_MODEL)\r\n**Who will benefit with this feature?**\r\nThose who using tensorflow framework to train model, then deploying the model on mobile/Embedded/IoT using Tflite/MNN etc lite inference framework.\r\n**Any Other info.**\r\n", "comments": ["The freeze graph 1.X API ([`convert_variables_to_constants`](https://www.tensorflow.org/api_docs/python/tf/graph_util/convert_variables_to_constants)) does not lower control flow. The API takes in a GraphDef and does not have sufficient information in order to prevent lowering.\r\n\r\nControl flow lowering happens in the TFLite code. If you want to prevent lowering from happening, please reference the [`OpHint`](https://www.tensorflow.org/api_docs/python/tf/lite/OpHint) API. We are also actively working on better supporting control flow in TensorFlow 2.0."]}, {"number": 32068, "title": "I have trained a .pb model which is in tf1.2 and I want to use it in the environment of tf2.0, but I fail to use it successfully. Anyone knows how to solve the problem", "body": "from __future__ import division\r\nfrom __future__ import print_function\r\n\r\ntry:\r\n    import tensorflow.compat.v1 as tf\r\n    tf.disable_v2_behavior()\r\nexcept (ImportError, AttributeError):\r\n    import tensorflow as tf\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\n        with tf.Graph().as_default():\r\n\r\n            output_graph_def = tf.GraphDef()\r\n\r\n            with open('./myModel.pb', \"rb\") as f:\r\n                output_graph_def.ParseFromString(f.read())\r\n                tf.import_graph_def(output_graph_def, name=\"\")\r\n\r\n            gpu_options = tf.GPUOptions(\r\n                per_process_gpu_memory_fraction=self.gpu_memory_fraction,\r\n                visible_device_list=self.visible_device_list,\r\n                allow_growth=self.allow_growth)\r\n            tfconfig = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=False)\r\n            tfconfig.gpu_options.allow_growth = True\r\n\r\n            with tf.Session(config=tfconfig) as sess:\r\n                sess.run(tf.global_variables_initializer())\r\n                input_photo = sess.graph.get_tensor_by_name('placeholder/photo:0')\r\n                output_photo = sess.graph.get_tensor_by_name('decoder/Sigmoid:0')\r\n\r\n\r\n################################## erros are as follows:\r\nTraceback (most recent call last):\r\nFile \"/Users/govan/Project/test.py\", line 31, in __call__\r\n    output_graph_def = tf.GraphDef()\r\nAttributeError: module 'tensorflow' has no attribute 'GraphDef'", "comments": ["@Govan111,\r\nDid you follow the instructions mentioned in the [Tensorflow website](https://www.tensorflow.org/beta/guide/upgrade) to upgrade from 1.x to 2.x. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32068\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32068\">No</a>\n"]}, {"number": 32067, "title": "Used tensorflow arithmetic in the C# of visual studio ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: window 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:1.14\r\n- **Python version**:3.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:10.1\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHow did i use tensorflow arithmetic in the C# of visual studio 2019 when I had finished my tensorflow arithmetic . please tell you first step second step and so on .  please give me a code example. Thank you!\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Since this issue requires TensorFlow API implementation in C#,\r\nYou may raise this issue on [SciSharp repository](https://github.com/SciSharp/TensorFlow.NET) to seek better guidance in this regards. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32067\">No</a>\n"]}]