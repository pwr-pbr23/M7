[{"number": 49831, "title": "Error while trying to run train.py on Experiencor's yolov3 with keras project", "body": "The git: \r\nhttps://github.com/experiencor/keras-yolo3\r\n\r\nThe execution:\r\n[full_run.txt](https://github.com/tensorflow/tensorflow/files/6553076/full_run.txt)\r\n\r\nThe error: \r\n[error.txt](https://github.com/tensorflow/tensorflow/files/6553063/error.txt)\r\n\r\n", "comments": ["@noorraghib12 ,\r\n\r\nCan you please provide the Tensorflow version you are using and also please take a look at this issues with similar error log.\r\n[Link1](https://github.com/tensorflow/tensorflow/issues/27120),[Link2](https://github.com/tensorflow/tensorflow/issues/42183),[Link3](https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio).\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49830, "title": "Patched C API implementation to correctly use interpreter num threads", "body": "Patched C API implementation to correctly use interpreter number of threads setting while invoking `InterpreterBuilder` since it is not enough to only call `SetNumThreads` method for setting to be applied for delegates as well. See https://github.com/tensorflow/tensorflow/issues/42277 for detailed description.", "comments": []}, {"number": 49829, "title": "image_dataset_from_directory returns dtype 'uint8' Dataset if interpolation='nearest'", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory\r\n\r\n### Clear description\r\ntf.keras.preprocessing.image_dataset_from_directory returns dtype 'uint8' Dataset if interpolation='nearest'\r\nhowever function docstring reports misleading:\r\n\r\n### Returns defined\r\n\r\nReturns:    A `tf.data.Dataset` object.      - If `label_mode` is None, it yields `float32` tensors of shape        `(batch_size, image_size[0], image_size[1], num_channels)`,        encoding images (see below for rules regarding `num_channels`).      - Otherwise, it yields a tuple `(images, labels)`, where `images`        has shape `(batch_size, image_size[0], image_size[1], num_channels)`,        and `labels` follows the format described below.\r\n\r\n[...]\r\n## Description of issue (what needs changing):\r\n\r\nreturn dtype 'uint8' for interpolation='nearest should be specified, such as:\r\n\r\n\r\nReturns:    A `tf.data.Dataset` object.      - If `label_mode` is None, it yields `float32` tensors of shape        `(batch_size, image_size[0], image_size[1], num_channels)`,        encoding images (see below for rules regarding `num_channels`).     \r\n - If `label_mode` is None and interpolation is 'bilinear' it yields `uint8`.\r\n - Otherwise, it yields a tuple `(images, labels)`, where `images`        has shape `(batch_size, image_size[0], image_size[1], num_channels)`,        and `labels` follows the format described below.\r\n\r\n[...]\r\n\r\n\r\n", "comments": ["Thank you for the information, Pr is created to track this issue #49861, this issue will be automatically closed once the pr is merged.", "As per comment on  #49861, this issue is moved to keras repo, hence moving this to closed status, requesting you to follow up at [this link](https://github.com/keras-team/keras/pull/15429)"]}, {"number": 49828, "title": "What is the purpose of autoshardpolicy when it actually gets the same outcome when it is turned off?", "body": "## URL(s) with the issue: https://www.tensorflow.org/tutorials/distribute/input\r\n\r\n## Description of issue (what needs changing):\r\nthe docs said when you set `options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF` , \r\neach worker will process all the data.\r\nHowever, in my experimentation, tf.data.experimental.AutoShardPolicy.OFF gets the same results(in terms of how much data a worker will process) as tf.data.experimental.AutoShardPolicy.DATA\r\nSo what is purpose of autoshardpolicy? or do I have any misunderstanding here?\r\n\r\ncodes:\r\n```\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(f'using distribution strategy\\nnumber of gpus:{strategy.num_replicas_in_sync}')\r\ntf2_dataset=tf.data.Dataset.from_tensor_slices(range(10))\r\ntf2_dataset=tf2_dataset.shuffle(10)\r\ntf2_dataset=tf2_dataset.batch(3)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\r\ntf2_dataset=tf2_dataset.with_options(options)\r\ndt=strategy.experimental_distribute_dataset(tf2_dataset)\r\nfor i,j in enumerate(dt):\r\n    print(f'batch {i}:')\r\n    print(j)\r\n```\r\nnumber of gpus:2\r\nbatch 0:\r\nPerReplica:{\r\n  0: tf.Tensor([0 7], shape=(2,), dtype=int32),\r\n  1: tf.Tensor([1], shape=(1,), dtype=int32)\r\n}\r\nbatch 1:\r\nPerReplica:{\r\n  0: tf.Tensor([3 9], shape=(2,), dtype=int32),\r\n  1: tf.Tensor([6], shape=(1,), dtype=int32)\r\n}\r\nbatch 2:\r\nPerReplica:{\r\n  0: tf.Tensor([5 2], shape=(2,), dtype=int32),\r\n  1: tf.Tensor([8], shape=(1,), dtype=int32)\r\n}\r\nbatch 3:\r\nPerReplica:{\r\n  0: tf.Tensor([4], shape=(1,), dtype=int32),\r\n  1: tf.Tensor([], shape=(0,), dtype=int32)\r\n}\r\n```\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(f'using distribution strategy\\nnumber of gpus:{strategy.num_replicas_in_sync}')\r\ntf2_dataset=tf.data.Dataset.from_tensor_slices(range(10))\r\ntf2_dataset=tf2_dataset.shuffle(10)\r\ntf2_dataset=tf2_dataset.batch(3)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\ntf2_dataset=tf2_dataset.with_options(options)\r\ndt=strategy.experimental_distribute_dataset(tf2_dataset)\r\nfor i,j in enumerate(dt):\r\n    print(f'batch {i}:')\r\n    print(j)\r\n```\r\nnumber of gpus:2\r\nbatch 0:\r\nPerReplica:{\r\n  0: tf.Tensor([6 1], shape=(2,), dtype=int32),\r\n  1: tf.Tensor([2], shape=(1,), dtype=int32)\r\n}\r\nbatch 1:\r\nPerReplica:{\r\n  0: tf.Tensor([9 4], shape=(2,), dtype=int32),\r\n  1: tf.Tensor([8], shape=(1,), dtype=int32)\r\n}\r\nbatch 2:\r\nPerReplica:{\r\n  0: tf.Tensor([3 0], shape=(2,), dtype=int32),\r\n  1: tf.Tensor([7], shape=(1,), dtype=int32)\r\n}\r\nbatch 3:\r\nPerReplica:{\r\n  0: tf.Tensor([5], shape=(1,), dtype=int32),\r\n  1: tf.Tensor([], shape=(0,), dtype=int32)\r\n}\r\n\r\n", "comments": ["@jvishnuvardhan \r\n\r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/0b7ea8dac63951bba7e4fb607a6753de/untitled15.ipynb).Thanks", "Hi @laplacericky, autoshardpolicy only applies when there are multiple workers. If you're using `MirroredStrategy`, there is only one worker, so there is no autosharding and the autosharding policy you set just registers as no-op. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49828\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49828\">No</a>\n"]}, {"number": 49827, "title": "Adding documentation for running doctests locally", "body": "Adding doc for running doctest locally.\r\nAlthough the doc is there on [website's contribute page](https://www.tensorflow.org/community/contribute/docs_ref), but we should also keep it in `CONTRIBUTING.md` along with other test instructions.\r\n\r\nThe reason behind it IMO is that one may want to see the steps of running doctest as well along with sanity checks and unit tests.\r\n\r\ncc @mihaimaruseac ", "comments": []}, {"number": 49826, "title": "RTX 3090 training custom data stuck on first epoch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-54779-g1236f723c6d 2.6.0-dev20210413\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 11.1/CUDNN 8.1.0\r\n- GPU model and memory: Nvidia RTX 3090 64GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI used to run the code on Colab and it run successfully without any errors. I want to copy this code on my new computer.\r\nBut when I run the code, it will stuck on the first epoch and never continue.\r\nI want to know what cause this problem, I can't see any errors on the log.\r\n\r\n**Describe the expected behavior**\r\nRun the training code successfully with GPU RTX 3090 and tensorflow.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n2021-05-27 14:03:20.253134: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n  1/221 [..............................] - ETA: 1:53:26 - loss: 1.8405 - accuracy: 0.12502021-05-27 14:03:27.764972: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\r\n2021-05-27 14:03:27.765052: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\r\n\r\n\r\n", "comments": ["After I close the stuck code, I run the code again.\r\nAnd I get the following log. Above this part is the same. Only this part different with issue.\r\nAfter this \"CUPTI activity buffer flushed\", the code will interrupt.\r\n\r\n>   1/221 [..............................] - ETA: 1:47:42 - loss: 1.7554 - accuracy: 0.31252021-05-27 15:13:22.496873: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\r\n2021-05-27 15:13:22.496957: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\r\n  2/221 [..............................] - ETA: 24:31 - loss: 1.7655 - accuracy: 0.2812  2021-05-27 15:13:23.337160: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\r\n2021-05-27 15:13:23.338859: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed", "I solve the problem. THX", "@er778899789 How did you solve it please?", "> After I close the stuck code, I run the code again. And I get the following log. Above this part is the same. Only this part different with issue. After this \"CUPTI activity buffer flushed\", the code will interrupt.\r\n> \r\n> > 1/221 [..............................] - ETA: 1:47:42 - loss: 1.7554 - accuracy: 0.31252021-05-27 15:13:22.496873: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\r\n> > 2021-05-27 15:13:22.496957: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\r\n> > 2/221 [..............................] - ETA: 24:31 - loss: 1.7655 - accuracy: 0.2812  2021-05-27 15:13:23.337160: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\r\n> > 2021-05-27 15:13:23.338859: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed\r\n\r\n@er778899789 I have the same problem as you\uff0chow did you solve it please?", "@Minxiangliu @yangzhenze1998 \r\nDo you get any error in log?\r\nex. Some dll file not found\u2026\u2026\r\nOr can you guys tell me which tensorflow version you used?\r\n\r\nBy the way, I remember I add this command in my training code like below. You guys can try it.\r\n\r\n> tf.disable_eager_execution()\r\n"]}, {"number": 49825, "title": "Fix typo on line 3918, \"tf.data.experimental.enable_debug_mode()\"", "body": "My PR for issue https://github.com/tensorflow/tensorflow/issues/49724", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49825) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Hi any reason why the MacOS CPU Python3 build is failing? Or is this expected?"]}, {"number": 49824, "title": "/usr/local/cuda/bin/ptxas --version returned -1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   ```\r\n   Linux 5.4.0-1048-aws #50~18.04.1-Ubuntu SMP Tue May 4 17:40:02 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\n   ```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n   ```\r\n   tensorflow-gpu         2.2.0\r\n   ```\r\n- Python version:\r\n   ```\r\n   Python 3.6.9\r\n   ```\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n   ```\r\n   nvcc: NVIDIA (R) Cuda compiler driver\r\n   Copyright (c) 2005-2019 NVIDIA Corporation\r\n   Built on Sun_Jul_28_19:07:16_PDT_2019\r\n   Cuda compilation tools, release 10.1, V10.1.243\r\n   ```\r\n- GPU model and memory:\r\n   ```\r\n   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n   ```\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nIn the TF logs i can see:\r\n```\r\n2021-05-27 02:41:38.569570: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running /usr/local/cuda/bin/ptxas --version returned -1\r\n2021-05-27 02:41:38.651452: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output:\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n```\r\n\r\n**which cause the first request take insanely long, like 1min+,**\r\n\r\nbut within the container i check the and get:\r\n```\r\nroot@3584ebf46aa4:/centaur# which ptxas\r\n/usr/local/cuda/bin/ptxas\r\nroot@3584ebf46aa4:/centaur# ptxas --version\r\nptxas: NVIDIA (R) Ptx optimizing assembler\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:06:54_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n```\r\n\r\nSo get lost here, why TF get `-1` for the command `/usr/local/cuda/bin/ptxas --version`, think it could be a bug.\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hello, I'm also facing this exact same issue :(", "\r\nTry with `pip`", "a bit more info, just now tried with `docker pull tensorflow/tensorflow:2.5.0-gpu` and get the below logs:\r\n```\r\n2021-05-27 06:49:23.156701: I tensorflow/stream_executor/gpu/asm_compiler.cc:167] Looking for ptxas at /usr/local/cuda-11.2/bin/ptxas\r\n2021-05-27 06:49:23.156792: I tensorflow/stream_executor/gpu/asm_compiler.cc:176] Using ptxas at /usr/local/cuda-11.2/bin/ptxas\r\n2021-05-27 06:49:23.158352: W tensorflow/stream_executor/gpu/asm_compiler.cc:64] Running /usr/local/cuda-11.2/bin/ptxas --version returned -1\r\n2021-05-27 06:49:23.158554: I tensorflow/stream_executor/gpu/asm_compiler.cc:223] ptx written to: /tmp/tempfile-d93b585ae759-2102af45-34-5c34a28ce4d87\r\n2021-05-27 06:49:23.216374: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output: ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'redzone_checker' for 'sm_70'\r\nptxas info    : Function properties for redzone_checker\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 8 registers, 384 bytes cmem[0]\r\n\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n```", "@lnshi \r\nCan you please verify as per [this comment](https://github.com/tensorflow/tensorflow/issues/40036#issuecomment-663728048)", "@Saduf2019 \r\nhmm, i don't think we can afford to re-compile tensorflw-gpu every version based on our customer's need.\r\n\r\nCould u help to point out at least one version of the tensorflow-gpu docker images in the [docker hub](https://hub.docker.com/r/tensorflow/tensorflow/tags?page=1&ordering=last_updated) doesn't have this issue?", "@lnshi \r\nCan you try on stable version 2.4.1 and let us know.", "@Saduf2019 \r\nI just tried out `docker pull tensorflow/tensorflow:2.4.1-gpu-jupyter` and encountered the exactly same issue, the 1st request takes `1m37.463020774s` bcoz of the ptxas compiling (normally should be around 190ms):\r\n```\r\n2021-05-28 06:09:17.170277: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-05-28 06:09:17.470810: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300005000 Hz\r\n2021-05-28 06:09:19.292785: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-05-28 06:09:20.906585: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running /usr/local/cuda-11.0/bin/ptxas --version returned -1\r\n2021-05-28 06:09:21.019335: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output:\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2021-05-28 06:09:22.725043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-05-28 06:09:23.134175: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n```", "Can you please share a command that can replicate this with one of the containers?", "@angerson and others, thanks a lot for ur effort and time.\r\n\r\nduring preparing the simple script to reproduce the issue we found actually the root cause is neither tensorflow nor gpu driver, but a piece of signal handling code in our program, which is:\r\n```\r\ncatchable_sigs = set(signal.Signals) - {signal.SIGKILL, signal.SIGSTOP}\r\nfor sig in catchable_sigs:\r\n    signal.signal(sig, signal.SIG_IGN)\r\n```\r\nif u happen to know how this link to the `Running /usr/local/cuda-11.0/bin/ptxas --version returned -1` pls pls enlighten me.", "@lnshi \r\nPlease let us know if this is still an issue.", "@lnshi, Sorry for late response. We are checking to see if you still need help on this issue. \r\n\r\nCan you try building the latest stable version of TF i.e 2.7.0 and let us know if the issue persists? You can use [this](https://www.tensorflow.org/install/source) guide for your reference and take a look at [this](https://www.tensorflow.org/install/source#linux) link for the tested build configs.Thanks!", "@chunduriv  pls see this comment i put: https://github.com/tensorflow/tensorflow/issues/49824#issuecomment-856446851\r\n\r\nit is due to we accidentally captured some signals in our program..., though how exactly these two affect each other is still unknown to me, but from my side, this issue can be considered as solved\r\n\r\nthank u all for the effort.", "@lnshi, Glad the issue is fixed on your end. Please feel free to move this to closed status. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49824\">No</a>\n"]}, {"number": 49823, "title": "AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__' when using keras", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Google colab\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: colab gpu\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nGetting the error AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__' when importing keras. Has only been happening very recently, although no changes to code were made during the time between.\r\n**Describe the expected behavior**\r\nPython file to run normally\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n[https://colab.research.google.com/drive/1e3d5U85ijexXILaKx2BXjAJ8Z010fQ2X?usp=sharing](https://colab.research.google.com/drive/1e3d5U85ijexXILaKx2BXjAJ8Z010fQ2X?usp=sharing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nError when running: \r\n`from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda`\r\n```\r\n Using TensorFlow backend.\r\n    Traceback (most recent call last):\r\n      File \"train.py\", line 6, in <module>\r\n        from yolo import create_yolov3_model, dummy_loss\r\n      File \"/content/drive/MyDrive/yolo/yolo_plz_work/yolo.py\", line 1, in <module>\r\n        from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/__init__.py\", line 3, in <module>\r\n        from . import utils\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py\", line 26, in <module>\r\n        from .vis_utils import model_to_dot\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py\", line 7, in <module>\r\n        from ..models import Model\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/models.py\", line 10, in <module>\r\n        from .engine.input_layer import Input\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/engine/__init__.py\", line 3, in <module>\r\n        from .input_layer import Input\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\", line 7, in <module>\r\n        from .base_layer import Layer\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 12, in <module>\r\n        from .. import initializers\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py\", line 124, in <module>\r\n        populate_deserializable_objects()\r\n      File \"/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py\", line 49, in populate_deserializable_objects\r\n        LOCAL.GENERATED_WITH_V2 = tf.__internal__.tf2.enabled()\r\n      File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/module_wrapper.py\", line 193, in __getattr__\r\n        attr = getattr(self._tfmw_wrapped_module, name)\r\n    AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__'\r\n```\r\n", "comments": ["I also have a same problem. Have you solved?", "@brian654321 \r\n\r\nThese compat.v2 module requires TF 2.x. Please upgrade to latest  tf  stable version,able to reproduce the code shared with no errors,here is the [gist](https://colab.research.google.com/gist/UsharaniPagadala/4ffca124685c02de4870a9ab6fcb2815/keras_error.ipynb) and let us know if it helps.Thanks\r\n\r\n", "https://github.com/googlecolab/colabtools/issues/2044#issuecomment-849161550", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49823\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49823\">No</a>\n"]}, {"number": 49822, "title": "[Intel MKL] Remove check for native format for INT8", "body": "Since all INT8 native-related PRs are merged now, we need to remove the check for native format.", "comments": ["@mahmoud-abuzaina  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned I see \"do_pylint\" is failing, but this PR does not touch any python code. Wondering if the error is not related to this PR?", "@gbaned @mahmoud-abuzaina Yes, the Sanity error is unrelated (existing failure)."]}, {"number": 49821, "title": "Prevent array OOB read/write", "body": "PiperOrigin-RevId: 371026165\nChange-Id: I26ac6372c87246e03c7eb8c94e84c84d86054b36", "comments": []}, {"number": 49820, "title": "Prevent array OOB read/write", "body": "PiperOrigin-RevId: 371026165\nChange-Id: I26ac6372c87246e03c7eb8c94e84c84d86054b36", "comments": []}, {"number": 49819, "title": "Prevent array OOB read/write", "body": "PiperOrigin-RevId: 371026165\nChange-Id: I26ac6372c87246e03c7eb8c94e84c84d86054b36", "comments": []}, {"number": 49818, "title": "Prevent array OOB read/write", "body": "PiperOrigin-RevId: 371026165\nChange-Id: I26ac6372c87246e03c7eb8c94e84c84d86054b36", "comments": []}, {"number": 49817, "title": "Prevent array write out-of-bounds.", "body": "If user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224", "comments": []}, {"number": 49816, "title": "Prevent array write out-of-bounds.", "body": "If user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224", "comments": []}, {"number": 49815, "title": "Prevent array write out-of-bounds.", "body": "If user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224", "comments": []}, {"number": 49814, "title": "Prevent array write out-of-bounds.", "body": "If user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224", "comments": []}, {"number": 49813, "title": "Fix a dangerous integer overflow and a malloc of negative size.", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49813) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49812, "title": "Fix a dangerous integer overflow and a malloc of negative size.", "body": "PiperOrigin-RevId: 371254154\nChange-Id: I250a98a3df26328770167025670235a963a72da0", "comments": []}, {"number": 49811, "title": "Fix a dangerous integer overflow and a malloc of negative size.", "body": "PiperOrigin-RevId: 371254154\nChange-Id: I250a98a3df26328770167025670235a963a72da0", "comments": []}, {"number": 49810, "title": "Fix a dangerous integer overflow and a malloc of negative size.", "body": "PiperOrigin-RevId: 371254154\nChange-Id: I250a98a3df26328770167025670235a963a72da0", "comments": []}, {"number": 49809, "title": "Fix integer overflow in TFLite concat", "body": "PiperOrigin-RevId: 371013841\nChange-Id: I6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29", "comments": []}, {"number": 49808, "title": "Fix integer overflow in TFLite concat", "body": "PiperOrigin-RevId: 371013841\nChange-Id: I6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29", "comments": []}, {"number": 49807, "title": "Fix integer overflow in TFLite concat", "body": "PiperOrigin-RevId: 371013841\nChange-Id: I6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29", "comments": []}, {"number": 49806, "title": "saved model predict function call leads to OOM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 64-bit\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.0, cuDNN 8.0.4.30\r\n- GPU model and memory: NVIDIA GeForce GTX 1660 Ti (6 GB)\r\n\r\n**Describe the current behavior**\r\nI am trying to load a saved model (h5 file=3.8MB) using the setup described above and then use the model.predict on a numpy array of size `(45000, 32, 32, 3)` and `float32` datatype and I get a OOM memory error. \r\n\r\nI have set the memory growth option:\r\nfor gpu in tf.config.experimental.list_physical_devices('GPU'):\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n**Describe the expected behavior**\r\nwhile training the same model with dataset does not cause an error, the predict function. So, I am not sure what is happening. Th\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/rajatsaxena/test/blob/main/Similarity-CIFAR10-car.ipynb", "comments": ["@rajatsaxena ,\r\n\r\nPlease take a look at this issues with similar error log.It helps.[link1](https://stackoverflow.com/questions/50760543/error-oom-when-allocating-tensor-with-shape/50764934),[link2](https://github.com/tensorflow/tensorflow/issues/16768),[link3](https://github.com/Borda/keras-yolo3/issues/7).\r\n\r\nThanks!", "reducing the batch size helps, but the dataset isn't that big for both a 6gb and 12gb GPU to crash. I had run the same script on these exact same GPUs with Cuda10 and TF2.2 without any problem", "I was able to reproduce the issue in TF [v2.4](https://colab.research.google.com/gist/tilakrayal/fa1959ee9212cd1a4510a4afb8d2d0f9/49806-2-4.ipynb),[v2.5](https://colab.research.google.com/gist/tilakrayal/3ba7475c03cd15a6a539062601e88508/49806.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/597e3e3396ae966d5a6361418e3f67ab/49806-nightly.ipynb).Please find the gist.", "@rajatsaxena \r\n For GPU, try  keras.backend.clear_session()\r\nto clear the session and let us know if it helps.", "Alright, I will give it a shot", "It did not help. Is there a specific location in the above script where I should add it?", "@rajatsaxena \r\nAdd it before the model training.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49806\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49806\">No</a>\n"]}, {"number": 49805, "title": "Fix integer overflow in TFLite concat", "body": "PiperOrigin-RevId: 371013841\nChange-Id: I6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29", "comments": []}, {"number": 49804, "title": "Prevent a division by 0", "body": "PiperOrigin-RevId: 371007407\nChange-Id: Iecf2718de48d6bf5a69b02a9df9deda8ec1b19d3", "comments": []}, {"number": 49803, "title": "Prevent a division by 0", "body": "PiperOrigin-RevId: 371007407\nChange-Id: Iecf2718de48d6bf5a69b02a9df9deda8ec1b19d3", "comments": []}, {"number": 49802, "title": "Prevent a division by 0", "body": "PiperOrigin-RevId: 371007407\nChange-Id: Iecf2718de48d6bf5a69b02a9df9deda8ec1b19d3", "comments": []}]