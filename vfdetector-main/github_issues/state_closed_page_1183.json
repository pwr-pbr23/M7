[{"number": 17697, "title": "Fix issue of tf.gfile.Exists with Unicode on Windows", "body": "This fix tries to address the issue raised in #17695 where\r\ntf.gfile.Exists with Unicode on Windows does not work as expected.\r\nThe issuse comes from the usage of `_access` (vs. _waccess).\r\n\r\nThis fix adds a test and fixes the issue.\r\n\r\nThis fix fixes #17695.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17696, "title": "Update documentation", "body": "This PR updates Readme file with instructions to help installation of external packages.", "comments": ["Tagging @gunan and @aaroey ", "@gunan - README.md files don't get pulled into tensorflow.org, it only has to render on github so there are no site-compatibility issues to worry about (LGTM).", "Hi @gunan @yifeif , who is going to merge this? And how do we send this to master branch?", "I can bring everything back to master. I will wait for #17772 to get in.", "Thanks @yifeif !"]}, {"number": 17695, "title": "tf.gfile doesn't understand NTFS Unicode filenames on Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows Server 2016 Datacenter, 64-bit, installed on a GCE VM\r\n- **TensorFlow installed from (source or binary)**: binary pip package\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `python -c \"dir = '/tmp/tftest/\\u8c37\\u6b4c'; import tensorflow as tf; import os; os.makedirs(dir); print(os.path.exists(dir), tf.gfile.Exists(dir))\"`\r\n\r\n### Describe the problem\r\nThe tf.gfile functions that involve file paths appear not to understand Windows with NTFS filenames that contain Unicode characters, such as \u8c37\u6b4c (\"Google\" in Chinese).  It seems like something about the name isn't translated in a way that actually results in successful lookups, so everything behaves as if those files didn't exist.\r\n\r\nCommand to repro:\r\n```\r\npython -c \"dir = '/tmp/tftest/\\u8c37\\u6b4c'; import tensorflow as tf; import os; os.makedirs(dir); print(os.path.exists(dir), tf.gfile.Exists(dir))\"\r\n```\r\n\r\nThis command run in Windows PowerShell prints \"True False\" i.e. `os.path.exists()` returns a different result than `tf.gfile.Exists()`.  This is true even for different ways of formulating that path -\r\n in addition to `/tmp/tftest/\\u8c37\\u6b4c` I tested `C:/tmp/tftest/\\u8c37\\u6b4c` and `C:\\\\tmp\\\\tftest\\\\\\u8c37\\u6b4c` (double backslashes for the path separators to escape within a python string) and they both produce the same result.  If you remove the unicode characters from the path, it prints \"True True\" as expected.\r\n\r\nIn contrast, on my gLinux workstation this prints \"True True\" all the time, i.e. the results are the same.\r\n\r\nI tested this with Exists(), IsDirectory(), and ListDirectory() but I'm assuming it applies generally to all the tf.gfile functions that take a path argument.  Note however that ListDirectory() will *return* the Unicode name just fine if run in the parent directory, the same as `os.listdir()`.\r\n\r\nWe've gotten a report about this for TensorBoard: https://github.com/tensorflow/tensorboard/issues/861\r\n", "comments": ["I think the issue is that `_access` instead of wchar_t version of `_waccess` is used for `FileExists`:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/windows_file_system.cc#L383-L389\r\n\r\nCreated a PR #17697 for _waccess, though it might be good to avoid unicode in Windows if possible, as it may not be well supported.", "Ah great, thanks for figuring out the underlying issue so quickly!  I guess it must just have been overlooked in https://github.com/tensorflow/tensorflow/pull/11562 which adds unicode support for most of the WindowsFileSystem methods, just not FileExists().\r\n\r\nIndeed, it looks like tf.gfile does indirectly invoke FileExists() for not only Exists() but also IsDirectory() (behavior inherited from FileSystem) and ListDirectory() (behavior inherited from file_io.py).  So if this fixes the issue for Exists() then the other functions I noticed failing should be fixed as well."]}, {"number": 17694, "title": "tf.test.TestCase not working properly with tf.map_fn", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac OSX\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0 and 1.5.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: Run the snippet below.\r\n\r\n### Describe the problem\r\n\r\nWhen trying to run tf.test.TestCase with a tensor of strings, tf.map_fn() throws errors that a normal tf.Session.run() does not encounter.\r\n\r\nI am trying to create unit tests with tf.test.TestCase, but could not find a way to test tf.map_fn without Tensorflow returning an error. (This occurs in both TF 1.4 and 1.5.) My guess is it has to do with how strings are treated by map_fn as tensors...\r\n\r\n### Source code / logs\r\n\r\nHere is a very simple example:\r\n```\r\n    def identity_map(input):\r\n      return input\r\n\r\n    # Using tf.Session() and sess.run(). This runs without errors.\r\n    input = ['123', 'abc']\r\n    x = tf.map_fn(identity_map, input_tensor, dtype=tf.string)\r\n    x = tf.stack(x)\r\n    x = tf.reshape(tensor=x, shape=[-1])\r\n\r\n    with tf.Session() as sess:\r\n      result = sess.run(x, {input_tensor: input})\r\n      print(result)\r\n```\r\n\r\nNow I create a unit test and try to run it:\r\n\r\n```\r\n    class SimpleTest(tf.test.TestCase):\r\n      def testMapString(self):\r\n        input = ['123', 'abc']\r\n        with self.test_session():\r\n          # Run map function\r\n          x = tf.map_fn(identity_map, input, dtype=tf.string)\r\n          x = tf.stack(x)\r\n          x = tf.reshape(tensor=x, shape=[-1])\r\n          result = x.eval()\r\n          print(result)\r\n```\r\n\r\nI get this output. Any idea what went wrong?\r\n```\r\nTraceback (most recent call last):\r\n...\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 344, in map_fn\r\n    n = array_ops.shape(elems_flat[0])[0]\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 706, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5430, in strided_slice\r\n    name=name)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2958, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2209, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2159, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 627, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"/Users/bfoo/venv3/strata/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 691, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: slice index 0 of dimension 0 out of bounds. for 'map_1/strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\n```", "comments": ["I've discovered a solution, which is always wrap your array of strings as a np.array, e.g.\r\n\r\nnp.array(['123', 'abc']) instead of ['123', 'abc']\r\n\r\nWould appreciate some clarity though on why one works and not the other. Thanks!", "In the code snippet provided, `tf.map_fn` is being called on `input_tensor`, which wasn't defined in the snippet. But given that it's being fed into the `sess.run()`, I'm guessing it corresponds to a placeholder tensor, which, I think, means the original code was more like this:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef identity_map(input):\r\n  return input\r\n\r\n# Using tf.Session() and sess.run(). This runs without errors.\r\ninput = ['123', 'abc']\r\ninput_tensor = tf.placeholder(tf.string)\r\nx = tf.map_fn(identity_map, input_tensor, dtype=tf.string)\r\nx = tf.stack(x)\r\nx = tf.reshape(tensor=x, shape=[-1])\r\n\r\nwith tf.Session() as sess:\r\n  result = sess.run(x, {input_tensor: input})\r\n  print(result)\r\n```\r\n\r\nWriting the test the same way works as well:\r\n\r\n```python\r\nclass SimpleTest(tf.test.TestCase):\r\n  def testMapString(self):\r\n    input = ['123', 'abc']\r\n    with self.test_session():\r\n      # Run map function\r\n      input_tensor = tf.placeholder(tf.string)\r\n      x = tf.map_fn(identity_map, input_tensor, dtype=tf.string)\r\n      x = tf.stack(x)\r\n      x = tf.reshape(tensor=x, shape=[-1])\r\n      result = x.eval(feed_dict={input_tensor:input})\r\n      print(result)\r\n```\r\n\r\nThe behavior you see in the test should be consistent with the behavior you see outside the test. Please do let me know if I'm mistaken. That said, coming to why you get an error with:\r\n\r\n```python\r\ntf.map_fn(identity_map, ['123', '456'], dtype=tf.string)\r\n```\r\n\r\nbut don't get the error with:\r\n\r\n```python\r\ntf.map_fn(identity_map, tf.convert_to_tensor(['123', '456']), dtype=tf.string)\r\n```\r\n\r\nor\r\n\r\n```python\r\ntf.map_fn(identity_map, np.array(['123', '456']), dtype=tf.string)\r\n```\r\n\r\nThis is because in the latter two cases, the `elems` parameter to [`tf.map_fn`](https://www.tensorflow.org/api_docs/python/tf/map_fn) is a single tensor (a vector of 2 elements). While the the first case, `elems` is being interpreted as a sequence of tensors (instead of a single tensor). `tf.map_fn` requires that `elems` be either a single `Tensor` with more than 1 dimension, or `elems` be a sequence of `Tensor`s, each with more than 1 dimension.\r\n\r\nIn the erroneous case, `elems` is a sequence of scalar (0-dimensional) tensors, which is where the error is coming from.\r\n\r\nThe error message could certainly be improved, and I'll try to do that. But hopefully the response makes sense?", "Thanks, I think I got it. Basically, passing a list directly into sess.run() in a test_session() is treated as sequences of elements rather than a tensor of elements."]}, {"number": 17693, "title": "Online Hard Example Mining", "body": "Is there a plan for OHEM in `tf.estimator.train_and_evaluate` or in combo with dataset api?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Reassigning to @xiejw who wrote train_and_evaluate. I doubt there's a plan, but if you have any suggestions, we're open to them.", "I think that generally there is a need of an intereplay between estimator train process and dataset api. Other than for OHEM this kind of interaction between the two APIs could be useful for other tasks like identify labels with noise in a dataset etc.", "assign @ispirmustafa for planning. ", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @bhack \r\nWe don't have any plan re: OHEM. Could you please elaborate more about your proposal?\r\nThank you", "Yes there are different approaches but just to make an example:\r\nhttp://www.erogol.com/online-hard-example-mining-pytorch/\r\n\r\nWith high level api i.e. estimators, if we want to automate this process, we need a way to let estimators to interact with the dataset api and its iterators to handle hard cases mining.\r\n\r\nThere was also an old request on Keras: https://github.com/keras-team/keras/issues/6569\r\n\r\nSee also OHEM section in https://arxiv.org/pdf/1604.03540.pdf", "Nagging Assignee @ispirmustafa: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since there isn't any plan. Having said that @bhack if you have come up examples i'll be happy to review.", "@ispirmustafa you asked @bhack for examples and that's exactly what he provided. I'm not sure why you chose to close the issue."]}, {"number": 17692, "title": "Fix floating point exception when loading from a small events log file using the loader", "body": "Issue:  Floating point exception when using the loader to load some data from a small events log file\r\n\r\n- Fixed the bps calculation to guard against div by 0\r\n- Tested this manually with the different size logs we have from our experiment.\r\n\r\nIf you have suggestions or a recommended way to add a unit test for the loader, I'd be glad to look into it.  Please let me know. Thanks.\r\n\r\nTESTING:\r\nFor now, I have tested this manually with the logs we have from our experiment.\r\n\r\n**With Fix:**\r\n ```\r\n./loader --events=/data_location/experiments/mitoses/jan/events.out.tfevents.xyz.com --db=/db_location/tb/db/test14db --experiment_name=test --run_name=run1 --user_name=s\r\n<snipped>\r\n2018-03-29 10:58:59.092540: I tensorflow/contrib/tensorboard/db/loader.cc:116] Loaded 624,975 bytes with 31 records at 2,130,206 bps\r\n```\r\n\r\nEmpty events file: \r\n```\r\n./loader --events=test --db=/db_location/tb/db/test14db --experiment_name=test --run_name=run1 --user_name=s\r\n<snipped>\r\n2018-03-29 12:20:03.849826: I tensorflow/contrib/tensorboard/db/loader.cc:116] Loaded 0 bytes with 0 records at 0 bps\r\n```\r\n\r\n**Without Fix:**\r\n```\r\n./loader --events=/data_location/experiments/mitoses/jan/events.out.tfevents.xyz.com --db=/db_location/tb/db/test14db --experiment_name=test --run_name=run1 --user_name=s\r\n2018-03-13 11:53:35.910348: I tensorflow/contrib/tensorboard/db/loader.cc:75] Opening SQLite file: /db_location/tb/db/test14db\r\n2018-03-13 11:53:35.911059: I tensorflow/contrib/tensorboard/db/loader.cc:82] Initializing TensorBoard schema\r\n2018-03-13 11:53:35.914771: I tensorflow/contrib/tensorboard/db/loader.cc:85] Creating SummaryDbWriter\r\n2018-03-13 11:53:35.914792: I tensorflow/contrib/tensorboard/db/loader.cc:91] Loading TF event log: /data_location/experiments/mitoses/jan/events.out.tfevents.xyz.com\r\nFloating point exception: 8\r\n```", "comments": ["Thank you for the review.  I have updated the code to use the commas and the log line like before. The bps calculation details are as listed in my earlier comment.  I used a variable to do the bps computation instead as it looked a bit more readable than putting the ternary code in the log line itself.", "There is a timeout in the CI test and I think it is not related to this change. \r\nIs it possible to get these tests to run again?   Thanks.", "Assigning to @protoget who's on sync duty.", "@protoget , The changes are reviewed and approved but it is waiting for CI tests to be rerun. Is it possible to get these tests to run again.  If there is anything else that is needed before this can get merged, please let me know. Thanks for your help. ", "I'm seeing a test failure on MacOS that may be related-- \r\n\r\n```\r\ntensorflow/contrib/lite/profiling/profiler_test.cc:34\r\nThe difference between expected_ms and duration_ms is 10.062999999999988, which exceeds eps_ms, where\r\nexpected_ms evaluates to 500,\r\nduration_ms evaluates to 510.06299999999999, and\r\neps_ms evaluates to 10.\r\n```\r\n@skambha would that be expected from your changes? @shashishekhar , can you comment as the author of that test?", "@karmel,  Thanks for triggering a test run and looking into this. \r\nI think that failure is not related to this pr's changes.  Also, fwiw, I ran the test on my macOS and it passed.  ", "Thanks, @skambha -- rerunning now.", "Nagging Assignee @protoget: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17691, "title": "tf 1.4 and tf1.5 and tf1.6 ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  No\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  \r\nMac OS\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npip install from binary\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.4 and 1.6 \r\n\r\n- **Python version**: \r\n2.7 \r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n\r\n### Describe the problem\r\nI had a model trained (with multiple days) using tf1.4, and I want to load it in tf1.6.  and the error occurs as below: \r\n\r\n  File \"/Users/iyukun/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2427, in set_shapes_for_outputs\r\n    return _set_shapes_for_outputs(op)\r\n  File \"/Users/iyukun/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2400, in _set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/Users/iyukun/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2330, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/Users/iyukun/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 627, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"/Users/iyukun/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 691, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Dimensions must be equal, but are 900 and 600 for 'dynamic_seq2seq/decoder/decoder/while/BeamSearchDecoderStep/multi_rnn_cell/cell_1/basic_lstm_cell/MatMul' (op: 'MatMul') with input shapes: [?,900], [600,1200].\r\n\r\nLooks like the MatMul op is changed from float16 to float32 in tf 1.5+?  If so,  is there a way to load the model in tf1.5+? \r\n", "comments": ["when I updata tensorflow1.6, I updata python3, for  reference.", "Hi @iyukuni ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.6  versions as many features and bug fixes has been done in newer versions and create a new  issue still persists in newer versions. Please follow this thread for installation in[ Mac.](https://developer.apple.com/metal/tensorflow-plugin/)Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 17690, "title": "Add zero padding option to `tf.image.extract_glimpse`", "body": "There has been several feature request in #16663 and https://github.com/tensorflow/tensorflow/issues/2134#issuecomment-326071624 asking for zero padding option to `tf.image.extract_glimpse`.\r\n\r\nThis fix is an attempt to address the issue through:\r\n1. Add an additional args of `noise` so that it is possible to specify `uniform`, `gaussian`, or `zero`.\r\n2. In case `zero` is specified, extract_glimpse will fill padding with zeros.\r\n\r\nNote this PR keeps the original arg `uniform_noise` intact (unless it conflict with `noise`), though it is possible to deprecate the `uniform_noise` if desired.\r\n\r\nThis fix fixes #16663.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Note: Not sure if it is better to have `ExtractGlimpseV2` instead?", "Thanks @benoitsteiner for the review. The PR has been updated. Please take a look.", "@benoitsteiner -- Can you please take a look at the updates from @yongtang  and approve if they LGTY?", "Please resolve conflicts.", "@rmlarsen Thanks for the review. The PR has been rebased and with comments addressed. Please take a look and let me know if there are any other issues.", "(For API review)\r\n\r\nCan't this break callers that use only positional arguments (since a new argument was inserted before the last one, `name`)? If so, let's add a wrapper (just for v1) to prevent that, i.e., where the new argument is after `name` in the Python API.", "@asimshankar Thanks for the review. The PR has been updated so that v1 and v2 exposed differently. Please take a look and let me know if there are any issues.", "@yongtang sorry for the delay. Could you please resolve the conflicts?", "Thanks @rmlarsen, the PR has been rebased and updated. Please take a look.", "Ping @rmlarsen to  take another look. Any other issues that need to address?", "@martinwicke This PR has been approved a month ago, but is pending api review.\r\n\r\nBefore this PR the noise could only be either `gaussian` or `uniform`, which controlled by a boolean argument `uniform_noise`. Since the argument is boolean, there is no way to expand to other noise (like zero noise requested by the original issue):\r\n```\r\n# Current signature before this PR:\r\ndef extract_glimpse(\r\n    input,\r\n    size,\r\n    offsets,\r\n    centered=True,\r\n    normalized=True,\r\n    uniform_noise=True,\r\n    name=None):\r\n```\r\n\r\nThis PR adds an additional argument `noise` which is a string, so it is possible to expand to other noise types like `zero` (and potentially more in the future).\r\n```\r\n# For updated 1.x after PR:\r\ndef extract_glimpse(\r\n    input,\r\n    size,\r\n    offsets,\r\n    centered=True,\r\n    normalized=True,\r\n    uniform_noise=True,\r\n    name=None,\r\n    noise=\"\"):\r\n```\r\n\r\nThe above changes is backward-compatible in 1.x, though the two argument `uniform_noise` and `noise` are actually overlapping.\r\n\r\nI am thinking in 2.0, maybe we could drop the `uniform_noise` completely? It is more convenient to keep only `noise` argument.\r\n\r\nSince 2.0 is coming soon, maybe it is time to just use `noise` (and drop `uniform_noise` in 2.0), like:\r\n```\r\n# For 2.0\r\ndef extract_glimpse(\r\n    input,\r\n    size,\r\n    offsets,\r\n    centered=True,\r\n    normalized=True,\r\n    noise=\"\",\r\n    name=None):\r\n```\r\n\r\nIf this seems reasonable, I can update the PR so that 1.x will remain the same old API signature, but in 2.0 we use `noise` and drop `uniform_noise`.\r\n\r\n\r\n", "Thank you for pinging this. I agree with you on the 2.0 signatures. Can you change the default for noise to explicitly be \"uniform\" (since this is the default, right?) \r\n\r\nOtherwise it looks good to me. If you could update the PR that would be great.\r\n\r\n@tensorflow/api-owners FYI.", "Thanks @martinwicke, the PR has been updated as discussed.\r\n\r\nThe `Sanity` is failing though it is unrelated (due to swift build):\r\n```\r\nERROR: error loading package 'tensorflow/lite/experimental/swift': Extension file not found. Unable to load file '@bazel_skylib//lib:new_sets.bzl': file doesn't exist or isn't a file\r\n```\r\n", "@saeta do you know about this?", "Thanks @martinwicke for the review. I have added the transformation of `extract_glimpse` in ` tools/compatibility/`, and updated the PR.\r\n\r\nPlease take a look and let me know if there are any issues.", "Thanks @martinwicke for the review. The PR has been updated. Please take a look.", "@yifeif can we turn off the \"Experimental clang-format Check\"?", "@martinwicke yep!", "@martinwicke It looks like there is a merge conflict. I am wondering if I should resolve the merge conflict and update the PR, or wait for the `import/copybara` to complete?", "Yes, please resolve the conflict. The import won't succeed while it is\nthere.\n\n>\n", "Thanks @martinwicke. The PR has been rebased with conflict resolved."]}, {"number": 17689, "title": "Installation Problem", "body": ">>> import numpy\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcudnn.so.6: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\nWhat's the problem?Please give me solution.Ubuntu 16.04Lts and GPU NVIDIA 1050 Ti 4GB,CUDA -8,CuDNN-5.1", "comments": ["Have you tried the solutions listed on this stackoverflow post? https://stackoverflow.com/questions/42013316/after-building-tensorflow-from-source-seeing-libcudart-so-and-libcudnn-errors", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Yes the Stack Overflow link above would most likely solve that. If you still feel there's a bug in TensorFlow, please fill out the information in the [Issue Template](https://github.com/tensorflow/tensorflow/issues/new)."]}, {"number": 17688, "title": "Branch 188893722", "body": "", "comments": []}, {"number": 17687, "title": "Unable to match the last equations from the nmt paper in the nmt implementation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.4.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: not compiled from source\r\n- **GCC/Compiler version (if compiling from source)**: not compiled from source\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: 8GB x 4 GTX 1080\r\n- **Exact command to reproduce**: N/A\r\n\r\nI have already asked the question over [tensorflow/nmt](https://github.com/tensorflow/nmt/issues/271) but also asking here to maybe grab some traction:\r\n\r\nIn the [NMT paper, pg 13-14](https://arxiv.org/pdf/1409.0473.pdf), they've defined the model architecture. In the Decoder section (A 2.2), they've proposed probability of a target word w.r.t the deep output (ti~) with a single maxout hidden layer.\r\n\r\nBut if I look at the source code where the [graph is built](https://github.com/tensorflow/nmt/blob/master/nmt/model.py#L274), it calls the [_build_decoder](https://github.com/tensorflow/nmt/blob/master/nmt/model.py#L358) function where the decoder section is built, I could not find the implementation of these equations: \r\n\r\n![image](https://user-images.githubusercontent.com/15987266/37344796-5b646920-26f1-11e8-999b-f9249b0cff51.png)\r\n\r\nCan someone clarify as to how those equations are included in the nmt implementation?", "comments": ["tensorflow/nmt is a better place to ask this question.\r\n\r\n"]}, {"number": 17686, "title": "Sfu2/numa test 2", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 17685, "title": "Fix format issue in ffmpeg", "body": "This fix tries to address the issue raised in #17533 where some ouput generated by ffmpeg may not be parsed correctly.\r\n\r\nThere are two issues in the ffmpeg parser:\r\n1. The size of the frame could be `rgb24, 640x360 [SAR 1:1 DAR 16:9]`, with extra `[...]` after `640x360`.\r\n2. The number of frames could be `frame=12488` or `frame=  166`, with or without the sapce (` ` in between `frame=` and the number).\r\n\r\nThis fix fixes the parser issues in ffmpeg.\r\n\r\nThis fix fixes #17533.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17684, "title": "Error converting the model to TF Lite", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04.4\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  r1.6 commit: cbc658095ae228f2f557af47e4901d552573aa15\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n- **CUDA/cuDNN version**: N/A (build without support CUDA)\r\n- **GPU model and memory**: N/A (build without support CUDA)\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nTrained model, successfully froze, it works on the tensorflow android, using TensorFlowInferenceInterface.\r\nI try to convert this into a TF Lite format, but I get an error.\r\n\r\n### Source code / logs\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n    --input_file=./test_model/frozen_graph.pb \\\r\n    --input_format=TENSORFLOW_GRAPHDEF \\\r\n    --output_file=./test_model/unet.tflite \\\r\n    --output_format=TFLITE \\\r\n    --input_array='input' \\\r\n    --input_data_type=FLOAT \\\r\n    --input_shape=2,192,320,1 \\\r\n    --inference_type=FLOAT \\\r\n    --inference_input_type=FLOAT \\\r\n    --output_array='final/Sigmoid' \\\r\n    --v=1\r\n```\r\n```\r\n2018-03-13 21:07:12.711948: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 282 operators, 479 arrays (0 quantized)\r\n2018-03-13 21:07:12.716274: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 282 operators, 479 arrays (0 quantized)\r\n2018-03-13 21:07:12.716893: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:86] Check failed: mean_shape.dims() == multiplier_shape.dims()\r\nAborted (core dumped)\r\n```", "comments": ["For example, such commands work successfully\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph\\\r\n    \u2014input_graph=./test_model/model/unet.pb \\\r\n    \u2014input_checkpoint=./test_model/model/unet \\\r\n    \u2014input_binary=true \\\r\n    \u2014output_graph=./test_model/frozen_graph.pb \\\r\n    \u2014output_node_names='final/Sigmoid'\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n    \u2014in_graph=./test_model/frozen_graph.pb \\\r\n    \u2014out_graph=./test_model/optimize_for_deployment.pb \\\r\n    \u2014inputs='input' \\\r\n    \u2014outputs='final/Sigmoid' \\\r\n    \u2014transforms='\r\n        strip_unused_nodes(type=float, shape=\"2,192,320,1\")\r\n        remove_nodes(op=Identity, op=CheckNumerics)\r\n        fold_constants(ignore_errors=true)\r\n        fold_batch_norms\r\n        fold_old_batch_norms'\r\n```\r\nAnd all models work on android, using TensorFlowInferenceInterface.", "@andrehentz Do you have some insight into this?", "[frozen_graph.zip](https://github.com/tensorflow/tensorflow/files/1847713/frozen_graph.zip)\r\nHere is an example of a frozen graph\r\n", "@Neargye I get the same problem when I try converting the mobilenet_v1 to a .tflite mode. The mobilenet_v1 which I used is in https://github.com/tensorflow/models/tree/master/research/slim/nets.\r\nI have tried you solution above,but it does not solve my problem. Is there any ops don't supported by tflite? @poxvoculi \r\nMany thanks.", "I have a very similar issue here. Graph froze successfully but getting the same error\r\n\r\nI0413 09:45:28.332509   34490 graph_transformations.cc:39] Before Removing unused ops: 282 operators, 471 arrays (0 quantized)\r\nI0413 09:45:28.360260   34490 graph_transformations.cc:39] After Removing unused ops pass 1: 277 operators, 464 arrays (0 quantized)\r\nI0413 09:45:28.395511   34490 graph_transformations.cc:39] Before general graph transformations: 277 operators, 464 arrays (0 quantized)\r\nF0413 09:45:28.397012   34490 resolve_batch_normalization.cc:86] Check failed: mean_shape.dims() == multiplier_shape.dims() \r\n", "You should not use graph.pbtxt(produced when training) to froze graph. You should use a eval.pbtxt to frozen_graph. Just like ziped files (each file contains a eval.pbtxt)in https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md", "I think [this](https://github.com/tensorflow/tensorflow/issues/15336)  might be the cause of the issue. Unfortunately, the issue template was never filled out so the issue is closed. I just started working with TensorFlow this week, I might be wrong. \r\n\r\nThe issue describes an error related to ResolveBatchNormalization input dimentions. \r\n\r\nI'm getting the same error as above :\r\n`2018-04-15 20:01:41.180669: F tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:86] Check failed: mean_shape.dims() == multiplier_shape.dims()\r\nAbort trap: 6` ", "I removed the training nodes, and got the following error\r\n\r\n```\r\n2018-04-16 00:48:55.759638: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.760237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.760341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.760466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.760565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.760653: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.760772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.760872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.760999: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.761141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.761246: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.761347: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.761479: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.761581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.761680: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.761807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.761908: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.762006: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.762126: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.762216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.762289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.762375: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.762435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.762493: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.762582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.762642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.762699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.762786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.762848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.762907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.763031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.763093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.763152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.763239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.763299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.763357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.763477: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.763538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.763671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.763760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.763820: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.763904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.764060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.764161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.764260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.764380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.764480: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.764569: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.764721: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.764821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.764922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.765042: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Prod\r\n2018-04-16 00:48:55.765143: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: SquaredDifference\r\n2018-04-16 00:48:55.765231: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1171] Converting unsupported operation: Reciprocal\r\n2018-04-16 00:48:55.769814: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 560 operators, 829 arrays (0 quantized)\r\n2018-04-16 00:48:55.781042: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 560 operators, 829 arrays (0 quantized)\r\n2018-04-16 00:48:55.790922: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 483 operators, 748 arrays (0 quantized)\r\n2018-04-16 00:48:55.799834: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 483 operators, 748 arrays (0 quantized)\r\n2018-04-16 00:48:55.805886: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 7864448 bytes, theoretical optimal value: 7864384 bytes.\r\n2018-04-16 00:48:55.808726: F tensorflow/contrib/lite/toco/tflite/export.cc:304] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: CAST, Prod, RSQRT, Reciprocal, SquaredDifference, Stack, TensorFlowShape, TensorFlowSquare, TensorFlowSum, TransposeConv.\r\nAborted (core dumped)\r\n```\r\n@GarryLau  I can not use this version TF Lite, because the necessary operations have not been implemented yet?", "@GarryLau how can I generate eval.pbtxt? I tried running mobilenet_v1_eval with latest checkpoint but it is not gearing anything.", "@smitshilu @Neargye The method to generate eval.pbtxt is in the following:\r\n```python\r\ndef export_eval_pbtxt():\r\n  \"\"\"Export eval.pbtxt.\"\"\"\r\n  with tf.Graph().as_default() as g:\r\n    images = tf.placeholder(dtype=tf.float32,shape=[None,224,224,3])\r\n    # using one of the following methods to create graph, depends on you\r\n    #_, _ = mobilenet_v1.mobilenet_v1(inputs=images,num_classes=NUM_CLASSES, is_training=False)\r\n    with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=False,regularize_depthwise=True)):\r\n      _, _ = mobilenet_v1.mobilenet_v1(inputs=images, is_training=False, depth_multiplier=1.0, num_classes=NUM_CLASSES)\r\n    eval_graph_file = '/home/garylau/Desktop/mobilenet_v1/mobilenet_v1_eval.pbtxt'\r\n    with tf.Session() as sess:\r\n        with open(eval_graph_file, 'w') as f:\r\n            f.write(str(g.as_graph_def()))\r\n```\r\nThen, call the function to generate eval.pbtxt.\r\nHope to help you.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm facing the same problem. Can batch normalization currently not be used?\r\n```\r\nF tensorflow/contrib/lite/toco/graph_transformations/resolve_batch_normalization.cc:86] Check failed: mean_shape.dims() == multiplier_shape.dims() \\nAborted (core dumped)\r\n```", "Maybe this holds only for me but I got it solved. I defined the graph using keras. The problem was I called K.learning_phase(0) *after* defining the graph. This led to the above error. When calling K.learning_phase(0) before defining the model it works :)", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I tried the conversion with the provided frozen_graph.zip and it seems to work now. Please reopen if you still encounter issues.", "@GarryLau \r\nHi Garry, \r\n\r\nI am able to use the code you provided to create the eval.pbtxt. \r\nBut I still don't understand how to use eval pbtxt for freezing.  \r\nIs the graph retrieved from the checkpoint file? If I need to use the eval pbtxt, should I use the eval gbtxt directly in the freeze script? \r\nFollowing is the code I used to freeze my model. \r\n```\r\nimport tensorflow as tf\r\n from tensorflow.python.framework import graph_util\r\n import os,sys\r\n output_node_names = \"MobilenetV1/Predictions/Reshape\"\r\n saver = tf.train.import_meta_graph('/home/users/saman/yitao/tensorflow_android/models/research/slim/batch_32/model.ckpt-156300.meta', clear_devices=True)\r\n \r\n graph = tf.get_default_graph()\r\n input_graph_def = graph.as_graph_def()\r\n sess = tf.Session()\r\n saver.restore(sess, \"/home/users/saman/yitao/tensorflow_android/models/research/slim/batch_32/model.ckpt-156300\")\r\n output_graph_def = graph_util.convert_variables_to_constants(\r\n             sess, # The session is used to retrieve the weights\r\n             input_graph_def, # The graph_def is used to retrieve the nodes\r\n             output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n )\r\n output_graph=\"frozen-model-conv6-bat-32.pb\"\r\n with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n     f.write(output_graph_def.SerializeToString())\r\n \r\n sess.close()\r\n```\r\n\r\n\r\n", "@ychen404\r\n**freeze_graph**:\r\n`bazel-bin/tensorflow/python/tools/freeze_graph  \\\r\n--input_graph=/home/lg/Desktop/inception_v3/inception_v3_eval.pbtxt \\\r\n--input_checkpoint=/home/lg/Desktop/inception_v3/checkpoint/model.ckpt-20000 \\\r\n--input_binary=false \\\r\n--output_graph=/home/lg/Desktop/inception_v3/frozen_inception_v3_299.pb  \\\r\n--output_node_names=InceptionV3/Predictions/Reshape_1  \\\r\n--checkpoint_version=2`\r\n**toco(float)**:\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=/home/lg/Desktop/inception_v3/frozen_inception_v3_299.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF  \\\r\n--output_format=TFLITE  \\\r\n--output_file=/home/lg/Desktop/inception_v3/frozen_graph_inception_v3.tflite \\\r\n--inference_type=FLOAT  \\\r\n--input_type=FLOAT \\\r\n--input_arrays=Placeholder  \\\r\n--output_arrays=InceptionV3/Predictions/Reshape_1  \\\r\n--input_shapes=1,299,299,3`\r\n**toco(QUANTIZED_UINT8)**:\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=/home/lg/Desktop/inception_v3/frozen_inception_v3_299.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF  \\\r\n--output_format=TFLITE  \\\r\n--output_file=/home/lg/Desktop/inception_v3/frozen_graph_inception_v3.tflite \\\r\n--inference_type=QUANTIZED_UINT8  \\\r\n--input_type=QUANTIZED_UINT8 \\\r\n--input_arrays=Placeholder  \\\r\n--output_arrays=InceptionV3/Predictions/Reshape_1  \\\r\n--input_shapes=1,299,299,3 \\\r\n--default_ranges_min=0.0 \\\r\n--default_ranges_max=255.0`", "@ychen404\r\n**freeze_graph**:\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph  \\\r\n--input_graph=/home/lg/Desktop/inception_v3/inception_v3_eval.pbtxt \\\r\n--input_checkpoint=/home/lg/Desktop/inception_v3/checkpoint/model.ckpt-20000 \\\r\n--input_binary=false \\\r\n--output_graph=/home/lg/Desktop/inception_v3/frozen_inception_v3_299.pb  \\\r\n--output_node_names=InceptionV3/Predictions/Reshape_1  \\\r\n--checkpoint_version=2\r\n```\r\n**toco(float)**:\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=/home/lg/Desktop/inception_v3/frozen_inception_v3_299.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF  \\\r\n--output_format=TFLITE  \\\r\n--output_file=/home/lg/Desktop/inception_v3/frozen_graph_inception_v3.tflite \\\r\n--inference_type=FLOAT  \\\r\n--input_type=FLOAT \\\r\n--input_arrays=Placeholder  \\\r\n--output_arrays=InceptionV3/Predictions/Reshape_1  \\\r\n--input_shapes=1,299,299,3\r\n```\r\n**toco(QUANTIZED_UINT8)**:\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=/home/lg/Desktop/inception_v3/frozen_inception_v3_299.pb \\\r\n--input_format=TENSORFLOW_GRAPHDEF  \\\r\n--output_format=TFLITE  \\\r\n--output_file=/home/lg/Desktop/inception_v3/frozen_graph_inception_v3.tflite \\\r\n--inference_type=QUANTIZED_UINT8  \\\r\n--input_type=QUANTIZED_UINT8 \\\r\n--input_arrays=Placeholder  \\\r\n--output_arrays=InceptionV3/Predictions/Reshape_1  \\\r\n--input_shapes=1,299,299,3 \\\r\n--default_ranges_min=0.0 \\\r\n--default_ranges_max=255.0\r\n```", "Hi GarryLau, \r\n\r\nIt's working now! Thank you very much!\r\n\r\n\r\n", "Hello GarryLau,\r\nThanks for sharing the commands,\r\nI am trying to convert my Keras model to the tflite model (8 bit quantized).\r\nI am facing the issue when I use the **--inference_type from FLOAT to QUANTIZED_UINT8** \r\n\r\nWhen I use :\r\n```\r\ntoco \\\r\n  --graph_def_file=./my_model.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  \\\r\n  --output_format=TFLITE  \\\r\n  --output_file=.//my_model.tflite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shapes=\"1, 1500, 3\" \\\r\n  --input_arrays=input_1 \\\r\n  --output_arrays='bottleneck/Elu' \\\r\n  --std_dev_values=128.0 --mean_values=0 \\\r\n  --allow_custom_ops \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=255.0 \r\n```\r\nIt generates the tflite file, but without weights quantization.\r\n\r\nOnce I change the --inference_type to QUANTIZED_UINT8 I get the abort, some logs truncated below:\r\n\r\n```\r\n2019-11-14 16:34:35.800516: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After default min-max range propagation graph transformations pass 1: 131 operators, 326 arrays (1 quantized)\r\n2019-11-14 16:34:35.803427: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 131 operators, 326 arrays (1 quantized)\r\n2019-11-14 16:34:35.803555: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type (Unsupported TensorFlow op: QuantizeV2) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f7450bf5b80 (most recent call first):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/home/superuser/.local/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/superuser/.local/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/usr/local/bin/toco_from_protos\", line 8 in <module>\r\nAborted (core dumped)\r\n```\r\n\r\nI want to use Post training quantization for my model, but it failed in all respect. Could you please help me?\r\n\r\nMy model architecture is as below:\r\n![Untitled](https://user-images.githubusercontent.com/13482732/68872172-df3c8780-06fd-11ea-8e23-10589490a56a.png)\r\n\r\n\r\nThanks in advance.\r\n\r\nKind Regards\r\nArun\r\n", "Hey Arun,\r\n\r\nI feel the issues is with TensorFlow version that you are using, there might be an issue with toco_protos if you install TensorFLow alternate sources.  I have tried it my self using 1.15, simplest way to try it yourself is to use Google colab and generate 8 bit quantized tflite file. \r\n\r\nHope this answers your question, feel free to reach out to me. \r\n\r\nBest, \r\nNaveen Dodda ", "Dear Naveen,\r\nI am too able to generate the tflite file (post quantization) but the only catastrophe is, the file is not 8-bit quantized. Could you share the Google colab code to do it. As first time, I am using APIs for Google Colab.\r\nHow it's different than the system installation and environment ?\r\n\r\nI am in parallel doing from beginning to create the quantization aware training, to avoid post-training quantization problems.\r\n\r\nKind Regards\r\nArun\r\n\r\n", "Hello Arun, \r\n\r\nThanks for checking back, could you share me details of your input and output ? or what have been using for conevrter.representative_dataset. I can try to perform 8 bit quatization from my end and wil be happy to sahre my colab code with you. \r\n\r\nIts not very diffrent from system installtion vesrion of TensorFlow but in general colab would be common environment. So we can make sure we are using official version and  it would easy to troubleshoot for third person like me intead of understanding your local environment. \r\n\r\nTrying out quatization aware traning could be a great option ; but for few models post-training quatization could be eaiser (transfer leraning or using a pre trained model)\r\n\r\nWill be happy to answer you question if you have more,\r\n\r\nBest, \r\nNaveen Dodda  \r\n"]}, {"number": 17683, "title": "graph_def.ParseFromString>google.protobuf.message.DecodeError: Error parsing message", "body": " I have the model fine tuned Inception model with my images set data by tensorflow/examples/image_retraining/retrain.py. Then, running python tensorflow/examples/label_image/label_image.py  for classification the custom image I get error:        `graph_def.ParseFromString(f.read())`\r\n`google.protobuf.message.DecodeError:` Error parsing message.\r\nTensorflow version 1.6.0 , GPU build. \r\nI have other fine tuned Inception model  that running without this error:\r\n` 95781819 Mar 12 19:37 carvajal_model.pb`  * this run with Parser error\r\n` 87458742 Mar  1 14:33 flow_model.pb`\r\n` 87622663 Mar  2 17:58 grocery_model.pb`\r\nThere is not a  big difference  the size of the model files , the problem model file has size 95m and well running models have  87m.\r\n\r\nTrace >\r\n`Traceback (most recent call last):\r\n  File \"./label_image.py\", line 117, in <module>\r\n    graph = load_graph(model_file)\r\n  File \"./label_image.py\", line 30, in load_graph\r\n    graph_def.ParseFromString(f.read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message`\r\n\r\nAre some advices about ?", "comments": ["@AlexDag, were you able to find a solution. I'm facing the same issue.", "@gmaggess , the my case was easy> by error, it was used the model from tensorflow retrain.py message:\"INFO:tensorflow:SavedModel written to: b'/tmp/saved_models/1/**saved_model.pb'\"**. The fine tuned model  is **/tmp/output_graph.pb**", "Sorry, I don't understand, can you tell me how you solve this problem in detail? thanks", "@Jason-xin,  for the inferencing(object classification)  we need fine tuning model, the binary file. It was made by train.py on folder /tmp/***(as my train process config) . I had error by using /tmp/saved_models/1/saved_model.pb. The tuned model for object classification is /tmp/output_graph.pb.", "For me, I was trying to use `saved_model/saved_model.pb`, which gives that protobuf error. I had to use `frozen_inference_graph.pb`, and then I was able to load it. (I'm using the UFF conversion library, it also calls something similar to `graph_def.ParseFromString(f.read())`)", "is anyone solved this problem??\r\nI have exactly same problem.\r\n\r\nin my case,\r\nI have my pytorch model and converted to onnx model.\r\nNextly, it is converted to tf2.3.1 saved_model.pb (including variables folder with data files) type model and I checked it ran perfectly.\r\n\r\nNow, I converted it to frozen.pb type model following here: https://stackoverflow.com/questions/59657166/convert-frozen-model-pb-to-savedmodel\r\nand finally got the frozen_model.pb file.\r\n\r\nNow, I am trying to run frozen_model.pb on my tensorflow 2.3.1 but got same error.\r\n\r\n<pre><code>\r\n...\r\ngraph_def = tf.compat.v1.GraphDef()\r\nloaded = graph_def.ParseFromString(open('path/to/frozen/pb/file/frozen_model.pb','rb').read())\r\n...\r\n</code></pre>\r\n\r\nand got following error\r\n\r\n<pre><code>\r\nTraceback (most recent call last):\r\n  File \"run_frozen.py\", line 392, in <module>\r\n    loaded = graph_def.ParseFromString(open(PB_MODEL_PATH,'rb').read())\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n</code></pre>\r\n\r\nis anyone solved this problem?", "@seongkyun \r\nDid you solve this issue? "]}, {"number": 17682, "title": "Wrong Library", "body": "", "comments": ["Sorry, wrong library!"]}, {"number": 17681, "title": "Fix warning in embedding_ops", "body": "This fix fixes the warning in embedding_ops.py by switching from keep_dims to keepdims for tf.reduce_prod\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17680, "title": "1.7.0rc1 cherry-pick request: Fixes a race condition in function instantiation.", "body": "This cherry pick fixes a bug that was discovered independently by multiple TPU users after the release branch was cut. The main consequence of *not* including this change in the release would be flaky crash failures (segmentation faults) in the input pipeline.\r\n\r\n---\r\n\r\nPreviously, if the same function was being concurrently instantiated\r\nand released:\r\n\r\n1. Thread one could begin to instantiate the function, determine\r\n   that it already existed in the runtime, then be preempted.\r\n2. Thread two could release the handle on the function, causing it to\r\n   be freed and removed from the `FunctionLibraryRuntime::items_` map.\r\n3. Thread one could then incorrectly assume that the function still\r\n   existed, and fail to find it in the `FunctionLibraryRuntime::items_`\r\n   map, causing a segfault when it attempted to increment the refcount\r\n   on an uninitialized object.\r\n\r\nPiperOrigin-RevId: 188661500", "comments": ["The error in the Ubuntu Python2 build seems to be unrelated, mentioning `tensorflow/tools/graph_transforms/sparsify_gather.pic.o`:\r\n\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/tools/graph_transforms/BUILD:89:1: Couldn't build file tensorflow/tools/graph_transforms/_objs/transforms_lib/tensorflow/tools/graph_transforms/sparsify_gather.pic.o: error while parsing .d file: /home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/tools/graph_transforms/_objs/transforms_lib/tensorflow/tools/graph_transforms/sparsify_gather.pic.d (No such file or directory)\r\n```\r\n\r\nIs it possible that this failure could already be occurring on the release branch?", "There was some infra issue we have been seeing, which should be fixed by now. Giving it another try now.", "Thanks for that! Looks like the next run was confounded by a flaky test, which should also be unrelated:\r\n\r\n```\r\n//tensorflow/core/platform/cloud:ram_file_block_cache_test               FAILED in 543.8s\r\n```", "It also looks like the Ubuntu CC build is frozen or its notification was lost, since it [seems to have completed](https://fusion.corp.google.com/runanalysis/test/prod2%3Atensorflow%2Fgithub%2Fubuntu%2Fcc%2Fpresubmit/prod2%3Atensorflow%2Fgithub%2Fubuntu%2Fcc%2Fpresubmit/KOKORO/bde06036-bc90-4058-9ad2-c01f13b2a7e7/1520971760015/prod2%3Atensorflow%2Fgithub%2Fubuntu%2Fcc%2Fpresubmit%20Build%20%232910/Targets). I'll kick off another run.", "Now it's the Mac build's turn to flake:\r\n\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/zlib_archive/BUILD:17:1: Couldn't build file external/zlib_archive/_objs/zlib/external/zlib_archive/crc32.o: C++ compilation of rule '@zlib_archive//:zlib' failed: Unexpected IO error.: xcode-locator failed with code 1.\r\n```\r\n\r\nRunning again....", "It passed! :tada:"]}, {"number": 17679, "title": "Error with combination of 'numpy_input_fn' and 'tf.contrib.seq2seq.AttentionWrapper'", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4 and 1.6 both\r\n- **Python version**: 2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8 for tensorflow 1.4 and 9.0 for tensorflow 1.6\r\n- **GPU model and memory**: gtx 1080ti\r\n- **Exact command to reproduce**: I would like to send my code via email.\r\n\r\n**Problem description:**\r\nI am now modeling an architecture that is related to encoder-decoder model. So, first of all, I wrote my own source code of encoder-decoder model with tensorflow api. the abstract structure of my source code(`model_fn` which is used for `tf.contrib.learn.Experiment`) is as follows:\r\n\r\n```\r\ndef model_fn(features, labels, mode, params):\r\n    \r\n    start_token = 1 \r\n    end_token = 2\r\n    # should pay attention to this batch_size from params\r\n    batch_size = params['batch_size']\r\n\r\n    input = features['input'] # [batch, length]\r\n    input_length = features['input_length']\r\n    \r\n    if mode != tf.estimator.ModeKeys.PREDICT:\r\n        target = features['target'] # label\r\n        target_length = features['target_length']\r\n    \r\n    # Embedding for sentence, question and rnn encoding of sentence\r\n    with tf.variable_scope('SharedScope'):\r\n        # Embedded inputs\r\n        # [batch, input_length] -> [batch, input_length, hidden_size]\r\n        embd_input = embed_op(input, params, name = 'embedded_input')\r\n        embd_target = embed_op(target, params, name = 'embedded_target')\r\n\r\n        # Build encoder cell\r\n        encoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n\r\n        # Run Dynamic RNN\r\n        #   encoder_outputs: [max_time, batch_size, num_units]\r\n        #   encoder_state: [batch_size, num_units]\r\n        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\r\n            encoder_cell, embd_input,\r\n            sequence_length=input_length,\r\n            dtype = tf.float32    \r\n            )   \r\n        \r\n    with tf.variable_scope('SharedScope/EmbeddingScope', reuse = True):\r\n        embedding_target = tf.get_variable('embedding_target')\r\n    \r\n    # Rnn decoding of sentence with attention \r\n    with tf.variable_scope('Decoder'):\r\n        # Memory for attention\r\n        attention_states = encoder_outputs\r\n\r\n        # Create an attention mechanism\r\n        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n                hidden_size, attention_states,\r\n                memory_sequence_length=input_length)\r\n\r\n        # Build decoder cell\r\n        decoder_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n\r\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n                decoder_cell, attention_mechanism,\r\n                attention_layer_size= hidden_size)\r\n\r\n        # Helper for decoder cell\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            maxlen_target = params['maxlen_target'] * tf.ones(batch_size], dtype = tf.int32)\r\n            helper = tf.contrib.seq2seq.TrainingHelper(\r\n                    embd_target, maxlen_target\r\n                    )\r\n        else: # EVAL & TEST\r\n            start_tokens = start_token * tf.ones([batch_size], dtype = tf.int32)\r\n            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n                    embedding_target, start_tokens, end_token\r\n                    )\r\n        # Decoder\r\n        initial_state = decoder_cell.zero_state(dtype = tf.float32, batch_size = batch_size)\r\n        projection_q = tf.layers.Dense(target_voca_size, use_bias = True)\r\n\r\n        decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            decoder_cell, helper, initial_state,\r\n            output_layer=None)\r\n\r\n        # Dynamic decoding\r\n        max_iter = params['maxlen_target_dev'] \r\n\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = None)\r\n        else: # Test\r\n            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations = max_iter)\r\n\r\n        logits_q = projection_q(outputs.rnn_output)\r\n.\r\n.\r\n.\r\n```\r\n\r\nand the data(`features`) are feeded through `tf.estimator.inputs.numpy_input_fn`:\r\nFor evaluation data:\r\n\r\n```\r\n    # Evaluation input function for estimator\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x = {\"input\": eval_input, 'target': eval_target, \r\n            'input_length': eval_input_length, 'len_q': eval_target_length},\r\n        y = None,\r\n        batch_size = model_params['batch_size'], # batch size that i specified\r\n        num_epochs=1,\r\n        shuffle=False)  \r\n```\r\n\r\n\r\n**When `batch_size` that I specified has lower value than 99(not including), it works fine**, I mean, when I run `neural_network_experiment.train_and_evaluate()`, it trains well and also evaluate without an error. **However, when I set `batch_size` bigger than 98, There is always an error only when evaluating(no matter with training period)** : \r\n\r\n`\r\nInvalidArgumentError (see above for traceback): assertion failed: [When applying AttentionWrapper attention_wrapper_1: Non-matching batch sizes between the memory (encoder output) and the query (decoder output).  Are you using\r\nthe BeamSearchDecoder?  You may need to tile your memory input via the tf.contrib.seq2seq.tile_batch function with argument multiple=beam_width.] [Condition x == y did not hold element-wise:] [x (QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x:0) = ] [99] [y (QuestionGeneration/LuongAttention/strided_slice_3:0) = ] [14]\r\n         [[Node: QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/All/_389, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_0, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_1, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_2, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/x/_391, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Assert/Assert/data_4, QuestionGeneration/decoder/while/BasicDecoderStep/decoder/attention_wrapper/assert_equal/Equal/Enter/_393)]]\r\n         [[Node: QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert/_384 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_397_QuestionGeneration/AttentionWrapperZeroState/assert_equal/Assert/Assert\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n`\r\n\r\nI solved this error by change the `batch_size` value in `model_fn` by `attention_mechanism._batch_size`.\r\nThe problem may caused from the redundant data that can't be divided by `batch_size`( if i have 1004 lines of data and my `batch_size` is 10, than 4 lines data will be left). So when I change the `batch_size` value in `model_fn` to `attention_mechanism._batch_size` from specified value, the last iteration's batch size will be 4 and no error anymore. About the error message above, I used batch size of 99, and there will be 14 lines of data left. Then, I want to ask for 2 questions that may related to error:\r\n\r\n1. when I fixed the `batch_size` by specified value smaller than 99, why no error(both training and evaluate)\r\n2. when I fixed the 'batch_size' by specified value bigger than 98, why no error in training period and does have error in evaluation period.\r\n\r\nI think there may be some errors in tensorflow api related to this. Or maybe I was wrong in some part.\r\n\r\nAlso, I hope to see the details of `numpy_input_fn`, such as: when used with `tf.estimator`, How will it treat the redundant data that can't be divided by batch_size", "comments": ["Sadly I just saw this issue for the first time. Is it fixed in newer versions of TF?", "Nagging Assignee @ebrevdo: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I didn't try it with the new version. I may re-open the issue after I trying with the new one."]}, {"number": 17678, "title": "Fix broken link of tf.train.Example in recurrent_quickdraw.md", "body": "In recurrent_quickdraw.md, the link `${tf.train.Example}` is not rendered correctly. This fix fixes the broken link with correct rendering. Now the link is rendered the same way as tf.train.Example in api_guides/python/reading_data.md and extend/new_data_formats.md\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17677, "title": "Tensorflow installation problem", "body": "Hello, \r\nI trie to install tensorflow on a SSH server and uisng the following command: pip install tensorflow. They give me that the installataion was Successfully done. When i execute my code i get the following error, any help will be so appreciated:\r\nTraceback (most recent call last):\r\n  ```\r\nFile \"/gs/project/tws-462-aa/Python_directory/src/TripleArray.py\", line 6, in <module>\r\n    from config import invert\r\n  File \"/gs/project/tws-462-aa/Python_directory/src/config.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/gs/project/tws-462-aa/Python_directory/ENV2.7/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/gs/project/tws-462-aa/Python_directory/ENV2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/gs/project/tws-462-aa/Python_directory/ENV2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/gs/project/tws-462-aa/Python_directory/ENV2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/gs/project/tws-462-aa/Python_directory/ENV2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/gs/project/tws-462-aa/Python_directory/ENV2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /lib64/libc.so.6: version `GLIBC_2.17' not found (required by /gs/project/tws-462-aa/Python_directory/ENV2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n", "comments": ["see #53", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@MarkDaoust are you the person to assign docs-related requests to?", "Oops wrong tab :)", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 61 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 76 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17676, "title": "Logging frequency hardcoded in tf.estimator", "body": "\r\n### Describe the problem\r\n\r\nThe logging frequency of the default LoggingTensorHook of tf.Estimator is hardcoded to 100 steps (see lines 828-837 in [https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/estimator/estimator.py]). It would really help me to unclutter my logs if I had access to this value (my training is fast).", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sounds like a problem, cc @ispirmustafa ", "https://github.com/tensorflow/tensorflow/pull/17157 is addressing it."]}, {"number": 17675, "title": "from tensorflow.contrib import *  \u5bfc\u5165\u5931\u8d25\uff0c\u6c42\u6559", "body": "\u5728\u5c06tensorflow\u5c01\u88c5\u6210c\u670d\u52a1\u7684\u8fc7\u7a0b\u4e2d\uff0c\u51fa\u73b0\u5982\u4e0b\u95ee\u9898\uff1a\r\nfrom tensorflow.contrib import * ------>\u5931\u8d25\uff08\u7531\u4e8e\u6c34\u5e73\u95ee\u9898\uff0c\u4e0d\u77e5\u600e\u4e48\u6355\u83b7\u9519\u8bef\u4fe1\u606f\uff0c\u73b0\u8c61\u5c31\u662fc\u670d\u52a1\u65e0\u6cd5\u8fd4\u56de\u6b63\u786e\u7ed3\u679c\uff09 \r\nfrom tensorflow.models import *  -------->\u6210\u529f\uff08c\u670d\u52a1\u80fd\u591f\u6b63\u5e38\u8fd4\u56de\u7ed3\u679c\uff0c\u4e0b\u540c\uff09\r\nfrom tensorflow.core import * ---------->\u6210\u529f\r\nfrom tensorflow.examples import *  --------->\u6210\u529f\r\n\r\ntensorflow\u5b89\u88c5\u5305\u4e0b\uff1acontrib\u3001models\u3001core\u3001examples\u7b49\u76ee\u5f55\u90fd\u662f\u5b58\u5728\u7684\u3002tensorflow\u7248\u672c\uff1a1.2\r\n", "comments": ["\u4f60\u6700\u597d\u628a\u4f60\u6240\u5199\u51fa\u6765\u7684\u6587\u5b57\u90fd\u7ffb\u8bd1\u6210\u82f1\u8bed\u3002\u53e6\u5916\uff0c\u6c42\u6559\u4e4b\u7c7b\u7684\u95ee\u9898\u5e94\u8be5\u5728https://stackoverflow.com/questions/tagged/tensorflow \u95ee\u3002\r\n\r\nYou'd better translate all of your problem into English. \r\nFrom the README, \"We use GitHub issues for tracking requests and bugs. So please see TensorFlow Discuss for general questions and discussion, and please direct specific questions to Stack Overflow.\" ", "\u4f1a\u4e0d\u4f1a\u662f\u56e0\u4e3acontrib\u4e2d\u4ee3\u7801\u7684\u6210\u5206\u6709\u4e0d\u540c\uff0c\u611f\u89c9\u5176\u4ed6\u51e0\u4e2a\u662f\u7eafC\u5199\u7684\uff0ccontrib\u4e2d\u53ef\u80fd\u6df7\u6742\u4e86\u4e0d\u5c11python\u4ee3\u7801\uff1f\u778e\u731c\u4ec5\u4ec5\u4f9b\u53c2\u8003.... \r\n\u540e\u7eed\u6709\u7ed3\u8bba\u544a\u77e5\u4e00\u4e0b\u54e6 ^ ^", "I don't know Chinese so I'm not sure what the issue is. But @ad26kt implied it should be asked on StackOverflow, so I'm closing the issue."]}, {"number": 17674, "title": "tf1.6 error: tf.train.BytesList(\"string\") ---> expected one of: bytes", "body": "OS Platform and Distribution-->Linux Ubuntu 14.04\r\nTensorFlow installed from-->binary\r\nTensorFlow version -->1.6\r\nPython version--> 3.5.5\r\nGCC/Compiler version -->GCC 4.8.4\r\nCUDA/cuDNN version-->cuda9/cuDNN7\r\nGPU model and memory-->Tesla K20c/Tesla K20m\r\n\r\ncode to reproduce:\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n\r\nall_record = str(\"abc\")\r\noutput_filename = \"tfrecord_test\"\r\n\r\nwriter = tf.python_io.TFRecordWriter(output_filename)\r\n\r\nexample = tf.train.Example(\r\n    features = tf.train.Features(\r\n        feature = {\r\n            \"features\": tf.train.Feature(\r\n                #int32_list = tf.train.Int32List(value = all_record)\r\n                bytes_list=tf.train.BytesList(value = all_record)\r\n            )\r\n        }\r\n    )\r\n)\r\nwriter.write(example.SerializeToString())\r\n\r\nwriter.close()\r\n```\r\n\r\nerror log:\r\n```\r\n$ python tf_record_test.py\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nv1.6.0-0-gd2e24b6039 1.6.0\r\nTraceback (most recent call last):\r\n  File \"tf_record_test.py\", line 15, in <module>\r\n    bytes_list=tf.train.BytesList(value = all_record)\r\nTypeError: 'a' has type str, but expected one of: bytes\r\n```", "comments": ["I transform \"string\" to \"bytes\" , by add \"b\" before string, but there is another error.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n\r\nall_record = b\"abc\"\r\noutput_filename = \"tfrecord_test\"\r\n\r\nwriter = tf.python_io.TFRecordWriter(output_filename)\r\n\r\nexample = tf.train.Example(\r\n    features = tf.train.Features(\r\n        feature = {\r\n            \"features\": tf.train.Feature(\r\n                #int32_list = tf.train.Int32List(value = all_record)\r\n                bytes_list=tf.train.BytesList(value = all_record)\r\n            )\r\n        }\r\n    )\r\n)\r\nwriter.write(example.SerializeToString())\r\n\r\nwriter.close()\r\n\r\n```\r\nerror log:\r\n\r\n```\r\n$ python tf_record_test.py\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nv1.6.0-0-gd2e24b6039 1.6.0\r\nTraceback (most recent call last):\r\n  File \"tf_record_test.py\", line 15, in <module>\r\n    bytes_list=tf.train.BytesList(value = all_record)\r\nTypeError: 97 has type int, but expected one of: bytes\r\n```", "Feed a list of bytes to BytesList. So instead of\r\n` bytes_list=tf.train.BytesList(value = all_record)`\r\nuse\r\n`bytes_list=tf.train.BytesList(value = [all_record])`.", "thks. it work well:\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n\r\nall_record = b\"abc\"\r\noutput_filename = \"tfrecord_test\"\r\n\r\nwriter = tf.python_io.TFRecordWriter(output_filename)\r\n\r\nexample = tf.train.Example(\r\n    features = tf.train.Features(\r\n        feature = {\r\n            \"features\": tf.train.Feature(\r\n                #int32_list = tf.train.Int32List(value = all_record)\r\n                bytes_list=tf.train.BytesList(value = [all_record])\r\n            )\r\n        }\r\n    )\r\n)\r\nwriter.write(example.SerializeToString())\r\n\r\nwriter.close()\r\n```"]}, {"number": 17673, "title": "tf1.6 error: ", "body": "OS Platform and Distribution-->Linux Ubuntu 14.04\r\nTensorFlow installed from-->binary\r\nTensorFlow version -->1.6\r\nPython version--> 3.5.5\r\nGCC/Compiler version -->GCC 4.8.4\r\nCUDA/cuDNN version-->cuda9/cuDNN7\r\nGPU model and memory-->Tesla K20c/Tesla K20m\r\n\r\ncode to reproduce(tf_record_test.py):\r\n`import tensorflow as tf\r\n\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n\r\nall_record = str(\"abc\")\r\noutput_filename = \"tfrecord_test\"\r\n\r\nwriter = tf.python_io.TFRecordWriter(output_filename)\r\n\r\nexample = tf.train.Example(\r\n    features = tf.train.Features(\r\n        feature = {\r\n            \"features\": tf.train.Feature(\r\n                #int32_list = tf.train.Int32List(value = all_record)\r\n                bytes_list=tf.train.BytesList(value = all_record)\r\n            )\r\n        }\r\n    )\r\n)\r\nwriter.write(example.SerializeToString())\r\n\r\nwriter.close()\r\n`\r\nerror log:\r\n`$ python tf_record_test.py\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nv1.6.0-0-gd2e24b6039 1.6.0\r\nTraceback (most recent call last):\r\n  File \"tf_record_test.py\", line 15, in <module>\r\n    bytes_list=tf.train.BytesList(value = all_record)\r\nTypeError: 'a' has type str, but expected one of: bytes`", "comments": []}, {"number": 17672, "title": "Added data_format to flatten", "body": "If you train a model in NCHW format but then do inference in NHWC the results are different as the reshape sets up the resulting array differently.", "comments": ["That's a reasonable change, and we can merge it. Please replicate it in `tensorflow/python/keras/_impl/keras/layers` and in the repo `keras-team/keras/` as well. Please also add a unit test for the behavior you are trying to achieve.", "Will add to the `keras-team/keras/` repo.\r\n\r\nI've added the arguments to `tensorflow/python/keras/_impl/keras/layers`, don't think anything else is needed as it defaults to the `tf.layers.Flatten` anyway.", "I have realised this implementation constrains the input to 4 dims.\r\nWill change to enable `N-dims, s.t. N > 2`.", "@fchollet Any more to do on the TF side?", "@fchollet and other comments?", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @martinwicke: It has been 36 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@joeyearsley can you resolve the conflicts?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@martinwicke No matter what I try I can't resolve the conflict with that file, is it due to that file not existing any more in the master repo? \r\n\r\nWould deleting that file fix this issue? **NM - Deleting worked**", "@joeyearsley could you pull rebase and push again?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Pull master and rebase my latest commit to master?\r\n\r\nShould I roll back a commit (undo the delete) before rebasing?", "I think this may have already been fixed at master. What happens when you rebase?", "Currently on vacation, however I can see that it\u2019s been fixed in Keras layers but the kwarg isn\u2019t passed through tf.layers.\r\nOnly a small fix now, will rebase when I\u2019m back, or would it be easier to make a new PR?", "If you can do a rebase, that may be easier, since we already have all the context here.", "Hopefully that should be done correctly @martinwicke ", "```\r\nFAIL: Found 2 non-whitelited pylint errors:\r\ntensorflow/python/layers/core_test.py:477: [C0301(line-too-long), ] Line too long (85/80)\r\ntensorflow/python/layers/core_test.py:493: [C0301(line-too-long), ] Line too long (81/80)\r\n```", "Resolved @drpngx ", "Sanity build failure is odd. I'll rerun.", "Ok, can you update the golden files? That'll fix the API test (instructions in the test log). \r\n\r\nAlso, the layers test is failing, can you take a look?", "I've fixed the tests now. \r\n\r\nI've ran the command attached to the log however I cannot tell if it has updated them or not.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It looks like the API tests are still failing-- did you run the scripts detailed at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/api/tests ? You will need to commit the changed files to this PR.", "Double-checking with @martinwicke for API review.", "Good for API review", "@karmel I followed the instructions provided, however I can't tell if it worked or not.\r\nCan you advise what file it outputs/location? - then I can check my local git to ensure it's not in the ignored files currently\r\n\r\nMeanwhile for some reason the other tests failing are:\r\n- an XLA op test which I've not touched\r\n- an timeout in `tf.lite`", "The API tests should modify a some files (`tensorflow/tools/api/golden/tensorflow.pbtxt` or similar), which then must be git committed to the fork you are working on here. Do you see those files being modified?", "This is the only output I get:\r\n\r\n> bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens true\r\nsWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n> ..\r\n> ----------------------------------------------------------------------\r\n> Ran 3 tests in 0.383s\r\n>\r\n>OK (skipped=1)\r\n\r\nAnd no updated protobufs are output.\r\n\r\nAny further ideas? - I don't understand why it's failing to update them", "You have to run the program with python2, otherwise the test will be skipped. This is so that we have a stable interface.", "Thanks @drpngx ! That was the step I was missing.", "Oh, you need to pull rebase and push again to resolve the conflict.", "Hopefully this time?", "Could you check https://source.cloud.google.com/results/invocations/452569d4-59c2-407c-875a-29752a47122e/log ? (look for `api_compatibility_test`)\r\n\r\nIn the diff `broadcast_to` seems to have disappeared. Maybe you ran in between updates. Thanks.", "@drpngx Seems to be the case! ", "@fchollet what is the state of this?", "@fchollet Any update? - This was merged into Keras months ago, I would really like to get it off my plate.", "Ok, this is weird, this should have been merged. I'll look into why it's stuck.", "I retriggered the tests. I think there's still a problem with the API goldens, could you take a look? It's probably as easy as applying this diff:\r\n\r\n```\r\n   member_method {\r\n      name: \"flatten\"\r\n-     argspec: \"args=[\\'inputs\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n+     argspec: \"args=[\\'inputs\\', \\'data_format\\', \\'name\\'], varargs=None, keywords=None, defaults=[\\'channels_last\\', \\'None\\'], \"\r\n    }\r\n```\r\nto api/golden/v2/tensorflow.layers....\r\n", "This seems to be the V2 branch? \r\nWill this not be updated frequently anyway internally as you are still making TF v2?", "It's updated whenever me make a change. \r\n\r\nYou are correct in that this particular class is likely to not survive, but we will make this change in a separate commit (in progress). Until then, we need to keep the v2 goldens up to date, we don't regenerate them in the background.", "@joeyearsley : It isn't in V2 only, it is for V1 as well. Basically, the `data_format` argument should be the last one (after `name`), otherwise existing callers to `flatten()` that use positional arguments will break (e.g., `tf.layers.flatten(x, 'foo')`)", "@martinwicke please can you re-run the tests before something else changes", "@joeyearsley : Did you see https://github.com/tensorflow/tensorflow/pull/17672#issuecomment-426088246 ? The tests will still fail since the ordering of the `name` argument has changed. Could you update that?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Ok, now I've updated to match the ordering - in future please comment on a code review so I can see exactly what line etc.\r\n\r\nHowever, if it's a kwarg it should probably be named in the test also.\r\n\r\nNow please can we get this sorted, 6 months is a bit long when it should have been updated when the code was refactored to rely on Keras' flatten layer - which has the `data_format` kwarg since I added that at the start of this PR.", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17671, "title": "concat puts gradients as 0.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: CuDA 9.0\r\n- **GPU model and memory**: 4Gb, Nvidia GTX960M\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen I write delt to get gradients of pi with respect to v, I get the gradients. Same for rem with respect to v. However, when I ask for gradients of z with respect to v, I get a matrix of 0s, which is clearly not the gradient as z is just the concatenation of pi and rem, both of which have non-zero gradients. Why is this happening?\r\n\r\n\r\n### Source code / logs\r\n\r\n\tdef latent_space(v):\r\n\t\t# Calculate pi\r\n\t\twith tf.name_scope('pi'):\r\n\t\t\tdef fn(previous_output,current_input):\r\n\t\t\t\t[stick,remaining] = previous_output\r\n\t\t\t\ti = current_input\r\n\t\t\t\tstick = v[:,i]*remaining\r\n\t\t\t\tremaining *= (1-v[:,i])\r\n\t\t\t\treturn [stick,remaining]\r\n\r\n\t\t\telems = tf.Variable(tf.range(latent-1))\r\n\t\t\t[pi,rem] = tf.scan(fn,elems,initializer=[tf.ones([bs]),tf.ones([bs])])\r\n\t\t\trem = tf.reshape(rem[-1,:],[1,bs])\r\n\t\t\tz = tf.concat([pi,rem],axis=0)\r\n\t\t\tz = tf.transpose(z)\r\n\t\t\tdelt = tf.gradients(z,v)\r\n\t\treturn z,delt", "comments": ["Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17670, "title": "Added data_format to flatten", "body": "If you train a model in NCHW format but then do inference in NHWC the results are different as the reshape sets up the resulting array differently.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 17669, "title": "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,5095,3729,3]", "body": "When I encountered the following error while performing the training, can someone help me? I am a beginner of tensorflow.\r\n\r\n`ssh://root@xxx.xxx.xxx.xxx:22/usr/local/python3/bin/python3 -u /xxx/xxx/object_detection/train.py --logtostderr\r\nWARNING:tensorflow:From /xxx/xxx/object_detection/trainer.py:210: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:depth of additional conv before box predictor: 0\r\nINFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.\r\n2018-03-13 21:57:41.631427: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-03-13 21:57:42.286105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-03-13 21:57:42.286417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2018-03-13 21:57:42.286455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from train/model.ckpt-0\r\nINFO:tensorflow:Starting Session.\r\nINFO:tensorflow:Saving checkpoint to path train/model.ckpt\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:global_step/sec: 0\r\nINFO:tensorflow:Recording summary at step 0.\r\nINFO:tensorflow:global step 1: loss = 243.3368 (9.080 sec/step)\r\nINFO:tensorflow:global step 2: loss = 220.1394 (0.513 sec/step)\r\nINFO:tensorflow:global step 3: loss = 206.3998 (0.430 sec/step)\r\nINFO:tensorflow:global step 4: loss = 195.3873 (0.486 sec/step)\r\nINFO:tensorflow:global step 5: loss = 182.0547 (0.427 sec/step)\r\nINFO:tensorflow:global step 6: loss = 166.7800 (0.434 sec/step)\r\nINFO:tensorflow:global step 7: loss = 151.5283 (0.474 sec/step)\r\nINFO:tensorflow:global step 8: loss = 135.1360 (0.429 sec/step)\r\nINFO:tensorflow:global step 9: loss = 120.5486 (0.425 sec/step)\r\nINFO:tensorflow:global step 10: loss = 105.3057 (0.499 sec/step)\r\nINFO:tensorflow:global step 11: loss = 91.5549 (0.514 sec/step)\r\nINFO:tensorflow:global step 12: loss = 78.9669 (0.434 sec/step)\r\nINFO:tensorflow:global step 13: loss = 68.9649 (0.439 sec/step)\r\nINFO:tensorflow:global step 14: loss = 59.8189 (0.423 sec/step)\r\nINFO:tensorflow:global step 15: loss = 52.2898 (0.458 sec/step)\r\nINFO:tensorflow:global step 16: loss = 44.7254 (0.418 sec/step)\r\nINFO:tensorflow:global step 17: loss = 39.1479 (0.499 sec/step)\r\nINFO:tensorflow:global step 18: loss = 35.2444 (0.468 sec/step)\r\nINFO:tensorflow:global step 19: loss = 32.0060 (0.454 sec/step)\r\nINFO:tensorflow:global step 20: loss = 27.3901 (0.437 sec/step)\r\nINFO:tensorflow:global step 21: loss = 25.8307 (0.432 sec/step)\r\nINFO:tensorflow:global step 22: loss = 23.7690 (0.454 sec/step)\r\nINFO:tensorflow:global step 23: loss = 21.4071 (0.430 sec/step)\r\nINFO:tensorflow:global step 24: loss = 20.4954 (0.590 sec/step)\r\nINFO:tensorflow:global step 25: loss = 18.6025 (0.440 sec/step)\r\nINFO:tensorflow:global step 26: loss = 17.7907 (0.434 sec/step)\r\nINFO:tensorflow:global step 27: loss = 17.7660 (0.489 sec/step)\r\nINFO:tensorflow:global step 28: loss = 16.7211 (0.512 sec/step)\r\nINFO:tensorflow:global step 29: loss = 16.5738 (0.429 sec/step)\r\nINFO:tensorflow:global step 30: loss = 15.5000 (0.490 sec/step)\r\nINFO:tensorflow:global step 31: loss = 15.0663 (0.412 sec/step)\r\nINFO:tensorflow:global step 32: loss = 14.5887 (0.442 sec/step)\r\nINFO:tensorflow:global step 33: loss = 14.8037 (0.563 sec/step)\r\nINFO:tensorflow:global step 34: loss = 13.5137 (0.489 sec/step)\r\nINFO:tensorflow:global step 35: loss = 13.0228 (0.427 sec/step)\r\nINFO:tensorflow:global step 36: loss = 13.4710 (0.428 sec/step)\r\nINFO:tensorflow:global step 37: loss = 13.7616 (0.419 sec/step)\r\nINFO:tensorflow:global step 38: loss = 13.2861 (0.427 sec/step)\r\nINFO:tensorflow:global step 39: loss = 12.2581 (0.505 sec/step)\r\nINFO:tensorflow:global step 40: loss = 12.7339 (0.429 sec/step)\r\nINFO:tensorflow:global step 41: loss = 13.2387 (0.444 sec/step)\r\nINFO:tensorflow:global step 42: loss = 12.3722 (0.439 sec/step)\r\nINFO:tensorflow:global step 43: loss = 12.8192 (0.428 sec/step)\r\nINFO:tensorflow:global step 44: loss = 12.0361 (0.488 sec/step)\r\nINFO:tensorflow:global step 45: loss = 12.1655 (0.566 sec/step)\r\nINFO:tensorflow:global step 46: loss = 11.7112 (0.425 sec/step)\r\nINFO:tensorflow:global step 47: loss = 11.3097 (0.464 sec/step)\r\nINFO:tensorflow:global step 48: loss = 12.2162 (0.435 sec/step)\r\nINFO:tensorflow:global step 49: loss = 11.7511 (0.428 sec/step)\r\nINFO:tensorflow:global step 50: loss = 11.2281 (0.425 sec/step)\r\nINFO:tensorflow:global step 51: loss = 10.9219 (0.477 sec/step)\r\nINFO:tensorflow:global step 52: loss = 11.8307 (0.430 sec/step)\r\nINFO:tensorflow:global step 53: loss = 11.4977 (0.486 sec/step)\r\nINFO:tensorflow:global step 54: loss = 11.5908 (0.436 sec/step)\r\nINFO:tensorflow:global step 55: loss = 11.7416 (0.450 sec/step)\r\nINFO:tensorflow:global step 56: loss = 10.8136 (0.441 sec/step)\r\nINFO:tensorflow:global step 57: loss = 10.9873 (0.482 sec/step)\r\nINFO:tensorflow:global step 58: loss = 11.0574 (0.470 sec/step)\r\nINFO:tensorflow:global step 59: loss = 11.4761 (0.454 sec/step)\r\nINFO:tensorflow:global step 60: loss = 11.7182 (0.427 sec/step)\r\nINFO:tensorflow:global step 61: loss = 11.6872 (0.461 sec/step)\r\nINFO:tensorflow:global step 62: loss = 10.8746 (0.462 sec/step)\r\nINFO:tensorflow:global step 63: loss = 10.6988 (0.438 sec/step)\r\nINFO:tensorflow:global step 64: loss = 10.5077 (0.615 sec/step)\r\nINFO:tensorflow:global step 65: loss = 11.0472 (0.432 sec/step)\r\nINFO:tensorflow:global step 66: loss = 10.6191 (0.444 sec/step)\r\nINFO:tensorflow:global step 67: loss = 10.3630 (0.453 sec/step)\r\nINFO:tensorflow:global step 68: loss = 10.0935 (0.565 sec/step)\r\nINFO:tensorflow:global step 69: loss = 10.5869 (0.446 sec/step)\r\nINFO:tensorflow:global step 70: loss = 10.6263 (0.435 sec/step)\r\nINFO:tensorflow:global step 71: loss = 10.4292 (0.479 sec/step)\r\nINFO:tensorflow:global step 72: loss = 10.5870 (0.474 sec/step)\r\nINFO:tensorflow:global step 73: loss = 10.6465 (0.476 sec/step)\r\nINFO:tensorflow:global step 74: loss = 10.8363 (0.559 sec/step)\r\nINFO:tensorflow:global step 75: loss = 10.9429 (0.486 sec/step)\r\nINFO:tensorflow:global step 76: loss = 10.3226 (0.419 sec/step)\r\nINFO:tensorflow:global step 77: loss = 10.7447 (0.638 sec/step)\r\nINFO:tensorflow:global step 78: loss = 10.1083 (0.491 sec/step)\r\nINFO:tensorflow:global step 79: loss = 10.7905 (0.450 sec/step)\r\nINFO:tensorflow:global step 80: loss = 10.6255 (0.657 sec/step)\r\nINFO:tensorflow:global step 81: loss = 10.9287 (0.657 sec/step)\r\nINFO:tensorflow:global step 82: loss = 10.2799 (0.439 sec/step)\r\nINFO:tensorflow:global step 83: loss = 10.1767 (0.425 sec/step)\r\nINFO:tensorflow:global step 84: loss = 10.3393 (0.639 sec/step)\r\nINFO:tensorflow:global step 85: loss = 10.6420 (0.466 sec/step)\r\nINFO:tensorflow:global step 86: loss = 10.5353 (1.067 sec/step)\r\nINFO:tensorflow:global step 87: loss = 9.7251 (0.483 sec/step)\r\nINFO:tensorflow:global step 88: loss = 9.9586 (0.491 sec/step)\r\nINFO:tensorflow:global step 89: loss = 10.5869 (0.435 sec/step)\r\nINFO:tensorflow:global step 90: loss = 10.6093 (0.428 sec/step)\r\nINFO:tensorflow:global step 91: loss = 10.6009 (1.195 sec/step)\r\nINFO:tensorflow:global step 92: loss = 10.1688 (0.654 sec/step)\r\nINFO:tensorflow:global step 93: loss = 10.5646 (0.751 sec/step)\r\nINFO:tensorflow:global step 94: loss = 10.0194 (0.449 sec/step)\r\nINFO:tensorflow:global step 95: loss = 9.9471 (0.458 sec/step)\r\nINFO:tensorflow:global step 96: loss = 10.1529 (0.429 sec/step)\r\nINFO:tensorflow:global step 97: loss = 10.2420 (0.715 sec/step)\r\nINFO:tensorflow:global step 98: loss = 10.2819 (0.435 sec/step)\r\nINFO:tensorflow:global step 99: loss = 10.1969 (0.625 sec/step)\r\nINFO:tensorflow:global step 100: loss = 9.9485 (0.478 sec/step)\r\nINFO:tensorflow:global step 101: loss = 9.9312 (0.568 sec/step)\r\nINFO:tensorflow:global step 102: loss = 10.0179 (0.415 sec/step)\r\nINFO:tensorflow:global step 103: loss = 9.9705 (0.419 sec/step)\r\nINFO:tensorflow:global step 104: loss = 9.8582 (0.484 sec/step)\r\nINFO:tensorflow:global step 105: loss = 10.4639 (0.431 sec/step)\r\nINFO:tensorflow:global step 106: loss = 10.2900 (0.413 sec/step)\r\nINFO:tensorflow:global step 107: loss = 10.3138 (0.430 sec/step)\r\nINFO:tensorflow:global step 108: loss = 9.6482 (0.461 sec/step)\r\nINFO:tensorflow:global step 109: loss = 9.5792 (0.422 sec/step)\r\nINFO:tensorflow:global step 110: loss = 9.7901 (0.466 sec/step)\r\nINFO:tensorflow:global step 111: loss = 10.4787 (0.472 sec/step)\r\nINFO:tensorflow:global step 112: loss = 10.6505 (0.434 sec/step)\r\nINFO:tensorflow:global step 113: loss = 9.7728 (0.435 sec/step)\r\nINFO:tensorflow:global step 114: loss = 9.7048 (0.431 sec/step)\r\nINFO:tensorflow:global step 115: loss = 10.3138 (0.439 sec/step)\r\nINFO:tensorflow:global step 116: loss = 9.5181 (0.428 sec/step)\r\nINFO:tensorflow:global step 117: loss = 10.2000 (0.914 sec/step)\r\nINFO:tensorflow:global step 118: loss = 10.3344 (0.462 sec/step)\r\nINFO:tensorflow:global step 119: loss = 9.5471 (0.442 sec/step)\r\nINFO:tensorflow:global step 120: loss = 9.4061 (0.427 sec/step)\r\nINFO:tensorflow:global step 121: loss = 9.5720 (0.488 sec/step)\r\nINFO:tensorflow:global step 122: loss = 9.3800 (0.462 sec/step)\r\nINFO:tensorflow:global step 123: loss = 9.4855 (0.426 sec/step)\r\nINFO:tensorflow:global step 124: loss = 8.9675 (0.526 sec/step)\r\nINFO:tensorflow:global step 125: loss = 9.4995 (0.487 sec/step)\r\nINFO:tensorflow:global step 126: loss = 9.2938 (0.539 sec/step)\r\nINFO:tensorflow:global step 127: loss = 9.7230 (0.433 sec/step)\r\nINFO:tensorflow:global step 128: loss = 9.8149 (0.510 sec/step)\r\nINFO:tensorflow:global step 129: loss = 10.1299 (0.466 sec/step)\r\nINFO:tensorflow:global step 130: loss = 9.7078 (0.477 sec/step)\r\nINFO:tensorflow:global step 131: loss = 10.0769 (1.196 sec/step)\r\nINFO:tensorflow:global step 132: loss = 9.4188 (0.543 sec/step)\r\nINFO:tensorflow:global step 133: loss = 8.9409 (0.443 sec/step)\r\nINFO:tensorflow:global step 134: loss = 9.3117 (0.472 sec/step)\r\nINFO:tensorflow:global step 135: loss = 8.9357 (0.477 sec/step)\r\nINFO:tensorflow:global step 136: loss = 10.4331 (0.681 sec/step)\r\nINFO:tensorflow:global step 137: loss = 9.3650 (0.418 sec/step)\r\nINFO:tensorflow:global step 138: loss = 9.1549 (0.455 sec/step)\r\nINFO:tensorflow:global step 139: loss = 9.2574 (0.420 sec/step)\r\nINFO:tensorflow:global step 140: loss = 9.2772 (0.448 sec/step)\r\nINFO:tensorflow:global step 141: loss = 9.6754 (0.442 sec/step)\r\nINFO:tensorflow:global step 142: loss = 10.1184 (0.602 sec/step)\r\nINFO:tensorflow:global step 143: loss = 9.1427 (0.477 sec/step)\r\nINFO:tensorflow:global step 144: loss = 9.2105 (0.754 sec/step)\r\nINFO:tensorflow:global step 145: loss = 9.1641 (0.421 sec/step)\r\nINFO:tensorflow:global step 146: loss = 9.6503 (0.490 sec/step)\r\nINFO:tensorflow:global step 147: loss = 10.1134 (0.430 sec/step)\r\nINFO:tensorflow:global step 148: loss = 9.3111 (0.475 sec/step)\r\nINFO:tensorflow:global step 149: loss = 10.0336 (0.427 sec/step)\r\nINFO:tensorflow:global step 150: loss = 8.5716 (0.463 sec/step)\r\nINFO:tensorflow:global step 151: loss = 9.5126 (0.427 sec/step)\r\nINFO:tensorflow:global step 152: loss = 9.9572 (0.480 sec/step)\r\nINFO:tensorflow:global step 153: loss = 9.0432 (0.444 sec/step)\r\nINFO:tensorflow:global step 154: loss = 8.7378 (0.500 sec/step)\r\nINFO:tensorflow:global step 155: loss = 9.2020 (0.487 sec/step)\r\nINFO:tensorflow:global step 156: loss = 9.8456 (0.528 sec/step)\r\nINFO:tensorflow:global step 157: loss = 8.7925 (0.429 sec/step)\r\nINFO:tensorflow:global step 158: loss = 8.8295 (0.435 sec/step)\r\nINFO:tensorflow:global step 159: loss = 8.9335 (0.509 sec/step)\r\nINFO:tensorflow:global step 160: loss = 9.1799 (0.444 sec/step)\r\nINFO:tensorflow:global step 161: loss = 9.4383 (0.433 sec/step)\r\nINFO:tensorflow:global step 162: loss = 9.5451 (0.484 sec/step)\r\nINFO:tensorflow:global step 163: loss = 9.8775 (0.460 sec/step)\r\nINFO:tensorflow:global step 164: loss = 9.1774 (0.462 sec/step)\r\nINFO:tensorflow:global step 165: loss = 8.8758 (0.757 sec/step)\r\nINFO:tensorflow:global step 166: loss = 9.2932 (0.428 sec/step)\r\nINFO:tensorflow:global step 167: loss = 9.1967 (0.441 sec/step)\r\nINFO:tensorflow:global step 168: loss = 9.6125 (0.760 sec/step)\r\nINFO:tensorflow:global step 169: loss = 9.0119 (0.644 sec/step)\r\nINFO:tensorflow:global step 170: loss = 9.7965 (0.429 sec/step)\r\nINFO:tensorflow:global step 171: loss = 8.8230 (0.548 sec/step)\r\nINFO:tensorflow:global step 172: loss = 8.6981 (0.482 sec/step)\r\nINFO:tensorflow:global step 173: loss = 8.4844 (0.491 sec/step)\r\nINFO:tensorflow:global step 174: loss = 9.4706 (0.431 sec/step)\r\nINFO:tensorflow:global step 175: loss = 8.5126 (0.478 sec/step)\r\nINFO:tensorflow:global step 176: loss = 9.2941 (0.443 sec/step)\r\nINFO:tensorflow:global step 177: loss = 8.9967 (0.597 sec/step)\r\nINFO:tensorflow:global step 178: loss = 9.5142 (0.640 sec/step)\r\nINFO:tensorflow:global step 179: loss = 9.4251 (0.487 sec/step)\r\nINFO:tensorflow:global step 180: loss = 9.5026 (0.469 sec/step)\r\nINFO:tensorflow:global step 181: loss = 9.3916 (0.504 sec/step)\r\nINFO:tensorflow:global step 182: loss = 9.7789 (0.485 sec/step)\r\nINFO:tensorflow:global step 183: loss = 8.8342 (0.530 sec/step)\r\nINFO:tensorflow:global step 184: loss = 9.1194 (0.661 sec/step)\r\n2018-03-13 21:59:44.843954: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 217.43MiB.  Current allocation summary follows.\r\n2018-03-13 21:59:44.845715: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (256): \tTotal Chunks: 709, Chunks in use: 709. 177.2KiB allocated for chunks. 177.2KiB in use in bin. 6.8KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.845777: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (512): \tTotal Chunks: 59, Chunks in use: 59. 29.8KiB allocated for chunks. 29.8KiB in use in bin. 18.4KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.845822: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (1024): \tTotal Chunks: 21, Chunks in use: 17. 25.2KiB allocated for chunks. 20.8KiB in use in bin. 19.6KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.845941: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (2048): \tTotal Chunks: 14, Chunks in use: 12. 35.8KiB allocated for chunks. 30.2KiB in use in bin. 27.1KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.845986: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (4096): \tTotal Chunks: 323, Chunks in use: 323. 2.34MiB allocated for chunks. 2.34MiB in use in bin. 2.33MiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846031: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (8192): \tTotal Chunks: 17, Chunks in use: 15. 189.2KiB allocated for chunks. 159.0KiB in use in bin. 136.9KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846103: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (16384): \tTotal Chunks: 22, Chunks in use: 20. 600.0KiB allocated for chunks. 540.0KiB in use in bin. 539.4KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846149: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (32768): \tTotal Chunks: 7, Chunks in use: 7. 318.8KiB allocated for chunks. 318.8KiB in use in bin. 249.9KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846193: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 448.0KiB allocated for chunks. 448.0KiB in use in bin. 448.0KiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846304: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (131072): \tTotal Chunks: 13, Chunks in use: 13. 2.13MiB allocated for chunks. 2.13MiB in use in bin. 2.04MiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846346: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (262144): \tTotal Chunks: 10, Chunks in use: 10. 3.69MiB allocated for chunks. 3.69MiB in use in bin. 3.46MiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846388: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (524288): \tTotal Chunks: 19, Chunks in use: 19. 11.10MiB allocated for chunks. 11.10MiB in use in bin. 10.88MiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846449: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (1048576): \tTotal Chunks: 8, Chunks in use: 8. 9.67MiB allocated for chunks. 9.67MiB in use in bin. 9.17MiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846491: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-13 21:59:44.846548: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-13 21:59:44.846593: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (8388608): \tTotal Chunks: 2, Chunks in use: 2. 24.57MiB allocated for chunks. 24.57MiB in use in bin. 24.57MiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846631: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-13 21:59:44.846668: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-13 21:59:44.846734: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-13 21:59:44.846771: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-13 21:59:44.846812: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 1. 10.16GiB allocated for chunks. 10.16GiB in use in bin. 5.10GiB client-requested in use in bin.\r\n2018-03-13 21:59:44.846855: I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin for 217.43MiB was 128.00MiB, Chunk State: \r\n2018-03-13 21:59:44.846889: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00000 of size 1280\r\n2018-03-13 21:59:44.846915: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00500 of size 256\r\n2018-03-13 21:59:44.846938: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00600 of size 256\r\n2018-03-13 21:59:44.846960: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00700 of size 256\r\n2018-03-13 21:59:44.846983: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00800 of size 256\r\n2018-03-13 21:59:44.847006: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00900 of size 256\r\n2018-03-13 21:59:44.847031: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00a00 of size 512\r\n2018-03-13 21:59:44.847054: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00c00 of size 256\r\n2018-03-13 21:59:44.847077: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00d00 of size 256\r\n2018-03-13 21:59:44.847100: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00e00 of size 256\r\n2018-03-13 21:59:44.847122: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da00f00 of size 256\r\n2018-03-13 21:59:44.847172: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01000 of size 256\r\n2018-03-13 21:59:44.847199: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01100 of size 256\r\n2018-03-13 21:59:44.847231: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01200 of size 256\r\n2018-03-13 21:59:44.847255: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01300 of size 256\r\n2018-03-13 21:59:44.847277: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01400 of size 512\r\n2018-03-13 21:59:44.847300: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01600 of size 256\r\n2018-03-13 21:59:44.847322: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01700 of size 256\r\n2018-03-13 21:59:44.847345: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01800 of size 256\r\n2018-03-13 21:59:44.847367: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01900 of size 256\r\n2018-03-13 21:59:44.847390: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01a00 of size 256\r\n2018-03-13 21:59:44.847412: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01b00 of size 256\r\n2018-03-13 21:59:44.847435: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01c00 of size 256\r\n2018-03-13 21:59:44.847458: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01d00 of size 512\r\n2018-03-13 21:59:44.847480: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da01f00 of size 256\r\n2018-03-13 21:59:44.847502: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02000 of size 512\r\n2018-03-13 21:59:44.847525: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02200 of size 256\r\n2018-03-13 21:59:44.847547: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02300 of size 256\r\n2018-03-13 21:59:44.847570: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02400 of size 256\r\n2018-03-13 21:59:44.847592: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02500 of size 256\r\n2018-03-13 21:59:44.847614: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02600 of size 256\r\n2018-03-13 21:59:44.847637: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02700 of size 256\r\n2018-03-13 21:59:44.847659: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02800 of size 256\r\n2018-03-13 21:59:44.847709: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02900 of size 512\r\n2018-03-13 21:59:44.847735: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02b00 of size 256\r\n2018-03-13 21:59:44.847757: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02c00 of size 256\r\n2018-03-13 21:59:44.847781: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02d00 of size 256\r\n2018-03-13 21:59:44.847803: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02e00 of size 256\r\n2018-03-13 21:59:44.847826: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da02f00 of size 256\r\n2018-03-13 21:59:44.847848: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03000 of size 256\r\n2018-03-13 21:59:44.847872: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03100 of size 256\r\n2018-03-13 21:59:44.847921: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03200 of size 256\r\n2018-03-13 21:59:44.847947: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03300 of size 256\r\n2018-03-13 21:59:44.847969: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03400 of size 256\r\n2018-03-13 21:59:44.847991: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03500 of size 256\r\n2018-03-13 21:59:44.848014: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03600 of size 256\r\n2018-03-13 21:59:44.848036: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03700 of size 256\r\n2018-03-13 21:59:44.848058: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03800 of size 512\r\n2018-03-13 21:59:44.848081: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03a00 of size 256\r\n2018-03-13 21:59:44.848103: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03b00 of size 512\r\n2018-03-13 21:59:44.848125: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03d00 of size 256\r\n2018-03-13 21:59:44.848176: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03e00 of size 256\r\n2018-03-13 21:59:44.848200: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da03f00 of size 256\r\n2018-03-13 21:59:44.848229: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04000 of size 256\r\n2018-03-13 21:59:44.852565: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04100 of size 256\r\n2018-03-13 21:59:44.852644: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04200 of size 256\r\n2018-03-13 21:59:44.852684: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04300 of size 256\r\n2018-03-13 21:59:44.852710: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04400 of size 512\r\n2018-03-13 21:59:44.852742: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04600 of size 256\r\n2018-03-13 21:59:44.852771: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04700 of size 256\r\n2018-03-13 21:59:44.852800: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04800 of size 256\r\n2018-03-13 21:59:44.852831: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04900 of size 256\r\n2018-03-13 21:59:44.852866: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04a00 of size 256\r\n2018-03-13 21:59:44.852902: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04b00 of size 256\r\n2018-03-13 21:59:44.852934: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04c00 of size 256\r\n2018-03-13 21:59:44.852962: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04d00 of size 512\r\n2018-03-13 21:59:44.852988: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da04f00 of size 256\r\n2018-03-13 21:59:44.853019: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05000 of size 256\r\n2018-03-13 21:59:44.853049: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05100 of size 256\r\n2018-03-13 21:59:44.853077: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05200 of size 256\r\n2018-03-13 21:59:44.853105: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05300 of size 256\r\n2018-03-13 21:59:44.853144: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05400 of size 256\r\n2018-03-13 21:59:44.853177: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05500 of size 256\r\n2018-03-13 21:59:44.853202: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05600 of size 256\r\n2018-03-13 21:59:44.853248: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05700 of size 256\r\n2018-03-13 21:59:44.853278: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05800 of size 256\r\n2018-03-13 21:59:44.853305: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05900 of size 256\r\n2018-03-13 21:59:44.853335: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05a00 of size 256\r\n2018-03-13 21:59:44.853362: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05b00 of size 256\r\n2018-03-13 21:59:44.853399: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05c00 of size 512\r\n2018-03-13 21:59:44.853448: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05e00 of size 256\r\n2018-03-13 21:59:44.853495: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da05f00 of size 256\r\n2018-03-13 21:59:44.853537: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06000 of size 256\r\n2018-03-13 21:59:44.853570: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06100 of size 256\r\n2018-03-13 21:59:44.853613: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06200 of size 512\r\n2018-03-13 21:59:44.853654: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06400 of size 256\r\n2018-03-13 21:59:44.853696: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06500 of size 256\r\n2018-03-13 21:59:44.853736: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06600 of size 256\r\n2018-03-13 21:59:44.853771: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06700 of size 256\r\n2018-03-13 21:59:44.853810: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06800 of size 512\r\n2018-03-13 21:59:44.853848: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06a00 of size 256\r\n2018-03-13 21:59:44.853884: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06b00 of size 256\r\n2018-03-13 21:59:44.853918: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06c00 of size 256\r\n2018-03-13 21:59:44.853957: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06d00 of size 256\r\n2018-03-13 21:59:44.853988: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06e00 of size 256\r\n2018-03-13 21:59:44.854033: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da06f00 of size 256\r\n2018-03-13 21:59:44.854068: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07000 of size 256\r\n2018-03-13 21:59:44.854100: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07100 of size 256\r\n2018-03-13 21:59:44.854142: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07200 of size 256\r\n2018-03-13 21:59:44.854179: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07300 of size 256\r\n2018-03-13 21:59:44.854215: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07400 of size 512\r\n2018-03-13 21:59:44.854267: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07600 of size 256\r\n2018-03-13 21:59:44.854311: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07700 of size 256\r\n2018-03-13 21:59:44.854351: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07800 of size 256\r\n2018-03-13 21:59:44.854379: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07900 of size 256\r\n2018-03-13 21:59:44.854415: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07a00 of size 256\r\n2018-03-13 21:59:44.854444: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07b00 of size 256\r\n2018-03-13 21:59:44.854483: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07c00 of size 256\r\n2018-03-13 21:59:44.854524: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07d00 of size 256\r\n2018-03-13 21:59:44.854578: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07e00 of size 256\r\n2018-03-13 21:59:44.854621: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da07f00 of size 512\r\n2018-03-13 21:59:44.854659: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08100 of size 256\r\n2018-03-13 21:59:44.854704: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08200 of size 256\r\n2018-03-13 21:59:44.854740: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08300 of size 512\r\n2018-03-13 21:59:44.854772: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08500 of size 256\r\n2018-03-13 21:59:44.854819: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08600 of size 256\r\n2018-03-13 21:59:44.854856: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08700 of size 256\r\n2018-03-13 21:59:44.854892: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08800 of size 256\r\n2018-03-13 21:59:44.854929: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08900 of size 256\r\n2018-03-13 21:59:44.854965: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08a00 of size 256\r\n2018-03-13 21:59:44.855000: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08b00 of size 256\r\n2018-03-13 21:59:44.855038: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08c00 of size 256\r\n2018-03-13 21:59:44.855070: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08d00 of size 256\r\n2018-03-13 21:59:44.855103: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08e00 of size 256\r\n2018-03-13 21:59:44.855142: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da08f00 of size 256\r\n2018-03-13 21:59:44.855187: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09000 of size 256\r\n2018-03-13 21:59:44.855234: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09100 of size 256\r\n2018-03-13 21:59:44.855281: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09200 of size 256\r\n2018-03-13 21:59:44.855321: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09300 of size 512\r\n2018-03-13 21:59:44.855362: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09500 of size 256\r\n2018-03-13 21:59:44.855406: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09600 of size 512\r\n2018-03-13 21:59:44.855445: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09800 of size 256\r\n2018-03-13 21:59:44.855494: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09900 of size 256\r\n2018-03-13 21:59:44.855535: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09a00 of size 256\r\n2018-03-13 21:59:44.855582: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09b00 of size 256\r\n2018-03-13 21:59:44.855616: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09c00 of size 256\r\n2018-03-13 21:59:44.855652: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09d00 of size 256\r\n2018-03-13 21:59:44.855691: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09e00 of size 256\r\n2018-03-13 21:59:44.855726: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da09f00 of size 256\r\n2018-03-13 21:59:44.855773: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a000 of size 256\r\n2018-03-13 21:59:44.855814: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a100 of size 256\r\n2018-03-13 21:59:44.855859: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a200 of size 256\r\n2018-03-13 21:59:44.855895: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a300 of size 512\r\n2018-03-13 21:59:44.855929: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a500 of size 256\r\n2018-03-13 21:59:44.855974: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a600 of size 256\r\n2018-03-13 21:59:44.856024: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a700 of size 256\r\n2018-03-13 21:59:44.856066: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a800 of size 256\r\n2018-03-13 21:59:44.856107: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0a900 of size 512\r\n2018-03-13 21:59:44.856149: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0ab00 of size 256\r\n2018-03-13 21:59:44.856189: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0ac00 of size 256\r\n2018-03-13 21:59:44.856250: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0ad00 of size 256\r\n2018-03-13 21:59:44.856284: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0ae00 of size 256\r\n2018-03-13 21:59:44.856322: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0af00 of size 256\r\n2018-03-13 21:59:44.856357: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b000 of size 256\r\n2018-03-13 21:59:44.856395: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b100 of size 256\r\n2018-03-13 21:59:44.856431: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b200 of size 256\r\n2018-03-13 21:59:44.856471: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b300 of size 512\r\n2018-03-13 21:59:44.856515: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b500 of size 256\r\n2018-03-13 21:59:44.856555: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b600 of size 256\r\n2018-03-13 21:59:44.856596: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b700 of size 256\r\n2018-03-13 21:59:44.856643: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b800 of size 256\r\n2018-03-13 21:59:44.856681: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0b900 of size 512\r\n2018-03-13 21:59:44.856715: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0bb00 of size 256\r\n2018-03-13 21:59:44.856749: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0bc00 of size 256\r\n2018-03-13 21:59:44.856790: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0bd00 of size 256\r\n2018-03-13 21:59:44.856823: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0be00 of size 256\r\n2018-03-13 21:59:44.856855: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0bf00 of size 256\r\n2018-03-13 21:59:44.856896: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c000 of size 256\r\n2018-03-13 21:59:44.856929: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c100 of size 256\r\n2018-03-13 21:59:44.856965: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c200 of size 256\r\n2018-03-13 21:59:44.856999: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c300 of size 256\r\n2018-03-13 21:59:44.857036: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c400 of size 256\r\n2018-03-13 21:59:44.857078: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c500 of size 512\r\n2018-03-13 21:59:44.857110: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c700 of size 256\r\n2018-03-13 21:59:44.857147: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c800 of size 256\r\n2018-03-13 21:59:44.857193: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0c900 of size 256\r\n2018-03-13 21:59:44.857235: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0ca00 of size 256\r\n2018-03-13 21:59:44.857280: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0cb00 of size 256\r\n2018-03-13 21:59:44.857322: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0cc00 of size 256\r\n2018-03-13 21:59:44.857357: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0cd00 of size 256\r\n2018-03-13 21:59:44.857406: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0ce00 of size 256\r\n2018-03-13 21:59:44.857438: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0cf00 of size 256\r\n2018-03-13 21:59:44.857475: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d000 of size 256\r\n2018-03-13 21:59:44.857510: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d100 of size 512\r\n2018-03-13 21:59:44.857554: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d300 of size 256\r\n2018-03-13 21:59:44.857593: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d400 of size 256\r\n2018-03-13 21:59:44.857633: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d500 of size 256\r\n2018-03-13 21:59:44.857669: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d600 of size 256\r\n2018-03-13 21:59:44.857690: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d700 of size 256\r\n2018-03-13 21:59:44.857699: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d800 of size 256\r\n2018-03-13 21:59:44.857706: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0d900 of size 256\r\n2018-03-13 21:59:44.857712: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0da00 of size 512\r\n2018-03-13 21:59:44.857717: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0dc00 of size 256\r\n2018-03-13 21:59:44.857723: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0dd00 of size 256\r\n2018-03-13 21:59:44.857743: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0de00 of size 256\r\n2018-03-13 21:59:44.857752: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0df00 of size 256\r\n2018-03-13 21:59:44.857758: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0e000 of size 256\r\n2018-03-13 21:59:44.857764: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0e100 of size 256\r\n2018-03-13 21:59:44.857769: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0e200 of size 256\r\n2018-03-13 21:59:44.857775: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0e300 of size 256\r\n2018-03-13 21:59:44.857782: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0e400 of size 256\r\n2018-03-13 21:59:44.857791: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020da0e500 of size 12882432\r\n2018-03-13 21:59:44.857798: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e657700 of size 256\r\n2018-03-13 21:59:44.857804: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e657800 of size 7680\r\n2018-03-13 21:59:44.857809: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e659600 of size 7680\r\n2018-03-13 21:59:44.857815: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e65b400 of size 7680\r\n2018-03-13 21:59:44.857820: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e65d200 of size 7680\r\n2018-03-13 21:59:44.857826: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e65f000 of size 7680\r\n2018-03-13 21:59:44.857835: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e660e00 of size 7680\r\n2018-03-13 21:59:44.857842: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e662c00 of size 7680\r\n2018-03-13 21:59:44.857847: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e664a00 of size 7680\r\n2018-03-13 21:59:44.857853: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e666800 of size 7680\r\n2018-03-13 21:59:44.857858: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e668600 of size 7680\r\n2018-03-13 21:59:44.857864: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e66a400 of size 7680\r\n2018-03-13 21:59:44.857869: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e66c200 of size 7680\r\n2018-03-13 21:59:44.857874: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e66e000 of size 7680\r\n2018-03-13 21:59:44.857880: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e66fe00 of size 7680\r\n2018-03-13 21:59:44.857887: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e671c00 of size 7680\r\n2018-03-13 21:59:44.857895: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e673a00 of size 7680\r\n2018-03-13 21:59:44.857900: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e675800 of size 7680\r\n2018-03-13 21:59:44.857906: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e677600 of size 7680\r\n2018-03-13 21:59:44.857912: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e679400 of size 7680\r\n2018-03-13 21:59:44.857918: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e67b200 of size 7680\r\n2018-03-13 21:59:44.857923: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e67d000 of size 7680\r\n2018-03-13 21:59:44.857929: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e67ee00 of size 7680\r\n2018-03-13 21:59:44.857934: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e680c00 of size 7680\r\n2018-03-13 21:59:44.857941: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e682a00 of size 7680\r\n2018-03-13 21:59:44.857947: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e684800 of size 7680\r\n2018-03-13 21:59:44.857952: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e686600 of size 7680\r\n2018-03-13 21:59:44.857958: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e688400 of size 7680\r\n2018-03-13 21:59:44.857964: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e68a200 of size 7680\r\n2018-03-13 21:59:44.857968: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e68c000 of size 7680\r\n2018-03-13 21:59:44.857974: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e68de00 of size 7680\r\n2018-03-13 21:59:44.857980: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e68fc00 of size 7680\r\n2018-03-13 21:59:44.857985: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e691a00 of size 7680\r\n2018-03-13 21:59:44.857992: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e693800 of size 7680\r\n2018-03-13 21:59:44.858000: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e695600 of size 7680\r\n2018-03-13 21:59:44.858006: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e697400 of size 7680\r\n2018-03-13 21:59:44.858012: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e699200 of size 7680\r\n2018-03-13 21:59:44.858017: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e69b000 of size 7680\r\n2018-03-13 21:59:44.858023: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e69ce00 of size 7680\r\n2018-03-13 21:59:44.858028: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e69ec00 of size 7680\r\n2018-03-13 21:59:44.858034: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6a0a00 of size 7680\r\n2018-03-13 21:59:44.858040: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6a2800 of size 7680\r\n2018-03-13 21:59:44.858046: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6a4600 of size 7680\r\n2018-03-13 21:59:44.858052: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6a6400 of size 7680\r\n2018-03-13 21:59:44.858057: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6a8200 of size 7680\r\n2018-03-13 21:59:44.858063: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6aa000 of size 7680\r\n2018-03-13 21:59:44.858068: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6abe00 of size 7680\r\n2018-03-13 21:59:44.858074: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6adc00 of size 7680\r\n2018-03-13 21:59:44.858080: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6afa00 of size 7680\r\n2018-03-13 21:59:44.858085: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6b1800 of size 7680\r\n2018-03-13 21:59:44.858091: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6b3600 of size 7680\r\n2018-03-13 21:59:44.858098: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6b5400 of size 7680\r\n2018-03-13 21:59:44.858105: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6b7200 of size 7680\r\n2018-03-13 21:59:44.858111: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6b9000 of size 7680\r\n2018-03-13 21:59:44.858116: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6bae00 of size 7680\r\n2018-03-13 21:59:44.858121: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6bcc00 of size 7680\r\n2018-03-13 21:59:44.858127: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6bea00 of size 7680\r\n2018-03-13 21:59:44.858133: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6c0800 of size 7680\r\n2018-03-13 21:59:44.858137: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6c2600 of size 7680\r\n2018-03-13 21:59:44.858143: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6c4400 of size 7680\r\n2018-03-13 21:59:44.858151: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6c6200 of size 7680\r\n2018-03-13 21:59:44.858159: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6c8000 of size 7680\r\n2018-03-13 21:59:44.858164: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6c9e00 of size 7680\r\n2018-03-13 21:59:44.858170: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6cbc00 of size 7680\r\n2018-03-13 21:59:44.858175: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6cda00 of size 7680\r\n2018-03-13 21:59:44.858180: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6cf800 of size 7680\r\n2018-03-13 21:59:44.858186: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6d1600 of size 7680\r\n2018-03-13 21:59:44.858191: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6d3400 of size 7680\r\n2018-03-13 21:59:44.858197: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6d5200 of size 7680\r\n2018-03-13 21:59:44.858204: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6d7000 of size 7680\r\n2018-03-13 21:59:44.858210: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6d8e00 of size 7680\r\n2018-03-13 21:59:44.858215: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6dac00 of size 7680\r\n2018-03-13 21:59:44.858222: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6dca00 of size 7680\r\n2018-03-13 21:59:44.858243: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6de800 of size 7680\r\n2018-03-13 21:59:44.858248: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6e0600 of size 7680\r\n2018-03-13 21:59:44.858255: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6e2400 of size 7680\r\n2018-03-13 21:59:44.858277: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6e4200 of size 7680\r\n2018-03-13 21:59:44.858283: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6e6000 of size 7680\r\n2018-03-13 21:59:44.858288: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6e7e00 of size 7680\r\n2018-03-13 21:59:44.858294: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6e9c00 of size 7680\r\n2018-03-13 21:59:44.858299: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6eba00 of size 7680\r\n2018-03-13 21:59:44.858307: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6ed800 of size 7680\r\n2018-03-13 21:59:44.858312: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6ef600 of size 7680\r\n2018-03-13 21:59:44.858318: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6f1400 of size 7680\r\n2018-03-13 21:59:44.858324: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6f3200 of size 7680\r\n2018-03-13 21:59:44.858329: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6f5000 of size 7680\r\n2018-03-13 21:59:44.858335: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6f6e00 of size 7680\r\n2018-03-13 21:59:44.858340: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6f8c00 of size 7680\r\n2018-03-13 21:59:44.858346: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6faa00 of size 7680\r\n2018-03-13 21:59:44.858351: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6fc800 of size 7680\r\n2018-03-13 21:59:44.858357: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e6fe600 of size 7680\r\n2018-03-13 21:59:44.858364: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e700400 of size 7680\r\n2018-03-13 21:59:44.858370: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e702200 of size 7680\r\n2018-03-13 21:59:44.858375: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e704000 of size 7680\r\n2018-03-13 21:59:44.858381: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e705e00 of size 7680\r\n2018-03-13 21:59:44.858386: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e707c00 of size 7680\r\n2018-03-13 21:59:44.858392: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e709a00 of size 7680\r\n2018-03-13 21:59:44.858397: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e70b800 of size 7680\r\n2018-03-13 21:59:44.858403: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e70d600 of size 7680\r\n2018-03-13 21:59:44.858408: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e70f400 of size 7680\r\n2018-03-13 21:59:44.858415: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e711200 of size 7680\r\n2018-03-13 21:59:44.858422: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e713000 of size 7680\r\n2018-03-13 21:59:44.858427: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e714e00 of size 7680\r\n2018-03-13 21:59:44.858433: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e716c00 of size 7680\r\n2018-03-13 21:59:44.858438: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e718a00 of size 7680\r\n2018-03-13 21:59:44.858444: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e71a800 of size 7680\r\n2018-03-13 21:59:44.858448: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e71c600 of size 7680\r\n2018-03-13 21:59:44.858454: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e71e400 of size 7680\r\n2018-03-13 21:59:44.858459: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e720200 of size 7680\r\n2018-03-13 21:59:44.858467: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e722000 of size 7680\r\n2018-03-13 21:59:44.858476: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e723e00 of size 7680\r\n2018-03-13 21:59:44.858483: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e725c00 of size 7680\r\n2018-03-13 21:59:44.858489: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e727a00 of size 7680\r\n2018-03-13 21:59:44.858494: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e729800 of size 7680\r\n2018-03-13 21:59:44.858500: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e72b600 of size 7680\r\n2018-03-13 21:59:44.858506: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e72d400 of size 7680\r\n2018-03-13 21:59:44.858511: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e72f200 of size 7680\r\n2018-03-13 21:59:44.858519: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e731000 of size 7680\r\n2018-03-13 21:59:44.858525: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e732e00 of size 7680\r\n2018-03-13 21:59:44.858531: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e734c00 of size 7680\r\n2018-03-13 21:59:44.858537: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e736a00 of size 7680\r\n2018-03-13 21:59:44.858541: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e738800 of size 7680\r\n2018-03-13 21:59:44.858547: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e73a600 of size 7680\r\n2018-03-13 21:59:44.858552: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e73c400 of size 7680\r\n2018-03-13 21:59:44.858558: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e73e200 of size 7680\r\n2018-03-13 21:59:44.858563: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e740000 of size 7680\r\n2018-03-13 21:59:44.858570: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e741e00 of size 7680\r\n2018-03-13 21:59:44.858580: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e743c00 of size 7680\r\n2018-03-13 21:59:44.858586: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e745a00 of size 7680\r\n2018-03-13 21:59:44.858592: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e747800 of size 7680\r\n2018-03-13 21:59:44.858597: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e749600 of size 7680\r\n2018-03-13 21:59:44.858603: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e74b400 of size 7680\r\n2018-03-13 21:59:44.858608: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e74d200 of size 7680\r\n2018-03-13 21:59:44.858614: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e74f000 of size 7680\r\n2018-03-13 21:59:44.858619: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e750e00 of size 7680\r\n2018-03-13 21:59:44.858627: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e752c00 of size 7680\r\n2018-03-13 21:59:44.858632: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e754a00 of size 7680\r\n2018-03-13 21:59:44.858638: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e756800 of size 7680\r\n2018-03-13 21:59:44.858643: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e758600 of size 7680\r\n2018-03-13 21:59:44.858649: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e75a400 of size 7680\r\n2018-03-13 21:59:44.858654: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e75c200 of size 7680\r\n2018-03-13 21:59:44.858659: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e75e000 of size 7680\r\n2018-03-13 21:59:44.858665: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e75fe00 of size 7680\r\n2018-03-13 21:59:44.858671: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e761c00 of size 7680\r\n2018-03-13 21:59:44.858678: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e763a00 of size 7680\r\n2018-03-13 21:59:44.858685: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e765800 of size 736256\r\n2018-03-13 21:59:44.858690: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e819400 of size 184064\r\n2018-03-13 21:59:44.858696: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e846300 of size 184064\r\n2018-03-13 21:59:44.858701: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e873200 of size 184064\r\n2018-03-13 21:59:44.858707: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8a0100 of size 184064\r\n2018-03-13 21:59:44.858712: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8cd000 of size 30720\r\n2018-03-13 21:59:44.858717: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4800 of size 256\r\n2018-03-13 21:59:44.858723: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4900 of size 256\r\n2018-03-13 21:59:44.858731: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4a00 of size 256\r\n2018-03-13 21:59:44.858739: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4b00 of size 256\r\n2018-03-13 21:59:44.858745: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4c00 of size 256\r\n2018-03-13 21:59:44.858750: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4d00 of size 256\r\n2018-03-13 21:59:44.858770: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4e00 of size 256\r\n2018-03-13 21:59:44.858776: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d4f00 of size 256\r\n2018-03-13 21:59:44.858785: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5000 of size 256\r\n2018-03-13 21:59:44.858790: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5100 of size 256\r\n2018-03-13 21:59:44.858795: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5200 of size 256\r\n2018-03-13 21:59:44.858814: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5300 of size 256\r\n2018-03-13 21:59:44.858819: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5400 of size 256\r\n2018-03-13 21:59:44.858825: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5500 of size 256\r\n2018-03-13 21:59:44.858830: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5600 of size 256\r\n2018-03-13 21:59:44.858840: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5700 of size 256\r\n2018-03-13 21:59:44.858846: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5800 of size 256\r\n2018-03-13 21:59:44.858851: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5900 of size 256\r\n2018-03-13 21:59:44.858856: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5a00 of size 256\r\n2018-03-13 21:59:44.858862: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5b00 of size 256\r\n2018-03-13 21:59:44.858867: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5c00 of size 256\r\n2018-03-13 21:59:44.858873: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5d00 of size 256\r\n2018-03-13 21:59:44.858878: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5e00 of size 256\r\n2018-03-13 21:59:44.858884: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d5f00 of size 256\r\n2018-03-13 21:59:44.858889: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6000 of size 256\r\n2018-03-13 21:59:44.858895: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6100 of size 256\r\n2018-03-13 21:59:44.858900: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6200 of size 256\r\n2018-03-13 21:59:44.858906: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6300 of size 256\r\n2018-03-13 21:59:44.858911: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6400 of size 256\r\n2018-03-13 21:59:44.858916: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6500 of size 256\r\n2018-03-13 21:59:44.858922: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6600 of size 256\r\n2018-03-13 21:59:44.858927: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6700 of size 256\r\n2018-03-13 21:59:44.858933: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6800 of size 256\r\n2018-03-13 21:59:44.858939: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6900 of size 256\r\n2018-03-13 21:59:44.858944: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6a00 of size 256\r\n2018-03-13 21:59:44.858950: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6b00 of size 256\r\n2018-03-13 21:59:44.858955: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6c00 of size 256\r\n2018-03-13 21:59:44.858961: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6d00 of size 256\r\n2018-03-13 21:59:44.858965: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6e00 of size 256\r\n2018-03-13 21:59:44.858972: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d6f00 of size 256\r\n2018-03-13 21:59:44.858977: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7000 of size 256\r\n2018-03-13 21:59:44.858982: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7100 of size 256\r\n2018-03-13 21:59:44.858988: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7200 of size 256\r\n2018-03-13 21:59:44.858993: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7300 of size 256\r\n2018-03-13 21:59:44.858999: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7400 of size 256\r\n2018-03-13 21:59:44.859004: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7500 of size 256\r\n2018-03-13 21:59:44.859010: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7600 of size 256\r\n2018-03-13 21:59:44.859015: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7700 of size 256\r\n2018-03-13 21:59:44.859021: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7800 of size 256\r\n2018-03-13 21:59:44.859026: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7900 of size 256\r\n2018-03-13 21:59:44.859031: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7a00 of size 256\r\n2018-03-13 21:59:44.859037: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7b00 of size 256\r\n2018-03-13 21:59:44.859043: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7c00 of size 256\r\n2018-03-13 21:59:44.859048: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7d00 of size 256\r\n2018-03-13 21:59:44.859053: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7e00 of size 256\r\n2018-03-13 21:59:44.859059: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d7f00 of size 256\r\n2018-03-13 21:59:44.859064: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8000 of size 256\r\n2018-03-13 21:59:44.859070: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8100 of size 256\r\n2018-03-13 21:59:44.859076: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8200 of size 256\r\n2018-03-13 21:59:44.859081: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8300 of size 256\r\n2018-03-13 21:59:44.859087: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8400 of size 256\r\n2018-03-13 21:59:44.859094: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8500 of size 256\r\n2018-03-13 21:59:44.859103: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8600 of size 256\r\n2018-03-13 21:59:44.859109: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8700 of size 256\r\n2018-03-13 21:59:44.859114: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8800 of size 256\r\n2018-03-13 21:59:44.859120: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8900 of size 256\r\n2018-03-13 21:59:44.859125: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8a00 of size 256\r\n2018-03-13 21:59:44.859131: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8b00 of size 256\r\n2018-03-13 21:59:44.859136: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8c00 of size 256\r\n2018-03-13 21:59:44.859142: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8d00 of size 256\r\n2018-03-13 21:59:44.859147: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8e00 of size 256\r\n2018-03-13 21:59:44.859153: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d8f00 of size 256\r\n2018-03-13 21:59:44.859158: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9000 of size 256\r\n2018-03-13 21:59:44.859164: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9100 of size 256\r\n2018-03-13 21:59:44.859169: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9200 of size 256\r\n2018-03-13 21:59:44.859175: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9300 of size 256\r\n2018-03-13 21:59:44.859180: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9400 of size 256\r\n2018-03-13 21:59:44.859185: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9500 of size 256\r\n2018-03-13 21:59:44.859191: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9600 of size 256\r\n2018-03-13 21:59:44.859196: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9700 of size 256\r\n2018-03-13 21:59:44.859202: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9800 of size 256\r\n2018-03-13 21:59:44.859210: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9900 of size 256\r\n2018-03-13 21:59:44.859215: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9a00 of size 256\r\n2018-03-13 21:59:44.859224: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9b00 of size 256\r\n2018-03-13 21:59:44.859245: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9c00 of size 256\r\n2018-03-13 21:59:44.859250: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9d00 of size 256\r\n2018-03-13 21:59:44.859256: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9e00 of size 256\r\n2018-03-13 21:59:44.859261: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8d9f00 of size 256\r\n2018-03-13 21:59:44.859280: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da000 of size 256\r\n2018-03-13 21:59:44.859289: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da100 of size 256\r\n2018-03-13 21:59:44.859294: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da200 of size 256\r\n2018-03-13 21:59:44.859300: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da300 of size 256\r\n2018-03-13 21:59:44.859305: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da400 of size 256\r\n2018-03-13 21:59:44.859310: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da500 of size 256\r\n2018-03-13 21:59:44.859316: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da600 of size 256\r\n2018-03-13 21:59:44.859322: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da700 of size 256\r\n2018-03-13 21:59:44.859326: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da800 of size 256\r\n2018-03-13 21:59:44.859332: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8da900 of size 256\r\n2018-03-13 21:59:44.859337: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8daa00 of size 256\r\n2018-03-13 21:59:44.859343: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8dab00 of size 512\r\n2018-03-13 21:59:44.859348: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8dad00 of size 1024\r\n2018-03-13 21:59:44.859354: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8db100 of size 256\r\n2018-03-13 21:59:44.859359: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8db200 of size 256\r\n2018-03-13 21:59:44.859365: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8db300 of size 256\r\n2018-03-13 21:59:44.859370: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8db400 of size 24576\r\n2018-03-13 21:59:44.859376: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8e1400 of size 98304\r\n2018-03-13 21:59:44.859381: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8f9400 of size 256\r\n2018-03-13 21:59:44.859387: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8f9500 of size 24576\r\n2018-03-13 21:59:44.859392: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e8ff500 of size 49152\r\n2018-03-13 21:59:44.859398: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e90b500 of size 98304\r\n2018-03-13 21:59:44.859404: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e923500 of size 524288\r\n2018-03-13 21:59:44.859408: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9a3500 of size 256\r\n2018-03-13 21:59:44.859414: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9a3600 of size 256\r\n2018-03-13 21:59:44.859419: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9a3700 of size 512\r\n2018-03-13 21:59:44.859425: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9a3900 of size 24576\r\n2018-03-13 21:59:44.859430: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9a9900 of size 256\r\n2018-03-13 21:59:44.859436: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9a9a00 of size 9216\r\n2018-03-13 21:59:44.859442: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9abe00 of size 18432\r\n2018-03-13 21:59:44.859447: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9b0600 of size 256\r\n2018-03-13 21:59:44.859453: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9b0700 of size 24576\r\n2018-03-13 21:59:44.859458: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9b6700 of size 256\r\n2018-03-13 21:59:44.859464: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9b6800 of size 262144\r\n2018-03-13 21:59:44.859469: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9f6800 of size 256\r\n2018-03-13 21:59:44.859475: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020e9f6900 of size 65536\r\n2018-03-13 21:59:44.859480: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea06900 of size 12288\r\n2018-03-13 21:59:44.859486: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea09900 of size 9216\r\n2018-03-13 21:59:44.859491: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea0bd00 of size 256\r\n2018-03-13 21:59:44.859497: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea0be00 of size 131072\r\n2018-03-13 21:59:44.859503: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea2be00 of size 12288\r\n2018-03-13 21:59:44.859509: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea2ee00 of size 4608\r\n2018-03-13 21:59:44.859514: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea30000 of size 65536\r\n2018-03-13 21:59:44.859520: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea40000 of size 4608\r\n2018-03-13 21:59:44.859526: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea41200 of size 2048\r\n2018-03-13 21:59:44.859531: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea41a00 of size 32768\r\n2018-03-13 21:59:44.859539: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea49a00 of size 49152\r\n2018-03-13 21:59:44.859547: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ea55a00 of size 524288\r\n2018-03-13 21:59:44.859554: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ead5a00 of size 2304\r\n2018-03-13 21:59:44.859560: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ead6300 of size 9216\r\n2018-03-13 21:59:44.859566: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ead8700 of size 1179648\r\n2018-03-13 21:59:44.859571: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ebf8700 of size 8192\r\n2018-03-13 21:59:44.859577: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ebfa700 of size 1280\r\n2018-03-13 21:59:44.859583: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ebfac00 of size 1024\r\n2018-03-13 21:59:44.859588: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ebfb000 of size 3584\r\n2018-03-13 21:59:44.859594: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ebfbe00 of size 430080\r\n2018-03-13 21:59:44.859599: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ec64e00 of size 8192\r\n2018-03-13 21:59:44.859605: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ec66e00 of size 256\r\n2018-03-13 21:59:44.859611: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ec66f00 of size 24576\r\n2018-03-13 21:59:44.859617: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ec6cf00 of size 1024\r\n2018-03-13 21:59:44.859622: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ec6d300 of size 1048576\r\n2018-03-13 21:59:44.859627: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ed6d300 of size 1280\r\n2018-03-13 21:59:44.859632: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ed6d800 of size 65536\r\n2018-03-13 21:59:44.859638: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ed7d800 of size 32768\r\n2018-03-13 21:59:44.859643: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ed85800 of size 430080\r\n2018-03-13 21:59:44.859649: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020edee800 of size 262144\r\n2018-03-13 21:59:44.859654: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ee2e800 of size 18432\r\n2018-03-13 21:59:44.859660: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ee33000 of size 9216\r\n2018-03-13 21:59:44.859665: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ee35400 of size 24576\r\n2018-03-13 21:59:44.859671: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ee3b400 of size 1179648\r\n2018-03-13 21:59:44.859676: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ef5b400 of size 1024\r\n2018-03-13 21:59:44.859681: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ef5b800 of size 3584\r\n2018-03-13 21:59:44.859687: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ef5c600 of size 2304\r\n2018-03-13 21:59:44.859692: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ef5cf00 of size 2048\r\n2018-03-13 21:59:44.859698: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ef5d700 of size 1792\r\n2018-03-13 21:59:44.859703: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ef5de00 of size 1048576\r\n2018-03-13 21:59:44.859709: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f05de00 of size 4608\r\n2018-03-13 21:59:44.859714: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f05f000 of size 1720320\r\n2018-03-13 21:59:44.859720: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f203000 of size 131072\r\n2018-03-13 21:59:44.859726: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f223000 of size 65536\r\n2018-03-13 21:59:44.859731: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f233000 of size 1024\r\n2018-03-13 21:59:44.859736: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f233400 of size 1792\r\n2018-03-13 21:59:44.859742: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f233b00 of size 4608\r\n2018-03-13 21:59:44.859748: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f234d00 of size 430080\r\n2018-03-13 21:59:44.859753: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f29dd00 of size 1792\r\n2018-03-13 21:59:44.859758: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f29e400 of size 1024\r\n2018-03-13 21:59:44.859764: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f29e800 of size 430080\r\n2018-03-13 21:59:44.859770: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f307800 of size 1024\r\n2018-03-13 21:59:44.859775: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f307c00 of size 1792\r\n2018-03-13 21:59:44.859781: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f308300 of size 430080\r\n2018-03-13 21:59:44.859786: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f371300 of size 1024\r\n2018-03-13 21:59:44.859793: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f371700 of size 1024\r\n2018-03-13 21:59:44.859803: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f371b00 of size 512\r\n2018-03-13 21:59:44.859810: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f371d00 of size 7424\r\n2018-03-13 21:59:44.859816: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f373a00 of size 430080\r\n2018-03-13 21:59:44.859821: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dca00 of size 512\r\n2018-03-13 21:59:44.859826: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dcc00 of size 2048\r\n2018-03-13 21:59:44.859847: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dd400 of size 512\r\n2018-03-13 21:59:44.859852: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dd600 of size 512\r\n2018-03-13 21:59:44.859858: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dd800 of size 1024\r\n2018-03-13 21:59:44.859864: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3ddc00 of size 4096\r\n2018-03-13 21:59:44.859870: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dec00 of size 256\r\n2018-03-13 21:59:44.859888: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3ded00 of size 256\r\n2018-03-13 21:59:44.859894: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dee00 of size 256\r\n2018-03-13 21:59:44.859899: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3def00 of size 512\r\n2018-03-13 21:59:44.859905: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3df100 of size 256\r\n2018-03-13 21:59:44.859910: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3df600 of size 256\r\n2018-03-13 21:59:44.859916: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3df700 of size 256\r\n2018-03-13 21:59:44.859921: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3df800 of size 256\r\n2018-03-13 21:59:44.859926: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3df900 of size 256\r\n2018-03-13 21:59:44.859931: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dfa00 of size 256\r\n2018-03-13 21:59:44.859936: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dfb00 of size 256\r\n2018-03-13 21:59:44.859943: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dfc00 of size 256\r\n2018-03-13 21:59:44.859947: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dfd00 of size 256\r\n2018-03-13 21:59:44.859953: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dfe00 of size 256\r\n2018-03-13 21:59:44.859958: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3dff00 of size 256\r\n2018-03-13 21:59:44.859963: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3e0000 of size 512\r\n2018-03-13 21:59:44.859969: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3e0200 of size 256\r\n2018-03-13 21:59:44.859974: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3e0300 of size 256\r\n2018-03-13 21:59:44.859979: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3e0400 of size 256\r\n2018-03-13 21:59:44.859985: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f3e0500 of size 131072\r\n2018-03-13 21:59:44.859990: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f400500 of size 512\r\n2018-03-13 21:59:44.859995: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f400700 of size 2048\r\n2018-03-13 21:59:44.860001: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f400f00 of size 3584\r\n2018-03-13 21:59:44.860006: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f401d00 of size 256\r\n2018-03-13 21:59:44.860012: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f401e00 of size 256\r\n2018-03-13 21:59:44.860018: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f401f00 of size 256\r\n2018-03-13 21:59:44.860023: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f402000 of size 256\r\n2018-03-13 21:59:44.860029: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f402100 of size 256\r\n2018-03-13 21:59:44.860035: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f402200 of size 256\r\n2018-03-13 21:59:44.860039: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f402300 of size 256\r\n2018-03-13 21:59:44.860046: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f402400 of size 256\r\n2018-03-13 21:59:44.860051: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f403300 of size 256\r\n2018-03-13 21:59:44.860061: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f403400 of size 131072\r\n2018-03-13 21:59:44.860068: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f423400 of size 5376\r\n2018-03-13 21:59:44.860074: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f424900 of size 5376\r\n2018-03-13 21:59:44.860080: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f426200 of size 677888\r\n2018-03-13 21:59:44.860086: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4cba00 of size 2048\r\n2018-03-13 21:59:44.860092: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4cca00 of size 4096\r\n2018-03-13 21:59:44.860097: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4cda00 of size 10240\r\n2018-03-13 21:59:44.860103: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4d0200 of size 5120\r\n2018-03-13 21:59:44.860108: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4d1600 of size 256\r\n2018-03-13 21:59:44.860114: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4d1700 of size 7680\r\n2018-03-13 21:59:44.860120: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4d3500 of size 14592\r\n2018-03-13 21:59:44.860125: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4d6e00 of size 7680\r\n2018-03-13 21:59:44.860130: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4d8c00 of size 7680\r\n2018-03-13 21:59:44.860135: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4daa00 of size 7680\r\n2018-03-13 21:59:44.860141: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4dc800 of size 7680\r\n2018-03-13 21:59:44.860146: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4de600 of size 7680\r\n2018-03-13 21:59:44.860152: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4e0400 of size 7680\r\n2018-03-13 21:59:44.860158: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f4e2200 of size 1720320\r\n2018-03-13 21:59:44.860163: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f686200 of size 1179648\r\n2018-03-13 21:59:44.860168: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7a6200 of size 7680\r\n2018-03-13 21:59:44.860173: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7a8000 of size 7680\r\n2018-03-13 21:59:44.860179: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7a9e00 of size 7680\r\n2018-03-13 21:59:44.860184: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7abc00 of size 7680\r\n2018-03-13 21:59:44.860190: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7ada00 of size 30720\r\n2018-03-13 21:59:44.860196: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7b5200 of size 30720\r\n2018-03-13 21:59:44.860201: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7bca00 of size 7680\r\n2018-03-13 21:59:44.860207: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7be800 of size 7680\r\n2018-03-13 21:59:44.860212: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c0600 of size 7680\r\n2018-03-13 21:59:44.860218: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c2400 of size 7680\r\n2018-03-13 21:59:44.860240: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c4200 of size 11776\r\n2018-03-13 21:59:44.860246: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c7000 of size 256\r\n2018-03-13 21:59:44.860251: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c7100 of size 256\r\n2018-03-13 21:59:44.860257: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c7200 of size 256\r\n2018-03-13 21:59:44.860274: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c7300 of size 2816\r\n2018-03-13 21:59:44.860280: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c8300 of size 2560\r\n2018-03-13 21:59:44.860285: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c8d00 of size 256\r\n2018-03-13 21:59:44.860291: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7c8e00 of size 7936\r\n2018-03-13 21:59:44.860296: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cad00 of size 8192\r\n2018-03-13 21:59:44.860301: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7ccd00 of size 9984\r\n2018-03-13 21:59:44.860308: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cf400 of size 256\r\n2018-03-13 21:59:44.860318: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cf500 of size 256\r\n2018-03-13 21:59:44.860325: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cf600 of size 256\r\n2018-03-13 21:59:44.860330: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cf700 of size 256\r\n2018-03-13 21:59:44.860336: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cf800 of size 256\r\n2018-03-13 21:59:44.860341: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cf900 of size 256\r\n2018-03-13 21:59:44.860347: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cfa00 of size 256\r\n2018-03-13 21:59:44.860353: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cfb00 of size 256\r\n2018-03-13 21:59:44.860358: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cfc00 of size 256\r\n2018-03-13 21:59:44.860364: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cfd00 of size 256\r\n2018-03-13 21:59:44.860368: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cfe00 of size 256\r\n2018-03-13 21:59:44.860374: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7cff00 of size 256\r\n2018-03-13 21:59:44.860379: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d0000 of size 256\r\n2018-03-13 21:59:44.860385: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d0100 of size 256\r\n2018-03-13 21:59:44.860391: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d0200 of size 256\r\n2018-03-13 21:59:44.860395: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d0300 of size 512\r\n2018-03-13 21:59:44.860401: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d0500 of size 256\r\n2018-03-13 21:59:44.860405: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d0600 of size 256\r\n2018-03-13 21:59:44.860411: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d0700 of size 7680\r\n2018-03-13 21:59:44.860416: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d2500 of size 7680\r\n2018-03-13 21:59:44.860422: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d4300 of size 256\r\n2018-03-13 21:59:44.860427: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d4400 of size 768\r\n2018-03-13 21:59:44.860433: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d4c00 of size 7680\r\n2018-03-13 21:59:44.860438: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7d6a00 of size 15104\r\n2018-03-13 21:59:44.860444: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7da500 of size 7680\r\n2018-03-13 21:59:44.860449: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7dc300 of size 7680\r\n2018-03-13 21:59:44.860454: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7de100 of size 7680\r\n2018-03-13 21:59:44.860459: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7dff00 of size 7680\r\n2018-03-13 21:59:44.860464: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7e1d00 of size 7680\r\n2018-03-13 21:59:44.860470: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7e3b00 of size 7680\r\n2018-03-13 21:59:44.860475: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7e5900 of size 60928\r\n2018-03-13 21:59:44.860481: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7f4700 of size 30720\r\n2018-03-13 21:59:44.860486: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f7fbf00 of size 15104\r\n2018-03-13 21:59:44.860491: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f803700 of size 30720\r\n2018-03-13 21:59:44.860497: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f80af00 of size 46080\r\n2018-03-13 21:59:44.860502: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f819f00 of size 30720\r\n2018-03-13 21:59:44.860507: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f821700 of size 30720\r\n2018-03-13 21:59:44.860512: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f828f00 of size 30720\r\n2018-03-13 21:59:44.860517: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f830700 of size 30720\r\n2018-03-13 21:59:44.860522: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f837f00 of size 30720\r\n2018-03-13 21:59:44.860528: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f846f00 of size 55552\r\n2018-03-13 21:59:44.860533: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f854800 of size 860160\r\n2018-03-13 21:59:44.860539: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f926800 of size 860160\r\n2018-03-13 21:59:44.860545: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020f9f8800 of size 532224\r\n2018-03-13 21:59:44.860550: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fa7a700 of size 262144\r\n2018-03-13 21:59:44.860556: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020faba700 of size 30720\r\n2018-03-13 21:59:44.860562: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fac1f00 of size 240128\r\n2018-03-13 21:59:44.860568: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fafc900 of size 534016\r\n2018-03-13 21:59:44.860578: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fb86700 of size 499712\r\n2018-03-13 21:59:44.860584: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fc00700 of size 536832\r\n2018-03-13 21:59:44.860590: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fc83800 of size 536320\r\n2018-03-13 21:59:44.860595: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fd06700 of size 536832\r\n2018-03-13 21:59:44.860600: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fd89800 of size 536832\r\n2018-03-13 21:59:44.860606: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020fe0c900 of size 1067520\r\n2018-03-13 21:59:44.860611: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ff11300 of size 536832\r\n2018-03-13 21:59:44.860616: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1020ff94400 of size 536832\r\n2018-03-13 21:59:44.860622: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210017500 of size 536832\r\n2018-03-13 21:59:44.860627: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021009a600 of size 536832\r\n2018-03-13 21:59:44.860632: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021011d700 of size 625664\r\n2018-03-13 21:59:44.860638: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6300 of size 256\r\n2018-03-13 21:59:44.860643: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6400 of size 256\r\n2018-03-13 21:59:44.860649: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6500 of size 256\r\n2018-03-13 21:59:44.860655: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6600 of size 512\r\n2018-03-13 21:59:44.860660: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6800 of size 256\r\n2018-03-13 21:59:44.860665: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6900 of size 256\r\n2018-03-13 21:59:44.860670: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6a00 of size 256\r\n2018-03-13 21:59:44.860675: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6b00 of size 256\r\n2018-03-13 21:59:44.860681: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6c00 of size 256\r\n2018-03-13 21:59:44.860686: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6d00 of size 256\r\n2018-03-13 21:59:44.860692: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6e00 of size 256\r\n2018-03-13 21:59:44.860697: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b6f00 of size 256\r\n2018-03-13 21:59:44.860703: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7000 of size 256\r\n2018-03-13 21:59:44.860708: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7100 of size 256\r\n2018-03-13 21:59:44.860714: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7200 of size 256\r\n2018-03-13 21:59:44.860719: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7300 of size 512\r\n2018-03-13 21:59:44.860725: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7500 of size 256\r\n2018-03-13 21:59:44.860731: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7600 of size 256\r\n2018-03-13 21:59:44.860736: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7700 of size 256\r\n2018-03-13 21:59:44.860741: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7800 of size 256\r\n2018-03-13 21:59:44.860747: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7900 of size 256\r\n2018-03-13 21:59:44.860752: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7a00 of size 256\r\n2018-03-13 21:59:44.860757: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7b00 of size 256\r\n2018-03-13 21:59:44.860763: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7c00 of size 256\r\n2018-03-13 21:59:44.860769: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7d00 of size 512\r\n2018-03-13 21:59:44.860774: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b7f00 of size 256\r\n2018-03-13 21:59:44.860780: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8000 of size 256\r\n2018-03-13 21:59:44.860785: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8100 of size 256\r\n2018-03-13 21:59:44.860790: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8200 of size 256\r\n2018-03-13 21:59:44.860796: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8300 of size 256\r\n2018-03-13 21:59:44.860801: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8400 of size 256\r\n2018-03-13 21:59:44.860807: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8500 of size 256\r\n2018-03-13 21:59:44.860812: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8600 of size 256\r\n2018-03-13 21:59:44.860817: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8700 of size 256\r\n2018-03-13 21:59:44.860823: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8800 of size 256\r\n2018-03-13 21:59:44.860828: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8900 of size 256\r\n2018-03-13 21:59:44.860836: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8a00 of size 512\r\n2018-03-13 21:59:44.860845: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8c00 of size 256\r\n2018-03-13 21:59:44.860852: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8d00 of size 256\r\n2018-03-13 21:59:44.860857: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8e00 of size 256\r\n2018-03-13 21:59:44.860863: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b8f00 of size 256\r\n2018-03-13 21:59:44.860867: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102101b9000 of size 736256\r\n2018-03-13 21:59:44.860873: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026cc00 of size 256\r\n2018-03-13 21:59:44.860879: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026cd00 of size 256\r\n2018-03-13 21:59:44.860884: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026ce00 of size 512\r\n2018-03-13 21:59:44.860890: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d000 of size 256\r\n2018-03-13 21:59:44.860895: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d100 of size 256\r\n2018-03-13 21:59:44.860901: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d200 of size 256\r\n2018-03-13 21:59:44.860921: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d300 of size 256\r\n2018-03-13 21:59:44.860926: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d400 of size 256\r\n2018-03-13 21:59:44.860932: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d500 of size 512\r\n2018-03-13 21:59:44.860937: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d700 of size 256\r\n2018-03-13 21:59:44.860943: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d800 of size 256\r\n2018-03-13 21:59:44.860948: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026d900 of size 256\r\n2018-03-13 21:59:44.860954: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026da00 of size 256\r\n2018-03-13 21:59:44.860972: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026db00 of size 256\r\n2018-03-13 21:59:44.860978: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026dc00 of size 256\r\n2018-03-13 21:59:44.860983: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026dd00 of size 256\r\n2018-03-13 21:59:44.860989: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026de00 of size 256\r\n2018-03-13 21:59:44.860995: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026df00 of size 512\r\n2018-03-13 21:59:44.861000: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e100 of size 256\r\n2018-03-13 21:59:44.861006: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e200 of size 256\r\n2018-03-13 21:59:44.861011: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e300 of size 256\r\n2018-03-13 21:59:44.861016: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e400 of size 256\r\n2018-03-13 21:59:44.861022: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e500 of size 256\r\n2018-03-13 21:59:44.861027: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e600 of size 256\r\n2018-03-13 21:59:44.861033: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e700 of size 512\r\n2018-03-13 21:59:44.861038: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026e900 of size 256\r\n2018-03-13 21:59:44.861043: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026ea00 of size 256\r\n2018-03-13 21:59:44.861049: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026eb00 of size 256\r\n2018-03-13 21:59:44.861054: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026ec00 of size 256\r\n2018-03-13 21:59:44.861059: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026ed00 of size 256\r\n2018-03-13 21:59:44.861065: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026ee00 of size 256\r\n2018-03-13 21:59:44.861070: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026ef00 of size 256\r\n2018-03-13 21:59:44.861076: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f000 of size 256\r\n2018-03-13 21:59:44.861080: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f100 of size 256\r\n2018-03-13 21:59:44.861089: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f200 of size 256\r\n2018-03-13 21:59:44.861095: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f300 of size 256\r\n2018-03-13 21:59:44.861101: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f400 of size 256\r\n2018-03-13 21:59:44.861106: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f500 of size 256\r\n2018-03-13 21:59:44.861111: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f600 of size 512\r\n2018-03-13 21:59:44.861116: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f800 of size 256\r\n2018-03-13 21:59:44.861121: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026f900 of size 512\r\n2018-03-13 21:59:44.861126: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026fb00 of size 256\r\n2018-03-13 21:59:44.861131: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026fc00 of size 256\r\n2018-03-13 21:59:44.861137: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026fd00 of size 256\r\n2018-03-13 21:59:44.861143: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026fe00 of size 256\r\n2018-03-13 21:59:44.861148: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021026ff00 of size 256\r\n2018-03-13 21:59:44.861153: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270000 of size 256\r\n2018-03-13 21:59:44.861159: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270100 of size 256\r\n2018-03-13 21:59:44.861164: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270200 of size 256\r\n2018-03-13 21:59:44.861169: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270300 of size 512\r\n2018-03-13 21:59:44.861175: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270500 of size 256\r\n2018-03-13 21:59:44.861180: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270600 of size 256\r\n2018-03-13 21:59:44.861186: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270700 of size 256\r\n2018-03-13 21:59:44.861191: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270800 of size 256\r\n2018-03-13 21:59:44.861197: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270900 of size 256\r\n2018-03-13 21:59:44.861202: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270a00 of size 256\r\n2018-03-13 21:59:44.861208: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270b00 of size 512\r\n2018-03-13 21:59:44.861213: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270d00 of size 256\r\n2018-03-13 21:59:44.861219: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270e00 of size 256\r\n2018-03-13 21:59:44.861240: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210270f00 of size 256\r\n2018-03-13 21:59:44.861247: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271000 of size 256\r\n2018-03-13 21:59:44.861252: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271100 of size 256\r\n2018-03-13 21:59:44.861258: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271200 of size 256\r\n2018-03-13 21:59:44.861276: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271300 of size 256\r\n2018-03-13 21:59:44.861281: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271400 of size 256\r\n2018-03-13 21:59:44.861286: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271500 of size 256\r\n2018-03-13 21:59:44.861292: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271600 of size 256\r\n2018-03-13 21:59:44.861297: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271700 of size 256\r\n2018-03-13 21:59:44.861302: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271800 of size 256\r\n2018-03-13 21:59:44.861308: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271900 of size 512\r\n2018-03-13 21:59:44.861313: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271b00 of size 256\r\n2018-03-13 21:59:44.861319: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271c00 of size 256\r\n2018-03-13 21:59:44.861324: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271d00 of size 256\r\n2018-03-13 21:59:44.861330: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271e00 of size 256\r\n2018-03-13 21:59:44.861335: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210271f00 of size 256\r\n2018-03-13 21:59:44.861341: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272000 of size 512\r\n2018-03-13 21:59:44.861347: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272200 of size 256\r\n2018-03-13 21:59:44.861356: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272300 of size 256\r\n2018-03-13 21:59:44.861363: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272400 of size 256\r\n2018-03-13 21:59:44.861369: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272500 of size 256\r\n2018-03-13 21:59:44.861375: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272600 of size 256\r\n2018-03-13 21:59:44.861380: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272700 of size 512\r\n2018-03-13 21:59:44.861385: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272900 of size 256\r\n2018-03-13 21:59:44.861391: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272a00 of size 256\r\n2018-03-13 21:59:44.861396: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272b00 of size 256\r\n2018-03-13 21:59:44.861401: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272c00 of size 256\r\n2018-03-13 21:59:44.861407: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272d00 of size 256\r\n2018-03-13 21:59:44.861412: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272e00 of size 256\r\n2018-03-13 21:59:44.861418: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210272f00 of size 256\r\n2018-03-13 21:59:44.861423: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273000 of size 256\r\n2018-03-13 21:59:44.861429: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273100 of size 256\r\n2018-03-13 21:59:44.861434: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273200 of size 512\r\n2018-03-13 21:59:44.861440: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273400 of size 256\r\n2018-03-13 21:59:44.861444: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273500 of size 256\r\n2018-03-13 21:59:44.861451: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273600 of size 256\r\n2018-03-13 21:59:44.861456: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273700 of size 256\r\n2018-03-13 21:59:44.861462: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273800 of size 256\r\n2018-03-13 21:59:44.861466: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273900 of size 256\r\n2018-03-13 21:59:44.861472: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273a00 of size 256\r\n2018-03-13 21:59:44.861478: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273b00 of size 256\r\n2018-03-13 21:59:44.861483: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273c00 of size 256\r\n2018-03-13 21:59:44.861488: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273d00 of size 256\r\n2018-03-13 21:59:44.861495: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273e00 of size 256\r\n2018-03-13 21:59:44.861500: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210273f00 of size 512\r\n2018-03-13 21:59:44.861506: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274100 of size 256\r\n2018-03-13 21:59:44.861510: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274200 of size 256\r\n2018-03-13 21:59:44.861516: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274300 of size 256\r\n2018-03-13 21:59:44.861521: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274400 of size 512\r\n2018-03-13 21:59:44.861526: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274600 of size 256\r\n2018-03-13 21:59:44.861531: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274700 of size 256\r\n2018-03-13 21:59:44.861537: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274800 of size 256\r\n2018-03-13 21:59:44.861542: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274900 of size 256\r\n2018-03-13 21:59:44.861548: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274a00 of size 256\r\n2018-03-13 21:59:44.861553: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274b00 of size 256\r\n2018-03-13 21:59:44.861558: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274c00 of size 256\r\n2018-03-13 21:59:44.861564: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274d00 of size 256\r\n2018-03-13 21:59:44.861569: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274e00 of size 256\r\n2018-03-13 21:59:44.861575: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210274f00 of size 256\r\n2018-03-13 21:59:44.861579: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275000 of size 256\r\n2018-03-13 21:59:44.861585: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275100 of size 512\r\n2018-03-13 21:59:44.861590: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275300 of size 256\r\n2018-03-13 21:59:44.861595: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275400 of size 256\r\n2018-03-13 21:59:44.861601: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275500 of size 256\r\n2018-03-13 21:59:44.861606: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275600 of size 256\r\n2018-03-13 21:59:44.861612: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275700 of size 256\r\n2018-03-13 21:59:44.861617: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275800 of size 256\r\n2018-03-13 21:59:44.861622: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275900 of size 512\r\n2018-03-13 21:59:44.861628: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275b00 of size 256\r\n2018-03-13 21:59:44.861633: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275c00 of size 256\r\n2018-03-13 21:59:44.861639: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275d00 of size 256\r\n2018-03-13 21:59:44.861643: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275e00 of size 256\r\n2018-03-13 21:59:44.861649: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210275f00 of size 256\r\n2018-03-13 21:59:44.861654: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276000 of size 256\r\n2018-03-13 21:59:44.861659: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276100 of size 256\r\n2018-03-13 21:59:44.861665: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276200 of size 512\r\n2018-03-13 21:59:44.861670: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276400 of size 256\r\n2018-03-13 21:59:44.861675: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276500 of size 256\r\n2018-03-13 21:59:44.861681: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276600 of size 256\r\n2018-03-13 21:59:44.861687: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276700 of size 256\r\n2018-03-13 21:59:44.861693: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276800 of size 256\r\n2018-03-13 21:59:44.861700: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276900 of size 256\r\n2018-03-13 21:59:44.861706: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276a00 of size 512\r\n2018-03-13 21:59:44.861712: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276c00 of size 256\r\n2018-03-13 21:59:44.861718: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276d00 of size 256\r\n2018-03-13 21:59:44.861723: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276e00 of size 256\r\n2018-03-13 21:59:44.861729: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210276f00 of size 256\r\n2018-03-13 21:59:44.861734: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277000 of size 256\r\n2018-03-13 21:59:44.861739: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277100 of size 256\r\n2018-03-13 21:59:44.861745: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277200 of size 256\r\n2018-03-13 21:59:44.861751: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277300 of size 256\r\n2018-03-13 21:59:44.861755: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277400 of size 256\r\n2018-03-13 21:59:44.861761: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277500 of size 512\r\n2018-03-13 21:59:44.861767: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277700 of size 256\r\n2018-03-13 21:59:44.861772: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277800 of size 256\r\n2018-03-13 21:59:44.861777: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277900 of size 256\r\n2018-03-13 21:59:44.861783: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277a00 of size 256\r\n2018-03-13 21:59:44.861788: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277b00 of size 256\r\n2018-03-13 21:59:44.861793: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277c00 of size 512\r\n2018-03-13 21:59:44.861799: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277e00 of size 256\r\n2018-03-13 21:59:44.861804: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210277f00 of size 256\r\n2018-03-13 21:59:44.861810: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210278000 of size 7680\r\n2018-03-13 21:59:44.861815: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210279e00 of size 7680\r\n2018-03-13 21:59:44.861821: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021027bc00 of size 7680\r\n2018-03-13 21:59:44.861826: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021027da00 of size 7680\r\n2018-03-13 21:59:44.861832: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021027f800 of size 7680\r\n2018-03-13 21:59:44.861837: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210281600 of size 7680\r\n2018-03-13 21:59:44.861843: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210283400 of size 7680\r\n2018-03-13 21:59:44.861848: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210285200 of size 7680\r\n2018-03-13 21:59:44.861853: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210287000 of size 7680\r\n2018-03-13 21:59:44.861859: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210288e00 of size 7680\r\n2018-03-13 21:59:44.861864: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021028ac00 of size 7680\r\n2018-03-13 21:59:44.861870: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021028ca00 of size 7680\r\n2018-03-13 21:59:44.861875: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021028e800 of size 7680\r\n2018-03-13 21:59:44.861881: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210290600 of size 7680\r\n2018-03-13 21:59:44.861885: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210292400 of size 7680\r\n2018-03-13 21:59:44.861892: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210294200 of size 7680\r\n2018-03-13 21:59:44.861897: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210296000 of size 7680\r\n2018-03-13 21:59:44.861902: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210297e00 of size 7680\r\n2018-03-13 21:59:44.861908: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210299c00 of size 7680\r\n2018-03-13 21:59:44.861913: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021029ba00 of size 7680\r\n2018-03-13 21:59:44.861918: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021029d800 of size 7680\r\n2018-03-13 21:59:44.861924: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021029f600 of size 7680\r\n2018-03-13 21:59:44.861929: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102a1400 of size 7680\r\n2018-03-13 21:59:44.861935: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102a3200 of size 7680\r\n2018-03-13 21:59:44.861940: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102a5000 of size 7680\r\n2018-03-13 21:59:44.861946: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102a6e00 of size 7680\r\n2018-03-13 21:59:44.861951: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102a8c00 of size 7680\r\n2018-03-13 21:59:44.861957: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102aaa00 of size 7680\r\n2018-03-13 21:59:44.861962: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ac800 of size 7680\r\n2018-03-13 21:59:44.861968: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ae600 of size 7680\r\n2018-03-13 21:59:44.861973: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102b0400 of size 7680\r\n2018-03-13 21:59:44.861978: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102b2200 of size 7680\r\n2018-03-13 21:59:44.861999: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102b4000 of size 7680\r\n2018-03-13 21:59:44.862005: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102b5e00 of size 7680\r\n2018-03-13 21:59:44.862010: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102b7c00 of size 7680\r\n2018-03-13 21:59:44.862015: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102b9a00 of size 7680\r\n2018-03-13 21:59:44.862021: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102bb800 of size 7680\r\n2018-03-13 21:59:44.862038: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102bd600 of size 7680\r\n2018-03-13 21:59:44.862044: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102bf400 of size 7680\r\n2018-03-13 21:59:44.862050: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102c1200 of size 7680\r\n2018-03-13 21:59:44.862055: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102c3000 of size 7680\r\n2018-03-13 21:59:44.862060: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102c4e00 of size 7680\r\n2018-03-13 21:59:44.862066: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102c6c00 of size 7680\r\n2018-03-13 21:59:44.862071: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102c8a00 of size 7680\r\n2018-03-13 21:59:44.862077: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ca800 of size 7680\r\n2018-03-13 21:59:44.862082: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102cc600 of size 7680\r\n2018-03-13 21:59:44.862087: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ce400 of size 7680\r\n2018-03-13 21:59:44.862093: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102d0200 of size 7680\r\n2018-03-13 21:59:44.862098: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102d2000 of size 7680\r\n2018-03-13 21:59:44.862104: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102d3e00 of size 7680\r\n2018-03-13 21:59:44.862109: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102d5c00 of size 7680\r\n2018-03-13 21:59:44.862115: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102d7a00 of size 7680\r\n2018-03-13 21:59:44.862120: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102d9800 of size 7680\r\n2018-03-13 21:59:44.862127: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102db600 of size 7680\r\n2018-03-13 21:59:44.862132: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102dd400 of size 7680\r\n2018-03-13 21:59:44.862138: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102df200 of size 7680\r\n2018-03-13 21:59:44.862143: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102e1000 of size 7680\r\n2018-03-13 21:59:44.862149: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102e2e00 of size 7680\r\n2018-03-13 21:59:44.862154: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102e4c00 of size 7680\r\n2018-03-13 21:59:44.862159: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102e6a00 of size 7680\r\n2018-03-13 21:59:44.862165: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102e8800 of size 7680\r\n2018-03-13 21:59:44.862170: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ea600 of size 7680\r\n2018-03-13 21:59:44.862176: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ec400 of size 7680\r\n2018-03-13 21:59:44.862181: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ee200 of size 7680\r\n2018-03-13 21:59:44.862187: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102f0000 of size 7680\r\n2018-03-13 21:59:44.862191: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102f1e00 of size 7680\r\n2018-03-13 21:59:44.862197: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102f3c00 of size 7680\r\n2018-03-13 21:59:44.862202: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102f5a00 of size 7680\r\n2018-03-13 21:59:44.862208: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102f7800 of size 7680\r\n2018-03-13 21:59:44.862213: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102f9600 of size 7680\r\n2018-03-13 21:59:44.862219: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102fb400 of size 7680\r\n2018-03-13 21:59:44.862239: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102fd200 of size 7680\r\n2018-03-13 21:59:44.862244: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102102ff000 of size 7680\r\n2018-03-13 21:59:44.862250: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210300e00 of size 7680\r\n2018-03-13 21:59:44.862255: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210302c00 of size 7680\r\n2018-03-13 21:59:44.862274: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210304a00 of size 7680\r\n2018-03-13 21:59:44.862279: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210306800 of size 7680\r\n2018-03-13 21:59:44.862285: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210308600 of size 7680\r\n2018-03-13 21:59:44.862290: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021030a400 of size 7680\r\n2018-03-13 21:59:44.862296: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021030c200 of size 7680\r\n2018-03-13 21:59:44.862300: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021030e000 of size 7680\r\n2018-03-13 21:59:44.862306: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021030fe00 of size 7680\r\n2018-03-13 21:59:44.862312: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210311c00 of size 7680\r\n2018-03-13 21:59:44.862317: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210313a00 of size 7680\r\n2018-03-13 21:59:44.862326: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210315800 of size 7680\r\n2018-03-13 21:59:44.862332: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210317600 of size 7680\r\n2018-03-13 21:59:44.862337: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210319400 of size 7680\r\n2018-03-13 21:59:44.862343: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021031b200 of size 7680\r\n2018-03-13 21:59:44.862348: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021031d000 of size 7680\r\n2018-03-13 21:59:44.862353: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021031ee00 of size 7680\r\n2018-03-13 21:59:44.862359: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210320c00 of size 7680\r\n2018-03-13 21:59:44.862364: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210322a00 of size 7680\r\n2018-03-13 21:59:44.862369: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210324800 of size 7680\r\n2018-03-13 21:59:44.862375: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210326600 of size 7680\r\n2018-03-13 21:59:44.862380: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210328400 of size 7680\r\n2018-03-13 21:59:44.862386: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021032a200 of size 7680\r\n2018-03-13 21:59:44.862391: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021032c000 of size 7680\r\n2018-03-13 21:59:44.862396: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021032de00 of size 7680\r\n2018-03-13 21:59:44.862402: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021032fc00 of size 7680\r\n2018-03-13 21:59:44.862407: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210331a00 of size 7680\r\n2018-03-13 21:59:44.862412: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210333800 of size 7680\r\n2018-03-13 21:59:44.862418: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210335600 of size 7680\r\n2018-03-13 21:59:44.862424: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210337400 of size 7680\r\n2018-03-13 21:59:44.862429: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210339200 of size 7680\r\n2018-03-13 21:59:44.862434: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021033b000 of size 7680\r\n2018-03-13 21:59:44.862440: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021033ce00 of size 7680\r\n2018-03-13 21:59:44.862446: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021033ec00 of size 7680\r\n2018-03-13 21:59:44.862451: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210340a00 of size 7680\r\n2018-03-13 21:59:44.862457: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210342800 of size 7680\r\n2018-03-13 21:59:44.862462: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210344600 of size 7680\r\n2018-03-13 21:59:44.862467: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210346400 of size 7680\r\n2018-03-13 21:59:44.862473: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210348200 of size 7680\r\n2018-03-13 21:59:44.862478: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021034a000 of size 7680\r\n2018-03-13 21:59:44.862484: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021034be00 of size 7680\r\n2018-03-13 21:59:44.862489: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021034dc00 of size 7680\r\n2018-03-13 21:59:44.862494: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021034fa00 of size 7680\r\n2018-03-13 21:59:44.862500: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210351800 of size 7680\r\n2018-03-13 21:59:44.862505: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210353600 of size 7680\r\n2018-03-13 21:59:44.862510: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210355400 of size 7680\r\n2018-03-13 21:59:44.862516: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210357200 of size 7680\r\n2018-03-13 21:59:44.862521: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210359000 of size 7680\r\n2018-03-13 21:59:44.862527: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021035ae00 of size 7680\r\n2018-03-13 21:59:44.862533: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021035cc00 of size 7680\r\n2018-03-13 21:59:44.862538: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021035ea00 of size 7680\r\n2018-03-13 21:59:44.862543: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210360800 of size 7680\r\n2018-03-13 21:59:44.862549: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210362600 of size 7680\r\n2018-03-13 21:59:44.862554: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210364400 of size 7680\r\n2018-03-13 21:59:44.862560: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210366200 of size 7680\r\n2018-03-13 21:59:44.862565: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210368000 of size 7680\r\n2018-03-13 21:59:44.862570: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210369e00 of size 7680\r\n2018-03-13 21:59:44.862577: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021036bc00 of size 7680\r\n2018-03-13 21:59:44.862586: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021036da00 of size 7680\r\n2018-03-13 21:59:44.862591: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021036f800 of size 7680\r\n2018-03-13 21:59:44.862597: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210371600 of size 7680\r\n2018-03-13 21:59:44.862602: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210373400 of size 7680\r\n2018-03-13 21:59:44.862608: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210375200 of size 7680\r\n2018-03-13 21:59:44.862613: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210377000 of size 7680\r\n2018-03-13 21:59:44.862618: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210378e00 of size 7680\r\n2018-03-13 21:59:44.862624: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021037ac00 of size 7680\r\n2018-03-13 21:59:44.862629: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021037ca00 of size 7680\r\n2018-03-13 21:59:44.862635: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021037e800 of size 7680\r\n2018-03-13 21:59:44.862640: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210380600 of size 7680\r\n2018-03-13 21:59:44.862646: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210382400 of size 7680\r\n2018-03-13 21:59:44.862651: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210384200 of size 7680\r\n2018-03-13 21:59:44.862657: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210386000 of size 30720\r\n2018-03-13 21:59:44.862662: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021038d800 of size 736256\r\n2018-03-13 21:59:44.862668: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10210441400 of size 184064\r\n2018-03-13 21:59:44.862673: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021046e300 of size 184064\r\n2018-03-13 21:59:44.862678: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021049b200 of size 184064\r\n2018-03-13 21:59:44.862684: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102104c8100 of size 184064\r\n2018-03-13 21:59:44.862689: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102104f5000 of size 256\r\n2018-03-13 21:59:44.862694: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102104f5100 of size 256\r\n2018-03-13 21:59:44.862699: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x102104f5200 of size 12882432\r\n2018-03-13 21:59:44.862705: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113e400 of size 256\r\n2018-03-13 21:59:44.862710: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113e500 of size 256\r\n2018-03-13 21:59:44.862716: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113e600 of size 256\r\n2018-03-13 21:59:44.862722: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113e700 of size 256\r\n2018-03-13 21:59:44.862727: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113e800 of size 256\r\n2018-03-13 21:59:44.862733: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113e900 of size 256\r\n2018-03-13 21:59:44.862738: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113ea00 of size 256\r\n2018-03-13 21:59:44.862744: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113eb00 of size 256\r\n2018-03-13 21:59:44.862749: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113ec00 of size 256\r\n2018-03-13 21:59:44.862754: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113ed00 of size 256\r\n2018-03-13 21:59:44.862759: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113ee00 of size 256\r\n2018-03-13 21:59:44.862764: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113ef00 of size 256\r\n2018-03-13 21:59:44.862770: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f000 of size 256\r\n2018-03-13 21:59:44.862775: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f100 of size 256\r\n2018-03-13 21:59:44.862781: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f200 of size 256\r\n2018-03-13 21:59:44.862786: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f300 of size 256\r\n2018-03-13 21:59:44.862792: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f400 of size 256\r\n2018-03-13 21:59:44.862797: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f500 of size 256\r\n2018-03-13 21:59:44.862802: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f600 of size 256\r\n2018-03-13 21:59:44.862806: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f700 of size 256\r\n2018-03-13 21:59:44.862812: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f800 of size 256\r\n2018-03-13 21:59:44.862817: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113f900 of size 256\r\n2018-03-13 21:59:44.862823: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113fa00 of size 256\r\n2018-03-13 21:59:44.862828: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113fb00 of size 256\r\n2018-03-13 21:59:44.862834: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113fc00 of size 256\r\n2018-03-13 21:59:44.862839: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113fd00 of size 256\r\n2018-03-13 21:59:44.862845: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113fe00 of size 256\r\n2018-03-13 21:59:44.862850: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021113ff00 of size 256\r\n2018-03-13 21:59:44.862856: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140000 of size 256\r\n2018-03-13 21:59:44.862861: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140100 of size 256\r\n2018-03-13 21:59:44.862867: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140200 of size 256\r\n2018-03-13 21:59:44.862872: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140300 of size 256\r\n2018-03-13 21:59:44.862878: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140400 of size 256\r\n2018-03-13 21:59:44.862883: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140500 of size 256\r\n2018-03-13 21:59:44.862889: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140600 of size 256\r\n2018-03-13 21:59:44.862894: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140700 of size 256\r\n2018-03-13 21:59:44.862900: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140800 of size 256\r\n2018-03-13 21:59:44.862904: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140900 of size 256\r\n2018-03-13 21:59:44.862910: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140a00 of size 256\r\n2018-03-13 21:59:44.862916: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140b00 of size 256\r\n2018-03-13 21:59:44.862921: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140c00 of size 256\r\n2018-03-13 21:59:44.862927: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140d00 of size 256\r\n2018-03-13 21:59:44.862932: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140e00 of size 256\r\n2018-03-13 21:59:44.862938: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211140f00 of size 256\r\n2018-03-13 21:59:44.862943: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141000 of size 256\r\n2018-03-13 21:59:44.862949: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141100 of size 256\r\n2018-03-13 21:59:44.862954: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141200 of size 256\r\n2018-03-13 21:59:44.862959: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141300 of size 256\r\n2018-03-13 21:59:44.862964: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141400 of size 256\r\n2018-03-13 21:59:44.862969: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141500 of size 256\r\n2018-03-13 21:59:44.862975: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141600 of size 256\r\n2018-03-13 21:59:44.862980: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141700 of size 256\r\n2018-03-13 21:59:44.862985: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141800 of size 256\r\n2018-03-13 21:59:44.862990: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141900 of size 256\r\n2018-03-13 21:59:44.862995: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141a00 of size 256\r\n2018-03-13 21:59:44.863000: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141b00 of size 256\r\n2018-03-13 21:59:44.863006: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141c00 of size 256\r\n2018-03-13 21:59:44.863010: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141d00 of size 256\r\n2018-03-13 21:59:44.863016: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141e00 of size 256\r\n2018-03-13 21:59:44.863021: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211141f00 of size 256\r\n2018-03-13 21:59:44.863027: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142000 of size 256\r\n2018-03-13 21:59:44.863032: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142100 of size 256\r\n2018-03-13 21:59:44.863038: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142200 of size 256\r\n2018-03-13 21:59:44.863043: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142300 of size 256\r\n2018-03-13 21:59:44.863049: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142400 of size 256\r\n2018-03-13 21:59:44.863053: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142500 of size 256\r\n2018-03-13 21:59:44.863074: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142600 of size 256\r\n2018-03-13 21:59:44.863079: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142700 of size 256\r\n2018-03-13 21:59:44.863085: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142800 of size 256\r\n2018-03-13 21:59:44.863091: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142900 of size 256\r\n2018-03-13 21:59:44.863097: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142a00 of size 256\r\n2018-03-13 21:59:44.863103: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142b00 of size 256\r\n2018-03-13 21:59:44.863107: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142c00 of size 256\r\n2018-03-13 21:59:44.863113: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142d00 of size 256\r\n2018-03-13 21:59:44.863117: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142e00 of size 256\r\n2018-03-13 21:59:44.863123: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211142f00 of size 256\r\n2018-03-13 21:59:44.863129: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143000 of size 256\r\n2018-03-13 21:59:44.863134: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143100 of size 256\r\n2018-03-13 21:59:44.863139: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143200 of size 256\r\n2018-03-13 21:59:44.863145: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143300 of size 256\r\n2018-03-13 21:59:44.863150: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143400 of size 256\r\n2018-03-13 21:59:44.863156: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143500 of size 256\r\n2018-03-13 21:59:44.863161: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143600 of size 256\r\n2018-03-13 21:59:44.863167: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143700 of size 256\r\n2018-03-13 21:59:44.863172: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143800 of size 256\r\n2018-03-13 21:59:44.863178: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143900 of size 256\r\n2018-03-13 21:59:44.863183: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143a00 of size 256\r\n2018-03-13 21:59:44.863188: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143b00 of size 256\r\n2018-03-13 21:59:44.863193: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143c00 of size 256\r\n2018-03-13 21:59:44.863199: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143d00 of size 256\r\n2018-03-13 21:59:44.863204: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143e00 of size 256\r\n2018-03-13 21:59:44.863210: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211143f00 of size 256\r\n2018-03-13 21:59:44.863216: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144000 of size 256\r\n2018-03-13 21:59:44.863223: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144100 of size 256\r\n2018-03-13 21:59:44.863228: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144200 of size 256\r\n2018-03-13 21:59:44.863234: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144300 of size 256\r\n2018-03-13 21:59:44.863239: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144400 of size 256\r\n2018-03-13 21:59:44.863257: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144500 of size 256\r\n2018-03-13 21:59:44.863262: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144600 of size 256\r\n2018-03-13 21:59:44.863268: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144700 of size 256\r\n2018-03-13 21:59:44.863274: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144800 of size 256\r\n2018-03-13 21:59:44.863279: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144900 of size 256\r\n2018-03-13 21:59:44.863284: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144a00 of size 256\r\n2018-03-13 21:59:44.863290: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144b00 of size 256\r\n2018-03-13 21:59:44.863295: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144c00 of size 256\r\n2018-03-13 21:59:44.863300: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144d00 of size 256\r\n2018-03-13 21:59:44.863306: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144e00 of size 256\r\n2018-03-13 21:59:44.863312: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211144f00 of size 256\r\n2018-03-13 21:59:44.863317: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145000 of size 256\r\n2018-03-13 21:59:44.863322: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145100 of size 256\r\n2018-03-13 21:59:44.863328: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145200 of size 256\r\n2018-03-13 21:59:44.863333: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145300 of size 256\r\n2018-03-13 21:59:44.863339: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145400 of size 256\r\n2018-03-13 21:59:44.863344: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145500 of size 256\r\n2018-03-13 21:59:44.863350: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145600 of size 256\r\n2018-03-13 21:59:44.863359: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145700 of size 256\r\n2018-03-13 21:59:44.863366: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145800 of size 256\r\n2018-03-13 21:59:44.863373: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145900 of size 256\r\n2018-03-13 21:59:44.863378: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145a00 of size 256\r\n2018-03-13 21:59:44.863383: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145b00 of size 256\r\n2018-03-13 21:59:44.863389: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145c00 of size 256\r\n2018-03-13 21:59:44.863394: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145d00 of size 256\r\n2018-03-13 21:59:44.863400: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145e00 of size 256\r\n2018-03-13 21:59:44.863405: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211145f00 of size 256\r\n2018-03-13 21:59:44.863411: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146000 of size 256\r\n2018-03-13 21:59:44.863416: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146100 of size 256\r\n2018-03-13 21:59:44.863422: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146200 of size 256\r\n2018-03-13 21:59:44.863427: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146300 of size 256\r\n2018-03-13 21:59:44.863433: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146400 of size 256\r\n2018-03-13 21:59:44.863438: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146500 of size 256\r\n2018-03-13 21:59:44.863444: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146600 of size 256\r\n2018-03-13 21:59:44.863448: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146700 of size 256\r\n2018-03-13 21:59:44.863455: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146800 of size 256\r\n2018-03-13 21:59:44.863460: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146900 of size 256\r\n2018-03-13 21:59:44.863466: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146a00 of size 256\r\n2018-03-13 21:59:44.863470: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146b00 of size 256\r\n2018-03-13 21:59:44.863476: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146c00 of size 256\r\n2018-03-13 21:59:44.863481: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146d00 of size 256\r\n2018-03-13 21:59:44.863487: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146e00 of size 256\r\n2018-03-13 21:59:44.863492: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211146f00 of size 256\r\n2018-03-13 21:59:44.863498: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147000 of size 256\r\n2018-03-13 21:59:44.863503: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147100 of size 256\r\n2018-03-13 21:59:44.863509: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147200 of size 256\r\n2018-03-13 21:59:44.863514: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147300 of size 256\r\n2018-03-13 21:59:44.863520: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147400 of size 256\r\n2018-03-13 21:59:44.863525: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147500 of size 256\r\n2018-03-13 21:59:44.863531: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147600 of size 256\r\n2018-03-13 21:59:44.863536: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147700 of size 256\r\n2018-03-13 21:59:44.863542: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147800 of size 256\r\n2018-03-13 21:59:44.863546: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147900 of size 256\r\n2018-03-13 21:59:44.863553: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147a00 of size 256\r\n2018-03-13 21:59:44.863557: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147b00 of size 256\r\n2018-03-13 21:59:44.863562: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147c00 of size 256\r\n2018-03-13 21:59:44.863567: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147d00 of size 256\r\n2018-03-13 21:59:44.863573: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147e00 of size 256\r\n2018-03-13 21:59:44.863578: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211147f00 of size 256\r\n2018-03-13 21:59:44.863584: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148000 of size 256\r\n2018-03-13 21:59:44.863588: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148100 of size 256\r\n2018-03-13 21:59:44.863594: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148200 of size 256\r\n2018-03-13 21:59:44.863600: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148300 of size 256\r\n2018-03-13 21:59:44.863607: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148400 of size 256\r\n2018-03-13 21:59:44.863615: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148500 of size 256\r\n2018-03-13 21:59:44.863622: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148600 of size 256\r\n2018-03-13 21:59:44.863627: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148700 of size 256\r\n2018-03-13 21:59:44.863633: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148800 of size 256\r\n2018-03-13 21:59:44.863638: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148900 of size 256\r\n2018-03-13 21:59:44.863643: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148a00 of size 256\r\n2018-03-13 21:59:44.863649: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148b00 of size 256\r\n2018-03-13 21:59:44.863653: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148c00 of size 256\r\n2018-03-13 21:59:44.863659: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148d00 of size 256\r\n2018-03-13 21:59:44.863664: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148e00 of size 256\r\n2018-03-13 21:59:44.863670: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211148f00 of size 256\r\n2018-03-13 21:59:44.863675: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149000 of size 256\r\n2018-03-13 21:59:44.863681: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149100 of size 256\r\n2018-03-13 21:59:44.863686: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149200 of size 256\r\n2018-03-13 21:59:44.863691: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149300 of size 256\r\n2018-03-13 21:59:44.863697: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149400 of size 256\r\n2018-03-13 21:59:44.863702: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149500 of size 256\r\n2018-03-13 21:59:44.863706: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149600 of size 256\r\n2018-03-13 21:59:44.863712: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149700 of size 256\r\n2018-03-13 21:59:44.863718: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149800 of size 256\r\n2018-03-13 21:59:44.863723: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149900 of size 256\r\n2018-03-13 21:59:44.863728: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149a00 of size 256\r\n2018-03-13 21:59:44.863734: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149b00 of size 256\r\n2018-03-13 21:59:44.863740: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149c00 of size 256\r\n2018-03-13 21:59:44.863744: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149d00 of size 256\r\n2018-03-13 21:59:44.863750: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149e00 of size 256\r\n2018-03-13 21:59:44.863755: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x10211149f00 of size 256\r\n2018-03-13 21:59:44.863761: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021114a000 of size 256\r\n2018-03-13 21:59:44.863766: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021114a100 of size 256\r\n2018-03-13 21:59:44.863771: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021114a200 of size 256\r\n2018-03-13 21:59:44.863777: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021114a300 of size 256\r\n2018-03-13 21:59:44.863782: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021114a400 of size 256\r\n2018-03-13 21:59:44.863788: I tensorflow/core/common_runtime/bfc_allocator.cc:661] Chunk at 0x1021114a500 of size 10910600704\r\n2018-03-13 21:59:44.863796: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f3df200 of size 1024\r\n2018-03-13 21:59:44.863802: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f402500 of size 3584\r\n2018-03-13 21:59:44.863807: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f425e00 of size 1024\r\n2018-03-13 21:59:44.863813: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f4cc200 of size 2048\r\n2018-03-13 21:59:44.863819: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f7c7e00 of size 1280\r\n2018-03-13 21:59:44.863825: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f7d4700 of size 1280\r\n2018-03-13 21:59:44.863830: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f7ffa00 of size 15616\r\n2018-03-13 21:59:44.863835: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f816300 of size 15360\r\n2018-03-13 21:59:44.863841: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020f83f700 of size 30720\r\n2018-03-13 21:59:44.863847: I tensorflow/core/common_runtime/bfc_allocator.cc:670] Free at 0x1020fb7ef00 of size 30720\r\n2018-03-13 21:59:44.863853: I tensorflow/core/common_runtime/bfc_allocator.cc:676]      Summary of in-use Chunks by size: \r\n2018-03-13 21:59:44.863862: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 709 Chunks of size 256 totalling 177.2KiB\r\n2018-03-13 21:59:44.863869: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 58 Chunks of size 512 totalling 29.0KiB\r\n2018-03-13 21:59:44.863875: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 768 totalling 768B\r\n2018-03-13 21:59:44.863881: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 10 Chunks of size 1024 totalling 10.0KiB\r\n2018-03-13 21:59:44.863887: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 3 Chunks of size 1280 totalling 3.8KiB\r\n2018-03-13 21:59:44.863892: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 4 Chunks of size 1792 totalling 7.0KiB\r\n2018-03-13 21:59:44.863898: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 5 Chunks of size 2048 totalling 10.0KiB\r\n2018-03-13 21:59:44.863904: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 2304 totalling 4.5KiB\r\n2018-03-13 21:59:44.863910: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 2560 totalling 2.5KiB\r\n2018-03-13 21:59:44.863915: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 2816 totalling 2.8KiB\r\n2018-03-13 21:59:44.863920: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 3 Chunks of size 3584 totalling 10.5KiB\r\n2018-03-13 21:59:44.863927: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 4096 totalling 8.0KiB\r\n2018-03-13 21:59:44.863932: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 4 Chunks of size 4608 totalling 18.0KiB\r\n2018-03-13 21:59:44.863938: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 5120 totalling 5.0KiB\r\n2018-03-13 21:59:44.863944: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 5376 totalling 10.5KiB\r\n2018-03-13 21:59:44.863950: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 7424 totalling 7.2KiB\r\n2018-03-13 21:59:44.863956: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 312 Chunks of size 7680 totalling 2.29MiB\r\n2018-03-13 21:59:44.863961: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 7936 totalling 7.8KiB\r\n2018-03-13 21:59:44.863967: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 3 Chunks of size 8192 totalling 24.0KiB\r\n2018-03-13 21:59:44.863973: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 4 Chunks of size 9216 totalling 36.0KiB\r\n2018-03-13 21:59:44.863979: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 9984 totalling 9.8KiB\r\n2018-03-13 21:59:44.863985: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 10240 totalling 10.0KiB\r\n2018-03-13 21:59:44.863990: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 11776 totalling 11.5KiB\r\n2018-03-13 21:59:44.863996: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 12288 totalling 24.0KiB\r\n2018-03-13 21:59:44.864003: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 14592 totalling 14.2KiB\r\n2018-03-13 21:59:44.864009: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 15104 totalling 29.5KiB\r\n2018-03-13 21:59:44.864016: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 18432 totalling 36.0KiB\r\n2018-03-13 21:59:44.864022: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 6 Chunks of size 24576 totalling 144.0KiB\r\n2018-03-13 21:59:44.864029: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 12 Chunks of size 30720 totalling 360.0KiB\r\n2018-03-13 21:59:44.864035: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 32768 totalling 64.0KiB\r\n2018-03-13 21:59:44.864041: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 46080 totalling 45.0KiB\r\n2018-03-13 21:59:44.864047: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 49152 totalling 96.0KiB\r\n2018-03-13 21:59:44.864053: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 55552 totalling 54.2KiB\r\n2018-03-13 21:59:44.864058: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 60928 totalling 59.5KiB\r\n2018-03-13 21:59:44.864064: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 4 Chunks of size 65536 totalling 256.0KiB\r\n2018-03-13 21:59:44.864070: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 98304 totalling 192.0KiB\r\n2018-03-13 21:59:44.864076: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 4 Chunks of size 131072 totalling 512.0KiB\r\n2018-03-13 21:59:44.864081: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 8 Chunks of size 184064 totalling 1.40MiB\r\n2018-03-13 21:59:44.864087: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 240128 totalling 234.5KiB\r\n2018-03-13 21:59:44.864094: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 3 Chunks of size 262144 totalling 768.0KiB\r\n2018-03-13 21:59:44.864100: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 6 Chunks of size 430080 totalling 2.46MiB\r\n2018-03-13 21:59:44.864106: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 499712 totalling 488.0KiB\r\n2018-03-13 21:59:44.864111: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 524288 totalling 1.00MiB\r\n2018-03-13 21:59:44.864117: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 532224 totalling 519.8KiB\r\n2018-03-13 21:59:44.864123: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 534016 totalling 521.5KiB\r\n2018-03-13 21:59:44.864129: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 536320 totalling 523.8KiB\r\n2018-03-13 21:59:44.864149: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 7 Chunks of size 536832 totalling 3.58MiB\r\n2018-03-13 21:59:44.864155: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 625664 totalling 611.0KiB\r\n2018-03-13 21:59:44.864163: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 677888 totalling 662.0KiB\r\n2018-03-13 21:59:44.864169: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 3 Chunks of size 736256 totalling 2.11MiB\r\n2018-03-13 21:59:44.864174: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 860160 totalling 1.64MiB\r\n2018-03-13 21:59:44.864180: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 1048576 totalling 2.00MiB\r\n2018-03-13 21:59:44.864186: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 1067520 totalling 1.02MiB\r\n2018-03-13 21:59:44.864192: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 3 Chunks of size 1179648 totalling 3.38MiB\r\n2018-03-13 21:59:44.864198: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 1720320 totalling 3.28MiB\r\n2018-03-13 21:59:44.864203: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 2 Chunks of size 12882432 totalling 24.57MiB\r\n2018-03-13 21:59:44.864209: I tensorflow/core/common_runtime/bfc_allocator.cc:679] 1 Chunks of size 10910600704 totalling 10.16GiB\r\n2018-03-13 21:59:44.864215: I tensorflow/core/common_runtime/bfc_allocator.cc:683] Sum Total of in-use chunks: 10.21GiB\r\n2018-03-13 21:59:44.864228: I tensorflow/core/common_runtime/bfc_allocator.cc:685] Stats: \r\nLimit:                 10968576820\r\nInUse:                 10968474112\r\nMaxInUse:              10968477952\r\nNumAllocs:                  779120\r\nMaxAllocSize:          10910600704\r\n\r\n2018-03-13 21:59:44.864302: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ***************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n2018-03-13 21:59:44.865043: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n2018-03-13 21:59:44.866899: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.867009: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.867294: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.867331: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.867488: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.867525: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.867646: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.868072: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.868157: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.868303: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.868351: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.868670: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.869464: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.869526: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.869538: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.869852: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.870282: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.870358: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.870575: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.870704: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.870971: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.871354: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.871392: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.871426: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.871736: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.871936: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.872103: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.872924: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.872974: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.875581: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.879603: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.880900: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.880959: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.882434: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.883553: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.883996: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.884396: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.884762: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.884811: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.886082: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.887126: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.887543: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.888555: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.889075: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.890308: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.893012: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.895479: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.897031: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.897207: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.897935: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.898023: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.898068: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.898086: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.898795: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.899419: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.899428: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.899505: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.899899: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900093: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900400: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900661: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900722: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900786: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900812: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900906: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.900960: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.901567: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.902509: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.902827: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.903117: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.903427: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.903817: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.904486: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.904768: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.905273: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.906383: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.906688: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.907023: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.907185: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.907518: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.908547: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.909451: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.909776: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.909979: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911313: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911635: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911738: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911775: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911849: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911860: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911893: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.911933: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.912020: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.912103: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.912264: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.912579: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.912669: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.912776: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913080: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913158: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913218: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913334: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913411: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913490: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913875: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.913895: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914029: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914081: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914315: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914388: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914504: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914577: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914618: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914699: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914702: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914749: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914750: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.914958: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n2018-03-13 21:59:44.915987: W tensorflow/core/framework/op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.ResourceExhaustedError'>, OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n\t [[Node: FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read/_5248 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_13633_FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read)]]\r\n\r\nCaused by op 'unstack_2', defined at:\r\n  File \"/xxx/xxx/object_detection/train.py\", line 163, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/xxx/xxx/object_detection/train.py\", line 159, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/xxx/xxx/object_detection/trainer.py\", line 228, in train\r\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\r\n  File \"/xxx/xxx/slim/deployment/model_deploy.py\", line 194, in create_clones\r\n    outputs = model_fn(*args, **kwargs)\r\n  File \"/xxx/xxx/object_detection/trainer.py\", line 153, in _create_losses\r\n    train_config.merge_multiple_label_boxes)\r\n  File \"/xxx/xxx/object_detection/trainer.py\", line 112, in get_inputs\r\n    read_data_list = input_queue.dequeue()\r\n  File \"/xxx/xxx/object_detection/core/batcher.py\", line 116, in dequeue\r\n    unbatched_tensor_list = tf.unstack(batched_tensor)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1027, in unstack\r\n    return gen_array_ops._unpack(value, num=num, axis=axis, name=name)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5868, in _unpack\r\n    \"Unpack\", value=value, num=num, axis=axis, name=name)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n\t [[Node: FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read/_5248 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_13633_FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read)]]\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n\t [[Node: FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read/_5248 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_13633_FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/xxx/xxx/object_detection/train.py\", line 163, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/xxx/xxx/object_detection/train.py\", line 159, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/xxx/xxx/object_detection/trainer.py\", line 332, in train\r\n    saver=saver)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 763, in train\r\n    sess, train_op, global_step, train_step_kwargs)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 487, in train_step\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n\t [[Node: FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read/_5248 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_13633_FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read)]]\r\n\r\nCaused by op 'unstack_2', defined at:\r\n  File \"/xxx/xxx/object_detection/train.py\", line 163, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/xxx/xxx/object_detection/train.py\", line 159, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/xxx/xxx/object_detection/trainer.py\", line 228, in train\r\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\r\n  File \"/xxx/xxx/slim/deployment/model_deploy.py\", line 194, in create_clones\r\n    outputs = model_fn(*args, **kwargs)\r\n  File \"/xxx/xxx/object_detection/trainer.py\", line 153, in _create_losses\r\n    train_config.merge_multiple_label_boxes)\r\n  File \"/xxx/xxx/object_detection/trainer.py\", line 112, in get_inputs\r\n    read_data_list = input_queue.dequeue()\r\n  File \"/xxx/xxx/object_detection/core/batcher.py\", line 116, in dequeue\r\n    unbatched_tensor_list = tf.unstack(batched_tensor)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 1027, in unstack\r\n    return gen_array_ops._unpack(value, num=num, axis=axis, name=name)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5868, in _unpack\r\n    \"Unpack\", value=value, num=num, axis=axis, name=name)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/python3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,5095,3729,3]\r\n\t [[Node: unstack_2 = Unpack[T=DT_FLOAT, axis=0, num=24, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](prefetch_queue_Dequeue/_3249)]]\r\n\t [[Node: FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read/_5248 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_13633_FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/BatchNorm/gamma/read)]]\r\n\r\n\r\nProcess finished with exit code 1\r\n`", "comments": ["ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,5095,3729,3]\r\n\r\nThis just means that you don't have enough GPU memory for your setting.\r\nyou may try to reduce the size of the model.", "I fixed this issue by changed the config file to \"faster_rcnn_inception_resnet_v2_atrous_coco.config\".", "I fixed this issue by changing the batch size to a lower value. Or you should raise the per_process_gpu_memory_fraction in your model.", "hello, \r\ni am beginner of tensorflow and Neural network, I am working on a semantic segmentation project with Neural network  mobilenet/Unet.\r\ni reduce the batch size and i fix it to 1 and the image have the size 500*500. \r\n\r\nthe machine utilised is 64 bit: lenovo i3 500,  RAM 4G, OS wind10\r\n\r\nwhen l run the training, i have the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,500,500,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[Node: BatchNorm_19/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](SeparableConv2d_9/BiasAdd, BatchNorm_23/Const, BatchNorm_19/beta/read, BatchNorm_23/Const_1, BatchNorm_23/Const_1)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\henda boudegga\\Desktop\\retinal-image-segmentation - Copie30-8\\test-main.py\", line 283, in <module>\r\n    _,current=sess.run([opt,cost],feed_dict={net_input:input_image_batch, net_output:segmented_image_batch})\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[1,500,500,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[Node: BatchNorm_19/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](SeparableConv2d_9/BiasAdd, BatchNorm_23/Const, BatchNorm_19/beta/read, BatchNorm_23/Const_1, BatchNorm_23/Const_1)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'BatchNorm_19/FusedBatchNorm', defined at:\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\idlelib\\run.py\", line 124, in main\r\n    ret = method(*args, **kwargs)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\idlelib\\run.py\", line 351, in runcode\r\n    exec(code, self.locals)\r\n  File \"C:\\Users\\henda boudegga\\Desktop\\retinal-image-segmentation - Copie30-8\\test-main.py\", line 190, in <module>\r\n    network = build_mobile_unet(net_input, preset_model = args.model, num_classes=num_classes)\r\n  File \"C:\\Users\\henda boudegga\\Desktop\\retinal-image-segmentation - Copie30-8\\MobileUNet.py\", line 80, in build_mobile_unet\r\n    b10 = DepthwiseSeparableConvBlock(b09, 512)\r\n  File \"C:\\Users\\henda boudegga\\Desktop\\retinal-image-segmentation - Copie30-8\\MobileUNet.py\", line 25, in DepthwiseSeparableConvBlock\r\n    net = slim.batch_norm(net, fused=True)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\framework\\python\\ops\\arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py\", line 650, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 774, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 329, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 703, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 158, in call\r\n    return super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\", line 511, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\", line 398, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 51, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\", line 54, in smart_cond\r\n    return true_fn()\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\", line 384, in _fused_batch_norm_training\r\n    data_format=self._data_format)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 906, in fused_batch_norm\r\n    name=name)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3810, in _fused_batch_norm\r\n    is_training=is_training, name=name)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\henda boudegga\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,500,500,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\r\n\t [[Node: BatchNorm_19/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](SeparableConv2d_9/BiasAdd, BatchNorm_23/Const, BatchNorm_19/beta/read, BatchNorm_23/Const_1, BatchNorm_23/Const_1)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\n\r\n**this is my following Neural network:** \r\n\r\nimport os,time,cv2\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nimport numpy as np\r\n\r\ndef ConvBlock(inputs, n_filters, kernel_size=[3, 3]):\r\n\t\"\"\"\r\n\tBuilds the conv block for MobileNets\r\n\tApply successivly a 2D convolution, BatchNormalization relu\r\n\t\"\"\"\r\n\t# Skip pointwise by setting num_outputs=Non\r\n\tnet = slim.conv2d(inputs, n_filters, kernel_size=[1, 1], activation_fn=None)\r\n\tnet = slim.batch_norm(net, fused=True)\r\n\tnet = tf.nn.relu(net)\r\n\treturn net\r\n\r\ndef DepthwiseSeparableConvBlock(inputs, n_filters, kernel_size=[3, 3]):\r\n\t\"\"\"\r\n\tBuilds the Depthwise Separable conv block for MobileNets\r\n\tApply successivly a 2D separable convolution, BatchNormalization relu, conv, BatchNormalization, relu\r\n\t\"\"\"\r\n\t# Skip pointwise by setting num_outputs=None\r\n\tnet = slim.separable_convolution2d(inputs, num_outputs=None, depth_multiplier=1, kernel_size=[3, 3], activation_fn=None)\r\n\r\n\tnet = slim.batch_norm(net, fused=True)\r\n\tnet = tf.nn.relu(net)\r\n\tnet = slim.conv2d(net, n_filters, kernel_size=[1, 1], activation_fn=None)\r\n\tnet = slim.batch_norm(net, fused=True)\r\n\tnet = tf.nn.relu(net)\r\n\treturn net\r\n\r\ndef conv_transpose_block(inputs, n_filters, kernel_size=[3, 3]):\r\n\t\"\"\"\r\n\tBasic conv transpose block for Encoder-Decoder upsampling\r\n\tApply successivly Transposed Convolution, BatchNormalization, ReLU nonlinearity\r\n\t\"\"\"\r\n\tnet = slim.conv2d_transpose(inputs, n_filters, kernel_size=[3, 3], stride=[2, 2], activation_fn=None)\r\n\tnet= slim.batch_norm(net)\r\n\tnet = tf.nn.relu(net)\r\n\treturn net\r\n\r\ndef build_mobile_unet(inputs, preset_model, num_classes):\r\n\r\n\thas_skip = False\r\n\tif preset_model == \"MobileUNet\":\r\n\t\thas_skip = False\r\n\telif preset_model == \"MobileUNet-Skip\":\r\n\t\thas_skip = True\r\n\telse:\r\n\t\traise ValueError(\"Unsupported MobileUNet model '%s'. This function only supports MobileUNet and MobileUNet-Skip\" % (preset_model))\r\n\r\n    #####################\r\n\t# Downsampling block  #\r\n\t#####################\r\n\tb00 = ConvBlock(inputs, 32)\r\n\t\r\n\tb01 = DepthwiseSeparableConvBlock(b00, 64)\r\n\t#net = slim.pool(net, [2, 2], stride=[2, 2], pooling_type='MAX')\r\n\t#skip_1 = net\r\n\r\n\tb02 = DepthwiseSeparableConvBlock(b01, 128)\r\n\tb03 = DepthwiseSeparableConvBlock(b02, 128)\r\n\t#net = slim.pool(net, [2, 2], stride=[2, 2], pooling_type='MAX') #2x2 max pooling operation\r\n\t#skip_2 = net\r\n\r\n\tb04 = DepthwiseSeparableConvBlock(b03, 256)\r\n\tb05 = DepthwiseSeparableConvBlock(b04, 256)\r\n\t#net = DepthwiseSeparableConvBlock(net, 256)\r\n\t#net = slim.pool(net, [2, 2], stride=[2, 2], pooling_type='MAX')\r\n\t#skip_3 = net\r\n\r\n\tb06 = DepthwiseSeparableConvBlock(b05, 512)\r\n\tb07 = DepthwiseSeparableConvBlock(b06, 512)\r\n\tb08 = DepthwiseSeparableConvBlock(b07, 512)\r\n\t#net = slim.pool(net, [2, 2], stride=[2, 2], pooling_type='MAX')\r\n\t#skip_4 = net\r\n\r\n\t\r\n\tb09 = DepthwiseSeparableConvBlock(b08, 512)\r\n\tb10 = DepthwiseSeparableConvBlock(b09, 512)\r\n\tb11 = DepthwiseSeparableConvBlock(b10, 512)\r\n\t#net = slim.pool(net, [2, 2], stride=[2, 2], pooling_type='MAX')\r\n\t#skip_5 = net\r\n\r\n\tb12 = DepthwiseSeparableConvBlock(b11, 1024)\r\n\tb13 = DepthwiseSeparableConvBlock(b12, 1024)\r\n\t##################################################\r\n\t##################################################\r\n\t\r\n\tfilters = int(512 * 1.0)\r\n\tnet_01 = conv_transpose_block(b13, filters)\r\n\tup1 = tf.add(net_01, b11)\r\n\tb14 = DepthwiseSeparableConvBlock(up1, filters)\r\n\t\r\n\r\n\t\r\n\tfilters = int(256 * 1.0)\r\n\tnet02 = conv_transpose_block(b14, filters)\r\n\tup2 = tf.add(net02, b05)\r\n\tb15 = DepthwiseSeparableConvBlock(up2, filters)\r\n\t\r\n\r\n\tfilters = int(128 * 1.0)\r\n\tnet03 = conv_transpose_block(b15, filters)\r\n\tup3 = tf.add(net03, b03)\r\n\tb16 = DepthwiseSeparableConvBlock(up3, filters)\r\n\t\r\n\r\n\tfilters = int(64 * 1.0)\r\n\tnet04=conv_transpose_block(b16, filters)\r\n\tup4 = tf.add(net04, b01)\r\n\tb17= DepthwiseSeparableConvBlock(up4, filters)\r\n\t\r\n\r\n\tfilters = int(32 * 1.0)\r\n\tnet05=conv_transpose_block(b17, filters)\r\n\tup5 = tf.add(net05, b00)\r\n\tb18 = ConvBlock(up5, filters)\r\n\t\r\n\r\n\t#net = slim.conv2d(b18, num_classes, [1, 1], activation_fn='linear', scope='logits')\r\n\tnet = slim.conv2d(b18, num_classes, [1, 1], activation_fn=None, scope='logits')\r\n\t#net = BilinearUpSampling2D(net, size=(2, 2))\r\n\tnet = tf.sigmoid(net,name=None)\r\n\t\r\n\r\n\treturn net\r\n\r\n\r\n\r\ncan someone help me, and sorry for the poor english\r\n\r\n"]}, {"number": 17668, "title": "Fix to 'Model' object has no attribute '_container_nodes' error when using tf.keras.utils.plot_model().", "body": "Fix to #17633. Duplicate of #17658\r\n'Model' object has no attribute '_container_nodes' error when using tf.keras.utils.plot_model().\r\nReplaced\r\nif node_key in model._container_nodes:\r\nwith\r\nif node_key in model._network_nodes: # pylint: disable=protected-access\r\n\r\nin tensorflow\\python\\keras_impl\\keras\\utils\\vis_utils.py.", "comments": []}]