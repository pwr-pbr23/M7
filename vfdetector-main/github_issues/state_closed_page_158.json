[{"number": 50072, "title": "Revert \"Stubbed out memory allocation calls in audio library\"", "body": "Reverts tensorflow/tensorflow#47935 which broke the Xtensa build (https://github.com/tensorflow/tflite-micro/issues/123) and possibly made the x86 tests flaky (https://github.com/tensorflow/tflite-micro/issues/120)\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 50071, "title": "XLA persistent compilation cache", "body": "Store results of llvm->ptx and ptx->cubin compilations for\r\nsubsquent executions.", "comments": ["@bas-aarts  Can you please resolve conflicts? Thanks!", "@bas-aarts  Can you please resolve conflicts? Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50071) for more info**.\n\n<!-- need_author_consent -->", "@bas-aarts Can you please sign CLA. Thanks!", "@gbaned this is strange. @bas-aarts already did many PR:\r\nhttps://github.com/tensorflow/tensorflow/pulls?q=is%3Apr+author%3Abas-aarts+is%3Aclosed\r\nSo he must have signed the CLA.\r\n\r\nHe started this PR and I'm finishing it. Could that have confused the bot?\r\n\r\nAnyway, there was a merge conflict, so I just rebased it. Maybe it will fix the bot?", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50071) for more info**.\n\n<!-- need_author_consent -->", "> He started this PR and I'm finishing it. Could that have confused the bot?\r\n\r\nYes, I suspect so.  @mihaimaruseac Do you know what could be happening here?", "@googlebot I consent.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50071) for more info**.\n\n<!-- need_author_consent -->", "> > He started this PR and I'm finishing it. Could that have confused the bot?\r\n> \r\n> Yes, I suspect so. @mihaimaruseac Do you know what could be happening here?\r\n\r\nStatus is (see the \"information_source Googlers: Go here for more info.\" line)\r\n\r\nGitHub Login | Email | State\r\n-- | -- | --\r\nbas-aarts | <email hidden> | ok\r\nnouiz | <email hidden> | need_author_consent\r\n\r\nIf both of you signed the CLA individually then this is probably because there are commits having both of you as co-authors and CLA might not like that. We can override in that case.", "Seems both of you have signed the CLA, so I'll override the check", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50071) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50071) for more info**.\n\n<!-- need_author_consent -->", "I rebased due to conflict. I implemented the code changes you suggested and a few other changes in 6 commits. I still need to add a test.", "@nouiz Any update on this PR? Please. Thanks!", "@nouiz Any update on this PR? Please. Thanks!"]}, {"number": 50070, "title": "[determinism] Add sparse softmax/xent GPU-determinism", "body": "This PR adds and tests deterministic forward and backward operation of `tf.nn.sparse_softmax_cross_entropy_with_logits` when running on a GPU. This PR is a follow-on from PR [49178](https://github.com/tensorflow/tensorflow/pull/49178) (Add non-sparse softmax/xent GPU-determinism).\r\n\r\nNote that there are significant changes and enhancements to the existing tests that may be obscured by the restructuring of the test files.\r\n\r\nThis implementation uses `tf.gather` in a way that does not exercise the nondeterminism that is currently possible through its backprop path on GPU: in each batch slot, it gathers only once from the correct logit, so it backprops (using `tf.math.unsorted_segment_sum`) from only one gradient value (associated with the loss) per batch slot. Therefore, this solution is deterministic. However, due to the d9m-unimplemented exception-throwing that was added to `tf.math.unsorted_segment_sum` via PR [47925](https://github.com/tensorflow/tensorflow/pull/47925) (Add softmax/cross-entropy op exceptions for GPU determinism) use of `tf.gather`'s backprop on a GPU while op-determinism is enabled will cause a `tf.errors.UnimplementedError` to be thrown. For this reason, the test currently disables the exception by setting `TF_DISABLE_SPARSE_SOFTMAX_XENT_WITH_LOGITS_OP_DETERMINISM_EXCEPTIONS` to `'1'` and this current PR should not be merged until a deterministic GPU implementation for `tf.math.unsorted_segment_sum` has been merged into the top-of-tree.\r\n\r\nThe pre-existing, nondeterministic op enforces validity of the `labels` input by raising an exception when running on CPU, or injecting NaNs into the forward and backwards paths associated with the illegal entries in `labels` when running on a GPU. This functionality is [documented](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits). The initial state of this PR emulates the original on-GPU functionality (injecting NaNs) when the new deterministic functionality is running on either a CPU or a GPU. There is also the beginnings of a python-level emulation of the on-CPU functionality (throwing an exception) in case we decide to complete that implementation. Emulating the on-CPU functionality at the python-level seems very hacky (the way I started doing it) and I'm not sure that's it's possible. The emulation of the on-GPU functionality at the python-level is solid but adds a reasonable amount of compute. Let's discuss.\r\n\r\nNote that there is currently no `tf.float64` GPU implementation of this op.\r\n\r\nI was not able to make the determinism test exercise the pre-existing nondeterminism in the forward direction for this op when `logits` is of type `tf.float16` because of the way `logits` is up-sampled to 32-bits in `nn_ops.py`. I'm not sure if it's ever possible to exercise that potential nondeterminism. In other words, the pre-existing GPU implementation may always operate deterministically in the forward direction when `logits` is of type `tf.float16`. This is mostly moot, of course (at least for training), since the op is a loss function and the gradients back to `logits` are still nondeterministic (for the original GPU implementation) when `logits` is of type `tf.float16`.\r\n\r\nThis PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#softmax-xent).\r\n\r\ncc @reedwm @sanjoy @nluehr ", "comments": ["Note that @benbarsdell will soon open a PR to make `tf.math.unsorted_segment_sum` operate deterministically on a GPU. This will remove the d9m-unimplemented exception-throwing for this op when running on a GPU (and when op-determinism is expected). This current PR cannot be merged until that exception-throwing functionality has been removed from the top-of-tree.", "Perhaps this should be implemented within the C++ op instead of on the Python level, which would solve the issue of raising an exception on the CPU. The core logic of the op, which are the following two lines, can be done in the C++ `SparseSoftmaxCrossEntropyWithLogits` op when determinism is enabled:\r\n\r\n```python\r\n    log_probs = log_softmax_v2(logits)\r\n    cost = math_ops.negative(array_ops.gather(log_probs, labels, batch_dims=1))\r\n```\r\n\r\nI think the current partially-implemented approach of checking `math_ops.add(1, 2).device` cannot be made to work in all cases, such as in graph mode when a GraphDef is saved then run on a separate machine. Alternatively, we could just always return NaNs even on the CPU, which is what XLA does despite it violating the documentation, but I would prefer raising exceptions on the CPU.\r\n\r\nImplementing in C++ has a few more advantages. We don't have to deal with the awkwardness of a custom gradient. It's also probably faster as we don't have to run a lot of ops (like `tf.where`, `tf.logical_or`, etc). I also think it's slightly cleaner for the same op to be run regardless of whether determinism is enabled (we already run different ops for the non-sparse version but in that case the logic is a lot simpler).\r\n\r\nThe main disadvantage that I can think of is implementing the op in C++ will take more time. Unfortunately most of the non-test code in this PR would not be used.\r\n\r\n@duncanriach what do you think? To implement, you could call the [GatherFunctor](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/gather_functor.h;l=155;drc=3381da37560d64c7cb62b53879a0a931ff9036c4) and the [SoftmaxFunctor](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/softmax_op_functor.h;l=28;drc=34b98c8f0ef3ee852e90dcebdde935ceae31dfe0) from the SparseSoftmaxCrossEntropyWithLogits op. \r\n\r\nAlso /CC @sanjoy \r\n", "@reedwm, the NaN injection (in both the forward and backward directions) when running on GPU, is currently provided by the Eigen-based implementation (in `SparseXentLossGenerator` and `SparseXentGradGenerator` in `sparse_xent_op.h`). Calling `GatherFunctor` and `SoftmaxFunctor` in `SparseSoftmaxXentWithLogitsOp` (implementing the math as in the Python-level solution) would not properly implement the NaN injection (or even the backprop output from the op). It would, however, solve the problem of throwing the exception when running on CPU. Maybe I'm misunderstanding something about your proposal; let me know if you think I've missed something.\r\n\r\nI think that two reasonable (relatively temporary) compromises would be:\r\n\r\n1. When op-determinism is enabled, inject NaNs when running on CPU as well as GPU (already basically implemented).\r\n2. When op-determinism is enabled, neither inject NaNs nor throw an exception on either CPU or GPU (already basically implemented).\r\n\r\nThe second option would improve performance and is similar to what I did for `tf.nn.bias_add`, the current deterministic implementation of which skips some error checks (see [here](https://github.com/tensorflow/tensorflow/blob/75398449f3b41e5a986d5e79f033afffb1071c2c/tensorflow/python/kernel_tests/bias_op_deterministic_test.py#L138-L147)).\r\n\r\nI suspect that for these ops for which we provide interim, deterministic Python-level solutions, it may be possible to later implement higher-performance, CUDA-level solutions that also include complete, high-performance error checks.", "Some responses to your overall review comment, @reedwm:\r\n\r\n> We could get around this by adding a parameter to GatherFunctor to inject NaN instead of zeros.\r\n\r\nTrue. However, this does not address the backprop path that would need to be implemented under `SparseSoftmaxXentWithLogitsOp` as well, including the injection of NaN into the parts of the gradient associated with illegal label values.\r\n\r\n> I prefer (2) since the implementation is simpler, there is precedent in `tf.nn.bias_add`, and we can eventually solve this by moving the determinism codepath of this op to C++.\r\n\r\nRight. And the details of how that's implemented at the C++ level can be worked out later. I plan to include one or more TODOs.\r\n\r\n> On the CPU, an exception will still be raised with (2) because `tf.gather` raises an exception on the CPU.\r\n\r\nThis doesn't match with my memory of what I saw in the testing (no exception on CPU), and I cannot find this exception-raising functionality in the code. I do see that the documentation claims that there will be an exception raised on the CPU. I'll review this while revising the tests.\r\n\r\n> On the GPU, zeros will be returned instead of NaN if the index is out of bounds due to `tf.gather` returning zeros.\r\n\r\nYes, and I seem to remember that zeros are also returned by the backprop of the deterministic solution (rather than NaNs), which I think makes sense mathematically: when an invalid logit is the \"correct\" one, then changes in the actual logits will have no effect on the loss.", "The following code will be removed from this pull request (from `python/ops/nn_ops.py`) by the next commit. It enables the python-level, GPU-deterministic implementation of `tf.nn.spase_softmax_cross_entropy_with_logits` to inject NaNs into the batch entries for which the `labels` value is invalid (in both the forward and backward direction). I'm placing this code here for potential future reference.\r\n\r\n```Python\r\n@custom_gradient.custom_gradient\r\ndef _sparse_softmax_cross_entropy_gradient_nan_injector(logits, labels):\r\n  logits_shape = array_ops.shape(logits)\r\n  class_count = math_ops.cast(logits_shape[-1], labels.dtype)\r\n  nan_tensor = constant_op.constant(float('Nan'), dtype=logits.dtype)\r\n  def grad(upstream_gradient):\r\n    logits_all_nans = array_ops.broadcast_to(nan_tensor, logits_shape)\r\n    labels_on_logits = array_ops.transpose(\r\n        labels * array_ops.transpose(\r\n            [array_ops.ones(logits_shape[1], dtype=labels.dtype)]))\r\n    logits_grad = array_ops.where(\r\n        math_ops.logical_or(\r\n            math_ops.less(labels_on_logits, 0),\r\n            math_ops.greater_equal(labels_on_logits, class_count)),\r\n        logits_all_nans, upstream_gradient)\r\n    labels_grad = None\r\n    return [logits_grad, labels_grad]\r\n  return logits, grad\r\n\r\n\r\ndef _sparse_softmax_cross_entropy_with_rank_2_logits(logits, labels, name):\r\n  if _tf_deterministic_ops():\r\n    # Handle invalid labels before running the op\r\n    _sparse_softmax_cross_entropy_check_for_valid_labels_on_cpu(logits, labels)\r\n    logits = _sparse_softmax_cross_entropy_gradient_nan_injector(logits, labels)\r\n\r\n    # The actual op functionality\r\n    log_probs = log_softmax_v2(logits)\r\n    cost = math_ops.negative(array_ops.gather(log_probs, labels, batch_dims=1))\r\n\r\n    # Force the output to be NaN when the corresponding label is invalid\r\n    nan_tensor = constant_op.constant(float('Nan'), dtype=logits.dtype)\r\n    cost_all_nans = array_ops.broadcast_to(nan_tensor, array_ops.shape(cost))\r\n    class_count = math_ops.cast(array_ops.shape(logits)[-1], labels.dtype)\r\n    cost = array_ops.where(\r\n        math_ops.logical_or(\r\n          math_ops.less(labels, 0),\r\n          math_ops.greater_equal(labels, class_count)),\r\n        cost_all_nans, cost)\r\n  else:\r\n    # The second output tensor contains the gradients. We use it in\r\n    # _CrossEntropyGrad() in nn_grad but not here.\r\n    cost, _ = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\r\n        logits, labels, name=name)\r\n  return cost\r\n```", "Here is the difference in the gradient generated by `testInvalidLabelGPU` when the selective gradient gating after `tf.gather` is removed:\r\n\r\nExpected gradient:\r\n```\r\n[[ 0.    ,  0.    ,  0.    ,  0.    ],\r\n [ 0.25  ,  0.25  ,  0.25  , -0.75  ],\r\n [-0.968 ,  0.087 ,  0.237 ,  0.6439],\r\n [ 0.    ,  0.    ,  0.    ,  0.    ]])\r\n```\r\n\r\nActual gradient:\r\n```\r\n[[ 0.      ,  0.      ,  0.      ,  0.      ],\r\n [-0.5     ,  0.5     ,  0.5     , -0.5     ],\r\n [-0.935883,  0.174289,  0.473766,  0.287828],\r\n [ 0.      ,  0.      ,  0.      ,  0.      ]]\r\n```", "I'll mark this PR as approved once the unsorted_segment_sum is deterministic. The PR looks good to me and I have no further suggested changes. I don't want to approve now since someone may try to merge the PR and get confused by the \"DO NOT SUBMIT\".\r\n\r\nThe difference in gradients when the gating is disabled makes me suspect the `tf.gather` gradient or some other op might have a bug, as the NaN injection shouldn't change the gradients (unless I did the math wrong). Are you planning at looking at this? If not, I'll take a look myself.", "> I'll mark this PR as approved once the unsorted_segment_sum is deterministic.\r\n\r\nThanks. Sounds good.\r\n\r\n> The difference in gradients when the gating is disabled makes me suspect the `tf.gather` gradient or some other op might have a bug, as the NaN injection shouldn't change the gradients (unless I did the math wrong).\r\n\r\nI suspect that this is revealing a bug in `tf.math.unsorted_segment_sum` (used with the backprop of `tf.gather`): the \"illegal\" segment ID is probably not getting stopped before it gets to `tf.math.unsorted_segment_sum` and then address aliasing is leading to gradients from the batch indices associated with the illegal labels spilling over, and getting added into, the batch indices associated with legal labels.\r\n\r\nHowever, when the gradients associated with legal labels are blocked by injecting either zero or NaN in the forward direction (as this PR currently does), the associated gradient becomes zero and the gradients associated with legal labels come through correctly.\r\n\r\nSo, this is probably a bug associated with `tf.gather` (on GPU) such that while illegal values in its `indices` parameter lead to zeros being output in the forward direction, the associated back-propped gradients (channeled via `tf.math.unsorted_segment_sum`) are not being dropped appropriately.\r\n\r\n> Are you planning at looking at this? If not, I'll take a look myself.\r\n\r\nI intend to look more deeply into this at some point (I added a TODO), but not right now. I planned to, at least, wait until after the new GPU implementation of `tf.math.unsorted_segment_sum` was in the top-of-tree, just in case that happened to resolve it. However, you're welcome to investigate sooner.\r\n\r\n", "@duncanriach Can you please resolve conflicts? Thanks!", "@gbaned, force-pushed after rebase to resolve conflicts.", "@duncanriach Can you please resolve conflicts? Thanks!", "@gbaned: rebased, conflicts resolved, and force-pushed.", "I'm going to rebase to get the other changes in my local tree and run a test before force pushing, which will probably happen tomorrow.", "@reedwm, I rebased and force-pushed, but you only need to review the most recent commit.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR.", "Seems like this test is failing on Windows (we don't have log because the output is too big).", "I think this is because `unsorted_segment_sum` is not deterministic on Windows and so will raise an error when determinism is enabled. I'll disable the test on Windows for now."]}, {"number": 50068, "title": "Change profiler dir to non-temporary in Keras TensorBoard callback", "body": "This is to fix the issue https://github.com/tensorflow/tensorflow/issues/49852.\r\nWithout the fix, the profiler dir will always be deleted for non-chief node under MWMS mode when Keras TensorBoard callback(https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) is used.\r\n", "comments": ["CC @rchao \r\n", "@burgerkingeater thanks for the PR! Is there some graph/text showing the profiling results from both chief and non-chief that you can share?", "chief:\r\n![image](https://user-images.githubusercontent.com/1428327/120973407-be92b300-c723-11eb-9d7c-b74039bf2858.png)\r\nnon-chief\r\n![image](https://user-images.githubusercontent.com/1428327/120973625-074a6c00-c724-11eb-9152-9035b18ac109.png)\r\n", "@rchao  ^\r\n\r\nplease let me know if you have any feedback on this PR. thanks.", "@rchao any more feedback?", "@rchao anything i can help to get this merged?", "@rchao kindly ping...", "> @rchao kindly ping...\r\n\r\nLooks like there are test failures that are preventing this PR from getting merged. Can you check into them?", "> > @rchao kindly ping...\r\n> \r\n> Looks like there are test failures that are preventing this PR from getting merged. Can you check into them?\r\n\r\nActually, the failing tests seem to be internal.", "@rchao the failing tests doesn't seem to be related to this change:\r\n\r\nE.g one from https://source.cloud.google.com/results/invocations/d71b4c26-b055-42de-9f5f-9a42da6a8d75/targets/%2F%2Ftensorflow%2Flite%2Fkernels:test_delegate_providers_lib_test/log :\r\n```\r\nExecution platform: @local_execution_config_platform//:platform\r\nUndefined symbols for architecture x86_64:\r\n  \"CreateNnApiFromSupportLibrary(NnApiSLDriverImplFL5 const*)\", referenced from:\r\n      tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(NnApiSLDriverImplFL5 const*, tflite::StatefulNnApiDelegate::Options) in libnnapi_delegate.a(nnapi_delegate.o)\r\n      tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(NnApiSLDriverImplFL5 const*, tflite::StatefulNnApiDelegate::Options) in libnnapi_delegate.a(nnapi_delegate.o)\r\nld: symbol(s) not found for architecture x86_64\r\n```", "@gbaned can you help see what we should do to unblock the merge of this PR?", "@gbaned kindly ping...", "> @gbaned kindly ping...\r\n\r\n@burgerkingeater We are working on this PR internally. Thanks!", "@gbaned any update? let me know if there's anything I can help.", "> @gbaned any update? let me know if there's anything I can help.\r\n\r\n@burgerkingeater  It is processing internally, we will let you know if any help required from you. Thank you! ", "@gbaned any update on this?\r\n", "This has been merged and can be in keras-team/keras repo. Marking this as closed. \r\n\r\nhttps://github.com/keras-team/keras/commit/cb7cf4c255593fe773c9387787c9d232ceb88739"]}, {"number": 50067, "title": "TF_StringEncode & TF_StringEncodedSize not found", "body": "![Screenshot_2](https://user-images.githubusercontent.com/68463485/120733533-97649780-c511-11eb-95e9-759a51232b57.png)\r\n\r\ni need a source of TF_StringEncode & TF_StringEncodedSize.", "comments": ["@budibeta ,\r\n\r\nPlease, fill issue template.Can you please provide more information/details to analyse the issue[tf version, steps followed before you ran into this error].\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50066, "title": "Fix Cherrypick breakage", "body": "", "comments": []}, {"number": 50065, "title": "Fix json after cherrypicks", "body": "", "comments": []}, {"number": 50064, "title": "Fix Cherrypick issue", "body": "", "comments": []}, {"number": 50063, "title": "FIx Cherrypick issue", "body": "", "comments": []}, {"number": 50062, "title": "Fix Cherrypick issue", "body": "", "comments": []}, {"number": 50061, "title": "Fix CherryPick issue", "body": "", "comments": []}, {"number": 50060, "title": "Fix cherrypick breakage", "body": "", "comments": []}, {"number": 50059, "title": "Revert \"Use cudnn for grouped backward input convolution.\"", "body": "This reverts commit f76e9c291518babf9b69b75b969dfb296a3a9988.\r\n\r\nWith the current cudnn, using native backward grouped convolution\r\nis better for performance", "comments": []}, {"number": 50058, "title": "Fix cherrypick breakage", "body": "", "comments": []}, {"number": 50057, "title": "Fix broken code after cherrypick", "body": "", "comments": []}, {"number": 50056, "title": "[tf.data] graduate unique API from experimental to tf.data.Dataset", "body": "This PR graduates the `tf.data.experimental.unique` API into `tf.data.Dataset.unique` by making the following changes:\r\n\r\n- [x] Adds the deprecation decorator for the experimental API.\r\n- [x] Add the `unique()` method to `DatasetV2` class.\r\n- [x] Update example in docstring with new API.\r\n- [x] Regenerate golden API's.\r\n- [x] Moved and updated the `unique_test` target from `experimental/kernel_tests` to `kernel_tests`\r\n- [x] Updated the RELEASE.md file\r\n\r\nTEST LOG\r\n```\r\nINFO: Elapsed time: 28.062s, Critical Path: 23.53s\r\nINFO: 354 processes: 133 internal, 221 local.\r\nINFO: Build completed successfully, 354 total actions\r\n//tensorflow/python/data/kernel_tests:unique_test                        PASSED in 3.9s\r\n\r\nINFO: Build completed successfully, 354 total actions\r\n```\r\n\r\ncc: @jsimsa @aaudiber ", "comments": []}, {"number": 50055, "title": "Fix broekn branch after cherrypick", "body": "", "comments": []}, {"number": 50054, "title": "Adding EfficientNetV2 to tf.keras.applications ", "body": "I've noticed that the source code of [EfficientNetV2](https://github.com/google/automl/tree/master/efficientnetv2) has been released in `tf. keras` already. The subclassed API is used to build the model by customizing the `.fit` method. It seems interesting and unlike previous `keras` models until now where functional API is used.\r\n\r\nSo, I was just wondering will it be part of the `tf.keras.applications` any near soon? or it won't. ", "comments": ["Thanks for reaching out. We would welcome `EfficientNetV2` as a new Keras Application. Would you be interested in making a PR for it yourself (or finding other contributors who could do it)?", "Officially the `efficientnet v2` code is already in `tf. keras`. In that case, I would cordially request @mingxingtan san to contribute here. I can assist if it's needed. ", "Hi @innat ,  yes, the automl/efficientnetv2 is already using keras and can be composed with other layers such as:\r\n\r\n```\r\nmode = tf.keras.models.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=[224, 224, 3]),\r\n    effnetv2_model.get_model('efficientnetv2-b0', include_top=False, pretrained=True),\r\n    tf.keras.layers.Dropout(rate=0.2),\r\n    tf.keras.layers.Dense(4, activation='softmax'),\r\n])\r\n```\r\n\r\nI would be really great to integrate them into keras applications, and benefit more users. \r\n\r\n@innat Could you prepare a PR, and I am happy to help review/iterate it, with the help from @fchollet , Thanks!"]}, {"number": 50053, "title": "Fix broken cherrypick", "body": "", "comments": []}, {"number": 50052, "title": "Update version numbers for TensorFlow 2.3.3", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 3 -> 3\nPatch: 2 -> 3\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.3.2\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/workspace.bzl:521:2.3.2\ntensorflow/workspace.bzl:524:2.3.2\ntensorflow/workspace.bzl:525:2.3.2\ntensorflow/tools/pip_package/setup.py.orig:62:2.3.2\ntensorflow/tools/pip_package/setup.py:64:2.3.2\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.3.2\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/workspace.bzl:521:2.3.2\ntensorflow/workspace.bzl:524:2.3.2\ntensorflow/workspace.bzl:525:2.3.2\ntensorflow/tools/pip_package/setup.py.orig:62:2.3.2\ntensorflow/tools/pip_package/setup.py:64:2.3.2\n```", "comments": []}, {"number": 50051, "title": "Update version numbers for TensorFlow 2.2.3", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 2\nPatch: 2 -> 3\n\nNo lingering old version strings \"2.2.2\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.2.2\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 50050, "title": "Update version numbers for TensorFlow 2.1.4", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 1 -> 1\nPatch: 3 -> 4\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.3\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/toco/logging/testdata/generated.html:178:2.1.3\ntensorflow/lite/toco/logging/template.html:178:2.1.3\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.3\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/toco/logging/testdata/generated.html:178:2.1.3\ntensorflow/lite/toco/logging/template.html:178:2.1.3\n```", "comments": []}, {"number": 50049, "title": "Update version numbers for TensorFlow 2.4.2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 1 -> 2\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.4.1\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/g3doc/models/pose_estimation/overview.md:150:2.4.1\ntensorflow/lite/g3doc/models/segmentation/overview.md:78:2.4.1\ntensorflow/lite/g3doc/models/image_classification/overview.md:244:2.4.1\ntensorflow/lite/g3doc/models/style_transfer/overview.ipynb:439:2.4.1\ntensorflow/lite/g3doc/models/style_transfer/overview.ipynb:444:2.4.1\ntensorflow/lite/g3doc/models/object_detection/overview.md:312:2.4.1\ntensorflow/lite/g3doc/models/bert_qa/overview.md:84:2.4.1\ntensorflow/lite/g3doc/models/text_classification/overview.md:85:2.4.1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.4.1\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/lite/g3doc/models/pose_estimation/overview.md:150:2.4.1\ntensorflow/lite/g3doc/models/segmentation/overview.md:78:2.4.1\ntensorflow/lite/g3doc/models/image_classification/overview.md:244:2.4.1\ntensorflow/lite/g3doc/models/style_transfer/overview.ipynb:439:2.4.1\ntensorflow/lite/g3doc/models/style_transfer/overview.ipynb:444:2.4.1\ntensorflow/lite/g3doc/models/object_detection/overview.md:312:2.4.1\ntensorflow/lite/g3doc/models/bert_qa/overview.md:84:2.4.1\ntensorflow/lite/g3doc/models/text_classification/overview.md:85:2.4.1\n```", "comments": []}, {"number": 50048, "title": "Incorrect batch size info with keras.utils.Sequence", "body": "I had reported this issue before, but I now find the source is in the `train_generator_v1.py`. \r\n\r\nTensorFlow 2.4/2.5\r\n\r\nBatch size info is miscalculated when data is wrapped in Sequence. The example below is the output for the first epoch training on 1,000 points with batch_size=32, which should result in 32 mini-batch updates. The batch number on the far left is correct (32/32), however, the size and batch before loss seem to be incorrect.\r\n\r\nOutput:\r\nEpoch 1/100\r\nTotal samples: 1000 \r\nBatch size: 32 \r\nTotal batches: 32 \r\n\r\nEpoch 1/10\r\n32/32 [==============================] - 1s 2ms/step - **batch: 15.5000 - size: 31.2500** - loss: 0.1369\r\nEpoch 2/10\r\n32/32 [==============================] - 0s 1ms/step - **batch: 15.5000 - size: 31.2500** - loss: 0.0307\r\nEpoch 3/10\r\n32/32 [==============================] - 0s 1ms/step - **batch: 15.5000 - size: 31.2500** - loss: 0.0148\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np \r\nprint(tf.__version__)\r\nfrom tensorflow import keras as k\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nclass Data(k.utils.Sequence):\r\n    \"\"\"\r\n    Converts fit() into fit_generator() interface.\r\n    \"\"\"\r\n\r\n    def __init__(self, inputs, outputs, sample_weights, batch_size, shuffle):\r\n        self._inputs = inputs\r\n        self._outputs = outputs\r\n        self._sample_weights = sample_weights\r\n        self._size = inputs[0].shape[0]\r\n        self._batch_size = batch_size\r\n        self._num_batches = int((self._size-1)/batch_size) + 1\r\n        self._shuffle = shuffle\r\n        self._ids = np.arange(0, self._size)\r\n        self._reshuffle()\r\n        print(\"\\nTotal samples: {} \".format(self._size))\r\n        print(\"Batch size: {} \".format(min(self._batch_size, self._size)))\r\n        print(\"Total batches: {} \\n\".format(self._num_batches))\r\n\r\n    def __len__(self):\r\n        return self._num_batches\r\n\r\n    def __getitem__(self, index):\r\n        start = index * self._batch_size\r\n        end = min(start + self._batch_size, self._size)\r\n        ids = self._ids[start: end]\r\n        inputs = [v[ids, :] for v in self._inputs]\r\n        outputs = [v[ids, :] for v in self._outputs]\r\n        sample_weights = [v[ids] for v in self._sample_weights]\r\n        return inputs, outputs, sample_weights\r\n    \r\n    def on_epoch_end(self):\r\n        self._reshuffle()\r\n\r\n    def get_data(self):\r\n        return self._inputs, self._outputs, self._sample_weights\r\n\r\n    def _reshuffle(self):\r\n        if self._num_batches > 1 and self._shuffle:\r\n            self._ids = np.random.choice(self._size, self._size, replace=False)\r\n\r\n\r\nx = k.Input((1,))\r\nl1 = k.layers.Dense(10, activation='tanh')(x)\r\ny = k.layers.Dense(1)(l1)\r\n\r\nmodel = k.Model(x, y)\r\nmodel.compile(loss=k.losses.MSE)\r\n\r\ninputs = [np.linspace(0, 1, 1000).reshape(-1,1)]\r\noutputs = list(map(lambda x: np.sin(2*x), inputs))\r\nweights = list(map(lambda x: np.ones(x.size), inputs))\r\n\r\ndg = Data(inputs, outputs, weights, 32, True)\r\n\r\nmodel.fit(dg, epochs=10)\r\n````", "comments": ["@Saduf2019 \r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/3d303f6652cfe77aebb37baf40201858/untitled78.ipynb).Thanks", "@sciann \r\n\r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50048\">No</a>\n"]}, {"number": 50047, "title": "Update release notes for TensorFlow 2.3.3", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.3.3\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 50046, "title": "Update release notes for TensorFlow 2.4.2", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.4.2\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 50045, "title": "Python: Tensorflow 2.5 requires grpcio 1.34", "body": "Tensorflow 2.5.0 requires grpcio 1.34, which prevents installation with some other packages that require 1.37 and later.\r\n\r\nRelated: #48109, https://github.com/tensorflow/tensorflow/commit/f1a51f07937e28e3194b3f8964a06f8390e798c3#diff-f526feeafa1000c4773410bdc5417c4022cb2c7b686ae658b629beb541ae9112\r\n\r\nIt seems like the above commit should've landed for 2.5.0, but it appears to not be that way in pypi.\r\n\r\nLog output from Poetry:\r\n\r\n```\r\n(snip)\r\nAnd because tensorflow (2.5.0) depends on grpcio (>=1.34.0,<1.35.0),\r\n(snip)\r\n```", "comments": ["On master I see https://github.com/tensorflow/tensorflow/blob/53931158504a88468ff35bfd492b529876a04697/tensorflow/tools/ci_build/release/common_win.bat#L56", "This is the only relevant direct dependency on `grpcio` on TF: https://github.com/tensorflow/tensorflow/blob/1595ee17ff02b7ee8ede2db19096d4c36bd14bf4/tensorflow/tools/pip_package/setup.py#L130\r\n\r\nCan you post the output of `pip list` and details about the architecture you are on?", "Oh, sorry, you meant the 2.5 release. There we indeed pin to 1.34. https://github.com/tensorflow/tensorflow/blob/15d5b930d7e6e4463e275cfcff22042105148e3f/tensorflow/tools/pip_package/setup.py#L126-L130\r\n\r\nUnfortunately, we won't be able to alter this without doing a patch release. We currently need to do patch releases for the vulns mentions in the 2.5 release notes and we're also very close to the 2.6 release. So let's wait until 2.6 and there this issue would be solved.\r\n\r\nIn the future, we will add automation to ensure dependencies are updated before release (CC @pak-laura FYI)", "That sounds good. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50045\">No</a>\n", "Why was this closed? I don't see a 2.6.0 release.", "TF 2.6 has now released https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0\r\nPerhaps we can close this thread now.", "Working for me, thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50045\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50045\">No</a>\n"]}, {"number": 50044, "title": "Update release notes for TensorFlow 2.2.3", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.2.3\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 50043, "title": "Update release notes for TensorFlow 2.1.4", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.1.4\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 50042, "title": "Tensorflow 2.5 wheels depend on an unreleased keras version and restrict numpy to 1.19", "body": "The published wheels for tensorflow 2.5 depend on a nightly dev version of keras and restrict numpy to 1.19.x.\r\n\r\nDepending on a nightly build of some package in a stable version is just, in lack of a better word, crazy. \r\n\r\nThe dependency on a specific numpy major version is maybe necessary, but is it really impossible to have a wheel that supports numpy >1.19 (including the current 1.20 release)?\r\n\r\n\r\n```\r\n\u276f unzip -p tensorflow-2.5.0-cp37-cp37m-macosx_10_11_x86_64.whl  tensorflow-2.5.0.dist-info/METADATA | grep Requires-Dist\r\nRequires-Dist: numpy (~=1.19.2)\r\nRequires-Dist: absl-py (~=0.10)\r\nRequires-Dist: astunparse (~=1.6.3)\r\nRequires-Dist: flatbuffers (~=1.12.0)\r\nRequires-Dist: google-pasta (~=0.2)\r\nRequires-Dist: h5py (~=3.1.0)\r\nRequires-Dist: keras-preprocessing (~=1.1.2)\r\nRequires-Dist: opt-einsum (~=3.3.0)\r\nRequires-Dist: protobuf (>=3.9.2)\r\nRequires-Dist: six (~=1.15.0)\r\nRequires-Dist: termcolor (~=1.1.0)\r\nRequires-Dist: typing-extensions (~=3.7.4)\r\nRequires-Dist: wheel (~=0.35)\r\nRequires-Dist: wrapt (~=1.12.1)\r\nRequires-Dist: gast (==0.4.0)\r\nRequires-Dist: tensorboard (~=2.5)\r\nRequires-Dist: tensorflow-estimator (<2.6.0,>=2.5.0rc0)\r\nRequires-Dist: keras-nightly (~=2.5.0.dev)\r\nRequires-Dist: grpcio (~=1.34.0)\r\n```\r\n\r\nMost version requirements here seem rather restrictive.", "comments": ["Where is the source code for keras-nightly? I'm trying to see where it is actually built? It seems necessary to be able to package TF 2.5.0 into Nixpkgs.", "Regarding numpy 1.20: See #47691, #48918, #48935. Please help ~~in advocating to get it unpinned.~~ to fix the incompatibilities.", "Keras-nightly is an unfortunate oversight, but given the version number it's a pinned version. In the future we will not have this anymore. CC @qlzh727 \r\n\r\n@bnavigator, we have already explained what are the blockers for numpy 1.20. Piling on other issues to ask for support is a little bit against the code of conduct, I'd say.", "@CMCDragonkai keras wheels are built from https://github.com/keras-team/keras\r\n\r\nSee also https://github.com/tensorflow/community/blob/master/rfcs/20200205-standalone-keras-repository.md", "Can it be changed the Keras nightly dependency? It really is stopping my team from using 2.5. Thanks!", "@maxnoe Is this still an issue for you? numpy 1.20 is compatible with latest TF version.\r\n\r\nPlease verify once and close the issue if this was resolved. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50042\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50042\">No</a>\n"]}]