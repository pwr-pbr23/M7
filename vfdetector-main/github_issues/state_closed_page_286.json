[{"number": 45730, "title": "How to specify `force_gpu_compatible` in TensorFlow 2.x?", "body": "In TF 1.x, we can use these codes to force CPU tensor located on pinned CPU memory, whose bindwidth will be higher than pageable CPU memory.\r\n```python\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.force_gpu_compatible = True\r\n...\r\nwith tf.Session(config = config) as sess:\r\n    ...\r\n```\r\n\r\nBut in TF 2.x, what is the mechanism to force CPU tensor located on pinned memory?", "comments": ["@Jianbing-D,\r\nInstead of `config = tf.ConfigProto()`, please try `tf.compat.v1.ConfigProto` and check if it works. \r\n\r\nPlease check [this gist](https://colab.research.google.com/gist/amahendrakar/ca4186d881a51f9d09242dfabd322f4b/45730.ipynb) for reference. Thanks!", "@amahendrakar Thanks for clearifying.\r\nBut what I want to ask is how to force tensor to located on CPU pinned memory with TF 2.x APIs(rather than via `tf.compat.v1` to use TF 1.x APIs in TF 2.x).\r\n\r\nBTW, I want to accelerate the tensor transition from CPU->GPU, therefore CPU pinned memory is what I want for higher performance. \r\nFor example, this [issue](https://github.com/tensorflow/tensorflow/issues/44836) is related to bad performance on CPU pageable memory.", "@Jianbing-D,\r\nYou can try using **`tf.config.experimental.set_visible_devices(gpus[0], 'GPU')`**. Please refer [documentation of GPU](https://www.tensorflow.org/guide/gpu) for more information. Thanks!", "> @Jianbing-D,\r\n> You can try using **`tf.config.experimental.set_visible_devices(gpus[0], 'GPU')`**. Please refer [documentation of GPU](https://www.tensorflow.org/guide/gpu) for more information. Thanks!\r\n\r\nThanks. But I think your reply is irrelevant to my issue. That method only set the `visible_devices`, while tensors still can be allocated on CPU pageable memory rather than CPU pinned memory, which is what I really wanted. \r\n\r\nYou can try the following code snippet to verify it:\r\n```python\r\nimport tensorflow as tf\r\n\r\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\ntf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n\r\na = tf.constant(0)\r\nprint(a.device)\r\n``` ", "To clarify: are you looking to use the tensors on gpu, or do you specifically want to place tensors on CPU (w/ cpu compute) while pinning them in memory so they never page out to disk?\r\n\r\n---------------\r\nIf you're just looking to use gpus without dealing with manual device control, you can try distribution strategies:\r\nhttps://www.tensorflow.org/guide/distributed_training\r\n(There's a one device strategy that works even if you have just one GPU)\r\n\r\nOr if you need to you can manually control the devices your tensors are placed on as described in the previously-linked gpu guide:\r\nhttps://www.tensorflow.org/guide/gpu\r\n\r\n---------------\r\nIf you're looking to place your tensors and computation on the CPU *and* pin them in memory so nothing pages out to disk even when your virtual memory is greater than your allocated program memory... off the top of my head I'm not sure if TF has ever had support for that. I would have to ask around.\r\n", "Thanks @tomerk , \r\n> If you're looking to place your tensors and computation on the CPU and pin them in memory so nothing pages out to disk even when your virtual memory is greater than your allocated program memory... off the top of my head I'm not sure if TF has ever had support for that. I would have to ask around.\r\n\r\nThis is what I am looking.\r\n", "@agarwal-ashish suggested a fix in our internal tracker: `tf.compat.v1.enable_eager_execution` currently allows passing in a `ConfigProto`, which could be a workaround for now.", "> @agarwal-ashish suggested a fix in our internal tracker: `tf.compat.v1.enable_eager_execution` currently allows passing in a `ConfigProto`, which could be a workaround for now.\r\n\r\nThanks, I will try.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45729, "title": "Fix sanity build", "body": "", "comments": []}, {"number": 45728, "title": "Fix sanity build", "body": "", "comments": []}, {"number": 45727, "title": "Issues about the TRTEngineOp", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.0/8.0.4\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI tried to convert tensorflow FP32 model to FP16 using tensorRT 7.2.1, when I tried to save the converted model, it complained:\r\n\r\n**....: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at trt_engine_resource_ops.cc:195 : Not found: Resource TF-TRT/TRTEngineOp_1_0/N10tensorflow8tensorrt22TRTEngineCacheResourceE does not exist.**\r\n\r\n**INFO:tensorflow:Could not find TRTEngineOp_1_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.**\r\n\r\nThe first message seems like a known issue which can be ignored, but the second message seems not OK.\r\n\r\nAfter that when I tried to infer using the converted model tensorflow complained:\r\n\r\n**....: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:587] Running native segment forPartitionedCall/TRTEngineOp_0_0 due to failure in verifying input shapes: Input shapes do not match input partial shapes stored in graph, for PartitionedCall/TRTEngineOp_0_0: [[28,28]] != [[?,28,28]]**\r\n\r\nDid I miss some configurations for the TRTEngine?\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nparams = tf.experimental.tensorrt.ConversionParams(\r\n    precision_mode='FP16')\r\nconverter = tf.experimental.tensorrt.Converter(\r\n    input_saved_model_dir=\"my_dir\", conversion_params=params)\r\nconverter.convert()\r\nconverter.save(output_saved_model_dir)\r\n```\r\nThe above codes are from https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter\r\n\r\n```\r\nprint(\"The signature keys are: \",list(loaded.signatures.keys())) \r\ninfer = loaded.signatures[\"serving_default\"]\r\n\r\nim_select = 0 # choose train-image you want to classify\r\nlabeling = infer(tf.constant(train_images[im_select],dtype=float))['LastLayer']   ## Here, the Image classification happens; we need the name of the last layer we defined in the beginning\r\n```\r\nThe above are from https://colab.research.google.com/gist/monkeyman85/e04e240cde74be48750d8160e078d5d7/convert_simplemodel_to_tensorrt_rev1.ipynb#scrollTo=oixTGKOSutMN\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to run the code without any issues on [TF v2.3](https://colab.research.google.com/gist/amahendrakar/64a21ab36a0bf583844cc5c44af589de/45727.ipynb#scrollTo=oixTGKOSutMN).\r\n\r\nHowever, Colab notebook crashes on running [TF v2.4](https://colab.research.google.com/gist/amahendrakar/de4332ce5b49f4745acd2376eea1897a/45727-2-4.ipynb) and TF-nightly. Please check the linked gist for reference. Thanks!", "@yummychop Did you test with TF 2.4 in your local with cuda 11 installed? Colab is still hosting cuda 10.1 and TF 2.3 therefore testing with TF 2.4 on Colab may not be the right indicator.\r\n", "> @yummychop Did you test with TF 2.4 in your local with cuda 11 installed? Colab is still hosting cuda 10.1 and TF 2.3 therefore testing with TF 2.4 on Colab may not be the right indicator.\r\n\r\nI didn't run the Colab directly, I copied the codes from Colab to the local terminal to run them. Anyways I downgraded my TF to 2.3 TR to 6, CUDA to 10.1 and CUDNN to 7. Are there some simple codes to check if the TF-TRT engine works properly? Thank you very much ", "You may try https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#usingtftrt", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45727\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45727\">No</a>\n"]}, {"number": 45726, "title": "Disable a few tests.", "body": "These tests now segfault after some dependency updated below us.", "comments": []}, {"number": 45725, "title": "Disable a few tests.", "body": "These tests now segfault after some dependency updated below us.", "comments": []}, {"number": 45724, "title": "Fix sanity build, reorder tags to satisfy buildifier.", "body": "", "comments": []}, {"number": 45723, "title": "Disable a few tests.", "body": "These tests now segfault after some dependency updated below us.", "comments": []}, {"number": 45722, "title": "Disable a few tests.", "body": "These tests now segfault after some dependency updated below us.", "comments": []}, {"number": 45721, "title": "Disable a few tests.", "body": "These tests now segfault after some dependency updated below us.", "comments": []}, {"number": 45720, "title": "Some initial documentation for the TfLite Micro CI system.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "copybara import error occurred. retrying with \"kokoro:force-run\" and \"ready to pull\" labels.", "copybara import error occurred. retrying with \"kokoro:force-run\" and \"ready to pull\" labels."]}, {"number": 45719, "title": "[Cherrypick:r2.2] SavedModel loading: Create temporary graphs for importing functions to avoid leaking memory", "body": "We need to add functions to some tf.Graph at the moment, but it doesn't need to\nbe held in a global variable.\n\nLeaves the memory leak for functions containing TRTEngineOp, since tests are failing otherwise. I've filed a bug to investigate.\n\nPiperOrigin-RevId: 298701457\nChange-Id: I5e2c8a07eeaec10c019bcec419a992cb65adc5f7", "comments": []}, {"number": 45718, "title": "[Cherrypick:r2.3] Don't do fused average updates inside XLA context as it may create extra tf.cond which causes OOM on TPUs.", "body": "PiperOrigin-RevId: 327294174\nChange-Id: I7caa62d77e5c86a6afe7aaca22c7231d8f2304b6", "comments": []}, {"number": 45716, "title": "Model training stalls forever after just a few batches. ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Windows 10.0.19041 Build 19041\r\n- TensorFlow installed via pip\r\n- TensorFlow version: 2.4.0-rc4\r\n- TensorFlow Git version: v2.4.0-rc3-20-g97c3fef64ba \r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: CUDA 11.0, cuDNN 8.0.4\r\n- GPU model and memory: Nvidia RTX 3090, 24GB RAM\r\n\r\n**Describe the current behavior**\r\nModel training regularly freezes for large models. \r\nSometimes the first batch or so works, but then just a few batches later and training seems stuck in a loop. From my activity monitor, I see GPU CUDA use hovering around 100%. This goes on for minutes or more, with no more batches being trained.\r\n\r\nI don't see an OOM error, nor does it seem like I'm hitting memory limits in activity monitor or `nvidia-smi`.\r\n\r\n**Describe the expected behavior**\r\nI would expect the first batch to take a bit longer, then any subsequent batches to take less than <1s. Never have a random batch take minutes (after previous batches took <1s) or stall forever.\r\n\r\n**Standalone code to reproduce the issue**\r\nRun through all the cells in the notebook shared below to initialize the model, then run the final cell just a few times. Eventually it will hang and never finish.\r\n\r\nhttps://github.com/not-Ian/tensorflow-bug-example/blob/main/tensorflow%20error%20example.ipynb\r\n\r\n**Other info / logs**\r\nMy intuition:\r\nSmaller models train quickly as expected, however I think even then they eventually stall out after training many, many batches.\r\nI had another similar, small VAE like in my example that trained for 5k-10k batches overnight before stalling. Could there be a memory leak somewhere?\r\n", "comments": ["@not-Ian,\r\nCould you please update TensorFlow to the stable version v2.4 and check if you are facing the same issue.\r\n\r\nAlso, try setting a hard limit on the total GPU memory as per [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it helps. Thanks!", "Thanks for the reply, but no luck. I ran:\r\n\r\n`> pip uninstall tensorflow-gpu`\r\n\r\nthen \r\n\r\n`> pip install tensorflow-gpu==2.4.0`\r\n\r\nSo now I'm on:\r\n\r\nTensorFlow version: 2.4.0\r\nTensorFlow Git version: v2.4.0-rc4-71-g582c8d236cb\r\n\r\nThen I included this code at the top of my notebook:\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # setting memory limit to 1024MB * 23 = 23GB. I've got 24, but 23 to be safe.\r\n    tf.config.experimental.set_virtual_device_configuration(\r\n        gpus[0],\r\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 23)])\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Virtual devices must be set before GPUs have been initialized\r\n    print(e)\r\n```\r\n\r\nAnd training stalled again (100% GPU usage, the next batch not completing) after 2 batches. ", "Also, for completion's sake, I saw this message get logged:\r\n\r\n`2020-12-16 15:50:23.428273: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295`\r\n\r\nI read once somewhere about a `ptxas.exe` compiler error when this happens. Something about using the CUDA 11.1 `ptxas.exe` file instead of the one that comes with CUDA 11.0, as 11.1 supports `-arch=sm_86` argument in this call:\r\n\r\n`2020-12-16 16:03:52.614874: I tensorflow/stream_executor/gpu/asm_compiler.cc:215] C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/bin/ptxas.exe C:\\Users\\Ian\\AppData\\Local\\Temp\\/tempfile-DESKTOP-IDQ189A-1554-3668-5b69c0cfcfd7f -o C:\\Users\\Ian\\AppData\\Local\\Temp\\/tempfile-DESKTOP-IDQ189A-1554-3668-5b69c0cfd038e -arch=sm_86 -v`\r\n\r\nAnyways I uninstalled, and reinstalled Tensorflow 2.4.0 and used the 11.1 CUDA `ptxas.exe` file instead and it still didn't work. Instead I got `SubProcess ended with return code: 0`, and it still stalled after 2 batches.\r\n\r\nTo be sure, I uninstalled and reinstalled TF 2.4.0 with the original CUDA 11.0 `ptxas.exe` file and this gets spammed:\r\n\r\n```\r\n2020-12-16 16:21:44.971468: I tensorflow/stream_executor/gpu/asm_compiler.cc:157] Looking for ptxas.exe at C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/bin/ptxas.exe\r\n2020-12-16 16:21:44.971614: I tensorflow/stream_executor/gpu/asm_compiler.cc:166] Using ptxas.exe at C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/bin/ptxas.exe\r\n2020-12-16 16:21:44.973041: I tensorflow/stream_executor/gpu/asm_compiler.cc:186] ptx written to: C:\\Users\\Ian\\AppData\\Local\\Temp\\/tempfile-DESKTOP-IDQ189A-cd8-3104-5b69c4ce7e1a7\r\n2020-12-16 16:21:44.973948: I tensorflow/stream_executor/gpu/asm_compiler.cc:215] C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/bin/ptxas.exe C:\\Users\\Ian\\AppData\\Local\\Temp\\/tempfile-DESKTOP-IDQ189A-cd8-3104-5b69c4ce7e1a7 -o C:\\Users\\Ian\\AppData\\Local\\Temp\\/tempfile-DESKTOP-IDQ189A-cd8-3104-5b69c4ce7ea30 -arch=sm_86 -v\r\n2020-12-16 16:21:44.980888: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n```\r\n\r\n\r\n", "Same with me", "@amahendrakar any other ideas?", "https://github.com/tensorflow/tensorflow/issues/45648\r\n\r\nthis fix solves the problem\r\n![102058534-11408180-3e09-11eb-9bea-839dce2e9870 1](https://user-images.githubusercontent.com/8076202/102778886-c7c5d880-43ac-11eb-87a9-35284b68883f.jpg)\r\n\r\nbut that is not a solution of course. I hate tensorflow.", "> #45648\r\n> \r\n> this fix solves the problem\r\n> ![102058534-11408180-3e09-11eb-9bea-839dce2e9870 1](https://user-images.githubusercontent.com/8076202/102778886-c7c5d880-43ac-11eb-87a9-35284b68883f.jpg)\r\n> \r\n> but that is not a solution of course. I hate tensorflow.\r\n\r\nWOW I think this fixed it. Weird fix, but thanks a lot.\r\n\r\nDo you know - did changing this remove 'CUDA' from the Task Manager?\r\n\r\nThanks again", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45716\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45716\">No</a>\n", "don't close the issue. \r\nThis shit should be fixed in tensorflow.", "yeah, you're right", "It does not work for me.", "Solution for Keras, remove activation='relu' from CONv2D\r\n\r\n`conv1 = Conv2D(denseList[0], 3, activation='relu', padding='same')(inputs)`\r\n\r\nto \r\n\r\n`\r\nconv1 = Conv2D(denseList[0], 3, padding='same')(inputs)`\r\n`conv1 = Activation(\"relu\")(conv1) `\r\n", "@psycho2012 lmao, it is not a solution. Solution is to migrate to pytorch for example, because TF become a piece of shit ", "Was able to replicate the issue in TF 2.6.0-dev20210602,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/301523f986df999a53a3e6aa799839ed/untitled185.ipynb#scrollTo=Bd7j7vhJXlkV)..Thanks !", "@not-Ian I am not able to reproduce the issue with `TF2.5`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/5926d407d86e4aa1559f26fa3e430561/untitled185.ipynb) and let me know if I am missing anything. Please note that i ran your code in colab (not on Windows). \r\n\r\nCan you please check with `TF2.5` and let us know whether the issue is persisting or not. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45716\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45716\">No</a>\n"]}, {"number": 45715, "title": "Add Cuda 11 custom-op Dockerfile", "body": "Patched into the 2.4 branch for future rebuilds of custom-op.", "comments": []}, {"number": 45714, "title": "Add Cuda 11.0 custom-op Dockerfile", "body": "FYI @seanpmorgan", "comments": []}, {"number": 45713, "title": "[CherryPick:r2.2] Don't do fused average updates inside XLA context as it may create extra tf.cond which causes OOM on TPUs.", "body": "PiperOrigin-RevId: 327294174\nChange-Id: I7caa62d77e5c86a6afe7aaca22c7231d8f2304b6", "comments": []}, {"number": 45712, "title": "Add upper bound to `h5py`.", "body": "Newer versions of `h5py` would cause errors in keras tests due to\r\ndifference between `unicode` and `str`. Since `h5py` comes from `keras`\r\nas an unbounded dependency, we have to manually patch this way.", "comments": []}, {"number": 45711, "title": "Add upper bound to `h5py`.", "body": "Newer versions of `h5py` would cause errors in keras tests due to\r\ndifference between `unicode` and `str`. Since `h5py` comes from `keras`\r\nas an unbounded dependency, we have to manually patch this way.", "comments": []}, {"number": 45710, "title": "Add upper bound to `h5py`.", "body": "Newer versions of `h5py` would cause errors in keras tests due to\r\ndifference between `unicode` and `str`. Since `h5py` comes from `keras`\r\nas an unbounded dependency, we have to manually patch this way.", "comments": []}, {"number": 45709, "title": "change LSTM to CuDNNLSTM cause core dumped", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS x86_64 GNU/Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow-gpu                     1.15.0, v1.15.0-rc3-22-g590d6ee 1.15.0\r\n- Python version: python3.6\r\n- CUDA/cuDNN version:  Cuda compilation tools, release 10.0, V10.0.130 / libcudnn.so.7\r\n- GPU model and memory: GeForce GTX 1660 Ti with Max-Q Design\r\n\r\n\r\n**Describe the current behavior**\r\nhttps://stackoverflow.com/questions/65311478/change-lstm-to-cudnnlstm-cause-core-dumped\r\n\r\nI tried a model, and changed LSTM (which is running fine) to CuDNNLSTM, then it core dumped.\r\n\r\n```\r\nfrom tensorflow.keras.layers import CuDNNLSTM as LSTM  # no core dump if just use LSTM\r\n\r\n...\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ngaussian_noise (GaussianNois (None, 63, 41)            0         \r\n_________________________________________________________________\r\ncu_dnnlstm_1 (CuDNNLSTM)     (None, 20)                5040      \r\n_________________________________________________________________\r\ndense (Dense)                (None, 1)                 21        \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 1)                 4         \r\n_________________________________________________________________\r\nre_lu (ReLU)                 (None, 1)                 0         \r\n_________________________________________________________________\r\nlambda (Lambda)              (None, 1)                 0         \r\n=================================================================\r\nTotal params: 5,065\r\nTrainable params: 5,063\r\nNon-trainable params: 2\r\n_________________________________________________________________\r\n\r\nEpoch 1/1000\r\n2020-12-15 12:58:38.233733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-12-15 12:58:38.410567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nSegmentation fault (core dumped)\r\n\r\n\r\npackage versions:\r\n\r\nKeras                              2.2.0\r\ntensorflow-gpu                     1.15.0\r\n```\r\n\r\n**Describe the expected behavior**\r\nCuDNNLSTM should running fine just as LSTM.\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nthe stack trace from the core file:\r\n```\r\n$ gdb python3 core \r\nGNU gdb (Ubuntu 9.1-0ubuntu1) 9.1\r\n...\r\nProgram terminated with signal SIGSEGV, Segmentation fault.\r\n#0  0x00007ff4a50edfc0 in tensorflow::Tensor::Tensor(tensorflow::Tensor const&) ()\r\n   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n[Current thread is 1 (Thread 0x7ff42cffd700 (LWP 12422))]\r\n(gdb) where\r\n#0  0x00007ff4a50edfc0 in tensorflow::Tensor::Tensor(tensorflow::Tensor const&) ()\r\n   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007ff4a5f1c4af in tensorflow::CudnnRNNForwardOpV2<Eigen::GpuDevice, double>::MaybeAutoTune(tensorflow::OpKernelContext*, tensorflow::(anonymous namespace)::CudnnRnnModelShapes const&, stream_executor::dnn::RnnInputMode const&, tensorflow::Tensor const*, tensorflow::Tensor const*, tensorflow::Tensor const*, tensorflow::Tensor const*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, stream_executor::dnn::AlgorithmConfig*) () from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007ff4a5f15e1f in tensorflow::CudnnRNNForwardOp<Eigen::GpuDevice, double>::ComputeAndReturnAlgorithm(tensorflow::OpKernelContext*, stream_executor::dnn::AlgorithmConfig*, bool, bool, int) ()\r\n   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007ff4a5f17400 in tensorflow::CudnnRNNForwardOpV2<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*) ()\r\n   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007ff4a1788dc2 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#5  0x00007ff4a17e4b47 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#6  0x00007ff4a17e513f in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> > const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#7  0x00007ff4a1895021 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#8  0x00007ff4a1892718 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/z/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.1\r\n#9  0x00007ff4a1eefb1f in std::execute_native_thread_routine (__p=0x55f81d1f75f0) at /dt7-src/libstdc++-v3/src/nonshared11/../c++11/thread.cc:83\r\n#10 0x00007ff50f75f609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#11 0x00007ff50f686103 in clone () from /usr/lib/x86_64-linux-gnu/libc.so.6\r\n(gdb) q\r\n```\r\n\r\n", "comments": ["@mingwugmail,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.4 and check if you are facing the same issue.\r\n\r\nAlso, in order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yes, CuDNNLSTM in v2.4 is working.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45709\">No</a>\n"]}, {"number": 45708, "title": "Validation Split causing  ResourceExhaustedError: OOM when allocating tensor with shape[54000,128,128,3]", "body": "Everything goes fine when I don't give validation_split while fitting the model.  Why is doing so? Does validation split need more memory although the data is being divided between training and validation right after the passing validation_split argument to model.fit() function. \r\n\r\nWhat could be the reasons of this odd behavior? \r\n\r\n\r\n`\r\n--------------------------------------------------------------------------\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n<ipython-input-18-bfe8abdce7db> in <module>\r\n----> 1 history = model.fit(train_images,train_labels, batch_size=batch_size, epochs=epochs,\r\n      2                     validation_split=0.1, callbacks=[callback], shuffle=True)\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    793       # `Tensor` and `NumPy` input.\r\n    794       (x, y, sample_weight), validation_data = (\r\n--> 795           data_adapter.train_validation_split((x, y, sample_weight),\r\n    796                                               validation_split=validation_split,\r\n    797                                               shuffle=False))\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in train_validation_split(arrays, validation_split, shuffle)\r\n   1335     return array_ops.gather_v2(t, indices)\r\n   1336 \r\n-> 1337   train_arrays = nest.map_structure(\r\n   1338       functools.partial(_split, indices=train_indices), arrays)\r\n   1339   val_arrays = nest.map_structure(\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    615 \r\n    616   return pack_sequence_as(\r\n--> 617       structure[0], [func(*x) for x in entries],\r\n    618       expand_composites=expand_composites)\r\n    619 \r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    615 \r\n    616   return pack_sequence_as(\r\n--> 617       structure[0], [func(*x) for x in entries],\r\n    618       expand_composites=expand_composites)\r\n    619 \r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _split(t, indices)\r\n   1333       return t\r\n   1334     t = ops.convert_to_tensor_v2(t)\r\n-> 1335     return array_ops.gather_v2(t, indices)\r\n   1336 \r\n   1337   train_arrays = nest.map_structure(\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in gather_v2(params, indices, validate_indices, axis, batch_dims, name)\r\n   4533               batch_dims=0,\r\n   4534               name=None):\r\n-> 4535   return gather(\r\n   4536       params,\r\n   4537       indices,\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)\r\n   4522     return params.sparse_read(indices, name=name)\r\n   4523   except AttributeError:\r\n-> 4524     return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n   4525 \r\n   4526 \r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py in gather_v2(params, indices, axis, batch_dims, name)\r\n   3753         pass  # Add nodes to the TensorFlow graph.\r\n   3754     except _core._NotOkStatusException as e:\r\n-> 3755       _ops.raise_from_not_ok_status(e, name)\r\n   3756   # Add nodes to the TensorFlow graph.\r\n   3757   if batch_dims is None:\r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6651   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6652   # pylint: disable=protected-access\r\n-> 6653   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6654   # pylint: enable=protected-access\r\n   6655 \r\n\r\n~/.conda/envs/my_env/lib/python3.8/site-packages/six.py in raise_from(value, from_value)\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[54000,128,128,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:GatherV2]\r\n`", "comments": ["Few days ago I got same issue like this.\r\n\r\nif you use TF2, try this:\r\n1. restart your kernel and clear the output\r\n2. clean your GPU-memory (in linux)\r\n`sudo kill -9 *PID*`\r\nyou can check PID number with `nvidia-smi`\r\n3. Run this code\r\n`from tensorflow.compat.v1 import ConfigProto`\r\n`from tensorflow.compat.v1 import InteractiveSession`\r\n`config = ConfigProto()`\r\n`config.gpu_options.allow_growth = True`\r\n`session = InteractiveSession(config=config)`\r\n\r\nI don't have much experience with this, but it works well for me\r\nnote: check your GPU-memory availability too!", "@shuaibaslam2019,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. \r\n\r\nAnd to resolve the OOM issue, please set a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth). Thanks!\r\n", "> @shuaibaslam2019,\r\n> In order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using.\r\n> \r\n> And to resolve the OOM issue, please set a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth). Thanks!\r\n\r\nI am using Tensorflow version 2.2.0 and keras 2.3.4.  The dataset is Fashion-Mnist. \r\n\r\n`history = model.fit(train_images,train_labels, batch_size=64, epochs=15,\r\n                    validation_split=0.2, shuffle=True)`", "@shuaibaslam2019,\r\nCould you please share the Python script/notebook you are running, so that we can reproduce the issue on our end.\r\n\r\n> And to resolve the OOM issue, please set a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth).\r\n\r\nAlso, please try the above mentioned workaround and let us know if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45708\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45708\">No</a>\n", "For others ending up here: I believe this is fixed with the following commit (so in tf>=2.3)\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/ce2f9824eee904d60fdc0444ac8a82b217ea9149#diff-a92a750392aea2d97a4ec3b844265799a63103e548d178d1f7d73b3b25bfbfd1L1427\r\n\r\nBefore that, numpy arrays where converted to tensors before splitting (via `validation_split`) which might have filled the GPU memory\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fe33f393b86904913898d25c5fd7f263058ed5a6/tensorflow/python/keras/engine/data_adapter.py#L1427"]}, {"number": 45707, "title": "iOS Build TFlite as framework", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: arm64 ios\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest from master - 2.4.0\r\n- Python version: Python 3.7.6\r\n- Installed using virtualenv? pip? conda?:  anaconda3\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source):  12 \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA \r\n\r\n\r\n**Describe the problem**\r\nRunning  build with the following command : \r\n```\r\nbazel build --config=ios_fat -c opt \\\r\n  //tensorflow/lite/ios:TensorFlowLiteC_framework\r\n\r\n```\r\nThis outputs a framework file i would like to add to my pod and distribute later. \r\nI've done it with previous versions but since moving to Xcode 12 i keep getting various issues.\r\n1. Running dev version works ok but in release build results from tensor running return faulty. \r\n2.  Building with \r\n```\r\nbazel build --config=ios_fat --ios_multi_cpus=armv7,arm64,i386,x86_64 -c opt \\\r\n  //tensorflow/lite/ios:TensorFlowLiteC_framework\r\n```\r\nCause build issues where multiple c code is missing for arm64. \r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The --config=ios_fat already configure the build for all armv7,arm64,i386,x86_64.\r\nIf you want to build for some archs, you should use --config=ios without \"fat\": --config=ios --ios_multi_cpus=...", "Can you share the exact errors you're getting? If this is related to CocoaPods lint step, you might be hitting https://github.com/CocoaPods/CocoaPods/issues/10104.", "@thaink thanks - I'll change the build config and retest\r\n@yyoon checking the lint issue thread and test again. \r\n", "Both remarks helped. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45707\">No</a>\n"]}, {"number": 45706, "title": "Dot layer incomplete description", "body": "### URL(s) with the issue: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot)\r\n\r\n### Description of issue (what needs changing):\r\nThe dot layer accepts (0,0) as an argument for the axes, which produces the same result as (1,1). This behavior is not documented and more confusing than helpful since it suggests that the numbering of the axes starts at zero, but it actually starts at one. Since (0,0) as an argument seems to work, it suggests that also combination like (0,1) or (0,2) should work, but in this cases errors are produced. It might be better to not accept (0,0) as input for the axes, or to specify the motivation behind it in the documentation. \r\n\r\nMoreover, it would great to clarify the requirements for matching axes sizes in the documentation. Right now the documentation only states that the size of the selected axes must match, but e.g., also the first axes must mach otherwise an error is produced.\r\n\r\n\r\n\r\n", "comments": ["@rschumi0,\r\n[ Keras Dot Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot) doesn't accept **`(0,0)`** for **`axes`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/c47fafc91a069b557dffc9b020ea0169/dot_layer.ipynb) which gives Value Error. \r\n\r\nCan you please confirm if we can close this issue as the functionality is as expected? Thanks!", "Yes, it seems it's not an issue any more with the latest version. I initially had an older version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45706\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45706\">No</a>\n"]}, {"number": 45705, "title": "Wrong default value in GRU layer documentaion", "body": "### URL(s) with the issue: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\r\n\r\n### Description of issue (what needs changing):\r\nThe default value for the recurrent_activation of the GRU layer does not seem to be sigmoid as it is specified in the documentation.\r\n\r\n### Usage example\r\nThe following two code examples illustrate the issue. The first one specifies the recurrent_activation argument by setting it to 'sigmoid' and the second does not do that, but should have it as a default. However, the examples produce a slightly different result which suggests that there is another default value. (Tensorflow version  'v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')\r\n```\r\nmodel = keras.Sequential([\r\nkeras.layers.LSTM(3,kernel_initializer='ones', recurrent_initializer='zeros',\r\n    bias_initializer='zeros',  input_shape=(2,2),recurrent_activation='sigmoid')])\r\nx= tf.constant([[[1, 2],[2,3]],[[1, 1],[0,1]],[[1, 2],[2,3]]])\r\nprint model.predict(x,steps=1)\r\n```\r\n\r\n```\r\nmodel = keras.Sequential([\r\nkeras.layers.LSTM(3,kernel_initializer='ones', recurrent_initializer='zeros',\r\n    bias_initializer='zeros',  input_shape=(2,2))])\r\nx= tf.constant([[[1, 2],[2,3]],[[1, 1],[0,1]],[[1, 2],[2,3]]])\r\nprint model.predict(x,steps=1)\r\n```\r\n", "comments": ["@rschumi0,\r\nTensorFlow 1.x is not actively supported. Please update TensorFlow to the latest stable version v2.4.\r\n\r\nUsing TensorFlow v2.4, I got similar results for both the cases. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/43411edfb227c92ac748314dc3280c80/45705.ipynb#scrollTo=OqpOs1vAvxzY). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45705\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45705\">No</a>\n"]}, {"number": 45704, "title": "copy reference kernels from lite to micro ", "body": "PR3 in #45693", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@stephanboner  Can you please resolve conflicts? Thanks!", "Merged master. Reference kernel of `space_to_batch_nd.cc` not needed anymore since the micro version from #46681 was already merged into master", "@stephanboner Can you please resolve conflicts? Thanks!", "Looks like all my changes are now already done in PR 4 and 5 so I'll close this one"]}, {"number": 45703, "title": "Wrong error message for DepthwiseConv2D", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 'v1.14.0-rc1-22-gaf24dc91b5', '1.14.0'\r\n- Python version: python 2.7\r\n\r\n**Describe the current behavior**\r\nThere seems to be a wrong error message for the DepthwiseConv2D when different kernel dimension are specified. An InvalidArgumentError that states that the strides must be equal length is produced, even though the strides are not specified and are equal per default:\"Current implementation only supports equal length strides in the row and column dimensions.\"\r\nThe error is e.g., produced by the following code.\r\n\r\nIt works without error when the kernel dimensions are changed from (2,3) to (3,3). Hence, a correct error message that states that unequal kernel dimensions are currently also not supported would be great. It might also be good if this is mentioned in the documentation.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nmodel = keras.Sequential([keras.layers.DepthwiseConv2D(3, (2, 3), depth_multiplier=4, input_shape=(3,3,3))])\r\nx = tf.constant([[[[8,3,5],[5,4,-2],[2,1,-1]],[[8,3,5],[5,4,-2],[2,1,-1]],[[8,3,5],[5,4,-2],[2,1,-1]]]])\r\nprint np.array2string(model.predict(x,steps=1), separator=', ')\r\n```\r\n\r\n\r\n", "comments": ["@rschumi0 \r\nI tried in colab with TF version 2.3 , nightly version(`2.5.0-dev20201215`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8d2b09537dcb336160145bd8f9538f10/untitled578.ipynb). Thanks!\r\n", "@rschumi0,\r\nThe **`Error Message`** is being displayed as expected because, as per the [Documentation of DepthwiseConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D#arguments), second argument is **`Strides`**, and hence the **`Error Message`** is related to **`Strides`**. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45703\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45703\">No</a>\n"]}, {"number": 45702, "title": "Why TPU uses more memory than GPU?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Mostly stock, very little custom\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 - **Colab PRO**\r\n- TensorFlow installed from (source or binary): Pre-installed\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.x\r\n- GPU model and memory: Using TPU\r\n\r\nI was trying to train a model with around ~700 Million Parameters, however, when I switch to TPU with the correct strategy and proper initialization (code taken from Official TensorFlow site) I find that it gives me a resource error just before training (when I compile the model):-\r\n\r\n```\r\nEpoch 1/3\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\n\r\n---------------------------------------------------------------------------\r\n\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n\r\n<ipython-input-15-331b28c97632> in <module>()\r\n      8     filepath=checkpoint_prefix, monitor='loss', save_best_only=True)\r\n      9 \r\n---> 10 history = model.fit(X_train, y_train, epochs=3, batch_size=4, verbose=1, callbacks=[checkpoint_callback])\r\n\r\n10 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nResourceExhaustedError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:346 run\r\n        return self.extended.tpu_run(fn, args, kwargs, options)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1095 tpu_run\r\n        return func(args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1162 tpu_function\r\n        padding_spec=padding_spec)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:915 replicate\r\n        padding_spec=padding_spec)[1]\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:1380 split_compile_and_replicate\r\n        outputs = computation(*computation_inputs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:1124 replicated_fn\r\n        result[0] = fn(*replica_args, **replica_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\r\n        self.trainable_variables)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2745 _minimize\r\n        experimental_aggregate_gradients=False)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:519 apply_gradients\r\n        self._create_all_weights(var_list)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:704 _create_all_weights\r\n        self._create_slots(var_list)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/adam.py:129 _create_slots\r\n        self.add_slot(var, 'v')\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:764 add_slot\r\n        initial_value=initial_value)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:262 __call__\r\n        return cls._variable_v2_call(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:256 _variable_v2_call\r\n        shape=shape)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2124 create_colocated_variable\r\n        return next_creator(**kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2024 creator_with_resource_vars\r\n        created = self._create_variable(next_creator, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:870 _create_variable\r\n        **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py:291 create_mirrored_variable\r\n        value_list = real_mirrored_creator(**kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:861 _real_mirrored_creator\r\n        v = next_creator(**kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:685 variable_capturing_scope\r\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:264 __call__\r\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:233 __init__\r\n        shape = initial_value.shape\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1167 shape\r\n        self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())\r\n\r\n    ResourceExhaustedError: Failed to allocate request for 377.93MiB (396288000B) on device ordinal 0\r\n\r\n```     \r\nThis is extremely confusing, as I have thought that TPU's have _wayy_ more memory than a GPU (Like at least 64GiB). Does this indicate a bug on my side? Is there any way to check the actual **memory** capacity and memory used of the TPU? please help\r\n. I am using Colab Pro\r\n\r\nTPU Initialization code:-\r\n>import tensorflow as tf\r\n>tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n>print(f\"Running on TPU: {tpu.cluster_spec().as_dict()['worker']}\")  \r\n>tf.config.experimental_connect_to_cluster(tpu)\r\n>tf.tpu.experimental.initialize_tpu_system(tpu)\r\n>strategy = tf.distribute.TPUStrategy(tpu)\r\n", "comments": ["Reproduction Code:-\r\n```\r\ndf = pd.read_csv('/content/drive/MyDrive/HashPro/hash.csv').abs()\r\ndataset = df.values\r\n\r\nX = dataset[:,0:40].astype(float)\r\nY = dataset[:,40].astype(int)\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)\r\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\r\n\r\nLrelu = tf.keras.layers.LeakyReLU(alpha=0.1)\r\nn_feautures = 40                 #Any number\r\ndef build_model():\r\n  model = tf.keras.Sequential([\r\n  tf.keras.layers.Dense(40, activation=Lrelu, input_shape=(n_features,)),\r\n  tf.keras.layers.Reshape((n_features,1), input_shape=(n_features,)),\r\n  tf.keras.layers.Dense(9000, activation=Lrelu),\r\n  tf.keras.layers.Dense(7500, activation=Lrelu),\r\n  tf.keras.layers.Conv1D(60, 6, 1, activation='relu'),\r\n  tf.keras.layers.Dense(5500, activation=Lrelu),\r\n  tf.keras.layers.Dense(8800, activation=Lrelu),\r\n  tf.keras.layers.Conv1D(60, 6, 1, activation=Lrelu),\r\n  tf.keras.layers.Dropout(0.15),\r\n\r\n  tf.keras.layers.Dense(9000, activation=Lrelu),\r\n  tf.keras.layers.Conv1D(60, 6, 1, activation=Lrelu),\r\n  tf.keras.layers.Dense(10000, activation=Lrelu),\r\n  tf.keras.layers.Dense(8000, activation=Lrelu),\r\n\r\n  tf.keras.layers.Dense(9000, activation=Lrelu),\r\n  tf.keras.layers.Conv1D(60, 6, 1, activation=Lrelu),\r\n  tf.keras.layers.Dense(11000, activation=Lrelu),\r\n\r\n  tf.keras.layers.Dense(9000, activation=Lrelu),\r\n  tf.keras.layers.Dense(11000, activation=Lrelu),\r\n  \r\n  tf.keras.layers.Dense(8000, activation=Lrelu),\r\n  tf.keras.layers.Dense(8100, activation=Lrelu),\r\n  tf.keras.layers.Dense(8000, activation=Lrelu),\r\n  tf.keras.layers.Dense(9000, activation=Lrelu),\r\n  tf.keras.layers.Dense(1, activation='linear'),\r\n  tf.keras.layers.BatchNormalization()\r\n])\r\n  return model\r\n\r\nwith strategy.scope():\r\n    model = build_model()\r\n    adam = Adam(0.001)\r\n    model.compile(optimizer=adam, loss=\"mean_squared_logarithmic_error\") #, metrics=['accuracy', 'acc']\r\n\r\n#Let's see the model's organs!\r\nmodel.summary()\r\n\r\n!mkdir checkpoints\r\n\r\ncheckpoint_dir = '/content/checkpoints/'\r\n\r\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.hdf5\")\r\n\r\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\r\n    filepath=checkpoint_prefix, monitor='loss', save_best_only=True)\r\n\r\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=32,\r\n                    verbose=1, callbacks=[checkpoint_callback],\r\n                    steps_per_epoch=2500)\r\n\r\n```\r\nComment below for more information and I would update this code. Also, before this code cell, I also import a 600Mb Pandas Dataset, but since that would use System RAM, It would have a negligible effect and would be totally useless. \r\nBut for reproduction, we can paste this below in a csv file, chose the path, and try to train the model:\r\n\r\n```\r\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,8889\r\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,3241314\r\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,937543\r\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,312565\r\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,4363\r\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,81121\r\n1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,54454\r\n````\r\nError has been already pasted above along with the TPU initialization code. Any update @amahendrakar ?", "@neel04,\r\nOn running the code, I am facing an error stating `NameError: name 'n_features' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/17e1be8ad836ee6419607fbc3f0e93d0/45702.ipynb#scrollTo=7i19WveD60w4).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Alternatively, you can also share the gist of the Colab notebook you are running. Thanks!", "@amahendrakar Sorry, there was a typo. The line should be `n_features = 40`. Can confirm reproduction of error, new Gist [here](https://colab.research.google.com/gist/neel04/d312b365e48b99b109f6dc9a81967d8e/45702.ipynb). Ping me whenever you reply", "Hi @neel04, when using `TPUStrategy` the model has to fit on each TPU core. So while the [V2 TPU might have 64GB](https://cloud.google.com/tpu/docs/tpus), each core only has 8GB.  `TPUStrategy` is a data parallelism strategy, and creates a model replica on each core, which you can verify by typing `strategy.num_replicas_in_sync` (this should be 8).\r\n\r\nFor a more in depth answer see #44689. However, that thread was about v3-8 TPUs, which have 16GB per core, and I think the default in Colab is the V2 TPUS, which have 8GB per core. If you want to double check, this [Stack Overflow answer](https://stackoverflow.com/questions/64719177/google-colab-tpu-version) shows how you can figure out the TPU version and memory in Colab. \r\nHope this clears up any confusion!", "@nikitamaia Thanx a lot for your reply. I understand the limit of each core in TPU, but perhaps is there a way to distrribute the model replica on every 2 cores rather than 1 core so as to fit all the parameters and train on it? Is there a way to adjust the strategy to accomodate something like that?", "There is no out of the box way to do that currently, as the tf.distribute.Strategy API only supports data parallelism. Distributing the replica over two cores would require model parallelism, which is not supported right now. If 16GB is enough for you, you could consider upgrading to the V3-8 TPUs on GCP. Or if you're interested in doing some custom work  to distribute the model you can check out [Mesh TensorFlow](https://github.com/tensorflow/mesh).\r\n\r\nClosing this issue now as there is no bug.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45702\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45702\">No</a>\n"]}, {"number": 45701, "title": "To provide the prediction probabilities for Binary classification task using the sigmoid activation in the output layer.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFeature: For binary classification with sigmoid activation in the output layer, the model.predict() should output the probability values for both class 0 and class 1 as it does in the case of multi-class classification.\r\n\r\nCurrent behavior/state: Using the model.predict() method, the output is a numpy array with single probability value. If the probability value>0.5 then it belongs to class 1 else class 0. i.e. It does not provide the probability values for each class separately as in a multiclass classification which uses the softmax activation in the output layer.\r\n\r\n**Will this change the current api? How?**\r\nMinor change for the output of model.predict() for binary classification task with 1 neuron and sigmoid activation in the output layer.\r\n\r\n**Who will benefit with this feature?**\r\nThis feature will be helpful for explaining the model. There are XAI libraries like Lime, Shap which depends on the probability values of each class for explaining the model. They take the model as an input parameter and expect the prediction/output of the model prediction in the form of probabilities for each class to explain the model.\r\n\r\n**Any Other info.**\r\nLime takes a function as an input to its explainer. This function can be user-defined which can implicitly call TF model.predict() and output the probability values for both the classes based on the 0.5 threshold value. \r\nBut in case of Shap, the Shap Deep Explainer (especially designed for TF and Pytorch Neural Netoworks) takes the TF/Pytorch model as input and internally calls the model.predict() and generates the shap values. Since, here in case of binary classification with sigmoid activation only single probability value is generated, so shap also generate single shap values instead of shap values for both the classes.\r\n[Link to Lime](https://github.com/marcotcr/lime)\r\n[Link to Shap Deep Explainer](https://shap.readthedocs.io/en/latest/generated/shap.DeepExplainer.html#shap.DeepExplainer)\r\n", "comments": ["This is intended behavior: `model.predict()` simply runs the forward pass of the model on some input and returns the result. For a model that ends with a single output unit and a sigmoid activation, only scalar values are returned. `model.predict()` is different from sklearn's `predict`. "]}, {"number": 45700, "title": "Request for a tutorial on building C++ API not pure C", "body": "**System information**\r\n- TensorFlow version: 2.4\r\n- Are you willing to contribute it: Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe [current documentation](https://www.tensorflow.org/install/lang_c) only tells user how to install TensorFlow for C API. But none of them tells how to install TensorFlow for **C++**.\r\n\r\n**Will this change the current api? How?**\r\nThis does not change the current api. It just enables user know how to make the existing functionality available on our computer.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who hate python but still wants to do machine learning with tensorflow\r\n\r\n**Any Other info.**\r\n", "comments": ["It is actually quite simple. I build the C++ library on Windows (VS2019) like this:\r\n```\r\nbazel build --config=opt --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc tensorflow:tensorflow_cc\r\n```\r\n\r\nA good tutorial is available here: https://medium.com/vitrox-publication/deep-learning-frameworks-tensorflow-build-from-source-on-windows-python-c-cpu-gpu-d3aa4d0772d8", "> It is actually quite simple. I build the C++ library on Windows (VS2019) like this:\r\n> \r\n> ```\r\n> bazel build --config=opt --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --announce_rc tensorflow:tensorflow_cc\r\n> ```\r\n> \r\n> A good tutorial is available here: https://medium.com/vitrox-publication/deep-learning-frameworks-tensorflow-build-from-source-on-windows-python-c-cpu-gpu-d3aa4d0772d8\r\n\r\nI ended up using [this one](https://github.com/FloopCZ/tensorflow_cc/tree/tf-v2.4.0) to install tensorflow c++ API on my computer.\r\nThe only problem I ran into is bazel couldn't find python despite python location was previously configured with ./configure. By brute force, I symliink the location where bazel is looking for python to the directory where python is actually being installed and solved the problem.", "> The only problem I ran into is bazel couldn't find python despite python location was previously configured with ./configure. By brute force, I symliink the location where bazel is looking for python to the directory where python is actually being installed and solved the problem.\r\n\r\nAnother option could be to set the environment variables `PYTHON_BIN_PATH` and `PYTHON_LIB_PATH` before ./cofigure as in the [CI script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/bazel/common_env.sh).", "C++ is the standard language of TensorFlow, so it is not under \"bindings.\" So just follow the build instructions. The point however is that building C++ libraries on different versions of the compilers is generally not safe, so that is why we don't really have a c++ only archive.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45700\">No</a>\n"]}]