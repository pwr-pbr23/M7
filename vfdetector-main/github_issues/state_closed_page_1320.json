[{"number": 13502, "title": "Allowing a single Tensor in a StagingArea", "body": "This brings the code in line with the documentation so that both of the\nfollowing are valid:\n\n    import tensorflow as tf\n    from tensorflow.contrib import staging\n\n    staging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))\n    staging.StagingArea(dtypes=[tf.int32]).put([tf.constant(2), tf.constant(2)])\n\nFixes #13288", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "There seem to be real failures in the GPU build. @JackDanger could you look into those?", "Ping @JackDanger. Marking as stalled for now.", "@JackDanger feel free to reopen if you get a chance to look at this."]}, {"number": 13501, "title": "Feature Request: collections scope", "body": "The is currently no way to assign variables to collections during creation time other than specifying in the explicit function call to `tf.get_variable` or `tf.Variable`.  This is problematic anytime you'd like to use existing functions that create variables as part of their calls and you want those variables to be part of a certain collection.  It's even more problematic when you want variables to not be members of  `GraphKeys.GLOBAL_VARIABLES`, because this is the default for the `tf.get_variable` and `tf.Variable`. \r\n", "comments": ["i agree with you", "workaround\r\n```\r\ng = tf.get_default_graph()\r\ns = tf.Variable(\"hi\")\r\nprint(\"Global variables\", tf.global_variables())\r\ndel g._collections[tf.GraphKeys.GLOBAL_VARIABLES][0]\r\nprint(\"Global variables\", tf.global_variables())\r\n\r\n```", "@yaroslavvb thanks for the workaround! \r\n\r\nFor anyone else who doesn't see how switching the collection would work, here is an example\r\n\r\n```\r\ng = tf.get_default_graph()\r\ns = tf.Variable(\"hi\")\r\nprint(\"Global variables\", tf.global_variables())\r\nadd_to_local = tf.add_to_collection(tf.GraphKeys.LOCAL_VARIABLES,s)\r\nprint(\"Local variables\", tf.local_variables())\r\ndel g._collections[tf.GraphKeys.GLOBAL_VARIABLES][0]\r\nprint(\"Global variables\", tf.global_variables())\r\nprint(\"Local variables\", tf.local_variables())\r\n```\r\n"]}, {"number": 13500, "title": "Tensorflow binary seems compiled to use SIMD instructions like AVX2 and FMA, but actually not?", "body": "I found similar issues mentioned as #8037, #7778 etc, but the issue seems not solved: the warnings did disappear after building with the necessary optimization options, but they appeared again when I followed this tutorial (https://www.tensorflow.org/performance/xla/tfcompile) to the last step. So, is the tensorflow binary compiled to use the SIMD instructions or not?\r\n\r\n### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: v1.3.0-rc1-3000-g840dcae\r\n- **Python version**: Python3\r\n- **Bazel version**: 0.6.0\r\n- **CPU**: Intel Core i7-4770, Haswell architecture, supporting AVX2 and FMA\r\n- **GPU**: No\r\n- **Compiler**: gcc 5.4.0\r\n\r\n### Issue reproducing:\r\n1. Building tensorflow from source:\r\n\r\n- Configure: only jemalloc and XLA JIT support are ticked. The default optimization flag is `-march=native`, therefore was not specified;\r\n\r\n- Build pip package:\r\n```\r\nbazel build --config=opt --copt=-mavx2 --copt=-mfma --config=mkl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/tools/pip_package:build_pip_package\r\n``` \r\n- Install pip package:\r\n```\r\nsudo -H python3 -m pip install /tmp/tensorflow_pkg/tensorflow-1.3.0-cp35-cp35m-linux_x86_64.whl\r\n```\r\n- The installation was validated using the \"Hello, TensorFlow!\" example, and no warnings are generated.\r\n\r\n2. Generating `tfcompile` binary:\r\n```\r\nbazel build --config=opt --copt=-mavx2 --copt=-mfma --config=mkl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/compiler/aot:tfcompile\r\n```\r\n\r\n3. Follow the tutorial here https://www.tensorflow.org/performance/xla/tfcompile, in the directory `//tensorflow/compiler/aot/tests`:\r\n- Step 1: The config file already exists as `test_graph_tfmatmul.config.pbtxt`;\r\n\r\n- Step 2.1: Generate the graph file `test_graph_tfmatmul.pb`:\r\n```\r\npython3 ./make_test_graphs.py --out_dir=./\r\n```\r\n- Step 2.2: Compile the graph using `tfcompile`:\r\n```\r\n~/tensorFlow_src/tensorflow/bazel-bin/tensorflow/compiler/aot/tfcompile --graph=\"./test_graph_tfmatmul.pb\" --config=\"./test_graph_tfmatmul.config.pbtxt\" --entry_point=\"test_graph_tfmatmul\" --cpp_class=\"foo::bar::MatMulComp\" --out_object=\"test_graph_tfmatmul.o\" --out_header=\"test_graph_tfmatmul.h\" --target_features=\"+avx2\"\r\n```\r\n\r\n- Step 3: Creating a file named `my_code.cc`:\r\n```\r\n#define EIGEN_USE_THREADS\r\n#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n\r\n#include <iostream>\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/compiler/aot/tests/test_graph_tfmatmul.h\" // generated\r\n\r\nint main(int argc, char** argv) {\r\n    Eigen::ThreadPool tp(2);  // Size the thread pool as appropriate.\r\n    Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());\r\n\r\n    foo::bar::MatMulComp matmul;\r\n    matmul.set_thread_pool(&device);\r\n\r\n    // Set up args and run the computation.\r\n    const float args[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};\r\n    std::copy(args + 0, args + 6, matmul.arg0_data());\r\n    std::copy(args + 6, args + 12, matmul.arg1_data());\r\n    matmul.Run();\r\n\r\n    // Check result\r\n    if (matmul.result0(0, 0) == 58) {\r\n        std::cout << \"Success\" << std::endl;\r\n    } else {\r\n        std::cout << \"Failed. Expected value 58 at 0,0. Got:\"\r\n                    << matmul.result0(0, 0) << std::endl;\r\n    }\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n- Step 4.1: Create the `BUILD` file:\r\n```\r\n# Example of linking your binary\r\n# Also see //third_party/tensorflow/compiler/aot/tests/BUILD\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\n# The same tf_library call from step 2 above.\r\ntf_library(\r\n    name = \"test_graph_tfmatmul\",\r\n    cpp_class = \"foo::bar::MatMulComp\",\r\n    graph = \"test_graph_tfmatmul.pb\",\r\n    config = \"test_graph_tfmatmul.config.pbtxt\",\r\n)\r\n\r\n# The executable code generated by tf_library can then be linked into your code.\r\ncc_binary(\r\n    name = \"my_binary\",\r\n    srcs = [\r\n        \"my_code.cc\",  # include test_graph_tfmatmul.h to access the generated header\r\n    ],\r\n    deps = [\r\n        \":test_graph_tfmatmul\",  # link in the generated object file\r\n        \"//third_party/eigen3\",\r\n    ],\r\n    linkopts = [\r\n        \"-lpthread\",\r\n    ]\r\n)\r\n```\r\n\r\n- Step 4.2: Create the final binary:\r\n```\r\nbazel build --config=opt --copt=-mavx2 --copt=-mfma --config=mkl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/compiler/aot/tests:my_binary\r\n```\r\n\r\nFinally, it will print:\r\n`INFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:\r\n2017-10-05 15:15:29.233159: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA`\r\n(An error will also occur, but that is another issue #13482).\r\n\r\nSo, is the tensorflow binary compiled to use the SIMD instructions (SSE4.1 SSE4.2 AVX AVX2 FMA) or not? May I have your advice?\r\n", "comments": ["@MartinZZZ This is slightly confusing.\r\n\r\nNote that the info log `Your CPU supports...` is actually coming from the invocation of the `tfcompile` tool, which generates the header and object file, and is a side-effect of how we perform the compilation re-using underlying TensorFlow infrastructure.  It's not actually trying to warn you that the generated code is missing support for SIMD.\r\n\r\nThe reason I know this is because the first part of the info log says:\r\n```\r\nFrom Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul\r\n```\r\n\r\nThe genrule above corresponds to this bazel genrule, which is created by the tf_library build macro:\r\nhttps://github.com/tensorflow/tensorflow/blob/263d025fb6dee974eefb30a51372188fb856d6cc/tensorflow/compiler/aot/tfcompile.bzl#L134\r\n\r\nI'm suspecting that you saw this info log in #13482 because you were running `tfcompile` manually; if instead you let blaze run it, via the `tf_library` macro, these info logs won't be visible.\r\n\r\nThat said, note that by default tfcompile doesn't assume any target-specific features (SIMD, etc).  If there are specific features you'd like to enable, you need to set them via the following tfcompile flags:\r\n```\r\n\t--target_cpu=\"\"                  \tstring\tTarget cpu, similar to the clang -mcpu flag.  http://clang.llvm.org/docs/CrossCompilation.html#cpu-fpu-abi\r\n\t--target_features=\"\"             \tstring\tTarget features, e.g. +avx2, +neon, etc.\r\n```\r\n\r\nThese may be specified in your `tf_library` build rule using the `tfcompile_flags` argument:\r\nhttps://github.com/tensorflow/tensorflow/blob/263d025fb6dee974eefb30a51372188fb856d6cc/tensorflow/compiler/aot/tfcompile.bzl#L25", "@tatatodd Thanks for the information. I specified those flags in the `tf_library`, but the log still exists:\r\n\r\nThe `tfcompile_flags` is specified as following (in //tensorflow/tensorflow/compiler/aot/tfcompile.bzl):\r\n```\r\n# -*- Python -*-\r\n\r\nload(\"//tensorflow:tensorflow.bzl\", \"if_android\", \"tf_copts\")\r\n\r\ndef tf_library(name, graph, config,\r\n               freeze_checkpoint=None, freeze_saver=None,\r\n               cpp_class=None, gen_test=True, gen_benchmark=True,\r\n               visibility=None, testonly=None,\r\n               tfcompile_flags=str('--target_cpu=\"x86-64\" --target_features=\"+sse4.1\" --target_features=\"+sse4.2\" --target_features=\"+avx\" --target_features=\"+avx2\" --target_features=\"+fma\"'),\r\n               #tfcompile_flags=None,\r\n               tfcompile_tool=\"//tensorflow/compiler/aot:tfcompile\",\r\n               include_standard_runtime_deps=True, deps=None, tags=None):\r\n \r\n  if not cpp_class:\r\n    fail(\"cpp_class must be specified\")\r\n\r\n  tfcompile_graph = graph\r\n  if freeze_checkpoint or freeze_saver:\r\n    if not freeze_checkpoint:\r\n      fail(\"freeze_checkpoint must be specified when freeze_saver is specified\")\r\n\r\n    freeze_name = \"freeze_\" + name\r\n    freeze_file = freeze_name + \".pb\"\r\n\r\n    # First run tfcompile to generate the list of out_nodes.\r\n    out_nodes_file = \"out_nodes_\" + freeze_name\r\n    native.genrule(\r\n        name=(\"gen_\" + out_nodes_file),\r\n        srcs=[config],\r\n        outs=[out_nodes_file],\r\n        cmd=(\"$(location \" + tfcompile_tool + \")\" +\r\n             \" --config=$(location \" + config + \")\" +\r\n             \" --dump_fetch_nodes > $@\"),\r\n        tools=[tfcompile_tool],\r\n        # Run tfcompile on the build host, rather than forge, since it's\r\n        # typically way faster on the local machine.\r\n        local=1,\r\n        tags=tags,\r\n    )\r\n\r\n    # Now run freeze_graph to convert variables into constants.\r\n    freeze_args = (\" --input_graph=$(location \" + graph + \")\" +\r\n                   \" --input_binary=\" + str(not graph.endswith(\".pbtxt\")) +\r\n                   \" --input_checkpoint=$(location \" + freeze_checkpoint + \")\" +\r\n                   \" --output_graph=$(location \" + freeze_file + \")\" +\r\n                   \" --output_node_names=$$(<$(location \" + out_nodes_file +\r\n                   \"))\")\r\n    freeze_saver_srcs = []\r\n    if freeze_saver:\r\n      freeze_args += \" --input_saver=$(location \" + freeze_saver + \")\"\r\n      freeze_saver_srcs += [freeze_saver]\r\n    native.genrule(\r\n        name=freeze_name,\r\n        srcs=[\r\n            graph,\r\n            freeze_checkpoint,\r\n            out_nodes_file,\r\n        ] + freeze_saver_srcs,\r\n        outs=[freeze_file],\r\n        cmd=(\"$(location //tensorflow/python/tools:freeze_graph)\" +\r\n             freeze_args),\r\n        tools=[\"//tensorflow/python/tools:freeze_graph\"],\r\n        tags=tags,\r\n    )\r\n    tfcompile_graph = freeze_file\r\n\r\n  # Rule that runs tfcompile to produce the header and object file.\r\n  header_file = name + \".h\"\r\n  object_file = name + \".o\"\r\n  ep = (\"__\" + PACKAGE_NAME + \"__\" + name).replace(\"/\", \"_\")\r\n  native.genrule(\r\n      name=(\"gen_\" + name),\r\n      srcs=[\r\n          tfcompile_graph,\r\n          config,\r\n      ],\r\n      outs=[\r\n          header_file,\r\n          object_file,\r\n      ],\r\n      cmd=(\"$(location \" + tfcompile_tool + \")\" +\r\n           \" --graph=$(location \" + tfcompile_graph + \")\" +\r\n           \" --config=$(location \" + config + \")\" +\r\n           \" --entry_point=\" + ep +\r\n           \" --cpp_class=\" + cpp_class +\r\n           \" --target_triple=\" + target_llvm_triple() +\r\n           \" --out_header=$(@D)/\" + header_file +\r\n           \" --out_object=$(@D)/\" + object_file +\r\n           \" \" + (tfcompile_flags or \"\")),\r\n      tools=[tfcompile_tool],\r\n      visibility=visibility,\r\n      testonly=testonly,\r\n      # Run tfcompile on the build host since it's typically faster on the local\r\n      # machine.\r\n      #\r\n      # Note that setting the local=1 attribute on a *test target* causes the\r\n      # test infrastructure to skip that test.  However this is a genrule, not a\r\n      # test target, and runs with --genrule_strategy=forced_forge, meaning the\r\n      # local=1 attribute is ignored, and the genrule is still run.\r\n      #\r\n      # https://www.bazel.io/versions/master/docs/be/general.html#genrule\r\n      local=1,\r\n      tags=tags,\r\n  )\r\n\r\n  # The cc_library rule packaging up the header and object file, and needed\r\n  # kernel implementations.\r\n  need_xla_data_proto = (tfcompile_flags and\r\n                         tfcompile_flags.find(\"--gen_program_shape\") != -1)\r\n  native.cc_library(\r\n      name=name,\r\n      srcs=[object_file],\r\n      hdrs=[header_file],\r\n      visibility=visibility,\r\n      testonly=testonly,\r\n      deps = [\r\n          # These deps are required by all tf_library targets even if\r\n          # include_standard_runtime_deps is False.  Without them, the\r\n          # generated code will fail to compile.\r\n          \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\r\n          \"//tensorflow/core:framework_lite\",\r\n      ] + (need_xla_data_proto and [\r\n          # If we're generating the program shape, we must depend on the proto.\r\n          \"//tensorflow/compiler/xla:xla_data_proto\",\r\n      ] or []) + (include_standard_runtime_deps and [\r\n          # TODO(cwhipkey): only depend on kernel code that the model actually needed.\r\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\",\r\n          \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_avx\",\r\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_neon\",\r\n          \"//tensorflow/compiler/xla/service/cpu:cpu_runtime_sse4_1\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\r\n          \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\r\n          \"//third_party/eigen3\",\r\n      ] or []) + (deps or []),\r\n      tags=tags,\r\n  )\r\n\r\n  # Variables used for gen_test and gen_benchmark.\r\n  no_ns_name = \"\"\r\n  cpp_class_split = cpp_class.rsplit(\"::\", maxsplit=2)\r\n  if len(cpp_class_split) == 1:\r\n    no_ns_name = cpp_class_split[0]\r\n  else:\r\n    no_ns_name = cpp_class_split[1]\r\n  sed_replace = (\r\n      \"-e \\\"s|{{TFCOMPILE_HEADER}}|$(location \" + header_file + \")|g\\\" \" +\r\n      \"-e \\\"s|{{TFCOMPILE_CPP_CLASS}}|\" + cpp_class + \"|g\\\" \" +\r\n      \"-e \\\"s|{{TFCOMPILE_NAME}}|\" + no_ns_name + \"|g\\\" \")\r\n\r\n  if gen_test:\r\n    test_name = name + \"_test\"\r\n    test_file = test_name + \".cc\"\r\n    # Rule to rewrite test.cc to produce the test_file.\r\n    native.genrule(\r\n        name=(\"gen_\" + test_name),\r\n        testonly=1,\r\n        srcs=[\r\n            \"//tensorflow/compiler/aot:test.cc\",\r\n            header_file,\r\n        ],\r\n        outs=[test_file],\r\n        cmd=(\"sed \" + sed_replace +\r\n             \" $(location //tensorflow/compiler/aot:test.cc) \" +\r\n             \"> $(OUTS)\"),\r\n        tags=tags,\r\n    )\r\n\r\n    # The cc_test rule for the generated code.\r\n    native.cc_test(\r\n        name=test_name,\r\n        srcs=[test_file],\r\n        deps=[\r\n            \":\" + name,\r\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n            \"//tensorflow/compiler/aot:runtime\",\r\n            \"//tensorflow/compiler/aot:tf_library_test_main\",\r\n            \"//tensorflow/compiler/xla:executable_run_options\",\r\n            \"//third_party/eigen3\",\r\n            \"//tensorflow/core:lib\",\r\n            \"//tensorflow/core:test\",\r\n            ],\r\n        tags=tags,\r\n    )\r\n\r\n  if gen_benchmark:\r\n    benchmark_name = name + \"_benchmark\"\r\n    benchmark_file = benchmark_name + \".cc\"\r\n    benchmark_main = (\"//tensorflow/compiler/aot:\" +\r\n                      \"benchmark_main.template\")\r\n\r\n    # Rule to rewrite benchmark.cc to produce the benchmark_file.\r\n    native.genrule(\r\n        name=(\"gen_\" + benchmark_name),\r\n        srcs=[\r\n            benchmark_main,\r\n            header_file,\r\n        ],\r\n        testonly = testonly,\r\n        outs=[benchmark_file],\r\n        cmd=(\"sed \" + sed_replace +\r\n             \" $(location \" + benchmark_main + \") \" +\r\n             \"> $(OUTS)\"),\r\n        tags=tags,\r\n    )\r\n\r\n    # The cc_benchmark rule for the generated code.\r\n    #\r\n    # Note: to get smaller size on android for comparison, compile with:\r\n    #    --copt=-fvisibility=hidden\r\n    #    --copt=-D_LIBCPP_TYPE_VIS=_LIBCPP_HIDDEN\r\n    #    --copt=-D_LIBCPP_EXCEPTION_ABI=_LIBCPP_HIDDEN\r\n    native.cc_binary(\r\n        name=benchmark_name,\r\n        srcs=[benchmark_file],\r\n        testonly = testonly,\r\n        copts = tf_copts(),\r\n        linkopts = if_android([\"-pie\", \"-s\"]),\r\n        deps=[\r\n            \":\" + name,\r\n            \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n            \"//tensorflow/compiler/aot:benchmark\",\r\n            \"//tensorflow/compiler/aot:runtime\",\r\n            \"//tensorflow/compiler/xla:executable_run_options\",\r\n            \"//third_party/eigen3\",\r\n        ] + if_android([\r\n            \"//tensorflow/compiler/aot:benchmark_extra_android\",\r\n        ]),\r\n        tags=tags,\r\n    )\r\n\r\n\r\ndef target_llvm_triple():\r\n  \"\"\"Returns the target LLVM triple to be used for compiling the target.\"\"\"\r\n  # TODO(toddw): Add target_triple for other targets.  For details see:\r\n  # http://llvm.org/docs/doxygen/html/Triple_8h_source.html\r\n  return select({\r\n      \"//tensorflow:android_armeabi\": \"armv5-none-android\",\r\n      \"//tensorflow:android_arm\": \"armv7-none-android\",\r\n      \"//tensorflow:android_arm64\": \"aarch64-none-android\",\r\n      \"//tensorflow:android_x86\": \"i686-none-android\",\r\n      \"//tensorflow:linux_ppc64le\": \"ppc64le-ibm-linux-gnu\",\r\n      \"//tensorflow:darwin\": \"x86_64-none-darwin\",\r\n      \"//conditions:default\": \"x86_64-pc-linux\",\r\n  })\r\n```\r\n\r\nThen instead of running `tfcompile` manually, I built the `cc_library` using the `tf_library` macro (i.e., skipping Step 2.2), and the final binary can be successfully created now with command:\r\n```\r\nbazel build //tensorflow/compiler/aot/tests:my_binary\r\n```\r\n\r\n but the logging info still exists:\r\n```\r\nINFO: Analysed target //tensorflow/compiler/aot/tests:my_binary (2 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Executing genrule //tensorflow/compiler/aot/tests:gen_test_graph_tfmatmul:\r\n2017-10-12 12:56:13.846822: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTarget //tensorflow/compiler/aot/tests:my_binary up-to-date:\r\n  bazel-bin/tensorflow/compiler/aot/tests/my_binary\r\nINFO: Elapsed time: 0.331s, Critical Path: 0.05s\r\nINFO: Build completed successfully, 2 total actions\r\n``` \r\n\r\nMay I have your advice?", "@MartinZZZ I see.  I had misinterpreted your previous message; I thought you meant that you no longer saw the log info when using `tf_library`.\r\n\r\nAs I mentioned, the log message is harmless, and is a side-effect of the way we perform the compilation.  We're changing the implementation (for a different reason), at which point the info log will go away.  In the meantime you should just ignore it.", "@tatatodd Thanks for the explanation. I seem to understand now, and may I check with you regarding the following two simple questions:\r\n\r\n(1) Does it mean that the created binary is actually able to use the SIMD instructions as specified by the `tfcompile_flags`, and the log message is just a false warning?\r\n\r\n(2) Whether the `tfcompile_flags` is successfully specifiled or not, will it affect the performance of `matmul` operation in this example? I thought it would not. As tensorflow is using Eigen's implementation for `matmul`, in which SIMD intrinsic instructions seem to be already used to exploit the SIMD hardware.\r\n", "@MartinZZZ Answers to your questions:\r\n\r\n1) Yes, if you set `tfcompile_flags` as in your example above, the code generated by XLA will use SIMD instructions.  The log message is a false warning, regardless of your tfcompile_flags settings.\r\n\r\n2) You're correct.  `tfcompile` uses XLA for compilation, and XLA currently calls out to Eigen for `matmul`.  This may change in the future; we do have a pure-XLA implementation of `matmul`, but it's quite slow.", "@tatatodd Thanks for your answers:)\r\n\r\nAnd please correct me if I am mistaken. I wonder if `XLA-AoT` mainly focuses on the space issue on mobile device, while `XLA-JIT` mainly focuses on the performance. If so, may I seek for your advice regarding the following short questions:\r\n\r\n(1) Will `XLA-JIT` generate code after fusing operations without calling to Eigen? \r\n\r\n(2) If so, is the code generation done by `XLA`, or totally relied on LLVM? and is this code released as well? Thanks.\r\n", "so am i", "@lijiansong, could you clarify your question?", "@carlthome MartinZZZ and lijiansong are asking which one has better runtime performance: XLA-JIT or XLA-AOT?\r\n\r\nThe documentation seems to imply that XLA-AOT is meant for space-constrained situations (e.g. mobile) but does not mention anything regarding runtime performance of XLA-AOT vs XLA-JIT. Any clarification on that point would be welcome.", "AOT and JIT provides the same performance benefits (e.g. op fusion, constant folding, common subexpression elimination and other HLOs).\r\n\r\nThe downside to AOT is that you have to specify static tensor shapes and know what hardware you're targeting, while JIT would do that for you.\r\n\r\nThe downside to JIT is that compilation happens in the runtime (which takes extra time) and that you have to bundle the compiler with your program."]}, {"number": 13499, "title": "Using third party library in custom op implementation with GPU memory manipulation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 14.04.5\r\n- **TensorFlow installed from (source or binary)**:pip\r\n- **TensorFlow version (use command below)**:1.2.0\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:CUDA 8.0\r\n- **GPU model and memory**:GeForce GTX 1080 8G\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nTo speed up the model's training and evaluating process, and to make the model more flexible, people would like to use some third party libraries for their customized ops. For example, CUDPP, http://cudpp.github.io/, allows people to build a hash table on GPU.\r\n\r\nHowever, I find that TensorFlow does not allow people to manipulate GPU memory by themselves, but only to use allocate_temp in the compute function, which is very inconvenient and inflexible. And these good third party libraries involve many GPU memory manipulations. I cannot find much information about this, but I met some errors in allocating memories.\r\n\r\n* What's the reason behind this prohibition?\r\n* Is there some method to allow us manipulate GPU by ourselves? (some switches, some options)\r\n* If we cannot manipulate GPU memory, then how can we use/adapt these third party GPU libraries for the customized ops? What's the good programming practice?\r\n\r\nMore specifically, the problem I met is that I implemented a customized op on GPU, using the multivalue hash table from the CUDPP library. For very small-scale data, the customized op passed the test. But when I use larger data, and incorporate the customized op into a larger model, then I cannot allocate the memory on CUDA. https://stackoverflow.com/questions/40183189/trouble-compiling-with-custom-tensorflow-gpu-op, this expalins something. I can avoid manipulating GPU memory myself, but the CUDPP library needs to manipulate GPU memory.\r\n\r\n\r\n\r\n### Source code / logs\r\nThe cudpp git repo: https://github.com/cudpp/cudpp/compare?expand=1.\r\n\r\nMy OpKernel:\r\n```C++\r\nvoid querySquarePointLauncher(int b, int n, int m, float grid_size, int nsample, const float *all_xyz, const float *centroids_xyz, const float *limits, const int *sizes, int *idx, int *pts_cnt, unsigned int *d_keys, unsigned int *d_vals, unsigned int *d_queries, uint2 *d_vals_multivalue);\r\nclass QuerySquarePointGpuOp : public OpKernel {\r\n    public:\r\n        explicit QuerySquarePointGpuOp(OpKernelConstruction* context) : OpKernel(context) {\r\n            OP_REQUIRES_OK(context, context->GetAttr(\"grid_size\", &grid_size_));\r\n            OP_REQUIRES(context, grid_size_ > 0, errors::InvalidArgument(\"QuerySquarePoint expects positive grid size\"));\r\n\r\n            OP_REQUIRES_OK(context, context->GetAttr(\"nsample\", &nsample_));\r\n            OP_REQUIRES(context, nsample_ > 0, errors::InvalidArgument(\"QuerySquarePoint expects positive nsample\"));\r\n        }\r\n\r\n        void Compute(OpKernelContext* context) override {\r\n            const Tensor& all_xyz_tensor = context->input(0);\r\n            OP_REQUIRES(context, all_xyz_tensor.dims()==3 && all_xyz_tensor.shape().dim_size(2)==3, errors::InvalidArgument(\"QuerySquarePoint expects (batch_size, ndataset, 3) all_xyz_tensor shape.\"));\r\n            int b = all_xyz_tensor.shape().dim_size(0);\r\n            int n = all_xyz_tensor.shape().dim_size(1);\r\n\r\n            const Tensor& centroids_xyz_tensor = context->input(1);\r\n            OP_REQUIRES(context, centroids_xyz_tensor.dims()==3 && centroids_xyz_tensor.shape().dim_size(2)==3, errors::InvalidArgument(\"QuerySquarePoint expects (batch_size, npoint, 3) centroids_xyz shape.\"));\r\n            int m = centroids_xyz_tensor.shape().dim_size(1);\r\n            \r\n            const Tensor& limits_tensor = context->input(2);\r\n            OP_REQUIRES(context, limits_tensor.dims()==1 && limits_tensor.shape().dim_size(0)==6, errors::InvalidArgument(\"QuerySquarePoint expects (6) limits shape.\"))\r\n            \r\n            const Tensor& sizes_tensor = context->input(3);\r\n            OP_REQUIRES(context, sizes_tensor.dims()==1 && sizes_tensor.shape().dim_size(0) == 3, errors::InvalidArgument(\"QuerySquarePoint expects (3) sizes shape.\"))\r\n\r\n            Tensor *idx_tensor = nullptr;\r\n            OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape{b,m,nsample_}, &idx_tensor));\r\n            Tensor *pts_cnt_tensor = nullptr;\r\n            OP_REQUIRES_OK(context, context->allocate_output(1, TensorShape{b,m}, &pts_cnt_tensor));\r\n            \r\n            Tensor keys_tensor;\r\n            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*n}), &keys_tensor));\r\n            \r\n            Tensor vals_tensor;\r\n            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*n}), &vals_tensor));\r\n            \r\n            Tensor vals_multivalue_tensor;\r\n            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*m*27*2}), &vals_multivalue_tensor));\r\n            \r\n            Tensor queries_tensor;\r\n            OP_REQUIRES_OK(context, context->allocate_temp(DT_INT32, TensorShape({b*m*27}), &queries_tensor));\r\n            \r\n\r\n            auto all_xyz_flat = all_xyz_tensor.flat<float>();\r\n            const float *all_xyz = &(all_xyz_flat(0));\r\n            \r\n            auto centroids_xyz_flat = centroids_xyz_tensor.flat<float>();\r\n            const float *centroids_xyz = &(centroids_xyz_flat(0));\r\n            \r\n            auto limits_flat = limits_tensor.flat<float>();\r\n            const float *limits = &(limits_flat(0));\r\n            \r\n            auto sizes_flat = sizes_tensor.flat<int>();\r\n            const int *sizes = &(sizes_flat(0));\r\n            \r\n            auto idx_flat = idx_tensor->flat<int>();\r\n            int *idx = &(idx_flat(0));\r\n            auto pts_cnt_flat = pts_cnt_tensor->flat<int>();\r\n            int *pts_cnt = &(pts_cnt_flat(0));\r\n            \r\n            auto keys_flat = keys_tensor.flat<int>();\r\n            unsigned int *keys = (unsigned int *)&(keys_flat(0));\r\n            \r\n            auto vals_flat = vals_tensor.flat<int>();\r\n            unsigned int *vals = (unsigned int *)&(vals_flat(0));\r\n            \r\n            auto queries_flat = queries_tensor.flat<int>();\r\n            unsigned int *queries = (unsigned int *)&(queries_flat(0));\r\n            \r\n            auto vals_multivalue_flat = vals_multivalue_tensor.flat<int>();\r\n            uint2 *vals_multivalue = reinterpret_cast<uint2*> (&(vals_multivalue_flat(0)));\r\n            \r\n            printf(\"Before launcher in cpp\\n\");\r\n            \r\n            querySquarePointLauncher(b, n, m, grid_size_, nsample_, all_xyz, centroids_xyz, limits, sizes, idx, pts_cnt, keys, vals, queries, vals_multivalue);         \r\n        }\r\n    private:\r\n        float grid_size_;\r\n        int nsample_;\r\n};\r\nREGISTER_KERNEL_BUILDER(Name(\"QuerySquarePoint\").Device(DEVICE_GPU), QuerySquarePointGpuOp);\r\n```\r\n\r\nThe CUDA implementation:\r\n```C++\r\n__global__ void compose_insert_items(int b, int n, float grid_size, const float *all_xyz, const float *limits, const int *sizes, unsigned int *d_keys, unsigned int *d_vals){\r\n    int index = threadIdx.x;\r\n   \r\n    if(index < n){\r\n        int batch_index = blockIdx.x;\r\n        all_xyz += batch_index * n * 3;\r\n        unsigned int *tmp_d_keys = d_keys + batch_index * n;\r\n        unsigned int *tmp_d_vals = d_vals + batch_index * n;\r\n        int stride = blockDim.x;\r\n        \r\n        for(int point_idx = index; point_idx < n; point_idx += stride){\r\n            unsigned int x_idx = __float2uint_rd((all_xyz[point_idx*3] - limits[0]) / grid_size) + 1;\r\n            unsigned int y_idx = __float2uint_rd((all_xyz[point_idx*3+1] - limits[2]) / grid_size) + 1;\r\n            unsigned int z_idx = __float2uint_rd((all_xyz[point_idx*3+2] - limits[4]) / grid_size) + 1;\r\n            \r\n            tmp_d_keys[point_idx] = z_idx + sizes[2] * (y_idx + sizes[1] * (x_idx + batch_index * sizes[0]));\r\n            tmp_d_vals[point_idx] = point_idx;\r\n        }\r\n    }\r\n}\r\n//compose_queries<<<b,256>>>(b, m, grid_size, centroids_xyz, limits, sizes, d_queries);\r\n__global__ void compose_queries(int b, int m, float grid_size, const float *centroids_xyz, const float *limits, const int *sizes, unsigned int *d_queries){\r\n\r\n    int index = threadIdx.x;\r\n    \r\n    if(index < m){\r\n        int stride = blockDim.x;\r\n        int batch_index = blockIdx.x;\r\n        centroids_xyz += batch_index * m * 3;\r\n        unsigned int *tmp_d_queries = d_queries + batch_index * m * 27;\r\n        \r\n        unsigned int x_idx = __float2uint_rd((centroids_xyz[index*3] - limits[0]) / grid_size);\r\n        unsigned int y_idx = __float2uint_rd((centroids_xyz[index*3+1] - limits[2]) / grid_size);\r\n        unsigned int z_idx = __float2uint_rd((centroids_xyz[index*3+2] - limits[4]) / grid_size);\r\n        \r\n        int cnt = 0;\r\n        for(int x_offset = 0; x_offset < 3; x_offset++){\r\n            for(int y_offset = 0; y_offset < 3; y_offset++){\r\n                for(int z_offset = 0; z_offset < 3; z_offset++){\r\n                    tmp_d_queries[index*27+cnt] = z_idx + z_offset + sizes[2] * (y_idx + y_offset + sizes[1] * (x_idx + x_offset + batch_index * sizes[0]));  \r\n                    cnt++;\r\n                }\r\n            }\r\n        }\r\n\r\n    }\r\n}\r\n__global__ void hash_square_idx_gpu(int b, int n, int m, int nsample, const uint2 *d_vals_multivalue, const unsigned int * d_all_values, int *idx, int *pts_cnt){\r\n    int index = threadIdx.x;\r\n    if(index < m){\r\n        int stride = blockDim.x;\r\n        int batch_index = blockIdx.x;\r\n        unsigned int sorted_idx[27] = {13, 4,10,12,14,16,22, 1,3,5,7,9,11,15,17,19,21,23,25,  0,2,6,8,18,20,24,26};\r\n                \r\n        idx += batch_index * m * nsample;\r\n        pts_cnt += batch_index * m;\r\n        int query_idx_base = batch_index*m*27+index*27;\r\n        \r\n        int cnt = 0;\r\n        for(int i = 0; i < 27; i++){\r\n            int query_idx = query_idx_base + sorted_idx[i];\r\n            unsigned int num_values = d_vals_multivalue[query_idx].y;\r\n            if(num_values > 0){\r\n                for(unsigned int j = 0; j < num_values && cnt < nsample; j++){\r\n                    idx[index*nsample + cnt] = d_all_values[d_vals_multivalue[query_idx].x + j];\r\n                    cnt++;\r\n                }\r\n            }\r\n        }\r\n        pts_cnt[index] = cnt;\r\n        for(;cnt < nsample;cnt++){\r\n            idx[index*nsample + cnt] = idx[index*nsample];\r\n        }\r\n    }\r\n}\r\n\r\nvoid querySquarePointLauncher(int b, int n, int m, float grid_size, int nsample, const float *all_xyz, const float *centroids_xyz, const float *limits, const int *sizes, int *idx, int *pts_cnt, unsigned int *d_keys, unsigned int *d_vals, unsigned int *d_queries, uint2 *d_vals_multivalue) {\r\n    printf(\"Start\\n\");    \r\n    unsigned int kInputSize = b * n;\r\n    printf(\"b %d, n %d, kInputSize: %u\\n\", b, n, kInputSize);\r\n    \r\n    compose_insert_items<<<b,256>>>(b, n, grid_size, all_xyz, limits, sizes, d_keys, d_vals);\r\n    cudaDeviceSynchronize();\r\n    \r\n    CUDPPHandle theCudpp;\r\n    CUDPPResult result = cudppCreate(&theCudpp);\r\n    if (result != CUDPP_SUCCESS){\r\n        fprintf(stderr, \"Error initializing CUDPP Library.\\n\");\r\n        exit(-1);\r\n    }\r\n\r\n    CUDPPHashTableConfig config;\r\n    config.type = CUDPP_MULTIVALUE_HASH_TABLE;\r\n    config.kInputSize = kInputSize;\r\n    config.space_usage = 2.0f;\r\n    CUDPPHandle hash_table_handle;\r\n    result = cudppHashTable(theCudpp, &hash_table_handle, &config);\r\n    if (result != CUDPP_SUCCESS){\r\n        fprintf(stderr, \"Error in cudppHashTable call in\"\r\n                \"testHashTable (make sure your device is at\"\r\n                \"least compute version 2.0\\n\");\r\n    }\r\n    \r\n    result = cudppHashInsert(hash_table_handle, d_keys,\r\n                                d_vals, kInputSize);\r\n    cudaThreadSynchronize();\r\n    printf(\"insert values\\n\");\r\n    if (result != CUDPP_SUCCESS){\r\n        fprintf(stderr, \"Error in cudppHashInsert call in\"\r\n                \"testHashTable\\n\");\r\n    }\r\n    \r\n    unsigned int values_size;\r\n    if (cudppMultivalueHashGetValuesSize(hash_table_handle,\r\n                                    &values_size) !=\r\n                                    CUDPP_SUCCESS){\r\n        fprintf(stderr, \"Error: \"\r\n                \"cudppMultivalueHashGetValuesSize()\\n\");\r\n    }\r\n    \r\n    unsigned int * d_all_values = NULL;\r\n    if (cudppMultivalueHashGetAllValues(hash_table_handle,\r\n                                        &d_all_values) !=\r\n                                        CUDPP_SUCCESS){\r\n        fprintf(stderr, \"Error: \"\r\n                \"cudppMultivalueHashGetAllValues()\\n\");\r\n    }\r\n    \r\n    compose_queries<<<b,256>>>(b, m, grid_size, centroids_xyz, limits, sizes, d_queries);\r\n    cudaDeviceSynchronize();\r\n    \r\n    result = cudppHashRetrieve(hash_table_handle,\r\n                                d_queries,\r\n                                d_vals_multivalue,\r\n                                b * m * 27);\r\n    cudaThreadSynchronize();\r\n    printf(\"retrieved values\\n\");\r\n    if (result != CUDPP_SUCCESS){\r\n        fprintf(stderr, \"Error in cudppHashRetrieve call\\n\");\r\n    }\r\n\r\n    hash_square_idx_gpu<<<b,256>>>(b, n, m, nsample, d_vals_multivalue, d_all_values, idx, pts_cnt);\r\n    cudaDeviceSynchronize();\r\n    printf(\"obtain idx\\n\");\r\n    \r\n    result = cudppDestroyHashTable(theCudpp, hash_table_handle);\r\n    if (result != CUDPP_SUCCESS){\r\n        fprintf(stderr, \"Error in cudppDestroyHashTable call in\"\r\n                \"testHashTable\\n\");\r\n    }\r\n\r\n    result = cudppDestroy(theCudpp);\r\n    if (result != CUDPP_SUCCESS){\r\n        printf(\"Error shutting down CUDPP Library.\\n\");\r\n    }\r\n    printf(\"Ends\\n\");\r\n}\r\n```\r\n\r\n\r\n", "comments": ["There are some technical reasons why it's hard to implement, ie https://github.com/tensorflow/tensorflow/issues/2210#issuecomment-224371844", "@yaroslavvb Thanks, but #2210 wants to make both cuda code and TensorFlow run on the same GPU and battle for the memory.\r\n\r\nMine problem is in the customized TensorFlow op's CUDA implementation, almost everything works, except that TensorFlow does not allow me or the third library to do cudaMalloc, cudaMemcpy, etc.\r\n\r\nSee https://stackoverflow.com/questions/40183189/trouble-compiling-with-custom-tensorflow-gpu-op. However, it's very hard to let the third party library to use OpKernelContext::allocate_temp(). Since large third party libraries have complicated implementation, for example, CUDPP is implemented as 4 layers.", "@zheng-xq -- can you see any way to have CUDPP work with tensorflow (since it does its own cudaMalloc)", "I would like to add some observations:\r\n\r\nIf plug in my customized op into a large scale deep neural network, I will get the following error message (I do not do any CUDA memory manipulation, but CUDPP does):\r\n```\r\n2017-10-10 02:04:52.138035: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ER\r\nROR_ILLEGAL_ADDRESS\r\n2017-10-10 02:04:52.138076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n```\r\n\r\nIf I write some CudaMemcpy statements in the kernel implementation, the error I receive is \"invalid argument\" (for the CudaMemcpy function call).\r\n\r\nBut I also use the following script to test my customized op, it's really strange that it works well even if I use many GPU memory. (`query_square_point` is my customized op implemented on GPU)\r\n\r\n```python\r\n#!/usr/bin/python\r\nimport os, sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\r\nsys.path.append(os.path.join(BASE_DIR, 'tf_ops/grouping'))\r\n\r\nfrom tf_grouping import query_square_point\r\n\r\ndef gather_point(features, idx):\r\n    '''\r\n    input:\r\n            batch_size * ndataset * x   float32\r\n            batch_size * npoints        int32\r\n    returns:\r\n            batch_size * npoints * x    float32\r\n    '''\r\n    features_shape = features.get_shape()\r\n    batch_size = features_shape[0].value\r\n    ndatasets = features_shape[1].value\r\n    nfeatures = features_shape[2].value\r\n    npoint = idx.get_shape()[1].value\r\n    \r\n    offsets = tf.tile(tf.range(batch_size)[:, None], [1, npoint]) * ndatasets + idx\r\n    offsets = tf.reshape(offsets, [-1])\r\n    reshaped_features = tf.reshape(features, [-1, nfeatures])\r\n    new_features = tf.reshape(tf.gather(reshaped_features, offsets), [batch_size, npoint, -1])  \r\n    \r\n    return new_features\r\n\r\ndef query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz):\r\n    '''\r\n    Input:\r\n        grid_size: float32, ball search radius / 3.0\r\n        nsample: int32, number of points selected in each ball region\r\n        xyz: (batch_size, ndataset, 3) float32 array, input points\r\n        centroids_xyz: (batch_size, npoint, 3) float32 array, query points\r\n    Output:\r\n        idx: (batch_size, npoint, nsample) int32 array, indices to input points\r\n        pts_cnt: (batch_size, npoint) int32 array, number of unique points in each local region\r\n    '''\r\n    batch_size = centroids_xyz.get_shape()[0].value\r\n    npoint = centroids_xyz.get_shape()[1].value\r\n    ndataset = xyz.get_shape()[1].value\r\n    \r\n    min_x = tf.reduce_min(xyz[:, :, 0])\r\n    max_x = tf.reduce_max(xyz[:, :, 0])\r\n    min_y = tf.reduce_min(xyz[:, :, 1])\r\n    max_y = tf.reduce_max(xyz[:, :, 1])\r\n    min_z = tf.reduce_min(xyz[:, :, 2])\r\n    max_z = tf.reduce_max(xyz[:, :, 2])\r\n    limits = tf.stack([min_x, max_x, min_y, max_y, min_z, max_z])\r\n        \r\n    x_size = tf.cast(tf.ceil((max_x - min_x) / grid_size), tf.int32) + 1\r\n    y_size = tf.cast(tf.ceil((max_y - min_y) / grid_size), tf.int32) + 1\r\n    z_size = tf.cast(tf.ceil((max_z - min_z) / grid_size), tf.int32) + 1\r\n    sizes = tf.stack([x_size, y_size, z_size])\r\n    \r\n    idx, pts_cnt = query_square_point(grid_size, nsample, xyz, centroids_xyz, limits, sizes)\r\n    \r\n    return idx, pts_cnt\r\n\r\nchoice = sys.argv[2]\r\nradius = 0.4\r\ngrid_size = radius * 2.0 / 3.0\r\nnsample = 64\r\nbatch_size = 32\r\nnpoint = 512\r\nndataset = 1024\r\n\r\nxyz = tf.random_uniform(shape=[batch_size, ndataset, 3], minval=-1.0, maxval=1.0, dtype=tf.float32, name=\"xyz\")\r\ncentroids_idx = tf.constant(np.random.choice(ndataset, [batch_size, npoint]), dtype=tf.int32, name=\"centroids_idx\")\r\ncentroids_xyz = gather_point(xyz, centroids_idx)\r\na, b = query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nconfig.log_device_placement = False\r\nsess = tf.Session(config=config)\r\nsess.run(xyz)\r\n\r\nstart = time.time()\r\nsess.run(a)\r\nprint 'first epoch', time.time() - start\r\n\r\nstart = time.time()\r\nfor i in xrange(1000):\r\n\r\n    sess.run(a)\r\nprint 'next 1000 epoch', time.time() - start\r\n```", "CUDPP is not designed with flexibility from a memory allocation perspective.  It uses cudaMalloc calls internally and provides no mechanism for a user provided allocator (like thrust) or for the user to do the memory allocation and then pass in pointers (like CUB).\r\n\r\nI think CUDPP is itself basically deprecated at this point.  Most of things it does are done better by libraries like CUB and thrust now.  With the exception of hash tables, which is what you want :)\r\n\r\nI think your best option to remove the code that does whatever you need from CUDPP put it into your Op directly and then you can allocate the memory with the context and pass the pointers to the kernels.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@laoreja what do you think about @ekelsen's advice?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 112 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "> I would like to add some observations:\r\n> \r\n> If plug in my customized op into a large scale deep neural network, I will get the following error message (I do not do any CUDA memory manipulation, but CUDPP does):\r\n> \r\n> ```\r\n> 2017-10-10 02:04:52.138035: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ER\r\n> ROR_ILLEGAL_ADDRESS\r\n> 2017-10-10 02:04:52.138076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n> ```\r\n> \r\n> If I write some CudaMemcpy statements in the kernel implementation, the error I receive is \"invalid argument\" (for the CudaMemcpy function call).\r\n> \r\n> But I also use the following script to test my customized op, it's really strange that it works well even if I use many GPU memory. (`query_square_point` is my customized op implemented on GPU)\r\n> \r\n> ```python\r\n> #!/usr/bin/python\r\n> import os, sys\r\n> import tensorflow as tf\r\n> import numpy as np\r\n> import time\r\n> BASE_DIR = os.path.dirname(os.path.abspath(__file__))\r\n> sys.path.append(os.path.join(BASE_DIR, 'tf_ops/grouping'))\r\n> \r\n> from tf_grouping import query_square_point\r\n> \r\n> def gather_point(features, idx):\r\n>     '''\r\n>     input:\r\n>             batch_size * ndataset * x   float32\r\n>             batch_size * npoints        int32\r\n>     returns:\r\n>             batch_size * npoints * x    float32\r\n>     '''\r\n>     features_shape = features.get_shape()\r\n>     batch_size = features_shape[0].value\r\n>     ndatasets = features_shape[1].value\r\n>     nfeatures = features_shape[2].value\r\n>     npoint = idx.get_shape()[1].value\r\n>     \r\n>     offsets = tf.tile(tf.range(batch_size)[:, None], [1, npoint]) * ndatasets + idx\r\n>     offsets = tf.reshape(offsets, [-1])\r\n>     reshaped_features = tf.reshape(features, [-1, nfeatures])\r\n>     new_features = tf.reshape(tf.gather(reshaped_features, offsets), [batch_size, npoint, -1])  \r\n>     \r\n>     return new_features\r\n> \r\n> def query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz):\r\n>     '''\r\n>     Input:\r\n>         grid_size: float32, ball search radius / 3.0\r\n>         nsample: int32, number of points selected in each ball region\r\n>         xyz: (batch_size, ndataset, 3) float32 array, input points\r\n>         centroids_xyz: (batch_size, npoint, 3) float32 array, query points\r\n>     Output:\r\n>         idx: (batch_size, npoint, nsample) int32 array, indices to input points\r\n>         pts_cnt: (batch_size, npoint) int32 array, number of unique points in each local region\r\n>     '''\r\n>     batch_size = centroids_xyz.get_shape()[0].value\r\n>     npoint = centroids_xyz.get_shape()[1].value\r\n>     ndataset = xyz.get_shape()[1].value\r\n>     \r\n>     min_x = tf.reduce_min(xyz[:, :, 0])\r\n>     max_x = tf.reduce_max(xyz[:, :, 0])\r\n>     min_y = tf.reduce_min(xyz[:, :, 1])\r\n>     max_y = tf.reduce_max(xyz[:, :, 1])\r\n>     min_z = tf.reduce_min(xyz[:, :, 2])\r\n>     max_z = tf.reduce_max(xyz[:, :, 2])\r\n>     limits = tf.stack([min_x, max_x, min_y, max_y, min_z, max_z])\r\n>         \r\n>     x_size = tf.cast(tf.ceil((max_x - min_x) / grid_size), tf.int32) + 1\r\n>     y_size = tf.cast(tf.ceil((max_y - min_y) / grid_size), tf.int32) + 1\r\n>     z_size = tf.cast(tf.ceil((max_z - min_z) / grid_size), tf.int32) + 1\r\n>     sizes = tf.stack([x_size, y_size, z_size])\r\n>     \r\n>     idx, pts_cnt = query_square_point(grid_size, nsample, xyz, centroids_xyz, limits, sizes)\r\n>     \r\n>     return idx, pts_cnt\r\n> \r\n> choice = sys.argv[2]\r\n> radius = 0.4\r\n> grid_size = radius * 2.0 / 3.0\r\n> nsample = 64\r\n> batch_size = 32\r\n> npoint = 512\r\n> ndataset = 1024\r\n> \r\n> xyz = tf.random_uniform(shape=[batch_size, ndataset, 3], minval=-1.0, maxval=1.0, dtype=tf.float32, name=\"xyz\")\r\n> centroids_idx = tf.constant(np.random.choice(ndataset, [batch_size, npoint]), dtype=tf.int32, name=\"centroids_idx\")\r\n> centroids_xyz = gather_point(xyz, centroids_idx)\r\n> a, b = query_square_point_wrapper(grid_size, nsample, xyz, centroids_xyz)\r\n> \r\n> config = tf.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> config.log_device_placement = False\r\n> sess = tf.Session(config=config)\r\n> sess.run(xyz)\r\n> \r\n> start = time.time()\r\n> sess.run(a)\r\n> print 'first epoch', time.time() - start\r\n> \r\n> start = time.time()\r\n> for i in xrange(1000):\r\n> \r\n>     sess.run(a)\r\n> print 'next 1000 epoch', time.time() - start\r\n> ```\r\n\r\nEcho with what your results, the custom ops would not behave in a large network, when manipulate the GPU memory with native APIs.\r\nMy code doesn't have any `Memcpy`, instead it has `memset` in it, also giving me\r\n```\r\n2017-10-10 02:04:52.138035: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ER\r\nROR_ILLEGAL_ADDRESS\r\n2017-10-10 02:04:52.138076: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n```"]}, {"number": 13498, "title": "Problem with \"AddControlInput\" in python_api.h", "body": "@asimshankar @skye I'm observing an issue with `AddControlInput` in `python_api.h`. It seems to not be working for me actually and I'm not sure why. Everything is ok if I add the control dependency during op creation. However, if I add it right after, it's not enforced during execution. A simple example is creating a `switch` op, creating two constant ops and adding one control dependency for each on each switch output, and then feeding these two constant ops into a `merge`op. The result will be whichever constant op was fed first into the merge op (i.e., the control dependencies are not satisfied and both ops are executed). If I add the control dependency during op construction, all is good.\r\n\r\nNote that the control input does show up in the `GraphDef` that I generated after the call. It's simply not enforced during execution.\r\n\r\nDo you have any idea why that might be happening? I am aware that this is not part of the public API and thus is not stable, but it is still unexpected behavior. I am using that in my implementation of control flow ops and gradients (I ended up re-implementing it based on the Python version because I depend on it and the C++ functionality is currently very limited -- I'll update on this later on, once I release it).", "comments": ["FYI, if you want to replicate that behavior using the Python API, you can do the following:\r\n\r\n1. Set `_USE_C_API` in `python/framework/ops.py` to `True`.\r\n2. Move this (in `python/framework/ops.py`):\r\n    ```python\r\n    # Add this op to the current control flow context:\r\n    self._control_flow_context = g._get_control_flow_context()  # pylint: disable=protected-access\r\n    if self._control_flow_context is not None:\r\n      # TODO(skyewm): consider refactoring this to call self._create_c_op()\r\n      # first. This would require updating the TF_Operation's ID (see the\r\n      # comment and self._id_value update below). The disadvantage of calling\r\n      # AddOp() first is that we need to maintain Operation state that is\r\n      # accessed by AddOp() in Python, e.g. the input Tensors.\r\n      self._control_flow_context.AddOp(self)\r\n    ```\r\n    after the `_c_op` initialization.\r\n3. Try running the following simple script:\r\n    ```python\r\n    p = tf.constant(True)\r\n    t = lambda: tf.constant(True)\r\n    f = lambda: tf.constant(False)\r\n    r = tf.cond(p, t, f)\r\n    print(r.eval(session=tf.Session()))\r\n    ```\r\n\r\nThis will print `False`, even though it should be printing `True`. The reason is that the false branch output is fed into the merge op first.\r\n\r\nAlso CCing @itsmeolivia since I think she's been working on making the Python API to use the C API.\r\n", "FYI, if I use `AddControlInput` from `python_api.h` and then export to `GraphDef` and create a new graph from that `GraphDef`, the control dependencies work fine. This makes me think that something needs to be added after the call to `Graph::AddControlEdge`, but I'm not sure what yet.", "I think this might have to do with the `NodeDef` stored within each node not being updated when `AddControlInput` is called. If that's the case, then I wonder, why does the executor use that serialized version of the node, even when it may be outdated?", "Hi Anthony. Good find, and apologies for this confusing behavior. I found this too, and have an internal patch in review that fixes this. I'll update this issue when it's committed. You're correct that it's due to the NodeDef not being updated. Re: your question about the executor, I don't know the exact answer off the top of my head, but in general it's somewhat complicated and brittle, but if you're careful it works (part of this also depends on the graph being de/serialized before it's run).\r\n\r\nAlso, why are you adding control inputs after the fact? It's supposed to be for the Python API only :)", "Hey @skye! :) That's great! Regarding the executor, I feel that, it should at least guarantee that the graph will not be serialized before session creation. In the case of nodes and `NodeDef`s though, I noticed that the node def is finalized when the node builder's finalize method is called and is probably used as is by the executor after that. I feel that this shouldn't really be happening. In either case, these are all edge cases I guess that are not even supported by the public API and so I understand that they are not a priority.\r\n\r\nBy the way, it might be worth also looking into the `ClearControlInputs` method because there will probably also be an issue there (although I haven't tested that).\r\n\r\nAnd yeah so, I was going to email you about that. I ended up implementing all of the control flow functionality (including conds and while loops) in the Scala side. There are a couple reasons for that:\r\n- The C++ functionality will probably take a lot of time to come to a point where I can use it. I need to be able to provide implementation of unsupported gradients from the Scala side and I also need support for indexed slices and sparse tensors. Given that I already support all these constructs on my Scala API, it felt easier to just go with that.\r\n- I've been needing that for my research and so this saves me lots of time.\r\n- Also, I already support stacks (i.e., nested loops and combinations of conds and loops).\r\nI've followed the Python implementation, while simplifying it wherever I could and re-structuring it a bit. The document helped a lot by the way, in getting a grasp of what's going on. :)\r\n\r\nI will push my implementation soon and link to it here if you want to use it and talk about it to help add support for nesting/stacks, etc. on the C++ side.\r\n\r\nAs a side note, there are a couple of things I don't really like in how control flow support is currently implemented and if there is indeed a discussion going on about that, it may be worth mentioning them. I guess an important starting point would be to clear up what's happening with control dependencies when nesting control flow constructs. Currently, it's a bit too complicated for no good reason, I think.", "Oh wow, yes I would love to see your code for control flow gradients.\r\n\r\nI haven't looked in detail at the control deps for nested control flow yet, but I'd be interested to hear how you think it can be simplified.", "I'll push once I have the tests working. :) When do you think your fix will be pushed to the public repo? Would it be too much to ask if you could provide me with a working draft so I can work on my tests until then?", "Not sure when it will get in, I'll ping my reviewers :) It's not complicated so shouldn't be long, like 1-3 days probably. Lemme get you a working draft.", "Thanks! :)", "Here's the change thus far: https://github.com/skye/tensorflow/commit/5c989fab9cd8f31e3f2fb84db95119ca15079ef1\r\n\r\nI don't actually execute a graph with a control edge in the new tests, I should add that. Lemme know if you run into any problems.", "Thanks! :) I'm recompiling now and I'll let you know what happens. Could you add the same option for `RemoveEdge` so that `ClearControlInputs` works as expected? I can also make a pull request if that helps.\r\n\r\nMore generally, is there any reason why you would not want to update the `NodeDef`? As far as I can see, not updating it creates an inconsistent state that can only be confusing. Although, I don't know what implications updating it might have if other parts of the library expect it to be immutable.", "Feel free to make a PR, although this patch might change and force you to change things (e.g. new interface or something). If you don't get around to it I can do it once this is committed.\r\n\r\nAs you can see in the patch, the default is to update the NodeDef :) I think most things don't look at the NodeDef at all so it doesn't make a difference (they access the Node's Edges instead), even though updating the NodeDef is technically correct. However, there are a few cases where updating the NodeDef breaks things, e.g., during graph construction, which creates the correct NodeDef and then manually creates all the Edges to reflect what's in the NodeDef.", "Ok, in that case I'll wait a bit and only make a PR if this seems to take long.\r\n\r\nThat comment about graph construction makes sense and I hadn't noticed it. Thanks! :)", "I ended up submitting #13505 :P.", "The fix works by the way and I can now run my tests. I will update you once I push my implementation.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 13497, "title": "Fix documentation error in tf.reverse docstring (#1)", "body": "The first example in the tf.reverse docstring causes a ValueError, as shown below.  `tf.reverse` requires `axis` to be 1D; -1 is not 1D, it's 0D; it should be [-1].\r\n\r\n```Python\r\nIn [1]: import tensorflow as tf\r\nIn [2]: t = tf.constant([[[[ 0,  1,  2,  3], [ 4,  5,  6,  7], [ 8,  9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]])\r\nIn [3]: dims = -1\r\nIn [4]: sess = tf.InteractiveSession()\r\nIn [5]: tf.reverse(t, dims).eval()\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-7752813cc8a9> in <module>()\r\n----> 1 tf.reverse(t, dims).eval()\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc in reverse(tensor, axis, name)\r\n   2332\r\n   2333 def reverse(tensor, axis, name=None):\r\n-> 2334   return gen_array_ops.reverse_v2(tensor, axis, name)\r\n   2335 reverse.__doc__ = gen_array_ops.reverse_v2.__doc__\r\n   2336\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc in reverse_v2(tensor, axis, name)\r\n   2697   \"\"\"\r\n   2698   result = _op_def_lib.apply_op(\"ReverseV2\", tensor=tensor, axis=axis,\r\n-> 2699                                 name=name)\r\n   2700   return result\r\n   2701\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\r\n    765         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    766                          input_types=input_types, attrs=attr_protos,\r\n--> 767                          op_def=op_def)\r\n    768         if output_structure:\r\n    769           outputs = op.outputs\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   2506                     original_op=self._default_original_op, op_def=op_def)\r\n   2507     if compute_shapes:\r\n-> 2508       set_shapes_for_outputs(ret)\r\n   2509     self._add_op(ret)\r\n   2510     self._record_op_seen_by_control_dependencies(ret)\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\r\n   1871       shape_func = _call_cpp_shape_fn_and_require_op\r\n   1872\r\n-> 1873   shapes = shape_func(op)\r\n   1874   if shapes is None:\r\n   1875     raise RuntimeError(\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in call_with_requiring(op)\r\n   1821\r\n   1822   def call_with_requiring(op):\r\n-> 1823     return call_cpp_shape_fn(op, require_shape_fn=True)\r\n   1824\r\n   1825   _call_cpp_shape_fn_and_require_op = call_with_requiring\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc in call_cpp_shape_fn(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\r\n    608     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\r\n    609                                   input_tensors_as_shapes_needed,\r\n--> 610                                   debug_python_shape_fn, require_shape_fn)\r\n    611     if not isinstance(res, dict):\r\n    612       # Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\r\n\r\n/Users/craffel/.pyenv/versions/2.7.13/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\r\n    674       missing_shape_fn = True\r\n    675     else:\r\n--> 676       raise ValueError(err.message)\r\n    677\r\n    678   if missing_shape_fn:\r\n\r\nValueError: Shape must be rank 1 but is rank 0 for 'ReverseV2' (op: 'ReverseV2') with input shapes: [1,2,3,4], [].\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n\r\nDo I need to sign a CLA if I'm a Google employee?", "Jenkins, test this please.", "I signed it!"]}, {"number": 13496, "title": "Add tf.sysconfig.get_compile_flags() & tf.sysconfig.get_link_flags() for custom operators", "body": "The goal is to make custom operators compilation a breeze.\r\n\r\n```\r\nIn [1]: import tensorflow as tf\r\nCouldn't import dot_parser, loading of dot files will not be possible.\r\n\r\nIn [2]: tf.sysconfig.get_compile_flags()\r\nOut[2]:\r\n['-I/home/devadmin/env/lib/python2.7/site-packages/tensorflow/include',\r\n '-I/home/devadmin/env/lib/python2.7/site-packages/tensorflow/include/external/nsync/public',\r\n '-D_GLIBCXX_USE_CXX11_ABI=1']\r\n\r\nIn [3]: tf.sysconfig.get_link_flags()\r\nOut[3]:\r\n['-L/home/devadmin/env/lib/python2.7/site-packages/tensorflow',\r\n '-ltensorflow_framework']\r\n```", "comments": ["Can one of the admins verify this patch?", "Please advise if `version_info.cc` is a good place to probe `_GLIBCXX_USE_CXX11_ABI`, whether this level of exposure is OK and what's the best place to add tests.", "Jenkins, test this please.", "Looks like test failure caught new functions in `tf.sysconfig`, which is expected.\r\n\r\n```\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n\r\nERROR:tensorflow:2 differences found between API and golden.\r\nERROR:tensorflow:Issue 1\t: Change detected in python object: tensorflow.\r\nERROR:tensorflow:Issue 2\t: Change detected in python object: tensorflow.sysconfig.\r\n```", "+ @jhseu ", "Thanks for the feedback, @drpngx.  Updated.", "Jenkins, test this please.", "Okay for API change, @drpngx : deferring to you to complete the review and approve.", "/CC @gunan @jart FYI. This is like `pkgconfig`. While it will be hard to maintain, I think it's still useful. Normally we might have a standalone tool, but it make the packaging harder.\r\n\r\n", "Jenkins, test this please.", "AFAICR, @m3bm3b had a plan to create a makefile for custom ops, which we would package with the pip package.", "@gunan would it work if the custom op has dependencies on other libraries, such as NCCL, CUDA or MPI? MPI typically comes with \"compilation flags\" and \"linking flags\". Also, it is more challenging to run `make` than `gcc`/`clang` from `pip install`.", "You are right, the custom dependencies for custom ops can be confusing.\r\nI think ultimately, we would move to distributing TF using RPM/DEB packages and also provide tools maybe not makefile but some tools to build custom ops easier.\r\nBut until then, this may work.\r\n\r\nIts existence in the core API, I dont know.\r\nIll let API owners to decide on that.\r\n", "@drpngx, gentle ping.", "Sorry about the delay.", "@drpngx, thanks!", "Did the tests pass?  I see two failures:\r\n1. `//tensorflow/tools/api/tests:api_compatibility_test` failed, which is expected.\r\n2. `//tensorflow/contrib/data/python/kernel_tests:sloppy_transformation_dataset_op_test` failed, and I can't see any logs - I don't believe it could be related to this change.", "You need to update goldens and package them in this Pull Request.\r\nCurrently api compatibility test only checks the API status in python2 and linux setup, you may use docker to run api compatibility test and generate new goldens.\r\n\r\nThe other test failure should go away if you rebase your change on top of master.", "@gunan, got you. Updated.", "Jenkins, test this please.", "@drpngx, @gunan - thanks!", "Next step is to update master version of https://www.tensorflow.org/versions/r1.4/extend/adding_an_op#build_the_op_library.  Is it hosted in a public repository?  If so, I could help to do the change.", "Sorry guys, have to follow this up with a bugfix #14159.\r\n\r\nAfter I tested this nightly build with gcc4 & 5, I realized that gcc4 -> gcc5 always requires ABI flag to be present with 0 value, even though it was undefined for gcc4.\r\n\r\nThis somewhat echoes @drpngx question whether we could have the default value of `_GLIBCXX_USE_CXX11_ABI` be 0, but we need to add that flag to complication flags even if it is 0."]}, {"number": 13495, "title": "Remove unneeded branch check", "body": "", "comments": ["Can one of the admins verify this patch?", "fixes #6911 ", "Jenkins, test this please.", "Is there a reason this can't be merged, or has it just slipped through the net? It's a pretty annoying problem.", "ping @aselle ", "I haven't really delved into the whole Bazel stamping thing yet, but shouldn't the git hash be a dependency of only a small part of the whole project? E.g. to rename the final artefacts or maybe to put in a `version.h` header that isn't `#include`d anywhere. I don't see why the entire project would need to depend on the git hash.", "@Timmmm updating GIT_VERSION just needs relinking pywrap_tensorflow.so which is fast, however, it's implemented through ./configure which touches touches a bunch of dependencies as a by-product, so building becomes slow.\r\n\r\n@aselle PTAL", "So, I had a look, and as far as I understand the process, here is how it currently works:\r\n\r\nIn `tensorflow/tools/git/gen_git_source.py` there is a function `generate()` which writes `version_info.cc` containing this function: `const char* tf_git_version() {return \"%s\";}` where `%s` is the result of `git describe --long --tags`, e.g. `v1.3.0-rc1-4901-gd4c5301d52`. `generate()` is called when you run `python gen_git_source.py --generate`.\r\n\r\nThis is called by a [genrule](https://docs.bazel.build/versions/master/be/general.html#genrule) defined by `tf_version_info_genrule()` in `tensorflow.bzl` which is called in `core/BUILD`. The genrule has these sources:\r\n\r\n```\r\n      srcs=[\r\n          clean_dep(\"//tensorflow/tools/git:gen/spec.json\"),\r\n          clean_dep(\"//tensorflow/tools/git:gen/head\"),\r\n          clean_dep(\"//tensorflow/tools/git:gen/branch_ref\"),\r\n      ],\r\n```\r\n\r\n`head` is symlinked to `.git/HEAD` which I guess means it is run every time the head or branch changes. There is a function in `configure.py` that calls `gen_git_source.py --configure` which writes `spec.json` and symlinks `head` and `branch_ref` to `.git/HEAD` and the current branch ref in `.git`. `spec.json` contains a flag to say if `git` is present, the name of the current branch and the full path to the repo.\r\n\r\nSo here's what I think happens:\r\n\r\n1. You `git clone tensorflow`\r\n2. You run `./configure`. This calls `gen_git_source.py --configure` which symlinks `gen/head` to `.git/HEAD`, and generates `spec.json` which contains the current branch name.\r\n3. You run a Bazel command. Some target somewhere depends on `version_info.cc` which is generated by the `tf_version_info_genrule()` genrule. This genrule calls `gen_git_source.py --generate` which checks that the current branch is the same as the one in `spec.json`. If so it writes out `version_info.cc` using `git describe` and everyone is happy.\r\n4. You switch branches. This causes `gen/head` to change.\r\n5. You run a Bazel command. Since `gen/head` changed, `gen_git_source.py --generate` is called which checks the current branch vs `spec.json` and finds that it has changed! It makes you run `./configure` again.\r\n\r\nThis seems silly. I can't even see that the branch is used anywhere! I think a simpler design would be like this:\r\n\r\n1. Remove the current branch from `spec.json`.\r\n2. Remove the branch check from `generate()`.\r\n3. Get rid of `gen/branch_ref`.\r\n3. That's it.\r\n\r\nIf head changes, then `//tensorflow/tools/git:gen/head` should change, and `gen_git_source.py --generate` will be called again by the genrule and write a new `version_info.cc`. Nothing needs to know the branch name.\r\n\r\nBy the way as far as I can tell none of this will work on Windows anyway because it copies files rather than creating symlinks.\r\n\r\nLet me know if I have totally misunderstood things!", "@aselle can you take another look?", "@Timmmm, your analysis is partially true. You need to track both changes of branches and commit changes to that branch. the gen/branch_ref tracks commits to the branch, and the gen/head tracks changes to what the current branch is. So if you get rid of gen/branch_ref, the git sha will not change when you commit a new change.\r\n\r\n---workspace_status_command is the right way to go in any case.\r\n\r\nBut let's put this change in now to remove the annoyance.", "Ah yes good point - I was thinking that `.git/HEAD` contains the hash of HEAD but actually it contains something like `ref: refs/heads/master` and the hash is in `.git/refs/heads/master`.", "@tensorflow-jenkins test this please", "Framework issue on the Sanity check? Retrying.\r\n\r\nJenkins, test this please.", "Py3: flaky test :-(\r\n\r\n```\r\nFAIL: //tensorflow/core/platform/cloud:file_block_cache_test (see /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/testlogs/tensorflow/core/platform/cloud/file_block_cache_test/test.log)\r\nINFO: From Testing //tensorflow/core/platform/cloud:file_block_cache_test:\r\n==================== Test output for //tensorflow/core/platform/cloud:file_block_cache_test:\r\nRunning test tensorflow/core/platform/cloud/file_block_cache_test on GPU 2\r\nRunning main() from test_main.cc\r\n[==========] Running 11 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 11 tests from FileBlockCacheTest\r\n[ RUN      ] FileBlockCacheTest.PassThrough\r\n[       OK ] FileBlockCacheTest.PassThrough (0 ms)\r\n[ RUN      ] FileBlockCacheTest.BlockAlignment\r\n[       OK ] FileBlockCacheTest.BlockAlignment (0 ms)\r\n[ RUN      ] FileBlockCacheTest.CacheHits\r\n[       OK ] FileBlockCacheTest.CacheHits (1 ms)\r\n[ RUN      ] FileBlockCacheTest.OutOfRange\r\n[       OK ] FileBlockCacheTest.OutOfRange (0 ms)\r\n[ RUN      ] FileBlockCacheTest.Inconsistent\r\n[       OK ] FileBlockCacheTest.Inconsistent (0 ms)\r\n[ RUN      ] FileBlockCacheTest.LRU\r\n[       OK ] FileBlockCacheTest.LRU (0 ms)\r\n[ RUN      ] FileBlockCacheTest.MaxStaleness\r\n[       OK ] FileBlockCacheTest.MaxStaleness (0 ms)\r\n[ RUN      ] FileBlockCacheTest.RemoveFile\r\n[       OK ] FileBlockCacheTest.RemoveFile (0 ms)\r\n[ RUN      ] FileBlockCacheTest.Prune\r\n[       OK ] FileBlockCacheTest.Prune (2002 ms)\r\n[ RUN      ] FileBlockCacheTest.ParallelReads\r\n[       OK ] FileBlockCacheTest.ParallelReads (13 ms)\r\n[ RUN      ] FileBlockCacheTest.CoalesceConcurrentReads\r\ntensorflow/core/platform/cloud/file_block_cache_test.cc:490: Failure\r\nValue of: WaitForNotificationWithTimeout(&notification, 10000)\r\n  Actual: false\r\nExpected: true\r\nTimeout waiting for concurrent thread to start.\r\n[  FAILED  ] FileBlockCacheTest.CoalesceConcurrentReads (111 ms)\r\n[----------] 11 tests from FileBlockCacheTest (2128 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 11 tests from 1 test case ran. (2128 ms total)\r\n[  PASSED  ] 10 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] FileBlockCacheTest.CoalesceConcurrentReads\r\n\r\n 1 FAILED TEST\r\n================================================================================\r\n[10,621 / 10,624] 401 / 403 tests, 1 failed; Testing //tensorflow/core:common_runtime_gpu_gpu_bfc_allocator_test; 19s local ... (3 actions running)\r\nINFO: Elapsed time: 1059.053s, Critical Path: 312.29s\r\nINFO: Build completed, 1 test FAILED, 10624 total actions\r\n```\r\n\r\nGPU: test timeout `TIMEOUT: //tensorflow/python/keras:data_utils_test (Summary)`"]}, {"number": 13494, "title": "model_dir keyword argument repeated", "body": "In https://www.tensorflow.org/tutorials/wide#adding_regularization_to_prevent_overfitting, the code snippet repeats the model_dir keyword argument, causing a syntax error if you try to run it (`SyntaxError: keyword argument repeated`).  This removes the second occurrence of the model_dir param.", "comments": ["Can one of the admins verify this patch?", "SUCCESS\n \n", "Jenkins, test this please."]}, {"number": 13493, "title": "Branch 171047652", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13492, "title": "Can't run new ops in new session after sess.run RuntimeError", "body": "Scenario:\r\n1. runtime error during session.run\r\n2. create new session\r\n3. new session fails to evaluate any tensor, giving same runtime error as in step 1.\r\n\r\nThe following example fails to evaluate `tf.constant` with error `tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Diag`\r\n\r\nUsing 12-day old version from this commit: https://github.com/tensorflow/tensorflow/commit/ea94bbe9fa9f9b3d01fb057c02ef7873d76bf09c\r\n\r\n```\r\nimport tensorflow as tf\r\ndef main():\r\n  \r\n  sess = tf.Session()\r\n  with tf.device(\"/gpu:0\"):\r\n    bad_op = tf.diag([1, 1])\r\n    \r\n  print(\"About to run tensor \", bad_op)\r\n  try:\r\n    sess.run(bad_op)\r\n  except:\r\n    print(\"First run failed, trying something else\")\r\n\r\n  sess = tf.Session()\r\n  good_op = tf.constant(1)\r\n  sess.run(good_op)\r\n\r\nmain()\r\n```\r\n\r\nI thought it's something to do with `TF_ExtendGraph` not being called, but manually adding ExtendGraph before second call results in segmentation fault\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\ndef main():\r\n  \r\n  sess = tf.Session()\r\n  with tf.device(\"/gpu:0\"):\r\n    bad_op = tf.diag([1, 1])\r\n    \r\n  print(\"About to run tensor \", bad_op)\r\n  try:\r\n    sess.run(bad_op)\r\n  except:\r\n    print(\"First run failed, trying something else\")\r\n\r\n  sess = tf.Session()\r\n  good_op = tf.constant(1)\r\n  from tensorflow.python import pywrap_tensorflow as tf_session\r\n  from tensorflow.python.framework import errors\r\n  with errors.raise_exception_on_not_ok_status() as status:\r\n    tf_session.TF_ExtendGraph(sess._session, sess.graph.as_graph_def().SerializeToString(), None)\r\n  sess.run(good_op)\r\n\r\nmain()\r\n\r\n```\r\n\r\n  \r\n", "comments": ["Well seeing From Previous issue  [ here ]( https://github.com/tensorflow/tensorflow/issues/2292) , you may have  problem in the line `with tf.device(\"/gpu:0\"):` , after searching , i found you need to set a particular variable called \"allowed_soft_placement = true\"  as  an parameter to  tf.configProto, you can also check out following  links  \r\n[1](https://github.com/dpressel/rude-carnie/issues/24)\r\n[2](https://github.com/tensorflow/tensorflow/issues/2285)", "Thank you, @dhruvmalik007 . It seems intentional by using `diag` with GPU to create a failure.", "Welcome ,  it is quite a long time i have got  positive response from issues .\r\n ", "@facaiy , What is the alternative then?\r\n", "If I understand correctly, what the issue indeed cares is how to recovery session from RuntimeError. Good question, but sadly, I have no idea. So let's be patient to wait for reply from tensorflowers. \r\nBy the way,  I would not be surprised that @yaroslavvb, if he would like, might solve it by himself before response :-)", "@yaroslavvb Thanks for filing the issue!  Indeed this looks like a bug.\r\n\r\n@asimshankar Can you suggest someone who might be available to look into a fix?", "@skye : Could you take a look? (And whatever the fix is, should also work when we fully transition Python graph construction to the C API :)", "Hi @yaroslavvb, the problem is that the entire graph will be placed, not the pruned graph. This means that even when you're not attempting to run the diag op with the invalid device, it will still try to place it, causing the error. You can set place_pruned_graph to true in the ConfigOptions passed to a Session to avoid this, see https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/protobuf/config.proto#L153. You can also use InteractiveSession, which sets this automatically.", "node1= tf.constant(3.0, tf.float32)\r\nnode2= tf.constant(4.0)\r\nsess= tf.compat.v1.Session()\r\nprint(sess.run(fetches=[node1,node2])\r\n\r\nRuntimeError: The Session graph is empty.  Add operations to the graph before calling run().\r\nThe above error is being shown. Can you please help!!!"]}, {"number": 13491, "title": "Feature Request: support tf.diag on GPU", "body": "This could be a good project for an external contributor, currently no GPU support for tf.diag so the following fails\r\n\r\n```\r\nimport tensorflow as tf\r\nwith tf.device(\"/gpu:0\"):\r\n  mat = tf.diag([1,1])\r\nsess = tf.Session()\r\nsess.run(mat)\r\n```", "comments": ["cc @alextp apparently there's internal bug for this too", "Closing as this is resolved, feel free to reopen if problem persists"]}, {"number": 13490, "title": "tf.sparse_add doesn't work for tf.SparseTensor that has tf.Variable as values", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 8.0 / cuDNN7\r\n- **GPU model and memory**: 4x GTX1080ti\r\n- **Exact command to reproduce**: see below\r\n\r\n\r\n### Describe the problem\r\n`tf.sparse_add` raises `RuntimeError` for `tf.SparseTensor` that has `tf.Variable` as `values`.\r\n\r\nSimplest example code to reproduce the problem:\r\n\r\n    import numpy as np\r\n    import scipy.sparse as sp\r\n    import tensorflow as tf\r\n    \r\n    A = sp.random(100, 100)\r\n    vals = tf.Variable(tf.ones([A.nnz]))\r\n    A_tf = tf.SparseTensor(np.column_stack([A.row, A.col]), vals, A.shape)\r\n    tf.sparse_add(A_tf, A_tf)  # raises RuntimeError\r\n\r\nRunning this snippet produces the following error message\r\n\r\n\t--------------------------------------------------------------\r\n\tRuntimeError                 Traceback (most recent call last)\r\n\t<ipython-input-177-3d37a994a045> in <module>()\r\n\t      6 vals = tf.Variable(tf.ones([A.nnz]))\r\n\t      7 A_tf = tf.SparseTensor(np.column_stack([A.row, A.col]), vals, A.shape)\r\n\t----> 8 tf.sparse_add(A_tf, A_tf)  # raises RuntimeError\r\n\r\n\t~/.conda/envs/rml/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py in sparse_add(a, b, thresh)\r\n\t    297     b = _convert_to_sparse_tensor(b)\r\n\t    298     thresh = ops.convert_to_tensor(\r\n\t--> 299         thresh, dtype=a.values.dtype.real_dtype, name=\"thresh\")\r\n\t    300     output_ind, output_val, output_shape = (gen_sparse_ops._sparse_add(\r\n\t    301         a.indices, a.values, a.dense_shape,\r\n\r\n\t~/.conda/envs/rml/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n\t    609       name=name,\r\n\t    610       preferred_dtype=preferred_dtype,\r\n\t--> 611       as_ref=False)\r\n\t    612 \r\n\t    613 \r\n\r\n\t~/.conda/envs/rml/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)\r\n\t    688               \"dtype: requested = %s, actual = %s\"\r\n\t    689               % (error_prefix, conversion_func, base_type,\r\n\t--> 690                  dtype.name, ret.dtype.name))\r\n\t    691         return ret\r\n\t    692   raise TypeError(\"%sCannot convert %r with type %s to Tensor: \"\r\n\r\n\tRuntimeError: thresh: Conversion function <function _constant_tensor_conversion_function at 0x7fe50a9c0ea0> for type <class 'object'> returned incompatible dtype: requested = float32_ref, actual = float32\r\n\r\nI found a workaround, but I strongly believe that this is not the intended behavior\r\n\r\n\tzero_tensor = tf.SparseTensor(np.column_stack([0, 0]), [0.0], A.shape)\r\n\t# A_tf = tf.sparse_add(A_tf, zero_tensor)  # still same RuntimeError\r\n\tA_tf = tf.sparse_add(zero_tensor, A_tf)  # doesn't change the value of A_tf\r\n\ttf.sparse_add(A_tf, A_tf)  # works\r\n\r\n### Source code / logs\r\n\r\n#### Output of the log file\r\n\r\n\t== cat /etc/issue ===============================================\r\n\tLinux ************** 4.10.0-33-generic #37~16.04.1-Ubuntu SMP Fri Aug 11 14:07:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\t== cat /etc/issue ===============================================\r\n\tLinux ************** 4.10.0-33-generic #37~16.04.1-Ubuntu SMP Fri Aug 11 14:07:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\tVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\n\tVERSION_ID=\"16.04\"\r\n\tVERSION_CODENAME=xenial\r\n\r\n\t== are we in docker =============================================\r\n\tNo\r\n\r\n\t== compiler =====================================================\r\n\tc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n\tCopyright (C) 2015 Free Software Foundation, Inc.\r\n\tThis is free software; see the source for copying conditions.  There is NO\r\n\twarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n\t== uname -a =====================================================\r\n\tLinux ************** 4.10.0-33-generic #37~16.04.1-Ubuntu SMP Fri Aug 11 14:07:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\t== check pips ===================================================\r\n\tnumpy (1.13.1)\r\n\tprotobuf (3.4.0)\r\n\ttensorflow-gpu (1.3.0)\r\n\ttensorflow-tensorboard (0.1.6)\r\n\r\n\t== check for virtualenv =========================================\r\n\tFalse\r\n\r\n\t== tensorflow import ============================================\r\n\ttf.VERSION = 1.3.0\r\n\ttf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\n\ttf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\n\tSanity check: array([1], dtype=int32)\r\n\r\n\t== env ==========================================================\r\n\tLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64\r\n\tDYLD_LIBRARY_PATH is unset\r\n\r\n\t== nvidia-smi ===================================================\r\n\tWed Oct  4 17:59:42 2017       \r\n\t+-----------------------------------------------------------------------------+\r\n\t| NVIDIA-SMI 375.82                 Driver Version: 375.82                    |\r\n\t|-------------------------------+----------------------+----------------------+\r\n\t| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n\t| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n\t|===============================+======================+======================|\r\n\t|   0  GeForce GTX 108...  Off  | 0000:02:00.0     Off |                  N/A |\r\n\t| 20%   33C    P2    58W / 250W |    445MiB / 11172MiB |      0%      Default |\r\n\t+-------------------------------+----------------------+----------------------+\r\n\t|   1  GeForce GTX 108...  Off  | 0000:03:00.0     Off |                  N/A |\r\n\t| 20%   32C    P2    57W / 250W |    153MiB / 11172MiB |      0%      Default |\r\n\t+-------------------------------+----------------------+----------------------+\r\n\t|   2  GeForce GTX 108...  Off  | 0000:83:00.0     Off |                  N/A |\r\n\t| 20%   31C    P8    10W / 250W |   8671MiB / 11172MiB |      0%      Default |\r\n\t+-------------------------------+----------------------+----------------------+\r\n\t|   3  GeForce GTX 108...  Off  | 0000:84:00.0     Off |                  N/A |\r\n\t| 20%   37C    P8     9W / 250W |    153MiB / 11172MiB |      0%      Default |\r\n\t+-------------------------------+----------------------+----------------------+\r\n\t                                                                               \r\n\t+-----------------------------------------------------------------------------+\r\n\t| Processes:                                                       GPU Memory |\r\n\t|  GPU       PID  Type  Process name                               Usage      |\r\n\t|=============================================================================|\r\n\t|    0     19158    C   ****************************************       443MiB |\r\n\t|    1     19158    C   ****************************************       151MiB |\r\n\t|    2     20686    C   ********************************************  8669MiB |\r\n\t|    3     20686    C   ********************************************   151MiB |\r\n\t+-----------------------------------------------------------------------------+\r\n\r\n\t== cuda libs  ===================================================\r\n\t/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n\t/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n\t/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n\t/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n", "comments": ["@shchur I suspect that this was fixed on Sept 5 with this commit: 1f38a9c2f86cf1730d7bf782b4f4747106566850\r\n\r\nYou can try one of our nightly builds (or build from sources) to see if this fixes the problem:\r\nhttps://github.com/tensorflow/tensorflow#installation", "Thank you, that fixed the issue."]}, {"number": 13489, "title": "Make code Python 2 and 3 compatible", "body": "Update the Python implementation so that both Python 2 and Python 3 environment can execute", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 13488, "title": "Having issues with bazel build with respect to patch", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I'm just compiling\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Redhat 7\r\n- **TensorFlow installed from (source or binary)**: Installing from source\r\n- **TensorFlow version (use command below)**: Tried 1.0, 1.3 and latest\r\n- **Python version**:  2.7.10\r\n- **Bazel version (if compiling from source)**: 0.5.3\r\n- **CUDA/cuDNN version**: cuda 8/cudnn 5\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: bazel --output_base=./cache build -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma --config=cuda tensorflow_serving/...\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nTrying to compile with and without GPU doesn't seem to work for me because of the patch command. For r0.5.1, I can compile successfully without GPU, but for the other later versions, I am getting the following error:\r\n\r\nERROR: error loading package 'tensorflow_serving/resources': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n\tFile \"/serving-1.0/cache/external/org_tensorflow/tensorflow/workspace.bzl\", line 117\r\n\t\t_apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n\tFile \"/serving-1.0/cache/external/org_tensorflow/tensorflow/workspace.bzl\", line 108, in _apply_patch\r\n\t\t_execute_and_check_ret_code(repo_ctx, cmd)\r\n\tFile \"/serving-1.0/cache/external/org_tensorflow/tensorflow/workspace.bzl\", line 92, in _execute_and_check_ret_code\r\n\t\tfail(\"Non-zero return code({1}) when ...))\r\nNon-zero return code(2) when executing 'patch -p1 -d /serving-1.0/cache/external/protobuf -i /serving-1.0/cache/external/org_tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout: patching file src/google/protobuf/compiler/cpp/cpp_file.cc\r\nHunk #1 succeeded at 701 with fuzz 1 (offset 144 lines).\r\nHunk #2 succeeded at 803 (offset 147 lines).\r\nHunk #3 succeeded at 884 (offset 147 lines).\r\n\r\nStderr: patch: setting attribute security.selinux for security.selinux: Permission denied\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Seems like security settings on your installation are preventing execution of the command:\r\n\r\n```\r\npatch -p1 -d /serving-1.0/cache/external/protobuf -i /serving-1.0/cache/external/org_tensorflow/third_party/protobuf/add_noinlines.patch'\r\n```\r\n\r\nThis doesn't seem to be a TensorFlow issue and unfortunately it is beyond our area of expertise, so I'm going to close the issue. (We currently only support building from source on Ubuntu as per https://www.tensorflow.org/install/install_sources - and alas don't have the bandwidth to support various other Linux distributions).\r\n\r\nPerhaps the filesystem you're trying this on has restrictions?"]}, {"number": 13487, "title": "tweak validate_shape to remove \"Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]\" error", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No. I am running the Audio recognition tutorial\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:0.6.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:python tensorflow/examples/speech_commands/freeze.py \\\r\n--start_checkpoint=/tmp/speech_commands_train/conv.ckpt-18000 \\\r\n--output_file=/tmp/my_frozen_graph.pb\r\n\r\n\r\nI am asking about the error (attached below) here(rather than on stackoverflow) because of this related issue https://github.com/tensorflow/tensorflow/issues/5492 . I want to know if my error can be resolved by tweaking the validate_shape parameter,by setting it to false? . If not, please suggest alternatives.\r\n\r\nThe error is as follows: \r\n\r\nTraceback (most recent call last):\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_call\r\n    return fn(*args)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1300, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 467, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]\r\n\t [[Node: save/Assign_5 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_5\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_5, save/RestoreV2_5)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"/home/cogknit/tensorflow/tensorflow/examples/speech_commands/models.py\", line 123, in load_variables_from_checkpoint\r\n    saver.restore(sess, start_checkpoint)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1657, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1118, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1315, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]\r\n\t [[Node: save/Assign_5 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_5\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_5, save/RestoreV2_5)]]\r\n\r\nCaused by op 'save/Assign_5', defined at:\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 180, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/examples/speech_commands/freeze.py\", line 117, in main\r\n    models.load_variables_from_checkpoint(sess, FLAGS.start_checkpoint)\r\n  File \"/home/cogknit/tensorflow/tensorflow/examples/speech_commands/models.py\", line 122, in load_variables_from_checkpoint\r\n    saver = tf.train.Saver(tf.global_variables())\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1214, in __init__\r\n    self.build()\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1223, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1259, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 747, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 435, in _AddRestoreOps\r\n    assign_ops.append(saveable.restore(tensors, shapes))\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 160, in restore\r\n    self.op.get_shape().is_fully_defined())\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\r\n    validate_shape=validate_shape)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 56, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3082, in create_op\r\n    op_def=op_def)\r\n  File \"/home/cogknit/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1632, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [12] rhs shape= [3]\r\n\t [[Node: save/Assign_5 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_5\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_5, save/RestoreV2_5)]]\r\n\r\n\r\n", "comments": ["/CC @benoitsteiner can you comment on this issue and on #5492?", "Also /CC @petewarden.", "Did u manage to tweak the code? Can u share the code\r\nI'm facing the same problem ", "@vclteam, the issue here was resolved without tweaking the code.\r\n\r\n At the beginning of freeze.py file it is mentioned that the flags used during training command have to be preserved during the freezing too: \"One thing to watch out for is that you need to pass in the same arguments for `sample_rate` and other command line variables here as you did for the training\r\nscript.\" So in my case I had to use the flag --wanted_words=yes  again here ,something that I hadn't done previously.\r\n\r\nIn your case(https://github.com/tensorflow/tensorflow/issues/13581), I think if you have trained with the --clip_duration_ms=1000, then you can't change it to 2800 at the freezing stage. May I know if this is the case?", "@serendipity24 \r\n\r\nhi\r\n\r\nThanks for answering, as u can see below, i trained and freezed the model with the sample parameters.\r\nThe fact at the moment is that if u change the duration from 1000ms to any other value\r\nthe system crash on the freeze with the message\r\n\r\n**Assign requires shapes of both tensors to match**\r\n\r\npython tensorflow/examples/speech_commands/train.py --data_dir=/notebooks/yesdb/ \\\r\n--wanted_words=yes \\\r\n--how_many_training_steps=100,100 --learning_rate=0.001,0.0001  \\\r\n--train_dir=/notebooks/yesmodel \\\r\n--summaries_dir=/notebooks/yesmodellog\r\n--clip_duration_ms=2834 --sample_rate=16000 --window_size_ms=20\r\n\r\n\r\npython tensorflow/examples/speech_commands/freeze.py \\\r\n--wanted_words=yes \\\r\n--clip_duration_ms=2834 --sample_rate=16000 --window_size_ms=20 \\\r\n--start_checkpoint=/notebooks/yesmodel/conv.ckpt-100 \\\r\n--output_file=/notebooks/yesmodel/conv_frozen.pb\r\n", "and since i'm getting the same exception how exactly did u managed to overcome it?\r\n\r\n\r\n", "\"The fact at the moment is that if u change the duration from 1000ms to any other value\r\nthe system crash on the freeze with the message\"- I didn't change the duration from the default 1000ms during training. So I am sorry, I don't know the reason as of now. \r\n\r\nBut if the training worked with 2834 ms without any errors, I don't see why the freezing wouldn't work. If you initially trained using 1000ms, I suggest you delete the checkpoint files created during this training and then run the training with the new duration. This was something I came across while trying to solve my issue.\r\n", "@serendipity24 \r\ni deleted everything even used a clean docker.nothing!\r\nwhere i change the duration freeze just keep crashing\r\n ", "@vclteam just to be very clear, are you changing the duration during training or freezing? As previously mentioned, if the duration used in training and freezing are different, then it will crash.  So, may I know what duration are you using during training? Is it 2834 seconds as mentioned above by you?", "@serendipity24  2834 is the duration in milliseconds and im using the same value for the train and freeze.and while putting the value of 1000ms everything working great,change this value generate exception on the freeze \r\n\r\n\r\npython tensorflow/examples/speech_commands/train.py --data_dir=/notebooks/snoredb/ \\\r\n--wanted_words=yes \\\r\n--how_many_training_steps=100,100 --learning_rate=0.001,0.0001  \\\r\n--train_dir=/notebooks/snoremodel \\\r\n--summaries_dir=/notebooks/snoremodellog\r\n--clip_duration_ms=2834 --sample_rate=16000 --window_size_ms=20\r\n\r\n\r\npython tensorflow/examples/speech_commands/freeze.py \\\r\n--wanted_words=yes \\\r\n--clip_duration_ms=2834 --sample_rate=16000 --window_size_ms=20 \\\r\n--start_checkpoint=/notebooks/snoremodel/conv.ckpt-100 \\\r\n--output_file=/notebooks/snoremodel/conv_frozen.pb\r\n\r\n\r\n", "sorry found the problem,its related to linux command line not tensorflow at all\r\n@serendipity24 ", "@vclteam no need for sorry, glad it was resolved :), but curiosity sake I have two questions:\r\n(i) If you trained with 2834ms and gave the freeze command with 2834ms,  what do you mean by \"while putting the value of 1000ms\" ?\r\n(ii) What about linux command line caused this error? \r\n\r\nThanks", "if forgot to put \\ at the end of one of the line\r\nso the training was actually for 1000ms", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Closing this out - looks like it was resolved."]}, {"number": 13486, "title": "ImportError: No module named 'tensorflowvisu'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13485, "title": "tf.Session.list_devices seems to return an (int64 *) for device.memory_limit_bytes and leaks", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nno\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nbinary 1.3.0 GPU\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n\r\n- **Python version**: \r\n\r\nPython 2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nCUDA 8.0 + cuDNN 6.0\r\n\r\n- **GPU model and memory**:\r\n\r\nGeForce GTX 960M\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as S:\r\n    for d in S.list_devices():\r\n        print d.name, d.device_type, d.memory_limit_bytes\r\n```\r\n\r\nproduces:\r\n\r\n```bash\r\n2017-10-04 10:31:04.370386: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-04 10:31:04.370409: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-04 10:31:04.370415: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-04 10:31:04.370419: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-04 10:31:04.370423: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-04 10:31:04.566504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-04 10:31:04.566814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: \r\nname: GeForce GTX 960M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.92GiB\r\n2017-10-04 10:31:04.566862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 \r\n2017-10-04 10:31:04.566868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y \r\n2017-10-04 10:31:04.566875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)\r\n/job:localhost/replica:0/task:0/device:CPU:0 CPU <Swig Object of type 'int64_t *' at 0x7f5c22f61a50>\r\n/job:localhost/replica:0/task:0/device:GPU:0 GPU <Swig Object of type 'int64_t *' at 0x7f5c22f613f0>\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\n```\r\n\r\n### Describe the problem\r\n\r\nThe `memory_limit_bytes` attribute seems to be wrapping an `int64 *` pointer (as opposed to a int64) and swig seems to think it's leaking this pointer. Also, in the docs its referred to as [`memory_limit`](https://www.tensorflow.org/api_docs/python/tf/Session#list_devices)\r\n\r\n### Source code / logs\r\n\r\nN/A", "comments": ["Should be the same issue as https://github.com/tensorflow/tensorflow/issues/13359, fixed in https://github.com/tensorflow/tensorflow/commit/4b9e50686928b858a0045e1ddcc0bab4acc7ff37 (will be in 1.4)"]}, {"number": 13484, "title": "Branch 170960975", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13483, "title": "Fix tf-signal tests on pip packages.", "body": "", "comments": ["Jenkins, test this please.", "Sorry about this! Is there any way I should have noticed this? I'm pretty sure internally the pip tests were passing when I submitted.", "I think one way was pip_smoke_tests, but we need a better way to explain the situation in the presubmit tests."]}, {"number": 13482, "title": "Error in creating the final binary using AOT compilation for CPU backend", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.3.0-rc1-2665-g242b0f1\r\n- **Python version**: Python3\r\n- **Bazel version (if compiling from source)**: 0.6.0\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**: \r\nbazel build //tensorflow/compiler/aot/tests:my_binary\r\n\r\n\r\n### Describe the problem\r\nI simply followed the tutorial here: https://www.tensorflow.org/performance/xla/tfcompile\r\n\r\nAccording to Step 1 and 2, I compiled the subgraph and generated the header (`test_graph_tfmatmul.h`) and object (`test_graph_tfmatmul.o`) files using `tfcompile`;\r\n\r\nAccording to Step 3, I used the example code (named as `my_code.cc`) to invoke the subgraph;\r\n\r\nAccording to Step 4, I added the code snippet `cc_binary` to the existing `BUILD` file (`//tensorflow/compiler/aot/tests/BUILD`), and tried to create the final binary with the command:\r\n\r\n`bazel build //tensorflow/compiler/aot/tests:my_binary`\r\n\r\nbut I got the following error:\r\n\r\n`undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests:my_binary':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/my_code.cc':\r\n'/home/tensorFlow_src/tensorflow/tensorflow/compiler/aot/tests/test_graph_tfmatmul.h'`\r\n\r\n### Source code / logs\r\n`my_code.cc` (exactly the same as in the tutorial):\r\n```c++\r\n#define EIGEN_USE_THREADS\r\n#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n\r\n#include <iostream>\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/compiler/aot/tests/test_graph_tfmatmul.h\" // generated\r\n\r\nint main(int argc, char** argv) {\r\n  Eigen::ThreadPool tp(2);  // Size the thread pool as appropriate.\r\n  Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());\r\n\r\n  foo::bar::MatMulComp matmul;\r\n  matmul.set_thread_pool(&device);\r\n\r\n  // Set up args and run the computation.\r\n  const float args[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};\r\n  std::copy(args + 0, args + 6, matmul.arg0_data());\r\n  std::copy(args + 6, args + 12, matmul.arg1_data());\r\n  matmul.Run();\r\n\r\n  // Check result\r\n  if (matmul.result0(0, 0) == 58) {\r\n    std::cout << \"Success\" << std::endl;\r\n  } else {\r\n    std::cout << \"Failed. Expected value 58 at 0,0. Got:\"\r\n              << matmul.result0(0, 0) << std::endl;\r\n  }\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n`cc_binary` in `BUILD` file:\r\n```\r\ncc_binary(\r\n    name = \"my_binary\",\r\n    srcs = [\"my_code.cc\"],\r\n    deps = [\r\n        \"//tensorflow/compiler/aot/tests:test_graph_tfmatmul\",\r\n        \"//third_party/eigen3\",\r\n    ],\r\n    linkopts = [\"-lpthread\",]\r\n)\r\n```", "comments": ["Could you share the complete contents of the `BUILD` file and the full error message? From the snippet you've shared, it seems that it is having trouble building the target `//tensorflow/compiler/aot/tests:tfcompile_test`, which suggests some unexpected change to the `BUILD` file.\r\n\r\nAlso, please fill in all information asked for in the issue template. In this particular case, details about the exact version of the source code you're using to build would be helpful.\r\n\r\nFYI @tatatodd ", "@asimshankar Sorry for the misleading. The error message should be: \r\n`ERROR: /home/tensorFlow_src/tensorflow/tensorflow/compiler/aot/tests/BUILD:128:1: undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests:my_binary':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/my_code.cc':\r\n  '/home/tensorFlow_src/tensorflow/tensorflow/compiler/aot/tests/test_graph_tfmatmul.h'\r\nTarget //tensorflow/compiler/aot/tests:my_binary failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.467s, Critical Path: 1.14s\r\nFAILED: Build did NOT complete successfully\r\n`\r\n\r\nThe complete `BUILD` file:\r\n```\r\nlicenses([\"notice\"])  # Apache 2.0\r\n\r\npackage(\r\n    default_visibility = [\"//visibility:private\"],\r\n)\r\n\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\")\r\n\r\ntest_suite(\r\n    name = \"all_tests\",\r\n    tags = [\"manual\"],\r\n    tests = [\r\n        \":test_graph_tfadd_test\",\r\n        \":test_graph_tfadd_with_ckpt_saver_test\",\r\n        \":test_graph_tfadd_with_ckpt_test\",\r\n        \":test_graph_tffunction_test\",\r\n        \":test_graph_tfgather_test\",\r\n        \":test_graph_tfmatmul_test\",\r\n        \":test_graph_tfmatmulandadd_test\",\r\n        \":test_graph_tfsplits_test\",\r\n        \":tfcompile_test\",\r\n    ],\r\n)\r\n\r\npy_binary(\r\n    name = \"make_test_graphs\",\r\n    testonly = 1,\r\n    srcs = [\"make_test_graphs.py\"],\r\n    srcs_version = \"PY2AND3\",\r\n    deps = [\r\n        \"//tensorflow/core:protos_all_py\",\r\n        \"//tensorflow/python\",  # TODO(b/34059704): remove when fixed\r\n        \"//tensorflow/python:array_ops\",\r\n        \"//tensorflow/python:client\",\r\n        \"//tensorflow/python:framework_for_generated_wrappers\",\r\n        \"//tensorflow/python:math_ops\",\r\n        \"//tensorflow/python:platform\",\r\n        \"//tensorflow/python:session\",\r\n        \"//tensorflow/python:training\",\r\n        \"//tensorflow/python:variables\",\r\n    ],\r\n)\r\n\r\ngenrule(\r\n    name = \"gen_test_graphs\",\r\n    testonly = 1,\r\n    outs = [\r\n        \"test_graph_tfadd.pb\",\r\n        \"test_graph_tfadd_with_ckpt.ckpt\",\r\n        \"test_graph_tfadd_with_ckpt.pb\",\r\n        \"test_graph_tfadd_with_ckpt_saver.ckpt\",\r\n        \"test_graph_tfadd_with_ckpt_saver.pb\",\r\n        \"test_graph_tfadd_with_ckpt_saver.saver\",\r\n        \"test_graph_tffunction.pb\",\r\n        \"test_graph_tfgather.pb\",\r\n        \"test_graph_tfmatmul.pb\",\r\n        \"test_graph_tfmatmulandadd.pb\",\r\n        \"test_graph_tfsplits.pb\",\r\n    ],\r\n    cmd = \"$(location :make_test_graphs) --out_dir $(@D)\",\r\n    tags = [\"manual\"],\r\n    tools = [\":make_test_graphs\"],\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tfadd\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfadd.config.pbtxt\",\r\n    cpp_class = \"AddComp\",\r\n    graph = \"test_graph_tfadd.pb\",\r\n    # This serves as a test for the list of minimal deps included even when\r\n    # include_standard_runtime_deps is False.  If this target fails to\r\n    # compile but the others in this directory succeed, you may need to\r\n    # expand the \"required by all tf_library targets\" list in tfcompile.bzl.\r\n    include_standard_runtime_deps = False,\r\n    tags = [\"manual\"],\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tfadd_with_ckpt\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfadd_with_ckpt.config.pbtxt\",\r\n    cpp_class = \"AddWithCkptComp\",\r\n    freeze_checkpoint = \"test_graph_tfadd_with_ckpt.ckpt\",\r\n    graph = \"test_graph_tfadd_with_ckpt.pb\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tfadd_with_ckpt_saver\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfadd_with_ckpt.config.pbtxt\",\r\n    cpp_class = \"AddWithCkptSaverComp\",\r\n    freeze_checkpoint = \"test_graph_tfadd_with_ckpt_saver.ckpt\",\r\n    freeze_saver = \"test_graph_tfadd_with_ckpt_saver.saver\",\r\n    graph = \"test_graph_tfadd_with_ckpt_saver.pb\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tffunction\",\r\n    testonly = 1,\r\n    config = \"test_graph_tffunction.config.pbtxt\",\r\n    cpp_class = \"FunctionComp\",\r\n    graph = \"test_graph_tffunction.pb\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tfgather\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfgather.config.pbtxt\",\r\n    cpp_class = \"GatherComp\",\r\n    graph = \"test_graph_tfgather.pb\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tfmatmul\",\r\n    #testonly = 1,\r\n    config = \"test_graph_tfmatmul.config.pbtxt\",\r\n    cpp_class = \"foo::bar::MatMulComp\",\r\n    graph = \"test_graph_tfmatmul.pb\",\r\n    #tags = [\"manual\"],\r\n)\r\n\r\ncc_binary(\r\n    name = \"my_binary\",\r\n    srcs = [\"my_code.cc\"],  # include test_graph_tfmatmul.h to access the generated header\r\n    deps = [\r\n        \"//tensorflow/compiler/aot/tests:test_graph_tfmatmul\",  # link in the generated object file\r\n        \"//third_party/eigen3\",\r\n    ],\r\n    linkopts = [\r\n        \"-lpthread\",\r\n    ]\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tfmatmulandadd\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfmatmulandadd.config.pbtxt\",\r\n    cpp_class = \"MatMulAndAddComp\",\r\n    graph = \"test_graph_tfmatmulandadd.pb\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\ntf_library(\r\n    name = \"test_graph_tfsplits\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfsplits.config.pbtxt\",\r\n    cpp_class = \"SplitsComp\",\r\n    graph = \"test_graph_tfsplits.pb\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\ntf_cc_test(\r\n    name = \"tfcompile_test\",\r\n    srcs = [\"tfcompile_test.cc\"],\r\n    tags = [\"manual\"],\r\n    deps = [\r\n        \":test_graph_tfadd\",\r\n        \":test_graph_tfadd_with_ckpt\",\r\n        \":test_graph_tfadd_with_ckpt_saver\",\r\n        \":test_graph_tffunction\",\r\n        \":test_graph_tfgather\",\r\n        \":test_graph_tfmatmul\",\r\n        \":test_graph_tfmatmulandadd\",\r\n        \":test_graph_tfsplits\",\r\n        \"//tensorflow/core:test\",\r\n        \"//tensorflow/core:test_main\",\r\n        \"//third_party/eigen3\",\r\n    ],\r\n)\r\n\r\n#-----------------------------------------------------------------------------\r\n\r\nfilegroup(\r\n    name = \"all_files\",\r\n    srcs = glob(\r\n        [\"**/*\"],\r\n        exclude = [\r\n            \"**/METADATA\",\r\n            \"**/OWNERS\",\r\n        ],\r\n    ),\r\n    visibility = [\"//tensorflow:__subpackages__\"],\r\n)\r\n```\r\n\r\nThe exact source version:\r\nv1.3.0-rc1-2665-g242b0f1", "@asimshankar @aselle  FYI, I tried the latest source (version: v1.3.0-rc1-3000-g840dcae) this morning, and the error still exists. I simply followed the tutorial (https://www.tensorflow.org/performance/xla/tfcompile) without changing anything. In addition, I found the same error was also mentioned in #11726, where @ankitachandak initially was getting the same error in build:\r\n\r\n```\r\nERROR: /local/mnt/workspace/ankitac/virtual/tensorflow/tensorflow/compiler/aot/tests/aot_project/BUILD:12:1: undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests/aot_project:test_graph_binary':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/aot_project/test_graph.cc':\r\n  '/local/mnt/workspace/ankitac/virtual/tensorflow/tensorflow/compiler/aot/tests/aot_project/test_graph_tfmatmul.h'.\r\nTarget //tensorflow/compiler/aot/tests/aot_project:test_graph_binary failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 5.092s, Critical Path: 1.34s\r\n```\r\n\r\nMay I have your advice on this issue? Thanks.", "@MartinZZZ  What happens when you try to build the existing `tfcompile_test`?\r\n```\r\nblaze build //tensorflow/compiler/aot/tests:tfcompile_test\r\n```\r\n\r\nThat corresponds to the following source file:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tests/tfcompile_test.cc", "@tatatodd Thanks for your suggestion. \r\nI just built the existing `tfcompile_test` with the following command:\r\n```\r\nbazel build //tensorflow/compiler/aot/tests:tfcompile_test\r\n```\r\nand the error occurred as well:\r\n\r\n```\r\nERROR: /home/tensorFlow_src/tensorflow/tensorflow/compiler/aot/tests/BUILD:147:1: undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests:tfcompile_test':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/tfcompile_test.cc':\r\n  '/home/tensorFlow_src/tensorflow/tensorflow/compiler/aot/tests/test_graph_tfmatmul.h'\r\nTarget //tensorflow/compiler/aot/tests:tfcompile_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1433.945s, Critical Path: 50.08s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nHere is the `BUILD` file:\r\n\r\n```\r\nlicenses([\"notice\"])  # Apache 2.0\r\n\r\npackage(\r\n    default_visibility = [\"//visibility:private\"],\r\n)\r\n\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\")\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")\r\n\r\n# Optional runtime utilities for use by code generated by tfcompile.\r\ncc_library(\r\n    name = \"runtime\",\r\n    srcs = [\"runtime.cc\"],\r\n    hdrs = [\"runtime.h\"],\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\r\n        \"//tensorflow/core:framework_lite\",\r\n    ],\r\n)\r\n\r\ntf_cc_test(\r\n    name = \"runtime_test\",\r\n    srcs = [\"runtime_test.cc\"],\r\n    deps = [\r\n        \":runtime\",\r\n        \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:test\",\r\n        \"//tensorflow/core:test_main\",\r\n    ],\r\n)\r\n\r\n# Don't depend on this directly; this is only used for the benchmark test\r\n# generated by tf_library.\r\ncc_library(\r\n    name = \"tf_library_test_main\",\r\n    testonly = 1,\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\"//tensorflow/core:test_main\"],\r\n)\r\n\r\ncc_library(\r\n    name = \"tfcompile_lib\",\r\n    srcs = [\r\n        \"codegen.cc\",\r\n        \"compile.cc\",\r\n        \"flags.cc\",\r\n    ],\r\n    hdrs = [\r\n        \"codegen.h\",\r\n        \"compile.h\",\r\n        \"flags.h\",\r\n    ],\r\n    deps = [\r\n        \":runtime\",  # needed by codegen to print aligned_buffer_bytes\r\n        \"//tensorflow/compiler/tf2xla\",\r\n        \"//tensorflow/compiler/tf2xla:common\",\r\n        \"//tensorflow/compiler/tf2xla:tf2xla_proto\",\r\n        \"//tensorflow/compiler/tf2xla:tf2xla_util\",\r\n        \"//tensorflow/compiler/tf2xla:xla_compiler\",\r\n        \"//tensorflow/compiler/tf2xla/kernels:xla_cpu_only_ops\",\r\n        \"//tensorflow/compiler/tf2xla/kernels:xla_ops\",\r\n        \"//tensorflow/compiler/xla:shape_util\",\r\n        \"//tensorflow/compiler/xla:statusor\",\r\n        \"//tensorflow/compiler/xla:util\",\r\n        \"//tensorflow/compiler/xla:xla_data_proto\",\r\n        \"//tensorflow/compiler/xla/client:client_library\",\r\n        \"//tensorflow/compiler/xla/client:compile_only_client\",\r\n        \"//tensorflow/compiler/xla/service:compiler\",\r\n        \"//tensorflow/compiler/xla/service/cpu:cpu_compiler\",\r\n        \"//tensorflow/core:core_cpu\",\r\n        \"//tensorflow/core:core_cpu_internal\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:framework_internal\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core:protos_all_cc\",\r\n    ],\r\n)\r\n\r\ntf_cc_test(\r\n    name = \"codegen_test\",\r\n    srcs = [\"codegen_test.cc\"],\r\n    data = [\"codegen_test_h.golden\"],\r\n    deps = [\r\n        \":tfcompile_lib\",\r\n        \"//tensorflow/compiler/xla:shape_util\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core:test\",\r\n        \"//tensorflow/core:test_main\",\r\n    ],\r\n)\r\n\r\ntf_cc_binary(\r\n    name = \"tfcompile\",\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\":tfcompile_main\"],\r\n)\r\n\r\ncc_library(\r\n    name = \"tfcompile_main\",\r\n    srcs = [\"tfcompile_main.cc\"],\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\r\n        \":tfcompile_lib\",\r\n        \"//tensorflow/compiler/tf2xla:tf2xla_proto\",\r\n        \"//tensorflow/compiler/tf2xla:tf2xla_util\",\r\n        \"//tensorflow/compiler/xla/legacy_flags:debug_options_flags\",\r\n        \"//tensorflow/compiler/xla/service:compiler\",\r\n        \"//tensorflow/core:core_cpu\",\r\n        \"//tensorflow/core:core_cpu_internal\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:framework_internal\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core:protos_all_cc\",\r\n    ],\r\n)\r\n\r\n# NOTE: Most end-to-end tests are in the \"tests\" subdirectory, to ensure that\r\n# tfcompile.bzl correctly handles usage from outside of the package that it is\r\n# defined in.\r\n\r\n# A simple test of tf_library from a text protobuf, mostly to enable the\r\n# benchmark_test.\r\ntf_library(\r\n    name = \"test_graph_tfadd\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfadd.config.pbtxt\",\r\n    cpp_class = \"AddComp\",\r\n    graph = \"test_graph_tfadd.pbtxt\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\n# A test of tf_library that includes a graph with an unknown op, but where\r\n# the compilation works because the unknown op is not needed for the fetches.\r\ntf_library(\r\n    name = \"test_graph_tfunknownop\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfunknownop.config.pbtxt\",\r\n    cpp_class = \"UnknownOpAddComp\",\r\n    graph = \"test_graph_tfunknownop.pbtxt\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\n# A test of tf_library that includes a graph with an unknown op, but where\r\n# the compilation works because the op between the unknown op and the\r\n# fetches is a feed.\r\ntf_library(\r\n    name = \"test_graph_tfunknownop2\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfunknownop2.config.pbtxt\",\r\n    cpp_class = \"UnknownOpAddComp\",\r\n    graph = \"test_graph_tfunknownop.pbtxt\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\n# A test of tf_library that includes a graph with an unknown op, but where\r\n# the compilation works because the unknown op is fed.\r\ntf_library(\r\n    name = \"test_graph_tfunknownop3\",\r\n    testonly = 1,\r\n    config = \"test_graph_tfunknownop3.config.pbtxt\",\r\n    cpp_class = \"UnknownOpAddComp\",\r\n    graph = \"test_graph_tfunknownop.pbtxt\",\r\n    tags = [\"manual\"],\r\n)\r\n\r\n# Utility library for benchmark binaries, used by the *_benchmark rules that are\r\n# added by the tfcompile bazel macro.\r\ncc_library(\r\n    name = \"benchmark\",\r\n    srcs = [\"benchmark.cc\"],\r\n    hdrs = [\"benchmark.h\"],\r\n    visibility = [\"//visibility:public\"],\r\n    deps = [\r\n        # The purpose of the benchmark library is to support building an aot\r\n        # binary with minimal dependencies, to demonstrate small binary sizes.\r\n        #\r\n        # KEEP THE DEPENDENCIES MINIMAL.\r\n        \"//tensorflow/core:framework_lite\",\r\n    ],\r\n)\r\n\r\ncc_library(\r\n    name = \"benchmark_extra_android\",\r\n    tags = [\r\n        \"manual\",\r\n        \"notap\",\r\n    ],\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n\r\ntf_cc_test(\r\n    name = \"benchmark_test\",\r\n    srcs = [\"benchmark_test.cc\"],\r\n    tags = [\"manual\"],\r\n    deps = [\r\n        \":benchmark\",\r\n        \":test_graph_tfadd\",\r\n        \"//tensorflow/core:test\",\r\n        \"//tensorflow/core:test_main\",\r\n    ],\r\n)\r\n\r\ntest_suite(\r\n    name = \"all_tests\",\r\n    tags = [\"manual\"],\r\n    tests = [\r\n        \":benchmark_test\",\r\n        \":codegen_test\",\r\n        \":runtime_test\",\r\n        \":test_graph_tfadd_test\",\r\n        \":test_graph_tfunknownop2_test\",\r\n        \":test_graph_tfunknownop3_test\",\r\n        \":test_graph_tfunknownop_test\",\r\n        \"//tensorflow/compiler/aot/tests:all_tests\",\r\n    ],\r\n)\r\n\r\nexports_files([\r\n    \"benchmark_main.template\",  # used by tf_library(...,gen_benchmark=True)\r\n    \"test.cc\",  # used by tf_library(...,gen_test=True)\r\n])\r\n\r\n# -----------------------------------------------------------------------------\r\n\r\nfilegroup(\r\n    name = \"all_files\",\r\n    srcs = glob(\r\n        [\"**/*\"],\r\n        exclude = [\r\n            \"**/METADATA\",\r\n            \"**/OWNERS\",\r\n        ],\r\n    ),\r\n    visibility = [\"//tensorflow:__subpackages__\"],\r\n)\r\n```\r\n\r\nThe current TensorFlow version is `v1.3.0-rc1-3000-g840dcae`\r\n\r\nMay I have your advice?", "@MartinZZZ Can you try again, starting with a clean copy of the sources (without any of your changes), and running:\r\n```\r\nbazel build //tensorflow/compiler/aot/tests:tfcompile_test\r\n```\r\n\r\nIf that still fails, there's either some issue with the way your bazel is set up, or something is going wrong elsewhere.", "@tatatodd Thanks for your suggestion. I tried a clean copy of the sources, and built the `tfcompile_test` successfully.\r\n\r\nBut error still occurs when building `tfmatmul` according to the [tutorial](https://www.tensorflow.org/performance/xla/tfcompile). Here is the detailed procedure, and hope to have your advice (The following 4 steps are corresponding to the 4 steps in the tutorial):\r\n\r\n- Step 1: Configure the subgraph. The config file already exists as `test_graph_tfmatmul.config.pbtxt` in directory `//tensorflow/compiler/aot/tests`;\r\n\r\n- Step 2.1: Generate the graph file `test_graph_tfmatmul.pb`:\r\n```\r\npython3 ./make_test_graphs.py --out_dir=./\r\n```\r\n- Step 2.2: Compile the graph using `tfcompile`:\r\n```\r\n~/tensorFlow_src/tensorflow/bazel-bin/tensorflow/compiler/aot/tfcompile --graph=\"./test_graph_tfmatmul.pb\" --config=\"./test_graph_tfmatmul.config.pbtxt\" --entry_point=\"test_graph_tfmatmul\" --cpp_class=\"foo::bar::MatMulComp\" --out_object=\"test_graph_tfmatmul.o\" --out_header=\"test_graph_tfmatmul.h\" --target_features=\"+avx2\"\r\n```\r\n\r\n- Step 3: Creating a file named `my_code.cc`:\r\n```\r\n#define EIGEN_USE_THREADS\r\n#define EIGEN_USE_CUSTOM_THREAD_POOL\r\n\r\n#include <iostream>\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/compiler/aot/tests/test_graph_tfmatmul.h\" // generated\r\n\r\nint main(int argc, char** argv) {\r\n    Eigen::ThreadPool tp(2);  // Size the thread pool as appropriate.\r\n    Eigen::ThreadPoolDevice device(&tp, tp.NumThreads());\r\n\r\n    foo::bar::MatMulComp matmul;\r\n    matmul.set_thread_pool(&device);\r\n\r\n    // Set up args and run the computation.\r\n    const float args[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};\r\n    std::copy(args + 0, args + 6, matmul.arg0_data());\r\n    std::copy(args + 6, args + 12, matmul.arg1_data());\r\n    matmul.Run();\r\n\r\n    // Check result\r\n    if (matmul.result0(0, 0) == 58) {\r\n        std::cout << \"Success\" << std::endl;\r\n    } else {\r\n        std::cout << \"Failed. Expected value 58 at 0,0. Got:\"\r\n                    << matmul.result0(0, 0) << std::endl;\r\n    }\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n- Step 4.1: Create the `BUILD` file:\r\n```\r\n# Example of linking your binary\r\n# Also see //third_party/tensorflow/compiler/aot/tests/BUILD\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\n# The same tf_library call from step 2 above.\r\ntf_library(\r\n    name = \"test_graph_tfmatmul\",\r\n    cpp_class = \"foo::bar::MatMulComp\",\r\n    graph = \"test_graph_tfmatmul.pb\",\r\n    config = \"test_graph_tfmatmul.config.pbtxt\",\r\n)\r\n\r\n# The executable code generated by tf_library can then be linked into your code.\r\ncc_binary(\r\n    name = \"my_binary\",\r\n    srcs = [\r\n        \"my_code.cc\",  # include test_graph_tfmatmul.h to access the generated header\r\n    ],\r\n    deps = [\r\n        \":test_graph_tfmatmul\",  # link in the generated object file\r\n        \"//third_party/eigen3\",\r\n    ],\r\n    linkopts = [\r\n        \"-lpthread\",\r\n    ]\r\n)\r\n```\r\n\r\n- Step 4.2: Create the final binary:\r\n```\r\nbazel build --config=opt --copt=-mavx2 --copt=-mfma --config=mkl --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" //tensorflow/compiler/aot/tests:my_binary\r\n```\r\n\r\nFinally, it will print:\r\n```\r\nERROR: /home/tensorFlow_clean/tensorflow/tensorflow/compiler/aot/tests/BUILD:14:1: undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests:my_binary':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/my_code.cc':\r\n  '/home/tensorFlow_clean/tensorflow/tensorflow/compiler/aot/tests/test_graph_tfmatmul.h'\r\nTarget //tensorflow/compiler/aot/tests:my_binary failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 7.339s, Critical Path: 6.69s\r\nFAILED: Build did NOT complete successfully\r\n```", "@MartinZZZ If `tfcompile_test` builds successfully from clean sources, your edits must be causing the problem.\r\n\r\nI think the problem is your steps 2.1 and 2.2.  These steps are conflicting with the files `test_graph_tfmatmul.pb`, `test_graph_tfmatmul.h` and `test_graph_tfmatmul.o` that already exist.\r\n\r\nIn particular your step 2.2 doesn't actually exist in the tutorial.  Step 2 of the tutorial is trying to say that `tf_library` will run `tfcompile` for you to generate the header and object files.\r\n\r\nStart over from clean sources, skip your steps 2.1 and 2.2, and see if that helps.", "@tatatodd I skipped Step 2.2 and it worked. Thanks so much for your help:)\r\n\r\nBtw, it does not complain about the unused SIMD instructions either regarding issue #13500 (The same procedure as this issue). Does it mean the TensorFlow binary is now compiled to use these instructions?", "@MartinZZZ  great, I'm glad we figured out the problem!  :)\r\n\r\nI'll comment on #13500 separately on that issue."]}, {"number": 13481, "title": "crosstool_wrapper_driver_is_not_gcc failed building tensorflow/tools/graph_transforms:transform_graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.3.0 @ commit 635196\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: not complied from source but Build label: 0.6.0 \r\n- **CUDA/cuDNN version**: CUDA 8.0, cudnn6.0\r\n- **GPU model and memory**: GTX 1080 Ti\r\n- **Exact command to reproduce**: bazel build tensorflow/tools/graph_transforms:transform_graph  --verbose_failures\r\n\r\n### Describe the problem\r\nBuild tensorflow from source successful, verified it works, but cannot build the tool \"transform_graph\"\r\n~~~\r\nERROR: /home/local/ANT/luxial/tensorflow/tensorflow/tools/graph_transforms/BUILD:222:1: Linking of rule '//tensorflow/tools/graph_transforms:transform_graph' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (cd /home/local/ANT/luxial/.cache/bazel/_bazel_luxial/ce09802cfa8c7dbfadcb21edd190af0e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=6 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/tools/graph_transforms/transform_graph '-Wl,-rpath,$\r\nORIGIN/../../../_solib_local/_U_S_Stensorflow_Stools_Sgraph_Utransforms_Ctransform_Ugraph___Utensorflow' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccu\r\nblas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scu\r\nda_Slib' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/local_linux-opt/bin/_solib_l\r\nocal/_U_S_Stensorflow_Stools_Sgraph_Utransforms_Ctransform_Ugraph___Utensorflow -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Sloca\r\nl_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Lbazel-out/local_linux-opt/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..' -Wl,-z,muldefs -Wl,-z,muldefs -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -pthread -Wl,-no-as-needed -B/usr/bin/ -fPIC -pie -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,--gc-sections -Wl,@bazel-out/local_linux-opt/bin/tensorflow/tools/graph_transforms/transform_graph-2.params)\r\n~~~\r\n### What I tried\r\nBasically everything in #8790, #4365, #817\r\n1. export LD_LIBRARY_PATH=/usr/local/cuda 8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} \r\n2. Upgrade Bazel, current build label 0.6.0\r\n3. bazel clean --expunge\r\n4. Adding -fPIC to the options in ../third_party/gpus/crosstool/CROSSTOOL_nvcc.tpl line 60.\r\n5. Add cpu_compiler_flags.append('-fno-use-linker-plugin') in third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl\r\n\r\nNone of them solve the issue.\r\n", "comments": ["Fixed.\r\nsudo sh -c \"echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"\r\nsudo ldconfig", "> Fixed.\r\n> sudo sh -c \"echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"\r\n> sudo ldconfig\r\n\r\nsolved by your method on cuda-9.2!!!", "> Fixed.\r\n> sudo sh -c \"echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf\"\r\n> sudo ldconfig\r\n\r\nworks on CUDA 10.1 too"]}, {"number": 13480, "title": "Merge fixes related to Bazel depset to r1.3 branch.", "body": "Currently unable to build branch r1.3 with recommended version of Bazel. Cherry-picking two\r\nchanges that change set to depset within tensorflow and within a library Tensorflow depends\r\non.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "@caisq is the failure on GPU expected?", "> ERROR: No test targets were found, yet testing was requested.\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/7190/console\r\n@gunan Not sure. This looks weird to me. Trying again.\r\n\r\n@tensorflow-jenkins test this please", "GPU failure is expected for 1.3 I thought. This is okay to merge I believe."]}, {"number": 13479, "title": "Allow creating global tf.FIFOQueue in deep frame contexts", "body": "Let's say I'm building a graph. It's a pretty deep graph with many nested while loops. Let's also say I have some template/abstraction that allows me to create a tf.FIFOQueue on demand deep in my nested while loops as I encounter things I want to send out of the graph. This template allows me to infer the shape automatically from the tensors as they are encountered rather than to have to write a bunch of painful boilerplate to define all the queues and their corresponding shapes up-front.\r\n\r\nThis is currently not possible with TensorFlow, because this gives this error: \r\n\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'fifo_queue_DequeueMany' has inputs from different frames. The input 'while/fifo_queue' is in frame 'while/while/'. The input 'fifo_queue_DequeueMany/n' is in frame ''.\r\n\r\nI want to create a FIFOQueue() in the global \"frame\" so I don't get this error. In addition the documentation never explains what a \"frame\" even is. Having undocumented restrictions that suddenly explode without warning is a sign of leaky abstractions and very frustrating for users.", "comments": ["Similar issue: #8604\r\n\r\nI solved this with the workaround: `with tf.control_dependencies(None):`"]}, {"number": 13478, "title": "Branch 170919783", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13477, "title": "Bug in 1.3 Preventing Export of Canned Estimators", "body": "\r\n------------------------\r\n\r\n### System information\r\n\r\nTensorFlow v1.3.0-rc2-20-g0787eee 1.3.0\r\nPython version 3.5\r\n\r\n\r\n### Describe the problem\r\n\r\nSee below Stack Overflow Page. \r\nhttps://stackoverflow.com/questions/46098863/how-to-import-an-saved-tensorflow-model-train-using-tf-estimator-and-predict-on\r\n\r\nUse case #2 is what this bug is about. The model appears to export, but when attempting to perform predictions on new data, the following error message is rendered: \r\nValueError: Got unexpected keys in input_dict: {'feature1', 'feature2', 'feature3'...}\r\n\r\n### Source code / logs\r\n### Code to store the model:\r\n\r\n```python\r\ndef column_to_dtype(column):\r\n    if column in CATEGORICAL_COLUMNS:\r\n        return tf.string\r\n    else:\r\n        return tf.float32\r\n    \r\nfeature_spec = {\r\n    column: tf.FixedLenFeature(shape=[1], dtype=column_to_dtype(column))\r\n        for column in FEATURE_COLUMNS\r\n}\r\nserving_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\nservable_model_dir = \"DLModels\"\r\nservable_model_path = m.export_savedmodel(export_dir_base=servable_model_dir,\r\n                            serving_input_receiver_fn=serving_fn)\r\n```\r\n### Code to recall the model for predictions on new data:\r\n```python\r\nfrom tensorflow.contrib import predictor\r\npredict_fn = predictor.from_saved_model(servable_model_path)\r\npredictions = predict_fn(trainfeatures1)\r\nprint(predictions)'''\r\n\r\n### Error message:\r\n2017-10-03 21:23:37,175, INFO, Restoring parameters from b'DLModels/1507065657/variables/variables'\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3479-70528b4cfb46> in <module>()\r\n      1 from tensorflow.contrib import predictor\r\n      2 predict_fn = predictor.from_saved_model(servable_model_path)\r\n----> 3 predictions = predict_fn(trainfeatures1)\r\n      4 print(predictions)\r\n\r\n/opt/miniconda/lib/python3.5/site-packages/tensorflow/contrib/predictor/predictor.py in __call__(self, input_dict)\r\n     68     if unexpected_keys:\r\n     69       raise ValueError('Got unexpected keys in input_dict: {}'.format(\r\n---> 70           unexpected_keys))\r\n     71 \r\n     72     feed_dict = {}\r\n\r\nValueError: Got unexpected keys in input_dict: {'feature1', 'feature2', 'feature3'}\r\n", "comments": ["The exported model you make has a signature of (string) -> (predictions). By using build_parsing_serving_input_receiver your exported model will expect a single string tensor containing serialized tf.Examples. \r\n", "@martinwicke -- Can you please give a sample code to do this? I can see that it is expecting a string input (I checked this using the saved_model_cli tool.) But, I don't know how to parse data/features into this required string input. In particular, I am using an input_fn for parsing my data, which makes it even more confusing . Note -- this is what the saved_model_cli tool shows:\r\n\r\n`The given SavedModel SignatureDef contains the following input(s):\r\ninputs['inputs'] tensor_info:\r\n    dtype: DT_STRING\r\n    shape: (-1)\r\n    name: input_example_tensor:0\r\n\r\nThe given SavedModel SignatureDef contains the following output(s):\r\noutputs['outputs'] tensor_info:\r\n    dtype: DT_FLOAT\r\n    shape: (-1)\r\n    name: regression_head/predictions/Identity:0\r\nMethod name is: tensorflow/serving/regress`\r\n\r\n@befowler --  I managed to move a little ahead with the loaded predictor. Since I didn't know how to pass the input_fn I had for my data to the predict_fn obtained using predictor.fromsaved_model(), this is what I did, following your example above (regressor is DNNLinearCombinedRegressor from tf.contrib.learn):\r\n\r\n    from tensorflow.contrib import predictor\r\n    predict_fn = predictor.from_saved_model(servable_model_path)    \r\n    features = regressor._get_features_from_input_fn(input_fn=get_input_fn(X_test, y_test, num_epochs=1, shuffle=False))\r\n    input_dict ={\"inputs\": features}\r\n    predictions = predict_fn(input_dict)`\r\n\r\nNow I get the following error: \r\n` Cannot feed value of shape () for Tensor 'input_example_tensor:0', which has shape '(?,)'`\r\n\r\nIt seems like my predict_fn doesn't know the shape of the input? Is that correct? predict_fn looks like this:\r\n\r\n`SavedModelPredictor with feed tensors {'inputs': <tf.Tensor 'input_example_tensor:0' shape=(?,) dtype=string>} and fetch_tensors {'outputs': <tf.Tensor 'regression_head/predictions/Identity:0' shape=(?,) dtype=float32>}`\r\n\r\nAny suggestions?", "Your input_fn wants a batch of data, but you're only giving it a scalar. This is subtle: reshape your input to shape [1] (form its current []) should do the trick.", "I'm having a similar issue.  Are you suggesting that in this scenario, it needs to be:\r\n\r\n```\r\ninput_dict ={\"inputs\": [features]}\r\n```\r\n Let's assume `feature_spec` and `features` is something like:\r\n\r\n```\r\nfeature_spec = {\r\n'feature1': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None)\r\n'feature2': FixedLenFeature(shape=(1,), dtype=tf.float32, default_value=None),\r\n'feature3': VarLenFeature(dtype=tf.string)}\r\n\r\n\r\nfeatures = {\r\n'feature1': <tf.Tensor 'ParseExample_15/ParseExample:15' shape=(?, 1) dtype=float32>,\r\n'feature2': <tf.Tensor 'ParseExample_15/ParseExample:15' shape=(?, 1) dtype=float32>,\r\n'feature3': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f0ea4626a20>}\r\n```\r\n\r\nFor some arbitrary example:\r\n\r\n```\r\nexample = {'feature1': 1, 'feature2': 2, 'feature3': \"val1\"}\r\n```\r\n\r\nWhat should be passed to the `predict_fn`?", "The model you have says it has `feed tensors {'inputs': <tf.Tensor 'input_example_tensor:0' shape=(?,) dtype=string>}`\r\n\r\nThat means your input must be \r\n\r\n`{'inputs': ['example1', 'example2']}`, which will result in a fed Tensor of shape `(2,)`, compatible with the required `(?,)`.", "I am having a similar problem, where the inputs need to be (shape(?, )) and dtype string. However, my model requires a dtype of float32 to process the data. So whenever I have an input of something like ['12.1', '13.2', '11.223'] i get this error\r\n\r\n\r\nInvalidArgumentError (see above for traceback): Could not parse example input, value: '1.2'\r\n         [[Node: ParseExample/ParseExample = ParseExample[Ndense=3, Nsparse=0, Tdense=[DT_FLOAT, DT_FLOAT, DT_FLOAT], dense_shapes=[[1], [1], [1]], sparse_types=[], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_example_tensor_0_0, ParseExample/ParseExample/names, ParseExample/ParseExample/dense_keys_0, ParseExample/ParseExample/dense_keys_1, ParseExample/ParseExample/dense_keys_2, ParseExample/Const, ParseExample/Const, ParseExample/Const)]]\r\n\r\n. \r\nany help?\r\n", "Hi,\r\n@martinwicke, in your first answer, what do you mean with \"single string tensor containing serialized tf.Examples.\"? I saw some examples of[ tf.Example](https://stackoverflow.com/questions/45900653/tensorflow-how-to-predict-from-a-savedmodel/47645229#47645229), but what does the rest mean? \r\nI try to predict from a exported model I trained with a tensorflow hub module for German: \r\n`      embedded_text_feature_column = hub.text_embedding_column(\r\n        key='sentence',\r\n        module_spec='https://tfhub.dev/google/nnlm-de-dim128/1')\r\n\r\n    #Estimator\r\n    estimator = tf.estimator.DNNClassifier(\r\n        hidden_units=[500, 100],\r\n        feature_columns=[embedded_text_feature_column],\r\n        n_classes=num_of_class,\r\n        optimizer=tf.train.AdagradOptimizer(learning_rate=0.003) )\r\n\r\n...\r\n\r\n    feature_spec = tf.feature_column.make_parse_example_spec([embedded_text_feature_column])\r\n    serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\n\r\n    export_dir_base = self.cfg['model_path']\r\n    servable_model_path = estimator.export_savedmodel(export_dir_base, serving_input_receiver_fn)\r\n\r\n    # Example message for inference\r\n    message = \"Was ist denn los\"\r\n    saved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\r\n    content_tf_list = tf.train.BytesList(value=[str.encode(message)])\r\n    example = tf.train.Example(\r\n            features=tf.train.Features(\r\n                feature={\r\n                    'sentence': tf.train.Feature(\r\n                        bytes_list=content_tf_list\r\n                    )\r\n                }\r\n            )\r\n        )\r\n\r\n    with tf.python_io.TFRecordWriter('the_message.tfrecords') as writer:\r\n        writer.write(example.SerializeToString())\r\n\r\n    reader = tf.TFRecordReader()\r\n    data_path = 'the_message.tfrecords'\r\n    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\r\n    _, serialized_example = reader.read(filename_queue)\r\n    output_dict = saved_model_predictor({'inputs': [serialized_example]})\r\n\r\n`\r\nBut what I get is:\r\n\r\n`Traceback (most recent call last):\r\n....\r\n    output_dict = saved_model_predictor({'inputs': [serialized_example]})\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/contrib/predictor/predictor.py\", line 77, in __call__\r\n    return self._session.run(fetches=self.fetch_tensors, feed_dict=feed_dict)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/Users/dimitrs/anaconda3/envs/pythia/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Unable to get element as bytes.`\r\n\r\nSo the way I introduce the request is not correct?\r\n\r\n=========\r\nEDIT\r\n=========\r\n\r\nIn case somebody does the same like me - reading with tf.TFRecordReader() requires starting a session. That's where this `InternalError: Unable to get element as bytes.` comes from.\r\nFor the above example, simply serialising the the example works: \r\n\r\n```\r\n        message = \"Was ist denn los\"\r\n        saved_model_predictor = predictor.from_saved_model(export_dir=servable_model_path)\r\n        content_tf_list = tf.train.BytesList(value=[message.encode('utf-8')])\r\n        sentence = tf.train.Feature(bytes_list=content_tf_list)\r\n        sentence_dict = {'sentence': sentence}\r\n        features = tf.train.Features(feature=sentence_dict)\r\n        example = tf.train.Example(features=features)\r\n        serialized_example = example.SerializeToString()\r\n        output_dict = saved_model_predictor({'inputs': [serialized_example]})\r\n```", "> I am having a similar problem, where the inputs need to be (shape(?, )) and dtype string. However, my model requires a dtype of float32 to process the data. So whenever I have an input of something like ['12.1', '13.2', '11.223'] i get this error\r\n> \r\n> InvalidArgumentError (see above for traceback): Could not parse example input, value: '1.2'\r\n> [[Node: ParseExample/ParseExample = ParseExample[Ndense=3, Nsparse=0, Tdense=[DT_FLOAT, DT_FLOAT, DT_FLOAT], dense_shapes=[[1], [1], [1]], sparse_types=[], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_example_tensor_0_0, ParseExample/ParseExample/names, ParseExample/ParseExample/dense_keys_0, ParseExample/ParseExample/dense_keys_1, ParseExample/ParseExample/dense_keys_2, ParseExample/Const, ParseExample/Const, ParseExample/Const)]]\r\n> \r\n> .\r\n> any help?\r\n\r\nI also got this type of error. Any further suggestions?", "I was frustrated by a similar blockage. Don't know why `tf` has to make it so complicated. Here's the [http://shzhangji.com/blog/2018/05/14/serve-tensorflow-estimator-with-savedmodel/](link) that will make life a little easier and help understand how to feed data to the model imported using `tf.contrib.predictor.from_saved_model`.\r\n"]}, {"number": 13476, "title": "`tf.rnn.contrib` ops missing in Java API", "body": "I am trying to run an LSTM built with the Python API, exported to to protobuf, and evaluated in Java. After I updated my python scripts to use `tf.contrib.rnn.LSTMBlockCell`, `LSTMBlockFusedCell`, etc cell implementations, the Java portion stopped working and started throwing: \r\n\r\n```\r\norg.tensorflow.TensorFlowException: org.tensorflow.TensorFlowException: Op type not registered 'BlockLSTM' in binary running on xubuntu. Make sure the Op and Kernel are registered in the binary running in this process.\r\n```\r\n\r\nand\r\n\r\n```\r\norg.tensorflow.TensorFlowException: org.tensorflow.TensorFlowException: Op type not registered 'LSTMBlockCell' in binary running on xubuntu. Make sure the Op and Kernel are registered in the binary running in this process.\r\n```\r\n\r\nEvaluating the graph in Python works swimmingly. I didn't see anything on stack overflow. There are a couple tickets on github which might be related: #11847 and #12566 \r\n\r\nTF Version (Java and python): 1.3.0\r\nPython Version: 2.7\r\n", "comments": ["Unfortunately, operations compiled into their own shared library (which happens with operations in contrib) are not yet loadable in Java.\r\n\r\nWe will be making that possible, but no concrete timeline just yet.\r\nI'm closing this as a duplicate of #10454 - the workaround there (which requires building from source) and will post updates there.\r\n\r\nThanks!"]}, {"number": 13475, "title": "Rename set to depset (#13443)", "body": "Fixes #13377", "comments": []}, {"number": 13474, "title": "Fixing a typo in the docs for math_ops.", "body": "", "comments": []}, {"number": 13473, "title": "StochasticTensor strange behavior", "body": "I've noticed strange behavior of StochasticTensor. Please, look at this peace of code: \r\n\r\n    inputs = tf.placeholder(shape=(1, 10, ), name=\"inputs\", dtype=tf.float32)\r\n\r\n    outputs = tf.layers.dense(inputs, units=10, use_bias=False, activation=tf.nn.sigmoid)\r\n    outputs = st.StochasticTensor(distributions.Bernoulli(probs=outputs, dtype=tf.int32))\r\n    outputs = tf.reshape(outputs, shape=(-1, ))\r\n    \r\n    init_op = tf.group(\r\n        tf.global_variables_initializer(),\r\n        tf.local_variables_initializer()\r\n    )\r\n    \r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        x = np.random.rand(1, 10)\r\n        \r\n        tf.set_random_seed(2017)\r\n        z1 = sess.run(outputs, feed_dict={inputs: x})\r\n        \r\n        tf.set_random_seed(2017)\r\n        z2 = sess.run(outputs, feed_dict={inputs: x})\r\n    \r\n    print(z1)\r\n    print(z2)\r\n\r\nAs the result I get:\r\n`\r\n[1 0 1 0 0 1 1 1 0 0]\r\n[0 1 0 1 0 1 0 1 1 1]\r\n`\r\n\r\nBut numpy sampling has another behavior:\r\n\r\n        np.random.seed(2017)\r\n        x = np.random.randint(0, 10, size=10)\r\n\r\n        np.random.seed(2017)\r\n        y = np.random.randint(0, 10, size=10)\r\n\r\n        print(x)\r\n        print(y)\r\n\r\nAnd (again) the result:\r\n`\r\n[9 6 8 2 3 7 8 0 8 6]\r\n[9 6 8 2 3 7 8 0 8 6]\r\n`\r\n\r\nIs it an issue?\r\n\r\nP.S.\r\nTensorflow v1.3.0-rc2-20-g0787eee", "comments": ["Nope, not a bug, you are using Bernoulli distribution in your code which is over 0/1. (ps these kinds of questions are better for stackoverflow)", "I understand :) The problem is that tf.set_random_seed do not affect \"randomness\". In numpy we have another behavior.\r\n\r\n    tf.set_random_seed(2017)\r\n    z1 = sess.run(outputs, feed_dict={inputs: x})\r\n    \r\n    tf.set_random_seed(2017)\r\n    z2 = sess.run(outputs, feed_dict={inputs: x})\r\n\r\nSo, it looks like a bug :)", "cc @ebrevdo  -- how are you supposed to fix the seed of stochastic tensors?\r\n\r\nHere's a self-contained example:\r\n\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES']=''\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.contrib.bayesflow.python.ops import stochastic_tensor_impl\r\nfrom tensorflow.python.ops import distributions\r\nst = stochastic_tensor_impl\r\n\r\ntf.reset_default_graph()\r\n\r\noutputs = st.StochasticTensor(distributions.bernoulli.Bernoulli(probs= [[0.5]*10], dtype=tf.int32))\r\noutputs = tf.reshape(outputs, shape=(-1, ))\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n    \r\nfor i in range(10):\r\n  tf.set_random_seed(2017)\r\n  print(sess.run(outputs))\r\n```", "You'll get the same behavior even if you set a fixed seed:\n\na = tf.random_uniform([3], seed=5)\n\n for i in range(4):\n    tf.set_random_seed(2017)\n    print(s.run(a))\n   ...:\n[ 0.045017    0.30386245  0.43822038]\n[ 0.04852033  0.07889724  0.8716917 ]\n[ 0.98431134  0.39827144  0.79930592]\n[ 0.92832315  0.6284616   0.49420452]\n\nThe behavior that you want would actually require recreating the graph:\n\nfor i in range(4):\n    tf.reset_default_graph()\n    tf.set_random_seed(2017)\n    a = tf.random_uniform([3], seed=5)\n    print(tf.Session().run(a))\n\n[ 0.045017    0.30386245  0.43822038]\n[ 0.045017    0.30386245  0.43822038]\n[ 0.045017    0.30386245  0.43822038]\n[ 0.045017    0.30386245  0.43822038]\n\nand while it's true there's currently no way to set a seed for a\nStochasticTensor, that's not the crux of the problem here.  See the\ndocumentation here\n<https://www.tensorflow.org/api_docs/python/tf/set_random_seed>.\n\n\n\nOn Fri, Oct 6, 2017 at 8:40 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> cc @ebrevdo <https://github.com/ebrevdo> -- how are you supposed to fix\n> the seed of stochastic tensors?\n>\n> Here's a self-contained example:\n>\n> import os\n> os.environ['CUDA_VISIBLE_DEVICES']=''\n> import tensorflow as tf\n> import numpy as np\n> from tensorflow.contrib.bayesflow.python.ops import stochastic_tensor_impl\n> from tensorflow.python.ops import distributions\n> st = stochastic_tensor_impl\n>\n> tf.reset_default_graph()\n>\n> outputs = st.StochasticTensor(distributions.bernoulli.Bernoulli(probs= [[0.5]*10], dtype=tf.int32))\n> outputs = tf.reshape(outputs, shape=(-1, ))\n>\n> sess = tf.Session()\n> sess.run(tf.global_variables_initializer())\n>\n> for i in range(10):\n>   tf.set_random_seed(2017)\n>   print(sess.run(outputs))\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13473#issuecomment-334791518>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwh4Bj_ZMtZFKUJo4qwiy_-68hcAks5spknqgaJpZM4PstOQ>\n> .\n>\n", "Ah right, the seed complexity bites again.\r\n\r\n@denmoroz There are essentially three things affecting randomness -- graph level seed, operation level seed, and operation current state. Setting seeds will guarantee the same pseudo-random sequence of numbers for an operation. If you want to generate a sequence of constants you need to recreate the operation for each number\r\n\r\nThis is simpler in numpy because np.random doesn't create operation with persistent state\r\n\r\n", "@yaroslavvb Thanks! Yeah, it is not easy to understand such behavior cause it differs from the previous experience. Seems like tf.set_random_seed should be renamed :)", "If you want to always control your randomness exactly, check out this module\n<https://www.tensorflow.org/api_docs/python/tf/contrib/stateless>.\n\nOn Mon, Oct 9, 2017 at 6:21 AM, Dzianis Dus <notifications@github.com>\nwrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> Thanks! Yeah, it is not easy\n> to understand such behavior cause it differs from the previous experience.\n> Seems like tf.set_random_seed should be renamed :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13473#issuecomment-335155918>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwIdYgX5aCMP7cBHpoJ4dRg6gMfaks5sqh3egaJpZM4PstOQ>\n> .\n>\n", "Closing this issue because an @ebrevdo provided an explanation of the seed behavior."]}]