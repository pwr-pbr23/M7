[{"number": 1480, "title": "Renaming of input arguments for clarity", "body": "Sorry in advance for being a bit pedantic... I bring this up because of lack of clarity when it comes to `gather`.\n\nRight now there are many names for a generic input:\n- `to_float` and others use `x` instead of `tensor`\n- `shape`, `size`, `rank`, and others use `input` instead of `tensor`\n- `reshape` and others use `tensor`\n- `gather` uses `params` instead of `tensor`\n- `dynamic_partition` uses `data` instead of `tensor`\n- `pack` and `unpack` use `values` and `value` instead of `tensor_list` and `tensor`\n\n`params` suggests that `gather` is special purpose, perhaps for efficiency or some other reason. Is it? If so, should we update the documentation? If not, can we change the name of `params`?\n\nIn addition, I think `value`(s) and `input` should be changed: a `Tensor` isn't a value and `input` is a Python built-in.\n", "comments": ["Unfortunately it's not trivial to change these names, because some code might have used them in explicit named arguments. I agree it would be nicer to have them consistent, so perhaps we could consider a breaking change at some point in the future.\n", "Makes sense. Can you please comment on `gather`?\n", "@rdipietro: Sorry this fell through the cracks.  I agree that `params` is an odd name, but it doesn't really indicate any special purpose: it's hard to think of a less specific name for a parameter than \"params\".\n\nI'm going to close this for now since this bug is too general (it asks for all the parameters to be renamed).  I also don't think it's worth making \"data\", \"params\", \"values\", etc. be consistent, since they are all essentially meaningless names and no one should really be using them as keyword args.\n"]}, {"number": 1479, "title": "virtualenv tests should inherit system site-packages", "body": "This way we do not need compile and install anything. We will test with the same version of packages we have compiled with (and the same as bazel test is running). Also we do not need install gcc and compile pip packages.\n", "comments": ["@jendap I incorporated this change into 1472, along with some related changes. So this PR can be closed if it's okay with you.\n", "Closing this as it is incorporated into https://github.com/tensorflow/tensorflow/pull/1472\n"]}, {"number": 1478, "title": "Hard-coded bash path", "body": "Changed '#!/bin/bash' to '#!/usr/bin/env bash'.\nThere do not exist bash in most Unix system, like bsd family.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n", "The Linux CPU test has failed //tensorflow/tools/docs:gen_docs_test. @iblis17 can you fix it please?\n", "Hi @jendap \nPlease test it again, tnx!\n", "@tensorflow-jenkins test this please\n", "LGTM, thank you @iblis17 \n"]}, {"number": 1477, "title": "CUDA_ERROR_MISALIGNED_ADDRESS running CIFAR10 on CUDA7.5 with cuDNN v3, built from source", "body": "### Environment info\n\nOperating System: Ubuntu 14.04.4 LTS, NVIDIA GeForce 840M\n\nInstalled from sources,  commit hash:\n30b52579f6d66071ac7cdc7179e2c4aae3c9cb88\n### Steps to reproduce\n1. run python  cifar10_train.py\n2. program errorr out after a while\n   3.\n### What have you tried?\n1. restart machine and rerun several times. The same issue.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n2016-03-12 14:02:02.607728: step 4010, loss = 1.22 (421.6 examples/sec; 0.304 sec/batch)\n2016-03-12 14:02:05.731932: step 4020, loss = 1.18 (425.2 examples/sec; 0.301 sec/batch)\n2016-03-12 14:02:08.874431: step 4030, loss = 1.21 (440.1 examples/sec; 0.291 sec/batch)\n2016-03-12 14:02:12.053320: step 4040, loss = 1.30 (367.8 examples/sec; 0.348 sec/batch)\n2016-03-12 14:02:15.304938: step 4050, loss = 1.17 (391.1 examples/sec; 0.327 sec/batch)\n2016-03-12 14:02:18.456706: step 4060, loss = 1.23 (416.5 examples/sec; 0.307 sec/batch)\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8333348 get requests, put_count=8333340 evicted_count=1000 eviction_rate=0.00012 and unsatisfied allocation rate=0.00013296\n2016-03-12 14:02:21.675055: step 4070, loss = 1.19 (424.8 examples/sec; 0.301 sec/batch)\n2016-03-12 14:02:24.810989: step 4080, loss = 1.26 (412.4 examples/sec; 0.310 sec/batch)\n2016-03-12 14:02:28.031250: step 4090, loss = 1.21 (424.3 examples/sec; 0.302 sec/batch)\n2016-03-12 14:02:31.328527: step 4100, loss = 1.13 (396.7 examples/sec; 0.323 sec/batch)\n2016-03-12 14:02:35.028202: step 4110, loss = 1.23 (414.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:02:38.254790: step 4120, loss = 1.35 (364.6 examples/sec; 0.351 sec/batch)\n2016-03-12 14:02:41.484692: step 4130, loss = 1.47 (415.9 examples/sec; 0.308 sec/batch)\n2016-03-12 14:02:44.718649: step 4140, loss = 1.26 (406.7 examples/sec; 0.315 sec/batch)\n2016-03-12 14:02:47.988837: step 4150, loss = 1.07 (408.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:02:51.287844: step 4160, loss = 1.17 (404.1 examples/sec; 0.317 sec/batch)\n2016-03-12 14:02:54.654917: step 4170, loss = 1.33 (404.0 examples/sec; 0.317 sec/batch)\n2016-03-12 14:02:57.782336: step 4180, loss = 1.18 (404.1 examples/sec; 0.317 sec/batch)\n2016-03-12 14:03:01.022658: step 4190, loss = 1.15 (409.9 examples/sec; 0.312 sec/batch)\n2016-03-12 14:03:04.232610: step 4200, loss = 1.24 (365.7 examples/sec; 0.350 sec/batch)\n2016-03-12 14:03:07.854789: step 4210, loss = 1.11 (400.8 examples/sec; 0.319 sec/batch)\n2016-03-12 14:03:11.121573: step 4220, loss = 1.31 (371.6 examples/sec; 0.344 sec/batch)\n2016-03-12 14:03:14.307746: step 4230, loss = 1.46 (382.4 examples/sec; 0.335 sec/batch)\n2016-03-12 14:03:17.566388: step 4240, loss = 1.14 (425.0 examples/sec; 0.301 sec/batch)\n2016-03-12 14:03:20.750270: step 4250, loss = 1.21 (412.2 examples/sec; 0.311 sec/batch)\n2016-03-12 14:03:24.010454: step 4260, loss = 1.16 (401.9 examples/sec; 0.319 sec/batch)\n2016-03-12 14:03:27.315912: step 4270, loss = 1.16 (351.9 examples/sec; 0.364 sec/batch)\n2016-03-12 14:03:30.528823: step 4280, loss = 1.24 (397.8 examples/sec; 0.322 sec/batch)\n2016-03-12 14:03:33.777292: step 4290, loss = 1.45 (393.2 examples/sec; 0.326 sec/batch)\n2016-03-12 14:03:37.022320: step 4300, loss = 1.19 (396.9 examples/sec; 0.323 sec/batch)\n2016-03-12 14:03:40.659591: step 4310, loss = 1.36 (415.7 examples/sec; 0.308 sec/batch)\n2016-03-12 14:03:43.828922: step 4320, loss = 1.18 (396.4 examples/sec; 0.323 sec/batch)\n2016-03-12 14:03:47.227371: step 4330, loss = 0.96 (377.0 examples/sec; 0.340 sec/batch)\n2016-03-12 14:03:50.400040: step 4340, loss = 1.13 (415.3 examples/sec; 0.308 sec/batch)\n2016-03-12 14:03:53.671297: step 4350, loss = 1.12 (405.1 examples/sec; 0.316 sec/batch)\n2016-03-12 14:03:56.815646: step 4360, loss = 1.09 (414.5 examples/sec; 0.309 sec/batch)\n2016-03-12 14:04:00.007053: step 4370, loss = 1.22 (382.2 examples/sec; 0.335 sec/batch)\n2016-03-12 14:04:03.242795: step 4380, loss = 1.19 (414.5 examples/sec; 0.309 sec/batch)\n2016-03-12 14:04:06.424938: step 4390, loss = 1.13 (392.6 examples/sec; 0.326 sec/batch)\n2016-03-12 14:04:09.578756: step 4400, loss = 1.02 (402.1 examples/sec; 0.318 sec/batch)\n2016-03-12 14:04:13.179099: step 4410, loss = 1.34 (409.3 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:16.348951: step 4420, loss = 1.21 (408.8 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:19.599094: step 4430, loss = 1.10 (420.9 examples/sec; 0.304 sec/batch)\n2016-03-12 14:04:22.736091: step 4440, loss = 1.11 (407.3 examples/sec; 0.314 sec/batch)\n2016-03-12 14:04:25.979362: step 4450, loss = 1.27 (389.7 examples/sec; 0.328 sec/batch)\n2016-03-12 14:04:29.173873: step 4460, loss = 1.18 (387.9 examples/sec; 0.330 sec/batch)\n2016-03-12 14:04:32.369984: step 4470, loss = 1.36 (422.4 examples/sec; 0.303 sec/batch)\n2016-03-12 14:04:35.565128: step 4480, loss = 1.19 (409.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:04:38.783202: step 4490, loss = 1.14 (391.8 examples/sec; 0.327 sec/batch)\n2016-03-12 14:04:41.968468: step 4500, loss = 1.20 (389.9 examples/sec; 0.328 sec/batch)\n2016-03-12 14:04:45.529786: step 4510, loss = 1.14 (397.3 examples/sec; 0.322 sec/batch)\n2016-03-12 14:04:48.693115: step 4520, loss = 1.12 (393.7 examples/sec; 0.325 sec/batch)\n2016-03-12 14:04:51.892441: step 4530, loss = 1.25 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:04:55.102551: step 4540, loss = 1.26 (411.5 examples/sec; 0.311 sec/batch)\n2016-03-12 14:04:58.258904: step 4550, loss = 1.27 (395.5 examples/sec; 0.324 sec/batch)\n2016-03-12 14:05:01.485362: step 4560, loss = 1.08 (404.6 examples/sec; 0.316 sec/batch)\n2016-03-12 14:05:04.711979: step 4570, loss = 1.25 (365.1 examples/sec; 0.351 sec/batch)\n2016-03-12 14:05:07.864737: step 4580, loss = 1.31 (390.1 examples/sec; 0.328 sec/batch)\n2016-03-12 14:05:11.075490: step 4590, loss = 1.21 (385.9 examples/sec; 0.332 sec/batch)\n2016-03-12 14:05:14.205806: step 4600, loss = 0.96 (419.8 examples/sec; 0.305 sec/batch)\n2016-03-12 14:05:17.764024: step 4610, loss = 1.16 (419.8 examples/sec; 0.305 sec/batch)\n2016-03-12 14:05:20.992224: step 4620, loss = 1.17 (407.9 examples/sec; 0.314 sec/batch)\n2016-03-12 14:05:24.182559: step 4630, loss = 1.20 (406.1 examples/sec; 0.315 sec/batch)\n2016-03-12 14:05:27.346289: step 4640, loss = 1.30 (421.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:05:30.537761: step 4650, loss = 1.05 (403.8 examples/sec; 0.317 sec/batch)\n2016-03-12 14:05:33.755113: step 4660, loss = 1.10 (394.2 examples/sec; 0.325 sec/batch)\n2016-03-12 14:05:36.904973: step 4670, loss = 1.20 (402.2 examples/sec; 0.318 sec/batch)\n2016-03-12 14:05:40.113832: step 4680, loss = 1.10 (407.4 examples/sec; 0.314 sec/batch)\n2016-03-12 14:05:43.286804: step 4690, loss = 1.05 (406.8 examples/sec; 0.315 sec/batch)\n2016-03-12 14:05:46.492911: step 4700, loss = 1.19 (427.0 examples/sec; 0.300 sec/batch)\n2016-03-12 14:05:50.100880: step 4710, loss = 1.09 (414.9 examples/sec; 0.309 sec/batch)\n2016-03-12 14:05:53.295059: step 4720, loss = 1.16 (403.4 examples/sec; 0.317 sec/batch)\n2016-03-12 14:05:56.461129: step 4730, loss = 1.03 (418.4 examples/sec; 0.306 sec/batch)\n2016-03-12 14:05:59.686046: step 4740, loss = 1.01 (392.7 examples/sec; 0.326 sec/batch)\n2016-03-12 14:06:02.874850: step 4750, loss = 1.11 (403.2 examples/sec; 0.317 sec/batch)\n2016-03-12 14:06:06.063848: step 4760, loss = 1.15 (421.9 examples/sec; 0.303 sec/batch)\n2016-03-12 14:06:09.311339: step 4770, loss = 1.00 (401.5 examples/sec; 0.319 sec/batch)\n2016-03-12 14:06:12.400291: step 4780, loss = 1.03 (412.0 examples/sec; 0.311 sec/batch)\n2016-03-12 14:06:15.594995: step 4790, loss = 1.08 (372.0 examples/sec; 0.344 sec/batch)\n2016-03-12 14:06:18.774854: step 4800, loss = 1.31 (378.7 examples/sec; 0.338 sec/batch)\n2016-03-12 14:06:22.455985: step 4810, loss = 1.17 (359.9 examples/sec; 0.356 sec/batch)\n2016-03-12 14:06:25.655368: step 4820, loss = 1.04 (420.3 examples/sec; 0.305 sec/batch)\n2016-03-12 14:06:28.822332: step 4830, loss = 1.26 (404.5 examples/sec; 0.316 sec/batch)\n2016-03-12 14:06:32.007472: step 4840, loss = 1.01 (411.2 examples/sec; 0.311 sec/batch)\n2016-03-12 14:06:35.283917: step 4850, loss = 1.41 (434.8 examples/sec; 0.294 sec/batch)\n2016-03-12 14:06:38.506687: step 4860, loss = 1.03 (401.3 examples/sec; 0.319 sec/batch)\n2016-03-12 14:06:41.663434: step 4870, loss = 1.12 (395.4 examples/sec; 0.324 sec/batch)\n2016-03-12 14:06:44.828674: step 4880, loss = 1.21 (405.5 examples/sec; 0.316 sec/batch)\n2016-03-12 14:06:48.007878: step 4890, loss = 1.09 (410.4 examples/sec; 0.312 sec/batch)\n2016-03-12 14:06:51.222651: step 4900, loss = 1.00 (396.6 examples/sec; 0.323 sec/batch)\n2016-03-12 14:06:54.860536: step 4910, loss = 1.20 (383.4 examples/sec; 0.334 sec/batch)\n2016-03-12 14:06:58.063615: step 4920, loss = 1.17 (410.9 examples/sec; 0.312 sec/batch)\n2016-03-12 14:07:01.184445: step 4930, loss = 1.05 (397.0 examples/sec; 0.322 sec/batch)\n2016-03-12 14:07:04.394785: step 4940, loss = 1.06 (371.3 examples/sec; 0.345 sec/batch)\n2016-03-12 14:07:07.583337: step 4950, loss = 1.17 (394.5 examples/sec; 0.324 sec/batch)\n2016-03-12 14:07:10.724218: step 4960, loss = 1.09 (397.4 examples/sec; 0.322 sec/batch)\n2016-03-12 14:07:13.931494: step 4970, loss = 1.04 (389.5 examples/sec; 0.329 sec/batch)\n2016-03-12 14:07:17.116672: step 4980, loss = 1.30 (405.9 examples/sec; 0.315 sec/batch)\n2016-03-12 14:07:20.301438: step 4990, loss = 1.26 (427.9 examples/sec; 0.299 sec/batch)\n2016-03-12 14:07:23.464006: step 5000, loss = 1.18 (380.4 examples/sec; 0.337 sec/batch)\n2016-03-12 14:07:27.304553: step 5010, loss = 1.21 (378.2 examples/sec; 0.338 sec/batch)\n2016-03-12 14:07:30.435311: step 5020, loss = 0.96 (420.0 examples/sec; 0.305 sec/batch)\n2016-03-12 14:07:33.525055: step 5030, loss = 1.50 (426.5 examples/sec; 0.300 sec/batch)\n2016-03-12 14:07:36.664561: step 5040, loss = 1.09 (400.9 examples/sec; 0.319 sec/batch)\n2016-03-12 14:07:39.873659: step 5050, loss = 1.09 (422.8 examples/sec; 0.303 sec/batch)\n2016-03-12 14:07:43.060801: step 5060, loss = 1.00 (409.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:07:46.216866: step 5070, loss = 1.22 (424.8 examples/sec; 0.301 sec/batch)\n2016-03-12 14:07:49.424010: step 5080, loss = 1.20 (396.9 examples/sec; 0.323 sec/batch)\n2016-03-12 14:07:52.615564: step 5090, loss = 1.03 (393.3 examples/sec; 0.325 sec/batch)\n2016-03-12 14:07:55.773694: step 5100, loss = 1.43 (421.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:07:59.381470: step 5110, loss = 1.00 (420.5 examples/sec; 0.304 sec/batch)\n2016-03-12 14:08:02.574327: step 5120, loss = 1.08 (412.5 examples/sec; 0.310 sec/batch)\n2016-03-12 14:08:05.758566: step 5130, loss = 0.94 (413.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:08:08.940488: step 5140, loss = 1.07 (414.1 examples/sec; 0.309 sec/batch)\n2016-03-12 14:08:12.128661: step 5150, loss = 1.15 (374.6 examples/sec; 0.342 sec/batch)\n2016-03-12 14:08:15.321906: step 5160, loss = 1.05 (388.2 examples/sec; 0.330 sec/batch)\n2016-03-12 14:08:18.465653: step 5170, loss = 1.05 (394.7 examples/sec; 0.324 sec/batch)\n2016-03-12 14:08:21.695237: step 5180, loss = 1.17 (405.3 examples/sec; 0.316 sec/batch)\n2016-03-12 14:08:24.817479: step 5190, loss = 1.37 (404.4 examples/sec; 0.317 sec/batch)\n2016-03-12 14:08:28.045635: step 5200, loss = 1.06 (411.1 examples/sec; 0.311 sec/batch)\n2016-03-12 14:08:31.625961: step 5210, loss = 0.98 (393.0 examples/sec; 0.326 sec/batch)\n2016-03-12 14:08:34.803401: step 5220, loss = 1.16 (404.8 examples/sec; 0.316 sec/batch)\n2016-03-12 14:08:38.064956: step 5230, loss = 1.22 (386.5 examples/sec; 0.331 sec/batch)\n2016-03-12 14:08:42.036235: step 5240, loss = 0.93 (309.9 examples/sec; 0.413 sec/batch)\n2016-03-12 14:08:45.755935: step 5250, loss = 1.09 (332.2 examples/sec; 0.385 sec/batch)\n2016-03-12 14:08:48.961625: step 5260, loss = 1.24 (413.0 examples/sec; 0.310 sec/batch)\n2016-03-12 14:08:52.615963: step 5270, loss = 1.15 (318.3 examples/sec; 0.402 sec/batch)\n2016-03-12 14:08:56.692628: step 5280, loss = 1.28 (331.9 examples/sec; 0.386 sec/batch)\n2016-03-12 14:09:00.013660: step 5290, loss = 1.18 (429.9 examples/sec; 0.298 sec/batch)\n2016-03-12 14:09:03.394530: step 5300, loss = 0.97 (307.2 examples/sec; 0.417 sec/batch)\n2016-03-12 14:09:07.474397: step 5310, loss = 1.28 (250.3 examples/sec; 0.511 sec/batch)\n2016-03-12 14:09:11.289658: step 5320, loss = 1.03 (307.2 examples/sec; 0.417 sec/batch)\n2016-03-12 14:09:14.478318: step 5330, loss = 0.94 (399.5 examples/sec; 0.320 sec/batch)\n2016-03-12 14:09:17.677547: step 5340, loss = 1.04 (382.3 examples/sec; 0.335 sec/batch)\n2016-03-12 14:09:20.832572: step 5350, loss = 1.08 (433.3 examples/sec; 0.295 sec/batch)\n2016-03-12 14:09:24.024293: step 5360, loss = 1.07 (398.9 examples/sec; 0.321 sec/batch)\n2016-03-12 14:09:27.197269: step 5370, loss = 1.06 (416.3 examples/sec; 0.307 sec/batch)\n2016-03-12 14:09:30.623629: step 5380, loss = 1.04 (324.2 examples/sec; 0.395 sec/batch)\n2016-03-12 14:09:34.399873: step 5390, loss = 1.22 (241.2 examples/sec; 0.531 sec/batch)\n2016-03-12 14:09:38.530928: step 5400, loss = 1.13 (270.8 examples/sec; 0.473 sec/batch)\n2016-03-12 14:09:42.934882: step 5410, loss = 0.90 (268.9 examples/sec; 0.476 sec/batch)\n2016-03-12 14:09:47.006334: step 5420, loss = 0.95 (347.2 examples/sec; 0.369 sec/batch)\n2016-03-12 14:09:50.212403: step 5430, loss = 1.19 (441.3 examples/sec; 0.290 sec/batch)\n2016-03-12 14:09:53.401629: step 5440, loss = 1.11 (417.6 examples/sec; 0.307 sec/batch)\n2016-03-12 14:09:56.592893: step 5450, loss = 1.09 (413.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:09:59.731023: step 5460, loss = 1.14 (392.4 examples/sec; 0.326 sec/batch)\n2016-03-12 14:10:02.961094: step 5470, loss = 1.07 (413.1 examples/sec; 0.310 sec/batch)\n2016-03-12 14:10:06.190783: step 5480, loss = 1.17 (380.0 examples/sec; 0.337 sec/batch)\n2016-03-12 14:10:09.358733: step 5490, loss = 1.06 (390.4 examples/sec; 0.328 sec/batch)\n2016-03-12 14:10:12.525256: step 5500, loss = 1.03 (423.3 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:16.103487: step 5510, loss = 0.98 (418.0 examples/sec; 0.306 sec/batch)\n2016-03-12 14:10:19.302679: step 5520, loss = 1.08 (395.4 examples/sec; 0.324 sec/batch)\n2016-03-12 14:10:22.487560: step 5530, loss = 1.16 (423.4 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:25.632955: step 5540, loss = 1.20 (416.8 examples/sec; 0.307 sec/batch)\n2016-03-12 14:10:28.837518: step 5550, loss = 1.03 (409.2 examples/sec; 0.313 sec/batch)\n2016-03-12 14:10:32.006405: step 5560, loss = 1.06 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:10:35.165191: step 5570, loss = 0.95 (442.7 examples/sec; 0.289 sec/batch)\n2016-03-12 14:10:38.384486: step 5580, loss = 1.17 (439.7 examples/sec; 0.291 sec/batch)\n2016-03-12 14:10:41.575061: step 5590, loss = 1.10 (368.8 examples/sec; 0.347 sec/batch)\n2016-03-12 14:10:44.729184: step 5600, loss = 0.89 (423.4 examples/sec; 0.302 sec/batch)\n2016-03-12 14:10:48.343008: step 5610, loss = 1.09 (416.0 examples/sec; 0.308 sec/batch)\n2016-03-12 14:10:51.464913: step 5620, loss = 1.18 (388.3 examples/sec; 0.330 sec/batch)\n2016-03-12 14:10:54.620341: step 5630, loss = 1.05 (421.4 examples/sec; 0.304 sec/batch)\n2016-03-12 14:10:57.837872: step 5640, loss = 1.17 (419.3 examples/sec; 0.305 sec/batch)\n2016-03-12 14:11:01.090029: step 5650, loss = 1.13 (411.3 examples/sec; 0.311 sec/batch)\n2016-03-12 14:11:04.395310: step 5660, loss = 1.05 (399.1 examples/sec; 0.321 sec/batch)\n2016-03-12 14:11:07.784719: step 5670, loss = 1.05 (366.3 examples/sec; 0.349 sec/batch)\n2016-03-12 14:11:11.604344: step 5680, loss = 0.99 (380.1 examples/sec; 0.337 sec/batch)\n2016-03-12 14:11:15.243982: step 5690, loss = 1.10 (332.7 examples/sec; 0.385 sec/batch)\n2016-03-12 14:11:19.011393: step 5700, loss = 1.14 (388.5 examples/sec; 0.330 sec/batch)\n2016-03-12 14:11:22.897067: step 5710, loss = 1.10 (389.7 examples/sec; 0.328 sec/batch)\n2016-03-12 14:11:26.257571: step 5720, loss = 1.10 (392.0 examples/sec; 0.327 sec/batch)\n2016-03-12 14:11:29.648071: step 5730, loss = 1.05 (378.9 examples/sec; 0.338 sec/batch)\n2016-03-12 14:11:33.027954: step 5740, loss = 1.25 (349.1 examples/sec; 0.367 sec/batch)\n2016-03-12 14:11:36.389940: step 5750, loss = 0.88 (388.9 examples/sec; 0.329 sec/batch)\n2016-03-12 14:11:39.740017: step 5760, loss = 1.15 (386.6 examples/sec; 0.331 sec/batch)\n2016-03-12 14:11:43.111565: step 5770, loss = 0.99 (384.3 examples/sec; 0.333 sec/batch)\n2016-03-12 14:11:46.472003: step 5780, loss = 1.01 (387.7 examples/sec; 0.330 sec/batch)\n2016-03-12 14:11:49.950465: step 5790, loss = 0.89 (363.1 examples/sec; 0.352 sec/batch)\n2016-03-12 14:11:53.508106: step 5800, loss = 1.23 (348.4 examples/sec; 0.367 sec/batch)\n2016-03-12 14:11:57.342506: step 5810, loss = 1.13 (407.0 examples/sec; 0.314 sec/batch)\n2016-03-12 14:12:01.130117: step 5820, loss = 1.17 (409.0 examples/sec; 0.313 sec/batch)\n2016-03-12 14:12:04.352433: step 5830, loss = 1.05 (399.8 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:07.511581: step 5840, loss = 1.05 (435.9 examples/sec; 0.294 sec/batch)\n2016-03-12 14:12:10.730863: step 5850, loss = 1.02 (412.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:12:13.959593: step 5860, loss = 1.10 (355.5 examples/sec; 0.360 sec/batch)\n2016-03-12 14:12:17.241367: step 5870, loss = 1.10 (441.1 examples/sec; 0.290 sec/batch)\n2016-03-12 14:12:20.945846: step 5880, loss = 0.96 (339.6 examples/sec; 0.377 sec/batch)\n2016-03-12 14:12:24.060260: step 5890, loss = 1.14 (398.4 examples/sec; 0.321 sec/batch)\n2016-03-12 14:12:27.196011: step 5900, loss = 1.02 (425.1 examples/sec; 0.301 sec/batch)\n2016-03-12 14:12:30.779291: step 5910, loss = 1.06 (420.0 examples/sec; 0.305 sec/batch)\n2016-03-12 14:12:33.974906: step 5920, loss = 1.10 (403.1 examples/sec; 0.318 sec/batch)\n2016-03-12 14:12:37.188986: step 5930, loss = 1.03 (381.9 examples/sec; 0.335 sec/batch)\n2016-03-12 14:12:40.549341: step 5940, loss = 0.97 (399.6 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:43.776408: step 5950, loss = 1.10 (386.8 examples/sec; 0.331 sec/batch)\n2016-03-12 14:12:46.910332: step 5960, loss = 1.15 (383.1 examples/sec; 0.334 sec/batch)\n2016-03-12 14:12:50.070772: step 5970, loss = 0.97 (436.3 examples/sec; 0.293 sec/batch)\n2016-03-12 14:12:53.283043: step 5980, loss = 0.91 (400.2 examples/sec; 0.320 sec/batch)\n2016-03-12 14:12:57.333740: step 5990, loss = 1.07 (389.0 examples/sec; 0.329 sec/batch)\n2016-03-12 14:13:00.647640: step 6000, loss = 0.97 (371.8 examples/sec; 0.344 sec/batch)\n2016-03-12 14:13:04.837750: step 6010, loss = 1.17 (304.9 examples/sec; 0.420 sec/batch)\n2016-03-12 14:13:08.816024: step 6020, loss = 1.07 (336.6 examples/sec; 0.380 sec/batch)\n2016-03-12 14:13:12.670182: step 6030, loss = 1.00 (260.2 examples/sec; 0.492 sec/batch)\n2016-03-12 14:13:16.635798: step 6040, loss = 0.98 (295.0 examples/sec; 0.434 sec/batch)\n2016-03-12 14:13:20.590717: step 6050, loss = 1.05 (436.9 examples/sec; 0.293 sec/batch)\n2016-03-12 14:13:24.389427: step 6060, loss = 1.12 (303.6 examples/sec; 0.422 sec/batch)\n2016-03-12 14:13:27.919392: step 6070, loss = 1.12 (403.5 examples/sec; 0.317 sec/batch)\n2016-03-12 14:13:31.241030: step 6080, loss = 1.19 (374.7 examples/sec; 0.342 sec/batch)\n2016-03-12 14:13:36.332882: step 6090, loss = 1.19 (342.0 examples/sec; 0.374 sec/batch)\n2016-03-12 14:13:39.823059: step 6100, loss = 0.99 (363.2 examples/sec; 0.352 sec/batch)\n2016-03-12 14:13:44.250455: step 6110, loss = 1.10 (352.2 examples/sec; 0.363 sec/batch)\n2016-03-12 14:13:47.462684: step 6120, loss = 0.93 (429.3 examples/sec; 0.298 sec/batch)\n2016-03-12 14:13:50.718381: step 6130, loss = 1.24 (405.0 examples/sec; 0.316 sec/batch)\n2016-03-12 14:13:53.870993: step 6140, loss = 0.91 (382.4 examples/sec; 0.335 sec/batch)\n2016-03-12 14:13:56.999939: step 6150, loss = 1.09 (415.8 examples/sec; 0.308 sec/batch)\n2016-03-12 14:14:00.147406: step 6160, loss = 1.03 (408.5 examples/sec; 0.313 sec/batch)\n2016-03-12 14:14:03.290188: step 6170, loss = 1.04 (417.4 examples/sec; 0.307 sec/batch)\n2016-03-12 14:14:06.453738: step 6180, loss = 0.88 (422.1 examples/sec; 0.303 sec/batch)\n2016-03-12 14:14:09.616241: step 6190, loss = 0.98 (393.4 examples/sec; 0.325 sec/batch)\n2016-03-12 14:14:12.722891: step 6200, loss = 1.06 (413.3 examples/sec; 0.310 sec/batch)\n2016-03-12 14:14:16.362099: step 6210, loss = 1.01 (433.5 examples/sec; 0.295 sec/batch)\n2016-03-12 14:14:19.472411: step 6220, loss = 1.02 (436.2 examples/sec; 0.293 sec/batch)\n2016-03-12 14:14:22.663895: step 6230, loss = 1.07 (416.9 examples/sec; 0.307 sec/batch)\n2016-03-12 14:14:26.117081: step 6240, loss = 1.17 (386.0 examples/sec; 0.332 sec/batch)\n2016-03-12 14:14:29.231502: step 6250, loss = 0.90 (411.4 examples/sec; 0.311 sec/batch)\n2016-03-12 14:14:32.351659: step 6260, loss = 0.97 (409.6 examples/sec; 0.312 sec/batch)\n2016-03-12 14:14:35.495727: step 6270, loss = 0.98 (401.1 examples/sec; 0.319 sec/batch)\n2016-03-12 14:14:38.677070: step 6280, loss = 1.17 (390.6 examples/sec; 0.328 sec/batch)\n2016-03-12 14:14:41.822706: step 6290, loss = 0.97 (436.9 examples/sec; 0.293 sec/batch)\n2016-03-12 14:14:44.955144: step 6300, loss = 0.91 (442.9 examples/sec; 0.289 sec/batch)\n2016-03-12 14:14:48.526671: step 6310, loss = 0.89 (356.7 examples/sec; 0.359 sec/batch)\n2016-03-12 14:14:51.623469: step 6320, loss = 1.15 (395.3 examples/sec; 0.324 sec/batch)\n2016-03-12 14:14:54.781165: step 6330, loss = 1.25 (387.0 examples/sec; 0.331 sec/batch)\n2016-03-12 14:14:57.893973: step 6340, loss = 1.13 (426.3 examples/sec; 0.300 sec/batch)\n2016-03-12 14:15:01.030509: step 6350, loss = 0.92 (411.7 examples/sec; 0.311 sec/batch)\n2016-03-12 14:15:04.212174: step 6360, loss = 0.96 (414.8 examples/sec; 0.309 sec/batch)\n2016-03-12 14:15:07.610402: step 6370, loss = 1.07 (397.9 examples/sec; 0.322 sec/batch)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1088] could not wait stream on event: CUDA_ERROR_MISALIGNED_ADDRESS\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7012acd00\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_MISALIGNED_ADDRESS; host dst: 0x7fb6276cff80; GPU src: 0x7011c2300; size: 1=0x1\nI tensorflow/stream_executor/stream.cc:826] stream 0x2f7bdd0 did not wait for stream: 0x2f7a8b0\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:193] Unexpected Event status: 1\nI tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7011e2d00\nAborted (core dumped)\n", "comments": ["Did you install from sources?  If so, which commit hash?\n\nIf not, which pip package do you have installed?\n", "Installed from source.  commit hash \n30b52579f6d66071ac7cdc7179e2c4aae3c9cb88\n", "I recently made a change to the gpu allocator, so I might have introduced a bug.  Can you sync to\nf916ae47698c19c0fb3116e471abc39e9b788279 and try again?  I'd like to rule out that recent change from last night.\n", "FYI I just built from HEAD, built with 7.5/cudnnr4, and have so far run cifar10_train.py with no problems for 10000 steps -- I'll keep running it for a while to see if I can reproduce it.\n", "Thanks for the quick response!  After I switched my cuddn from V3 to V4, this problem disappeared. \n", "Interesting -- we should support r3 just fine (assuming you installed it from sources and said cudnn r3 during ./configure).  Thanks for the report, hopefully this is useful for others that run into this.\n", "I was able to train to the following steps. But then on my computer it seems the progress just stopped at step 12680 without update for almost 10 minutes now. Not sure if this is a related problem. one thing noticed is that one CPU usage is almost 100% yet GPU is mostly not busy. Could this be memory issue? My GPU memory is only 2G. \n\nConsole output:\n\n2016-03-12 20:53:11.140474: step 12650, loss = 0.92 (352.5 examples/sec; 0.363 sec/batch)\n2016-03-12 20:53:14.753551: step 12660, loss = 0.77 (368.4 examples/sec; 0.347 sec/batch)\n2016-03-12 20:53:18.339782: step 12670, loss = 0.79 (344.2 examples/sec; 0.372 sec/batch)\n2016-03-12 20:53:21.947387: step 12680, loss = 0.75 (353.8 examples/sec; 0.362 sec/batch)\n(program was stuck here for a long time. After 20 minutes I just killed it). \n\nGPU usage report:\n+------------------------------------------------------+  \n| NVIDIA-SMI 352.79     Driver Version: 352.79         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce 840M        Off  | 0000:04:00.0     Off |                  N/A |\n| N/A   50C    P0    N/A /  N/A |   1915MiB /  2047MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1135    G   /usr/bin/X                                     117MiB |\n|    0      1852    G   compiz                                          43MiB |\n|    0      2310    G   ...s-passed-by-fd --v8-snapshot-passed-by-fd    75MiB |\n|    0     27004    C   python                                        1670MiB |\n+-----------------------------------------------------------------------------+\n"]}, {"number": 1476, "title": "Improvements to Issue template language", "body": "Points people to StackOverflow for non bugs / features.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I already committed this, thanks.\n"]}, {"number": 1475, "title": "Udacity Example 1_notmnist failing to download", "body": "### Environment info\n\nOperating System: Mac OS X\n### Steps to reproduce\n1. `virtualenv venv`\n2. `source venv/bin/activate`\n3. `pip install ipython`\n4. `pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl`\n5. `git clone https://github.com/tensorflow/tensorflow.git`\n6. `cd tensorflow/tensorflow/examples/udacity`\n7. `ipython notebook`\n8. In the browser (Chrome 48.0.2564.116) load `1_notmnist.ipynb`\n9. Run cell 1 (imports)\n10. Run cell 2\n\nCell 2 fails with `Exception: Failed to verify notMNIST_large.tar.gz. Can you get to it with a browser?`. Checked url (`http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz`) with browser, downloads fine.\n\nChecked working directory, found 422 byte `notMNIST_large.tar.gz`. Opened and found this;\n\n```\n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n<html><head>\n<title>403 Forbidden</title>\n</head><body>\n<h1>Forbidden</h1>\n<p>You don't have permission to access /upload/notMNIST/notMNIST_large.tar.gz\non this server.</p>\n<p>Additionally, a 403 Forbidden\nerror was encountered while trying to use an ErrorDocument to handle the request.</p>\n<hr>\n<address>Apache Server at yaroslavvb.com Port 80</address>\n</body></html>\n```\n\nSmall thing, and easy to work around, but would improve our students' experience if they didn't have to debug this.\n", "comments": ["I don't understand, http://yaroslavvb.com/upload/notMNIST/notMNIST_large.tar.gz is there, where does \"403 Forbidden\" error come from?\n", "It's what's downloaded in cell 2 here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb\n", "Thanks for the report, looks like my ISP is finicky about User-Agent strings, rehosting it on Google Cloud, should get upstreamed soon\n", "I can't access the files on the either yarslavvb or google.\n", "Just checked http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz\nand it works for me.\nBut future version will refer to alternative location at\nhttp://commondatastorage.googleapis.com/books1000/notMNIST_small.tar.gz\n\nOn Sun, Mar 13, 2016 at 5:45 PM, Scott Lett notifications@github.com\nwrote:\n\n> I can't access the files on the either yarslavvb or google.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-196090146\n> .\n", "Hi, I only can access the small one, and large one failed.\nAny suggestion?\n\nThanks\n", "I just tried accessing http://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz and it works for me\n", "But I tried to accessing by 1_notmnist.ipynb. It still give me the error.\n\n`url = 'http://commondatastorage.googleapis.com/books1000/'\n\ndef maybe_download(filename, expected_bytes, force=False):\n  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n  if force or not os.path.exists(filename):\n    filename, _ = urlretrieve(url + filename, filename)\n  statinfo = os.stat(filename)\n  if statinfo.st_size == expected_bytes:\n    print('Found and verified', filename)\n  else:\n    raise Exception(\n      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n  return filename\n\ntrain_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\ntest_filename = maybe_download('notMNIST_small.tar.gz', 8458043)`\n", "What is the error?\nOn Apr 8, 2016 5:38 PM, \"KSTseng\" notifications@github.com wrote:\n\n> But I tried to accessing by 1_notmnist.ipynb. It still give me the error.\n> \n> `url = 'http://commondatastorage.googleapis.com/books1000/'\n> \n> def maybe_download(filename, expected_bytes, force=False):\n> \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n> if force or not os.path.exists(filename):\n> filename, _ = urlretrieve(url + filename, filename)\n> statinfo = os.stat(filename)\n> if statinfo.st_size == expected_bytes:\n> print('Found and verified', filename)\n> else:\n> raise Exception(\n> 'Failed to verify ' + filename + '. Can you get to it with a browser?')\n> return filename\n> \n> train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n> test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)`\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-207663388\n", "---\n\nException                                 Traceback (most recent call last)\n<ipython-input-14-61c8ed837e56> in <module>()\n     13   return filename\n     14 \n---> 15 train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n     16 test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n\n<ipython-input-14-61c8ed837e56> in maybe_download(filename, expected_bytes, force)\n     10   else:\n     11     raise Exception(\n---> 12       'Failed to verify ' + filename + '. Can you get to it with a browser?')\n     13   return filename\n     14 \n\nException: Failed to verify notMNIST_large.tar.gz. Can you get to it with a browser?\n", "Looks like there's size mismatch. Can you see what size it is and also try\nwith force=true\nOn Apr 8, 2016 5:56 PM, \"KSTseng\" notifications@github.com wrote:\n\n> ---\n> \n> Exception Traceback (most recent call last)\n> in ()\n> 13 return filename\n> 14\n> ---> 15 train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n> 16 test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n> \n> in maybe_download(filename, expected_bytes, force)\n> 10 else:\n> 11 raise Exception(\n> ---> 12 'Failed to verify ' + filename + '. Can you get to it with a\n> browser?')\n> 13 return filename\n> 14\n> \n> Exception: Failed to verify notMNIST_large.tar.gz. Can you get to it with\n> a browser?\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-207672738\n", "I just re-run the code without modifying with force = true, and it works right now!\nI don't know why.\n\nThank you!\n", "Are you in mainland China?\nOn Apr 8, 2016 6:04 PM, \"KSTseng\" notifications@github.com wrote:\n\n> I just re-run the code without modifying with force = true, and it works\n> right now!\n> I don't know why.\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1475#issuecomment-207675081\n", "No, I am in Taiwan.\n", "Also detected a size mismatch. With force=False the file size for notMNIST_large.tar.gz was coming in at 115793920. Setting force=True fixes the problem. \n\nI'm using the docker image per the instructions [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/README.md#running-the-docker-container-from-the-google-cloud-repository) - is there a file with this name already on the filesystem perhaps?  \n", "i keep getting \n\n[Errno socket error] [Errno -2] Name or service not known\n\naltough I am forcing true.. any idea how to fix this?\n", "Can you download it manually? I just clicked, and the download seems to be happening fine\n", "thank you very much for your quick response. i tried\n\n`docker run --net host -p 8888:8888 -it --rm b.gcr.io/tensorflow-udacity/assignments:0.5.0`\n\nas mentioned https://discussions.udacity.com/t/assignment-1-ioerror-errno-socket-error-errno-2-name-or-service-not-known/46838/3\n\nand it is working now\n\nthx!\n", "May download it fromhere \r\n(http://yaroslavvb.com/upload/notMNIST/)\r\n", "I could see its related to proxy.\r\nin my case http_proxy was set on the shell.  unset that. and It worked like charm.", "@Shaligram Could you elaborate a bit?\r\n\r\nI'm in mainland China, using proxy and having this trouble.\r\n\r\nfail to download `notMNIST_large.tar.gz`\r\n\r\n> URLError: <urlopen error [Errno 60] Operation timed out>\r\n\r\ndownload `notMNIST_large.tar.gz` and `notMNIST_small.tar.gz` from http://yaroslavvb.com/upload/notMNIST/ manually doesn't help (same error).", "downloading from this url  http://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz \r\nworks for me"]}, {"number": 1474, "title": "python3.4 doesn't works well?", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nubuntu 14.04\n\nenvy@ub1404:~/os_pri/github/tensorflow$ python3\nPython 3.4.3 (default, Oct 14 2015, 20:28:29) \n[GCC 4.8.4] on linux\n\nIf installed from sources, provide the commit hash:\nenvy@ub1404:~/os_pri/github/tensorflow$ git log\ncommit 639d48f1f225e5dba865a5eeef513dbe88d6470e\nMerge: aea1bf5 993c77d\nAuthor: Manjunath Kudlur keveman@gmail.com\nDate:   Mon Feb 22 21:28:30 2016 -0800\n\n```\nMerge pull request #1207 from dongjoon-hyun/avoid_copying_invalid_large_constant_data.\n```\n### Steps to reproduce\n\n1.\nimport tensorflow\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nenvy@ub1404:~/os_pri/github/tensorflow-deepq$ PYTHONPATH=~/os_pri/github/tensorflow-deepq:$PYTHONPATH python3 \nPython 3.4.3 (default, Oct 14 2015, 20:28:29) \n[GCC 4.8.4] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/__init__.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/__init__.py\", line 49, in <module>\n> > >     from tensorflow import contrib\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/contrib/__init__.py\", line 23, in <module>\n> > >     from tensorflow.contrib import layers\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/contrib/layers/__init__.py\", line 67, in <module>\n> > >     from tensorflow.contrib.layers.python.framework.tensor_util import *\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/contrib/layers/python/framework/tensor_util.py\", line 21, in <module>\n> > >     from tensorflow.python.framework.ops import Tensor\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/framework/ops.py\", line 39, in <module>\n> > >     from tensorflow.python.framework import versions\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/framework/versions.py\", line 22, in <module>\n> > >     from tensorflow.python import pywrap_tensorflow\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n> > >     _pywrap_tensorflow = swig_import_helper()\n> > >   File \"/home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n> > >     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n> > >   File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\n> > >     return load_dynamic(name, filename, file)\n> > > ImportError: /home/envy/os_pri/github/tensorflow/_python3_build/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyClass_Type\n", "comments": ["This problem doesn't seem to be happening with the [nightly build for Python 3](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-py3-none-any.whl), and it looks like you're using an old snapshot of the tree from February 22nd. Can you try again with the latest HEAD?\n", "I'm going to close this for now, since we can't reproduce. Let us know if you still have a problem with the up-to-date code.\n"]}, {"number": 1473, "title": "Tensorflow GPU version support with skflow", "body": "![error](https://cloud.githubusercontent.com/assets/10511526/13720964/02b25a4c-e83e-11e5-9a03-f4250d7c8ab2.png)\n![version](https://cloud.githubusercontent.com/assets/10511526/13720966/0807d9b8-e83e-11e5-83c8-26699934e698.png)\n\nI have been trying to execute text_classification.py example of skflow but it is generating an error. Please see the attached document for an error as well as the version of tensorflow and the skflow.\n\n**the error is tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'train/update_words/words_embeddings/Assign_1': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'**\n", "comments": ["Try updating your version of TensorFlow: 0.5 was the original release and there have been a ton of bug fixes and improvements since then.\n", "how to update gpu version of tensorflow\n", "Try reading through: https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#download-and-setup\n", "(Feel free to comment if, after upgrading, you still have the problem, and we can debug further!)\n"]}, {"number": 1472, "title": "pip test-on-install changes for skflow", "body": "1) Expanding the search scope for Python unit tests in order to include\nthe new tests in python/skflow/tests\n2) By default, installing scipy and sklearn in virtualenv for the\naforementioned Python unit tests\n3) Enabling caching of virtualenv directory to speed up pip test-on-install\n", "comments": ["@jendap's comments and questions addressed. Rebased and squashed. Ready to merge. \n", "I added an additional environment variable switch \"TF_BUILD_NO_CACHING_VIRTUALENV\" to allow removal of cached virtualenv folders if necessary. \n", "@tensorflow-jenkins test this please\n"]}, {"number": 1471, "title": "Android Demo Building Issue", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nUbuntu 14.04\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. I have followed step by step the Android Demo installation\n### What have you tried?\n1. changing the ndk version\n### Logs or other output that would be helpful\n\nERROR: /home/islamoc/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar ... (remaining 14 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nMar 12, 2016 4:13:36 AM com.google.devtools.build.android.AndroidResourceProcessingAction main\nSEVERE: Error during merging resources\nError: Failed to run command:\n        bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp4328338078871950922/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp4328338078871950922/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n        1\nOutput:\n        bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\n```\n    at com.android.ide.common.res2.MergeWriter.end(MergeWriter.java:54)\n    at com.android.ide.common.res2.MergedResourceWriter.end(MergedResourceWriter.java:113)\n    at com.android.ide.common.res2.DataMerger.mergeData(DataMerger.java:291)\n    at com.android.ide.common.res2.ResourceMerger.mergeData(ResourceMerger.java:48)\n    at com.google.devtools.build.android.AndroidResourceProcessor.mergeData(AndroidResourceProcessor.java:390)\n    at com.google.devtools.build.android.AndroidResourceProcessingAction.main(AndroidResourceProcessingAction.java:321)\n```\n\nCaused by: com.android.ide.common.internal.LoggedErrorException: Failed to run command:\n        bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp4328338078871950922/tmp-deduplicated/tensorflow/examples/android/res/drawable-hdpi/ic_launcher.png -o /tmp/android_resources_tmp4328338078871950922/merged_resources/drawable-hdpi-v4/ic_launcher.png\nError Code:\n        1\nOutput:\n        bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\n```\n    at com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:123)\n    at com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:96)\n    at com.android.ide.common.internal.AaptCruncher.crunchPng(AaptCruncher.java:58)\n    at com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:188)\n    at com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:139)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 369.392s, Critical Path: 319.96s\n", "comments": ["Does it not work on Ubuntu 15.10 ?\n", "No idea I could not build it on Ubuntu 14.04 I did not test it on 15.10\nI think it is dependencies problem\n", "You may need to run sdk_path/tools/android and ensure that you have the 23.0.1 Android build tools installed. Alternatively, if you already have a version >= 21, you can try changing the setting in WORKSPACE and point to that.\n", "Build tools 23.0.1 is installed normally\n\n```\n/home/islamoc/android-sdk-linux/build-tools# ls\n23.0.1\n```\n", "After updating the SDK I got this error\n\n```\nERROR: /home/islamoc/tensorflow/tensorflow/core/BUILD:647:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 63 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\narm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <http://source.android.com/source/report-bugs.html> for instructions.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n```\n", "What does it say if you try building with --verbose_failures?\n", "This it give the same error again\n\n```\nERROR: /home/islamoc/tensorflow/tensorflow/examples/android/BUILD:65:1: Processing resources failed: resources_processor failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar external/androidsdk/tools/support/annotations.jar --androidJar external/androidsdk/platforms/android-23/android.jar --primaryData tensorflow/examples/android/res:tensorflow/examples/android/assets:tensorflow/examples/android/AndroidManifest.xml --srcJarOutput bazel-out/local_linux-fastbuild/bin/tensorflow/examples/android/tensorflow_demo.srcjar --proguardOutput bazel-out/local_linux-fastbuild/bin/tensorflow/examples/android/proguard/tensorflow_demo/_tensorflow_demo_proguard.cfg --packagePath bazel-out/local_linux-fastbuild/bin/tensorflow/examples/android/tensorflow_demo.ap_ --debug --packageForR org.tensorflow.demo): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: resources_processor failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/android/resources_processor --buildToolsVersion 23.0.1 --aapt bazel-out/host/bin/external/androidsdk/aapt_binary --annotationJar external/androidsdk/tools/support/annotations.jar --androidJar external/androidsdk/platforms/android-23/android.jar --primaryData tensorflow/examples/android/res:tensorflow/examples/android/assets:tensorflow/examples/android/AndroidManifest.xml --srcJarOutput bazel-out/local_linux-fastbuild/bin/tensorflow/examples/android/tensorflow_demo.srcjar --proguardOutput bazel-out/local_linux-fastbuild/bin/tensorflow/examples/android/proguard/tensorflow_demo/_tensorflow_demo_proguard.cfg --packagePath bazel-out/local_linux-fastbuild/bin/tensorflow/examples/android/tensorflow_demo.ap_ --debug --packageForR org.tensorflow.demo): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nError: bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\nMar 16, 2016 12:22:57 PM com.google.devtools.build.android.AndroidResourceProcessingAction main\nSEVERE: Error during merging resources\nError: Failed to run command:\n        bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp4150814921996220848/tmp-deduplicated/tensorflow/examples/android/res/drawable-mdpi/ic_launcher.png -o /tmp/android_resources_tmp4150814921996220848/merged_resources/drawable-mdpi-v4/ic_launcher.png\nError Code:\n        1\nOutput:\n        bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\n        at com.android.ide.common.res2.MergeWriter.end(MergeWriter.java:54)\n        at com.android.ide.common.res2.MergedResourceWriter.end(MergedResourceWriter.java:113)\n        at com.android.ide.common.res2.DataMerger.mergeData(DataMerger.java:291)\n        at com.android.ide.common.res2.ResourceMerger.mergeData(ResourceMerger.java:48)\n        at com.google.devtools.build.android.AndroidResourceProcessor.mergeData(AndroidResourceProcessor.java:390)\n        at com.google.devtools.build.android.AndroidResourceProcessingAction.main(AndroidResourceProcessingAction.java:321)\nCaused by: com.android.ide.common.internal.LoggedErrorException: Failed to run command:\n        bazel-out/host/bin/external/androidsdk/aapt_binary s -i /tmp/android_resources_tmp4150814921996220848/tmp-deduplicated/tensorflow/examples/android/res/drawable-mdpi/ic_launcher.png -o /tmp/android_resources_tmp4150814921996220848/merged_resources/drawable-mdpi-v4/ic_launcher.png\nError Code:\n        1\nOutput:\n        bazel-out/host/bin/external/androidsdk/aapt_binary: line 3: /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary.runfiles/external/androidsdk/build-tools/23.0.1/aapt: No such file or directory\n\n        at com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:123)\n        at com.android.ide.common.internal.CommandLineRunner.runCmdLine(CommandLineRunner.java:96)\n        at com.android.ide.common.internal.AaptCruncher.crunchPng(AaptCruncher.java:58)\n        at com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:188)\n        at com.android.ide.common.res2.MergedResourceWriter$1.call(MergedResourceWriter.java:139)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\nSlow read: a 376492-byte read from /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/_objs/protos_all_cc/tensorflow/core/framework/step_stats.pb.o took 7615ms.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 61.284s, Critical Path: 46.00s\n```\n", "Here's the contents of my aapt_binary script:\n#!/bin/bash -eu\nSDK=${0}.runfiles/external/androidsdk\nexec ${SDK}/build-tools/23.0.1/aapt\n\nI'm not sure exactly what's going on, but it appears that on your system, \"${0}\" (the path to the current bash executable) is defined to be: `/root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow/bazel-out/host/bin/external/androidsdk/aapt_binary`\n\nYou don't appear to be running as root given the path to your build file (`/home/islamoc/tensorflow/tensorflow/examples/android/BUILD`), so it's not surprising that the aapt file can't be found due to not existing or not having permissions. Are you running bazel with su/sudo, or have you given the bazel any special permissions or anything like that could cause such a mixup?\n", "The problem was in my system it is 64bit the aapt bin was 32bit I fixed the problem now I m getting this\n\n```\nERROR: /home/islamoc/tensorflow/tensorflow/core/BUILD:647:1: C++ compilation of rule '//tensorflow/core:android_tensorflow_lib' failed: arm-linux-androideabi-gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -iquote . -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles -iquote external/re2 -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/eigen_archive -isystem external/re2 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -isystem external/bazel_tools/tools/cpp/gcc3 -isystem google/protobuf/src -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/google/protobuf/src -isystem third_party/eigen3 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-db7b61411772 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/eigen_archive/eigen-eigen-db7b61411772 '-mfpu=neon' '-std=c++11' '-DMIN_LOG_LEVEL=0' -DTF_LEAN_BINARY -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/cwise_op_sub.o' -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/cwise_op_sub.d -c tensorflow/core/kernels/cwise_op_sub.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/cwise_op_sub.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4: arm-linux-androideabi-gcc failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/a492118e4b53c9510c58595e273faae6/tensorflow && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -iquote . -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles -iquote external/re2 -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/eigen_archive -isystem external/re2 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/re2 -isystem external/bazel_tools/tools/cpp/gcc3 -isystem google/protobuf/src -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/google/protobuf/src -isystem third_party/eigen3 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-db7b61411772 -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/eigen_archive/eigen-eigen-db7b61411772 '-mfpu=neon' '-std=c++11' '-DMIN_LOG_LEVEL=0' -DTF_LEAN_BINARY -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/cwise_op_sub.o' -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/cwise_op_sub.d -c tensorflow/core/kernels/cwise_op_sub.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/cwise_op_sub.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\narm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <http://source.android.com/source/report-bugs.html> for instructions.\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 208.522s, Critical Path: 206.77s\n```\n", "Status 4 is an OOM issue. Try increasing your swapspace, giving bazel more memory/decreasing memory usage, or running fewer simultaneous jobs.\n\nSee http://stackoverflow.com/questions/34382360/decrease-bazel-memory-usage?rq=1 for more info.\n", "I will try those recommendations and get back to you thank you for the help\n", "@andrewharp Thank you it worked :D with 2 jobs\n", "Awesome! Glad that did the trick :)\n", "@andrewharp one more question please can we build the project for Android 4.4.2 ?\n", "We use the camera2 API in the official repo, which requires API 21+.\n\nHowever, @hamidb has a fork of TF using the old android.hardware.Camera that should run on devices api 15+. See https://github.com/hamidb/tensorflow/blob/api20/tensorflow/examples/android/src/org/tensorflow/demo/CameraConnectionFragment.java for the relevant changes.\n", "@islamoc  hi since you i have build android on ubuntu 14.04 it would be a great help if u you just give a brief steps on building it with its   the dependent versions . I have struggling from 1 week and no were near close to build the demo app "]}, {"number": 1470, "title": "Error when run optimizer.compute_gradients", "body": "I'm trying to do training with multiple threads\n\nI tried to run optimizer.minimize() in each threads but it didn't work because threads overwrited each other's weight updates. So I'm trying to stack computed gradients in each threads and apply mean or sum of them in another thread.\nSo I made a code like\n\nw1 = tf.Variable(tf.random_normal([shape], stddev=0.01))\nb1 = tf.Variable(tf.constant(0.1, shape=[filters]))\nrmsprop = tf.train.RMSPropOptimizer(lr,decay,0.0,eps)\ngrads = rmsprop.compute_gradients(cost,[w1,b1])\napply_grads = rmsprop.apply_gradients(grads)\n\nIf i run apply_grads, it works fine. but when i run grads to get gradients, error occurs like below.\n`[(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)] of [(<tf.Tensor ~~ shape=~~ dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x7f6da5fd7350>)]  has invalid type <type \"list\">, must be a string or Tensor. (Can not convert a list into a Tensor or Operation.)`\n\nHow can I get gradients for optimizer?\n\nOr is there a efficient way to do training with multiple threads?\n", "comments": ["Are you passing `grads` directly to `Session.run()`? I believe that will contain a list of (gradient, variable) tuples, which needs to converted to a list a tensors in order to pass it to `Session.run()`. For example, you could do:\n\n``` python\ngrads_and_vars = rmsprop.compute_gradients(cost, [w1, b1])\ngrads = [g for g, _ in grads_and_vars]\nvars = [v for _, v in grads_and_vars]\ngrad_vals = sess.run(grads)\nvar_to_grad_val_map = {v.name: val for (v, val) in zip(vars, grad_vals)}\n```\n", "I'm facing the same problem. Fetching the gradient values with session.run() always result in Type Error - which was mentioned by @gliese581gg .\nI've tried various terms, including code snippet @mrry suggested above, but not so effective.\n", "I had managed this problem. It was caused by using deprecated version of Tensorflow. I updated my experimental environment, and now it works fine.\n", "Now I see the problem. With your program ended without freeing related resources, Variables with trainable=True will remain and be called by optimizer's compute_gradients() unless you specify Variables you want to know about. And that will result in returning non value objects(which is NonType), for those remaining Variables has nothing to do with current program.\n\nThis can be avoided in various ways - like calling tf.reset_default_graph() - , but still, I believe this is non-intentional bug, and should be fixed soon enough.\n", "When I get the gradient of the InceptionV4 model, I get the same problem. The value of None appears in each BatchNorm layer, I'm not sure if this is an bug. Or, the BatchNorm layer would have no gradient."]}, {"number": 1469, "title": "Fix typos", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1468, "title": "Android Camera Demo: Android NDK r11 can't be compiled by bazel after modified the WORKSPACE", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\nOS X EI Capitan version 10.11.3\n\nIf installed from sources, provide the commit hash:\n13ea3ca91ba5aecab6f21acc14b9cb6a9afa8630\n### Steps to reproduce\n1. Build TensorFlow without uncommenting the Android SDK and NDK parts successfully.\n2. We can \n\n```\nimport tensorflow \n```\n\nin python environment after step 1 and following installation steps.\n4. Download SDK and NDK from what https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md suggested.\n3. Uncomment the Android SDK and NDK repository in WORKSPACE and change the path.\n4. Rebuild using \n\n```\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n```\n\nthe console returns the error,\n\n``` shell\nERROR: .../tensorflow/WORKSPACE:10:1: no such package '@androidndk//': Could not read RELEASE.TXT in Android NDK: /private/var/tmp/.../49fff0428.../external/androidndk/ndk/RELEASE.TXT (No such file or directory) and referenced by '//external:android/crosstool'.\n```\n### What have you tried?\n1. Commenting NDK repository and leave the SDK in TensorFlow WORKSPACE, build using \n\n```\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n```\n\npassed and without error.\n\n2.Finding that Downloaded NDK and NDK in /private/var/tmp/.../49fff0428.../external/androidndk/ndk are the same. And there are no RELEASE.TXT in in directory.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n``` shell\nERROR: .../tensorflow/WORKSPACE:10:1: no such package '@androidndk//': Could not read RELEASE.TXT in Android NDK: /private/var/tmp/.../49fff0428.../external/androidndk/ndk/RELEASE.TXT (No such file or directory) and referenced by '//external:android/crosstool'.\n```\n\nIs this error due to the codes or my building processes?\n", "comments": ["I would try downgrading to the previous version of Android NDK (r10e). Here's the Mac OS X link: https://dl.google.com/android/ndk/android-ndk-r10e-darwin-x86.bin\n\nI ran into similar issues using the latest version of Android NDK (r11) which no longer contains `RELEASE.TXT`. Manually creating that file with `echo r11 > RELEASE.TXT` fixed the immediate problem, but I ran into other bazel hard dependencies on r10e.\n\nFor linux users, here are the r10e 32/64 links, no longer listed on the Android NDK website but still accessible:\nhttps://dl.google.com/android/ndk/android-ndk-r10e-linux-x86_64.bin\nhttps://dl.google.com/android/ndk/android-ndk-r10e-linux-x86.bin\n(Thanks [Arch AUR](https://aur.archlinux.org/cgit/aur.git/commit/?h=android-ndk&id=988b6411ee099a762adf981004af86616fabedae))\n", "@moonboots \nThank you for your detailed answers, it really helps me. \nAndroid NDK (r10e) link for Mac OS X is: \nhttps://dl.google.com/android/repository/android-ndk-r10e-darwin-x86_64.zip\nWe should also install Android SDK build tools version 23.0.1 via Android SDK manager to build the android example.\n"]}, {"number": 1467, "title": "Allow suppressing log output", "body": "Two recent requests recently to suppress logging on console [1](http://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information), [2](http://stackoverflow.com/questions/35869137/avoid-tensorflow-print-on-standard-error)\n\nSince TensorFlow uses Google logging library standard flags should work, ie \"export GLOG_logtostderr=0\" should turn off logging to console. However that doesn't work, I suspect these flags are explicitly overriden in code somewhere\n", "comments": ["We don't use the google logging library -- our python flags are just a wrapper over argparse, and we have no C++ flags at all.  There's an open issue already covering this, let me find it.\n", "Duplicate of #1258\n"]}, {"number": 1466, "title": "Tf 0.7.1", "body": "Merging in tensorflow 0.7.1\n\nThis will change a lot of functionality. \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "My apologies. This was an accidental request. \n"]}, {"number": 1465, "title": "Cholesky backpropagation", "body": "This implements a gradient op for the Cholesky op as requested and discussed in #367 .\n\nThe code was written by myself and Dr. James Henman (user name @jameshensman ).\n\nThe implementation is a blocked cholesky backpropagation as advocated by Dr. Iain Murray in this recent paper http://arxiv.org/abs/1602.07527\n\nNew unit tests are included. \n\nA lot of work has gone into the implementation of the algorithm and I therefore believe the code has value to the community. Less work so far has gone in to tightly integrating it with TensorFlow code base and I am looking for help and advice from the TensorFlow team. As such it should be regarded as a work in progress.  @martinwicke suggested the best path forward was to submit this pull request. \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@rmlarsen: Want to take a look? \n", "I signed the CLA. \n", "I signed the CLA. All authors have now signed. \n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thank Alex, I'm reviewing the code now.\n\nOn Mon, Mar 14, 2016 at 1:09 AM, Alexander G. de G. Matthews <\nnotifications@github.com> wrote:\n\n> I signed the CLA. All authors have now signed.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-196192589\n> .\n", "Hi Alex & James,\n\nThanks for your contribution, this will be really nice to have in\nTensorFlow. I left a number of specific comments on the pull request.\n\nBest,\n   Rasmus\n\nOn Mon, Mar 14, 2016 at 8:49 AM, Rasmus Larsen rmlarsen@google.com wrote:\n\n> Thank Alex, I'm reviewing the code now.\n> \n> On Mon, Mar 14, 2016 at 1:09 AM, Alexander G. de G. Matthews <\n> notifications@github.com> wrote:\n> \n> > I signed the CLA. All authors have now signed.\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-196192589\n> > .\n", "Hi Rasmus,\nThanks for your comments that is a big help. We'll get on it!\nBest,\nAlex \n", "Hi @rmlarsen,\nI am about half way through your comments. I will keep you posted and let you know when I am done.\nAlex \n", "Sounds great. I'm back from a short vacation, so please let me know when the code is ready for another look.\n\nRasmus\n", "Hi @rmlarsen,\nI believe I have now addressed all of your comments so I think it would be good for you to have another look please. \n\nIn terms of complying with the coding standard I ran the lint python script they suggest that can be found here https://raw.githubusercontent.com/google/styleguide/gh-pages/cpplint/cpplint.py\n\nThank you for your lucid comments that I feel have definitely improved both the style and content of the code. \n\nIt is a public holiday here in the UK tomorrow (Friday) and Monday but I will be back at my desk on Tuesday. \n\nAlex \n", "Hi @rmlarsen,\nMy apologies for the slow reply. Thank you again for your comments. I've left some replies and some questions at the relevant lines. I will be looking working on defining local variables in the two main loops while I wait for your reply. Thanks again,\nAlex \n", "Would be great if you could make this batch cholesky grad, for which cholesky grad is a simple special case requiring a reshape.\n", "Hi @ebrevdo , I think it is best if that is done in a separate PR. This one is already getting rather large and I'm mindful of specification creep. Good suggestion though. \n", "Hi Alex, I think the code looks good now. I had a few nits about the formatting of the python code. I agree that implementing batching can be done in a followup PR.\n", "Hi @rmlarsen,\nI have added the longer comment about _bar you asked for. \n\n@jameshensman has spent some time using flake8 on the unit test to ensure pep8 compliance which also covers your points on this. It turns out some of the rest of the script wasn't pep8 compliant so he fixed that too. \n\nI you agree, I think that therefore means we are good to go. What happens next? \n\nThanks for all your help, \n@alexggmatthews \n", "On Tue, Apr 5, 2016 at 1:19 AM, Alexander G. de G. Matthews <\nnotifications@github.com> wrote:\n\n> Hi @rmlarsen https://github.com/rmlarsen,\n> I have added the longer comment about _bar you asked for.\n> \n> @jameshensman https://github.com/jameshensman has spent some time using\n> flake8 on the unit test to ensure pep8 compliance which also covers your\n> points on this. It turns out some of the rest of the script wasn't pep8\n> compliant so he fixed that too.\n> \n> I you agree, I think that therefore means we are good to go. What happens\n> next?\n\nThanks for all your work - it looks nice. I will test your PR internally\nand then we can to merge it. But first, it would be preferable if you could\nsquash your work into a single commit. You can either use the script\nat http://rebaseandsqua.sh/\nor follow these instructions: https://davidwalsh.name/squash-commits-git\n\nBest,\n   Rasmus\n\nThanks for all your help,\n\n> @alexggmatthews https://github.com/alexggmatthews\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-205716282\n", "Jenkins, test this please.\n", "Hi Rasmus, I agree it looks nice. Very satisfying. Thanks for all your work too. I've done the squash so we now have just the one commit as requested. Do we need to keep merging master to this branch while we wait for Jenkins so that it doesn't get out of date or is that dealt with automatically? \nAlex\n", "Hi Alex,\n\nI suspect you have to merge manually for now. Perhaps @martinwicke knows?\n", "Now that the squash/merge is done, we can test it once more and then merge.\n@tensorflow-jenkins: test this please.\n\n(alternatively, we could have squashed/merged ourselves).\n", "Looks like some of the tests are timing out -- can you figure out which test is taking so long and try to reduce the entire test suite to run in < 30 seconds?\n", "Also, the mac test failed with:\n\nINFO: From Testing //tensorflow/python:cholesky_op_test:\n==================== Test output for //tensorflow/python:cholesky_op_test:\n\n# .F..........\n\n## FAIL: testSmallMatrices (**main**.CholeskyGradTest)\n\nTraceback (most recent call last):\n  File \"/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/python/cholesky_op_test.runfiles/tensorflow/python/kernel_tests/cholesky_op_test.py\", line 100, in testSmallMatrices\n    self.runFiniteDifferences(shapes)\n  File \"/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/python/cholesky_op_test.runfiles/tensorflow/python/kernel_tests/cholesky_op_test.py\", line 138, in runFiniteDifferences\n    self.assertLess(error, 1e-3)\nAssertionError: 0.0011898875 not less than 0.001\n", "I am looking into the mac problem but I don't have one in my laboratory so I am going to liaise with @jameshensman . I guess there must be slight numerical differences between the two. \n", "Hi Alex, Given that the OS-X test fails due to a tiny numerical discrepancy, I'd say that increasing the test tolerance by a factor of 2, say, is an acceptable fix.\n", "OK @vrv @rmlarsen we should be good to go again. Please can you re-run the tests. \n", "@tensorflow-jenkins: test this please\n", "@vrv @rmlarsen @jameshensman it looks like it is all working. \n", "Woohoo!\n", "Excellent! \n", "Hi Alex,\n\nWould you have time & interest in extending the Cholesky gradient code to\nthe batch case?\n\nThanks,\n   Rasmus\n\nOn Fri, Apr 8, 2016 at 1:01 AM, Alexander G. de G. Matthews <\nnotifications@github.com> wrote:\n\n> Excellent!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1465#issuecomment-207289948\n", "Hi Rasmus, it does sound interesting but I'm now very near finishing my PhD thesis so I think I will have to mainly concentrate on that for the time being. I do have a small PR #1891 that I would be happy to help through. Best, Alex\n"]}, {"number": 1464, "title": "Feature request: make TensorBoard side bar resizable?", "body": "There's often a bunch of unused space on the right side of the TensorBoard interface, and if we have long directory names on the left (for example listing parameters during validation), they end up getting wrapped to multiple lines. Right now it's possible to adjust the sidebar width with Chrome's developer tools, but it seems natural for it to be resizable in the first place.\n", "comments": ["@danmane: Want to comment?  I think we'd be happy to accept patches for this. \n", "This is something I'd like to fix, but not super high priority. Patches would, indeed, be welcome. \n", "I migrated this issue to tensorflow/tensorboard#62 because TensorBoard has moved into its own repo. Lets continue discussion there.", "> There's often a bunch of unused space on the right side of the TensorBoard interface, and if we have long directory names on the left (for example listing parameters during validation), they end up getting wrapped to multiple lines. Right now it's possible to adjust the sidebar width with Chrome's developer tools, but it seems natural for it to be resizable in the first place.\r\n\r\napologies for the obvious untimely reply in this thread. However, since this has not been implemented in tensorboard yet, I would like to try your method with Chrome's developer tools. Would you be so kind to share how that would be done plase?\r\n\r\nthank you very much\r\n"]}, {"number": 1463, "title": "Can I specify other gpu device not first one  in notebook?", "body": "Can I specify other gpu device not first one  in notebook?thanks!\n", "comments": ["I run this before creating a `Session`:\n\n```\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = gpu\n```\n\n`gpu` is a string. '0' for GPU 0, '0,2' for GPUs 0 and 2, etc.\n\nNote if you do this then TensorFlow's '/gpu:0' will refer to the first in the list, not to the first GPU on the machine.\n", "This is a stackoverflow question, not a Github issue.\n"]}, {"number": 1462, "title": "added shortcuts to access tensorshape dims and ndims", "body": "for less typing.  \n**len** seems to cause problems though, will look into it.\n", "comments": ["Can one of the admins verify this patch?\n", "@mrry: Was there some plan to change `get_shape()` to be less verbose as well?\n", "Yes, but it depends on fixing all of the legacy code that uses the vestigial `Tensor.shape` property internally. Assuming this eventually happens, I'd be inclined to add `Tensor.shape` as an alias for `Tensor.get_shape()`, instead of accepting this PR.\n", "@mrry: What does the vestigial thing do?  I can't find it in the code.\n", "@mrry Are there any reasons to have convenience accessors on C++ but not python?\n", "It does seem like once we rename `get_shape()` to `shape`, these aliases fit naturally into the picture.\n", "Ok, looks like we have some internal work to do instead.  Thanks!\n", "(We can revive this later when it becomes possible to get to our end goal)\n"]}, {"number": 1461, "title": "Typo in How-To?", "body": "I'm reading [this](https://www.tensorflow.org/versions/r0.7/how_tos/adding_an_op/index.html#implement-the-kernel-for-the-op) how-to page, and read the line:\n\n`auto output = output_tensor->template flat<int32>();`\n\nIs the `template flat` a typo, or some feature of C++ that I'm not familiar with? \n\nThis line appears a few times in that section of the how-to.\n", "comments": ["This is a Stackoverflow question, not a Github issue.  `template` is indeed a feature of C++.\n", "More precisely: http://stackoverflow.com/questions/610245/where-and-why-do-i-have-to-put-the-template-and-typename-keywords\n", "Apologies, I (wrongly) thought I knew more about C++ than I actually do, and assumed this was indeed a typo. Thanks for the link!\n"]}, {"number": 1460, "title": "Update README.md", "body": "Scores obtained on running label_image does not display normalized value.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@petewarden: These scores do look pretty different.  Is there a potential issue here?\n", "Those new scores look correct. The difference comes because I was originally using Inception v1 (that we include with the Android example) to generate the scores, and we switched to v3, but I forgot to update this README, though the tutorial has the correct values:\nhttps://www.tensorflow.org/versions/master/tutorials/image_recognition/index.html\n", "Thanks for this change @shekkizh! We appreciate the fix.\n"]}, {"number": 1459, "title": "partial_run segfault", "body": "Running partial_run twice results in a segfault. By running twice I mean: setting up placeholder, ops and completing one run through the graph and then trying to do another run.\n### Environment info\n\nOperating System: Linux. \nVersion: 0.7.1\nHash: 028d0b4\n### Steps to reproduce\n\n```\nimport tensorflow as tf\nfrom tensorflow.python import array_ops\nfrom tensorflow.python import math_ops\nfrom tensorflow.python.framework import dtypes\nsess = tf.Session()\n\na = array_ops.placeholder(dtypes.float32, shape=[])\nb = array_ops.placeholder(dtypes.float32, shape=[])\nr1 = math_ops.add(a, b)\n\nh = sess.partial_run_setup([r1], [a, b])\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\n\nh = sess.partial_run_setup([r1], [a, b])\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\n\n> Segmentation fault (core dumped)\n```\n### What have you tried?\n1. Changing execution target to CPU instead of GPU (same problem)\n2. Reusing the same handle for the second run results in a `StatusNotOK` error because the input has already been fed\n### Logs or other output that would be helpful\n\nCore dump is 1.7 GB. Can attach if useful\n", "comments": ["Checking with the main partial run person (I'm not sure I know his Github username).\n", "Just assign it to me.\n", "I don't seem to be able to reproduce the reported issue.  Here is the code I use:\n\n```\nimport tensorflow as tf\nsess = tf.Session()\na = tf.placeholder(tf.float32, shape=[])\nb = tf.placeholder(tf.float32, shape=[])\nr1 = tf.add(a, b)\n\nh = sess.partial_run_setup([r1], [a, b])\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\nprint(\"111\", res)\n\nh = sess.partial_run_setup([r1], [a, b])\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\nprint(\"222\", res)\n\nh = sess.partial_run_setup([r1], [a, b])\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\nprint(\"333\", res)\n```\n\nRunning it produced:\n\n```\n('111', 3.0)\n('222', 3.0)\n('333', 3.0)\n```\n", "Same code results in segfault here:\n\n```\n...\n\nres = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\nSegmentation fault (core dumped)\n```\n\nAre you running 0.7.1 028d0b4 ? Will try another reinstall from scratch\n", "I am using HEAD. Could you try HEAD?\n", "Trying it now\n", "(ran into bazel issues) \nworks!\n\n```\n('111', 3.0)\n('222', 3.0)\n('333', 3.0)\n```\n\nSeems the bug was fixed somewhere between last release and HEAD. Thanks!\n", "Excellent!  Closing.  @yuanbyu: We should probably add a note to `RELEASE.md` saying some bugs were fixed in partial_run.\n"]}, {"number": 1458, "title": "CIFAR-10 Model has (almost) pointless ops in distorted_inputs", "body": "While preparing an intro talk I noticed something odd happening in cifar10_input.py. \n\nIn the following piece of code:\nhttps://github.com/tensorflow/tensorflow/blob/a641ff6aca15b8e6e5ecf96a7a9188c0c1246c8e/tensorflow/models/image/cifar10/cifar10_input.py#L176\n\nThe sequence of ops is: random brightness, which adds a random constant to each pixel, then random contrast, which multiplies the difference from the mean by a random factor ... and then per_image_whitening, which transforms the image to zero mean and unit variance, effectively canceling the previous random operations out, if you don't cross the threshold the per_image_whitening has for low-variance images. That doesn't make sense to me - I would think you want either the data augmentation ops for brightness/contrast OR the whitening to deal with different lighting conditions, but not both. Am I mistaken there?\n", "comments": ["@shlens: Want to comment here? \n", "Agreed. The image preprocessing is confused and superfluous.\n\nFor the most part, the distortions provided by random_brightness combined with random_contrast would be largely removed by per_image_whitening performed in the subsequent operation. If you removed the per_image_whitening, I suspect that the model should train nearly identically.\n\nWould you be able to perform a side-by-side comparison where you retain and then remove per_image_whitening? If the results are identical, we would remove per_image_whitening from the preprocessing of this network.\n\n(In an earlier incarnation of this network we employed random_hue prior to per_image_whitening, however this operation we removed for other reasons.)\n", "I actually tried a version without without the random brightness and random contrast and that trained identically (which I expected since the per_image_whitening is cancelling them out). I will try removing the whitening if there's an idle gpu sometime.\n", "@stefan-w: Pull requests welcome! \n", "I believe that the whitening should be different on a per image basis (one because there is saturation effects that adjusting the contrast and brightness may cross, two the adjustments for data augmentation should be on a per 'image' basis). Take for example (using numpy):\n\n```\nX_batch_sample = X_train[0:256]\nX_0 = (X_train[0:256] - np.mean(X_train[0:256], axis=0))/np.std(X_train[0:256],axis=0)\n#X_0 is the whitened 'images'\n\nshift_1 = 63 * np.random.normal(size=(256,1,1,1))\nX_0_shift1 = (X_train[0:256] + shift_1 - np.mean(X_train[0:256] + shift_1, axis=0)) / np.std(X_train[0:256] + shift_1, axis=0)\n#whitening the shifted case\nshift_2 = np.random.uniform(low=0.2, high=1.8, size=(256,1,1,3))\nX_0_shift2 = (X_train[0:256]*shift_2 - np.mean(X_train[0:256]*shift_2, axis=0)) / np.std(X_train[0:256]*shift_2, axis=0)\n\n#Let's look at only the first image:\nprint(np.mean(X_0[0] - X_0_shift1[0]))\n#This should be non-zero shift (because its the difference of two independent Gaussian processes)\nprint(np.mean(X_0[0] - X_0_shift2[0]))\n```\n\nEDIT: forgot a line of code;\n"]}, {"number": 1457, "title": "add code to register Mean GPU kernel", "body": "see #1433 \n", "comments": ["Can one of the admins verify this patch?\n", "This looks good, just checking with @benoitsteiner to make sure nothing strange is going on.\n", "Jenkins, test this please.\n", "Uh oh, it looks like this PR has uncovered a bug in the previously untested GPU reduce_mean code:\n\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-gpu_pip/308/console\n\n@benoitsteiner: Can you investigate? \n", "@benoitsteiner just submitted the registration along with a fix to the untested kernel, so it should show up in open source soon. \n"]}, {"number": 1456, "title": "Fixing broken link in \"Using the Op in Python\"", "body": "In \"Using the Op in Python\" section: the load_op_library link is broken.  Changed from ../../api_docs/python/framework#load_op_library to https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#load_op_library\n", "comments": ["Can one of the admins verify this patch?\n", "The broken link is unfortunate, but this PR is bad because it would make all versions of the docs link to the r.07 version.  Cc @martinwicke.  \n", "Yes, the absolute path is bad, can you just add a .md after framework? That should fix it as well.\n", "(feel free to comment if you've updated this as above, otherwise closing due to inactivity)\n"]}, {"number": 1455, "title": "Fixed broken link in \"Links\"", "body": "In \"Links\" paragraph there is a broken link to tensorflow.org. \nFixed it by changing from tensorflow.org to \nhttps://www.tensorflow.org/\n", "comments": ["Can one of the admins verify this patch?\n", "@danmane: Looks reasonable to me, is there any website subtlety here? \n"]}, {"number": 1454, "title": "Python multiple inequalities do not work as expected", "body": "```\nimport tensorflow as tf\n\nwith tf.device(\"/cpu:0\"):\n    lower_bound = tf.constant(\n        [[ 0.,   0.,   0. ],\n         [ 0.,   0.,   0. ],\n         [ 0.5,  0.5,  0.5],\n         [ 0.5,  0.5,  0.5],])\n    upper_bound = tf.constant(\n        [[ 0.5,  0.5,  0.5],\n         [ 0.5,  0.5,  0.5],\n         [ 1.,   1.,   1. ],\n         [ 1.,   1.,   1. ],])\n    test = tf.constant(\n        [[ 0.30000001,  0.2,  0.82],\n         [ 0.30000001,  0.2,  0.82],\n         [ 0.30000001,  0.2,  0.82],\n         [ 0.30000001,  0.2,  0.82],])\n    multiple_inequality = lower_bound <= test < upper_bound\n    lower_bound_filter = lower_bound <= test\n    upper_bound_filter = test < upper_bound\n    expected_filter = tf.logical_and(lower_bound_filter, upper_bound_filter)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print('Multiple inequality:', multiple_inequality.eval())\n    print('Upper bound inequality:', upper_bound_filter.eval())\n    print('Lower bound inequality:', lower_bound_filter.eval())\n    print('Expected:', expected_filter.eval())\n```\n\nOutput:\n\n> ('Multiple inequality:', array([[ True,  True, False],\n>        [ True,  True, False],\n>        [ True,  True,  True],\n>        [ True,  True,  True]], dtype=bool))\n> ('Upper bound inequality:', array([[ True,  True, False],\n>        [ True,  True, False],\n>        [ True,  True,  True],\n>        [ True,  True,  True]], dtype=bool))\n> ('Lower bound inequality:', array([[ True,  True,  True],\n>        [ True,  True,  True],\n>        [False, False,  True],\n>        [False, False,  True]], dtype=bool))\n> ('Expected:', array([[ True,  True, False],\n>        [ True,  True, False],\n>        [False, False,  True],\n>        [False, False,  True]], dtype=bool))\n\nIt seems like `multiple_inequality = lower_bound <= test < upper_bound` becomes `multiple_inequality = test < upper_bound`\n", "comments": ["Unfortunately numpy as the same issue and for the same reason: our comparison operators return tensors of comparisons rather than single comparisons, and such tensors can't be combined with Python's `and` routine.\n", "Is there a particular library or language on which you're basing your expectation (other than mathematics, where obviously the expression is well defined :)...)?\n\nGenerally speaking, we've been trying to maintain consistency between the TensorFlow Python API and NumPy. If I try to compute `multiple_inequality` using NumPy, I get the following:\n\n``` python\n>>> import numpy as np\n>>> lower_bound = np.array(\n...         [[ 0.,   0.,   0. ],\n...          [ 0.,   0.,   0. ],\n...          [ 0.5,  0.5,  0.5],\n...          [ 0.5,  0.5,  0.5],])\n>>> upper_bound = np.array(\n...         [[ 0.5,  0.5,  0.5],\n...          [ 0.5,  0.5,  0.5],\n...          [ 1.,   1.,   1. ],\n...          [ 1.,   1.,   1. ],])\n>>> test = np.array(\n...         [[ 0.30000001,  0.2,  0.82],\n...          [ 0.30000001,  0.2,  0.82],\n...          [ 0.30000001,  0.2,  0.82],\n...          [ 0.30000001,  0.2,  0.82],])\n>>> lower_bound <= test < upper_bound\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n```\n\nMy intuition would be that this expression would be evaluated as one of the following (depending on the associativity of Python's comparison operators... I think it's the first of the two):\n\n``` python\nx = lower_bound <= test  # a bool array\nmultiple_inequality = x < upper_bound  # bool vs. float\n# or\nx = test < upper_bound  # a bool array\nmultiple_inequality = lower_bound <= x  # bool vs. float\n```\n\n...however, either of these examples seems to work in NumPy (albeit with potentially wacky results, since NumPy converts `False` to `0.0` and `True` to `1.0`), so I'm not quite sure of the types.\n\n---\n\n**EDIT:** TIL that Python has [special syntax for chained comparisons](https://docs.python.org/2/reference/expressions.html#not-in)! However, its functionality seems to be predicated on interpreting the result of a comparison as a boolean. Unfortunately, the result of TensorFlow comparisons&mdash;like NumPy comparisons&mdash;is _not_ a simple `bool` but rather a `tf.Tensor` object. We don't currently override `Tensor.__nonzero__()` (Python 2) or `Tensor.__bool__()` (Python 3), so a `tf.Tensor` is always considered to be \"`True`\" in comparisons etc.\n\nSince this is a common error, I'm going to submit a quick fix that prevents this comparison. Unfortunately it isn't feasible to make the chained comparison syntax work, because Python's [bytecode translation for such comparisons](http://stackoverflow.com/a/12675160/3574081) relies on the value of the `Tensor` which isn't available when you construct the graph. However, we can at least provide a better error message.\n", "@mrry: Thank you for the more detailed explanation of my admittedly terse reply. :)  Good idea to produce a reasonable error!\n", "Thanks for the fast reply guys. I'm not familiar with numpy so I didn't know it also had this limitation. I do think throwing an error preventing this comparison is a good idea, as I was surprised with the results.\n", "Hmm, it looks like I may have spoken too soon... raising an error in `__nonzero__()`/`__bool__()` breaks a lot of previously legitimate code where we have:\n\n``` python\nif tensor:\n  # ...\n```\n\n...where really it should be:\n\n``` python\nif tensor is not None:\n  # ...\n```\n\nWe can fix all of the instances in the TensorFlow library, but it might take longer to fix up all of the existing code that relies on the first version working as expected. Let's keep this issue open though, because the improved error messages are probably worth the pain.\n"]}, {"number": 1453, "title": "Multidimensional RNN", "body": "Hi all,\n\nThis is a feature request for [Multidimensional LSTM](http://people.idsia.ch/~juergen/nips2009.pdf). Is there any plan to support this in tensorflow?\n\nI might be able to help with implementation if there is such a plan.\n\nCheers,\n", "comments": ["@ebrevdo: Want to comment on this?  Fancier variants of LSTMs would be great to have, the only question is whether they should go in core tensorflow, the contrib directory, or the models repo.  If you can make the interface similar to Eugene's existing RNN cell setup contrib might be the best place at first.\n", "Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN.  etc.  Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.  And maybe feed the result into another RNN.\n\nThis allows one to implement both separable and non-separable multidimensional RNNs.  Not sure we need the sugar until people show it's superior to convolutional networks for something, and there's lots of use cases.\n\n**That said**, it would be really nice to implement GridLSTM (also by Alex Graves, and according to their paper showing a lot of promise with fewer parameters).  GridLSTM uses standard LSTM cells, so it would be a function that uses RNNCell objects.  It connects them in interesting, convolutional, ways.\n\nGridLSTM would probably go well in contrib, until enough people use it and it shows to be a very successful technique.\n", "Thanks, GridLSTM looks interesting. I will give it a shot. [This](https://github.com/coreylynch/grid-lstm) seems to be a good starting point.\n\nI do have a use-case for this, but it is still in a pilot study. Will be nice to make it into contrib until there are more use-cases.\n", "> Right now it's possible to implement various types of multidimensional RNNs by feeding in your data as time being one direction (say, x), taking the output of the RNN, transposing it, and feeding it into a second RNN. etc.\r\n\r\nHow exactly do I transpose the output? Say I have this code:\r\n\r\n```\r\ninput = tf.placeholder(tf.float32, [batch_size, time_steps, num_features]\r\ncell = rnn.BasicLSTMCell(num_units=1)\r\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\r\ntranposed_lstm_output = some_transpose_function(lstm1) # how does this work?\r\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_lstm_output, dtype=tf.float32)\r\n```\r\n\r\nAs for the other one:\r\n\r\n> Alternatively feed your data & its transpose into separate RNNs (possibly with tied weights) and depth-concatenate the results.\r\n\r\nHow do I \"depth-concatenate\" the results?\r\n\r\n```\r\ninput = tf.placeholder(tf.float32, [batch_size, time_steps, num_features])\r\ntransposed_input = tf.placeholder(tf.float32, [batch_size, num_features, time_steps])\r\ncell = rnn.BasicLSTMCell(num_units=1)\r\nlstm1 = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32)\r\nlstm2 = tf.nn.dynamic_rnn(cell, transposed_input, dtype=tf.float32)\r\nmdlstm1 = some_depth_concatenation_function([lstm1, lstm2]) # how does this work?\r\n```", "Is this the mdlstm implementation? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/ndlstm/python/lstm2d.py"]}, {"number": 1452, "title": "install error", "body": "when i am installing from sources, there is an error like this:\n\nERROR: tensorflow/tensorflow/core/BUILD:98:1: //tensorflow/core:worker_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.\nERROR: tensorflow/tensorflow/core/BUILD:121:1: //tensorflow/core:master_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.\nERROR: tensorflow/tensorflow/cc/BUILD:61:1: Target '//tensorflow/core:tensorflow' contains an error and its package is in error and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: Loading failed; build aborted.\n", "comments": ["I also got the similar error with fresh master:\n\n```\nERROR: /home/igorkorsunov/tensorflow/tensorflow/core/BUILD:98:1: //tensorflow/core:worker_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.\nERROR: /home/igorkorsunov/tensorflow/tensorflow/core/BUILD:121:1: //tensorflow/core:master_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.\nERROR: /home/igorkorsunov/tensorflow/tensorflow/python/BUILD:154:1: Target '//tensorflow/core:protos_all_py' contains an error and its package is in error and referenced by '//tensorflow/python:framework_for_generated_wrappers'.\nERROR: Loading failed; build aborted.\n```\n\nI am using following command to build TensorFlow:\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \n```\n\nBazel version is 0.2.0\n", "@mrry: Have you seen this before?\n", "Urgh, this sounds like `protobuf.bzl` might have changed... did we recently move to a newer version of protobuf? @keveman knows the mystic runes that make it work with gRPC.\n", "I would also suggest maybe refreshing your submodules -- someone more git-competent than I can explain why git fetch/pull doesn't also update your submodules.\n", "@vrv: If it did that there's a risk of stomping on someone's changes inside a submodule.  Submodules are a pretty flaky part of git, unfortunately.\n", "I thought that would be no different than resolving conflicts from the main repo :)\n", "Maybe if submodules weren't a bolted on hack.\n", "I got this kind of error, too\n\n`ERROR: /home/hieunguyen/tensorflow/tensorflow/core/BUILD:98:1: //tensorflow/core:worker_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.\nERROR: /home/hieunguyen/tensorflow/tensorflow/core/BUILD:121:1: //tensorflow/core:master_service_proto_cc: no such attribute 'use_grpc_plugin' in 'cc_library' rule.\nERROR: /home/hieunguyen/tensorflow/tensorflow/cc/BUILD:61:1: Target '//tensorflow/core:tensorflow' contains an error and its package is in error and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: Loading failed; build aborted.\nINFO: Elapsed time: 0.072s`\n\nwhen\n`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\n", "So what's the workaround for this?\n", "Try:\n\n```\ngit pull --recurse-submodules\ngit submodule update --recursive\n```\n\n(I haven't tried this, but this is what seems like might work -- essentially the tree of each of tensorflow's submodules)\n", "Thanks for you reply.\nI deleted everything, then, made a fresh clone of master source. Now the installation works well.\n", "I just had the same problem as mentioned above, and the solution proposed by @vrv fixed the issues for me.\n", "My problem was also solved via @vrv 's proposed fix. \n", "I have been trying to install tensorflow on an EC2 instance based on [these instructions](http://eatcodeplay.com/installing-gpu-enabled-tensorflow-with-python-3-4-in-ec2/), and keep getting a similar error to the ones discussed above: \n\n`ERROR: /mnt/tmp/tensorflow/tensorflow/core/BUILD:87:1: //tensorflow/core:protos_all_py: no such attribute 'imports' in 'py_library' rule.\nERROR: /mnt/tmp/tensorflow/tensorflow/cc/BUILD:61:1: Target '//tensorflow/core:all_kernels' contains an error and its package is in error and referenced by '//tensorflow/cc:tutorials_example_trainer'.\nERROR: Loading failed; build aborted.`\n\nUnfortunately, @vrv 's fix does not seem to work, and neither does retrying with a fresh master source. Any thoughts? \n", "This was with \n\n`bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\n", "@davidzchen is this related to your change to remove python protobuf as a submodule?\n", "The following error message seems to indicate that the version of Bazel being run does not support the `imports` attribute for the Python rules:\n\n```\nERROR: /mnt/tmp/tensorflow/tensorflow/core/BUILD:87:1: //tensorflow/core:protos_all_py: no such attribute 'imports' in 'py_library' rule.\n```\n\n@hrajanie Which version of Bazel are you running? The `imports` attribute was added in 0.2.0.\n", "@davidzchen Using Bezel 0.2.0 seems to have solved the problem I reported, although I'm still having problems building the Pip package. I may just try again using [these scripts](https://github.com/RealScout/deep-learning-images) instead. Thanks for your help! \n", "I was also getting an error like this and fixed it by upgrading bazel from `0.1.4` to `0.2.3-2016-06-10` (installed bazel from source).\n\nThe specific error was:\n\n```\nERROR: /tensorflow/tensorflow/contrib/session_bundle/BUILD:160:1: //tensorflow/contrib/session_bundle:manifest_proto_py: no such attribute 'imports' in 'py_library' rule.\nERROR: /tensorflow/tensorflow/tools/pip_package/BUILD:23:1: Target '//tensorflow/contrib/session_bundle:manifest_proto_py' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:build_pip_package'.\nERROR: Loading failed; build aborted.\n```\n\nafter running \n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n", "We have the same error like @hrajanie and @corcra in AWS EC2. \n\nAnd it works after upgrading bazel to 0.2.3 according to https://github.com/tensorflow/tensorflow/issues/2909\n"]}, {"number": 1451, "title": "Is there a way to cache intermediate Tensor result?", "body": "I attempt to implement naive RBM/DBN through tensorflow's python API.\nThe functionality is easy to implement if you don't care about computational performance.\n\nFor now, I've tried many alternatives solutions and find it difficult to implement with high performance since there's no way to cache the intermediate tensor result.\n\nWhile lacking of the ability to cache the intermediate result, there will be too much memory migrate from GPU to RAM or vice versa (say I'm using GPU) if I want to do something through intermediate result.\n\nHere's part of my code to illustrate my meaning.\n\n``` python\n    import tensorflow as tf\n    alpha = .01\n    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n    trX, trY, teX, teY = mnist.train.images, mnist.train.labels,\\\n        mnist.test.images, mnist.test.labels\n    W = tf.Variable(tf.random_normal([784, 100]), name=\"weights\")\n    hb = tf.Variable(tf.zeros([100]), name=\"hbias\")\n    vb = tf.Variable(tf.zeros([784]), name=\"vbias\")\n    v0 = tf.placeholder(\"float\", [None, 784])\n\n    h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n    h0 = sample_prob(h0)\n    v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb)\n    v1 = sample_prob(v1)\n    h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)\n\n    pg = tf.matmul(tf.transpose(v0), h0)\n    ng = tf.matmul(tf.transpose(v1), h1)\n\n    dW = (pg - ng) / tf.to_float(tf.shape(v0)[0])\n    dhb = tf.reduce_mean(h0 - h1, 0)\n    dvb = tf.reduce_mean(v0 - v1, 0)\n\n    update_W_op = W.assign_add(alpha * dW)\n    update_hb_op = hb.assign_add(alpha * dhb)\n    update_vb_op = vb.assign_add(alpha * dvb)\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        for _ in range(10):\n            for start, end in zip(\n                    range(0, 50000, 100), range(100, 50000, 100)):\n                X = trX[start:end]\n                # sess.run([dW, dhb, dvb, h1], feed_dict={v0: X})\n                # sess.run(h1, feed_dict={v0: X})\n                # sess.run(dW)\n                # sess.run(dhb)\n                # sess.run(dvb)\n                # _W = sess.run(update_W_op)\n                # _hb = sess.run(update_hb_op)\n                # _vb = sess.run(update_vb_op)\n                [_W, _hb, _vb] = sess.run([update_W_op, update_hb_op, update_vb_op], feed_dict={v0: X})\n```\n\nThis is the most efficient way I could think out. But, there's no way to optimize the progress by calculating `[_W, _hb, _vb]` with the aid of intermediate result `pg, ng, h0, h1, v0, v1`. It's obvious that here comes so much duplicated  but useless extra computation.\nIf feed `v0` with `X` once, the follower `Tensor` need the former `Tensor`.\n`h0` needs `v0`, **so needs `X`**\n`v1` needs  `h0`, **so needs `X`**\n`h1` needs `v1`, **so needs `X`**\n......\n`update_W_op` needs `dW`, **so needs `X`**\n`update_hb_op` needs  `dhb`, **so needs `X`**\n`update_vb_op` needs `dvb`, **so needs `X`**\n\nSince there's no way to cache the intermediate result, you can only get one `Tensor` result for one `computation`. There's no way to avoid the duplication computation and `X` memory migration cost.\n\nI was wondering if there's a property of `Tensor` say `cached = False`, we could set it to `True` manually. And when intermediate cached result is enough to go on, we need not to feed data in, and just using intermediate cache to compute. If you want to flush the cache, just feed in the new data into the placeholder. But for now, if the graph contains **placeholder** in the path, the data must be feed in specifically.\n\nFor example, if the cache is enabled.\n\n``` python\nh0.cached = True\nh1.cached = True\nv0.cached = True\nv1.cached = True\nsess.run(h1, feed_dict={v0: X})\nsess.run([update_W_op, update_hb_op, update_vb_op])  # using cache h0,h1,v0,v1 is enough to compute the three update without X fed in.\n```\n", "comments": ["Yes, it's called `partial_run`: see #672.  It's experimental at this stage so the API might fluctuate, but it should be what you're looking for.\n", "Closing as duplicate.\n", "@girving I have no idea the C++ code implementation for `paritial_run`, but there seems kind of memory mallloc and release problem.\n\nHere's the code I use with `partial_run` as you suggest:\n\n``` python\nh = sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])\nfor i in range(epoches):\n    for batch in batches:\n        # h = sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])\n        X = batch\n        sess.partial_run(h, h1, feed_dict={v0: X})\n        # sess.partial_run(h, h0, feed_dict={v0: X})\n        # sess.partial_run(h, v1)\n        # sess.partial_run(h, h1)\n        sess.partial_run(h, dW)\n        sess.partial_run(h, dhb)\n        sess.partial_run(h, dvb)\n        _W = sess.partial_run(h, update_W_op)\n        _hb = sess.partial_run(h, update_hb_op)\n        _vb = sess.partial_run(h, update_vb_op)\n```\n\nThis code will give the following error when calculating `h1` in the second iteration.\n\n```\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: The feed Placeholder:0 had already been fed.\n```\n\nWhen I change `partial_run_setup` to the comment position, it will run out of GPU memory after several tens of seconds with a segment fault under Ubuntu. During the calculating, the utility of GPU is never higher than 5% according to nvidia-smi monitoring.\n\nSo, any idea about how to solve this? Or could only wait until the API is stable?\n", "From #1459 \n\nSeems that there's a bug in the 0.7.1 release - try using HEAD if you're running into partial_run segfaults\n"]}]