[{"number": 14113, "title": "Add GCC/Compiler version to issue template.", "body": "As suggested in #13930", "comments": []}, {"number": 14112, "title": "[iOS] No OpKernel was registered to support Op 'Prod' with these attrs", "body": "I'm trying to load the faster_rcnn_resnet101_coco model from the [Tensorflow Object Detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) onto the iOS camera example.  Currently, I'm running in to the issue with \r\n```\r\nNo OpKernel was registered to support Op 'Prod' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n```\r\n\r\nThe environment is as follows:\r\n\r\n- python 3.6\r\n- xcode 9.0.1\r\n- mac os 10.12\r\n- tensorflow built from source using r1.4\r\n- bazel 0.5.4\r\n\r\nI have done things as follow:\r\n\r\n1. Following the iOS static library build [instructions](https://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow/contrib/makefile),\r\n\r\n2. Successfully ran \r\n\r\n`tensorflow/contrib/makefile/download_dependencies.sh\r\n`\r\n3. Successfully ran \r\n\r\n`tensorflow/contrib/makefile/compile_ios_protobuf.sh`\r\n\r\n4. Successfully ran \r\n\r\n```\r\nexport HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh`\r\nexport TARGET_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh -t ios`\r\n```\r\n\r\n5. Created faster-rcnn specific ops:\r\n```\r\n  bazel build tensorflow/python/tools:print_selective_registration_header \r\n  bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\r\n    --graphs=frozen_inference_graph.pb > ops_to_register.h\r\n```\r\n\r\n6. Moved `ops_to_register.h` under `tensorflow/core/framework/`\r\n\r\n7. Successfully built `libtensorflow-core.a` to account for additional types and custom ops\r\n```\r\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-O3  -DANDROID_TYPES=ANDROID_TYPES_FULL -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION\"\r\n```\r\n\r\n8. Verified the creation of \r\n```\r\ntensorflow/contrib/makefile/gen/lib/libtensorflow-core.a\r\ntensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a\r\ntensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf-lite.a\r\n```\r\n\r\n9. Check for `tf_op_files.txt`, and made sure `tensorflow/core/ops/math_ops.cc` exists (I think this is where \"Prod\" is covered since there's no such file as `cwise_op_prod.cc`?)\r\n\r\n10. Updated iOS camera example with faster-rcnn model, imported \"mscoco_label_map\", and corresponding input/output layers.\r\n\r\nWhen running, I can't get pass error \r\n```\r\n2017-10-30 18:00:28.307769: E [path]/tensorflow_utils.mm:209] Could not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'Prod' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: SecondStageBoxPredictor/Flatten/Prod = Prod[T=DT_INT32, Tidx=DT_INT32, keep_dims=false](SecondStageBoxPredictor/Flatten/Slice_1, SecondStageBoxPredictor/Flatten/Const)]]\r\n2017-10-30 18:00:28.320101: F [path]CameraExampleViewController.mm:495] Couldn't load model: Invalid argument: No OpKernel was registered to support Op 'Prod' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: SecondStageBoxPredictor/Flatten/Prod = Prod[T=DT_INT32, Tidx=DT_INT32, keep_dims=false](SecondStageBoxPredictor/Flatten/Slice_1, SecondStageBoxPredictor/Flatten/Const)]]\r\n```\r\n\r\nIf I try the same steps with the model `ssd_mobilenet_v1_coco` from the same model zoo, it works fine.  I've also tried the instructions from [JieHe96/iOS_Tensorflow_ObjectDetection_Example](https://github.com/JieHe96/iOS_Tensorflow_ObjectDetection_Example) but runs into the exact same problem.\r\n\r\nCan anyone help?\r\n\r\n", "comments": ["When modifying examples, you might need to tailor the build to make sure all the kernel libraries (which use a registration pattern) are being included. With that said, this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is a larger community that reads questions there. You can also likely search for many similar issues.", "Thanks. On a more general question, I figured that it's missing the right kernel libraries as suggested.  What is a good way to identify which kernel library I'm missing since I'm using the default faster_rcnn_resnet101_coco model from the Tensorflow model zoo and there are quite a few \"Prod\" ops in the overall kernel libraries.", "Actually I figured it out after reproducing the \"ops_to_register.h\" file using \"print_selective_registration.py\".  The \"Prod\" op is associated with \"reduction_prod_op.cc\".", "Hi, @tamias65 , Could you please give some more details on how you figure out which 'cc' file is associated with the \"Prod\" op?. \r\n I'm running into a similar problem here, which says \"No OpKernel was registered to support Op 'Equal'\". ", "@nanhuo I think you need to register the math operator https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/equal"]}, {"number": 14111, "title": "CUDNN_STATUS_INTERNAL_ERROR", "body": "Hi, \r\n\r\nI did not do anything but find all my programs about tensorflow could not work. That is very strange that my two computers has the same error at the same time.\r\n\r\nThe error is showed here:\r\n\r\n2017-10-30 17:55:23.525428: E tensorflow/stream_executor/cuda/cuda_dnn.cc:371] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-10-30 17:55:23.525452: E tensorflow/stream_executor/cuda/cuda_dnn.cc:338] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-10-30 17:55:23.525458: F tensorflow/core/kernels/conv_ops.cc:672] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms) \r\nAborted (core dumped)\r\n\r\n\r\nThanks so much\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\nIn general though, that error is indicative of an issue with initializing CUDNN structures, which could be because of some version mismatch in CUDA/CUDNN/driver libraries."]}, {"number": 14110, "title": "GPU underutilized using DNNClassifier", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Using stock code: https://www.tensorflow.org/get_started/estimator\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary from pip\r\n- **TensorFlow version (use command below)**: 1.3.1, 1.4rc1, master\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.7\r\n- **CUDA/cuDNN version**: CUDA 8; cuDNN 6.0\r\n- **GPU model and memory**: NVidia GeForce 1070 (8GB)\r\n- **Exact command to reproduce**: run the above code for the estimator (or see below)\r\n\r\n### Describe the problem\r\nRunning this code results in GPU use of up to 5-10% of GPU resources (measured using nvidia-smi). Changing the network nodes won't improve performance. Using derived code (but essentially the same) on much larger datasets increases GPU performance up to 25%. I would expect max GPU utilization. Memory allocation is in any case 100%.\r\n\r\n### Source code / logs\r\nThe code used is from https://www.tensorflow.org/get_started/estimator , here reproduced with minor fixes (it won't run on python3 otherwise):\r\n\r\n```from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os, json\r\n#import urllib\r\nfrom urllib.request import urlopen\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Data sets\r\nIRIS_TRAINING = \"iris_training.csv\"\r\nIRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\r\n\r\nIRIS_TEST = \"iris_test.csv\"\r\nIRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\r\n\r\ndef main():\r\n  # If the training and test sets aren't stored locally, download them.\r\n  if not os.path.exists(IRIS_TRAINING):\r\n    raw = urlopen(IRIS_TRAINING_URL).read()\r\n    print(raw)\r\n    with open(IRIS_TRAINING, \"w\") as f:\r\n      #f.write(raw)\r\n      f.write(json.loads(raw.decode('utf-8')))\r\n\r\n  if not os.path.exists(IRIS_TEST):\r\n    raw = urlopen(IRIS_TEST_URL).read()\r\n    with open(IRIS_TEST, \"w\") as f:\r\n      #f.write(raw)\r\n      f.write(json.loads(raw.decode('utf-8')))\r\n\r\n\r\n  # Load datasets.\r\n  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n      filename=IRIS_TRAINING,\r\n      target_dtype=np.int,\r\n      features_dtype=np.float32)\r\n  test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n      filename=IRIS_TEST,\r\n      target_dtype=np.int,\r\n      features_dtype=np.float32)\r\n\r\n  # Specify that all features have real-value data\r\n  feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\r\n\r\n  # Build 3 layer DNN with 10, 20, 10 units respectively.\r\n  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\r\n                                          hidden_units=[100],\r\n                                          n_classes=3,\r\n                                          model_dir=\"./iris_model\")\r\n  # Define the training inputs\r\n  train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x={\"x\": np.array(training_set.data)},\r\n      y=np.array(training_set.target),\r\n      num_epochs=None,\r\n      shuffle=True)\r\n\r\n  # Train model.\r\n  classifier.train(input_fn=train_input_fn, steps=2000)\r\n\r\n  # Define the test inputs\r\n  test_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x={\"x\": np.array(test_set.data)},\r\n      y=np.array(test_set.target),\r\n      num_epochs=1,\r\n      shuffle=False)\r\n\r\n  # Evaluate accuracy.\r\n  accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\r\n\r\n  print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\r\n\r\n  # Classify two new flower samples.\r\n  new_samples = np.array(\r\n      [[6.4, 3.2, 4.5, 1.5],\r\n       [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\r\n  predict_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n      x={\"x\": new_samples},\r\n      num_epochs=1,\r\n      shuffle=False)\r\n\r\n  predictions = list(classifier.predict(input_fn=predict_input_fn))\r\n  predicted_classes = [p[\"classes\"] for p in predictions]\r\n\r\n  print(\r\n      \"New Samples, Class Predictions:    {}\\n\"\r\n      .format(predicted_classes))\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n", "comments": ["Performance is a top priority for the TensorFlow team. The best way to help us improve it, is to make sure other potential causes have been ruled out. I recommend reaching out to the [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for help troubleshooting. If a solid case can be made that there's a performance issue in TensorFlow, please report back to this issue, and I'll re-open it."]}, {"number": 14109, "title": "The implementation in source code of Backpropagation in TensorFlow (Conv2DBackpropFilter and Conv2DBackpropInput)", "body": "Hi,\r\n\r\nSince two operations Conv2DBackpropFilter and Conv2DBackpropInput count most of the time for lots of applications(AlexNet/VGG/GAN/Inception, etc.), I am analyzing the complexity of these two operations (back-propagation) in TensorFlow and I found out that there are three implementation versions (custom, fast and slot) for Conv2DBackpropFilter (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_filter_ops.cc ) and Conv2DBackpropInput (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_input_ops.cc). While I profile, all computations are passed to \"custom\" version instead of \"fast\" or \"slow\" which directly calls Eigen function SpatialConvolutionBackwardInput to do that. \r\n\r\nThe issue is:\r\nConv2DBackpropFilter uses Eigen:\u201cTensorMap.contract\" to do the tensor contraction and Conv2DBackpropInput uses Eigen:\"MatrixMap.transpose\" to do the matrix transposition in the Compute() function. Beside these two functions, I didn't see any convolutional operations which are needed for back-propagation theoretically. Beside convolutions, what else would be run inside these two operations for back-propagation? Does anyone know how to analyze the computation complexity of \"back propagation\" operation in TensorFlow?\r\n\r\nI am looking for any advise/suggestion. Thank you!", "comments": ["I don't think I quite follow your question. As you pointed out, there are a few different implementations of those kernels, and the `REGISTER_KERNEL_BUILDER` calls in the files you listed will tell you which conditions a particular kernel implementation is applicable for.\r\n\r\nThe `Compute()` methods of these classes lists out exactly the code run when these operations are invoked.\r\n\r\nCould you be more detailed on what you're looking for?\r\n(Also, perhaps this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. )", "Asim, thank you for your reply. I asked in StackOverflow but no one responses me yet and I am in a hurry in this issue.\r\n\r\nTo be simple, the issue is that the normal Back-Propagation should include convolution, differentiate,  derivate and other operations to calculate a gradient. However, in TensorFlow, it only uses tensor contraction (for Conv2DBackpropFilter) and matrix transposition(for Conv2DBackpropInput) to do the Back-Propagation. How can it be? ", "I'm still not sure I understand the question, but to be clear, if you want to see all the operations conducted during the backpropagation pass then you should inspect the graph. The gradient computation is added to the graph by `tf.gradients`. You can use TensorBoard's [graph visualization](https://www.tensorflow.org/get_started/graph_viz) to see all the computation done in the forward and backward pass.\r\n\r\nHope that helps.\r\n", "Thank you though I still can not find the operations' workflow of backpropagation in TensorBoard's generated graph.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. Please move your issue there. Thanks!"]}, {"number": 14108, "title": "Custom OP Reader AttributeError: 'module' object has no attribute 'read'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCustom Code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.6.1\r\n- **CUDA/cuDNN version**:\r\nLaptop CPU\r\n- **GPU model and memory**:\r\nLaptop CPU\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nfilename_queue = tf.train.string_input_producer([\"test.csv\"])\r\nreader = tf.load_op_library('./tensorflow/bazel-bin/tensorflow/core/user_ops/redis_reader.so')\r\nkey, value = reader.read(filename_queue) \r\n```\r\n\r\n### Describe the problem\r\nFor my custom op reader, I am getting `AttributeError: 'module' object has no attribute 'read'`. This is most puzzling to me because I thought the python wrapper would inherit `read` from `/tensorflow/python/ops/io_ops.py`. Furthermore, `gen_user_ops.py` in my bazel-genfiles doesn't seem to have code that reflects my custom op reader. Is it suppose to show in that file? \r\n\r\n### Source code / logs\r\nHere I have registed the OP:\r\n\r\n```\r\nREGISTER_OP(\"RedisReader\")\r\n    .Output(\"reader_handle: resource\")                            \r\n    .Attr(\"container: string = ''\")                               \r\n    .Attr(\"shared_name: string = ''\")                             \r\n    .SetIsStateful()                                              \r\n    .SetShapeFn(TwoElementOutput)\r\n    .Doc(R\"doc(Description Goes Here)doc\");\r\n```\r\n\r\nHere is my kernel and kernel registration:\r\n\r\n```\r\nclass RedisReaderOp : public ReaderOpKernel {\r\n    public:\r\n        explicit RedisReaderOp(OpKernelConstruction* context) : ReaderOpKernel(context) {\r\n            \r\n            Env* env = context->env();\r\n\r\n            SetReaderFactory([this, env]() {\r\n                return new RedisReader(name(), env);\r\n            });\r\n        }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"RedisReader\").Device(DEVICE_CPU), RedisReaderOp);\r\n```\r\n\r\nAnd here is my python wrapper located in `user_ops.py`:\r\n\r\n```\r\nfrom tensorflow.python.ops import gen_user_ops as _gen_user_ops\r\n\r\n# go/tf-wildcard-import\r\nfrom tensorflow.python.ops.gen_user_ops import *  # pylint: disable=wildcard-import\r\n\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import common_shapes\r\nfrom tensorflow.python.ops import io_ops\r\n\r\nclass RedisReader(io_ops.ReaderBase) :\r\n\r\n\tdef __init__(self, name=None):\r\n        \trr = _gen_user_ops.redis_reader(name=name)\r\n        \tsuper(RedisReader, self).__init__(rr)\t\r\n\r\nops.NotDifferentiable(\"RedisReader\")\r\n```", "comments": ["The C++ compiled into the `.so` knows nothing about your Python wrapper. It sounds like you want to import `user_ops` rather than running `load_op_library`?", "@allenlavoie I imported user_ops, but I get the `AttributeError: 'module' object has no attribute 'redis_reader'` from line `rr = _gen_user_ops.redis_reader(name=name)`, which is what I feared. The gen_user_ops  does not show my custom op reader in it, but only the default _fact op provided as an example.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nHowever I must note that I am a fan of Redis. I would be interested in learning more about what sort of things you're interested in building with TensorFlow, because I'm currently doing work along similar lines."]}, {"number": 14107, "title": "TensorFlow 1.4.0 takes more resources and is slower on GPU and CPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: https://github.com/tkuanlun350/Tensorflow-SegNet\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64\r\n- **TensorFlow installed from (source or binary)**:  https://pypi.python.org/pypi/tensorflow-gpu/1.4.0rc1\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.5\r\n- **CUDA/cuDNN version**:  Cuda release 8.0, V8.0.60. cuDNN 6.\r\n- **GPU model and memory**:  NVIDIA P4 \r\n- **Exact command to reproduce**:   `c:\\python35\\python3 main.py --log_dir=./logs --image_dir={image dir} --val_dir= {validation dir} --batch_size=15 --training=True`\r\n\r\n### Describe the problem\r\nUnder 1.3.0 I was able to use a batch size of {15, put your max batch size here} for training.  Under 1.4.0 I get Resource Exhausted errors for that batch size.  So use of GPU resources is going up.  Not the right direction.\r\n\r\nFor me here are the performance effects:\r\n\r\n- TensorFlow GPU 1.3.0: 9.8 images/sec for batch size: 15\r\n- TensorFlow GPU 1.4.0:  Can't do batch size: 15. 7.8 images/sec for batch size: 12\r\n\r\n### Source code / logs\r\n[tf_bug2.txt](https://github.com/tensorflow/tensorflow/files/1428590/tf_bug2.txt)\r\n", "comments": ["And it is slower than release 1.3, at least for the NMT model I am using. When I train it in 1.3, each epoch took about 600 seconds, now it takes about 700 seconds.", "same here on linux, performance drop, but I tried the rc1, I'll evaluate the official release.", "Thank you very much for your feedback. It seems some op consumed more memory than before. If you have time, can you please help figure out which op on your graph used more memory, maybe by simplifying your code and finding out the bottleneck? At the same time on our side, we plan to add better debugging tool for gpu memory allocation and add memory regression tests. \r\n\r\n", "On my side, I have a resnet in the same style as the examples in the official tensorflow models repository. Thanks a lot for looking into this.", "I have already provided the steps to reproduce the error.   MNIST data works well.  I have real issues with detailed performance profiling on TensorFlow so I've stopped looking at detailed profiling on an op-by-op basis for now.  A better profiling tool would be much appreciated.", "Have the same performance drop as mentioned above (and a slightly increased use of memory). A batch takes 0.35 s on 1.4.0, used to take 0.25 s on 1.3.", "on cpu, my model's peak RAM usages is larger by more than 300MB for 1.4 compared to 1.3, which is a >30% increase.", "Same here. Training a resnet against imagenet becomes slower by 30% with v1.4 compared to v1.3. With v1.4, I noticed GPU's starvation (GPU usage is only 60-70% in average, and is fluctuating a lot).  ", "should i open a separate issue for what i'm seeing on CPU or should we change this bug the include it?", "it has been a 50 % increase in cpu for me and a 50% decrease in GPU. I mean utility. Due to the gpu starvation, performance is therefore 50% of the tf 1.3.", "@songgc btw, I've recently [experimented](https://github.com/yaroslavvb/stuff/tree/master/cluster/imagenet64) with scripts from \"High Performance Models\" and have not observed CPU starvation/speed degradation on TF 1.4, even when running on V100 GPUs which put much more pressure on CPU\r\n\r\nSo the way to isolate the problem would be to see what the problematic script is doing that that @tfboyd's reference scripts are not doing (ie are they using probably-will-be-deprecated Queue's to read data?).\r\n\r\nAlso, the official resnet-50 performance has been stable since September (although it doesn't rule out degradation from 1.3 which was last summer). It uses fixed image size with autotune enabled, so that's another thing to try\r\n\r\nhttps://benchmarks-dot-tensorflow-testing.appspot.com/test/tf-cnn-benchmark-resnet50", "There is a performance regression that affects nmt decoders in tf 1.4 that we have solved in the nightlies. If you have a performance regression, can you check if it persists with the tf nightlies?", "hi, after installing tf nightlies via pip command, the problem persists.", "Im facing the same resource exhausted error when running predictions on validation and test sets (changing batch size doesnt affect the outcome) in tf 1.4 gpu. The same script worked flawlessly (But slowly) when i ran on the CPU (tf 1.3). The problem occured when i compiled tensorflow to run on the GPU. I have one GeForce 1080 ti 11gb.   \r\n\r\nMy conclusion are so far that either (1) the GPU cant offload the memory fast enough when all free memory is in use, (2) or that tensorflow stores to much information without offloading, (3) or that my setup is to weak for the dataset im running(See details below).\r\nTraining shape (430056, 21)\r\nValidation shape (119042, 21)\r\nTest shape (119043, 21)\r\nTest 2 shape (892816, 21)\r\nHidden layer 1 shape (2000,)\r\nhidden layer 2 shape (1000,)\r\n\r\n\r\nAny help would be appreciated (I can open a separate issue, but as this discussion deals with tensorflow using to many resources, which is my second conclusion, i've put it here)", "@yaroslavvb We do use queue and python threading for GPU feeding, which works well with v1.3.", "I used queues and python threading (and also StagingArea) for feeding as well. And I haven't seen significant speed difference in ResNet-50 training in 1.4.", "The OOM issue reported by @johnsrude is likely caused by fused batch norm. The documentation incorrectly states that `tf.contrib.layers.batch_norm` with `Fused=None` will use the default implementation. This is not true: it will call the newer, fused version, which is more expensive in terms of gpu resources.\r\n\r\n@johnsrude, can you please replace `batch_norm_layer` in model.py with the following:\r\n\r\n```\r\ndef batch_norm_layer(inputT, is_training, scope):\r\n  return tf.cond(is_training,\r\n          lambda: tf.contrib.layers.batch_norm(inputT, is_training=True,\r\n                           center=False, updates_collections=None, scope=scope+\"_bn\", fused=False),\r\n          lambda: tf.contrib.layers.batch_norm(inputT, is_training=False,\r\n                           updates_collections=None, center=False, scope=scope+\"_bn\", reuse = True, fused=False))\r\n```\r\n\r\nLet me know if this solves the OOM problem.\r\n\r\n@bshao001 @jmaye @11maxed11 @eyaler @songgc @colmantse @NicholaiStaalung \r\nplease consider posting the code that reproduces the issue, otherwise it is very hard to find the root cause.", "So i partially solved my problem by splitting the graph and run it on both the CPU and GPU with tf.device(). What made most sense was running forward and backprop on the CPU while running the predictions on the GPU. Its a bit slower than running everything on the GPU, but way faster than CPU only. So i guess my problem was a combination of my conclusions 1 and 2, and thus not a tensorflow issue. ", "Problem seems to be solved by using Tensorflow 1.4.1. Might be related to this commit: https://github.com/tensorflow/tensorflow/pull/15187/commits/03ef0a0c73eebabb4c6ff0faf16ecf9ab4665b3e", "Are you talking about the time performance issue? I can reproduce the OOM problem with Tensorflow 1.4.1 and the code above. I tracked it down to fused batch norm, which uses more memory because it transforms tensors internally from NHWC to NCHW.", "I'm talking about time performance issue indeed. This was far worse with tf 1.4 as compared to 1.3. At the beginning, it was starting good, but then the global step/sec was jittering like hell. Stopping and restarting made it stable again for a while.\r\nFor information, I'm using tf.layers.batch_normalization with fused=True in my implementation without any problem.\r\n\r\nEdit: it was a false hope, it just lasted a bit more before seeing the performance drop:)", "i am running the code for tensorpack/pix2pix on on windows 10 64 with gpu 1080ti (cuda 8, cudnn 6):\r\nhttps://github.com/ppwwyyxx/tensorpack/blob/master/examples/GAN/Image2Image.py\r\nin tf 1.4 i am seeing an increase of 30% in runtime over 1.3", "Yes. This should occur if you do not have a clean build of tf for win10 from source. I had the same prob, with cpu utility goes up triple and gpu utility drops to the same extent. I decided to switch to linux and build from source (much easier) and now achieve full gpu utility.", "@eyaler On a P100 I saw no performance difference between 1.3 and 1.4, also with Image2Image.py. My tensorflow was installed from pypi.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Some updates. I also had a similar issue like #14942, in which there were just data pipelines reading proto data and preprocessing rather than any training ops in the graph. I saw throughput degradation with v1.4.x. \r\n\r\nAfter I moved to v1.5rc0 with CUDA 9, CUDNN 7, all problems go away, and both data pipelines and training process go back to normal and even slightly faster by (2%-3%). I also noticed RAM usage reduction (but no exact number). I have decided to skip v1.4.\r\n  ", "@songgc question is if you should not expect better performance, i.e. there may be still a problem but perhaps it is negated by cuda 9 etc", "@eyaler I tried two cases. One was the whole training process, and another was data reading and preprocessing only. I agree that the first one might be affected by different CUDA versions . However, the second one used CPU only and had nothing to do with GPUs, in which I saw a better throughput with v1.5.", "on my side, 1.5 definitely solves the problem.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "my memory issue on linux/cpu is solved. however i am now seeing +90% (!) longer train times on windows with tf1.4 and cuda 8 on 1080ti. same issue with tf 1.5rc1 + cuda 9 + cudnn 7. would appreciate any help debugging this.", "@johnsrude does switching to TF 1.5 solve your problem, as many others have reported?\r\n\r\n@eyaler please file a new issue with details describing your problem, and a minimum reproducible test case (if possible).", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 14106, "title": "Batch normalization for LSTM", "body": "A recurrent batch normalization is implemented based on https://arxiv.org/pdf/1603.09025.pdf. The new functionality allows for setting normalization for each of the inputs, the actual input or the state, and the cell prior to the activation function. To avoid a double implementation, tensorflow.python.layers.normalization.BatchNormalization is called to perform the normalization, and it can be controlled fully, but if no extra configuration is provided a good default configuraiton is used. By default no normalization is done. The only aspect mentioned in the paper that was not implemented, was that it was recommended there that the statistics be taken on a time step basis, rather than having all time steps share the same statistics. This would have necessitated a lot of tinkering in BatchNormalization, to the point that it would be more economical if that behaviour was desired, to simply port the parts needed into LSTM and adjust them as necessary, but that should not be part of the main branch.", "comments": ["Can one of the admins verify this patch?", "@ebrevdo, can you comment on this?\r\n\r\n@Andres-Hernandez can you look into the conflicts? I am sorry it took us so long to get to this.", "Can one of the admins verify this patch?", "I have resolved the conflicts, which were due to a refactoring of the LSTM cell which happened over the weekend between me getting my working copy and when I checked in on Monday. The refactoring that had been done including things like e.g. name as parameter and use of standard build function to create variables, instead of creating them in call the first time it is invoked. The new normalization code has been adapted to this more standard implementation. ", "@Andres-Hernandez any progress? Marking as stalled for now.", "@Andres-Hernandez feel free to reopen if you get a chance to reply. Thanks!"]}, {"number": 14105, "title": "tf.reshape() fails for shapes containing non-32 bit precision TF integer types", "body": "**Environment**: TensorFlow v1.3.0-rc2-20-g0787eee 1.3.0 running on Mac OS X (v10.12.6)\r\n\r\n**Issue**:\r\nFunction `tensorflow.reshape()` works when provided a list-based shape containing `tf.int32`; but, fails for, e.g., `tf.int16` and `tf.int64` with error message `TypeError: List of Tensors when single Tensor expected`.\r\n\r\n**Example**:\r\n```\r\nimport tensorflow as tf\r\nsrc = tf.constant(list(range(32)))\r\nfor dtype in ['int32', 'int64', 'int16']:\r\n\tprint(\"Calling tf.reshape using 'tf.{:s}'\".format(dtype))\r\n\ttf.reshape(src, [tf.cast(16, dtype), 2])\r\n```", "comments": ["The behavior is expected, see API: [tf.reshape](https://www.tensorflow.org/versions/master/api_docs/python/tf/reshape)\r\n> shape: A Tensor. Must be one of the following types: int32, int64. Defines the shape of the output tensor.", "Not sure I fully understand. The API seems to suggest that it should work for both `int32` and `int64`. In the example provided above, the first iteration (using `int32`) succeeds but the remaining do not.", "Yes, you're right. The `shape` must be either int32 or int64, unfortunately, mixing them is not permitted. To be honest, it is not convenient. Perhaps you can open a feature request:-) ", "By the way, cast `shape` to valid type might be useful for you.\r\n\r\n```python\r\nIn [9]: tf.reshape(src, tf.cast([tf.cast(16, tf.int64), 2], dtype=tf.int64))\r\nOut[9]: <tf.Tensor 'Reshape_1:0' shape=(16, 2) dtype=int32>\r\n```", "Thanks for helping to clarify this issue. Using what you've mentioned, the following work for both `int32` and `int64`: `tf.reshape(x, tf.cast([16, 2], dtype))`. This usage pattern suffices for my purposes; however, I would like to point out the following nuance (which still seems like a bug).\r\n\r\nOf the following, the former works but the latter does not: \r\n```\r\ntf.reshape(x, [tf.cast(16, 'int32'), tf.cast(2, 'int32')]) #works\r\ntf.reshape(x, [tf.cast(16, 'int64'), tf.cast(2, 'int64')]) #fails\r\n```\r\nPresumably, support was added somewhere along the lines to explicitly allow for this convention when using `int32`; otherwise, it should throw the same exception as the latter --- namely: `TypeError: List of Tensors when single Tensor expected`", "It's not perfect, I agree with you, @j-wilson .\r\nHowever, note that `tf.cast([16, 2], dtype)` is a tensor, while `[tf.cast(16, 'int64'), tf.cast(2, 'int64')]` is a list of tensors. In fact, `tf.shape` expects only a tensor for `shape`.", "Thank you for helping our friend @facaiy. The Python API makes a *best effort* to do type coercion, to make TensorFlow easier. It's highly nontrivial code and I'm not sure if collections with these sorts of mixed types was considered. While I'm sure it's something we'd like to support, I'm not sure if there would be performance ramifications to recursive type coercion. Therefore it might not be a feature we'd consider.\r\n\r\nAlso, one thing you might want to consider is to express these operations as TensorFlow operations, which could be verbose, but might help ensure that graph is fully defined and there isn't any performance bottlenecking magic that's happening."]}, {"number": 14104, "title": "tf.scatter_add causes error in loop", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, it's below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: no CUDA\r\n- **GPU model and memory**: no GPU\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI found very strange behavior of tf.scatter_add: I created a tf.while_loop that creates a Tensor wrapped inside a tf.Variable. If I don't add something to the Variable outside the loop, tensorflow causes an error telling me that the Variable is not mutable.\r\n\r\nI asked the on StackOverflow and was told to create a bug report.\r\nhttps://stackoverflow.com/questions/46935216/tf-scatter-add-causes-error-in-loop?noredirect=1#comment80914069_46935216\r\n\r\nUncommenting the commented line removes the error. But I don't think this is intended behavior.\r\n\r\n### Source code / logs\r\nimport tensorflow as tf        \r\n\r\nm = 25\r\nbatch_num = 32\r\nnum_bus = 50\r\n\r\nC = tf.zeros((m, batch_num, num_bus, m),tf.float64)\r\nC = tf.Variable(C)\r\n\r\nc = tf.ones((batch_num, num_bus, m), tf.float64)\r\n#C = tf.scatter_add(C,0,c)\r\n\r\nk = tf.constant(1)\r\n\r\nstop_cond = lambda k,C: k<m\r\n\r\ndef construct_C(k, C):\r\n    upd_c = c+1\r\n    C = tf.scatter_add(C,k,upd_c)\r\n    return k+1,C\r\n\r\nk,C = tf.while_loop(stop_cond,construct_C, (k,C))\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nC1 = sess.run(C)\r\n", "comments": ["Thank you so much for taking the time to file an issue as @ebrevdo requested. I shall now punt this tf.while_loop API issue to him.", "This is a case of tf.Variable being handled differently from a ref tensor by while_loop.  Thanks for reporting it.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Looking more carefully; I see that you're expecting the mutable variable C in the body of `construct_C`.  But Variables are not like regular tensors; you cannot pass them through a `tf.while_loop` in this way.  Instead what you want to do is:\r\n\r\n```python\r\ndef construct_C(k):\r\n  with tf.control_dependencies([tf.scatter_add(C, k, upd_c)]):\r\n    return k + 1\r\n\r\nthen later:\r\n\r\nwith tf.control_dependencies([tf.while_loop(stop_cond, construct_C, [tf.constant(1)]):\r\n  constructed_C_value = tf.identity(C)  # or C.read_value(), i think\r\n```\r\n\r\nthe reason is that `C`, as a Variable, exists outside of regular flow control.  you can mutate it as you see fit.  We do provide some sugar, like `C1 = tf.scatter_add(C, ...)`, but all this does is creates a control dependency to update the variable pointed to by both `C` and `C1`, to be executed when you access `C1`.  This kind of sugar doesn't propagate through `tf.while_loop`; but the example I gave above should work just fine."]}, {"number": 14103, "title": " Resolve //tensorflow relative to tensorflow repo", "body": "Fixes #13984 ", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Signed CLA with thomas.deegan@getcruise.com", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Added thomas.deegan@getcruise.com to my emails.. Are you satisfied now Google bot?!!", "@gunan Can you take a look?", "Jenkins, test this please.", "Hmmm. Can blaze resolve @org_tensorflow internally within g3? Any idea what these failures are?", "Those are infra issues we have been having today on our test cluster.\r\nI retriggered the tests to clear them.\r\n\r\nFor the CLA issues, I think it is because your commit emails are not matching the email you signed the CLA with. I can manually mark the change good, as I verified the email you provided.\r\n", "Manually verified CLA with user email. Merging."]}, {"number": 14102, "title": "Dockerfile: do not perform cleanup in a separate RUN statement", "body": "Cleanup must be performed in the same statement, otherwise the build\r\nfiles are still stored in the upper layer and no space is reclaimed.\r\n\r\nSigned-off-by: Felix Abecassis <fabecassis@nvidia.com>\r\n\r\n\r\n@gunan\r\nImage size before the patch: 8.21GB\r\nImage size after the patch: 4.99GB\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@flx42 are you covered by NVIDIA CLA?\r\nYou may need to change your commit email to your company email.", "Yeah, I'm trying to get added to the corporate CLA. Or maybe @benbarsdell will submit a new PR on my behalf if it takes too long.", "Jenkins, test this please.", "Ping!\r\n@flx42 where are we on the CLA?", "The gatekeeper of our corporate CLA was OOO until today, hopefully it can be completed today.\r\n\r\nShould I be worried by these CI failures?", "No, we are having infra issues these days.\r\nAs soon as CLA is taken care of, I will merge the PR.", "@googlebot I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Is an image of this available on the docker hub anywhere, officially or not? Just a build of `Dockerfile.devel-gpu-cuda9-cudnn7` since this patch. ", "No official docker images exist. You can create an image simply by running docker build on this image, but this is just for preparing for migration of everything to cuda9.", "@gunan do you have prebuilt pip files for CUDA 9.0 yet?", "We are trying to get them ready, but we do not have one yet."]}, {"number": 14101, "title": "Computing gradients within tf.while_loop", "body": "I am posting this here because [a similar question on stackoverflow](https://stackoverflow.com/questions/42313788/how-to-do-opt-compute-gradients-multiple-times-in-single-sess-run) is still unanswered, so I suspect it might be a bug. \r\n\r\nAdding gradient ops within a tf.while_loop for computing gradients of loop variables w.r.t external variables results in an error. \r\n\r\nProgram reproducing the error:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.reset_default_graph()\r\nF = lambda x: tf.cumsum(x)\r\nG = lambda x: x[-1]\r\nH = lambda x: x\r\nencoder_emb_inp = tf.placeholder(dtype=tf.float32, shape=[4])\r\nencoder_outputs = F(encoder_emb_inp)\r\ndecoder_initial_state = G(encoder_outputs)\r\ndecoder_initial_output = H(decoder_initial_state)\r\ndef cond(time, unused_state, unused_output):\r\n    return tf.less(time, 3)\r\n\r\ndef body(time, state, inputs):\r\n    step = lambda s, i: (tf.multiply(s,s), tf.multiply(s,i))\r\n    (next_state, next_output) = step(state, inputs)\r\n    next_grads = tf.gradients(next_output, decoder_initial_state)\r\n    tf.Print(next_grads, next_grads)\r\n    return (time + 1, next_state, next_output)\r\n\r\ninitial_time = tf.constant(0, dtype=tf.int32)\r\n\r\n\r\nfinal_time, final_state, final_outputs = tf.while_loop(cond, body, loop_vars = [initial_time, decoder_initial_state, decoder_initial_output])\r\n```\r\nError message: \r\n```\r\n<ipython-input-126-5205901211cc> in body(time, state, inputs)\r\n      6     (next_state, next_output) = step(state, inputs)\r\n      7 #    next_grads = tf.gradients(next_output, state)\r\n----> 8     next_grads = tf.gradients(next_output, decoder_initial_state)\r\n      9     tf.Print(next_grads, next_grads)\r\n     10     return (time + 1, next_state, next_output)\r\n\r\n/home/pramodkm/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\r\n    591                 out_grads[i] = loop_state.ZerosLike(op, i)\r\n    592               else:\r\n--> 593                 out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\r\n    594           with ops.name_scope(op.name + \"_grad\"):\r\n    595             # pylint: disable=protected-access\r\n\r\n/home/pramodkm/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in ZerosLikeOutsideLoop(op, index)\r\n   1342     if op_ctxt:\r\n   1343       # We are in a cond context. Use a switch to create zeros only when needed.\r\n-> 1344       pred = op_ctxt.pred\r\n   1345       branch = op_ctxt.branch\r\n   1346       switch_val = switch(op.inputs[0], pred)[1 - branch]\r\n\r\nAttributeError: 'WhileContext' object has no attribute 'pred'\r\n```\r\nI am using tf-nightly-gpu 1.5.0-dev20171026\r\n\r\nThanks!", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "No update at this time. I'll try to take a look soon but have a lot of other stuff on my plate currently.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It seems like this is the same as [#9450](https://github.com/tensorflow/tensorflow/issues/9450).", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hey, I finally took a look at this. Unfortunately I believe this is impossible to compute without a major change to how we compute gradients :( I'll at least work on providing a better error message though.", "Thanks!", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 14100, "title": "Branch 173904309", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 14099, "title": "fix fused_batchnorm_test.py", "body": "In file fused_batchnorm_test.py,  the function [_reference_training](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/fused_batchnorm_test.py#L39) use \r\n```python\r\nelement_count = np.size(x) / int(np.shape(x)[0])\r\n```\r\nto get element_count for `NHWC` data_format, but it should be \r\n```\r\nelement_count = np.size(x) / int(np.shape(x)[-1])\r\n```\r\nThe last dimention is the channel number, the reason why it works now is that the test data is `[2,2,6,2]`   \r\n```python\r\nnp.shape(x)[0] == np.shape(x)[-1] == 2\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Can anyone help to review this PR? @tensorflow-jenkins ", "what is the CI fail about? It does not have any error log. @andrewharp ", "The issue looks like an interesting flake, I cant see how it can be related to your change. Will rerun tests to see what is going on.\r\nJenkins, test this please."]}, {"number": 14098, "title": "Add KafkaReader for processing streaming data with Apache Kafka", "body": "This is a proposal to add KafkaReader so that it is possible to read data from Kafka like TextLineReader and TFRecordReader.\r\n\r\nApache Kafka is a widely used distributed streaming platform in open source community. The goal of this fix is to create a contrib Reader ops (inherits ReaderBase and is similiar to TextLineReader / TFRecordReader) so that it is possible to reader Kafka streaming data from TensorFlow in a similiar fashion.\r\n\r\nThis fix uses a C/C++ Apache Kafka client library librdkafka which is released under the 2-clause BSD license, and is widely used in a number of Kafka bindings such as Go, Python, C#/.Net, etc.\r\n\r\nBelow is a sample usage:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.kafka as kafka\r\n\r\n# The name of test:0:0:10\r\n# test: topic\r\n# 0: partition\r\n# 0: offset\r\n# 10: length (or -1 for unlimited)\r\n#\r\n# The parameters for KafkaReader\r\n# servers: bootstrap.servers\r\n# group: group.id\r\n# eof: If True then read will block until the length of the data\r\n#      has been read and ignore EOF error from Kafka\r\n\r\nfilename_queue = tf.train.string_input_producer([\"test:0:0:10\"])\r\n\r\nreader = kafka.KafkaReader(servers=\"localhost\", group=\"test\", eof=True)\r\nkey, value = reader.read(filename_queue)\r\nwith tf.Session() as sess:\r\n  coord = tf.train.Coordinator()\r\n  threads = tf.train.start_queue_runners(coord=coord)\r\n  for i in range(10):\r\n    print(sess.run([key, value]))\r\n  coord.request_stop()\r\n  coord.join(threads)\r\n```\r\n\r\nNOTE: Because of #1419, the clean_deps of `tensorflow/core:framework` and `tensorflow/core:lib` has been commented out temporarily so that it is possible to build with ReaderBase. Any suggestions to address this issue is welcomed.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "/cc @terrytangyuan for the discussions.", "@jhseu can you take a look?", "Seems reasonable to add, but can we implement this as a Dataset instead? See `tensorflow/core/kernels/reader_dataset_ops.cc`", "The queue style of reading will be deprecated.", "Thanks @jhseu for the feedback! Let me take a look and update the PR accordingly.", "It seems that Dataset requires `tensorflow/core/kernels/dataset.cc` to link as a `.so`. However, that is problematic becuase the class of `Dataset` will exist in two places and the kernel will not recognize the one in `.so`.\r\n\r\nThat is quite similar to the issue faced in  #1419. Not sure how to get around it other than building the KafkaDataset into the core kernel.", "@mrry @allenlavoie Derek or Allen, any thoughts on how we might make a dataset in contrib that gets exposed as a shared library? I'm guessing it would just require moving some of the classes over to the framework directory?", "@jhseu It should be possible in the next week or two. The current layering is bit more intricate than that, but a couple of changes in flight will enable us to remove the current dependency on `core/common_runtime` and allow building `DatasetOpKernel` and `DatasetBase` subclasses in a separate shared library.", "Thanks @jhseu @mrry, please keep me posted on the changes in Dataset so that I could update the PR accordingly.", "@jhseu @mrry have the changes been pushed? Thanks.", "Not yet: we're waiting on a CL from @rohan100jain that will sever the dependency between `captured_function.cc` and `tensorflow/core/common_runtime/...`, so we can split all the `tf.contrib.data` kernels into a separate library that only depends on `tensorflow/core/framework/...`.", "Thanks! @rohan100jain please ping this thread once ready.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mrry @rohan100jain was the CL submitted?", "Yes that change was submitted. https://github.com/tensorflow/tensorflow/commit/9f4118d00fa9eb85f81a4eb3f96a5583ae5afcdc ", "OK, @jhseu I think it's ready for review.", "Thanks @jhseu for the review. The PR has been updated with comments addressed. Please take a look and let me know if there are any issues.", "Thanks! This is greatly appreciated."]}, {"number": 14097, "title": "how can i get a specific version of tf", "body": "## platform\r\nraspbian 8.0\r\n\r\n## problem\r\nI need this specific release of tensorflow... and does anyone have the repository of that version?\r\nit needs to be a source file.. because of some specific situation and some difficulties that hard to solve, I have to compile it from source and get a consequence similar to `git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git` but in the version of 1.1.0\r\n\r\n**related:**\r\nhttps://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md\r\nafter bazel build in this guidance, the contributor gave the command like this:\r\n`sudo pip install /tmp/tensorflow_pkg/tensorflow-1.1.0-cp27-none-linux_armv7l.whl `\r\nI don't know whether this command will have some influence.. but an release can adapt to this command will be ideal...\r\n\r\nmany thx!", "comments": ["You can switch to the release branch of the appropriate version. The release branches are named `r<version_number>`, see all branches at https://github.com/tensorflow/tensorflow/branches/all\r\n\r\nFor example:\r\n\r\n```sh\r\ngit checkout r1.2\r\n```\r\n\r\nwill take you to the version 1.2 branch.\r\n", "thx !"]}, {"number": 14096, "title": "Unable to use NeonDepthwiseConv2dNativeOp ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 14095, "title": "matrix_triangular_solve has broken docstring", "body": "### System information\r\nnot applicable (regards documentation)\r\n\r\n### Describe the problem\r\nThe docstring of matrix_triangular_solve is not clear, see https://www.tensorflow.org/api_docs/python/tf/matrix_triangular_solve. There are formatting issues, but moreover, it is not immediately clear what the effect of `adjoint=True` is. It would be helpful to describe the effect using linear algebra notation.\r\n\r\n### Source code / logs\r\nnot applicable (regards documentation)", "comments": ["What language would you suggest?\r\nContributions are welcome :) - you'd want to change the string here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/linalg_ops.cc#L430 \r\n\r\nFYI @rmlarsen in case he has some comments.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to inactivity.\r\n\r\nBTW the documentation for the op is now here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_MatrixTriangularSolve.pbtxt."]}, {"number": 14094, "title": "Utility classes for writing Java source code from a C++ process", "body": "This PR sets up a few tools for writing Java source files from the op generator (or any future generator of Java code written in C++).\r\n\r\nThey are handling most of the tricky cases known to be encountered when we'll generate the ops, including generics, wildcards, optional inner classes, etc. Note that they focus on simplicity rather than performance or memory optimitizations since they will only be used at compile time.\r\n\r\nAs stated in a previous PR, java_writer and source_writer are being kept separate since the later might be a good candidate to be move to the core::io layer if required some day.", "comments": ["Can one of the admins verify this patch?", "@asimshankar , noticing that you have not been assigned this PR yet, in case you missed it... thanks", "Just a little poke for taking a look at this PR please, thanks!", "@asimshankar PTAL", "Hi @asimshankar , sorry for the delay, I was very busy lately. So as you suggested, I split this PR into smaller ones, starting from the Java definitions. I also incorporated your previous suggestions related to them, please take a look when you'll have a minute, thanks!", "@karllessard I am not sure you pushed the comment that addresses @asimshankar comments?", "@drpngx , you are right, last comments are not pushed yet but I'll do it in the following days, thank you.", "Ok, that's completed now.", "```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n112,113d111\r\n<         \"src/gen/cc/op_generator.h\",\r\n<         \"src/gen/cc/op_generator.cc\",\r\n114a113,114\r\n>         \"src/gen/cc/op_generator.cc\",\r\n>         \"src/gen/cc/op_generator.h\",\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n\r\n```", "Done, sorry for that.", "Jenkins, test this please.", "Jenkins, test this please", "Jenkins, test this please\r\n\r\n(The previous test failure seems test infrastructure related)", "FYI I think Jenkins might be stuck today\n\nOn Fri, Dec 29, 2017, 10:00 PM Asim Shankar <notifications@github.com>\nwrote:\n\n> Jenkins, test this please\n>\n> (The previous test failure seems test infrastructure related)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14094#issuecomment-354529507>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbXSyeu-_lvzQzzJNtqBj0amtZdNcks5tFdFmgaJpZM4QLRYK>\n> .\n>\n", "The one test failure there is definitely unrelated, so I think this PR is ready to be merged."]}, {"number": 14093, "title": "Error: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE", "body": "Hi, I am working on a remote machine with many GPU cards that has CUDA v8.0.61 and Cudnn 5.1.10 installed. Upgrading these is not possible because of permission rights. In my own environment I installed TensorFlow version 1.2.1, but I get the following error when launching a `Session()`:\r\n\r\n```\r\nimport tensorflow as tf\r\nsess=tf.Session()\r\n2017-10-30 13:48:47.652605: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-30 13:48:47.652665: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-30 13:48:47.652680: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-30 13:48:47.652694: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-30 13:48:47.652708: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-30 13:48:47.930480: E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1292, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 562, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/home/user/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.'\r\n```\r\n\r\nThe output when calling `nvidia-smi` is\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\r\n| N/A   52C    P0    61W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\r\n| N/A   40C    P0    76W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K80           Off  | 0000:0A:00.0     Off |                    0 |\r\n| N/A   46C    P0    60W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K80           Off  | 0000:0B:00.0     Off |                    0 |\r\n| N/A   37C    P0    74W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla K80           Off  | 0000:10:00.0     Off |                    0 |\r\n| N/A   44C    P0    59W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla K80           Off  | 0000:11:00.0     Off |                    0 |\r\n| N/A   33C    P0    75W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla K80           Off  | 0000:14:00.0     Off |                    0 |\r\n| N/A   43C    P0    58W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla K80           Off  | 0000:15:00.0     Off |                    0 |\r\n| N/A   33C    P0    73W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   8  Tesla K80           Off  | 0000:87:00.0     Off |                    0 |\r\n| N/A   35C    P0    58W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   9  Tesla K80           Off  | 0000:88:00.0     Off |                    0 |\r\n| N/A   31C    P0    74W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  10  Tesla K80           Off  | 0000:8B:00.0     Off |                    0 |\r\n| N/A   51C    P0   142W / 149W |   8361MiB / 11439MiB |    100%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  11  Tesla K80           Off  | 0000:8C:00.0     Off |                    0 |\r\n| N/A   31C    P0    86W / 149W |     86MiB / 11439MiB |     48%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  12  Tesla K80           Off  | 0000:91:00.0     Off |                    0 |\r\n| N/A   51C    P0   143W / 149W |   8330MiB / 11439MiB |     99%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  13  Tesla K80           Off  | 0000:92:00.0     Off |                    0 |\r\n| N/A   23C    P8    30W / 149W |      2MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  14  Tesla K80           Off  | 0000:95:00.0     Off |                    0 |\r\n| N/A   25C    P8    28W / 149W |      2MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  15  Tesla K80           Off  | 0000:96:00.0     Off |                    0 |\r\n| N/A   23C    P8    30W / 149W |      2MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n                                                                               \r\nAny ideas on the error `E tensorflow/core/common_runtime/direct_session.cc:138] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE`? Running `tf.Session()` multiple times results in the same error but will increase the number behind \"ordinal\".\r\n", "comments": ["Also, it is not a memory error on the GPUs. When explicitly setting CUDA_VISIBLE_DEVICES to a non occupied GPU I get the same error. One time I seemed to get lucky as the following command worked:\r\n\r\n```\r\n(root) [~]$ CUDA_VISIBLE_DEVICES=13 python -c \"import tensorflow as tf;tf.InteractiveSession()\r\n\r\n\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:92:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:92:00.0)\r\n```\r\n\r\nBut it keeps failing all other times (on GPUs that aren't occupied)", "It now seems I can only use the GPUs that have a P8 status (not the ones with P0)\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\r\n| N/A   53C    P0    61W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:07:00.0     Off |                    0 |\r\n| N/A   41C    P0    76W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K80           Off  | 0000:0A:00.0     Off |                    0 |\r\n| N/A   47C    P0    60W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K80           Off  | 0000:0B:00.0     Off |                    0 |\r\n| N/A   37C    P0    74W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla K80           Off  | 0000:10:00.0     Off |                    0 |\r\n| N/A   45C    P0    59W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla K80           Off  | 0000:11:00.0     Off |                    0 |\r\n| N/A   34C    P0    75W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla K80           Off  | 0000:14:00.0     Off |                    0 |\r\n| N/A   43C    P0    58W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla K80           Off  | 0000:15:00.0     Off |                    0 |\r\n| N/A   34C    P0    73W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   8  Tesla K80           Off  | 0000:87:00.0     Off |                    0 |\r\n| N/A   36C    P0    58W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|   9  Tesla K80           Off  | 0000:88:00.0     Off |                    0 |\r\n| N/A   32C    P0    74W / 149W |     65MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  10  Tesla K80           Off  | 0000:8B:00.0     Off |                    0 |\r\n| N/A   51C    P0   103W / 149W |   8361MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  11  Tesla K80           Off  | 0000:8C:00.0     Off |                    0 |\r\n| N/A   24C    P8    30W / 149W |      2MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  12  Tesla K80           Off  | 0000:91:00.0     Off |                    0 |\r\n| N/A   51C    P0   119W / 149W |   8330MiB / 11439MiB |    100%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  13  Tesla K80           Off  | 0000:92:00.0     Off |                    0 |\r\n| N/A   24C    P8    30W / 149W |      2MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  14  Tesla K80           Off  | 0000:95:00.0     Off |                    0 |\r\n| N/A   25C    P8    28W / 149W |      2MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n|  15  Tesla K80           Off  | 0000:96:00.0     Off |                    0 |\r\n| N/A   23C    P8    30W / 149W |      2MiB / 11439MiB |      0%   E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nAnd the following works (all other GPUs/combinations don't):\r\n\r\n```\r\n$ CUDA_VISIBLE_DEVICES=13,14,15,11 python -c \"import tensorflow as tf;tf.InteractiveSession()\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:92:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x7f71ffcba0b0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:95:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x7f71ffcbda40\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:96:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x7f71ffcc1680\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:8c:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:92:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:95:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: Tesla K80, pci bus id: 0000:96:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: Tesla K80, pci bus id: 0000:8c:00.0)\r\n```", "hi, how do you solve this problem?"]}, {"number": 14092, "title": "There is a Error about: Too many include files", "body": "when i try to built example/label_image/main.cc . Visual studio will throw an error :c1014 too many include files . Click the message , this error point to a file of Tensor.  can anybody tell me how to fix this problem.thank you.\r\nThis is my first time to write English litter , and i am not good at english , wish you can understood what i said . lol ", "comments": ["I'd be interested in learning more about this. Please fill out the issue template. Also how exactly is Visual Studio happening? Are you importing all the code into the IDE? Running CMake? Running Makefile? Running Bazel? What version of Visual Studio? I must admit I haven't used it since 6.0 but when I did it was awesome.", " thanks for your reply @jart  ,\r\n\r\n1.  i used visual studio 2015 to run the exampale\r\n2.  i only add main.cc file into a new project as a soruce file \r\n3.   in the Option dialog , fill the head file Directory into the \"VC++ Directories->include Directories\"\r\n     E:\\TensorFlow\\tensorflow-master\r\n     E:\\TensorFlow\\tensorflow-master\\tensorflow\r\n     E:\\TensorFlow\\tensorflow-master\\third_party\\eigen3\r\n     E:\\TensorFlow\\tensorflow-master\\tensorflow\\core\\framework\r\n\r\n\r\n\r\n\r\n", "Last night, i download the Negin3 soruce code form internet . replace  local Tensorflow\\eigen3 file , then Error was dispeared , but still has a problem that i can not found .ph.h files . How can i get those ?  ", "I wish I could help but this question is probably better for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There's a larger community there that provides support. This issue tracker is more for bugs and feature requests and very few members of the team has VS experience.", "OK,thanks for your suggestion ."]}, {"number": 14091, "title": "small unseen typos", "body": "small typos fixed\r\n\r\nsee the changed files", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14090, "title": "grammer fixed", "body": "fixed a small typo", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14089, "title": "Unable to download the training images from curl http://download.tensorflow.org/example_images/flower_photos.tgz", "body": "![image](https://user-images.githubusercontent.com/16485968/32168606-5394209a-bd93-11e7-91a2-003a3f08e743.png)\r\n", "comments": ["Please provide details on what exactly you're doing. It seems like you're using `curl` but not specifying the file to download into, so it's dumping the contents to your screen.\r\n\r\n```sh\r\ncurl \"http://download.tensorflow.org/example_images/flower_photos.tgz\" -o flower_photos.tgz\r\n```\r\n\r\nseems to work fine.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Yeah still issue persist ", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "Closing this out.\r\n\r\n@Lavz see the example @asimshankar provided about how to run `curl` with the `-o` output file option.\r\n\r\nIf you feel you've hit a TensorFlow bug, please file a new issue and include details on exactly what you're doing.", "downloaded but can't open these files\r\n"]}, {"number": 14088, "title": "typo fixed", "body": "there is an additional whitespace between ' the method '", "comments": ["Can one of the admins verify this patch?", "Can any one review this code"]}, {"number": 14087, "title": "tf.contrib.boosted_trees cannot be used in 1.4.0rc1", "body": "TF Version: 1.4.0rc1\r\nPy Version: 2.7\r\nOS: Mac OS\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeClassifier\r\nfrom tensorflow.contrib.boosted_trees.proto import learner_pb2\r\n\r\nlearner_config = learner_pb2.LearnerConfig()\r\n\r\nlearner_config.learning_rate_tuner.fixed.learning_rate = 0.1\r\nlearner_config.num_classes = 10\r\nlearner_config.regularization.l1 = 0.0\r\nlearner_config.regularization.l2 = 1.0 / 1000\r\nlearner_config.constraints.max_tree_depth = 4\r\nlearner_config.growing_mode = learner_pb2.LearnerConfig.LAYER_BY_LAYER\r\nlearner_config.multi_class_strategy = (learner_pb2.LearnerConfig.DIAGONAL_HESSIAN)\r\n\r\nestimator = GradientBoostedDecisionTreeClassifier(\r\n    learner_config = learner_config,\r\n    n_classes = 10,\r\n    examples_per_layer = 1000,\r\n    num_trees = 10,\r\n    center_bias = False)\r\n\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nX_train = (X_train / 255.).reshape(-1, 28*28).astype(np.float32)\r\ny_train = y_train.astype(np.int32)\r\n\r\nestimator.fit(input_fn=tf.estimator.inputs.numpy_input_fn(\r\n    x={'features':X_train}, y=y_train, batch_size=128, num_epochs=1, shuffle=True))\r\n```\r\n\r\nA minimal example to train on mnist dataset throws error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"boost_tree.py\", line 29, in <module>\r\n    x={'features':X_train}, y=y_train, batch_size=128, num_epochs=1, shuffle=True))\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 316, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 480, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 986, in _train_model\r\n    model_fn_ops = self._get_train_ops(features, labels)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1202, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1166, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py\", line 116, in model_builder\r\n    logits=logits)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1064, in create_model_fn_ops\r\n    enable_centered_bias=self._enable_centered_bias)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 648, in _create_model_fn_ops\r\n    batch_size, loss_fn, weight_tensor)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1923, in _train_op\r\n    train_op = train_op_fn(loss)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py\", line 105, in _train_op_fn\r\n    update_op = gbdt_model.train(loss, predictions_dict, labels)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py\", line 543, in train\r\n    hessian_list = self._diagonal_hessian(gradients, predictions)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py\", line 845, in _diagonal_hessian\r\n    aggregation_method=None)\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/zhedongzheng/pyenv/py2.7/tf_1.4/tf_1.4/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 352, in _PreventGradientGrad\r\n    \"Gradient explicitly disabled. Reason: %s\" % op.get_attr(\"message\"))\r\nLookupError: Gradient explicitly disabled. Reason: Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation's interaction with tf.gradients()\r\n```", "comments": ["@tkoeppe : Mind taking a look?", "@asimshankar: Do you mean because of my BUILD cleanups? All the tests passed, but maybe. @zhedongzheng, would you mind testing with the relevant changes from 966016b7f2382658e7c84baae0596d35f0a49bae and 6ef542822b8760de4beeb4ad2602fb863888d47f reverted?", "@tkoeppe hi, i want to help, but i don't really know how to go back to previous commits in the codebase.", "@zhedongzheng: Oh, I just meant to edit the changes back in manually. No need for fancy git gymnastics.", "@tkoeppe The code does not work in TF 1.3 either, so i don't think it is a BUILD problem, it is an implementation problem i suppose.", "@ispirmustafa any insight about this? or do you know who should be called? Many thanks.", "@asimshankar: I'm not otherwise terribly familiar with TF, sorry! Back to you?\r\n\r\n@zhedongzheng: Thanks for investigating!", "One of my colleagues suggested that second-order gradients are not supported well across all of TF, which is what the error message is trying to say. Does that help? Has this code ever worked in the past?", "The issue has been fixed in 34a4b21f8f9dea64d3e99a97f639396f2d5556d3 by switching to a custom loss function that had support for second order gradient within the GBDTClassifier estimator.  You can wait for the release of that change or see boosted_trees/examples/mnist.py [before the above commit](https://github.com/tensorflow/tensorflow/blob/c41dbc3c1832bc6c3662d4d942d095baa1fb49c9/tensorflow/contrib/boosted_trees/examples/mnist.py#L92) which uses the custom loss function. "]}, {"number": 14086, "title": "Minor change in tolerance to pass resnet_v1_test test on ppc64le", "body": "As discussed here - https://github.com/tensorflow/tensorflow/issues/13992 , I am raising this PR to fix resnet_v1_test failure.\r\n\r\nThanks!\r\nSandip", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 14085, "title": "How can I run a Tensorflow file on GPUs? Error: Cannot assign a device for operation 'eval_step'", "body": "Do you have any idea how can I run [\"eval_image_classifier.py\"][1] on GPUs? Should I change any functions or do any modifications? Or whether there exist any other specific functions for evaluation on GPUs?\r\n\r\nI can already run [\"train_image_classifier.py\"][2] on GPUs because of having the associated flag for switching between CPU and GPU:\r\n\r\n    tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\r\n                            'Use CPUs to deploy clones.')\r\n\r\n\r\nI did try to add the same line to `eval_image_classifier.py`, but it had no effect. I'm using Python 2.7.13 and Tensorflow 1.3.0 .\r\n\r\n    from __future__ import absolute_import\r\n    from __future__ import division\r\n    from __future__ import print_function\r\n    import math\r\n    import tensorflow as tf\r\n    from deployment import model_deploy\r\n    from datasets import dataset_factory\r\n    from nets import nets_factory\r\n    from preprocessing import preprocessing_factory\r\n    slim = tf.contrib.slim\r\n    \r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'batch_size', 32, 'The number of samples in each batch.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'max_num_batches', None,\r\n        'Max number of batches to evaluate by default use all.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'master', '', 'The address of the TensorFlow master to use.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'checkpoint_path', '...',\r\n        'The directory where the model was written to or an absolute path to a '\r\n        'checkpoint file.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'eval_dir', '...',\r\n        'Directory where the results are saved to.')\r\n    \r\n    tf.app.flags.DEFINE_integer('num_clones', 1,\r\n                                'Number of model clones to deploy.')\r\n    \r\n    tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\r\n                                'Use CPUs to deploy clones.')\r\n    \r\n    tf.app.flags.DEFINE_integer('worker_replicas', 1, 'Number of worker replicas.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'num_readers', 4,\r\n        'The number of parallel readers that read data from the dataset.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'num_ps_tasks', 0,\r\n        'The number of parameter servers. If the value is 0, then the parameters '\r\n        'are handled locally by the worker.')\r\n    \r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'num_preprocessing_threads', 4,\r\n        'The number of threads used to create the batches.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'dataset_name', '...', 'The name of the dataset to load.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'dataset_split_name', 'validation', 'The name of the train/test split.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'dataset_dir', '...', \r\n        'The directory where the dataset files are stored.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'labels_offset', 0,\r\n        'An offset for the labels in the dataset. This flag is primarily used to '\r\n        'evaluate the VGG and ResNet architectures which do not use a background '\r\n        'class for the ImageNet dataset.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'model_name', 'densenet161', 'The name of the architecture to evaluate.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'preprocessing_name', None, 'The name of the preprocessing to use. If left '\r\n        'as `None`, then the model_name flag is used.')\r\n    \r\n    tf.app.flags.DEFINE_float(\r\n        'moving_average_decay', None,\r\n        'The decay to use for the moving average.'\r\n        'If left as None, then moving averages are not used.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'eval_image_size', None, 'Eval image size')\r\n    \r\n    FLAGS = tf.app.flags.FLAGS\r\n    \r\n    \r\n    def main(_):\r\n      if not FLAGS.dataset_dir:\r\n        raise ValueError('You must supply the dataset directory with --dataset_dir')\r\n        \r\n        #######################\r\n        # Config model_deploy #\r\n        #######################\r\n      tf.logging.set_verbosity(tf.logging.INFO)\r\n      with tf.Graph().as_default():\r\n          \r\n        deploy_config = model_deploy.DeploymentConfig(\r\n          num_clones=FLAGS.num_clones,\r\n          clone_on_cpu=FLAGS.clone_on_cpu,\r\n          #replica_id=FLAGS.task,\r\n          num_replicas=FLAGS.worker_replicas,\r\n          num_ps_tasks=FLAGS.num_ps_tasks)\r\n          \r\n        # Create global_step\r\n        with tf.device(deploy_config.variables_device()):\r\n          tf_global_step = slim.create_global_step()\r\n        ######################\r\n        # Select the dataset #\r\n        ######################\r\n        dataset = dataset_factory.get_dataset(\r\n            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\r\n    \r\n        ####################\r\n        # Select the model #\r\n        ####################\r\n        network_fn = nets_factory.get_network_fn(\r\n            FLAGS.model_name,\r\n            num_classes=(dataset.num_classes - FLAGS.labels_offset),\r\n            is_training=False)\r\n    \r\n        ##############################################################\r\n        # Create a dataset provider that loads data from the dataset #\r\n        ##############################################################\r\n        with tf.device(deploy_config.inputs_device()):\r\n            provider = slim.dataset_data_provider.DatasetDataProvider(\r\n                dataset,\r\n                num_readers=FLAGS.num_readers,\r\n                shuffle=False,\r\n                common_queue_capacity=2 * FLAGS.batch_size,\r\n                common_queue_min=FLAGS.batch_size)\r\n            [image, label] = provider.get(['image', 'label'])\r\n            label -= FLAGS.labels_offset\r\n    \r\n        #####################################\r\n        # Select the preprocessing function #\r\n        #####################################\r\n        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\r\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n            preprocessing_name,\r\n            is_training=False)\r\n    \r\n        eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\r\n    \r\n        image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\r\n    \r\n        images, labels = tf.train.batch(\r\n            [image, label],\r\n            batch_size=FLAGS.batch_size,\r\n            num_threads=FLAGS.num_preprocessing_threads,\r\n            capacity=5 * FLAGS.batch_size)\r\n        batch_queue = slim.prefetch_queue.prefetch_queue(\r\n            [images, labels], capacity=2 * deploy_config.num_clones)\r\n        ####################\r\n        # Define the model #\r\n        ####################\r\n        def clone_fn(batch_queue):\r\n          \"\"\"Allows data parallelism by creating multiple clones of network_fn.\"\"\"\r\n          with tf.device(deploy_config.inputs_device()):\r\n            images, labels = batch_queue.dequeue()\r\n          logits, end_points = network_fn(images)\r\n          logits = tf.squeeze(logits)\r\n    \r\n          #############################\r\n          # Specify the loss function #\r\n          #############################\r\n          if 'AuxLogits' in end_points:\r\n            tf.losses.mean_squared_error(\r\n                predictions=end_points['AuxLogits'], labels=labels, weights=0.4, scope='aux_loss')\r\n          tf.losses.mean_squared_error(\r\n              predictions=logits, labels=labels, weights=1.0)\r\n          return end_points\r\n    \r\n        #clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\r\n        #first_clone_scope = deploy_config.clone_scope(0)\r\n        ####################\r\n        # Define the model #\r\n        ####################\r\n        logits, _ = network_fn(images)\r\n    \r\n        if FLAGS.moving_average_decay:\r\n          variable_averages = tf.train.ExponentialMovingAverage(\r\n              FLAGS.moving_average_decay, tf_global_step)\r\n          variables_to_restore = variable_averages.variables_to_restore(\r\n              slim.get_model_variables())\r\n          variables_to_restore[tf_global_step.op.name] = tf_global_step\r\n        else:\r\n          variables_to_restore = slim.get_variables_to_restore()\r\n    \r\n        logits = tf.squeeze(logits)\r\n    \r\n        # Define the metrics:\r\n        predictions = logits\r\n    \r\n        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n            'Accuracy': tf.metrics.root_mean_squared_error(predictions, labels),\r\n            'Recall_5': slim.metrics.streaming_recall(\r\n                logits, labels),\r\n        })\r\n    \r\n        # Print the summaries to screen.\r\n        print_ops = []\r\n        summary_ops = []\r\n        for name, value in names_to_values.items():\r\n          summary_name = 'eval/%s' % name\r\n          op = tf.summary.scalar(summary_name, value, collections=[])\r\n          op = tf.Print(op, [value], summary_name)\r\n          summary_ops.append(op)\r\n          print_ops.append(tf.Print(value, [value], summary_name))\r\n          tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\r\n    \r\n        # TODO(sguada) use num_epochs=1\r\n        if FLAGS.max_num_batches:\r\n          num_batches = FLAGS.max_num_batches\r\n        else:\r\n          # This ensures that we make a single pass over all of the data.\r\n          num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\r\n    \r\n        if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\r\n            if tf.train.latest_checkpoint(FLAGS.checkpoint_path):\r\n                checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\r\n            else:\r\n                checkpoint_path = FLAGS.checkpoint_path\r\n    \r\n        eval_interval_secs = 6\r\n        \r\n        tf.logging.info('Evaluating %s' % checkpoint_path)\r\n    \r\n        slim.evaluation.evaluation_loop(\r\n            master=FLAGS.master,\r\n            checkpoint_dir=checkpoint_path,\r\n            logdir=FLAGS.eval_dir,\r\n            num_evals=num_batches,\r\n            eval_op=list(names_to_updates.values()) + print_ops,\r\n            variables_to_restore=variables_to_restore,\r\n            eval_interval_secs = eval_interval_secs )\r\n    if __name__ == '__main__':\r\n      tf.app.run()\r\n\r\nI tried to use some code like Tensorflow tutorial as well:\r\n    \r\n    # Creates a graph.\r\n    with tf.device('/gpu:2'):\r\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n    c = tf.matmul(a, b)\r\n    # Creates a session with allow_soft_placement and log_device_placement set to True.\r\n    sess = tf.Session(config=tf.ConfigProto(\r\n             allow_soft_placement=True, log_device_placement=True))\r\n    # Runs the op.\r\n    print(sess.run(c))\r\n\r\nI modified the code in this way:\r\n\r\n    \r\n    from __future__ import absolute_import\r\n    from __future__ import division\r\n    from __future__ import print_function\r\n    \r\n    import math\r\n    import tensorflow as tf\r\n    \r\n    from datasets import dataset_factory\r\n    from nets import nets_factory\r\n    from preprocessing import preprocessing_factory\r\n    \r\n    slim = tf.contrib.slim\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'batch_size', 32, 'The number of samples in each batch.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'max_num_batches', None,\r\n        'Max number of batches to evaluate by default use all.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'master', '', 'The address of the TensorFlow master to use.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'checkpoint_path', '...',\r\n        'The directory where the model was written to or an absolute path to a '\r\n        'checkpoint file.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'eval_dir', '...',\r\n        'Directory where the results are saved to.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'num_preprocessing_threads', 4,\r\n        'The number of threads used to create the batches.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'dataset_name', '...', 'The name of the dataset to load.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'dataset_split_name', 'validation', 'The name of the train/test split.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'dataset_dir', '...', \r\n        'The directory where the dataset files are stored.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'labels_offset', 0,\r\n        'An offset for the labels in the dataset. This flag is primarily used to '\r\n        'evaluate the VGG and ResNet architectures which do not use a background '\r\n        'class for the ImageNet dataset.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'model_name', 'densenet161', 'The name of the architecture to evaluate.')\r\n    \r\n    tf.app.flags.DEFINE_string(\r\n        'preprocessing_name', None, 'The name of the preprocessing to use. If left '\r\n        'as `None`, then the model_name flag is used.')\r\n    \r\n    tf.app.flags.DEFINE_float(\r\n        'moving_average_decay', None,\r\n        'The decay to use for the moving average.'\r\n        'If left as None, then moving averages are not used.')\r\n    \r\n    tf.app.flags.DEFINE_integer(\r\n        'eval_image_size', None, 'Eval image size')\r\n    \r\n    FLAGS = tf.app.flags.FLAGS\r\n    \r\n    # Initialize all global and local variables\r\n    init = tf.group(tf.global_variables_initializer(),\r\n                       tf.local_variables_initializer())\r\n    \r\n    def main(_):\r\n      if not FLAGS.dataset_dir:\r\n        raise ValueError('You must supply the dataset directory with --dataset_dir')\r\n    \r\n      tf.logging.set_verbosity(tf.logging.INFO)\r\n      \r\n      sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\r\n                                              log_device_placement=True))\r\n      \r\n      with tf.Graph().as_default(), tf.device('/gpu:0'):\r\n          \r\n        sess.run(init)\r\n        tf_global_step = slim.get_or_create_global_step()\r\n    \r\n        ######################\r\n        # Select the dataset #\r\n        ######################\r\n        dataset = dataset_factory.get_dataset(\r\n            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\r\n    \r\n        ####################\r\n        # Select the model #\r\n        ####################\r\n        network_fn = nets_factory.get_network_fn(\r\n            FLAGS.model_name,\r\n            num_classes=(dataset.num_classes - FLAGS.labels_offset),\r\n            is_training=False)\r\n    \r\n        ##############################################################\r\n        # Create a dataset provider that loads data from the dataset #\r\n        ##############################################################\r\n        provider = slim.dataset_data_provider.DatasetDataProvider(\r\n            dataset,\r\n            shuffle=False,\r\n            common_queue_capacity=2 * FLAGS.batch_size,\r\n            common_queue_min=FLAGS.batch_size)\r\n        [image, label] = provider.get(['image', 'label'])\r\n        label -= FLAGS.labels_offset\r\n    \r\n        #####################################\r\n        # Select the preprocessing function #\r\n        #####################################\r\n        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\r\n        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\r\n            preprocessing_name,\r\n            is_training=False)\r\n    \r\n        eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\r\n    \r\n        image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\r\n    \r\n        images, labels = tf.train.batch(\r\n            [image, label],\r\n            batch_size=FLAGS.batch_size,\r\n            num_threads=FLAGS.num_preprocessing_threads,\r\n            capacity=5 * FLAGS.batch_size)\r\n    \r\n        ####################\r\n        # Define the model #\r\n        ####################\r\n        logits, _ = network_fn(images)\r\n    \r\n        if FLAGS.moving_average_decay:\r\n          variable_averages = tf.train.ExponentialMovingAverage(\r\n              FLAGS.moving_average_decay, tf_global_step)\r\n          variables_to_restore = variable_averages.variables_to_restore(\r\n              slim.get_model_variables())\r\n          variables_to_restore[tf_global_step.op.name] = tf_global_step\r\n        else:\r\n          variables_to_restore = slim.get_variables_to_restore()\r\n    \r\n        logits = tf.squeeze(logits)\r\n    \r\n        # Define the metrics:\r\n        predictions = logits\r\n\r\n        names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\r\n            'Accuracy': tf.metrics.root_mean_squared_error(predictions, labels),\r\n            'Recall_5': slim.metrics.streaming_recall(\r\n                logits, labels),\r\n        })\r\n    \r\n        # Print the summaries to screen.\r\n        print_ops = []\r\n        summary_ops = []\r\n        for name, value in names_to_values.items():\r\n          summary_name = 'eval/%s' % name\r\n          op = tf.summary.scalar(summary_name, value, collections=[])\r\n          op = tf.Print(op, [value], summary_name)\r\n          summary_ops.append(op)\r\n          print_ops.append(tf.Print(value, [value], summary_name))\r\n          tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\r\n          \r\n        # TODO(sguada) use num_epochs=1\r\n        if FLAGS.max_num_batches:\r\n          num_batches = FLAGS.max_num_batches\r\n        else:\r\n          # This ensures that we make a single pass over all of the data.\r\n          num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\r\n    \r\n        if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\r\n            if tf.train.latest_checkpoint(FLAGS.checkpoint_path):\r\n                checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\r\n            else:\r\n                checkpoint_path = FLAGS.checkpoint_path\r\n    \r\n        #print(checkpoint_path)\r\n        eval_interval_secs = 6\r\n        \r\n        tf.logging.info('Evaluating %s' % checkpoint_path)\r\n    \r\n        slim.evaluation.evaluation_loop(\r\n            master=FLAGS.master,\r\n            checkpoint_dir=checkpoint_path,\r\n            logdir=FLAGS.eval_dir,\r\n            num_evals=num_batches,\r\n            eval_op=list(names_to_updates.values()) + print_ops,\r\n            variables_to_restore=variables_to_restore,\r\n            eval_interval_secs = eval_interval_secs )\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n      tf.app.run()\r\n\r\nWhen I run this code, I face this error:\r\n\r\n        Traceback (most recent call last):\r\n      File \"/home/zgholami/test1/GZ_Project/GZ_DenseNet_TF-slim/eval_image_classifier.py\", line 210, in <module>\r\n        tf.app.run()\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n      File \"/home/zgholami/test1/GZ_Project/GZ_DenseNet_TF-slim/eval_image_classifier.py\", line 206, in main\r\n        eval_interval_secs = 60 )\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py\", line 296, in evaluation_loo\r\n    p\r\n        timeout=timeout)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py\", line 447, in evalua\r\n    te_repeatedly\r\n        session_creator=session_creator, hooks=hooks) as session:\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 668, in __init__\r\n        stop_grace_period_secs=stop_grace_period_secs)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 490, in __init__\r\n        self._sess = _RecoverableSession(self._coordinated_creator)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 842, in __init__\r\n        _WrappedSession.__init__(self, self._create_session())\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 847, in _create_session\r\n        return self._sess_creator.create_session()\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 551, in create_session\r\n        self.tf_sess = self._session_creator.create_session()\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 425, in create_session\r\n        init_fn=self._scaffold.init_fn)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 273, in prepare_session\r\n        config=config)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 189, in _restore_checkpoin\r\n    t\r\n        saver.restore(sess, checkpoint_filename_with_path)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1560, in restore\r\n        {self.saver_def.filename_tensor_name: save_path})\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n        run_metadata_ptr)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n        feed_dict_tensor, options, run_metadata)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n        options, run_metadata)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'eval_step': Could not satisfy explicit device s\r\n    pecification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n    Colocation Debug Info:\r\n    Colocation group had the following types and devices: \r\n    Const: GPU CPU \r\n    AssignAdd: CPU \r\n    VariableV2: CPU \r\n    Identity: GPU CPU \r\n    Assign: CPU \r\n    IsVariableInitialized: CPU \r\n    \t [[Node: eval_step = VariableV2[_class=[\"loc:@eval_step\"], container=\"\", dtype=DT_INT64, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]\r\n    ()]]\r\n    \r\n    Caused by op u'eval_step', defined at:\r\n      File \"/home/zgholami/test1/GZ_Project/GZ_DenseNet_TF-slim/eval_image_classifier.py\", line 210, in <module>\r\n        tf.app.run()\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n        _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n      File \"/home/zgholami/test1/GZ_Project/GZ_DenseNet_TF-slim/eval_image_classifier.py\", line 206, in main\r\n        eval_interval_secs = 60 )\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py\", line 296, in evaluation_loo\r\n    p\r\n        timeout=timeout)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/contrib/training/python/training/evaluation.py\", line 410, in evalua\r\n    te_repeatedly\r\n        eval_step = get_or_create_eval_step()\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/training/evaluation.py\", line 57, in _get_or_create_eval_step\r\n        collections=[ops.GraphKeys.LOCAL_VARIABLES, ops.GraphKeys.EVAL_STEP])\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\r\n        use_resource=use_resource, custom_getter=custom_getter)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\r\n        use_resource=use_resource, custom_getter=custom_getter)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 367, in get_variable\r\n        validate_shape=validate_shape, use_resource=use_resource)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\r\n        use_resource=use_resource)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\r\n        validate_shape=validate_shape)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\r\n        expected_shape=expected_shape)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 283, in _init_from_args\r\n        name=name)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.py\", line 131, in variable_op_v2\r\n        shared_name=shared_name)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 682, in _variable_v2\r\n        name=name)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n        op_def=op_def)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n        original_op=self._default_original_op, op_def=op_def)\r\n      File \"/group/pawsey0245/zgholami/pyml/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n    \r\n    InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'eval_step': Could not satisfy explicit device specification '\r\n    /device:GPU:0' because no supported kernel for GPU devices is available.\r\n    Colocation Debug Info:\r\n    Colocation group had the following types and devices: \r\n    Const: GPU CPU \r\n    AssignAdd: CPU \r\n    VariableV2: CPU \r\n    Identity: GPU CPU \r\n    Assign: CPU \r\n    IsVariableInitialized: CPU \r\n    \t [[Node: eval_step = VariableV2[_class=[\"loc:@eval_step\"], container=\"\", dtype=DT_INT64, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]\r\n    ()]]\r\n    \r\n    ERROR:tensorflow:==================================\r\n    Object was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\r\n    <tf.Tensor 'report_uninitialized_variables_1/boolean_mask/Gather:0' shape=(?,) dtype=string>\r\n    If you want to mark it as used call its \"mark_used()\" method.\r\n\r\n  [1]: https://github.com/pudae/tensorflow-densenet/blob/master/eval_image_classifier.py\r\n  [2]: https://github.com/pudae/tensorflow-densenet/blob/master/train_image_classifier.py\r\n", "comments": ["You don't have to follow examples in train_image_classifier. You can directly assign GPU to computing logits from the network. For example:\r\nwith tf.device('/gpu:0'):\r\n   logits, _ = network_fn(images)\r\n\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Just type `os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"` at the beginning of main function. 0 is the id of your gpu and you can view it by typing `nvidia-smi` at the cmd line. Cheers.", "Due to comments of @chrisrn, I resolved the same problem of @Ellie68."]}, {"number": 14084, "title": "Tensorflow hooks do not properly write events to HDFS", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Alpine Linux 3.6.2 (running with Docker)\r\n- **TensorFlow installed from (source or binary)**: Binary (conda-forge)\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: - \r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: using CPU\r\n- **Exact command to reproduce**: `CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath --glob) jupyter notebook`\r\n\r\n### Describe the problem\r\nI am reading files from hdfs and also want to use hdfs as `model_dir` to store the tensorflow output. Reading and writing of the checkpoints works fine. However, the the events file just gets created, but it does not get updated with the evaluation events. \r\n\r\nWhen I kill the notebook then the output is written, but not in between. Changing the model directory to a local one solves this. Also adding an extra `SummarySaverHook` which points to a local directory did not help.\r\n\r\nI am running my code using the instructions here: <https://www.tensorflow.org/deploy/hadoop>\r\n\r\n### Source code / logs\r\nI normally use an Estimator with a custom model function, but I reduced it to the following:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy\r\n\r\nhdfs_namenode = 'hdfs://xx.xx.xx.xx:8020'\r\nfiles = ['{}/data/part-00000-2a4cdbb2-f152-4503-9da8-cba63c7648fc-c000.csv'.format(hdfs_namenode)]\r\n\r\noutput_directory = '{}/tresults/'.format(hdfs_namenode)\r\n\r\nfeature_columns = [tf.feature_column.numeric_column(\"x\", shape=[70])]\r\n\r\ndef input_function_training:\r\n  ...\r\ndef input_function_evaluation:\r\n  ...\r\n\r\ndef experiment_fn(run_config, hparams):\r\n    estimator = tf.estimator.DNNClassifier(hidden_units=[4], \r\n        feature_columns=feature_columns,\r\n        model_dir=output_directory, \r\n        n_classes=2,\r\n        config=run_config)\r\n    \r\n    return tf.contrib.learn.Experiment(estimator=estimator, \r\n        train_input_fn=input_function_training, \r\n        eval_input_fn=input_function_evaluation)\r\n\r\ntf.contrib.learn.learn_runner.run(experiment_fn,\r\n    run_config=tf.contrib.learn.RunConfig(model_dir=output_directory,\r\n        save_checkpoints_steps=10,\r\n        save_checkpoints_secs=None,\r\n        save_summary_steps=1))\r\n```\r\n\r\nThe logs do not show any problems:\r\n\r\n```\r\nWARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\r\nINFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb30e032780>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1.0\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 10, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'hdfs://xx.xx.xx.xx:8020/tresults/'}\r\nWARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\r\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\r\nInstructions for updating:\r\nMonitors are deprecated. Please use tf.train.SessionRunHook.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Saving checkpoints for 1 into hdfs://xx.xx.xx.xx:8020/tresults/model.ckpt.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nINFO:tensorflow:Starting evaluation at 2017-10-30-05:41:22\r\nINFO:tensorflow:Restoring parameters fromhdfs://xx.xx.xx.xx:8020/tresults/model.ckpt-1\r\nINFO:tensorflow:Evaluation [1/100]\r\n...\r\nINFO:tensorflow:Evaluation [100/100]\r\nINFO:tensorflow:Finished evaluation at 2017-10-30-05:42:24\r\nINFO:tensorflow:Saving dict for global step 1: accuracy = 0.46836, accuracy_baseline = 0.75205, auc = 0.697368, auc_precision_recall = 0.591545, average_loss = 0.692944, global_step = 1, label/mean = 0.24795, loss = 692.944, prediction/mean = 0.524391\r\nINFO:tensorflow:Validation (step 1): accuracy = 0.46836, accuracy_baseline = 0.75205, auc = 0.697368, auc_precision_recall = 0.591545, average_loss = 0.692944, label/mean = 0.24795, loss = 692.944, prediction/mean = 0.524391, global_step = 1\r\nINFO:tensorflow:loss = 42.1861, step = 1\r\nINFO:tensorflow:Saving checkpoints for 11 into hdfs://xx.xx.xx.xx:8020/tresults/model.ckpt.\r\n```", "comments": ["@jhseu Would you be able to help out with the HDFS issue?", "I kind of fixed the error. My guess is that Hadoop only shows the initial file size as long as content is only appended to the event file and shows the \"real\" file size only when the training is stopped (ended or aborted). My problem further might have been related to the fact, that I have got Tensorflow running in a Docker container which was stopped immediately after Tensorflow was finished. I added some sleep at the end of my Python code, so that potential background operations have time to finish.", "Thank you for sharing that information for people who google this issue in the future. Sleep sounds like it would do the trick. If you end up discovering a way to do it without sleep in the future, please let us know. We're always interested in hearing about these things.", "After reading this, how can I write tf events to hdfs?", "@kelly-tlz Just do it as described in https://www.tensorflow.org/deploy/hadoop. After the last evaluation is finished, wait a bit (should not be longer than 30min), then you should see the results on hdfs.", "@wochinge can you share code of writing events to hdfs? Since I haven't use this, and I want to save my checkpoint file and events logs into hdfs directly. Now my code is like this:\r\n`hdfs_path=\"hdfs://*\"\r\nlocal_path = \"./\"\r\nwith tf.Session(graph=tf.get_default_graph()) as sess:\r\n    W = tf.Variable([[1,2,3],[3,4,5]], dtype=tf.float32, name='weights')\r\n    b = tf.Variable([[1,2,3]], dtype=tf.float32, name='biases')\r\n    inti_op = tf.group(tf.global_variables_initializer(),\r\n                       tf.local_variables_initializer())  # local including FLAGS data\r\n    saver = tf.train.Saver()\r\n    sess.run(inti_op)\r\n    summary_writer = tf.summary.FileWriter(local_path, graph_def=sess.graph_def)\r\n    saver.save(sess, save_path=local_path+\"save_net.ckpt\")\r\n`", "I am using `tf.estimator` (model_dir points to hadoop), but I think you simply need to set the directory of your `FileWriter` to a hdfs path, e.g.\r\n\r\n```python\r\ntf.summary.FileWriter('hdfs://url_of_your_namenode:8020/any_directory', graph_def=sess.graph_def)\r\n```\r\n\r\nJust be sure to follow all the stops from the documentation. Some hadoop namenodes also may be reachable through port 9000.", "@wochinge  this does not help when my version is 1.0, when I change the local_path to hdfs_path, it raise problem that \"parent directory is not exists\", the question I ask in https://stackoverflow.com/questions/47456415/file-test-hdfs-py-save-path-saver-savesess-hdfs-pathsave-net-ckpt-pa", "Did you properly format your hdfs url?\r\nSince `hdfs_path+\"save_net.ckpt` would give `\"hdfs://*save_net.ckpt\" `, which is wrong? I cannot comment on Stackoverflow since I have to less reputation there.", "@wochinge Maybe your version(mine is tf1.0) or environment of hadoop is different from me, I'm sure the hdfs path is exist and I can use subpreprocess.Popen to write and read hdfs file through python code.", "We have this problem as well. The problem is most acute when using the Estimator APIs, where we don't have an explicit handle on the hdfs writer object. The problem is that hdfs only updates the file size written either at the end of block being fully written or when an explicit hflush is called on the file. When the file sizes are much smaller than the block size and they aren't closed, they just appear to hang - not available for reading. ", "Hey,\r\nso for me there a to options, which work:\r\n- Just quit your TensorFlow services and wait for 1 to 2 hours. The files should then appear on HDFS\r\n- you can use `hdfs debug recoverLease` on the files, which are not fully written yet (<https://hadoop.apache.org/docs/r2.7.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#recoverLease>)", "> We have this problem as well. The problem is most acute when using the Estimator APIs, where we don't have an explicit handle on the hdfs writer object. The problem is that hdfs only updates the file size written either at the end of block being fully written or when an explicit hflush is called on the file. When the file sizes are much smaller than the block size and they aren't closed, they just appear to hang - not available for reading.\r\n\r\nIs there any way to read the event file when training, because I want to run tensorboard while training"]}]