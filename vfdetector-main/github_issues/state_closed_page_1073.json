[{"number": 21085, "title": "Cannot compile toco on windows", "body": "### Error Message\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build saying Cannot open include file: 'sys/time.h'\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: none\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**:  3.5.1-32 bit\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: nothing since i have AMD radeon\r\n- **GPU model and memory**:  Intel(R) HD Graphics 620\r\n- **Exact command to reproduce**:\r\n\r\n`bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\ --input_file=maonani/tflite_graph.pb \\ --output_file=maonani/detect.tflite \\ --input_shapes=1,300,300,3 \\ --input_arrays=normalized_input_image_tensor \\ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\ --inference_type=QUANTIZED_UINT8 \\ --mean_values=128 \\ --std_values=128 \\ --change_concat_input_ranges=false \\ --allow_custom_ops`\r\n\r\n### Describe the problem\r\ni have been trying for weeks to convert custom object detection api models to tflite and i have found this article [https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193](url) but i just skip to the part where it converts the resulting frozen graph (tflite_graph.pb) to the TensorFlow Lite flatbuffer format (detect.tflite) and when i run the command that i specified above i get an error saying\r\n\r\n```\r\nERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 2)\r\ntensorflow/contrib/lite/profiling/time.cc(17): fatal error C1083: Cannot open include file: 'sys/time.h': No such file or directory\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\n```\r\n\r\n### Source code / logs\r\n\r\nC:\\Users\\LENOVO-PC\\tensorflow> bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\ --input_file=maonani/tflite_graph.pb \\ --output_file=maonani/detect.tflite \\ --input_shapes=1,300,300,3 \\ --input_arrays=normalized_input_image_tensor \\ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\ --inference_type=QUANTIZED_UINT8 \\ --mean_values=128 \\ --std_values=128 \\ --change_concat_input_ranges=false \\ --allow_custom_ops\r\nINFO: Build options have changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (60 packages loaded).\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc [for host]:\r\nexternal/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/map_field.cc [for host]:\r\nexternal/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nINFO: From Compiling tensorflow/core/platform/default/logging.cc [for host]:\r\nc:\\users\\lenovo-pc\\_bazel_lenovo-pc\\buizwhxs\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\default\\logging.cc(147) : warning C4722: 'tensorflow::internal::LogMessageFatal::~LogMessageFatal': destructor never returns, potential memory leak\r\nINFO: From Compiling tensorflow/core/platform/default/logging.cc:\r\nc:\\users\\lenovo-pc\\_bazel_lenovo-pc\\buizwhxs\\execroot\\org_tensorflow\\tensorflow\\core\\platform\\default\\logging.cc(147) : warning C4722: 'tensorflow::internal::LogMessageFatal::~LogMessageFatal': destructor never returns, potential memory leak\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/generated_message_reflection.cc:\r\nexternal/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/generated_message_reflection.cc(2429): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nINFO: From Compiling external/protobuf_archive/src/google/protobuf/map_field.cc:\r\nexternal/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Message *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::NewFromPrototype(const GenericType *,google::protobuf::Arena *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'google::protobuf::Arena *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetArena(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nexternal/protobuf_archive/src/google/protobuf/map_field.cc(465): warning C4506: no definition for inline function 'void *google::protobuf::internal::GenericTypeHandler<google::protobuf::Message>::GetMaybeArenaPointer(GenericType *)'\r\n        with\r\n        [\r\n            GenericType=google::protobuf::Message\r\n        ]\r\nINFO: From Compiling external/flatbuffers/src/util.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/reflection.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/flatc.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_parser.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_fbs.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/code_generators.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/grpc/src/compiler/go_generator.cc [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_python.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_php.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_js.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_cpp.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/grpc/src/compiler/cpp_generator.cc [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_go.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_json_schema.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/flatc_main.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_text.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/grpc/src/compiler/java_generator.cc [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nINFO: From Compiling external/flatbuffers/src/idl_gen_grpc.cpp [for host]:\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nERROR: C:/users/lenovo-pc/tensorflow/tensorflow/contrib/lite/profiling/BUILD:37:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:time' failed (Exit 2)\r\ntensorflow/contrib/lite/profiling/time.cc(17): fatal error C1083: Cannot open include file: 'sys/time.h': No such file or directory\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 287.186s, Critical Path: 15.02s\r\nINFO: 295 processes: 295 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "thank you for reminding. it's been updated", "Thanks for the issue. We are in the progress of fixing compilation on Windows. TensorFlow is now building  Bazel on Windows which is making this much easier. Please stay tuned.", "The toco conversion utility should now run on Windows. It hasn't been put through exhaustive testing, though, due to some limitations with our testing infrastructure, so feel free to file bugs for any issues you encounter."]}, {"number": 21084, "title": "Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_12 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Reshape_6, stack_12)", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.5\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:5.4\r\n- **CUDA/cuDNN version**: cuda9.0 cudnn 7 nvidia- 384driver \r\n- **GPU model and memory**: 12G\r\n- **Exact command to reproduce**:\r\npython object_detection/model_main.py     --pipeline_config_path=${PIPELINE_CONFIG_PATH}     --model_dir=${MODEL_DIR}     --num_train_steps=${NUM_TRAIN_STEPS}     --num_eval_steps=${NUM_EVAL_STEPS}     --alsologtostderr\r\n TITANX  python object_detection/model_main.py \\\r\n>     --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n>     --model_dir=${MODEL_DIR} \\\r\n>     --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n>     --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n>     --alsologtostderr\r\nWARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7eff51d9a6e0>) includes params argument, but params are not passed to Estimator.\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From /mnt/object_detection/core/box_predictor.py:407: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nWARNING:tensorflow:From /mnt/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2037: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.get_or_create_global_step\r\nWARNING:tensorflow:From /mnt/object_detection/core/losses.py:317: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee tf.nn.softmax_cross_entropy_with_logits_v2.\r\n\r\nWARNING:tensorflow:From /mnt/object_detection/meta_architectures/faster_rcnn_meta_arch.py:1930: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n2018-07-24 16:38:16.076136: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-07-24 16:38:16.529363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-24 16:38:16.532926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:00:06.0\r\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\r\n2018-07-24 16:38:16.532987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:00:06.0, compute capability: 6.1)\r\n2018-07-24 16:38:34.429255: E tensorflow/core/common_runtime/executor.cc:651] Executor failed to create kernel. Not found: No registered 'Pad' OpKernel for CPU devices compatible with node Pad_12 = Pad[T=DT_STRING, Tpaddings=DT_INT32](Reshape_6, stack_12)\r\n         (OpKernel was found, but attributes didn't match)\r\n        .  Registered:  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; Tpaddings in [DT_INT32]\r\n  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]; Tpaddings in [DT_INT32]\r\n  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT64]\r\n  device='GPU'; T in [DT_FLOAT]; Tpaddings in [DT_INT32]\r\n  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT64]\r\n  device='GPU'; T in [DT_HALF]; Tpaddings in [DT_INT32]\r\n  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT64]\r\n  device='CPU'; T in [DT_BOOL]; Tpaddings in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tpaddings in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tpaddings in [DT_INT64]\r\n", "comments": ["got the same error.", "Hi\uff0chave you solved the problem yet? I have the same problem", "It seems that tf.pad doesn't support string tensor. Could you take a test on the latest version of tensorflow?", "I had the same issue. However, after I updated tensorflow from 1.5.0 to 1.9.0. It runs smoothly.", "@xinyuegtxy does this issue resolve after up-grading to the latest tensorflow?\r\n", "To me, upgrading tensorflow from 1.5.0 to 1.9.0 resolves the issue.", "It has been 30 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Tf1.9 resolve it", "tensorflow 1.11 or 1.9 is ok."]}, {"number": 21083, "title": "update from tensorflow_master", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Closing no-change PR"]}, {"number": 21082, "title": "Refactored lite makefile to centralize platform-specific settings", "body": "I'm working on making it easier to support platforms like the Blue Pill when compiling TF Lite, and as a step towards that I've moved the compiler options for each platform into sub-files. I've tested the Linux, Raspberry Pi, and iOS options, which work, and tried to build the STM32 options but I don't think I have the correct toolchains. I'm hoping this is a fairly mechanical change as a start though, and will make it easier to add new platforms going forward.", "comments": ["Closing since I'm moving this to an internal change instead."]}, {"number": 21081, "title": "Tensort RT Engine Object Detection", "body": "tensorflow.python.framework.errors_impl.ResourceExhaustedError: Requested batch size is not available and engine cache is full\r\n\t [[Node: import/my_trt_op_16 = TRTEngineOp[InT=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,1], [?,1], [?,1], [?,1]], max_cached_engines_count=1, output_shapes=[[?,4]], precision_mode=\"FP32\", segment_funcdef_name=\"my_trt_op_16_native_segment\", serialized_segment=\"\\2007\\000\\...00\\000\\000\", static_engine=true, workspace_size_bytes=65075264, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](import/MultiClassNonMaxSuppression/ClipToWindow/split, import/MultiClassNonMaxSuppression/ClipToWindow/split:2, import/MultiClassNonMaxSuppression/ClipToWindow/split:1, import/MultiClassNonMaxSuppression/ClipToWindow/split:3)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: import/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField/strided_slice/_89 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1900_...ided_slice\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@samikama not clear whether this is a bug or working-as-intended. Would you please take a look?", "It looks like @snownus created the engine with a batch size smaller than the execution time. Either a bigger batch size should be used or dynamic op should be generated.", "@samikama , in faster rcnn, the second stage will have one more dimension. It seems tensorflow/tensorrt see another dimension as batch size and in fact, it isn't. Faster RCNN, at the second stage have 300 boxes.\r\n", "I have same error. If no use tensorflow.contrib.tensorrt.create_inference_graph then no error.\r\n\r\nResourceExhaustedError (see above for traceback): Requested batch size is not available and engine cache is full\r\n         [[Node: model_1/prior_layer_1/my_trt_op_11 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[10], calibration_data=\"\", fixed_input_size=true, input_shapes=[[?,2]], max_cached_engines_count=1, output_shapes=[[?,2]], precision_mode=\"FP32\", segment_funcdef_name=\"model_1/prior_layer_1/my_trt_op_11_native_segment\", serialized_segment=\"\\270\\020\\0...00\\000\\000\", static_engine=true, workspace_size_bytes=18292682, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_1/prior_layer_1/ToFloat)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[Node: while/Switch_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2429_while/Switch_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopwhile/strided_slice_5/stack/_1)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nSystem:\r\nubuntu 16.04\r\ntensorflow v1.10.1 from source\r\ncuda 9.0, cudnn 7.1\r\ntensorrt 4.1.2\r\npython 3.5", "use create_inference_graph with maximum_cached_engines=3 no error", "@snownus,\r\n\r\nIn your case it is a known issue of integration. Since TF doesn't have batch dimension concept and TRT requires a batch dimension, our assumption of Rank0 being the batch dimension is failing in FasterRCNN. It is being worked on. ", "There will be an NMS op implementation soon which  should resolve this.", "@trevor-m @pooyadavoodi  Can you PTAL", "It has been 32 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Our (somewhat) recent improvements to the engine cache fixes this issue. It is available in the master branch of tensorflow (tf-nightly-gpu).", "I think it was resolved. I am closing the issue. If you think I made a mistake, please open another issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=21081\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=21081\">No</a>\n"]}, {"number": 21080, "title": "fix typo", "body": "fix typo\r\n", "comments": ["@caisq unrelated failure"]}, {"number": 21079, "title": "typo", "body": "fix typo\r\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "update CLA", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21078, "title": "Fix:  Validate variable dtype before restoring checkpoint", "body": "Fix #20487 ", "comments": ["Thank you for your review. Could you take a look again?", "Thanks. I think all comments have been resolved.", "The failures seems unrelated, right?", "@drpngx Hi, I have resolved the merge conflict with upstream/master. Could you take a look again?", "Unrelated failure, presumably (OOM in `scatter_nd_test`. Running again to check.", "Build failure\r\n\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/gif_archive/BUILD.bazel:8:1: Couldn't build file external/gif_archive/_objs/gif/external/gif_archive/lib/dgif_lib.o: C++ compilation of rule '@gif_archive//:gif' failed: Unexpected IO error.: Running '/Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/execroot/org_tensorflow/_bin/xcode-locator 8.3.3' failed with code 1.\r\n40\r\nThis most likely indicates that xcode version 8.3.3 is not available on the host machine.\r\n41\r\nProcess exited with status 1\r\n42\r\nstdout: error: Unable to extract CFBundleShortVersionString from URL: file:///Volumes/BuildData/tmpfs/bazel_output/_bazel_kbuilder/d1b2600cd78e76a92812a06683f5de10/external/aws/aws-cpp-sdk-sagemaker/include/aws/sagemaker\r\n```"]}, {"number": 21077, "title": "Nasnet-mobile tfhub module issue.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Stretch 9(frozen model user), Windows 10 (trainer)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Raspberry Pi 3 Model B\r\n- **TensorFlow installed from (source or binary)**:Uhhh not sure.  I installed from the binary in June.\r\n- **TensorFlow version (use command below)**: b'v1.9.0-0-g25c197e023' 1.9.0\r\n- **Python version**:3.6.6 on Windows PC, 3.5.3 on Raspberry Pi\r\n- **CUDA/cuDNN version**:9.0, 7.1.4\r\n- **GPU model and memory**: GTX1060 6GB\r\n- **Exact command to reproduce**:\r\npython retrain.py --image_dir=\"C:\\fake\\data\\directory\" --learning_rate 0.0005 --how_many_training_steps=200 --tfhub_module=\"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/1\" --random_brightness 25 --flip_left_right --print_misclassified_test_images\r\n\r\nI wrote a lot of custom code and used alot of custom data (Cue Debugger's facepalm) both of which I am not sure I can release because both are technically company property.  And this is my first Github submission, so I don't know what I need to mention or do.  Disclaimer/Excusing over.\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen I produce the frozen model after training off of nasnet_mobile, I get a really large variable/parameter count of around 1100.  That's larger than a regular non-mobile friendly 800 variable count.  However, the model size was still a magnitude less than resnet-152.  It was about the same size as a typical tf-lite mobilenet v2 sized model though a little larger.  I decided to try the model out, by transferring it on a chimeric version of label_image.py.  Then, it gave the printouts nearly identical to any model.  After an exceptionally long time, it just prints out \r\n\r\n'Bus error'\r\n\r\nThe large variable count leads me to suspect that it's not being prepared for mobile properly and nothing to do with my modified code.  \r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI used a modified version of label_image.py and pretty much an exact copy of retrain.py in the image retraining tutorial.  I only changed the squeeze_dims() to axis() in the args for retrain.py.  Just to be clear.  I am running retrain.py on a Windows 10 PC with the aforementioned specs, then running label_image.py on a Raspberry Pi 3 Model B after flashdriving over the model and code.INFO:tensorflow:Initialize variable module/reduction_cell_1/beginning_bn/moving_mean:0 from checkpoint \r\n'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/beginning_bn/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/beginning_bn/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/beginning_bn/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_1/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/bn_sep_5x5_2/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_1/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_1/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_1/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_1/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_2/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_2/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/left/separable_5x5_2/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/left/separable_5x5_2/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_1/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/bn_sep_7x7_2/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_1/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_1/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_1/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_1/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_2/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_2/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_0/right/separable_7x7_2/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_0/right/separable_7x7_2/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_1/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/bn_sep_7x7_2/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_1/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_1/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_1/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_1/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_2/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_2/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_1/right/separable_7x7_2/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_1/right/separable_7x7_2/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_1/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/bn_sep_5x5_2/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_1/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_1/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_1/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_1/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_2/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_2/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_2/right/separable_5x5_2/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_2/right/separable_5x5_2/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_1/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/bn_sep_3x3_2/moving_variance\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_1/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_1/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_1/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_1/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_2/depthwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_2/depthwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/comb_iter_4/left/separable_3x3_2/pointwise_weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/comb_iter_4/left/separable_3x3_2/pointwise_weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/prev_1x1/weights:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/prev_1x1/weights\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/beta:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/prev_bn/beta\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/gamma:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/prev_bn/gamma\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/moving_mean:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/prev_bn/moving_mean\r\nINFO:tensorflow:Initialize variable module/reduction_cell_1/prev_bn/moving_variance:0 from checkpoint b'C:\\\\Users\\\\wontae\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\54b246e9ba27d2ebb70a5d0b8d9ff99ca6f7baa4\\\\variables\\\\variables' with reduction_cell_1/prev_bn/moving_variance\r\n2018-07-24 11:34:25.376617: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-07-24 11:34:25.379309: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-24 11:34:25.382994: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0\r\n2018-07-24 11:34:25.386328: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N\r\n2018-07-24 11:34:25.389811: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4734 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:09:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint\r\nINFO:tensorflow:Froze 1128 variables.\r\nINFO:tensorflow:Converted 1128 variables to const ops.\r\n", "comments": ["This was a terrible way to post what I thought was a bug.  I think I just lacked data, and the model when run on my Windows PC proved useless anyways.  I'd just advise whoever runs into a bus error running nasnet_mobile to try some other better suited model like mobilenet in tfhub.  Keep plugging more data and toy with the hyperparams.  "]}, {"number": 21076, "title": "ERROR: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)", "body": "Hi, I am trying to build TF from source. But I met an error which is listed below. Does anyone know how to fix it? Thanks a lot!\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: \r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.2 / 7.1\r\n- **GPU model and memory**: GTX 1080Ti\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\nINFO: From Compiling tensorflow/core/kernels/padding_fifo_queue.cc:\r\nIn file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:24,\r\n                 from ./tensorflow/core/framework/queue_interface.h:23,\r\n                 from ./tensorflow/core/kernels/queue_op.h:22,\r\n                 from ./tensorflow/core/kernels/fifo_queue.h:26,\r\n                 from ./tensorflow/core/kernels/padding_fifo_queue.h:27,\r\n                 from tensorflow/core/kernels/padding_fifo_queue.cc:26:\r\n./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':\r\n./tensorflow/core/util/tensor_format.h:500:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {\r\n                                             ^\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/refcount.h:22,\r\n                 from ./tensorflow/core/platform/tensor_coding.h:21,\r\n                 from ./tensorflow/core/framework/resource_handle.h:19,\r\n                 from ./tensorflow/core/framework/register_types.h:21,\r\n                 from tensorflow/core/kernels/padding_fifo_queue.cc:22:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nERROR: /home/xxx/tensorflow-git/tensorflow/BUILD:581:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/.cache/bazel/_bazel_root/10dd4b2d41234e5022064db22e215668/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/xxx/.cache/bazel/_bazel_root/10dd4b2d41234e5022064db22e215668/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/home/xxx/.cache/bazel/_bazel_root/10dd4b2d41234e5022064db22e215668/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\r\n    __import__('pkg_resources').declare_namespace(__name__)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2927, in <module>\r\n    @_call_aside\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2913, in _call_aside\r\n    f(*args, **kwargs)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in _initialize_master_working_set\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 956, in subscribe\r\n    callback(dist)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2952, in <lambda>\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2515, in activate\r\n    declare_namespace(pkg)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2097, in declare_namespace\r\n    _handle_ns(packageName, path_item)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2047, in _handle_ns\r\n    _rebuild_mod_path(path, packageName, module)\r\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2066, in _rebuild_mod_path\r\n    orig_path.sort(key=position_in_sys_path)\r\nAttributeError: '_NamespacePath' object has no attribute 'sort'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 112.766s, Critical Path: 89.56s\r\nINFO: 323 processes: 323 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Hi Paul @poxvoculi Can you help to fix this issue? This issue is still there after I reclone the newest version of source codes. Thanks!", "I get the same issue, please help me ", "> Hi Paul @poxvoculi Can you help to fix this issue? This issue is still there after I reclone the newest version of source codes. Thanks!\r\n\r\nsame here", "how did you guys solve this problem?"]}, {"number": 21075, "title": "[tftrt]", "body": "  trt 4 update input check on tensor dimensions.\r\n  relaxing tensor dimension for trt 4 to support non-4-dimensional inputs\r\n  added unit test (currently disabled due to failed INT8 conversion)", "comments": ["Friendly ping @aaroey, in case you missed the commits", "Thanks @jjsjann123. I'll wait for #21138 before merging this one.", "Hi @jjsjann123, would help to merge the conflicts? I'll test and merge after that.", "As chatted offline, I'll try to extend this PR to fix all commented tests, by either minimizing the number of disabled test cases, or just fixing them.", "sync @pooyadavoodi since he's also looking at fixing unit tests"]}, {"number": 21074, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": "I have tensorflow-gpu 1.9 installed for python3.5 system environment. I have cuda 9.0 installed. I do not have any other versions of cuda installed. I verified that libcublas.so.9.0 is located in /usr/local/cuda-9.0/lib64. I added the following to my ~/.bashrc file, as per the NVIDIA docs:\r\n```\r\nexport PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\\\r\n                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n```\r\nI still get this error. Please help. Thank you.\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n", "comments": ["You can set cuda library using below command for this issue\r\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-8.0/lib64\"\r\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-8.0/lib64\"\r\n", "@dhgokul that did not work. I am using cuda-9.0, so why would that command work? Why did you write the command twice?", "Fixed. I think it was because I was using PyCharm in the system environment. I had to set the variables in my system environment, not in ~/.bashrc: \r\n\r\nsudo gedit /etc/environment\r\n\r\nPATH=\"**/usr/local/cuda-9.0/bin:**/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\"\r\n\r\nLD_LIBRARY_PATH=\"/usr/local/cuda-9.0/lib64\""]}, {"number": 21073, "title": "Network.to_json should handle numpy.ndarray correctly", "body": "Pick the fix at https://github.com/keras-team/keras/pull/10754 to ```tf.keras```.", "comments": ["Can you review this please, @fchollet?", "I fixed the bad indentation, can anyone trigger the build again? Thanks.", "@yanboliang Done", "@caisq I fixed the bad indentation again, can you re-trigger it? Thanks. BTW, do you know how I can trigger python lint checker locally before I submit the PR?", "@yanboliang With the necessary dependencies installed, you should be able to run it with this script (from the root directory of the repo): https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh", "@caisq Great. I just run that script and verified this PR can pass sanity check locally. Can you trigger this test again? I'm sorry to bother you again. Thanks.", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Gently ping @fchollet @caisq . Would mind to have a look at this? Thanks.", "I think @fchollet has implicitly approved this PR when he applied the \"awaiting test then merge\" tag. I've applied the \"ready to pull\" tag, which should have triggered the merging process. let me try it again.", "@caisq Thanks!"]}, {"number": 21072, "title": "Workaround for RefCounted error when using custom ops.", "body": "Fixes #17316.\r\n\r\nE.g. as described in #17316 under some circumstances, when adding a custom op, after attempting to use it one gets a:\r\n```\r\n2018-07-23 20:59:22.060710: F external/tf_op_include/tensorflow/core/lib/core/refcount.h:79] Check failed: ref_.load() == 0 (1 vs. 0)\r\n/bin/bash: line 1: 31765 Aborted                 (core dumped) ( /tmp/run_bazel_1532379546 )\r\n```\r\n\r\nRelates to issues seen in other repos:\r\nhttps://github.com/cyberfire/tensorflow-mtcnn/issues/8\r\nhttps://github.com/cyberfire/tensorflow-mtcnn/issues/5\r\nhttps://github.com/tensorflow/tensorflow/issues/4640\r\n\r\nSolution conform suggestion of @josh11b in #17316.\r\n@mrry Derek, i've taken the liberty to assign you a TODO here.", "comments": ["Bump", "@mrry Derek can you take a look since this PR assigns TODO to you.", "Please do not submit this with a TODO in my name, because I am not planning to work on it.", "You do have the most context here though; per style guide:\r\n\r\nhttps://google.github.io/styleguide/cppguide.html#TODO_Comments\r\n> (...) or other identifier of the person or issue with the best context about the problem referenced by the TODO.\r\n\r\nand \r\n\r\n> (..) A TODO is not a commitment that the person referenced will fix the problem. ", "All that means is that I get more email, which is just as bad :). If the aim is to give more context, `# TODO(Issue #17316): ...` would give readers a direct link to where the problem is discussed.", "Nagging Assignee @caisq: It has been 92 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21071, "title": "[tf.keras] Bug - Fix Stateful Metrics in fit_generator with TensorBoard", "body": "**Description:**\r\n\r\nThis mirrors the changes in ``keras:master``: https://github.com/keras-team/keras/pull/10673", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Looks like this was fixed internally."]}, {"number": 21070, "title": "Documentation request: tf.contrib.data.AUTOTUNE", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: tf.contrib.data.AUTOTUNE\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nAccording to release notes for 1.8, there is a new parameter called tf.contrib.data.AUTOTUNE. It's functionality is described in release notes as follows:\r\n 'Add tf.contrib.data.AUTOTUNE, which allows the tf.data runtime to automatically tune the prefetch buffer sizes based on your system and environment.'\r\nBut I couldn't find any other documentation associated with it in [tf.dataset ](https://www.tensorflow.org/api_docs/python/tf/data)\r\n\r\nI used it in my pipeline as shown below and it didn't give any errors. But I have no idea how it works and which circumstances I should use it. Please provide some documentation on how to use this feature.\r\n\r\n### Source code / logs\r\n```\r\n#Make dataset for training\r\ndataset_train = tf.data.Dataset.from_tensor_slices((file_ids_training,file_names_training))\r\ndataset_train = dataset_train.flat_map(lambda file_id,file_name: tf.data.Dataset.from_tensor_slices(\r\n    tuple (tf.py_func(_get_data_for_dataset, [file_id,file_name], [tf.float32,tf.float32]))))\r\n\r\ndataset_train= dataset_train.shuffle(buffer_size=train_buffer_size)\r\ndataset_train= dataset_train.repeat()\r\ndataset_train= dataset_train.batch(train_batch_size) #Make dataset, shuffle, and create batches\r\ndataset_train = dataset_train.prefetch(tf.contrib.data.AUTOTUNE)\r\n\r\ndataset_train_iterator = dataset_train.make_one_shot_iterator()\r\nget_train_batch = dataset_train_iterator.get_next()\r\n```", "comments": ["that's my concern too. There is not any documentation on this AUTOTUNE. how does this work?", "> that's my concern too. There is not any documentation on this AUTOTUNE. how does this work?\r\n\r\ncheckout the interleave doc part https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset", "This symbol has been moved to `tf.data.experimental.AUTOTUNE`. Details about its implementation can be found in the [`tf.data` performance guide](https://www.tensorflow.org/guide/data_performance#pipelining).\r\n\r\nThanks for the suggestion! \ud83d\ude04 ", "@dynamicwebpaige : I didn't find any documentation about `AUTOTUNE`'s implementation in the guide you linked. It seems as though it's easy to set up a 3 or 4 dimensional parameter space if I use `AUTOTUNE` for several `num_parallel_calls` and `buffer_size` arguments.\r\n\r\nDocumenting how `AUTOTUNE` functions seems important since I'm not sure how long it takes TF to optimize these values or how to profile my pipeline while it's performing the optimization.", "agree on the need of having a better understanding of how the AUTOTUNE actually works. i have just tried to get a quick good grasp of what this actually does and couldn't do so. out of curiosity, at the end of the day this is open source "]}, {"number": 21069, "title": "writing tfrecord with multithreading is not fast as expected", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04 and MacOS High Sierra 10.13.4\r\nTensorFlow installed from (source or binary): docker and virtualenv\r\nTensorFlow version (use command below): v1.8.0-0-g93bc2e2072\r\nPython version: 2.7.12\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce: N/A\r\n\r\n### Describe the problem\r\nTried to write tfrecord w/ and w/o multithreading, and found the speed difference is not much (w/ 4 threads: 434 seconds; w/o multithread 590 seconds).  Not sure if I used it correctly. Is there any better way to write tfrecord faster?\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np \r\nimport threading \r\nimport time \r\n\r\n\r\ndef generate_data(shape=[15,28,60,1]):\r\n\treturn np.random.uniform(size=shape)\r\n\r\n\r\ndef _bytes_feature(value):\r\n\treturn tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\n\r\ndef _int64_feature(value):\r\n\treturn tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n\r\ndef write_instances_to_tfrecord(tfrecord_file, filenames):\r\n\ttfrecord_writer = tf.python_io.TFRecordWriter(tfrecord_file)\r\n\tfor i, filename in enumerate(filenames):\r\n\t\tcurr_MFCC = generate_data()\r\n\t\tcurr_MFCC_raw = curr_MFCC.tostring()\r\n\t\tcurr_filename_raw = str(filename)+'-'+str(i)\r\n\t\texample = tf.train.Example(features=tf.train.Features(\r\n\t\t\tfeature={\r\n\t\t\t'MFCC': _bytes_feature(curr_MFCC_raw),\r\n\t\t\t'filename': _bytes_feature(curr_filename_raw)\r\n\t\t\t})\r\n\t\t)\r\n\t\ttfrecord_writer.write(example.SerializeToString())\r\n\ttfrecord_writer.close()\r\n\r\n\r\ndef test():\r\n\tthreading_start = time.time()\r\n\tcoord = tf.train.Coordinator()\r\n\tthreads = []\r\n\tfor thread_index in xrange(4):\r\n\t\targs = (str(thread_index), range(200000))\r\n\t\tt = threading.Thread(target=write_instances_to_tfrecord, args=args)\r\n\t\tt.start()\r\n\t\tthreads.append(t)\r\n\tcoord.join(threads)\r\n\tprint 'w/ threading takes', time.time()-threading_start\r\n\r\n\tstart = time.time()\r\n\twrite_instances_to_tfrecord('5', range(800000))\r\n\tprint 'w/o threading takes', time.time()-start\r\n\r\nif __name__ == '__main__':\r\n\ttest()\r\n```\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21068, "title": "Removed allowBackup, label and supportsRtl", "body": "This should not be present in a library.", "comments": ["Can you review this, @aselle?", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 140 days with no activity and the `awaiting review` label has been applied.", "140 days :sob: "]}, {"number": 21067, "title": "AttributeError: 'Series' object has no attribute 'columns'", "body": "Hi,\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Hugh Sierra\r\nTensorFlow installed from (source or binary): I did pip install tensor flow\r\nTensorFlow version (use command below): v1.9.0-0-g25c197e023 1.9.0\r\nPython version: 3.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce:\r\nPlease find below the error am having.\r\n\r\n\r\n/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nupdated\r\nPerforming function: predict\r\non outcome: continuous_static_outcome_occupancy\r\nPredicting for date 2017-01-01 00:00:00\r\nTraceback (most recent call last):\r\n  File \"./src/NeirbiLSTM-occupancy.py\", line 52, in <module>\r\n    predict.run()\r\n  File \"/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/predict.py\", line 51, in run\r\n    [rawdata, var_type,var_timing,var_use,key] = preprocess.preprocess(rawdata,checks=False)\r\n  File \"/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/preprocess.py\", line 29, in preprocess\r\n    meta_data = np.core.defchararray.split(np.asarray(rawdata.columns).astype(str),sep=\"_\")\r\n  File \"/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\", line 4372, in __getattr__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'Series' object has no attribute 'columns'", "comments": ["I would be glad if you can help resolve this issue, as am stuck on my work\nbecause of it. Please let me know if you need more info.\n\nOn Tue, Aug 7, 2018 at 3:13 PM, Alfred Sorten Wolf <notifications@github.com\n> wrote:\n\n> Nagging Assignee @tatatodd <https://github.com/tatatodd>: It has been 14\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21067#issuecomment-411167503>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Ag8LYm4ouTSpflQTeyExcCAXR_rF5uD1ks5uOebbgaJpZM4VbgK0>\n> .\n>\n", "Nagging Assignee @tatatodd: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We recommend GitHub Issues for TensorFlow related code bugs and feature requests.  This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21066, "title": "add Kernel Inception Distance to tf.contrib.gan.eval", "body": "The KID is a score similar to the FID, but with an unbiased, asymptotically normal estimator. It was introduced by our paper [_Demystifying MMD GANs_](https://arxiv.org/abs/1801.01401); a very similar metric was also recommended by [_An empirical study on evaluation metrics of generative adversarial networks_](https://arxiv.org/abs/1806.07755).\r\n\r\nFor comparison to the FID, see section 4 (starting page 7) and appendices D/E (starting page 30) of our paper. As noted in the docstrings for the FID here, the FID estimator is biased and you can absolutely only compare estimates based on the same number of samples. But there's no guarantee that you still won't be very misled by the bias even then; in particular, see our Appendix D.2:\r\n\r\n> This example thus gives a case where, for the dimension and sample sizes at which we actually apply the FID and for somewhat-realistic distributions, comparing two models based on their FID estimates will not only not reliably give the right ordering \u2013 with relatively close true values and high dimensions, this is not too surprising \u2013 but, more distressingly, will _reliably give the wrong answer_, with misleadingly small variance. This emphasizes that unbiased estimators, like the natural KID estimator, are important for model comparison.\r\n\r\nCompared to our original code ([here](https://github.com/mbinkowski/MMD-GAN/blob/master/gan/compute_scores.py)), this version uses a slightly different estimator. Both are unbiased, but I think this block variant is more intuitive for general usage; its output is also \"more normal\" and gives a very simple estimate of the variance of the estimator (unlike the asymptotic one we used before, which is [not a very pretty expression](https://github.com/mbinkowski/MMD-GAN/blob/master/gan/compute_scores.py#L251)).\r\n\r\nThe functions that estimate the variance currently return an estimate as long as there are at least two blocks. This could be a little misleading; it might make sense to refuse to estimate the std if there are fewer than, say, 10 blocks, but I don't know if that added code complexity is worth it.\r\n\r\nxref: https://github.com/google/compare_gan/pull/7", "comments": ["I'm not trained to be a TensorFlow reviewer, but I just wanted to comment that this is an important feature, contributed by the authors of the paper that introduced the method. My team is likely to use this metric in several of our research papers and we'd like it to be added to TensorFlow to encourage the general community to use it as well. I'm a co-author of the existing \"Inception Score\" metric that is already in TensorFlow, and I believe researchers should abandon Inception Score and switch to the new metric.\r\n\r\n", "@joel-shor could you please take a look?", "sorry for the delay; im on vacation. ill review this next week", "Hey @joel-shor @ispirmustafa, just checking in about this. :)", "Test failures were mostly from a strange non-ascii whitespace character; no idea how that snuck in, something weird with github's web editor maybe? The only other one I noticed was an XLA error that clearly had nothing to do with this PR.", "Please wait for the tests to come back green, then we can submit.\r\n\r\nThanks for your patience, and thanks for the excellent contribution!!", "Sorry, the MacOS Contrib failure was a legit mistake in the test code: `math.ceil` is apparently a float in py2 even though it's an int in py3. One more time. :/", "Sorry @joel-shor -- can you run the tests again? Should be fixed now.", "@joel-shor @ispirmustafa, can you add the label to run the tests again? They should pass now.", "`keras_test` timeouts and the XLA error have nothing to do with this code, and the Ubuntu Python3 tests that infrastructure-errored passed on the previous commit, FYI. So it seems like tests essentially passed. :)", "Just for the record, now: the XLA error is clearly unrelated, and so is the Windows Bazel GPU error, which is:\r\n\r\n```\r\nerror C3083: '`global namespace'': the symbol to the left of a '::' must be a type\r\nexternal/com_google_absl\\absl/container/inlined_vector.h(85): note: while compiling class template member function 'absl::InlinedVector<tensorflow::TensorReference,4,std::allocator<T>>::InlinedVector(void) noexcept'\r\n        with\r\n        [\r\n            T=tensorflow::TensorReference\r\n        ]\r\n```", "@caisq The test failures are unrelated. What's the proper protocol for merging this code? Can you go ahead and merge despite the erroneous test failures?", "Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Rebased to current `master` in case the other errors have been fixed in the meantime. @joel-shor, can you hit run again?", "Ping; again, the test failures clearly have nothing to do with this code.", "Really sorry about this; let me track down someone who can submit this.", ":tada: Thanks for handling this @joel-shor!"]}, {"number": 21065, "title": "Disable sdca test on all GPU builds.", "body": "This test uses ops that don't have GPU implementations.", "comments": ["Making this change to master. Made this change to release branch last Friday"]}, {"number": 21064, "title": "Fix some minor typo/formatting in 1.10 release notes.", "body": "", "comments": []}, {"number": 21063, "title": "Remove unneeded s3_crypto[.cc,.h] files", "body": "Both s3_crypto.cc and s3_crypto.h have been moved to\r\naws_crypto.cc and aws_crypto.h files. However, due to some\r\noverlapping merge or update, s3_crypto.cc and s3_crypto.h\r\nreappeared.\r\n\r\nThis fix removes unneeded s3_crypto.cc and s3_crypto.h files.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 21062, "title": "Quantize nodes with Graph Transform tool", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**:bazel\r\n- **TensorFlow version (use command below)**:latest commit\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:0.14.1\r\n- **CUDA/cuDNN version**:9/7.1\r\n- **GPU model and memory**:1080ti\r\n- **Exact command to reproduce**:\r\n`\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=model.pb \\\r\n--out_graph=optimized_model.pb \\\r\n--inputs='Input/input' \\\r\n--outputs='fc2/Relu' \\\r\n--transforms='\r\n  add_default_attributes\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n`\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nValueError: Node 'conv2/act_quant/AssignMinEma/conv2/act_quant/min/AssignAdd/value' expects to be colocated with unknown node 'conv2/act_quant/min'\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@dhingratul, Could you provide more details on the issue and provide any codes to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 21061, "title": "[tfgan] Respect use_loss_summaries in GANEstimator", "body": "Since the refactor done in 47dea684efa41981e10299c2737317c504ce41af the `use_loss_summaries` argument of GANEstimator isn't respected anymore.\r\nThis PR restores the original behavior and passes `use_loss_summaries` down to the loss functions.", "comments": ["Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you review this please, @joel-shor ?", "@joel-shor Sorry for the delay, I removed the check for `add_summaries` using `inspect`.", "Great catch, and good fix! Thanks, @lgeiger ", "@lgeiger would you mind investigating the test failures?", "This looked like random time-outs, not related to this PR. Thanks for merging \ud83d\udc4d "]}, {"number": 21060, "title": "Fixed Point Quantization on GPU", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:n/a\r\n- **GCC/Compiler version (if compiling from source)**:n/a\r\n- **CUDA/cuDNN version**:9/7.1\r\n- **GPU model and memory**:1080ti\r\n- **Exact command to reproduce**: n/a\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI want to run a fixed point quantized model on a GPU, but looking at the documentation of TF lite, it only allows you to run the flat buffer file on ios, android or x86 cpu. Is there a way to utilize the quantization speedup on a GPU ?\r\n\r\nhttps://www.tensorflow.org/performance/quantization\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Unfortunately, there isn't a way to run TF Lite ops on GPUs, other than via the NNAPI on Android.", "Is there a future support in the pipeline for this? ", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Work on adding quantization support for GPUs has not yet been prioritized. This is because much of the acceleration available in the past is for CPU, but TF GPU quantization is on our radar, but no concrete timelines yet.", "Please comment and reopen if you have comments on the specific devices you want to run quantized on!"]}, {"number": 21059, "title": "there is still an isue with cuda 9.1", "body": "the problem in isue CUDA 9.1 Support #18961 is still unsolved is the any good workaround", "comments": ["It looks like eventually TensorFlow will use Cuda 9.2, and 9.1 will be skipped. See #18906. If you want to use Cuda 9.1, you have to compile from source."]}, {"number": 21058, "title": "Mobilenet v1 with cifar10 unexpected behavior ", "body": "\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:NA\r\n- **TensorFlow installed from (source or binary)**:SOURCE\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**:3.5\r\n\r\nHi,\r\n\r\nI am using mobilenet_v1_eval code, instead of the imagenet dataset, i change the data to cifar10 training from scratch. The only change for the architecture is 1st conv layer with stride 1 instead of 2.\r\n\r\n\r\n_CONV_DEFS = [\r\n    Conv(kernel=[3, 3], stride=1, depth=32),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=64),\r\n    DepthSepConv(kernel=[3, 3], stride=2, depth=128),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=128),\r\n    DepthSepConv(kernel=[3, 3], stride=2, depth=256),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=256),\r\n    DepthSepConv(kernel=[3, 3], stride=2, depth=512),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\r\n    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),\r\n    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)\r\n] \r\n\r\nTraining was no problem, loss decrease and prediction seems good. But in the evaluation, I use the same code as the mobilenet_v1_eval with input data as cifar10, I am gettting the same output for each image I pass in to the model. I have double checked the my input is definitely different every time, but it is very weird to get an exact same output for different images.\r\n\r\n[[-0.11333117 -0.5380551   0.18907356  0.7664664   0.07711207  0.04618246\r\n   0.13568665  0.1360816  -0.36744678 -0.33176792]]\r\n[[-0.11333117 -0.5380551   0.18907356  0.7664664   0.07711207  0.04618246\r\n   0.13568665  0.1360816  -0.36744678 -0.33176792]]\r\n[[-0.11333118 -0.5380551   0.18907356  0.7664664   0.07711206  0.04618246\r\n   0.13568665  0.1360816  -0.36744678 -0.33176792]]\r\n[[-0.11333118 -0.5380551   0.18907356  0.7664664   0.07711206  0.04618246\r\n   0.13568665  0.1360816  -0.36744678 -0.33176792]]\r\n[[-0.11333118 -0.5380551   0.18907356  0.7664664   0.07711206  0.04618246\r\n   0.13568665  0.1360816  -0.36744678 -0.33176792]]\r\n\r\nPlease help, any suggestion will be helpful! Thank you in advance!\r\n", "comments": ["I have figured out issue is due to the slim.batch_norm, like other people was having the same problem as well (i.e. https://github.com/tensorflow/models/issues/3556) \r\nBUT in the mobilenet_v1 eval code, `scope = mobilenet_v1.mobilenet_v1_arg_scope(is_training=False,weight_decay=0.0)` \r\nif I set the is_training to True in eval, it outputs different predictions, if I set is_training to False (which I think I should) the predictions are the same for different images.\r\n\r\nI see other people are mention training using `slim.learning.create_train_op` can solve the prolem, this is what I am using. but I am still having the issue.\r\n\r\nSo I am confused about the slim.batch_norm in mobilenetv1 now, should I set the is_training to True in eval? or is there anything else that I am missing?\r\nThank you in advance.", "Hello @xiao1228, I had a similar error and I found out that I didn't save the moving mean and moving variance variables from slim.batch_norm after training, so I couldn't use is_training=False\r\nDo you create a Saver with tf.trainable_variables()? If that's the case you should remove tf.trainable_variables() and create a saver like this: saver = tf.train.Saver() then you save tf.global_variables() including moving mean and moving variance", "Thank you @NPetsky You are right it is due to the moving mean & variance. I find another way to solve it by just adding `'updates_collections':None,` in the batch_norm_params. This is suggested here https://github.com/tensorflow/tensorflow/issues/1122 \r\nHowever, I am using the code directly from the example, so I am wondering, for imagenet training and eval, did people have the same issue? Or just cifar10 cause this problem?", "Good question, example code is supposed to work without code manipulation :)\r\nDoes your eval work now with 'updates_collections':None in training? Because I thought if you use slim.learning.create_train_op moving mean and moving variance are updated and you don't need 'updates_collections':None (this only updates the variables in place instead of adding them to the GraphKeys.UPDATE_OPS collection)\r\nAnother link maybe useful: https://github.com/tensorflow/tensorflow/issues/11965", "Yea, everything seems to work fine with` 'updates_collections':None` both in training and eval. However, as other issues mentioned (https://github.com/tensorflow/tensorflow/issues/1122), this may take longer time to train as it is not efficient. But if I am using slim.learning.create_train_op with update_ops =tf.get_collection( GraphKeys.UPDATE_OPS ) like the original code, my eval results was the same for different images. Another reason might also due to the batch_norm_decay, I see in https://github.com/tensorflow/tensorflow/issues/11965, they also mentioned that. The original value was 0.997 for imagenet, and I changed it to 0.9. With values like 0.997 it may requires more steps to see the changes in eval results, however we don't know what is the roughly the step size for cifar10. I use decay as 0.997 with `update_ops =tf.get_collection( GraphKeys.UPDATE_OPS )`  (original code) and ran it up to 10k steps still output the same prediction for different images. But after I changed it to 0.9 with ` 'updates_collections':None`  , in the first 50 steps or less I already can see that eval predictions give different labels. "]}, {"number": 21057, "title": "Introduce support for tf.float16 in tf.contrib.image", "body": "Hi,\r\n\r\nWould be nice to use these helper functions with tf.float16 tensors to avoid having to cast them back.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/image/python/ops/image_ops.py\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@tomvars PR #20249 adds the tf.float16 support for tf.contrib.image and has been merged."]}, {"number": 21056, "title": "Build failed on windows 10 MSB3073: :VCEnd\" exited with code 1", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: Visual Studio 2017\r\n- **CUDA/cuDNN version**: 9.2/7.1\r\n- **GPU model and memory**: GTX 1080 8GB\r\n- **Exact command to reproduce**: MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj\r\n\r\n### Describe the problem\r\nBuild failed with error MSB6006: \"cmd.exe\" exited with code 1. Before that, there is an ModuleNotFound Error: No module named 'tensorflow.python.tools.api' while running create_python_api, even though I do find it from the specified location.\r\n\r\n### Source code / logs\r\n```\r\nTask Parameter:BuildSuffix=\r\n                     :VCEnd (TaskId:7050)\r\n                     Task Parameter:TrackerLogDirectory=x64\\Release\\estimator_python_api\\estimato.8A00678D.tlog\\ (TaskId:7050)\r\n                     Task Parameter:MinimalRebuildFromTracking=True (TaskId:7050)\r\n                     Task Parameter:TrackFileAccess=True (TaskId:7050)\r\n                     Task Parameter:ToolArchitecture=Native32Bit (TaskId:7050)\r\n                     Write Tracking Logs: (TaskId:7050)\r\n                     \tx64\\Release\\estimator_python_api\\estimato.8A00678D.tlog\\custombuild.write.1.tlog (TaskId:7050)\r\n                     Read Tracking Logs: (TaskId:7050)\r\n                     \tx64\\Release\\estimator_python_api\\estimato.8A00678D.tlog\\custombuild.read.1.tlog (TaskId:7050)\r\n                     No output for C:\\WORK\\LIBRARIES_LOCAL\\TENSORFLOW\\TENSORFLOW\\CONTRIB\\CMAKE\\BUILD\\CMAKEFILES\\C75366AEF4FA6332EF71118C646F7D1C\\__INIT__.PY.RULE was found in the tracking log; source compilation required. (TaskId:7050)\r\n                     C:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\CMakeFiles\\c75366aef4fa6332ef71118c646f7d1c\\__init__.py.rule will be compiled because it was not found in the tracking log. (TaskId:7050)\r\n                     setlocal\r\n                     cd C:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\r\n                     if %errorlevel% neq 0 goto :cmEnd\r\n                     C:\r\n                     if %errorlevel% neq 0 goto :cmEnd\r\n                     C:\\work\\tools\\cmake-3.12.0-win32-x86\\bin\\cmake.exe -E env PYTHONPATH=C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python C:/Anaconda3/envs/python36/python.exe C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/tools/api/generator/create_python_api.py --apidir=C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/estimator/api --package=tensorflow.python.estimator --apiname=estimator --output_package=tensorflow.python.estimator.api C:/work/libraries_local/tensorflow/estimator_api_init_files_list.txt\r\n                     if %errorlevel% neq 0 goto :cmEnd\r\n                     :cmEnd\r\n                     endlocal & call :cmErrorLevel %errorlevel% & goto :cmDone\r\n                     :cmErrorLevel\r\n                     exit /b %1\r\n                     :cmDone\r\n                     if %errorlevel% neq 0 goto :VCEnd (TaskId:7050)\r\n                     Generating __init__.py files for Python API. (TaskId:7050)\r\n                     Traceback (most recent call last): (TaskId:7050)\r\n                       File \"C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module> (TaskId:7050)\r\n                         from tensorflow.python.tools.api.generator import doc_srcs (TaskId:7050)\r\n                     ModuleNotFoundError: No module named 'tensorflow.python.tools.api' (TaskId:7050)\r\n15:52:23.015     2>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj]\r\n                   Done executing task \"CustomBuild\" -- FAILED. (TaskId:7050)\r\n15:52:23.016     2>Done building target \"CustomBuild\" in project \"estimator_python_api.vcxproj\" -- FAILED.: (TargetId:18239)\r\n15:52:23.016     2>Done Building Project \"C:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\estimator_python_api.vcxproj\" (default targets) -- FAILED.\r\n15:52:23.016     1>Done executing task \"MSBuild\" -- FAILED. (TaskId:8)\r\n15:52:23.016     1>Done building target \"ResolveProjectReferences\" in project \"tf_python_build_pip_package.vcxproj\" -- FAILED.: (TargetId:12)\r\n15:52:23.016     1>Done Building Project \"c:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default targets) -- FAILED.\r\n```\r\n", "comments": ["I receive the same error when Building TensorFlow in Windows 10 (without GPU version) 1.9 with Visual Studio 2015 after 2,5 hours of buidling time. \r\n\r\nFile \"D:/temp/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module> from tensorflow.python.tools.api.generator import doc_srcs\r\nModuleNotFoundError: No module named 'tensorflow.python.tools.api'\r\nC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1.\r\n\r\nProtobuf, google and absl-py packages in the python environment were updated beforehand.\r\n \r\nThe machine that was used has 16 GB RAM and 2GB RAM for the nVidia GPU.\r\nIf someone has a clue how to avoid this error message, please help.", "It turns out previously ModuleNotFoundError was caused by the fact that CUDNN directory wasn't in my PATH. (I found out by importing the same module in python which gives me more information.) After fixing that and installing/upgrading protobuf and absl-py, I now hit a different error error MSB3073: :VCEnd\" exited with code 1. Here is the part of log where it first occurred:\r\n\r\n```\r\n                     creating build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\Eigen\\src\\Geometry\\arch (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\Geometry\\arch\\Geometry_SSE.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\Geometry\\arch (TaskId:1454)\r\n                     creating build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\Eigen\\src\\Householder (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\Householder\\BlockHouseholder.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\Householder (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\Householder\\Householder.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\Householder (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\Householder\\HouseholderSequence.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\Householder (TaskId:1454)\r\n                     creating build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\BasicPreconditioners.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\BiCGSTAB.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\ConjugateGradient.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\IncompleteCholesky.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\IncompleteLUT.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\IterativeSolverBase.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n                     copying tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\LeastSquareConjugateGradient.h -> build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers (TaskId:1454)\r\n12:44:07.533     1>EXEC : error : could not create 'build\\bdist.win-amd64\\wheel\\tensorflow_gpu-1.9.0.data\\purelib\\tensorflow\\include\\tensorflow/include/external/eigen_archive\\Eigen\\src\\IterativeLinearSolvers\\LeastSquareConjugateGradient.h': No such file or directory [c:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\n12:44:08.519     1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(133,5): error MSB3073: The command \"setlocal [c:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(133,5): error MSB3073: C:\\work\\tools\\cmake-3.12.0-win32-x86\\bin\\cmake.exe -E copy C:/work/libraries_local/tensorflow/tensorflow/tools/pip_package/setup.py C:/work/libraries_local/tensorflow/tensorflow/contrib/cmake/build/tf_python/ [c:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(133,5): error MSB3073: if %errorlevel% neq 0 goto :cmEnd [c:\\work\\libraries_local\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(133,5): error MSB3073: :cmEnd\r\n...\r\n```\r\nIt appears it's attempting to copy files over but somehow didn't manage to copy LeastSquareConjugateGradient.h to the target location, even though it exists in source location. I also tried manually copying the file over as well as a clean build, but each time it still hits the same error.\r\n", "Did you build from HEAD?\r\n", "@bignamehyp Yes it was built against HEAD.", "Figured it out - in my case it was because target copy path is too long so it has problem creating a file with specified path. It would be nice if the error message can be made more informative.", "Nagging Assignee @bignamehyp: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]