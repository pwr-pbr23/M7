[{"number": 46536, "title": "Example code bug in documentation related to tf.data.Dataset.interleave", "body": "## URL with the issue:\r\nhttps://www.tensorflow.org/guide/data_performance\r\n\r\n## Description of issue:\r\nThe following has been tested with TF version v2.4.0-0-g582c8d236cb.\r\n\r\nOn the above-mentioned documentation page, it is defined:\r\n\r\n`def __new__(cls, num_samples=3):`\r\n\r\nwhich is ok up to some point, but results in a bug for the variant using `interleave`, since then internally a tensor is provided as second argument, so that `num_samples` won't be 3. In fact, it alternates between 0 and 1 in `_generator`. One can easily check this out by using the `print` function - both in `__new__` and in `_generator`.\r\n\r\nAssuming the person writing this part of the documentation wasn't aware of this and this problem is not version specific, it is very likely that the timing results are wrong accordingly when using `interleave`.\r\n\r\nIt is also worth mentioning that when using `tf.data.Dataset.range(2).interleave`, the generated data in total is doubled in size (**if fixing the above-mentioned bug**), so that the timing results cannot be compared 1:1. When using `tf.data.Dataset.range(2).interleave`, the timing results should be divided by two to allow for a fair comparison, which isn't mentioned in the documentation.\r\n\r\nSee my gist [here](https://gist.github.com/padoremu/de948c6133c365bb3249c77b172aa4fd).", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/e73270cc9aa9431b7f95ea69ca5bfb20/46536-2-3.ipynb), [TF v2.4](https://colab.research.google.com/gist/amahendrakar/3a515cd6ca720be96268009d941a5f37/46536.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/aa7f3ae07a3ce055139691c4a3a94b72/46536-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!"]}, {"number": 46535, "title": "AssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects", "body": "I want to make a prediction(do beam search) after every save of ckpt using Estimator for saving best ckpt. So I add a saving_listeners(CheckpointSaverListener) to estimator.train. But every time I raise a mistake. \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n\r\nHere is my CheckpointSaverListener code.\r\n```\r\nclass SaveBestCheckpointSaverListener(tf.train.CheckpointSaverListener):\r\n    def __init__(self, save_checkpoints_steps, keep_checkpoint_max, output_dir):\r\n        self.best_ckpt = None\r\n        self.best_path_match = None\r\n        self.best_global_step_value = None\r\n        self.save_checkpoints_steps = save_checkpoints_steps\r\n        self.keep_checkpoint_max = keep_checkpoint_max\r\n        self.output_dir = output_dir\r\n\r\n    def begin(self):\r\n        # You can add ops to the graph here.\r\n        print('Starting the session.')\r\n        # self.your_tensor = ...\r\n\r\n    def before_save(self, session, global_step_value):\r\n        print('About to write a checkpoint')\r\n\r\n    def after_save(self, session, global_step_value):\r\n        print('Done writing checkpoint at {}.'.format(global_step_value))\r\n        global_step_value = int(global_step_value)\r\n        if global_step_value == 0:\r\n            return\r\n        current_ckpt = 'model.ckpt-{}'.format(global_step_value)\r\n       # do beam search prediction\r\n        current_path_match = do_predict()\r\n        print('current result: {} : {}'.format(current_ckpt, current_path_match))\r\n        if not self.best_ckpt:\r\n            self.best_ckpt = current_ckpt\r\n            self.best_path_match = current_path_match\r\n            self.best_global_step_value = str(global_step_value)\r\n        else:\r\n            if current_path_match > self.best_path_match:\r\n                self.best_ckpt = current_ckpt\r\n                self.best_path_match = current_path_match\r\n                self.best_global_step_value = str(global_step_value)\r\n                print('Saved best ckpt with path_match {}'.format(current_path_match))\r\n\r\n    def end(self, session, global_step_value):\r\n        print('best model {}, remove useless models.'.format(self.best_ckpt))\r\n        # for f in os.listdir(self.output_dir):\r\n        #     file = os.path.join(self.output_dir, f)\r\n        #     if os.path.isfile(file) and f.startswith('model'):\r\n        #         model_global_step = f[f.index('-') + 1:f.rindex('.')]\r\n        #         if model_global_step != self.best_global_step_value:\r\n        #             os.remove(file)\r\n        #             print('remove {}'.format(file))\r\n\r\n```\r\nAfter all ckpts were saved, training process will report errors. Here are errors.\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_classifier_nsp_multitask_typeEmb.py\", line 797, in <module>\r\n    tf.app.run()\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"run_classifier_nsp_multitask_typeEmb.py\", line 767, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps, hooks=hooks, saving_listeners=[listener])\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\r\n    saving_listeners)\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\r\n    yield g\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/bytedance/opt/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 5295, in get_controller\r\n    type(default))\r\nAssertionError: Nesting violated for default stack of <class 'tensorflow.python.framework.ops.Graph'> objects\r\n```\r\n", "comments": ["@gongel \r\nPlease upgrade to 2.x as there is no support for 1.x and let us know if you face any issues.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46535\">No</a>\n"]}, {"number": 46534, "title": "Segfault upon importing tensorflow", "body": "Linux Mint 20\r\nPython 3.8.5\r\nNo virtual environment\r\ni7 870 (old processor)\r\npip3 install tensorflow, also fails with tf-nightly\r\nTensorFlow version 2.4.0\r\nGTX 1060 3GB\r\n\r\n\"import tensorflow\" caues a segfault", "comments": ["@Tobs40 \r\n\r\nPlease, see tested build configuration from [here](https://www.tensorflow.org/install/source#gpu).\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).Thanks!", "...so I have to buy a new computer in order to use tensorflow?", "No. You can use TensorFlow via Google Colab or by compiling it from source (if AVX is the issue).\r\n\r\nDoes it crash on TF 2.3 too? Can you provide more logs?", "#46516 looks similar", "> No. You can use TensorFlow via Google Colab or by compiling it from source (if AVX is the issue).\r\n> \r\n> Does it crash on TF 2.3 too? Can you provide more logs?\r\n\r\nCompiling from source looks complicated. Haven't tried TF 2.3 yet. If you can tell me where those logs are stored or how I can create them, then I'd be happy to post them here.", "Try `python -vvv -c \"import tensorflow as tf\" > log.txt` and then attach `log.txt`", "The command you suggested created an empty file log.txt\r\nI used the following instead:\r\n\r\n`python3 -vvv -c \"import tensorflow as tf\" 2>&1 | tee log.txt`\r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5842192/log.txt)\r\n`\r\n", "There doesn't seem to be anything relevant there either. Except, maybe, what is the Python version you use, ouput of `python3 --version`?", "> \r\n> \r\n> There doesn't seem to be anything relevant there either. Except, maybe, what is the Python version you use, ouput of `python3 --version`?\r\n\r\nAlready mentioned it in the main post: 3.8.5", "Can you try with the 2.4.1 release?", "> Can you try with the 2.4.1 release?\r\n\r\nI thought my CPU is too old for AVX? Is 2.4.1 supposed to work for non-AVX CPU's again?", "Oh, wait. I thought the issue is that you get an import but the CPU supports all extensions needed. Was confusing with a different issue.\r\n\r\nIf your CPU does not support AVX then the segfault is to be expected", "Ok, I've been wondering", "@Tobs40\r\n\r\nPlease, close this thread if your query has been answered. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46534\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46534\">No</a>\n", "Sorry, this was one of my first issues, I'm fairly new to githubAm 28.01.2021 14:27 schrieb ravikyram <notifications@github.com>:\n@Tobs40\nPlease, close this thread if your query has been answered. Thanks!\n\n\u2014You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub, or unsubscribe."]}, {"number": 46533, "title": "Extracting item from a list with tf.function() :  TypeError: list indices must be integers or slices, not Tensor", "body": "\r\n**System information**\r\n- Have I written custom code (see below):\r\nOS Platform and Distribution: Linux Ubuntu 18.04.5\r\nPython version: 3.7.4\r\nTensorflow : 2.4.0\r\n\r\n\r\n**Describe the current behavior**\r\nI have a list of objects (custom type), I want to extract an object in a specific index from a list. However, when running using tf.function() I'm getting an error that the index is a tensor and it should be an integer or slices : \r\n\r\n    TypeError: list indices must be integers or slices, not Tensor\r\n\r\nI know I can use tf.gather but the list can't be converted to a tensor since it includes unsupported types (specifically in my case these are loaded estimators). \r\n\r\n    TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [<__main__.my_class object at 0x7f36d11b9c88>, \r\n    <__main__.my_class object at 0x7f36d11b9828>]. Consider casting elements to a supported type.\r\n\r\nIn addition, I can't find a way to evaluate the index to convert it to a number with the tf.function.. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n          import tensorflow as tf\r\n          \r\n          class my_class():\r\n              def __init__(self,name):\r\n                  self.name = name #\r\n          x = [my_class('ron'),my_class('john')]\r\n          \r\n          def extract(idx):\r\n              y = x[idx]\r\n              return y\r\n          y = extract(1)\r\n          print(y)\r\n          \r\n          \r\n          @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.int32)])\r\n          def extract(idx):\r\n              y = x[idx]\r\n              return y\r\n          \r\n          y = extract(1)\r\n          print(y)\r\n\r\nOutput: \r\n\r\n    <__main__.my_class object at 0x7f36d1841978>\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-9-1b61ba209201> in <module>()\r\n         18     return y\r\n         19 \r\n    ---> 20 y = extract(1)\r\n         21 print(y)\r\n    \r\n    8 frames\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n        983           except Exception as e:  # pylint:disable=broad-except\r\n        984             if hasattr(e, \"ag_error_metadata\"):\r\n    --> 985               raise e.ag_error_metadata.to_exception(e)\r\n        986             else:\r\n        987               raise\r\n    \r\n    TypeError: in user code:\r\n    \r\n        <ipython-input-9-1b61ba209201>:17 extract  *\r\n            y = x[idx]\r\n    \r\n        TypeError: list indices must be integers or slices, not Tensor.\r\n\r\nIs there a way to evaluate the index with tf.function()? or some other function to extract the object from the list (it is a list wrapper)?\r\nThanks you", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/005e051feacc3282885642e9e51f0016/46533.ipynb). Thanks!", "@AiaHaruv Based on the error description (`TypeError: list indices must be integers or slices, not Tensor.`), i changed two lines in your code as shown below.\r\n\r\n```\r\n\r\n# x = [my_class('ron'),my_class('john')] # changed this line\r\nx = [my_class('ron').name,my_class('john').name]\r\n\r\n@tf.function#(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.int32)]) # changed this line \r\n```\r\n\r\nAfter those two modifications, everything worked as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/a3cf9175313c2695b0dd8293ab25bc20/46533.ipynb). \r\n\r\nPlease feel free to check the [guide](https://www.tensorflow.org/guide/function) on `tf.funciton`.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "@jvishnuvardhan - thank you for your response. \r\nThis actually doesn't solve my issue since my problem is to extract from a list which contain a custom class (and not a supported type as str, float, etc). I created this class just for an example to easily reproduce. My actual list includes estimators and I want to extract from the list an estimator from a specific position.\r\nAlso, in my case the idx is initially a tensor and not a number (there are other processes before the extraction from the list). I see that if the idx is just a number and not a tensor, it succeed to extract the class from the list. Is there a way to convert it to a number somehow within the run of tf.function? \r\n\r\n \r\n", "There are a few ways to index a static list based on a Tensor value, but there are caveats you should know first, especially if the list may contain different custom objects.\r\n\r\nThe 100% proof solution is to use a [tf.py_function](https://www.tensorflow.org/api_docs/python/tf/py_function) - pass the index as argument, and close over the list, and then return any tensors you need to extract from the objects.\r\n\r\nTo see whether methods that don't rely on py_function, can you tell us more about the use case:\r\n * is the list expected to change much in between calls to `extract`?\r\n * is the type of objects at each index expected to change across calls?\r\nThe reason why those questions are important is because reading from a Python list, and working with custom object counts as Python side effect - [this guide](https://www.tensorflow.org/guide/function#executing_python_side_effects) explains a bit about their caveats.", "@mdanatg - thank you for your answer. \r\nI read about the tf.py_function and I saw this caveat:\r\n\r\n  \"The body of the function (i.e. func) will not be serialized in a GraphDef. Therefore, you should not use this function if you need to serialize your model and restore it in a different environment.\"\r\n\r\nMy use case is that I need to save my model and then convert it to a tflite model (to run inside an android app). \r\nSo if I understand correctly, I can't use tf.py_function for that, right? \r\n\r\nSpecifically, my list is a list of loaded estimators (https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator), and I want to be able to get as an input to the function the indices to use the right models for a user (instead of loading them on runtime). \r\n\r\nNot 100% related, but I encountered a similar problem also when trying to predict using the loaded estimator. \r\n(I detailed this error in this issue : https://github.com/tensorflow/tensorflow/issues/46340 ).\r\nWhen generating an example to input to the estimator, I followed the examples here: https://www.tensorflow.org/api_docs/python/tf/train/Example \r\nHowever, to create the example using the recommended function :\r\n```\r\n    def _float_feature(value):\r\n         \"\"\"Returns a float_list from a float / double.\"\"\"\r\n          return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n```\r\nI also get an error if the value is a tensor (and it is by default if I'm using tf.function) . And to create the proto message I must have the values themselves.\r\n(It is related for me since in both of these issues I need to get the tensor evaluation and if I understand correctly, currently there is no way of doing so when using the tf.function). \r\n\r\nThank you in advance, \r\n\r\n\r\n\r\n\r\n \r\n\r\n\r\n\r\n", "That's correct, if you need to save or export the model, then py_function is not an option.\r\n\r\nIf the list of loaded estimators is static (i.e. you load it once, and doesn't ever change without re-saving the model), then this can be done using [tf.switch_case](https://www.tensorflow.org/api_docs/python/tf/switch_case). The restriction is that you need to write things as tensor-in, tensor-out functions.\r\n\r\nMeaning, that instead of this:\r\n\r\n```\r\ndo_domething(estimator):\r\n  ...  code ...\r\ndo_something(estimators[0])\r\ndo_something(estimators[1])\r\n```\r\n\r\nwe write\r\n```\r\ndo_something_with_ron():\r\n  ... code using estimators[0] ...\r\ndo_something_with_john():\r\n  ... code using estimators[1] ...\r\ndo_something_with_ron()\r\ndo_something_with_john()\r\n```\r\n\r\nThen you can use tf.switch_case in a way like this:\r\n\r\n```\r\ndef use_item_0():\r\n  obj = x[0]\r\n  y = # do calculations with obj\r\n  return y  # y must be a tensor or structure of tensors\r\n\r\ndef extract(idx):\r\n  y = tf.switch_case(\r\n    idx,\r\n    [use_item_0, use_item_1, ...]\r\n  )\r\n```\r\n\r\nThis dance is necessary because TF graphs are static and fairly limited, and can't represent things like objects in a dynamic manner. So instead of writing the code with the traditional OOP, we instead think about it as building a separate graph for each estimator in the list, then using switch_case to choose which graph to run.\r\n\r\nNow, does this mean you'll have to write duplicate code? Not necessarily, because inside a single tf.function you can still write arbitrary code, like for instance a factory. So you should be able to write this:\r\n\r\n```\r\ndef make_use_item_function(i):\r\n  obj = x[i]\r\n  def use_item():\r\n    y = # do_calculations with obj\r\n    return y\r\n  return use_item\r\n\r\ndef extract(idx):\r\n  y = tf.switch_case(\r\n    idx,\r\n    [make_use_item_function(0), make_use_item_function(1), ...]\r\n  )\r\n```", "@mdanatg - got it, thank you! \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46533\">No</a>\n"]}, {"number": 46531, "title": "TFLite model runs slowly in TV devices when set use NNAPI", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Yes, all codes were written by myself. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Win10\u3001Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TV devices (Hisense U7F) with mediatek 9652 SOC\r\n- TensorFlow installed from (source or binary):\r\n- Offical\r\n- TensorFlow version (use command below):\r\n- TF2.3\u3001TF1.14\u3001TF1.13.1 were tried\r\n- Python version:\r\n- 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the expected behavior**\r\nAim: Run a MobileNet TFLite model in TV devices with NNAPI in 100ms.\r\n\r\n**Describe the current behavior**\r\nI write a MobileNet-v2 custom code and convert it to tflite model. We set the tflite file to Hisense U7F TV devices with mediatek 9652 SOC (containing NNAPI). TV capture the image through camera (android.camera2) and inference with Interpreter but the reference time up to 300ms. It should be mentioned that the mediatek profiler reported that all OPs were run in NNAPI successfully and time consuming in NNAPI was 10ms.\r\nBesides, we tried different TF version and its java denpendencies including 2.3, 1.14, 1.13.1. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nThe tflite model as :\r\n[TFModel-short.zip](https://github.com/tensorflow/tensorflow/files/5834848/TFModel-short.zip)\r\n\r\nThe core java code as following:\r\n```\r\n                try {\r\n                    Interpreter.Options options = (new Interpreter.Options()).setAllowFp16PrecisionForFp32(true);\r\n                    mTFLite = new Interpreter(loadModelFile(mModelname), options);\r\n                    mTFLite.setUseNNAPI(true);\r\n                } catch (IOException e) {\r\n                    e.printStackTrace();\r\n                }\r\n\r\n                float[][][][] firstOutput = new float[1][7][7][34];\r\n\r\n                long startTime = System.currentTimeMillis();\r\n                mTFLite.run(byteBuffer,firstOutput);\r\n                long endTime = System.currentTimeMillis();\r\n                inferenceTime = endTime - startTime;\r\n\r\n                System.out.println(\"Inference Time: \"+inferenceTime+\"ms\");\r\n                mTFLite.close();\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThe android Studio log as following:\r\n\r\n> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: ****************** Profiler Start ******************\r\n> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Execution Step   : 0\r\n> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Execution Result : 1\r\n> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Device Name      : neuron-ann\r\n> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Operations       : CONV:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:ADD:CONV:DEPTHWISE_CONV:CONV:CONV:DEPTHWISE_CONV:CONV:CONV\r\n> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: Spent Time       : 13747 us\r\n> 2021-01-19 15:48:50.041 15661-16292/com.example.android_camera2 I/MtkExecutionBuilder: ****************** Profiler End   ******************\r\n> 2021-01-19 15:48:50.043 15661-15699/com.example.android_camera2 I/System.out: Inference Time: 319ms\r\n\r\n\r\n\r\n\r\n", "comments": ["It was solved for the Interpreter initializing: \r\n```\r\n                try {\r\n                    Interpreter.Options options = (new Interpreter.Options()).setAllowFp16PrecisionForFp32(true);\r\n                    mTFLite = new Interpreter(loadModelFile(mModelname), options);\r\n                    mTFLite.setUseNNAPI(true);\r\n                } catch (IOException e) {\r\n                    e.printStackTrace();\r\n                }\r\n```\r\nwas a time consuming processing. Make sure it was initialized only once and avoid initialzing repeatly or write it in a callback function."]}, {"number": 46530, "title": "save custom training loop with keras API (train_step)", "body": "Hi, based on the #38103 issue, I'd like to reopen the discussion:\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Windows/Linux as well)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): last one, 2.4.0\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nI'm using the feature: custom training logic with `model.fit` by overriding `model.train_step` (and `model.test_step`). It's working well, except for saving/loading cases, I have the following issue:\r\nWhen I save my model (`model.save`) with the custom training logic and then I want to load it, the custom training loop is not saved and when I apply a `model.fit` for my loaded model, it comes back to the default one.\r\n\r\n\r\n**Describe the expected behavior**\r\nI'd expect to save as well the model.train_step (and also `test_step`). Especially with the new 2.4 feature `model.save(save_traces=True)`. Is it possible (and if yes, how) ? If no, is a such feature planned or should I write a feature request ?\r\n\r\n**Standalone code to reproduce the issue**\r\n[Gist](https://colab.research.google.com/gist/quetil/51c1b89bddd5e25d896678b87b62712f/save-custom-train_step-in-subclassing-model.ipynb)\r\n\r\nThank you in advance,", "comments": ["Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/1b04d27710a0136b72ff546df22f5760/46530.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/5e5a3ad01ed5d2c6a1c853d4a2bb4619/46530-tf-nightly.ipynb). \r\n\r\nWhereas running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/2deed26ac2f6239ca51f08d1bfa65865/46530-2-3.ipynb#scrollTo=OkFh162Qzj-M) throws an error stating `TypeError: save() got an unexpected keyword argument 'save_traces'`. Please check the linked gist for reference. Thanks!", "@quetil \r\nTL/DR:  you need to implement `get_config` and `from_config` methods. There is an extensive [guide](https://www.tensorflow.org/guide/keras/save_and_serialize#how_savedmodel_handles_custom_objects). \r\n\r\nThere are some differences while saving and loading of Sequential/Functional models when compared to Subclass models. The approach you followed is correct for Sequential/Functional models but need slight modification as shown in the following [gist](https://colab.research.google.com/gist/jvishnuvardhan/9e531f99119f1f13d6a0f7916aa23c95/46530-tf-nightly.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46530\">No</a>\n"]}, {"number": 46529, "title": "Should pb file match CUDA version?", "body": "\r\nI used tensorflow.git and bazel to build a model. The detail process is\r\n\r\n1. git clone tensorflow.git\r\n2. ./configure << tensorflow configure\r\n3. I answered N in every question of configure process including 'Use CUDA'.\r\n4. bazel-build my model to pb file\r\n\r\nThen, can I use my model in any device that set CPU or GPU(any CUDA, Cudnn version)?\r\n\r\nWhen I run my pb file, it occurs to fail to make cuFFT batched plan.\r\n\r\nAs I searched, it was because CUDA version doesn't match, is it right?", "comments": ["@Ellie1013 \r\nPlease find cuda version compatibility [here](https://www.tensorflow.org/install/source). cuda 11.0 is compatible with latest version of tensorflow and stable version 2.4.\r\nPlease share logs of any failure faced in case it is still an issue.", "What does mean 'Do you wish to build TensorFlow with CUDA support?' for tensorflow configure process? to build pb file? or to run pb file?\r\n\r\nLast week pb file worked correctly, so it returned the result. But, now it doesn't work. I don't know what happen.\r\n\r\nThe detail setting is...\r\n\r\nLinux \r\nTensorflow v.1.15 (It must use tensorflow version 1) \r\nCUDA 10.2.89 \r\nCudnn 7.6.5.32\r\nbazel 0.26.1\r\n\r\nError log is... 2021-01-19 17:00:45.138402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1 2021-01-19 17:00:45.138469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0 2021-01-19 17:00:45.140340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix: 2021-01-19 17:00:45.140359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] 0 1 2021-01-19 17:00:45.140368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N N 2021-01-19 17:00:45.140392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1: N N 2021-01-19 17:00:45.142855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10024 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5) 2021-01-19 17:00:45.144418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 5035 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:b3:00.0, compute capability: 7.5) 2021-01-19 17:00:45.146850: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564476c72980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2021-01-19 17:00:45.146877: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5 2021-01-19 17:00:45.146889: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5 2021-01-19 17:00:55.558024: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0 2021-01-19 17:00:55.757115: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7 2021-01-19 17:00:57.329001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0 2021-01-19 17:00:57.469277: E tensorflow/stream_executor/cuda/cuda_fft.cc:223] failed to make cuFFT batched plan:5 2021-01-19 17:00:57.469333: E tensorflow/stream_executor/cuda/cuda_fft.cc:426] Initialize Params: rank: 1 elem_count: 2048 input_embed: 1025 input_stride: 1 input_distance: 1025 output_embed: 2048 output_stride: 1 output_distance: 2048 batch_count: 46 2021-01-19 17:00:57.469343: F tensorflow/stream_executor/cuda/cuda_fft.cc:435] **failed to initialize batched cufft plan with customized allocator: Failed to make cuFFT batched plan.**", "@Ellie1013\r\nThere is no support for tensorflow 1.x now, please upgrade to 2.x.\r\nTensorFlow 2.4 is built and tested against CUDA 11.0 and cuDNN 8. Fore more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nCould you please install CUDA 11.0 with cuDNN 8 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46529\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46529\">No</a>\n"]}, {"number": 46528, "title": "tflite.Interpreter version 2.5.0 not run on widows 10  , please get simple python code example", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@Abdelghafar-Refat \r\n\r\nPlease, refer the [link](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) and see if it helps you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46528\">No</a>\n"]}, {"number": 46527, "title": "RecvAsync is cancelled error on Ryzen High Performance mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n>No\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n> Windows 10\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n>No\r\n\r\n- TensorFlow installed from (source or binary):\r\n>binary\r\n\r\n- TensorFlow version (use command below):\r\n>2.4.0\r\n\r\n- Python version:\r\n>3.8.7\r\n\r\n- Bazel version (if compiling from source):\r\nN/A\r\n\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n\r\n- CUDA/cuDNN version:\r\n>V11.0.221\r\n\r\n- GPU model and memory:\r\n>RTX 3070 FE (VRAM - 8GB, system memory- 32GB)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n>v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n\r\n**Describe the current behavior**\r\n\r\n>I am trying to run the sample code [here](https://www.tensorflow.org/tutorials/text/text_classification_rnn). \r\n1. If the computer is on Ryzen High Performance mode, while executing the statement \"model.fit()\", the code breaks with an error \"RecvAsync is cancelled\" after 2 epochs.  (attached is the screenshot of the error)\r\nerror- \r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node gradient_tape/sequential/embedding/embedding_lookup/Reshape/_54}}]] [Op:__inference_train_function_22374]\r\n\r\nFunction call stack:\r\ntrain_function\r\n\r\n\r\n\r\n\r\n\r\n2. If the computer is on Power saving mode, the model trains successfully without an error. \r\nIn both, the above cases the CPU, RAM and GPU utilization was below 20%\r\n \r\nSystem specifications - \r\nRyzen 3700X | 32GB 3000MHz CL15 DDR4 ram | RTX 3070 | Windows 10\r\n\r\n**Describe the expected behavior**\r\nThe code should run successfully regardless of the Power option selected. Especially in Ryzen High Performance mode, the code should run faster and without any issues. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI am trying to run the code from the tensorflow website - \r\nhttps://www.tensorflow.org/tutorials/text/text_classification_rnn\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n![system_sw_info](https://user-images.githubusercontent.com/25884862/104998987-20db6780-5a52-11eb-835e-5f25b8a97cdc.png)\r\n![tensorflow_error](https://user-images.githubusercontent.com/25884862/105003986-5a63a100-5a59-11eb-93b6-fc6203bdc681.png)\r\n", "comments": ["@chiranshu14,\r\nCould you please try limiting the total GPU memory using any one of the methods listed in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if you are facing the same error.\r\n\r\nAlso, please take a look at [this comment](https://stackoverflow.com/a/58517209) from a similar StackOverflow query and let us know if it helps. Thanks!", "Hi @amahendrakar ,\r\n\r\nI tried the following solutions that you mentioned - \r\n1.  Enable memory growth\r\n2. Restricting GPU memory limit to 1024 and 4096 MB\r\n3. setting the OS environment variable TF_FORCE_GPU_ALLOW_GROWTH to true\r\n\r\nBut none of these worked. Note that, the PC was on Ryzen High Performance mode. ( I also tried High Performance and Balanced modes) and it still didn't work.\r\n\r\nWhen I switched to \"Power Saving\" mode, the code ran without having to do any of the above steps. \r\nAlso, I was continuously monitoring the hardware info of the GPU in the course of training and it never crossed 50% of the GPU memory and 95% of utilization. So it seems like it is not that tf isn't getting enough memory. There is something with Battery Saving option that makes it run without any issues.\r\n![image](https://user-images.githubusercontent.com/25884862/105109358-68a9cf80-5ae2-11eb-84e5-add42eeb9701.png)\r\n\r\n\r\n\r\nThanks!", "@amahendrakar ,\r\n\r\nHere is the complete error description for your reference (Battery option - Ryzen High Performance)\r\n\r\n\r\nEpoch 1/10\r\n391/391 [==============================] - 38s 80ms/step - loss: 0.6847 - accuracy: 0.5111 - val_loss: 0.5003 - val_accuracy: 0.7255\r\nEpoch 2/10\r\n373/391 [===========================>..] - ETA: 1s - loss: 0.4305 - accuracy: 0.8031\r\n---------------------------------------------------------------------------\r\nCancelledError                            Traceback (most recent call last)\r\n<ipython-input-19-7944b517869f> in <module>\r\n----> 1 history = model.fit(train_dataset, epochs=10,\r\n      2                     validation_data=test_dataset,\r\n      3                     validation_steps=30)\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1098                 _r=1):\r\n   1099               callbacks.on_train_batch_begin(step)\r\n-> 1100               tmp_logs = self.train_function(iterator)\r\n   1101               if data_handler.should_sync:\r\n   1102                 context.async_wait()\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    853       # In this case we have created variables on the first call, so we run the\r\n    854       # defunned version which is guaranteed to never create variables.\r\n--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    856     elif self._stateful_fn is not None:\r\n    857       # Release the lock early so that multiple threads can perform the call\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2940       (graph_function,\r\n   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n-> 2942     return graph_function._call_flat(\r\n   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   2944 \r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1916         and executing_eagerly):\r\n   1917       # No tape is watching; skip to running the function.\r\n-> 1918       return self._build_call_outputs(self._inference_function.call(\r\n   1919           ctx, args, cancellation_manager=cancellation_manager))\r\n   1920     forward_backward = self._select_forward_and_backward_functions(\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    553       with _InterpolateFunctionError(self):\r\n    554         if cancellation_manager is None:\r\n--> 555           outputs = execute.execute(\r\n    556               str(self.signature.name),\r\n    557               num_outputs=self._num_outputs,\r\n\r\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nCancelledError:  [_Derived_]RecvAsync is cancelled.\r\n\t [[{{node Adam/Adam/update/AssignSubVariableOp/_57}}]]\r\n\t [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_54]] [Op:__inference_train_function_22374]\r\n\r\nFunction call stack:\r\ntrain_function", "@amahendrakar Some more insights - \r\nI was able to run the [Neural style transfer code ](https://www.tensorflow.org/tutorials/generative/style_transfer) and the [Image classifier code ](https://www.tensorflow.org/tutorials/images/classification) without any issues in the high-performance mode. \r\nSo my guess is there is an issue specifically with the RNN implementation.", "@chiranshu14,\r\nCould you please try running the code in a new virtual environment and check if it works.\r\n\r\nAlso, check if you are facing the same issue with the latest TF-nightly as well. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46527\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46527\">No</a>\n"]}, {"number": 46526, "title": "CrossShardOptimizer must be used for model training on TPUs", "body": "Running [the example](https://github.com/EleutherAI/gpt-neo) on a Colab TPU results in the following error:\r\n```\r\nFile \"main.py\", line 256, in <module>\r\n    main(args)\r\n  File \"main.py\", line 230, in main\r\n    estimator.train(input_fn=partial(input_fn, global_step=current_step, eval=False), max_steps=next_checkpoint)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3130, in train\r\n    rendezvous.raise_errors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py\", line 150, in raise_errors\r\n    six.reraise(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3125, in train\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 349, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1175, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1204, in _train_model_default\r\n    self.config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 2962, in _call_model_fn\r\n    config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1163, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3386, in _model_fn\r\n    _validate_tpu_training_graph(ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py\", line 3817, in _validate_tpu_training_graph\r\n    'CrossShardOptimizer must be used for model training on TPUs.')\r\nValueError: CrossShardOptimizer must be used for model training on TPUs.\r\n```", "comments": ["@StrangeTcy \r\nPlease share minimum code to replicate the issue faced or share a colab gist with the issue.", "@Saduf2019 \r\n[try following this notebook](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb#scrollTo=R918l14UhrBR) (https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb#scrollTo=R918l14UhrBR)", "@StrangeTcy\r\nplease attach the data set used for us to replicate the issue.", "@Saduf2019 \r\nOk, sorry, disregard the aforementioned notebook, use this one instead:\r\nhttps://colab.research.google.com/drive/1MaamQYdVsGvpWnUZ_5tDtT-OsLYcZOHI?usp=sharing\r\nIt uses the data set on my Google Drive, which I'll modify the code to `gdown`", "@StrangeTcy\r\nWe see that you are using tf 1.x version, can you please use 2.x and let us know as there is no support for 1.x.", "@Saduf2019 , \r\nI've thought of that already, and if I leave the 2.x tf, I run into [the wrong `mesh_shape` error](https://github.com/EleutherAI/gpt-neo/issues/119)", "I ran the code shared but face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/f2ce91b1d11a44cac830af5806819fa4/untitled514.ipynb).", "@jvishnuvardhan \r\nOk, I'm running the gist and running into the wrong `mesh_shape` error as mentioned above.\r\nKindly quote the error you were facing with your gist if you deem it helpful.\r\n", "@jvishnuvardhan \r\nany way to speed up the resolution of this issue?", "@jvishnuvardhan \r\nOh, no-one cares any more. Okay", "@StrangeTcy Sorry for the delay. Thanks!", "Hi @StrangeTcy, if I understand correctly, you're trying to run the code in this repo on TPUs. That repo indicates that it provides TPU support so my suggestion would be to post an issue in that repo since this seems like a support request. I do not have much familiarity with TF1.x but as I recall you need to wrap the optimizer in a [ CrossShardOptimizer](https://www.tensorflow.org/api_docs/python/tf/compat/v1/tpu/CrossShardOptimizer) if you want to use TPUs. [This blog post here might help.](https://medium.com/tensorflow/how-to-write-a-custom-estimator-model-for-the-cloud-tpu-7d8bd9068c26)", "Looks like you probably want to wrap your optimizer with `CrossShardOptimizer` when using TPU?\r\n\r\nhttps://github.com/EleutherAI/gpt-neo/blob/c0961f0177b3d8bd8aa0552df24a7da2fa5c2537/model_fns.py#L187\r\n\r\nYou might want to condition on `if params[\"use_tpu\"]:` there. ", "In the meantime I created a PR:\r\nhttps://github.com/EleutherAI/gpt-neo/pull/131\r\n\r\nHaven't really tested it though..\r\n", "@hthu\r\nWell, it definitely looks like a thing that'd help, but I'd say that it's not that I want to wrap my optimizer into a `CrossShardOptimizer`, I want for the tensorflow/mesh-tensorflow code to do it for me.", "@StrangeTcy \r\nI understood, but this is the one of the most important pieces when using TPU and this is the correctness guarantee for your computation.\r\n\r\nOtherwise your gradients over 8 TPU cores will not be fully aggregated.\r\n\r\nIf it is the framework that you're using, the framework should probably follow the rule as well.\r\n\r\nThere is also a reference on cloud tpu mentioning using the `CrossShardOptimizer`, here:\r\nhttps://cloud.google.com/tpu/docs/using-estimator-api\r\n", "@hthu \r\nhttps://stackoverflow.com/questions/66193962/best-way-to-wrap-an-optimizer-in-crossshardoptimizer -- perhaps you might recommend the best course of action?", "Looks like you are using `mesh tensorflow` here.\r\n\r\nI believe you probably shouldn't try to wrap things inside `CrossShardOptimizer` anymore as `MTF` already does cross replica sums automatically based on your mesh and layouts.\r\n\r\nCan you try to run a `2x4` or `all:8` mesh on MTF and see if those would work without wrapping `CrossShardOptimizer`?\r\n\r\n", "@hthu \r\nAs I see it, this is what led me here to begin with: if I use some unwrapped `mtf` optimizer, I get an error that `CrossShardOptimizer must be used for training on TPUs`, and I don't know the correct way to wrap v2 optimizers in `CrossShardOptimizer`", "The idea is that you shouldn't try to wrap those, at all.\r\nMesh Tensorflow and the framework you're using should already handle the `CrossReplicaSum` needed (see my comments on the other thread as well).\r\n\r\nIf you run into errors, this is likely how the framework or `MTF` handles TPU computation.\r\n\r\nLet's move the discussion to the [thread](https://github.com/EleutherAI/gpt-neo/issues/120) inside the framework you're using.", "@StrangeTcy,\r\n\r\nI can see that this [discussion](https://github.com/EleutherAI/gpt-neo/issues/120) is marked as closed, Can you let us know if its still an issue?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46526\">No</a>\n"]}, {"number": 46525, "title": "Create config.yml", "body": "Created this file for the New Template to be reflected", "comments": []}, {"number": 46524, "title": "Tflite own trained model is not running", "body": " I am using tflite object detection android sample the project is running perfectly with default model but when i change my custom object trained model and build on phone then application crushed. Is there are some steps to change model ? I just imported my tflite model in asset folder and changed name in DetectorActivity.java and DetectorTest.java. Is there any more step i need to do please let me know.\r\n \r\n Thank you", "comments": ["@syedali1621,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@syedali1621,\r\nPlease find the steps for using a [Custom Object Detection Model](https://docs.google.com/spreadsheets/d/1LozyRzbCo8GbofYEZxG0Yl37qFmRV3kVrCTJ_mEtB-M/edit?ts=5ebdba16#gid=0). \r\nThanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46524\">No</a>\n"]}, {"number": 46523, "title": "Flow", "body": "https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/python/data/experimental/ops/data_service_ops.py#L301-L483", "comments": ["@smannew02,\r\nIn order to expedite the trouble-shooting process, could you please explain in detail the issue you are facing and also provide the following details\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nThanks!", "Spam."]}, {"number": 46522, "title": "BatchNormalization inference equation in doc is incorrect", "body": "\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThis documentation states the following:\r\n\r\n> During inference (i.e. when using evaluate() or predict() or when calling the layer/model with the argument training=False (which is the default), the layer normalizes its output using a moving average of the mean and standard deviation of the batches it has seen during training. That is to say, it returns (batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta.\r\n\r\nThis equation is incorrect, testing with the source code shows it gives the wrong output.  The correct equation that gives the right output (and matches the correct equation from literature) is:\r\n\r\n`(batch - self.moving_mean) / sqrt(self.moving_var + epsilon) * gamma + beta.`\r\n\r\nI.e., it is missing square-root.\r\n\r\nFurther, for more clarity (to avoid confusion for some) it would be better to write it as:\r\n\r\n`gamma*(batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta.`\r\n", "comments": []}, {"number": 46521, "title": "partially convert_variables_to_constants use variable_names_whitelist/variable_names_blacklist will make the save/restore op disappeared and can't restore after the conversion", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not use Mobile device\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): Not use Bazel\r\n- GCC/Compiler version (if compiling from source): GCC7.4\r\n- CUDA/cuDNN version: Not use\r\n- GPU model and memory: Not use GPU\r\n\r\n**Describe the current behavior**\r\ncode is \r\n'''\r\n\r\nbuilder = tf.compat.v1.saved_model.Builder('converted')\r\n\r\nwith tf.compat.v1.Session(graph=tf.Graph()) as sess:\r\n\r\n    loaded = tf.compat.v1.saved_model.loader.load(sess, [\"serve\"], model_dir)\r\n\r\n    builder.add_meta_graph_and_variables(sess,[\"serve\"], signature_def_map=loaded.signature_def)\r\n\r\n    graph_def = convert_variables_to_constants(\r\n\r\n        sess,\r\n\r\n        graph_def,\r\n\r\n        output_node_names=output_node_list,\r\n\r\n        variable_names_blacklist=embedding_var_list,\r\n\r\n       #variable_names_whitelist=freeze_var_list)\r\n\r\nwith tf.compat.v1.Session(graph=tf.Graph()) as sess:\r\n\r\n    tf.import_graph_def(graph_def, name=\"\")\r\n\r\n    output_tensors = prepare_output_tensors(sess)\r\n\r\n    input_tensors = prepare_input_tensors(sess)\r\n\r\n    sigs = {}\r\n\r\n    sigs['eval'] = tf.compat.v1.saved_model.signature_def_utils.predict_signature_def(output_tensors, input_tensors)\r\n\r\n    builder.add_meta_graph(['eval'], signature_def_map=sigs)\r\n'''\r\nthe reason to use variable_name_blacklist is because the embedding variables is larger than the limit 2GB to freeze into the inference proto buffer, so i want to partially keep them as variables and freeze other nodes, then optimize for performance improvement. \r\nbut after i get the graph_def and add the meta graph to builder, i got a new saved model, when i loading it:\r\n'''\r\nwith tf.compat.v1.Session(graph=tf.Graph()) as sess:\r\n\r\n    model = tf.compat.v1.saved_model.loader.load(sess, [\"eval\"], model_dir)\r\n\r\n'''\r\nit comes out :\r\nINFO: tensorflow : Saver not created because there are no variables in the graph to restore.\r\n\r\n**Describe the expected behavior**\r\ni also check the graph def after convert_variables_to_constants , i found the save/resore nodes all disappeared including the kept variables.\r\n\r\n", "comments": ["@ClarkChin08 \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ClarkChin08 \r\n\r\nWill it be possible to share the colab file or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "> @ClarkChin08\r\n> \r\n> Will it be possible to share the colab file or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\nsure, but how to give you the python script? i can also prepare env for you.", "https://colab.research.google.com/drive/1OWWn140wByBXfbWTeyaQCwZqQqvmNdNT?usp=sharing", "> @ClarkChin08\r\n> \r\n> Will it be possible to share the colab file or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\ncan you see the colab sharing?", "@ClarkChin08 \r\n\r\nRequest you to grant me the access for the colab link. Thanks!", "> @ClarkChin08\r\n> \r\n> Request you to grant me the access for the colab link. Thanks!\r\n\r\nhave given you the access, i have a quetsion:  can tensorflow freeze partially the graph and then save as saved model? that means we convert some node(like MatMul that we can do fusion after get the weights to constant) but keep other variables' save/restore op for saving? \r\n\r\ncan i get some show cases of this kind or where can i find the implementation in tensorflow code? ", "@ymodak do you have the plan when to finish this support? now we are pending on this important issue, thanks!", "Perhaps this [stackoverflow thread](https://stackoverflow.com/questions/46104525/how-to-restore-a-partial-graph-in-tensorflow) can point you to a right direction.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46521\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46521\">No</a>\n", "I couldn't make it work for converting some of the variables to constants using tf.graph_util.convert_variables_to_constants. As far as I understand it does the conversion properly, but messes up variable definitions in the graph, so converted graph can't be loaded after that.\r\nIt means that tf.graph_util.convert_variables_to_constants can only be used to convert all variables, and this doesn't work  when resulting protobuf is bigger than 2GB.\r\n\r\nAs an alternative it is possible to do a manual graph rewrite, see: https://github.com/hanxiao/bert-as-service/blob/master/server/bert_serving/server/graph.py or using https://github.com/CODAIT/graph_def_editor"]}, {"number": 46520, "title": "dso_loader.cc Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found", "body": "**System information**\r\n- Windows 10 - 20H2 Build 19042.746\r\n- TensorFlow installed from (source or binary): Nightly\r\n- TensorFlow version: \r\n- tensorboard==2.4.1\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow-estimator==2.4.0\r\ntensorflow-gpu==2.4.0\r\ntensorflow-hub==0.11.0\r\ntf-estimator-nightly==2.5.0.dev2021011401\r\ntf-nightly-gpu==2.5.0.dev20210114\r\n\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? conda - with pip installs on top\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.1/8.0.5.39 for CUDA 11.1\r\n- GPU model and memory: RTX 3090 - 24GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nRunning through a search for the best hyper params. This means I am looping through these actions in the same process - creating ANN(LSTM), compiling , training, validating an ANN, then changing the hypers and running again etc. This used to work on TF2.1 with my Super 2080, now I have changed to TF 2.5 as it supports the 3090. The system crashes on this line with no traceback, and no exception thrown.\r\n\r\ntraining_history = self.model.fit (ann_training_input_batched_scaled_3d, training_output,\r\n                                               epochs = regressor_number_epochs, batch_size = regressor_batch_size,\r\n                                               callbacks = callbks,\r\n                                               verbose=1,\r\n                                               validation_data = (ann_validation_input_batched_scaled_3d, valoutput))\r\n\r\nThis is the second iteration, so the second time this gets executed (but its with a new ANN so its the the first time with a new ANN, second time in the process). Note that if I use a CPU instead of a GPU I get the same errors about CUPTI, but it doesnt crash so quickly, it dos a few more iterations then crashes with no exception/logging etc.\r\n\r\nBefore it dies, I see these warnings and finally errors\r\n\r\nensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found\r\n2021-01-18 17:40:51.410137: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found\r\n2021-01-18 17:40:51.410283: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1644] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2021-01-18 17:40:51.410657: I tensorflow/core/profiler/lib/profiler_session.cc:158] Profiler session tear down.\r\n2021-01-18 17:40:51.410812: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1735] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nI dont understand how the process is dying, and I have the offending code wrapped in a try block, but no exception is being thrown. \r\n\r\nAlso how do I resolve this dll CUPTI warning ? It should be using cupti64_2020.2.1.dll. It works for the first iteration.\r\n\r\nMarcus", "comments": ["@marcusobrien,\r\nIssue [#43030](https://github.com/tensorflow/tensorflow/issues/43030) with a similar error log has already been addressed by a member of the TensorFlow team.\r\n\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/43030#issuecomment-736206189) and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46520\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46520\">No</a>\n"]}, {"number": 46519, "title": "Use flattened tflite namespace for lite/micro/kernels/cast.cc; clean-up", "body": "PR4 for issue #45608: Use flattened tflite namespace for CAST; clean-up the test code.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46517, "title": "Make tf.image.resize compatible with XLA compilation", "body": "Forward part of #46447. This PR supports combinations of `half_pixel_centers` and `align_corners`, which makes `tf.image.resize` compatible with XLA compilation.\r\nSpeedup at least 10x compared to original implementation (dilated conv) and reduce required space for intermediate output to O(output height * output width).\r\n\r\nI do not get a chance to test on local GPU, but I benchmark on colab with similar python operations with jit compilation.\r\n\r\n- nearest neighbor: https://colab.research.google.com/drive/1W6pFlFn1fVB3O885opiChMUd9C2Dn1cI?usp=sharing\r\n- bilinear: https://colab.research.google.com/drive/11ysUlrCXcaIV0fLGPUZbYUY2CNkRJOVi?usp=sharing\r\n\r\nAt least 10x speedup, and less memory requirement (change to larger size and CUDA will be OOM for original implementation).\r\n\r\nHi @hawkinsp, I see you're the author of original implementation from commit history. Could you review this when time allows? Thank you!", "comments": ["I'm no longer working on TF, so I'm not the best reviewer for this.\r\n\r\nHowever we did implement something similar in JAX (including cases this PR does not handle, e.g., bicubic resizes) a few months ago: https://cs.opensource.google/jax/jax/+/master:jax/_src/image/scale.py?q=jax%20resize&ss=jax%2Fjax:jax%2F \r\n\r\nIf you look in the source history you can see we previously used `gather` as this change does, but switched to using `einsum` because it performed better especially on TPU. I suspect which performs best really does depend on the hardware platform.", "@jpienaar Can you please review this PR ? Thanks!", "@jpienaar Can you please review this PR ? Thanks!", "@jpienaar Can you please review this PR ? Thanks!", "@smit-hinsu could you look at this one? Perhaps follow on what @hawkinsp remarked here and we may need a separate TPU kernel here, or gate one pattern when legalizing based on platform.", "@smit-hinsu Any update on this PR? Please. Thanks!", "@smit-hinsu Any update on this PR? Please. Thanks!", "Sorry for dropping the ball on this earlier.\r\n\r\nI couldn't get to this today. Will do the initial review in a day or two.", "@WindQAQ Could you confirm if you are still looking for merging this PR? It seems that you stopped contributing to TensorFlow.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "This change will be helpful any progress on this? Having similar issue with half_pixel_center on nnapi.", "I can double down on that, very useful when extracting features at different scales, and for now we cannot leverage XLA on these models. Can we reopen this PR?"]}, {"number": 46516, "title": "TensorFlow 2.4.0 core dumped issue on Ubuntu 16.04", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS\r\n- TensorFlow installed from (source or binary): installed from pip, with this version: tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl (394.7 MB)\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: No GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nAfter installation exactly following the official guideline at: https://www.tensorflow.org/install/pip\r\n\r\n(venv) $ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nIllegal instruction (core dumped)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nExactly the same as the guideline listed above, namely\r\n$ python3 -m venv --system-site-packages ./venv\r\n$ source ./venv/bin/activate  # sh, bash, or zsh\r\n(venv) $ pip install --upgrade pip\r\n(venv) $ pip install --upgrade tensorflow\r\n\r\n**Any other info / logs**\r\nI also did the installations with tensorflow==1.15 and tf-nightly. Both are working fine without the \"core dumped\" issue, however, I would really like to use the stable TensorFlow version 2.4.0.\r\n\r\nThank you very much for your time!\r\n", "comments": ["@wyqian1027 \r\n\r\ncould you please let us know the make and model of the CPU. \r\nCan you try installing in a fresh environment and install from scratch and see whether you are able to import Tensorflow to avoid and conflicts in dependencies. Thanks!", "I found the same error today:\r\n`illegal hardware instruction (core dumped)  python`\r\n\r\n**System information**\r\n- OS Platform and Distribution: `Ubuntu 20.04 LTS`\r\n- TensorFlow installed from: `pip`, in a clean conda env\r\n- TensorFlow version: `tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl` (394.7 MB)\r\n- Python version: `Python 3.6.12`\r\n- CUDA version: `cudatoolkit 10.2.89  build: hfd86e86_1`\r\n- cuDNN version: `7.6.5`\r\n- CPU: `Intel Xeon E5 v2680v2`\r\n- GPU model and memory: `Nvidia GTX 1080`\r\n- Terminal: `zsh` using `Oh My Zsh`\r\n\r\n**Describe the problem**\r\nWhet I try to import tensorflow in a python console from terminal I get:\r\n`illegal hardware instruction (core dumped)  python`\r\nI tried with Python `3.8.5`, both with `cudatoolkit=11.0` and without cudatoolkit installed, but the same error appears.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. conda create -n test python=3.6 pip\r\n2. conda activate test\r\n3. pip install tensorflow\r\n4. python\r\n5. import tensorflow as tf\r\n\r\nThen the error appears.", "It could be an issue on the conda build.", "Can you also try with the 2.4.1 patch?", "Upgrade to `Tensorflow 2.4.1` solved my issue. Thank you!", "Awesome, thanks for confirming. Looks like the issue was #45744 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46516\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46516\">No</a>\n"]}, {"number": 46514, "title": "Unable to selectively build TensorFlow Lite with Docker", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.1\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n\r\n**Describe the problem**\r\nUnable to build custom tflite AAR packages using Docker.\r\n\r\nMy goal is to build a custom AAR for Select Tensorflow Ops based on a tflite model containing some of the select tf ops. Using the AAR hosted at JCenter is not an option as it is too big for the project. I've tried both\r\n- Running the [script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/build_aar_with_docker.sh) that does building inside docker itself\r\n- Manually running the [script that builds the aars](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/build_aar.sh) from inside the docker\r\n\r\nBoth failed at exactly the same place during building `tensorflow-lite-select-tf-ops` stage with `ERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/aws/BUILD.bazel:12:11: C++ compilation of rule '@aws//:aws' failed (Exit 4)`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Built and ran the tflite-docker container following instructions from [here](https://www.tensorflow.org/lite/guide/build_android#set_up_build_environment_using_docker). First downloaded dockerfile from this place and then ran\r\n```\r\ndocker build . -t tflite-builder -f tflite-android.Dockerfile\r\ndocker run -it -v $PWD:/host_dir tflite-builder bash\r\nandroid update sdk --no-ui -a --filter tools,platform-tools,android-${ANDROID_API_LEVEL},build-tools-${ANDROID_BUILD_TOOLS_VERSION}\r\n\r\nsudo apt install curl gnupg\r\ncurl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor > bazel.gpg\r\nsudo mv bazel.gpg /etc/apt/trusted.gpg.d/\r\necho \"deb [arch=amd64] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\nsudo apt update && sudo apt install bazel\r\nlsb_release -a\r\nsudo apt install openjdk-11-jdk\r\n```\r\n2. From outside the docker ran the script that is supposed to build custom packages based on provided tflite models as per [instructions](https://www.tensorflow.org/lite/guide/reduce_binary_size#build_aar_files_for_android_project_2)\r\n\r\n```\r\ncurl -o build_aar_with_docker.sh \\\r\n  https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/tools/build_aar_with_docker.sh &&\r\nchmod +x build_aar_with_docker.sh\r\nsh build_aar_with_docker.sh \\\r\n  --input_models=$PWD/reduce_all_tf_ops.lite, \\\r\n  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --checkpoint=master\r\n```\r\n\r\nThe model reproducing the issue is attached, it's build from this function:\r\n```\r\ndef reduce_all(array):\r\n    return tf.cast(tf.reduce_all(tf.cast(x, tf.bool)), tf.int32)\r\n```\r\n[reduce_all_tf_ops.tflite.zip](https://github.com/tensorflow/tensorflow/files/5831835/reduce_all_tf_ops.tflite.zip)\r\n\r\nThe same failure happened with other models containing other different tf ops, so the issue doesn't seem to be related to the models used.\r\n\r\n**Failure logs**\r\n```\r\n...*GIT UPDATES*...\r\nYour branch is up to date with 'origin/master'.\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_API_LEVEL=18 --action_env ANDROID_BUILD_TOOLS_VERSION=28.0.0 --action_env ANDROID_SDK_API_LEVEL=23 --action_env ANDROID_SDK_HOME=/android/sdk --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /tensorflow_src/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /tensorflow_src/WORKSPACE:16:10: in <toplevel>\r\n  /tensorflow_src/tensorflow/workspace0.bzl:65:34: in workspace\r\n  /Users/irina/Feebris/tflite-docker/bazel-build-cache/cache/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /Users/irina/Feebris/tflite-docker/bazel-build-cache/cache/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tmp:tensorflow-lite (108 packages loaded, 8156 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tmp:tensorflow-lite up-to-date:\r\n  bazel-bin/tmp/tensorflow-lite.aar\r\nINFO: Elapsed time: 5603.110s, Critical Path: 382.75s\r\nINFO: 1724 processes: 214 internal, 1510 local.\r\nINFO: Build completed successfully, 1724 total actions\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_API_LEVEL=18 --action_env ANDROID_BUILD_TOOLS_VERSION=28.0.0 --action_env ANDROID_SDK_API_LEVEL=23 --action_env ANDROID_SDK_HOME=/android/sdk --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /tensorflow_src/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /tensorflow_src/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Build options --cxxopt, --define, and --fat_apk_cpu have changed, discarding analysis cache.\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /tensorflow_src/WORKSPACE:16:10: in <toplevel>\r\n  /tensorflow_src/tensorflow/workspace0.bzl:65:34: in workspace\r\n  /Users/irina/Feebris/tflite-docker/bazel-build-cache/cache/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /Users/irina/Feebris/tflite-docker/bazel-build-cache/cache/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/lite/tools:list_flex_ops_no_kernel_main (1 packages loaded, 781 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/tools:list_flex_ops_no_kernel_main up-to-date:\r\n  bazel-bin/tensorflow/lite/tools/list_flex_ops_no_kernel_main\r\nINFO: Elapsed time: 102.143s, Critical Path: 10.05s\r\nINFO: 79 processes: 12 internal, 67 local.\r\nINFO: Build completed successfully, 79 total actions\r\n/tensorflow_src/tmp /tensorflow_src\r\n/tensorflow_src\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env ANDROID_NDK_HOME=/android/ndk --action_env ANDROID_NDK_API_LEVEL=18 --action_env ANDROID_BUILD_TOOLS_VERSION=28.0.0 --action_env ANDROID_SDK_API_LEVEL=23 --action_env ANDROID_SDK_HOME=/android/sdk --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /tensorflow_src/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Build options --cxxopt, --define, and --fat_apk_cpu have changed, discarding analysis cache.\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /tensorflow_src/WORKSPACE:16:10: in <toplevel>\r\n  /tensorflow_src/tensorflow/workspace0.bzl:65:34: in workspace\r\n  /Users/irina/Feebris/tflite-docker/bazel-build-cache/cache/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /Users/irina/Feebris/tflite-docker/bazel-build-cache/cache/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tmp:tensorflow-lite-select-tf-ops (310 packages loaded, 34804 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /Users/irina/Feebris/tflite-docker/bazel-build-cache/cache/43801f1e35f242fb634ebbc6079cf6c5/external/aws/BUILD.bazel:12:11: C++ compilation of rule '@aws//:aws' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 93 argument(s) skipped)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tmp:tensorflow-lite-select-tf-ops failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 7490.271s, Critical Path: 79.89s\r\nINFO: 1549 processes: 23 internal, 1526 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@thaink could you take a look at this?", "Hi @irinakhismatullina.\r\nI just tried and those aar files are built successfully.\r\nI am not sure why you need so many commands in step 1, but by design, you just need to download the dockerfile and run `docker build . -t tflite-builder -f tflite-android.Dockerfile`.", "Hi @thaink, thanks for looking into it!\r\n\r\n1. At first I've done exactly that, docker build . -t tflite-builder -f tflite-android.Dockerfile and then running build_aar_with_docker.sh right after that. It lead to the same result. Only after that I've tried doing some of the set up manually in hope of fixing the problem. Any ideas why this C++ compilation of rule '@aws//:aws' failed might've happened?\r\n\r\n2. I've tried doing the same things on Ubuntu machine and it worked! So maybe the issue might have something to do with mac os as a host system?\r\n\r\n3. So I've managed to build the 2 aars (`tensorflow-lite.aar` and `tensorflow-lite-select-tf-ops.aar`) on Ubuntu, but when loading them as modules in my project in Android Studio I get \r\n```\r\nerror: cannot find symbol\r\nimport org.tensorflow.lite.Interpreter;\r\n                          ^\r\n  symbol:   class Interpreter\r\n  location: package org.tensorflow.lite\r\n```\r\nand when I look inside the tensorflow-lite.aar I see that there're no classes.\r\n<img width=\"1061\" alt=\"Screenshot 2021-01-20 at 13 56 11\" src=\"https://user-images.githubusercontent.com/40793894/105184458-44241680-5b27-11eb-8012-df8962e5ec81.png\">\r\n\r\nNot sure I'm doing everything right, but can it be something to do with loading the module, or it may be a build issue?", "@yyoon Do we support Android build in macOS?\r\nThanks for reporting the non-classes issue. I'll send a fix soon.", "@thaink That should be doable, I think. Especially when using Docker, it should be essentially the same as building things from Linux. Will try to reproduce this issue on my mac.", "@irinakhismatullina I just fixed the no-classes error at the master branch.", "I am also seeing this exact error when trying to build `summarize_graph`:\r\n\r\n```\r\ndocker run -it --rm tensorflow/tensorflow:latest-devel-gpu bash\r\nbazel build tensorflow/tools/graph_transforms:summarize_graph\r\n```\r\n\r\nI've tried tag: `latest-devel-gpu` and `devel` and both fail with:\r\n\r\n```\r\nroot@41a1272a823a:/tensorflow_src#  bazel build tensorflow/tools/graph_transforms:summarize_graph\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=238\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /tensorflow_src/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --//tensorflow/core/kernels/mlir_generated:enable_gpu=false --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file /tensorflow_src/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /tensorflow_src/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:linux in file /tensorflow_src/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /tensorflow_src/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /tensorflow_src/WORKSPACE:23:14: in <toplevel>\r\n  /tensorflow_src/tensorflow/workspace0.bzl:105:34: in workspace\r\n  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/graph_transforms:summarize_graph (139 packages loaded, 6329 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/aws/BUILD.bazel:12:11: C++ compilation of rule '@aws//:aws' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 92 argument(s) skipped)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/graph_transforms:summarize_graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 269.942s, Critical Path: 169.85s\r\nINFO: 2678 processes: 1357 internal, 1321 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nsame system info.", "@TylerLeonhardt At what commit/version did you build?", "I don't recall... It's whatever commit comes in the container. The repro steps are fairly simple if you have an Intel-based Mac. Were you able to repro?\n\nI've moved on to another project and won't be able to help further, I'm afraid.", "@irinakhismatullina \r\nCould you please try on latest stable version of tf and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46514\">No</a>\n"]}, {"number": 46513, "title": "Apple M1 `MLC` ops not supported (e.g., tf.MLCConv2D)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nmacOS 11.1 (Big Sur) running on a MacBook Pro (13-inch, M1, 2020)\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\nBinary, from https://github.com/apple/tensorflow_macos\r\n\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n'2.4.0-rc0'\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    tflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n/python/framework/func_graph.py:590:0: error: 'tf.MLCConv2D' op is neither a custom op nor a flex op\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nThere's a similar issue here:\r\nhttps://github.com/apple/tensorflow_macos/issues/133\r\n```\r\n\r\n**Failure details**\r\nConversion fails, since tf.MLCConv2D is not the same as tf.Conv2D.  The same is true for other operations like tf.MLCMatMul.\r\n\r\n**Any other info / logs**\r\nWhat's the right path forward here for using Lite, and then Lite Micro on an M1 Mac?  Should conversion from MLC be supported?  I attempted to work around this using `TF_DISABLE_MLC_EAGER` as mentioned at the very bottom of https://github.com/apple/tensorflow_macos but did not understand what syntax I should be using.", "comments": ["@mtamburro \r\n\r\nCan you try with below code and see if it helps you.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\r\nconverter.experimental_new_converter=True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_quant_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\nIf you are still facing the issue please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "> @mtamburro\r\n> \r\n> Can you try with below code and see if it helps you.\r\n> \r\n> ```\r\n> converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\r\n> converter.experimental_new_converter=True\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n>                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n> tflite_quant_model = converter.convert()\r\n> open(\"converted_model.tflite\", \"wb\").write(tflite_quant_model)\r\n> ```\r\n> \r\n> If you are still facing the issue please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\nHi @ravikyram--\r\n\r\nNo, this did not help--but would we expect lite to already have converter support for these `MLC` ops?\r\n\r\nTo reproduce the issue, you can even use the magic wand notebook found in the Tensorflow repository here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/magic_wand/train/train_magic_wand_model.ipynb\r\n\r\nWorth noting: I have steps similar to these later in my code, where I'm quantizing the model to run on microcontrollers that support only INT-8.  All of that works fine on my identically configured Intel MacBook Pro, but fails at this earlier step on the M1 MacBook Pro.\r\n\r\n", "The `MLC` operators are not a part of the official TensorFlow project. You need to enable them through user-defined TF Select option in the TFLite. @thaink could you help this?", "@mtamburro Where is the definition of MLC ops?", "Great question. Perhaps the authors of this blog post would know?\n\nhttps://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html?m=1\n\n\nOn Tue, Jan 19, 2021 at 2:02 AM Thai Nguyen <notifications@github.com>\nwrote:\n\n> @mtamburro <https://github.com/mtamburro> Where is the definition of MLC\n> ops?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/46513#issuecomment-762644454>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAB4V4Q6OU3ZKO5ZEU5CDUTS2UU6VANCNFSM4WHQFCZA>\n> .\n>\n", "If the TF version you use comes from the fork, the issue should be opened against the fork", "Thank you, Mihai.  I'l continue following up on that fork here: https://github.com/apple/tensorflow_macos/issues/133", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46513\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46513\">No</a>\n"]}, {"number": 46512, "title": "Unable to import tensorflow in python3 after wheels install / Python3.7 / RPi 3B+ Raspbian aarch64", "body": "\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: NO\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux raspberrypi 5.10.5-v8+ #1392 SMP PREEMPT Sat Jan 9 18:56:30 GMT 2021 aarch64 GNU/Linux\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: Raspberry Pi 3 B+\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: 2.4.0\r\n-   **Python version**: 3.7\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**: GCC 8.3.0\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**: \r\n        sudo -H pip3 install tensorflow-2.4.0-cp37-cp37m-linux_aarch64.whl\r\n        python3 -c \"import tensorflow as tf\"\r\n\r\n\r\n### Describe the problem\r\nunable to import tensorflow module from python3 (3.7) :\r\npi@raspberrypi:~ $ python3 -c \"import tensorflow as tf\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 23, in <module>\r\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB\\x87\\x01\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\\xf8\\x01\\x01\\x62\\x06proto3')\r\nTypeError: __init__() got an unexpected keyword argument 'serialized_options'\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\nHere is the output tf_env.txt generated by tf_env_collect.sh :\r\n\r\n== check python ===================================================\r\npython version: 3.7.3\r\npython branch: \r\npython build version: ('default', 'Jul 25 2020 13:03:44')\r\npython compiler version: GCC 8.3.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1392 SMP PREEMPT Sat Jan 9 18:56:30 GMT 2021\r\nos release version: 5.10.5-v8+\r\nos platform: Linux-5.10.5-v8+-aarch64-with-debian-10.7\r\nlinux distribution: ('debian', '10.7', '')\r\nlinux os distribution: ('debian', '10.7', '')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='raspberrypi', release='5.10.5-v8+', version='#1392 SMP PREEMPT Sat Jan 9 18:56:30 GMT 2021', machine='aarch64', processor='')\r\narchitecture: ('64bit', 'ELF')\r\nmachine: aarch64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 8.3.0-6) 8.3.0\r\nCopyright (C) 2018 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                  1.19.5\r\nprotobuf               3.0.0\r\ntensorflow             2.4.0\r\ntensorflow-estimator   2.4.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\n      7074:\tfind library=libcrypt.so.1 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libcrypt.so.1\r\n      7074:\t\r\n      7074:\tfind library=libpthread.so.0 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libpthread.so.0\r\n      7074:\t\r\n      7074:\tfind library=libdl.so.2 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libdl.so.2\r\n      7074:\t\r\n      7074:\tfind library=libutil.so.1 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libutil.so.1\r\n      7074:\t\r\n      7074:\tfind library=libexpat.so.1 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libexpat.so.1\r\n      7074:\t\r\n      7074:\tfind library=libz.so.1 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libz.so.1\r\n      7074:\t\r\n      7074:\tfind library=libm.so.6 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libm.so.6\r\n      7074:\t\r\n      7074:\tfind library=libc.so.6 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libc.so.6\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libpthread.so.0\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libc.so.6\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libm.so.6\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libz.so.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libexpat.so.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libutil.so.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libdl.so.2\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libcrypt.so.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tinitialize program: python3\r\n      7074:\t\r\n      7074:\t\r\n      7074:\ttransferring control: python3\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_opcode.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\tfind library=libffi.so.6 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libffi.so.6\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libffi.so.6\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\tfind library=libstdc++.so.6 [0]; searching\r\n      7074:\t search path=/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls/aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls:/usr/local/lib/python3.7/dist-packages/tensorflow/python/aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python:/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls/aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls:/usr/local/lib/python3.7/dist-packages/tensorflow/python/../aarch64:/usr/local/lib/python3.7/dist-packages/tensorflow/python/..\t\t(RUNPATH from file /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls/aarch64/libstdc++.so.6\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/tls/libstdc++.so.6\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/aarch64/libstdc++.so.6\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/libstdc++.so.6\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls/aarch64/libstdc++.so.6\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../tls/libstdc++.so.6\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../aarch64/libstdc++.so.6\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../libstdc++.so.6\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n      7074:\t\r\n      7074:\tfind library=librt.so.1 [0]; searching\r\n      7074:\t search path=/usr/local/lib/python3.7/dist-packages/tensorflow/python:/usr/local/lib/python3.7/dist-packages/tensorflow/python/..\t\t(RUNPATH from file /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/librt.so.1\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../librt.so.1\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/librt.so.1\r\n      7074:\t\r\n      7074:\tfind library=libgcc_s.so.1 [0]; searching\r\n      7074:\t search path=/usr/local/lib/python3.7/dist-packages/tensorflow/python:/usr/local/lib/python3.7/dist-packages/tensorflow/python/..\t\t(RUNPATH from file /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/libgcc_s.so.1\r\n      7074:\t  trying file=/usr/local/lib/python3.7/dist-packages/tensorflow/python/../libgcc_s.so.1\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libgcc_s.so.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libgcc_s.so.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/librt.so.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n      7074:\t\r\n      7074:\tfind library=libssl.so.1.1 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libssl.so.1.1\r\n      7074:\t\r\n      7074:\tfind library=libcrypto.so.1.1 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libcrypto.so.1.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libcrypto.so.1.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libssl.so.1.1\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_hashlib.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/termios.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_csv.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\tfind library=libopenblas.so.0 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libopenblas.so.0\r\n      7074:\t\r\n      7074:\tfind library=libgfortran.so.5 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libgfortran.so.5\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libgfortran.so.5\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libopenblas.so.0\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/linalg/lapack_lite.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/linalg/_umath_linalg.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\tfind library=libbz2.so.1.0 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/libbz2.so.1.0\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/libbz2.so.1.0\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_bz2.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\tfind library=liblzma.so.5 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/lib/aarch64-linux-gnu/liblzma.so.5\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /lib/aarch64-linux-gnu/liblzma.so.5\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_lzma.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\tfind library=libmpdec.so.2 [0]; searching\r\n      7074:\t search cache=/etc/ld.so.cache\r\n      7074:\t  trying file=/usr/lib/aarch64-linux-gnu/libmpdec.so.2\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/aarch64-linux-gnu/libmpdec.so.2\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/lib/python3.7/lib-dynload/_decimal.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/fft/_pocketfft_internal.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/mtrand.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/bit_generator.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_common.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_bounded_integers.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_mt19937.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_philox.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_pcg64.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_sfc64.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling init: /usr/local/lib/python3.7/dist-packages/numpy/random/_generator.cpython-37m-aarch64-linux-gnu.so\r\n      7074:\t\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 16, in <module>\r\n    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py\", line 23, in <module>\r\n    serialized_pb=_b('\\n,tensorflow/core/framework/tensor_shape.proto\\x12\\ntensorflow\\\"z\\n\\x10TensorShapeProto\\x12-\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32 .tensorflow.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB\\x87\\x01\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\\xf8\\x01\\x01\\x62\\x06proto3')\r\nTypeError: __init__() got an unexpected keyword argument 'serialized_options'\r\n      7074:\t\r\n      7074:\tcalling fini: python3 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libcrypt.so.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libutil.so.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libexpat.so.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_opcode.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libffi.so.6 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libstdc++.so.6 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/librt.so.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_hashlib.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libssl.so.1.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libcrypto.so.1.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libdl.so.2 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/termios.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_csv.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/linalg/lapack_lite.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/linalg/_umath_linalg.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libopenblas.so.0 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libgfortran.so.5 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libgcc_s.so.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libz.so.1 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_bz2.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libbz2.so.1.0 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_lzma.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/liblzma.so.5 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/python3.7/lib-dynload/_decimal.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/lib/aarch64-linux-gnu/libmpdec.so.2 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/fft/_pocketfft_internal.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/mtrand.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/bit_generator.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_common.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_bounded_integers.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_mt19937.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_philox.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_pcg64.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_sfc64.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /usr/local/lib/python3.7/dist-packages/numpy/random/_generator.cpython-37m-aarch64-linux-gnu.so [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libm.so.6 [0]\r\n      7074:\t\r\n      7074:\t\r\n      7074:\tcalling fini: /lib/aarch64-linux-gnu/libpthread.so.0 [0]\r\n      7074:\t\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 146: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.4.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python3.7/dist-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(2, 7, 16, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n", "comments": ["@dev-fr,\r\nCould you please download the TensorFlow Python package from [this link](https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-2.3.0rc2-cp35-none-linux_armv6l.whl) and check if you are able to install it?\r\n\r\nAlso, please go through [this guide](https://github.com/tensorflow/build/tree/master/raspberry_pi_builds#raspberry-pi-builds) for building TensorFlow for Raspberry Pi and check if it helps. Thanks!", "I can't use the wheels package link provided is for ARM 32bits v6. https://storage.googleapis.com/tensorflow/raspberrypi/tensorflow-2.3.0rc2-cp35-none-linux_armv6l.whl\r\nI use aarch64, 64 Bit Raspbian OS as mentioned in the title, and Rpi 3B+ is Armv7.\r\n\r\nIn order to get the wheels python tensorflow package for Python 3.7 / aarch64, I tried : \r\n - to build from source following the instruction from the guide : https://github.com/tensorflow/build/tree/master/raspberry_pi_builds#raspberry-pi-builds.\r\n - to install it from : pip3 install https://github.com/bitsy-ai/tensorflow-arm-bin/releases/download/v2.4.0-rc2/tensorflow-2.4.0rc2-cp37-none-linux_aarch64.whl\r\n\r\nOnce installed, I get the same error mentioned above after \"import tensorflow as tf\" from python3.\r\n", "Any idea ?", "Can you try upgrading protobuf version and check?\r\nSee https://github.com/tensorflow/models/issues/3995#issuecomment-400183986", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46512\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46512\">No</a>\n"]}, {"number": 46511, "title": "Tracing of tf function is stuck when the function contains custom CRF layer that wraps tfa crf functions inside", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Big Sur\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): from pypi\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen wrapping `crf` functions from `tensorflow_addons` into a custom Keras layer and then creating a training function with it that will be converted into `tf.function` with dynamic first dimension of the input shape, the process is stuck during tracing when trying to create gradient tensors. if I create `tf.function` with static shapes, it works.\r\n\r\nIf I use `tfa.text.crf.crf_decode` and `tfa.text.crf.crf_log_likelihood` directly without custom Keras layer wrapper, tracing `tf.function` seems to be working.\r\n\r\nThe problem appeared in version `2.4`, it is working in `2.3`\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\npotentials = tf.random.uniform((3,4,5))\r\nsequence_lengths = tf.ones((3,))\r\ntag_indices = tf.ones((3,4), dtype=tf.int32)\r\n\r\nclass CRF(tf.keras.layers.Layer):\r\n    def build(self, input_shape: tf.TensorShape) -> None:\r\n        self.transition_params = self.add_weight(\r\n            shape=(5, 5),\r\n            regularizer=tf.keras.regularizers.l2(0.1),\r\n            name=\"transitions\",\r\n        )\r\n        self.built = True\r\n    \r\n    def call(self, potentials, sequence_lengths):\r\n        x = tfa.text.crf.crf_decode(\r\n            potentials, self.transition_params, sequence_lengths\r\n        )\r\n        y = tfa.text.crf.crf_log_likelihood(\r\n            potentials, tag_indices, sequence_lengths, self.transition_params\r\n        )\r\n        return x, y\r\n\r\ncrf = CRF()\r\n\r\ndef foo(potentials, sequence_lengths):\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        x, y = crf(potentials, sequence_lengths)\r\n\r\n    return x, y\r\n\r\ntf_foo_static =  tf.function(foo)\r\ntf_foo_static(potentials, sequence_lengths)  # works fine\r\n\r\ntf_foo_dynamic = tf.function(foo, input_signature=[tf.TensorSpec((None, None, 5)), tf.TensorSpec((None,))])\r\ntf_foo_dynamic(potentials, sequence_lengths)  # the execution is stuck without raising any error\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWhen the process is terminated for `tfa.text.crf.crf_log_likelihood`, it looks like the gradient calculation through [`tf.scan` in `crf_forward`](https://github.com/tensorflow/addons/blob/6e7e06c56c2044e12a0d05d1381c6071defdf216/tensorflow_addons/text/crf.py#L366) is stuck, if we add `back_prop=False`, it manage to do the tracing, but it is a part of the loss function, so we need to backdrop the gradient. \r\n\r\nWhen the process is terminated for `tfa.text.crf.crf_decode`, it looks like the gradient calculation through [`tf.keras.layers.RNN` in `crf_decode_forward`](https://github.com/tensorflow/addons/blob/6e7e06c56c2044e12a0d05d1381c6071defdf216/tensorflow_addons/text/crf.py#L492) is stuck.\r\n", "comments": ["@Ghostvv \r\nI ran the code shared on tf 2.4 and nightly,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/c68946be60e2fad0eb1b06e0a173e775/untitled501.ipynb) and let us know.", "@Saduf2019 thank you very much for creating colab. Indeed `tf2.4` is stuck, while `tf2.5-nightly` seems to be working", "@Ghostvv \r\nCan you please upgrade and use nightly as the issue does not occur in nightly.", "I'm sorry we cannot use nightly releases. Unfortunately, as we ship `rasa` library, we have to use stable releases. Do you have a timeline when `2.5` might be released?", "Thanks for your issue. We are tracking this as `to-do` for upcoming TF 2.5 release. Will keep you posted as we know more.", "@ymodak do you have any more information on this?", "The original code snippet is fixed with latest release TF 2.5.\r\nSee [gist](https://colab.research.google.com/gist/ymodak/3ada8cf90a99cd743f5c28cb618439d9/47450.ipynb) to verify. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46511\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46511\">No</a>\n"]}, {"number": 46510, "title": "Add tf.io.encode_raw", "body": "This PR implemented `tf.io.encode_raw` as a counterpart for `tf.io.decode_raw` as requested in #46493.\r\n\r\nThe interface of `encode_raw` is simply:\r\n```python\r\ndef encode_raw(tensor, name=None):\r\n```\r\nAnd the function will return a Tensor with type `tf.string`. Because the `decode_raw` api already supports different host encoding method with `little_endian`, I think it's not necessary to have one for `encode_raw`. And it doesn't seem reasonable to have a `fixed_length` arg for `encode_raw` as well.\r\n\r\nTo make sure the `encode_raw` is the reverse operation of `decode_raw`, it will regard the first dimension of the input tensor as batch size and encode the input tensor into a batch of string tensor. In this way, we can do the following check:\r\n```python\r\nval == tf.io.decode_raw(tf.io.encode_raw(val), val.dtype)\r\n```\r\n(which is also how I implemented the test in `encode_raw_op_test.py`).\r\n\r\nThank you for your time on reviewing this PR :).\r\n", "comments": ["@mihaimaruseac \r\nThank you for your review. The new commit adds some lacking configs for this op and should help fix the build errors. Could you have a second look?", "@mihaimaruseac \r\nThe Linux GPU build error is from `ResizeBilinearTest` in `image_ops_test_gpu`, which should have nothing to do with `encode_raw`. To be more specific, the detailed error is:\r\n```\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0\r\n\t [[{{node ResizeBilinear}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0\r\n\t [[{{node ResizeBilinear}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[ResizeBilinear/_5]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n```\r\nAnd as for the Ubuntu CPU build error, it is from `ApiCompatibilityTest.testAPIBackwardsCompatibilityV1`. I should have fixed similar issues in the last commit with some addition in the following files:\r\n```\r\ntensorflow/core/api_def/base_api/api_def_EncodeRaw.pbtxt\r\ntensorflow/tools/api/golden/v1/tensorflow.io.pbtxt\r\ntensorflow/tools/api/golden/v1/tensorflow.pbtxt\r\ntensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt\r\ntensorflow/tools/api/golden/v2/tensorflow.io.pbtxt\r\ntensorflow/tools/api/golden/v2/tensorflow.pbtxt\r\ntensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt\r\n```\r\nI hope that you could tell me if there are any other config files I should modify to fix the current error. Thank you :).", "The API owners are moving to a rotation based system instead of the meetings and assigning several owners to follow along and review.   Leaving @jsimsa and adding one more.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "As the origin feature requesting user has not given any response for such a long time, I'll close this PR at the moment. Feel free to reopen if some other people have the same request.", "Hi @zhuzilin , I am very interest by the function tf$io$encode_raw and i can give you concrete a example.\r\nI work with geotiff (.tif) which are georeferenced image (satellite image with geolocation information) in my case  INT1U. The image is coded in bytes so that the first bytes contain information on the geolocation of the image and then comes the bytes of the image. So i read the image using tf$io$decode_raw(tf$io$read_file(input_img, out_type=tf$uint8)[first_img_bytes:last_img_bytes] and then run the model training on the tensor.  For prediction would like to save the image to produce a geotiff, that is, to concatenate the bytes of geolocation information to the resulting tensor, and doing something like tf$io$write_file(tf$io$encode_raw(my_output_img)) to save the image in geotiff directly from tensorflow. This would be of great use for the remote sensing community. Would it be possible to have this tf.encode_raw function ? thank you very much !"]}, {"number": 46509, "title": "TFLM: Fix GCC unsafe pointer conversions", "body": "Cast first to uintptr_t in Ethos-U kernel.\r\n\r\nThis is fixing: https://github.com/tensorflow/tensorflow/issues/46508", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46508, "title": "Unsafe conversion from pointer to uint64_t in Ethos-U kernel", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Ethos-U\r\n\r\n**Describe the problem**\r\nExample:\r\nreinterpret_cast<uint64_t>((void*)0x78000000)=0000000078000000\r\nreinterpret_cast<uint64_t>((void*)0x80000000)=ffffffff80000000\r\nreinterpret_cast<uint64_t>((void*)0x88000000)=ffffffff88000000\r\n\r\nThis happens specifically for GCC and prevents using addresses at 0x80000000 or above.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46508\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46508\">No</a>\n"]}, {"number": 46507, "title": "Instead of x2 variable name should be x0", "body": "https://www.tensorflow.org/guide/autodiff\r\nInstead of x2 variable name should be x0 in last line\r\n\r\nx0 = tf.Variable(3.0)\r\nx1 = tf.Variable(0.0)\r\n\r\nwith tf.GradientTape() as tape:\r\n  // Update x1 = x1 + x0.\r\n  x1.assign_add(x0)\r\n  // The tape starts recording from x1.\r\n  y = x1**2   # y = (x1 + x0)**2\r\n\r\n// This doesn't work.\r\nprint(tape.gradient(y, x0))   // dy/dx0 = 2*(x1 + x2)[](url)", "comments": ["@raushan291,\r\nThank you for reporting the issue. Would you be interested in submitting a PR for this?", "@raushan291,\r\nThe issue was fixed by [PR #1794](https://github.com/tensorflow/docs/pull/1794) and the changes are reflected in the [guide](https://www.tensorflow.org/guide/autodiff#4_took_gradients_through_a_stateful_object) as well. \r\n\r\nClosing this issue as it is fixed. Thanks!"]}, {"number": 46506, "title": "python3 pip Install Error: No matching distribution found for tensorflow==2.2.0", "body": "I have macOS Big Sur on a Apple Silicon M1 and I'm unable to install Tensorflow in python3.\r\nI removed xcode python3 and installed brew arm64 python3 (x86 python3 doesn't work as well)\r\n\r\n(This is  a follow up of closed https://github.com/tensorflow/tensorflow/issues/39130)\r\n\r\nI checked successful 64 bis version \r\n`python3 -c \"import sys; print(sys.version)\" or python -c \"import struct; print(struct.calcsize('P')*8)\"`\r\n\r\n> 3.8.7 (default, Dec 30 2020, 02:09:32) \r\n> [Clang 12.0.0 (clang-1200.0.32.28)]\r\n\r\n\r\n<img width=\"606\" alt=\"image\" src=\"https://user-images.githubusercontent.com/3314607/104881650-f6aa7c80-5961-11eb-8cbb-6128190a0a3a.png\">\r\n\r\nCan this work anyhow, or who knows, how to make this work ?\r\n\r\n", "comments": ["@hannesa2 \r\nPlease ensure you have 1) pip version >19.0 [as it is a tensorFlow 2 package requirement ]. Could you please upgrade pip using the below command and let us know if it works.\r\n\r\n```pip install --upgrade pip.```\r\n2) and  could you please check if you are using the 64 bit version of Python and you meet all the requirements in [this link](https://github.com/tensorflow/tensorflow/issues/42367#issuecomment-674531556)\r\n\r\n3)also check similar issues at #45360, #46377\r\n\r\nFor more information please check this [TensorFlow guide.](https://www.tensorflow.org/install#download-a-package)", "> Please ensure you have 1) pip version >19.0 [as it is a tensorFlow 2 package requirement ]. Could you please upgrade pip using the below command and let us know if it works.\r\n> `pip install --upgrade pip.`\r\n\r\nIt's already fulfilled\r\n\r\n```\r\npip install --upgrade pip\r\nRequirement already satisfied: pip in /Users/hannes/.local/share/virtualenvs/ML-jdJEXb4p/lib/python3.8/site-packages (20.3.3)\r\n```\r\n\r\n> 2) and could you please check if you are using the 64 bit version of Python and you meet all the requirements in [this link](https://github.com/tensorflow/tensorflow/issues/42367#issuecomment-674531556)\r\n\r\nIt's already fulfilled\r\n\r\n`python3 -c \"import sys; print(sys.version)\" or python -c \"import struct; print(struct.calcsize('P')*8)\"`\r\n`3.8.7 (default, Dec 30 2020, 02:09:32) [Clang 12.0.0 (clang-1200.0.32.28)]`\r\n \r\n> 3)also check similar issues at #45360, #46377\r\n\r\nThey are Linux related, I use macOS Big Sur Apple Silicon M1\r\n\r\n> For more information please check this [TensorFlow guide.](https://www.tensorflow.org/install#download-a-package)\r\n\r\nAfter stuck on this guide, I wrote this issue\r\n\r\n", "@hannesa2 \r\nplease upgrade to tf 2.4 as you are using python 3.8 as mentioned in the issues shared.", "for M1 you might want to use the Apple at fork https://github.com/apple/tensorflow_macos", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46506\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46506\">No</a>\n"]}, {"number": 46505, "title": "micro: copy operator FLOOR_DIV kernel from lite", "body": "This is a copy with minimal modification of the kernel and test for\r\noperator FLOOR_DIV from tensorflow/lite/kernels.\r\nAdaptations to micro and addition to the micro build to follow.\r\n\r\nPR step 3 for issue #45657\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46503, "title": "Can I use cmdline option `--allow_nudging_weights_to_use_fast_gemm_kernel` via python API?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source from git\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\n// input, output and weight settings above..\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, input_tensors, output_tensors)\r\n...\r\ntflite_model = converter.convert()\r\n```\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2021-01-05 08:02:27.757106: F tensorflow/lite/toco/graph_transformations/ensure_uint8_weights_safe_for_fast_int8_kernels.cc:190] Bad value for Conv2D_4/filter at index 134, previous bad value at index 132, distance=2, kMinDistanceBetweenBadValues=16. Consider passing --allow_nudging_weights_to_use_fast_gemm_kernel if you don't care about accuracy.\r\nFatal Python error: Aborted\r\n```\r\n\r\n**Failure details**\r\n- We randomly generate Fully Connected weights and convert the tf session to tflite using `toco` - python API. It seems that `toco` doesn't allow a certain type of weights with some zero values in the vicinity to other zero values.\r\n- The console output says I need to pass `--allow_nudging_weights_to_use_fast_gemm_kernel`, which is only useful for the command line. I cannot find a way to pass it via Python API.\r\n\r\nIs there any way that I can pass the `--allow_nudging_weights_to_use_fast_gemm_kernel` option via Python API?", "comments": ["The TOCO converter is deprecated. Please use MLIR converter instead. If you have any issues related to MLIR converter, please upload the problem in here.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you for the answer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46503\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46503\">No</a>\n"]}]