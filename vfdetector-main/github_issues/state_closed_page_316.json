[{"number": 44740, "title": "TFLu - Apollo3 - Fix Existing Issues", "body": "Addressing #44737\r\n\r\nThis PR is meant to fix existing issues with Apollo3 support in TensorFlow Lite Micro.\r\n\r\n* improves apollo3evb audio provider\r\n* silences compilation warning/error for sparkfun_edge hello_world\r\n* enables 96 MHz burst mode on apollo3evb to match sparkfun_edge performance", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "closing this pr per discussion in #44737"]}, {"number": 44739, "title": "Wrong path separator in error message when saved model does not exist (in keras.models.load_model)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home Single Language (build 19042.610)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2070 SUPER 8 GB\r\n\r\n**Current behavior**\r\nWhen I use function `keras.models.load_model` with incorrect path, I catch IOError:\r\n`OSError: SavedModel file does not exist at: C:\\IncorrectPathToSavedModel/{saved_model.pbtxt|saved_model.pb}`\r\nBecause I use Windows, path separator \"/\" is incorrect,  and when I view code of `parse_saved_model` in \"\\tensorflow\\python\\saved_model\\loader_impl.py\" I see hardcoded symbol in error text definition.\r\n\r\n**Expected behavior**\r\nPath separator should be recived from `os.path.sep`, for example in Windows this error message must be:\r\n`OSError: SavedModel file does not exist at: C:\\IncorrectPathToSavedModel\\{saved_model.pbtxt|saved_model.pb}`\r\nin Linux:\r\n`OSError: SavedModel file does not exist at: ~/IncorrectPathToSavedModel/{saved_model.pbtxt|saved_model.pb}`\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow import keras\r\nmodel_dir = 'C:\\\\IncorrectPathToSavedModel'\r\nmodel = keras.models.load_model(model_dir)\r\n```", "comments": ["Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/0dda7279b6a754877759f8721fb8bc9e/44739.ipynb). Thanks!", "@spopovru Looks like this was resolved by your PR. Can we close this issue? Thanks!\r\n\r\nError with `TF2.5`\r\n\r\n`OSError: SavedModel file does not exist at: C:\\IncorrectPathToSavedModel/{saved_model.pbtxt|saved_model.pb}`\r\n\r\nError with `tf-nightly`\r\n\r\n`OSError: No file or directory found at C:\\IncorrectPathToSavedModel`\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @spopovru Looks like this was resolved by your PR. Can we close this issue? Thanks!\r\n> \r\n> Error with `TF2.5`\r\n> \r\n> `OSError: SavedModel file does not exist at: C:\\IncorrectPathToSavedModel/{saved_model.pbtxt|saved_model.pb}`\r\n> \r\n> Error with `tf-nightly`\r\n> \r\n> `OSError: No file or directory found at C:\\IncorrectPathToSavedModel`\r\n\r\nYes, this issue may be closed.\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44739\">No</a>\n"]}, {"number": 44738, "title": "Fix and clean up how we run the bluepill renode tests.", "body": "Addresses http://b/146226672\r\n\r\n * With #44457, we were not checking for a specific string on the\r\n   UART, so even if a test failed, the test_bluepill_binary.sh script\r\n   would still report everything as passing. The current changes checks\r\n   for \"~~~ALL TESTS PASSED~~~\" on the UART.\r\n\r\n * Only run binaries with _test suffix as part of the test suite.\r\n\r\n * Use the Robot Framework variables mechanism for passing parameters\r\n   from the command line, instead of using environment variables.\r\n\r\n * Added some pointers into the Renode and Robot Framework\r\n   documentation.\r\n\r\n * We will do a more in-depth documentation in renode.md once we settle\r\n   on how we are going to use renode (currently in flux).\r\n\r\nTested that all bluepill tests pass.\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill build -j8\r\ntensorflow/lite/micro/testing/test_bluepill_binary.sh tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/\r\n```\r\n\r\nOutput:\r\n```\r\n...\r\n+++++ Finished test 'bluepill.Run All Bluepill Tests' in 25.37 seconds with status OK\r\n...\r\nTests finished successfully :)\r\nPASS\r\n```\r\n\r\nManually changed a test to fail and confirmed that the failure was properly detected and reported.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@JakubJatczak and @PiotrZierhoffer: once this PR is merged we should be ready to turn on Renode as part of CI for bluepill."]}, {"number": 44736, "title": "Validation step overwrites callback's internal predict call", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.1 LTS \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2080 Ti 11GB x8\r\n\r\nI'm training a network, and I'm trying to collect metrics on two separate validation sets. I've been using the built-in validation for one of them, and I wrote a callback for the other. However, in epochs when both run, the callback seems to always get the same answer as the built-in validation set.\r\n\r\nI have a minimal test case below. This is a completely trivial network where the output and input are equal. I've built the situation as follows:\r\n* Let x = np.ones(5,).\r\n* It trains on input = output = x, so the training error is always 0.\r\n* It validates on input = x, output = 2*x, so the validation error is always 1.\r\n* The callback validates on input = x, output = 3*x, so the callback's validation error should always be 2.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input\r\nfrom tensorflow.keras.models import Model\r\n\r\ninput_data = Input(shape=(5,))\r\nmodel = Model(inputs=input_data, outputs=input_data)\r\nmodel.compile(loss='mae')\r\n\r\nclass valCallback(tf.keras.callbacks.Callback):\r\n    def __init__(self, model, inputs, outputs):\r\n        self.model = model\r\n        self.inputs = inputs\r\n        self.outputs = outputs\r\n        \r\n    def on_epoch_end(self, epoch, logs={}):\r\n        val = self.model.evaluate(self.inputs, self.outputs, verbose=0)\r\n        print(\"\\nVAL: \", val)\r\n\r\ntraindata = np.ones((5))\r\nvc = valCallback(model, traindata, 3*traindata)  \r\nmodel.fit(traindata, traindata, \r\n          validation_data=(traindata, 2*traindata),\r\n          epochs=4, callbacks=[vc], validation_freq=2, verbose=0)\r\n```\r\n\r\nIn epoch 1, the callback correctly prints 2. In epoch 2, when the validation runs, the callback prints 1 instead (!).\r\n\r\nSomehow the validation_data is overwriting the data in the callback, and I don't know how or why. The self.model.evaluate calls in the callback aren't getting the right answer any more, after validation has happened.\r\n\r\nDesired output:\r\nVAL: 2\r\nVAL: 2\r\nVAL: 2\r\nVAL: 2\r\n\r\nActual output:\r\nVAL: 2\r\nVAL: 1\r\nVAL: 1\r\nVAL: 1", "comments": ["@creidieki \r\n\r\nI have tried in colab with TF version and was able to reproduce the issue. However issue got resolved in nightly version`(2.5.0-dev20201110`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a3dedb7ebfe9d30e668853a18a078726/untitled506.ipynb).Please,verify once and close the issue.Thanks!", "This works! Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44736\">No</a>\n"]}, {"number": 44735, "title": "[TFLite] Disable the unfolding of the BatchMatMulOp for 16-bit inference", "body": "Hello,\r\n\r\nThis PR disables the unfolding of the 16x16 BatchMatMulOp into a 16x16 MatMulOp operator which in turn would be transformed into an unsupported 16x16 FullyConnectedOp. The 16-bit FullyConnectedOp only supports 16-bit inputs and 8-bit weights (16x8) but the unfolding generates a FC with 16-bit inputs and 16-bit weights which result in the following error on inference:\r\n`RuntimeError: tensorflow/lite/kernels/fully_connected.cc:123 input->type != kTfLiteFloat32 (INT16 != FLOAT32)Node number 8 (FULLY_CONNECTED) failed to prepare.`\r\n\r\nThe PR solve part of the #44707 issue and uses the `toco_flags.inference_type()` set in [lite.py](https://github.com/tensorflow/tensorflow/blob/e9994aed883a12c5a7f77f2b3b6f59895d8faa07/tensorflow/lite/python/lite.py#L264) to check for int16 inference.\r\n\r\nCode to reproduce the problem this PR solves:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput_shape = [4, 2]\r\ninput1 = tf.keras.Input(shape=input_shape, batch_size=1)\r\ninput2 = tf.keras.Input(shape=input_shape, batch_size=1)\r\noutput = tf.keras.layers.Lambda(lambda x: tf.linalg.matmul(\r\n                                   x[0], x[1], transpose_b=True)\r\n                                )([input1, input2])\r\nmodel = tf.keras.Model(inputs=[input1, input2], outputs=output)\r\n\r\n\r\ndef get_rand_date():\r\n    return np.random.rand(1, *input_shape).astype(np.float32)\r\n\r\ndef representative_data_gen():\r\n    yield [get_rand_date(), get_rand_date()]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\ntflite_model = converter.convert()\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(interpreter.get_input_details()[0][\"index\"],\r\n                       get_rand_date())\r\ninterpreter.set_tensor(interpreter.get_input_details()[1][\"index\"],\r\n                       get_rand_date())\r\ninterpreter.invoke()\r\n```\r\n\r\nThanks,\r\nThibaut", "comments": ["Hi @Tessil Thanks for your contribution.\r\n\r\nI will take a look at the alternative of this solution and get back to you soon.\r\nI would take a look at the possibility that we can provide a solution in the op lowering part.\r\n\r\nBest regards,\r\nJaesung", "@Tessil Could you confirm that the following change can resolve your case?\r\n\r\nhttps://github.com/abattery/tensorflow/commit/5ae20e7d55dc4b9374edd02e88d991fa8a2381a5", "Nvm. The op lowering does not happens at the MLIR side. I will take a look at the optimization toolkit bit."]}, {"number": 44734, "title": "Dependencies for BERT-QA android not clear", "body": "**System information**\r\n- OS Platform and Distribution - (Linux Ubuntu 16.04)\r\n\r\nGradle and android SDK version not specified in the README of Android bert-qa for Question Answering\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android\r\n\r\nIt would be helpful if you could add those details.\r\n", "comments": ["Gradle 4.6 or higher.\r\nSDK Build Tools 28.0.3 or higher.\r\nhttps://developer.android.com/studio/releases/gradle-plugin#3-2-0", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44733, "title": "AttributeError: module 'tensorflow.compat.v2' has no attribute 'app'", "body": "Hi,\r\nFirst of all, I'm new to github and Iwasn't sure where to post this so sorry if it's in the wrong place. I'm trying to set up an object detector for my master's but I keep on running into issues.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow version: 2.3.0\r\n- CUDA: 10.1\r\n- cuDNN:  7.6.5\r\n\r\n\r\n\r\nI've tried many many different things but I keep on running into issues that prevent me from training a model, \"AttributeError: module 'tensorflow.compat.v2' has no attribute 'app'\" is the latest\r\nI have followed the guide which can be found on this github page: \r\nhttps://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model\r\nI have followed this pretty much to the letter aside from the fact that I didn't download the master branch but went with r2.3.0 instead to ensure compatibility.\r\nI ran the following command in anaconda to try and train the model:\r\n`python model_main_tf2.py --pipeline_config_path=training/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --model_dir=training --alsologtostderr`\r\n\r\nThis gave me the following error:\r\n![image](https://user-images.githubusercontent.com/74254802/98692645-a31f1f80-236f-11eb-8b7a-d74f0b876243.png)\r\n\r\nThis is the code of the model_main_tf2.py file:\r\n```\r\nfrom absl import flags\r\nimport tensorflow.compat.v2 as tf\r\nfrom object_detection import model_hparams\r\nfrom object_detection import model_lib_v2\r\n\r\nflags.DEFINE_string('pipeline_config_path', None, 'Path to pipeline config '\r\n                    'file.')\r\nflags.DEFINE_integer('num_train_steps', None, 'Number of train steps.')\r\nflags.DEFINE_bool('eval_on_train_data', False, 'Enable evaluating on train '\r\n                  'data (only supported in distributed training).')\r\nflags.DEFINE_integer('sample_1_of_n_eval_examples', None, 'Will sample one of '\r\n                     'every n eval input examples, where n is provided.')\r\nflags.DEFINE_integer('sample_1_of_n_eval_on_train_examples', 5, 'Will sample '\r\n                     'one of every n train input examples for evaluation, '\r\n                     'where n is provided. This is only used if '\r\n                     '`eval_training_data` is True.')\r\nflags.DEFINE_string(\r\n    'hparams_overrides', None, 'Hyperparameter overrides, '\r\n    'represented as a string containing comma-separated '\r\n    'hparam_name=value pairs.')\r\nflags.DEFINE_string(\r\n    'model_dir', None, 'Path to output model directory '\r\n                       'where event and checkpoint files will be written.')\r\nflags.DEFINE_string(\r\n    'checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '\r\n    '`checkpoint_dir` is provided, this binary operates in eval-only mode, '\r\n    'writing resulting metrics to `model_dir`.')\r\n\r\nflags.DEFINE_integer('eval_timeout', 3600, 'Number of seconds to wait for an'\r\n                     'evaluation checkpoint before exiting.')\r\nflags.DEFINE_integer(\r\n    'num_workers', 1, 'When num_workers > 1, training uses '\r\n    'MultiWorkerMirroredStrategy. When num_workers = 1 it uses '\r\n    'MirroredStrategy.')\r\n\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef main(unused_argv):\r\n  flags.mark_flag_as_required('model_dir')\r\n  flags.mark_flag_as_required('pipeline_config_path')\r\n  tf.config.set_soft_device_placement(True)\r\n\r\n  if FLAGS.checkpoint_dir:\r\n    model_lib_v2.eval_continuously(\r\n        hparams=model_hparams.create_hparams(FLAGS.hparams_overrides),\r\n        pipeline_config_path=FLAGS.pipeline_config_path,\r\n        model_dir=FLAGS.model_dir,\r\n        train_steps=FLAGS.num_train_steps,\r\n        sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\r\n        sample_1_of_n_eval_on_train_examples=(\r\n            FLAGS.sample_1_of_n_eval_on_train_examples),\r\n        checkpoint_dir=FLAGS.checkpoint_dir,\r\n        wait_interval=300, timeout=FLAGS.eval_timeout)\r\n  else:\r\n    if tf.config.get_visible_devices('TPU'):\r\n      resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n      tf.config.experimental_connect_to_cluster(resolver)\r\n      tf.tpu.experimental.initialize_tpu_system(resolver)\r\n      strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n    elif FLAGS.num_workers > 1:\r\n      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    else:\r\n      strategy = tf.compat.v2.distribute.MirroredStrategy()\r\n\r\n    with strategy.scope():\r\n      model_lib_v2.train_loop(\r\n          hparams=model_hparams.create_hparams(FLAGS.hparams_overrides),\r\n          pipeline_config_path=FLAGS.pipeline_config_path,\r\n          model_dir=FLAGS.model_dir,\r\n          train_steps=FLAGS.num_train_steps,\r\n          use_tpu=FLAGS.use_tpu)\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\n\r\nI did a quick search and I found that tf.app has been removed from TF2, yet it is still used in TF2 files?\r\nIs there an easy workaround to this? I can't seem to get it to work, but that might just be because I'm not exactly a good programmer.\r\n\r\nThanks.\r\n\r\n", "comments": ["@007Nick700 \r\nCould you please refer to this comment of similar [resolved issue](https://github.com/tensorflow/tensorflow/issues/37128#issuecomment-600352999) and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44733\">No</a>\n"]}, {"number": 44732, "title": "[Dockerfile] move stubs to the end of LD_LIBRARY_PATH", "body": "Fix #36974 and maybe #38205 (not tested).\r\nI hit #36974 again today.\r\n\r\nSee also https://github.com/tensorflow/tensorflow/issues/38205#issuecomment-637506594 and https://github.com/tensorflow/tensorflow/issues/38205#issuecomment-637505286\r\n\r\nMaybe we could remove the stubs. But I'm not able to test if it would broke something. So I went with the safer fix.\r\n\r\n@r4nt @angerson ", "comments": ["Thanks! Can you follow the instructions in the dockerfiles/ directory README to update all the files, please?", "I amended the commit to include the change in the source file.\r\n\r\nBut I'm not able to regenerates the Dockerfiles automatically. I get errors if I try in my own shell or if I follow the instruction. Here is the error I have:\r\n```> Error: _TAG_PREFIX is not a valid slice_set, and also isn't an arg provided on the command line. If it is an arg, please specify it with --arg. If not, check the slice_sets list.```\r\n\r\nI do not plan to debug this error. I think my manual edit of the source and destination of the Dockerfile should be enough for this issue.", "Ok, I'll have to do it myself if you're unable to -- but it will take more time. Thanks for your work!", "@angerson Is there a chance that this get finished this mount?", "@sanjoy ", "> But I'm not able to regenerates the Dockerfiles automatically. I get errors if I try in my own shell or if I follow the instruction. Here is the error I have:\r\n\r\nCan you share full logs for this error?  In theory the regeneration tool should work fine in open source so I want to understand why it isn't working for you.\r\n\r\nI'll separately check in an auto-generated version of this PR to unblock you as discussed in the sync today.", "Created https://github.com/tensorflow/tensorflow/pull/46190.", "Read and tried again the instruction at: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/README.md#contributing\r\n\r\nThose instruction aren't very clear. By the first read, I think I must execute most of those command inside the container started in the 2nd command `docker run --user $(id -u):$(id -g) -it -v $(pwd):/tf tf-tools bash`. After reading it completly, I suppose it isn't what is intended.\r\n\r\nHere is the command to execute from the directory `tensorflow/tools/dockerfiles` to regenerates the Dockerfiles:\r\n\r\n```\r\ndocker build -t tf-tools -f tools.Dockerfile .\r\nalias asm_dockerfiles=\"docker run --rm -u $(id -u):$(id -g) -v $(pwd):/tf tf-tools python3 assembler.py \"\r\nasm_dockerfiles --release dockerfiles --construct_dockerfiles\r\n```\r\n\r\nHere is the error I have:\r\n\r\n```\r\n> Skipping release nightly\r\n> Skipping release nightly\r\n> Skipping release versioned\r\n> Emptying Dockerfile dir \"./dockerfiles\"\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 706, in urlopen\r\n    chunked=chunked,\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 394, in _make_request\r\n    conn.request(method, url, **httplib_request_kw)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1151, in request\r\n    self._send_request(method, url, body, headers)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1196, in _send_request\r\n    self.endheaders(body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1147, in endheaders\r\n    self._send_output(message_body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 950, in _send_output\r\n    self.send(msg)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 893, in send\r\n    self.connect()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/transport/unixconn.py\", line 43, in connect\r\n    sock.connect(self.unix_socket)\r\nFileNotFoundError: [Errno 2] No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/adapters.py\", line 449, in send\r\n    timeout=timeout\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 756, in urlopen\r\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/util/retry.py\", line 531, in increment\r\n    raise six.reraise(type(error), error, _stacktrace)\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/packages/six.py\", line 734, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 706, in urlopen\r\n    chunked=chunked,\r\n  File \"/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py\", line 394, in _make_request\r\n    conn.request(method, url, **httplib_request_kw)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1151, in request\r\n    self._send_request(method, url, body, headers)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1196, in _send_request\r\n    self.endheaders(body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 1147, in endheaders\r\n    self._send_output(message_body)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 950, in _send_output\r\n    self.send(msg)\r\n  File \"/usr/lib/python3.5/http/client.py\", line 893, in send\r\n    self.connect()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/transport/unixconn.py\", line 43, in connect\r\n    sock.connect(self.unix_socket)\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 214, in _retrieve_server_version\r\n    return self.version(api_version=False)[\"ApiVersion\"]\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/daemon.py\", line 181, in version\r\n    return self._result(self._get(url), json=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/utils/decorators.py\", line 46, in inner\r\n    return f(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 237, in _get\r\n    return self.get(url, **self._set_request_timeout(kwargs))\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/sessions.py\", line 555, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/sessions.py\", line 542, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/sessions.py\", line 655, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/requests/adapters.py\", line 498, in send\r\n    raise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"assembler.py\", line 719, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.5/dist-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"assembler.py\", line 504, in main\r\n    dock = docker.from_env()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/client.py\", line 101, in from_env\r\n    **kwargs_from_env(**kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/client.py\", line 45, in __init__\r\n    self.api = APIClient(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 197, in __init__\r\n    self._version = self._retrieve_server_version()\r\n  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 222, in _retrieve_server_version\r\n    'Error while fetching server API version: {0}'.format(e)\r\ndocker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\r\n```", "Thanks @nouiz!  @angerson any idea what's going on in the logs?", "Thanks for the fix, Sanjoy! I'm not sure what's wrong with the logs. This looks bizarre.\r\n\r\nPer https://github.com/tensorflow/tensorflow/issues/46062, ideally we'll deprecate this assembler system and avoid the problem entirely.\r\n\r\n"]}, {"number": 44731, "title": "Fix tpu_executor_dlsym_initializer windows build error", "body": "This PR enables me build jax natively on windows, https://github.com/google/jax/pull/4843", "comments": []}, {"number": 44730, "title": "Bug/Issue tf.data.Dataset for Keras multi-input model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nI built a multi-input model with the Keras functional API. The idea is to classify a text and its metadata. The model works fine with NumPy format inputs but fails with a tf.data.Dataset. I'm not sure how to interpret it as both inputs should be equivalent. Thanks in advance for any guidance. I attached below a dummy equivalent of my project.\r\n\r\n**Standalone code to reproduce the issue**\r\n**Model:**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras import Input, Model, layers\r\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\r\n\r\n\r\nMAX_LEN = 20\r\n\r\nSTRING_CATEGORICAL_COLUMNS = [\r\n    \"Organization\",\r\n    \"Sector\",\r\n    \"Content_type\",\r\n    \"Geography\",\r\n    \"Themes\",\r\n]\r\n\r\nVOCAB = {\r\n    \"Organization\": [\"BNS\", \"FED\", \"ECB\"],\r\n    \"Sector\": [\"BANK\", \"ASS\", \"MARKET\"],\r\n    \"Content_type\": [\"LAW\", \"NOTES\", \"PAPER\"],\r\n    \"Geography\": [\"UK\", \"FR\", \"DE\", \"CH\", \"US\", \"ES\", \"NA\"],\r\n    \"Themes\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"],\r\n}\r\n\r\nDIM = {\r\n    \"Organization\": 7,\r\n    \"Sector\": 2,\r\n    \"Content_type\": 3,\r\n    \"Geography\": 4,\r\n    \"Themes\": 5,\r\n}\r\n\r\n\r\n# BERT branch\r\ntf_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\", name=\"tfbert\")\r\n\r\ninput_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\r\nattention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\r\n\r\n\r\nembedding = tf_model(input_ids, attention_mask=attention_mask)[0][:, 0]\r\n\r\nbert_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\r\nmodel_bert = Model(inputs=[bert_input], outputs=[embedding])\r\n\r\n\r\n# meta branch\r\nmeta_inputs = {}\r\nmeta_prepocs = []\r\n\r\nfor key in VOCAB:\r\n    inputs = Input(shape=(None,), dtype=tf.string, name=key)\r\n    meta_inputs[key] = inputs\r\n\r\n    vocab_list = VOCAB[key]\r\n    vocab_size = len(vocab_list)\r\n    embed_dim = DIM[key]\r\n\r\n    x = layers.experimental.preprocessing.StringLookup(\r\n        vocabulary=vocab_list, num_oov_indices=1, mask_token=\"PAD\", name=\"lookup_\" + key\r\n    )(inputs)\r\n\r\n    x = layers.Embedding(\r\n        input_dim=vocab_size + 2,  # 2 = PAD + NA\r\n        output_dim=embed_dim,\r\n        mask_zero=True,\r\n        name=\"embedding_\" + key,\r\n    )(x)\r\n\r\n    x = layers.GlobalAveragePooling1D(\r\n        data_format=\"channels_last\", name=\"poolembedding_\" + key\r\n    )(x)\r\n\r\n    meta_prepocs.append(x)\r\n\r\nmeta_output = layers.concatenate(meta_prepocs, name=\"concatenate_meta\")\r\nmodel_meta = Model(meta_inputs, meta_output)\r\n\r\n\r\n# combining branches\r\ncombined = layers.concatenate(\r\n    [model_bert.output, model_meta.output], name=\"concatenate_all\"\r\n)\r\nouput = layers.Dense(128, activation=\"relu\", name=\"dense\")(combined)\r\nouput = layers.Dense(4, name=\"class_output\")(ouput)\r\nmodel = Model(inputs=[model_bert.input, model_meta.input], outputs=ouput)\r\n\r\nmodel.compile(\r\n    optimizer=keras.optimizers.RMSprop(1e-3),\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n)\r\n```\r\n\r\n**Dataset**\r\nA dummy dataset of 5 texts and respective metadata\r\n\r\n```\r\n# input meta\r\ndict_meta = {\r\n    \"Organization\": [\r\n        [\"BNS\", \"NA\"],\r\n        [\"ECB\", \"PAD\"],\r\n        [\"NA\", \"PAD\"],\r\n        [\"NA\", \"PAD\"],\r\n        [\"NA\", \"PAD\"],\r\n    ],\r\n    \"Sector\": [\r\n        [\"BANK\", \"PAD\", \"PAD\"],\r\n        [\"ASS\", \"PAD\", \"NA\"],\r\n        [\"MARKET\", \"NA\", \"NA\"],\r\n        [\"NA\", \"PAD\", \"NA\"],\r\n        [\"NA\", \"PAD\", \"NA\"],\r\n    ],\r\n    \"Content_type\": [\r\n        [\"NOTES\", \"PAD\"],\r\n        [\"PAPER\", \"UNK\"],\r\n        [\"LAW\", \"PAD\"],\r\n        [\"LAW\", \"PAD\"],\r\n        [\"LAW\", \"NOTES\"],\r\n    ],\r\n    \"Geography\": [\r\n        [\"UK\", \"FR\"],\r\n        [\"DE\", \"CH\"],\r\n        [\"US\", \"ES\"],\r\n        [\"ES\", \"PAD\"],\r\n        [\"NA\", \"PAD\"],\r\n    ],\r\n    \"Themes\": [[\"A\", \"B\"], [\"B\", \"C\"], [\"C\", \"PAD\"], [\"C\", \"PAD\"], [\"G\", \"PAD\"]],\r\n}\r\n\r\n# input text\r\nlist_text = [\r\n    \"Trump in denial over election defeat as Biden gears up to fight Covid\",\r\n    \"Feds seize $1 billion in bitcoins they say were stolen from Silk Road\",\r\n    \"Kevin de Bruyne misses penalty as Manchester City and Liverpool draw\",\r\n    \"United States nears 10 million coronavirus cases\",\r\n    \"Fiji resort offers the ultimate in social distancing\",\r\n]\r\n\r\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\r\nparams = {\r\n    \"max_length\": MAX_LEN,\r\n    \"padding\": \"max_length\",\r\n    \"truncation\": True,\r\n}\r\ntokenized = tokenizer(list_text, **params)\r\ndict_text = tokenized.data\r\n\r\n#input label\r\nlabel = [[1], [0], [1], [0], [1]]\r\n```\r\n\r\n**Training with NumPy format**\r\n\r\n```\r\nds_meta = tf.data.Dataset.from_tensor_slices((dict_meta))\r\nds_meta = ds_meta.batch(5)\r\nexample_meta = next(iter(ds_meta))\r\n\r\nds_text = tf.data.Dataset.from_tensor_slices((dict_text))\r\nds_text = ds_text.batch(5)\r\nexample_text = next(iter(ds_text))\r\n\r\nds_label = tf.data.Dataset.from_tensor_slices((label))\r\nds_label = ds_label.batch(5)\r\nexample_label = next(iter(ds_label))\r\n\r\nmodel.fit([example_text, example_meta], example_label)\r\n```\r\n\r\n```\r\n1/1 [==============================] - 0s 1ms/step - loss: 2.4866\r\n```\r\n\r\n**Training with tf.data.Dataset**\r\n\r\n```\r\nds = tf.data.Dataset.from_tensor_slices(\r\n    (\r\n        {\r\n            \"attention_mask\": dict_text[\"attention_mask\"],\r\n            \"input_ids\": dict_text[\"input_ids\"],\r\n            \"Content_type\": dict_meta[\"Organization\"],\r\n            \"Geography\": dict_meta[\"Geography\"],\r\n            \"Organization\": dict_meta[\"Organization\"],\r\n            \"Sector\": dict_meta[\"Sector\"],\r\n            \"Themes\": dict_meta[\"Themes\"],\r\n        },\r\n        {\"class_output\": label},\r\n    )\r\n)\r\n\r\n\r\nds = ds.batch(5)\r\nmodel.fit(ds, epochs=1)\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n2020-11-10 14:52:47.502445: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at cast_op.cc:124 : Unimplemented: Cast string to int32 is not supported\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-10-a894466398cd>\", line 1, in <module>\r\n    model.fit(ds, epochs=1)\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 807, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n\r\n  File \"/opt/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\n\r\nUnimplementedError:  Cast string to int32 is not supported\r\n\t [[node functional_5/Cast (defined at <ipython-input-3-8e2b230c1da3>:17) ]] [Op:__inference_train_function_24120]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "comments": ["I have tried in colab with TF version 2.3 [gist](https://colab.research.google.com/gist/ravikyram/40cd7f4575bfcbc6acb4d185f40f6e37/untitled507.ipynb) and nightly version(`2.5.0-dev20201110`) [gist](https://colab.research.google.com/gist/ravikyram/7a0cd68016ff9ba86885f58fcf0ceb99/untitled508.ipynb) and was able to reproduce the issue. Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ca1782bad356947fa26afea2e1befa9b/44730.ipynb). Thanks!", "You are defining your model wrong.  \r\nDo this instead.\r\n\r\n`model = Model(inputs=model_bert.inputs + model_meta.inputs, outputs=ouput)`\r\n\r\nYou have \r\n\r\n`model = Model(inputs=[model_bert.input, model_meta.input], outputs=ouput)`\r\n\r\nand I don't know what that does.", "@lschneidpro With the modification suggestion by @grofte , it works on my local without any issue. \r\n\r\nHowever, you need to update code by looking at the warnings as shown in the following trace.\r\n\r\n```\r\n2021-07-02 16:22:41.037332: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\r\n2021-07-02 16:22:41.037389: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\r\n2021-07-02 16:22:41.037397: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\r\n2021-07-02 16:22:41.037405: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\r\n2.6.0-dev20210623\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 442/442 [00:00<00:00, 120kB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 363M/363M [00:30<00:00, 11.9MB/s]\r\n2021-07-02 16:23:18.234832: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-02 16:23:18.287024: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nSome layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\r\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\r\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\r\nWARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\r\nWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\r\nWARNING:tensorflow:From /Users/vishnuvardhanj/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5059: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00<00:00, 1.86MB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.0/28.0 [00:00<00:00, 7.84kB/s]\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 466k/466k [00:00<00:00, 2.72MB/s]\r\n2021-07-02 16:23:27.890619: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nWARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\r\nWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\r\nWARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\r\nWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\r\n1/1 [==============================] - 9s 9s/step - loss: 1.7420\r\nWARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\r\nWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\r\n1/1 [==============================] - 6s 6s/step - loss: 1.1598\r\n```\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44730\">No</a>\n"]}, {"number": 44729, "title": "Change TFLu Renode tests flow", "body": "With this changes :\r\n* robot file is generated in `${TEST_TMPDIR}`,\r\n* all console output is handled by robot,\r\n* all tests are executed even if one of them fails.\r\n\r\n\r\nExample output after running `tensorflow/lite/micro/testing/test_bluepill_binary.sh tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin:\r\n\r\n* In case of pass\r\n```\r\nPreparing suites\r\nStarted Renode instance on port 9999; pid 101044\r\nStarting suites\r\nRunning /tmp/test_bluepill_binary//Bluepill.robot\r\n+++++ Starting test 'Bluepill.Should Create Platform'\r\n+++++ Finished test 'Bluepill.Should Create Platform' in 1.96 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_activations_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_activations_test' in 0.68 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_add_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_add_test' in 0.47 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_arg_min_max_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_arg_min_max_test' in 0.47 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_ceil_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_ceil_test' in 0.42 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_comparisons_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_comparisons_test' in 0.49 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_concatenation_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_concatenation_test' in 0.47 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_conv_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_conv_test' in 0.45 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_depthwise_conv_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_depthwise_conv_test' in 0.42 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_dequantize_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_dequantize_test' in 0.44 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_elementwise_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_elementwise_test' in 0.56 seconds with status OK\r\nCleaning up suites\r\nClosing Renode pid 101044\r\nAggregating all robot results\r\nOutput:  /tmp/test_bluepill_binary/robot_output.xml\r\nLog:     /tmp/test_bluepill_binary/log.html\r\nReport:  /tmp/test_bluepill_binary/report.html\r\nTests finished successfully :)\r\n```\r\n* In case of fail\r\n```\r\nPreparing suites\r\nStarted Renode instance on port 9999; pid 99620\r\nStarting suites\r\nRunning /tmp/test_bluepill_binary//Bluepill.robot\r\n+++++ Starting test 'Bluepill.Should Create Platform'\r\n+++++ Finished test 'Bluepill.Should Create Platform' in 2.30 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_activations_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_activations_test' in 0.73 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_add_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_add_test' in 4.74 seconds with status failed\r\n      \u2554\u2550\r\n      \u2551 UART OUTPUT:\r\n      \u2551 \r\n      \u2551 Testing FloatAddNoActivation\r\n      \u2551 golden[i] (-1.9249991*2^2) near output[i] (-1.8999992*2^0) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\n      \u2551 golden[i] (1.9249991*2^2) near output[i] (1.5999993*2^-2) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\n      \u2551 golden[i] (1.9249991*2^2) near output[i] (1.0*2^0) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\n      \u2551 golden[i] (1.9249991*2^2) near output[i] (1.2999993*2^0) failed at tensorflow/lite/micro/kernels/add_test.cc:87\r\n      \u2551 Testing FloatAddActivationRelu1\r\n      \u2551 Testing FloatAddVariousInputShapes\r\n      \u2551 Testing FloatAddWithScalarBroadcast\r\n      \u2551 Testing QuantizedAddNoActivationUint8\r\n      \u2551 Testing QuantizedAddNoActivationInt8\r\n      \u2551 Testing QuantizedAddActivationRelu1Uint8\r\n      \u2551 Testing QuantizedAddActivationRelu1Int8\r\n      \u2551 Testing QuantizedAddVariousInputShapesUint8\r\n      \u2551 Testing QuantizedAddVariousInputShapesInt8\r\n      \u2551 Testing QuantizedAddWithScalarBroadcastUint8\r\n      \u2551 Testing QuantizedAddWithScalarBroadcastFloat\r\n      \u2551 Testing QuantizedAddWithScalarBroadcastInt8\r\n      \u2551 Testing QuantizedAddWithMixedBroadcastUint8\r\n      \u2551 Testing QuantizedAddWithMixedBroadcastInt8\r\n      \u2551 14/15 tests passed\r\n      \u2551 ~~~SOME TESTS FAILED~~~\r\n      \u2551 \r\n      \u2551 \r\n      \u255a\u2550\r\n+++++ Starting test 'Bluepill.Should Run kernel_arg_min_max_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_arg_min_max_test' in 0.49 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_ceil_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_ceil_test' in 0.46 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_comparisons_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_comparisons_test' in 0.45 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_concatenation_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_concatenation_test' in 0.43 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_conv_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_conv_test' in 0.55 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_depthwise_conv_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_depthwise_conv_test' in 0.47 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_dequantize_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_dequantize_test' in 0.46 seconds with status OK\r\n+++++ Starting test 'Bluepill.Should Run kernel_elementwise_test'\r\n+++++ Finished test 'Bluepill.Should Run kernel_elementwise_test' in 0.43 seconds with status OK\r\nCleaning up suites\r\nClosing Renode pid 99620\r\nAggregating all robot results\r\nOutput:  /tmp/test_bluepill_binary/robot_output.xml\r\nLog:     /tmp/test_bluepill_binary/log.html\r\nReport:  /tmp/test_bluepill_binary/report.html\r\nSome tests failed :( See logs for details!\r\n```", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain  I merged this branch with master and resolved all conflicts.", "https://github.com/tensorflow/tensorflow/pull/45761 adds an exception to the newly added license checks for the .resource file."]}, {"number": 44728, "title": "Keras/Tensorflow train_on_batch memory leak", "body": "Please do help\r\n\r\ntrain_on_batch has a memory leak in my and many other people's code. It was thought to only be on predict and fit, which was solved with a del and gc.collect(), but train_on_batch cannot be solved by this. Please someone help!\r\n\r\n", "comments": ["@Emile0205 \r\nPlease share a simple stand alone code to replicate the issue reported.", "Hi @Emile0205,\r\njust to tell you that I have found a solution since last time for this, but it works for me, maybe not for everyone.\r\nI just wrote my train loop manually as indicated in [Writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#low-level_handling_of_losses_tracked_by_the_model).\r\nBe carefull about how you send your data to your model if you use a GPU, remember that the way you index a **numpy** array will impact the amount of memory your GPU allocates (prefer using **copy**)!\r\nHope it will help you !\r\nGood luck !", "Good day @bguetarni !!!! That link is amazing, thank you! I really enjoy the idea of taking more control of the loss functions and gradients than using Model and compile (ugh!).... I am, however, struggling with altering the GAN example code in the shared link to allow for feature matching  - i.e. how can I access a sequential model's final output and some intermediate layer output as well. Any help from anyone will be greatly appreciated!!", "I constantly meet the bug and it's hard to catch because the leak is very slow and sometimes it disappears after touching the model code, sometimes it is slow enough to finish the training, sometimes it dies after few hours eating 64 Gb of RAM. Turning eager mode off helped to make it slower. There was also an option to try `fit(...epochs=1...)` but I don't remember if this helps.\r\nBut I confirm that it is inside `train_on_batch` and still persists on tf2.3.1 from pip. I don't remember it in tf1.\r\n\r\nAs @bguetarni says, the only 100% workaround was to use a custom training step instead of train_on_batch. Note that I had to implement LossScaleOptimizer for mixed precision training after reading the original code of fit/train_on_batch/make_train_function and this code is better in nuances than the documentation.\r\nMost Internet tutorials like https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch doesn't have this, so you may meet a weird loss function behavior when turn mixed precision on.\r\n\r\n```\r\nfrom tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer as lso\r\n@tf.function                                                                                \r\ndef train_step(inputs, targets):                                                                                                          \r\n    with tf.GradientTape() as tape:                                                         \r\n        preds = model(inputs, training=True)                                      \r\n        loss_value = model.compiled_loss(targets, preds)                          \r\n        scaled_loss_value = loss_value                                                      \r\n        if isinstance(model.optimizer, lso.LossScaleOptimizer):                   \r\n            scaled_loss_value = model.optimizer.get_scaled_loss(scaled_loss_value)\r\n    grad = tape.gradient(scaled_loss_value, model.trainable_variables)            \r\n    model.optimizer.apply_gradients(zip(grad, model.trainable_variables))     \r\n    return loss_value                                  \r\n```\r\n\r\nThis is basically a drop-in replacement for train_on_batch: `loss_gen = train_step(xbatch, ybatch)` so I'm quite happy now with this.\r\n\r\n\r\n\r\n\r\n\r\n\r\n"]}, {"number": 44727, "title": "Running tf from docker with --read-only mode", "body": "- TensorFlow version: 2.2.0\r\nI'd love to run my app with docker run --read-only option, but startup fails when tf runs tests that try to create a temporary folder.\r\n```\r\n  import tensorflow as tf  # noqa  \r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 41, in <module>  \r\n  from tensorflow.python.tools import module_util as _module_util  \r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 132, in <module>  \r\n  from tensorflow.python.platform import test  \r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/test.py\", line 24, in <module>  \r\n  from tensorflow.python.framework import test_util as _test_util  \r\nFile \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/test_util.py\", line 36, in <module>  \r\n  from absl.testing import parameterized  \r\nFile \"/usr/local/lib/python3.6/dist-packages/absl/testing/parameterized.py\", line 186, in <module>  \r\n  from absl.testing import absltest  \r\nFile \"/usr/local/lib/python3.6/dist-packages/absl/testing/absltest.py\", line 244, in <module>  \r\n  flags.DEFINE_string('test_tmpdir', get_default_test_tmpdir(),  \r\nFile \"/usr/local/lib/python3.6/dist-packages/absl/testing/absltest.py\", line 180, in get_default_test_tmpdir  \r\n  tmpdir = os.path.join(tempfile.gettempdir(), 'absl_testing')  \r\nFile \"/usr/lib/python3.6/tempfile.py\", line 437, in gettempdir  \r\n  tempdir = _get_default_tempdir()  \r\nFile \"/usr/lib/python3.6/tempfile.py\", line 372, in _get_default_tempdir  \r\n  dirlist)  \r\nFileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', `'/opt/proj']  \r\n```\r\nI wonder if it's possible to turn off these tests?\r\nProvide param to turn off tests on starting up?\r\nThose who are willing to run tf from docker in a read-only mode which is safer will benefit from this possibility.\r\nIf it's already possible please share how. \r\n", "comments": ["This is not something we'll be able to support. Sorry.\r\n\r\nYou may have some success with the [--tmpfs](https://www.projectatomic.io/blog/2015/12/making-docker-images-write-only-in-production/) flag."]}, {"number": 44726, "title": "Expand the TFLu Renode docs", "body": "This adds:\r\n* information about the estimated test suite execution time,\r\n* brief example of running the simulation interactively. ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain  I merged this branch with master and resolved all conflicts.", "@advaitjain Platform creation commands moved to `.resc` script."]}, {"number": 44725, "title": "module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'", "body": "# Convert the model to the TensorFlow Lite format without quantization\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\r\n\r\n\r\n\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-57-448f10b1b0c8> in <module>()\r\n     18 # Provide a representative dataset to ensure we quantize correctly.\r\n     19 converter.representative_dataset = representative_dataset\r\n---> 20 model_tflite = converter.convert()\r\n     21 \r\n     22 # Save the model to disk\r\n\r\nF:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    745         self.inference_input_type, self.inference_output_type)\r\n    746     if flags_modify_model_io_type:\r\n--> 747       result = _modify_model_io_type(result, **flags_modify_model_io_type)\r\n    748 \r\n    749     if self._experimental_sparsify_model:\r\n\r\nF:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py in modify_model_io_type(model, inference_input_type, inference_output_type)\r\n    833     return model\r\n    834 \r\n--> 835   model_object = _convert_model_from_bytearray_to_object(model)\r\n    836 \r\n    837   if len(model_object.subgraphs) > 1:\r\n\r\nF:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py in _convert_model_from_bytearray_to_object(model_bytearray)\r\n    570 def _convert_model_from_bytearray_to_object(model_bytearray):\r\n    571   \"\"\"Converts a tflite model from a bytearray into a parsable object.\"\"\"\r\n--> 572   model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)\r\n    573   model_object = schema_fb.ModelT.InitFromObj(model_object)\r\n    574   model_object = copy.deepcopy(model_object)\r\n\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n", "comments": ["@YANxu-666 \r\n\r\nPlease, refer similar issue #41846 and see if it helps you.\r\nIf issue still persists please, provide the below information to debug further.\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "I reported a similar issue in a comment of #44079 ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44725\">No</a>\n"]}, {"number": 44724, "title": "Compiling tensorflow Lite into Android dynamic library ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Centos 7.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):tensorflow2.3.0\r\n- TensorFlow version:tensorflow2.3.0\r\n- Python version:python3.5\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):Bazel3.1.0\r\n- GCC/Compiler version (if compiling from source):GCC6.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n-NDK version:r21b\r\n\r\n**Describe the problem**\r\nI have generated the static library libtensorflow-lite.a with build_aarch64_lib.sh\r\nI want to use static library to generate dynamic library \r\nI copy the static library and c++ source code into jni, make Android.mk Application.mk\r\nwhen I use ndk-build, There have been a lot of mistakes \r\nundefined reference to 'std::ios_base::Init::~Init()'\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAndroid.mk\r\nLOCAL_PATH:=$(call my-dir)\r\n\r\ninclude $(CLEAR_VARS)\r\nLOCAL_MODULE:=libtensorflow-lite\r\nLOCAL_SRC_FILES:=/home/wintone/tflitetool/tflitelib/lib/libtensorflow-lite.a\r\ninclude $(PREBUILT_STATIC_LIBRARY)\r\ninclude $(CLEAR_VARS)\r\n\r\n\r\nMY_CPP_LIST:=$(wildcard $(LOCAL_PATH)/*.cpp)\r\n\r\nLOCAL_SRC_FILES:=$(MY_CPP_LIST:$(LOCAL_PATH)/%=%)\r\n\r\nLOCAL_C_INCLUDES:=/home/wintone/tensorflow /home/wintone/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include/\r\n\r\nLOCAL_STATIC_LIBRARIES:=libtensorflow-lite\r\n\r\nLOCAL_MODULE:=libtflite-android\r\n\r\nLOCAL_LDLIBS:=-llog -landroid -fuse-ld=gold\r\n\r\nLOCAL_CPPFLAGS:=-std=c++14 -pthread -frtti -fexceptions \r\n\r\ninclude $(BUILD_SHARED_LIBRARY)\r\nApplication.mk\r\nAPP_STL:=c++_static\r\n\r\nAPP_ABI:=arm64-v8a\r\n\r\nAPP_PLATFORM:=android-16\r\n\r\nAPP_CPPFLAGS:=-frtti -fexceptions -std=c++14 -fpermissive\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nlibtensorflow-lite.a(util.o):util.cpp:function faltbuffers::ConcatPathFileName(std::__cxx11::basic_string<char,std::char_traits<char>,std::allocator<char>>const&, std::__cxx11::basic_string<char,std::char_traits<char>,std::allocator<char>>const&)error:undefind reference to 'std::__cxx11::basic_string<char,std::char_traits<char>,std::allocator<char>>::_M_erase(unsigned long, unsigned long)'\r\nlibtensorflow-lite.a(format_converter.o):format_converter.cc::function _GLOBAL__sub_I_format_converter.cc:error:undefined reference to 'std::ios_base::Init::~Init()' \r\n", "comments": ["Cross compiler version\uff1agcc-linaro-6.1.1-2016.08-x86_64_aarch64-linux-gnu", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44724\">No</a>\n"]}, {"number": 44723, "title": "When using opt.apply_gradient(zip(grads,model.trainable variables))", "body": "ValueError: No gradients provided for any variable: ['dense_flipout_1/kernel_posterior_loc:0', 'dense_flipout_1/kernel_posterior_untransformed_scale:0', 'dense_flipout_1/bias_posterior_loc:0', 'dense_flipout_2/kernel_posterior_loc:0', 'dense_flipout_2/kernel_posterior_untransformed_scale:0', 'dense_flipout_2/bias_posterior_loc:0'].\r\n", "comments": ["@maxkaustav \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "tf.version=2.3\r\n\r\n```model3 = tf.keras.Sequential([\r\n                              tfp.layers.Convolution1DFlipout(64,1,padding=\"same\",kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),activation=tf.nn.relu),\r\n                              tfp.layers.DenseFlipout(32,kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),activation=tf.nn.relu),\r\n                              tf.keras.layers.Dropout(0.4),\r\n                              tfp.layers.DenseFlipout(8,kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),activation=tf.nn.relu),\r\n                              tf.keras.layers.Dropout(0.4),\r\n                              tfp.layers.DenseFlipout(2,kernel_divergence_fn=kl_divergence_function,kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn())\r\n                              ])\r\n```\r\n```\r\ndef train_loop(x,y):\r\n  with tf.GradientTape() as tape:\r\n    logits=model33(x,training=True)\r\n    l=loss(y,logits)\r\n  grad=tape.gradient(l,model3.trainable_variables)\r\n  optimizer.apply_gradients(zip(grad,model3.trainable_variables))\r\n  return l,logits\r\n```", "@maxkaustav \r\nI ran the code shared and it has indentation issues, please share a colab gist with the issue reported.", "Thanks but this issue is solved in another issue in tensorflow probability", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44723\">No</a>\n"]}, {"number": 44722, "title": "TimeDistributed layer and Nested Modules", "body": "Hi there,\r\nI am using time distributed layer to apply another model on each frame and processing the final features by conv 3D. the model is trained properly. but when I want to load the model and use it for test, the model acc on the same val dataset is not reproducible.\r\nWhen I looked the model summary I saw that if I use TimeDistributed layer with a model I can see that model parameters in the summary, but if I use a layer combined with Lambda layer, the number of parameters for that TimeDistributed layer is zero.\r\n\r\n- tf version '1.15.0'\r\n\r\n- keras version '2.2.3'\r\n\r\n```\r\ndef feature_extractor_model():\r\n    feature_extractor = keras.applications.mobilenet.MobileNet(\r\n        include_top=False,\r\n        input_shape=(100, 100, 1),\r\n        weights=None)\r\n    for layer in feature_extractor.layers:\r\n        layer.trainable = True\r\n    output_layer = feature_extractor.get_layer(name='conv_pw_1_relu').output\r\n    feature_extractor = Model(inputs=feature_extractor.inputs, outputs=output_layer, name='Feature_Extractor') \r\n    return feature_extractor\r\ndef base_model(inputs, N_classes,\r\n               growth_rate=32, num_features=64,\r\n               bn_size=4, drop_rate=0.5,\r\n               feature_size=256, reg_type='l1'):\r\n    feature_extractor = feature_extractor_model()\r\n    shape = inputs.get_shape().as_list()[2:]\r\n    init_features = TimeDistributed(feature_extractor, input_shape=shape)(inputs)\r\n```\r\nthis code works well and I can reproduce the results, but the following code does not work and the number of parameters for the second TimeDistributed layer is Zero:\r\n\r\n```\r\ndef Squeeze_Att_(features):\r\n    conv_feature_att = Conv2D(filters=1, kernel_size=(1, 1), strides=(1, 1))(features)\r\n    emphasized_features = Multiply()([features, conv_feature_att])\r\n    emphasized_features = Activation('relu')(emphasized_features)\r\n    return emphasized_features\r\n\r\ndef base_model(inputs, N_classes,\r\n               growth_rate=32, num_features=64,\r\n               bn_size=4, drop_rate=0.5,\r\n               feature_size=256, reg_type='l1'):\r\n    feature_extractor = feature_extractor_model()\r\n    shape = inputs.get_shape().as_list()[2:]\r\n    print(shape)\r\n    init_features = TimeDistributed(feature_extractor, input_shape=shape)(inputs)\r\n    Squeeze_Att__ = Lambda(Squeeze_Att_, name=\"attention_layer_1\")\r\n    init_features = TimeDistributed(Squeeze_Att__, input_shape=shape)(init_features)\r\n```\r\n\r\nit seems that TimeDistributed layer only works when the input is a model. Any Suggestion??", "comments": ["@marziehoghbaie \r\n\r\nCan you please fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know the TF version you are using.\r\n\r\nPlease, share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\nTNX, I edited my issue and submitted it in **Issue: Other Issues** template.", "@marziehoghbaie \r\n\r\nIn the given code you have defined the function but not calling the function anywhere. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d90c60779afb5b428dc3014adf073a6a/untitled510.ipynb) with TF version 2.4-rc0. \r\nPlease share the Python script/notebook you are running, so that we can reproduce the issue on our end.\r\nAlso, check if you facing the same issue with TF v2.3 as well. Thanks!\r\n", "@ravikyram \r\nthanks for your reply, \r\nthis is my colab link:\r\n[https://colab.research.google.com/drive/1uTWfdp4IQP80Ke9xZekP3KGDff_fmudp?usp=sharing](url)\r\nthe problem still remains, however the model is trained successfully but in inference time I get different results even on the same dataset, and to answer the last question about the tensorflow version, because of hardware limitations I can not install tensorflow 2, So I have to use tensorflow 1.x. \r\nI'd be happy if you could help. ", "I should admit that the MobileNet part in the above code works well, and if I train a model with the MobileNet layer it works fine.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation. Thanks!\r\n"]}, {"number": 44721, "title": "TFLite TensorArrayScatterV3 failed (not found)", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 (work) / Android 9.0 (inference)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Motorola Nexus 6\r\n- TensorFlow installed from (source or binary): Source (master branch at 4625f1d87f17537a8d50dc68a213d2da875c12ce)\r\n- TensorFlow version (use command below): 4625f1d87f17537a8d50dc68a213d2da875c12ce\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: No GPU\r\n- GPU model and memory: N/A\r\n\r\n- Android SDK 30.0.2 and NDK r21d\r\n\r\n**Describe the current behavior**\r\n\r\nI compiled `benchmark_model_plus_flex` and ran into the same issue. I was trying to benchmark the following model:\r\n\r\n<http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz>\r\n\r\nwith it converted into TFLite format with the following code:\r\n\r\n```python\r\ndef do_convert(saved_model, output_file):\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model)\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    model = converter.convert()\r\n    with open(output_file, \"wb\") as f:\r\n        f.write(model)\r\n```\r\n\r\nThe exact error is\r\n\r\n```text\r\nnative : op_kernel.cc:1763 OP_REQUIRES failed at tensor_array_ops.cc:1035 : Not found: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_774)\r\nERROR: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_774)\r\n         (while executing 'TensorArrayScatterV3' via Eager)\r\nERROR: Node number 276 (TfLiteFlexDelegate) failed to invoke.\r\n```\r\n\r\nand the command line in `adb shell` is\r\n\r\n```shell\r\n./benchmark_model_plus_flex --graph=ssd_mobilenet_v1_coco_2018_01_28.tflite\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe benchmark should proceed successfully.\r\n\r\n**Standalone code to reproduce the issue**\r\n", "comments": ["@iBug \r\nI ran the code shared on tf nightly and did not see any error, can you please share a colab gist with the code and error reported.\r\nCould you please refer to these resolved issues with the same error and let us know: #42214 , #43270", "Hi @Saduf2019, thanks for your quick response.\r\n\r\nIn case I wasn't clear enough, here's everything I've done from scratch to reach that error:\r\nssd_mobilenet_v1_coco_2018_01_28\r\n\r\n- Spin up a new Ubuntu 20.04.1 LTS system in LXD container and install basic requirements (Git, `build-essential` etc.)\r\n- Install Bazel 3.7.0 following [the documentation](https://docs.bazel.build/versions/master/install-ubuntu.html) and Android SDK 30.0.2 and NDK r21d using [sdkmanager CLI tool](https://developer.android.com/studio/command-line/sdkmanager)\r\n- Clone this repository somewhere, and `git checkout 4625f1d87f17537a8d50dc68a213d2da875c12ce`\r\n- Using the configure script, produce the following `.tf_configure.bazelrc`:\r\n\r\n    ```\r\n    build --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n\tbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.8/dist-packages\"\r\n\tbuild --python_path=\"/usr/bin/python3\"\r\n\tbuild --config=xla\r\n\tbuild:opt --copt=-march=native\r\n\tbuild:opt --copt=-Wno-sign-compare\r\n\tbuild:opt --host_copt=-march=native\r\n\tbuild:opt --define with_default_optimizations=true\r\n\tbuild --action_env ANDROID_NDK_HOME=\"/home/ubuntu/Android/Sdk/ndk-bundle\"\r\n\tbuild --action_env ANDROID_NDK_API_LEVEL=\"23\"\r\n\tbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"29.0.3\"\r\n\tbuild --action_env ANDROID_SDK_API_LEVEL=\"30\"\r\n\tbuild --action_env ANDROID_SDK_HOME=\"/home/ubuntu/Android/Sdk\"\r\n\ttest --flaky_test_attempts=3\r\n\ttest --test_size_filters=small,medium\r\n\ttest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\n\ttest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\n\ttest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\n\ttest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\n\tbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\t```\r\n\r\n- Compile TFLite benchmark tool with Flex support:\r\n\r\n    ```shell\r\n    bazel build -c opt \\\r\n      --config=android_arm \\\r\n      --config=monolithic \\\r\n      tensorflow/lite/tools/benchmark:benchmark_model_plus_flex\r\n    ```\r\n\r\n    Here I used `android_arm` because Nexus 6 is a 32-bit device. I also tried `android_arm64` with another phone (OnePlus 3T) with the same outcome.\r\n\r\n- Transfer `bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex` to the Android device, enter `adb shell` and `chmod 755` on the binary\r\n\r\n- Grab the model tarball from <http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz> (using Wget or cURL) and extract it (`tar zxf`)\r\n- Convert to TFLite:\r\n\r\n    ```python\r\n    import tensorflow as tf\r\n\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(\"ssd_mobilenet_v1_coco_2018_01_28/saved_model\")\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    model = converter.convert()\r\n    with open(\"ssd_mobilenet_v1_coco_2018_01_28.tflite\", \"wb\") as f:\r\n        f.write(model)\r\n    ```\r\n\r\n- Transfer `ssd_mobilenet_v1_coco_2018_01_28.tflite` file to the Android device\r\n- Run benchmark:\r\n\r\n    ```shell\r\n    ./benchmark_model_plus_flex --graph=ssd_mobilenet_v1_coco_2018_01_28.tflite\r\n    ```\r\n\r\n- The error message comes from the above command.\r\n\r\nMay I know how you managed to run the benchmark successfully?", "Hi @Saduf2019, thank you for your references.\r\n\r\nThe first linked issue (#42214) was easy to find from Google and I've already seen it a dozen times. I couldn't extract anything helpful to me. However one of the comments (<https://github.com/tensorflow/tensorflow/issues/43270#issuecomment-694619122>) from your second linked issue said:\r\n\r\n> TensorArrayScatterV3 op is currently not supported by either TensorFlow Lite builtin op set or Flex delegate.\r\n\r\nDo I have to manipulate the SSD MobileNet V1 model somehow before I could run it with TFLite?", "Sorry for encountering this issue on your side. We are working on TensorArrayScatterV3 support in TFLite. Will update this thread once the TensorArray related issues in TFLite have been resolved.", "@abattery Thanks for your effort, I have seen #48126, but I am wondering whether TensorArrayScatterV3 is supported in ARM inference(i.e. I have already converted a model with TensorArrayScatterV3 to tflite, can I deploy the model on android?) Thanks!", "Please try out the recent TF version. We delivered a better story for supporting tensor list ops.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44721\">No</a>\n"]}, {"number": 44720, "title": "C++ inference with tensorflow1.15", "body": "when I do inference with c++, this problem arised,\r\n\"2020-11-10 13:41:29.363429: F tensorflow/core/framework/op.cc:200] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Already exists: Op with name _Arg Aborted (core dumped)\r\n\"\r\ntensorflow1.15 python 3.6", "comments": ["@ZhaoyangLi-nju \r\nTf 1.15 is not supported, please upgrade to 2.x and let us know if you still face any issues.", "> @ZhaoyangLi-nju\r\n> Tf 1.15 is not supported, please upgrade to 2.x and let us know if you still face any issues.\r\n\r\nhow about Tf1.14?", "Tf 2.x only is supported. Please upgrade and let us know."]}, {"number": 44719, "title": "Refactor SparseFillEmptyRows[Grad] CPU kernels into classes", "body": "This is a NFC in preparation for adding a GPU implementation.\r\nAlso generalizes int64 indexing to template type Tindex.\r\n\r\ncc @nluehr ", "comments": ["Is there any indication of why this was rolled back?", "It broke one of the internal projects. Working on roll forward, hopefully will submit tomorrow."]}, {"number": 44718, "title": "lib/python3.6/site-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py\", line 44, in <module>     from tensorflow.python.autograph.core import naming ImportError: cannot import name 'naming'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): conda install\r\n- TensorFlow version: 1.15\r\n- Python version: 3.6.1\r\n- Installed using virtualenv? pip? conda?: conda \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory:\r\n\r\n\r\n\r\nGetting error:\r\nlib/python3.6/site-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py\", line 44, in <module>\r\n    from tensorflow.python.autograph.core import naming\r\nImportError: cannot import name 'naming'\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nRan a .py file with \"python <file.py>\" command\r\n\r\n\r\n", "comments": ["How can I get it working with tensorflow version 1.15? ", "Complete log:\r\n\r\n\r\n\r\npython handObjectTrackingSingleFrame.py --seq 'test' --showFig --doPyRender\r\n2020-11-09 22:41:54.700734: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2020-11-09 22:41:54.700770: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"handObjectTrackingSingleFrame.py\", line 6, in <module>\r\n    from ghope.common import *\r\n  File \"folder/honnotate/HOnnotate-master/optimization/ghope/__init__.py\", line 4, in <module>\r\n    from ghope.optimization import Optimizer\r\n  File \"folder/honnotate/HOnnotate-master/optimization/ghope/optimization.py\", line 5, in <module>\r\n    import tensorflow_probability as tfp\r\n  File \"folder/anaconda3/envs/honnotate/lib/python3.6/site-packages/tensorflow_probability/__init__.py\", line 75, in <module>\r\n    from tensorflow_probability.python import *  # pylint: disable=wildcard-import\r\n  File \"folder/anaconda3/envs/honnotate/lib/python3.6/site-packages/tensorflow_probability/python/__init__.py\", line 24, in <module>\r\n    from tensorflow_probability.python import experimental\r\n  File \"folder/anaconda3/envs/honnotate/lib/python3.6/site-packages/tensorflow_probability/python/experimental/__init__.py\", line 34, in <module>\r\n    from tensorflow_probability.python.experimental import auto_batching\r\n  File \"folder/anaconda3/envs/honnotate/lib/python3.6/site-packages/tensorflow_probability/python/experimental/auto_batching/__init__.py\", line 24, in <module>\r\n    from tensorflow_probability.python.experimental.auto_batching import frontend\r\n  File \"folder/anaconda3/envs/honnotate/lib/python3.6/site-packages/tensorflow_probability/python/experimental/auto_batching/frontend.py\", line 44, in <module>\r\n    from tensorflow.python.autograph.core import naming\r\nImportError: cannot import name 'naming'\r\n", "@anjugopinath \r\n\r\nI have tried in colab  with TF version 1.15 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d0a8523a9563bfee371ef212780a2b5c/untitled505.ipynb).\r\n\r\nPlease, refer similar issues #40584, #40385 and see if it helps you. Thanks!", "@ravikyram  Thank You ... Can I try this command: \" pip install tfp-nightly 0.11.0.dev20200708\" ?", "I had tensorflow-probability version 0.8.0 ... I installed version 0.7 with the command \"pip install tensorflow-probability==0.7\" inside the conda virtual environment and I am not getting that error now...\r\n\r\nThank You very much!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44718\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44718\">No</a>\n"]}, {"number": 44717, "title": "Add to release notes: deterministic tf.image.resize (bilinear)", "body": "This current pull request (into the r2.4 branch) adds information to the version 2.4.0 release notes about the changes made by pull request [39243](https://github.com/tensorflow/tensorflow/pull/39243), which was merged into the master branch on 2020-09-22.", "comments": ["Thanks @duncanriach for updating the release notes. ", "Thanks @goldiegadde for making the release notes available for update in the release branch. It looks like you've streamlined that part of the release process."]}, {"number": 44716, "title": "Merge pull request #1 from tensorflow/master", "body": "Bring up to date with upstream", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44716) for more info**.\n\n<!-- need_sender_cla -->", "GAH!  Sorry folks please ignore the noise - I'm having a bad day on github."]}, {"number": 44715, "title": "Merge pull request #1 from tensorflow/master", "body": "Bring up to date with upstream", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44715) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 44712, "title": "[ROCm] MLIR-unranked kernels with ROCm", "body": "Adding ROCm support for MLIR-unranked kernels with ROCm, as discussed in the email thread.\r\n\r\nI have tested this locally with the following lines set in the `.tf_configure.bazelrc` file\r\n```\r\nbuild --config=rocm\r\nbuild:rocm --define tensorflow_enable_mlir_generated_gpu_kernels=1\r\n```\r\nand then the commands\r\n```bash\r\nroot@prj47-rack-15: bazel test --define enable_unranked_kernels=1 //tensorflow/core/kernels/mlir_generated/...\r\n...\r\nINFO: Build completed successfully, 241 total actions\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test                    PASSED in 3.8s\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test_gpu                PASSED in 3.8s\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test                   PASSED in 3.8s\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test_gpu               PASSED in 1.3s\r\n\r\nINFO: Build completed successfully, 241 total actions\r\n...\r\nroot@prj47-rack-15:/root/tensorflow# bazel test //tensorflow/core/kernels/mlir_generated/...\r\n...\r\nINFO: Build completed successfully, 250 total actions\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test                    PASSED in 1.3s\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test_gpu                PASSED in 1.4s\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test                   PASSED in 1.5s\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test_gpu               PASSED in 1.3s\r\n\r\nINFO: Build completed successfully, 250 total actions\r\n...\r\n```\r\n\r\n\r\n/cc @whchung  @pifon2a @sherhut @thomasjoerg @akuegel ", "comments": ["working on resolving the merge-conflict....will push out a commit later today", "fix the merge conflict, but the rebase to tip results in compile failure when building with `-define enable_unranked_kernels=1` :( \r\n\r\nThis is the error I get....m assuming this is something that will get resolved upstream, and I do not need to worry about it.\r\n```\r\nroot@prj47-rack-15:/root/tensorflow# bazel test --define enable_unranked_kernels=1 //tensorflow/core/kernels/mlir_generated/...\r\n...\r\nIn file included from tensorflow/core/kernels/mlir_generated/unranked_op_gpu_base.cc:16:0:\r\n./tensorflow/core/kernels/mlir_generated/unranked_op_gpu_base.h:19:10: fatal error: third_party/llvm/llvm-project/llvm/include/llvm/ADT/ArrayRef.h: No such file or directory\r\n #include \"third_party/llvm/llvm-project/llvm/include/llvm/ADT/ArrayRef.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nINFO: Elapsed time: 11.483s, Critical Path: 0.93s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test                 NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test_gpu             NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_add_test                 NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_add_test_gpu             NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test                NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test_gpu            NO STATUS\r\n\r\nFAILED: Build did NOT complete successfully\r\n\r\n```\r\n\r\nThe other path does pass\r\n```\r\nroot@prj47-rack-15:/root/tensorflow# bazel test //tensorflow/core/kernels/mlir_generated/...\r\n...\r\nINFO: Build completed successfully, 5314 total actions\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test                    PASSED in 1.3s\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test_gpu                PASSED in 1.4s\r\n//tensorflow/core/kernels/mlir_generated:gpu_add_test                    PASSED in 1.5s\r\n//tensorflow/core/kernels/mlir_generated:gpu_add_test_gpu                PASSED in 1.3s\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test                   PASSED in 1.4s\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test_gpu               PASSED in 3.0s\r\n\r\nINFO: Build completed successfully, 5314 total actions\r\n\r\n```\r\n", "pushed out a fix for the compile error. The compile error is not ROCm specific, and the fix is trivial.\r\n\r\nBut now am running into the following compile error (which also does not seem ROCm specific)\r\n```\r\nroot@prj47-rack-15:/root/tensorflow# bazel test --define enable_unranked_kernels=1 //tensorflow/core/kernels/mlir_generated/...\r\n...\r\nERROR: /root/tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:123:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels/mlir_generated:cwise_binary_op':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/mlir_generated/unranked_gpu_add.cc':\r\n  'tensorflow/core/kernels/mlir_generated/unranked_op_gpu_base.h'\r\nINFO: Elapsed time: 19.089s, Critical Path: 8.65s\r\nINFO: 6 processes: 6 local.\r\nFAILED: Build did NOT complete successfully\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test                 NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_abs_test_gpu             NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_add_test                 NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_add_test_gpu             NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test                NO STATUS\r\n//tensorflow/core/kernels/mlir_generated:gpu_tanh_test_gpu            NO STATUS\r\n\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "pushed out another commit with a potential fix the for bazel dependency error. With this commit I can get all of the unit tests within `//tensorflow/core/kernels/mlir_generated` to pass with and without `--define enable_unranked_kernels=1` ", "@pifon2a @sherhut \r\n\r\npushed out a new commit to address your code review comments"]}, {"number": 44711, "title": "memory leak in tf.keras.Model.predict", "body": "https://stackoverflow.com/questions/64199384/tf-keras-model-predict-results-in-memory-leak\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["https://stackoverflow.com/questions/64199384/tf-keras-model-predict-results-in-memory-leak\r\n\r\nSeems to be an issue with tf.keras.Model.predict", "I have tried in TF GPU with version [2.3 gist](https://colab.research.google.com/gist/ravikyram/c9430bc7667e6299feaeab3b905418f7/untitled501.ipynb) ,nightly version(`2.5.0-dev20201109`) [gist](https://colab.research.google.com/gist/ravikyram/bf1336b625d9c416c9838280e162247c/untitled502.ipynb) and was able to reproduce the issue.Thanks!", "@plooney `model.predict` is a high-level API which is designed for batch-predicting outside of any loops. It automatically wraps your model into a tf.function and maintains graph based execution. Which means, if there is any change in input signature (`shape` and `dtype`) to that function (here model.predict), then it traces multiple models instead of a single model as you are expecting.\r\n\r\nIn your case, `inImm` is a numpy input which is considered as different signature each time you provide it in a for loop to a function wrapped by tf.function. However, providing `inImm` as a tensor will result in same input signature and hence there is a single graph to which these inputs are fed and results are obtained. In the numpy case, there are 60 static graphs (which is not what you want). As there are many static graphs, the memory is increasing with each for loop iteration.\r\n\r\nWhen I added one line in your code, the code is not crashing. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/36ebe9b17279c12ac26cc6f171eb125b/untitled502.ipynb). Thanks!\r\n\r\n\r\n`inImm=tf.convert_to_tensor(inImm)`\r\n\r\n\r\nPlease read [1](https://github.com/tensorflow/tensorflow/issues/40261), [2](https://www.tensorflow.org/guide/function), [3](https://github.com/tensorflow/tensorflow/issues/40708#issuecomment-653823716), and [4](https://github.com/tensorflow/tensorflow/issues/39458#issuecomment-669588856). These resources will help you more. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n\r\n\r\n", "@jvishnuvardhan Predict in a loop it is a quite recurrent issue, I remember some weeks ago I've just triaged two of this tickets. \r\nHow we could better expose this in the docs? /cc @lamberta @MarkDaoust ", "@bhack Good point. I think updating one of the docs (tutorial/guides) would help resolving this kind of issues. Thanks!", "@jvishnuvardhan Yes we need to find a quite popular entry point in the Docs if any internal team member has some stats about Docs page views.", "Also a more specific \"entry level\" warning (instead of the generic function retracing) could be very useful for newcomers.", "@jvishnuvardhan thanks for the clear explanation. If this were calling a `tf.function` in a loop that would be 100% the correct answer. But `Model.predict` manages some of this to avoid this problem (in general keras fit/evaluate/predict never require that the user convert inputs to tensors). It looks like something more complicated is happening.\r\n\r\nThe first two clues that suggest it are:\r\n\r\n1. It's not printing the frequent retracing warning.\r\n2. You're creating a single numpy array and passing that multiple times and it still goes OOM. Except for constants the caching logic is based on object identity, re-using the object should reuse the same function trace.\r\n\r\nInvestigating a little farther you can find the `model.predict_function` is the `@tf.function` that runs in here.\r\nInspecting that, both it's `._list_all_concrete_functions()` and `.pretty_printed_concrete_signatures()` show that there is only one graph, and predict is handling the conversion of the numpy array to a Tensor.\r\n\r\nSo I agree that this is leaking memory somewhere. But I've confirmed that it's not the `tf.function` cache causing it.\r\n\r\n@tomerk, you're pretty familiar with this code, do you have any ideas?", "I've just modified the original stackoverflow mentioned code for Colab:\r\n```\r\npip install --upgrade memory_profiler\r\n```\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.layers import Input,Conv2D, Activation\r\n\r\n%load_ext memory_profiler\r\n\r\nmatrixSide = 512 #define a big enough matrix to give memory issues\r\n\r\ninputL = Input([matrixSide,matrixSide,12]) #create a toy model\r\nl1 = Conv2D(32,3,activation='relu',padding='same') (inputL) #120\r\nl1 = Conv2D(64,1,activation='relu',padding='same')(l1)\r\nl1 = Conv2D(64,3,activation='relu',padding='same')(l1)\r\nl1 = Conv2D(1,1,padding='same')(l1)\r\nl1 = Activation('linear')(l1)\r\nmodel = tf.keras.Model(inputs= inputL,outputs = l1)\r\n\r\n\r\n#run predictions\r\ninImm = np.zeros((64,matrixSide,matrixSide,12))\r\nfor i in range (60):\r\n  print(i)\r\n  %memit outImm = model.predict(inImm)\r\n```", "```\r\npeak memory: 5729.88 MiB, increment: 3190.20 MiB\r\n1\r\npeak memory: 6498.93 MiB, increment: 769.06 MiB\r\n2\r\npeak memory: 6499.14 MiB, increment: 0.21 MiB\r\n3\r\npeak memory: 6499.14 MiB, increment: 0.00 MiB\r\n4\r\npeak memory: 6499.14 MiB, increment: 0.00 MiB\r\n5\r\npeak memory: 6499.15 MiB, increment: 0.00 MiB\r\n6\r\npeak memory: 6499.17 MiB, increment: 0.02 MiB\r\n7\r\npeak memory: 6499.17 MiB, increment: 0.00 MiB\r\n8\r\npeak memory: 6499.17 MiB, increment: 0.00 MiB\r\n9\r\npeak memory: 6499.17 MiB, increment: 0.00 MiB\r\n10\r\npeak memory: 6499.17 MiB, increment: 0.00 MiB\r\n11\r\npeak memory: 6499.17 MiB, increment: 0.00 MiB\r\n12\r\npeak memory: 6499.18 MiB, increment: 0.01 MiB\r\n13\r\npeak memory: 6499.18 MiB, increment: 0.00 MiB\r\n14\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n15\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n16\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n17\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n18\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n19\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n20\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n21\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n22\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n23\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n24\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n25\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n26\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n27\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n28\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n29\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n30\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n31\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n32\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n33\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n34\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n35\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n36\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n37\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n38\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n39\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n40\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n41\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n42\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n43\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n44\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n45\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n46\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n47\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n48\r\npeak memory: 6499.19 MiB, increment: 0.00 MiB\r\n49\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n50\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n51\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n52\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n53\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n54\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n55\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n56\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n57\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n58\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n59\r\npeak memory: 6499.20 MiB, increment: 0.00 MiB\r\n```", "It doesn't OOM with memory_profiler running because `%memit` calls `gc.collect`: \r\n\r\nhttps://github.com/pythonprofilers/memory_profiler/blob/bd6da910f791cef725640fa207fb4ee433c93586/memory_profiler.py#L1060\r\n\r\nIf you add a `gc.collect` there it continues as well. If you monkey-patch out `gc.collect` then you get the explosive memory growth.\r\n\r\nSo that points towards an issue with cyclical-garbage not getting cleaned up fast enough.", "> So that points this towards an issue with cyclical-garbage not getting cleaned up fast enough.\n\nYes It was one of the suspects", "Also at step `0`\r\n`outImm = model(inImm)`\r\n\r\n```\r\nResourceExhaustedError: OOM when allocating tensor with shape[64,512,512,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd]\r\n```\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "So, random thoughts (I re-opened just to leave these)\r\n\r\n1. as mentioned above, `predict` shouldn't be used in a loop. Sad things happen if you do. Ideally use model call w/ training=False directly (manually wrapping your model call in a tf.function if needed for performance reasons)\r\n\r\n2. Numpy inputs to predict/fit/evaluate get converted to tf.data datasets then iterated over. The specific conversion implementation in place currently ends up copying the data and is poorly suited for large input sizes (it is prone to ooms for large inputs). There's a number of other github issues related to this floating around, but we have so far been unable to prioritize this in core Tensorflow. If you need performant numpy input to keras fit/evaluate/predict my current recommendation is using Tensorflow-io's numpy inputs: https://www.tensorflow.org/io/api_docs/python/tfio/experimental/IODataset#from_numpy because it should be more performant and avoid excess memory copies.\r\n\r\n3. The fact that gc.collect fixes this makes me think something about the numpy conversion is also creating cyclical references that the python gc doesn't trigger for (the memory is consumed by the gpu which isn't tracked by the python gc, so the python gc fails to trigger because it thinks there's still plenty of memory). We've seen cyclical issues like this cause issues elsewhere (e.g. when creating multiple models), but due to the two aforementioned points we don't have the bandwidth right now to prioritize tracking down & fixing this specific one.", "> Also at step `0`\r\n> `outImm = model(inImm)`\r\n> \r\n> ```\r\n> ResourceExhaustedError: OOM when allocating tensor with shape[64,512,512,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:BiasAdd]\r\n> ```\r\n\r\nHi, I am wondering what this mean? Cannot solve the OOM issue even if using model.__call__ directly ?\r\nWhat is the workaround for this problem?", "```\r\n>>> 64*64*512*512*4/2**30\r\n4.0\r\n```\r\n\r\nThat 1 tensor is 4GB. Have you tried a smaller batch size?", "So? I understand the OOM of GPU is because the GPU memory is too small. It means it is not the same issue with the leakage problem. The bug i met is that after 7 hours of running model.predict() in a loop, there is a GPU OOM."]}, {"number": 44709, "title": "sequences_to_texts on character level tf.keras.preprocessing.text.Tokenizer adds spaces between characters", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 2004**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.3.0-54-gfcc4b966f1 2.3.1** and **v2.3.0-0-gb36436b087 2.3.0**\r\n- Python version: **3.7.9** and **3.6.9**\r\n\r\n\r\n**Describe the current behavior**\r\nThe `tf.keras.preprocessing.text.Tokenizer` with `char_level=True` adds spaces between characters when the `sequences_to_texts` is called. For example if our data is `[\"Hi\"]` and we convert them to sequence and then back to text the output will be `[\"h i\"]`\r\n\r\n**Describe the expected behavior**\r\nSpaces should not be added.\r\nFor example if our data is `[\"Hi\"]` -> sequence -> [\"hi\"].\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\n\r\ntext_data=[\"Hello\",\"Run\"]\r\nprint(text_data)\r\n\r\ntok = Tokenizer(char_level=True)\r\ntok.fit_on_texts(text_data)\r\nsequences = tok.texts_to_sequences(text_data)\r\nprint(sequences)\r\n\r\nreconstructed_text = tok.sequences_to_texts(sequences)\r\nprint(reconstructed_text)\r\n```", "comments": ["@SteveGdvs \r\nThis is a character level sequencer, hence the result, you may join it afterwards.", "@Saduf2019 \r\nMaybe I didn't explain it correctly or I am mistaken, but I will try again.\r\n\r\nIf it was a word level sequencer and we give it as input `[\"hello world my name is Steve\"]` it would convert it to a sequence of integers (for example) `[1,2,3,4,5,6]` and by calling `sequences_to_texts` we can convert the numbers back to the original string `[\"hello world my name is Steve\"]`.\r\n\r\nBut in a character level sequencer if we give as input `[\"hello\"]` it will be converted to a sequence `[1,2,3,3,4]` but by calling the `sequences_to_texts` we don't get the same input back but we get this `[\"h e l l o\"]`.\r\n\r\nMaybe I don't understand how to use the `sequences_to_texts` in a character level sequencer but I think the output should be `[\"hello\"]` and not `[\"h e l l o\"]`.\r\n\r\nWith the following code I demonstrate what I say above\r\n\r\n```\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\n\r\ntext_data=[\"Hello world\",\"Run as fast as you can!\"]\r\nprint(text_data)\r\n\r\nprint(\"char level Tokenizer\")\r\n\r\nchar_tok = Tokenizer(char_level=True)\r\nchar_tok.fit_on_texts(text_data)\r\nchar_sequences = char_tok.texts_to_sequences(text_data)\r\nprint(char_sequences)\r\n\r\nchar_reconstructed_text = char_tok.sequences_to_texts(char_sequences)\r\nprint(char_reconstructed_text)\r\n\r\nprint(\"word level Tokenizer\")\r\n\r\nword_tok = Tokenizer(char_level=False)\r\nword_tok.fit_on_texts(text_data)\r\nword_sequences = word_tok.texts_to_sequences(text_data)\r\nprint(word_sequences)\r\n\r\nword_reconstructed_text = word_tok.sequences_to_texts(word_sequences)\r\nprint(word_reconstructed_text)\r\n```", "i am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/87427fe212c2935cf9b5d0bbc617b537/untitled462.ipynb).", "I think this is by design an intended behavior. The document says,\r\n> By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character)\r\n\r\nhttps://github.com/keras-team/keras-preprocessing/blob/58df11e1145b2088092252c4dba02168c6da2b13/keras_preprocessing/text.py#L363\r\nThere is a space in the string quote while using `.join()` method.\r\n` vect = ' '.join(vect)`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry for the late response.\r\n\r\nIf this is intended then how can I convert a sequence to the original string?\r\nIf the problem is only on the line you mentioned then can this be the solution?\r\n\r\n```\r\nif self.char_level:\r\n    vect = ''.join(vect)\r\nelse:\r\n    vect = ' '.join(vect)\r\n```\r\n\r\n", "You may try with \r\n```python\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\n\r\ntext_data=[\"Hello\",\"Run\"]\r\nprint(text_data)\r\n\r\ntok = Tokenizer(char_level=True)\r\ntok.fit_on_texts(text_data)\r\nsequences = tok.texts_to_sequences(text_data)\r\nprint(sequences)\r\n\r\nreconstructed_text = tok.sequences_to_texts(sequences)\r\nprint(reconstructed_text)\r\n\r\n# Generating text from sequence (mapping back)\r\nword_index = tok.word_index\r\nreverse_map = {val:key for key, val in word_index.items()}\r\nprint(reverse_map) \r\n#{1: 'l', 2: 'h', 3: 'e', 4: 'o', 5: 'r', 6: 'u', 7: 'n'}\r\nretext = ''\r\nfor seq in sequences:\r\n    for q in seq:\r\n       retext += reverse_map[q]\r\n    retext += ' ' \r\nprint(retext)\r\n#hello run ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44709\">No</a>\n", "> You may try with\r\n> \r\n> ```python\r\n> from tensorflow.keras.preprocessing.text import Tokenizer\r\n> \r\n> text_data=[\"Hello\",\"Run\"]\r\n> print(text_data)\r\n> \r\n> tok = Tokenizer(char_level=True)\r\n> tok.fit_on_texts(text_data)\r\n> sequences = tok.texts_to_sequences(text_data)\r\n> print(sequences)\r\n> \r\n> reconstructed_text = tok.sequences_to_texts(sequences)\r\n> print(reconstructed_text)\r\n> \r\n> # Generating text from sequence (mapping back)\r\n> word_index = tok.word_index\r\n> reverse_map = {val:key for key, val in word_index.items()}\r\n> print(reverse_map) \r\n> #{1: 'l', 2: 'h', 3: 'e', 4: 'o', 5: 'r', 6: 'u', 7: 'n'}\r\n> retext = ''\r\n> for seq in sequences:\r\n>     for q in seq:\r\n>        retext += reverse_map[q]\r\n>     retext += ' ' \r\n> print(retext)\r\n> #hello run \r\n> ```\r\n\r\nSure, but what's the point of having a Tokenizer class if we have to basically reimplement the `sequences_to_text` method by hand."]}, {"number": 44708, "title": "TFLite set_tensor fails unexpectedly in Tensorflow version 2.3.1", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device: Snapdragon 855\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.7\r\n- Bazel version: None\r\n- GCC/Compiler version:None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nI have a Keras model which I am able to convert to tflite both without quantization and with full integer quantization. \r\nI'm then able to run inference using the interpreter and get the expected output, all using the code below. In addition, this works as expected when using the Hexagon delegate on a mobile device\r\n\r\nWhen using Tensorflow version 2.2.0 this works. When using Tensorflow version 2.3.1, it does not work, with the following error:\r\n\r\n>ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type INT8 for input 0, name: input_1\r\n\r\nI also have the following logs of the \"input details\" of the tflite intepreter:\r\n\r\nTF version 2.3.1\r\n\r\nnon quantized: \r\ninput details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\ninput shape:  [  1 224 224   3]\r\n\r\nquantized:\r\n\r\ninput details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([ -1, 224, 224,   3]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.007843137718737125, -1), 'quantization_parameters': {'scales': array([0.00784314], dtype=float32), 'zero_points': array([-1]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\ninput shape:  [  1 224 224   3]\r\n\r\nTF version 2.2.0\r\nnon quantized:\r\n\r\ninput details:  [{'name': 'input_1', 'index': 0, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\ninput shape:  [  1 224 224   3]\r\n\r\nquantized:\r\n\r\ninput details:  [{'name': 'input_1', 'index': 235, 'shape': array([  1, 224, 224,   3]), 'shape_signature': array([  1, 224, 224,   3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\ninput shape:  [  1 224 224   3]\r\n\r\nalso, see attached the input image, the reference images and the model. \r\n\r\nI see that with version 2.2.0, the input layer of the quantized version is not quantized. However, the quantized model works fine on Hexagon DSP with Hexagon delegate. What's going on? \r\n\r\n`\r\n[norm_images.zip](https://github.com/tensorflow/tensorflow/files/5512065/norm_images.zip)\r\n[dataset.zip](https://github.com/tensorflow/tensorflow/files/5512071/dataset.zip)\r\n[model.zip](https://drive.google.com/file/d/12ORteaMaDInKpJl3xhlTwyoi0m_V2yvV/view?usp=sharing)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    import cv2\r\n\r\n    def read_and_process_image(fname):\r\n        img = cv2.imread(fname)\r\n        img = cv2.resize(img,dsize = (224,224),interpolation = cv2.INTER_LANCZOS4)\r\n        img = img / 127.5\r\n        img = img - 1\r\n        img = img.astype(np.float32)\r\n        img = img.reshape(1,224,224,3)\r\n        return img\r\n\r\n    def rep_data_gen0():\r\n        a = []\r\n        for i in range(128):\r\n            img = np.fromfile('norm_images/image_'+str(i)+\".bin\",dtype=np.float32)\r\n            img = img.reshape(224,224,3)\r\n            a.append(img)\r\n        \r\n        a = np.array(a)\r\n        img = tf.data.Dataset.from_tensor_slices(a).batch(1)\r\n        for i in img.take(128):\r\n            yield [i]\r\n\r\n\r\n     def convert(model):\r\n        converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n        tflite_model = converter.convert()\r\n        return tflite_model\r\n\r\n    def quantize(model):\r\n        converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.representative_dataset = rep_data_gen0\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        converter.inference_input_type = tf.int8  # or tf.uint8\r\n        converter.inference_output_type = tf.int8  # or tf.uint8\r\n\r\n        print(\"Converting using full integer quantization\")\r\n        quant_model = converter.convert()\r\n        return quant_model\r\n\r\n    def excute_tflite(model,image):\r\n        interpreter = tf.lite.Interpreter(model_content=model)\r\n        interpreter.allocate_tensors()\r\n        # Get input and output tensors.\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n\r\n        input_shape = input_details[0]['shape']\r\n        print (\"input details: \", input_details)\r\n        print(\"input shape: \", input_shape)\r\n        interpreter.set_tensor(input_details[0]['index'], image)\r\n        interpreter.invoke()\r\n\r\n        tflite_results = interpreter.get_tensor(output_details[0]['index'])\r\n        tflite_results_img = tflite_results[0, :, :, 0]\r\n        return tflite_results_img\r\n\r\n    def main_func():\r\n        img = read_and_process_image(\"dataset/COCO_train2014_000000003860.jpg\")\r\n        keras_model = tf.keras.models.load_model('sod_latest.hdf5')\r\n        tf_model = convert(keras_model)\r\n        quant01_model = quantize(keras_model)\r\n    \r\n        result = keras_model.predict(img)\r\n        result_img = result[0,:,:,0]\r\n\r\n        result_tf_img = excute_tflite(tf_model,img)\r\n        result_quant_img0 = excute_tflite(quant01_model, img)\r\n\r\n        print(np.min(result_img), np.max(result_img))\r\n        print(np.min(result_tf_img), np.max(result_tf_img))\r\n        print(np.min(result_quant_img0), np.max(result_quant_img0))\r\n\r\n        cv2.imshow('keras',result_img)\r\n        cv2.resizeWindow('keras', 400, 400)\r\n        cv2.imshow('tflite', result_tf_img)\r\n        cv2.resizeWindow('tflite', 400, 400)\r\n        cv2.imshow('quantized tflite', result_quant_img0)\r\n        cv2.resizeWindow('quantized tflite', 400, 400)\r\n        cv2.waitKey(0)\r\n\r\n    if __name__ == \"__main__\":\r\n        print(tf.__version__)\r\n        main_func()\r\n\r\n\r\n`\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.5.0-dev20201109`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0c936ae628894ef6cefba54a0ade47c6/untitled503.ipynb).Thanks!", "In TF 2.2 i am not seeing any value error.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/064c1b4e0ba8cde69cef8446e26028a9/untitled515.ipynb).Thanks!", "I've attached the wrong model by mistake. Please try this:\r\n\r\n[test_model.zip](https://github.com/tensorflow/tensorflow/files/5532127/test_model.zip)\r\n\r\n\r\nThere is a second issue here: note that when converting to a quantized tflite model in TF 2.2.0, the input layer's type is float32 despite setting the input layer to be int8", "Is there any update on this?", "Your code is passing float input to the Interpreter as the error message shows.\r\nYou need to pass int8 input for the quantized model.", "One more thing (not sure it is causing problem or not) but you should do same preprocessing while getting representative data, looks like you're not (compare read_and_process_image and  rep_data_gen0)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44708\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44708\">No</a>\n"]}, {"number": 44707, "title": "[TFLite] TF MatMul operator converted to an unsupported TFL 16x16 FullyConnected operator with 16x8 post-training quantization", "body": "Hello,\r\n\r\nWhen using the `tf.linalg.matmul` function with TFLite 16-bit post-training quantization the operation is sometimes transformed into a `TFL::FullyConnectedOp` operator with 16-bit inputs and 16-bit weights. Unfortunately the 16-bit version of the FC operator only supports 16-bit inputs and 8-bit weights.\r\n\r\nThe conversion occurs in two places depending on how `tf.linalg.matmul` is interpreted (as a `TF::MatMulOp` operator or a `TF::BatchMatMul(V2)Op`):\r\n* The `TF::MatMulOp` is converted to a `TFL::FullyConnectedOp` in [legalize_tf.cc](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/transforms/legalize_tf.cc#L242).\r\n* The `TF::BatchMatMul(V2)Op` on the other hand is converted in [unroll_batch_matmul.cc](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/tensorflow/transforms/unroll_batch_matmul.cc#L190) to a `TF::MatMulOp` which is then converted by the previous transformation into a `TFL::FullyConnectedOp`. This conversion pass is only enabled when [`PrepareTFPass::unfold_batch_matmul_`](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/transforms/prepare_tf.cc#L1214) is true.\r\n\r\nTo disable the second conversion we could add a check on the `toco_flags.inference_type()` flag in [graphdef_to_tfl_flatbuffer.cc](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc#L88) (and other appropriate places) and set `pass_config.unfold_batch_matmul` to false when the inference type is int16. The `TF::BatchMatMul(V2)Op` would then be converted to a `TFL::BatchMatMulOp` which support 16x16 inputs without trouble.\r\n\r\nThe problem remains though if we have a simple 16x16 `TF::MatMulOp` operator. We can't just disable the conversion in the same way as for `TF::BatchMatMul(V2)Op` as there isn't any `TFL::MatMulOp`. \r\n\r\nOne option would be to disable this transformation in int16 post-training quantization in the same way as for `TF::BatchMatMul(V2)Op` and add a `TF::MatMulOp` to `TF::BatchMatMul(V2)Op` conversion pass. The `TFL::BatchMatMulOp`  would then be used for the cases where we didn't converted the `TF::MatMulOp` to a `TFL::FullyConnectedOp`.\r\n\r\nThibaut", "comments": ["@talumbau @karimnosseir thoughts on addressing this?", "I am confused with the details you mention. If you're using post training quantization then the ops should be a floating point not int16. The int16 quantization happens after the conversion which you point too.\r\n\r\nOn the other hand\r\nSince TFLite kernel doesn't support filter with int16 type, then we should remove the type from here\r\nhttps://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/ir/tfl_ops.td#L950\r\nThen if your model has Matmul with int16 type originally it will not be converted to TFL, and will remain as TF op since the kernel doesn't support it.\r\n\r\n", "I modified the title as it was a bit confusing. The MatMul op is initially in floating-point when the conversion to `TFL::FullyConnectedOp` happens. It's later during the post-training quantization that the `TFL::FullyConnectedOp` is transformed to an unsupported 16x16 version.\r\n\r\nThe https://github.com/tensorflow/tensorflow/pull/44735 PR solved the second problem and disabled the unfolding of the `TF::BatchMatMul(V2)Op` when 16x8 post-training quantization is used.\r\n\r\nThe problem remains if we initially have a floating-point `TF::MatMulOp` which is [converted](https://github.com/tensorflow/tensorflow/blob/c86b90dc9e3dc51dfc95964449570160cb12c1fe/tensorflow/compiler/mlir/lite/transforms/legalize_tf.cc#L242) to a floating-point `TFL::FullyConnectedOp`  which is then converted to an unsupported 16x16 `TFL::FullyConnectedOp` during the quantization.\r\n\r\nIf we modify a bit the example of #44735 PR to have a `TF::MatMulOp` instead of a `TF::BatchMatMulV2Op` (a bit contrived example but it's to illustrate the problem):\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput_shape = [4, 2]\r\ninput1 = tf.keras.Input(shape=input_shape, batch_size=1)\r\ninput2 = tf.keras.Input(shape=input_shape, batch_size=1)\r\noutput = tf.keras.layers.Lambda(lambda x: tf.linalg.matmul(\r\n                                   x[0][0], x[1][0], transpose_b=True)\r\n                                )([input1, input2])\r\nmodel = tf.keras.Model(inputs=[input1, input2], outputs=output)\r\n\r\n\r\ndef get_rand_date():\r\n    return np.random.rand(1, *input_shape).astype(np.float32)\r\n\r\ndef representative_data_gen():\r\n    yield [get_rand_date(), get_rand_date()]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\ntflite_model = converter.convert()\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(interpreter.get_input_details()[0][\"index\"],\r\n                       get_rand_date())\r\ninterpreter.set_tensor(interpreter.get_input_details()[1][\"index\"],\r\n                       get_rand_date())\r\ninterpreter.invoke()\r\n```\r\n\r\nWe end-up with the following error: `RuntimeError: tensorflow/lite/kernels/fully_connected.cc:123 input->type != kTfLiteFloat32 (INT16 != FLOAT32)Node number 4 (FULLY_CONNECTED) failed to prepare.`\r\n\r\nThis problem is more minor for us (the `TF::BatchMatMulV2Op`  was more problematic) but it may be worth looking into it.", "Ethan, Can you please triage and check if PTQ still introduce this\r\n\r\nThanks", "Solved by 0546868a385cb10028f831437980934299e3d13b as we can now use `_experimental_disable_batchmatmul_unfold=True`. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44707\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44707\">No</a>\n"]}]