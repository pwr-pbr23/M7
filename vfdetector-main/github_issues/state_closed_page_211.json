[{"number": 48326, "title": "How can I train tensorflow model to take specific rows of historical training data in each training iteration that is related to current row?", "body": "I have a buses dataset between stations with the entering time for each stop. Each row has information about the bus current station including (time entering this station, long/lat, date, and also the target station with its arrival time and distance(I add a sample of data below). What I want to do is to estimate the bus arrival time to any of the following station in the route. Since the data is not recorded with time-stamp I can't use LSTM to train the model using the previous window time(for example two hours before). How can I train the model for each link between two stops with only historical records that are two hours before. Any idea to build a training function that takes the previous links with two hours before from the data as the batch size.\r\n![Screenshot from 2021-04-06 16-19-14](https://user-images.githubusercontent.com/60028253/113696247-53693800-9704-11eb-9925-2b4d347cf453.png)\r\n", "comments": ["@khaled-alkilane \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n\r\nand the the exact sequence of commands / steps that you executed before running into the problem\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48325, "title": "tensorflow-2.4.1 crash by pthread_mutex_lock while enable Hexagon ", "body": "I enable Hexagon to run tensorflow-lite by ADB, it crash everytime. In ADB shell window, the error log is: \r\nFORTIFY: pthread_mutex_lock called on a destroyed mutex (0x70d105cf70)\r\nAborted\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Android 9.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: BBK S5d (Chinese made pad for kid learning)\r\n- TensorFlow installed from (source or binary): build by myself\r\n- TensorFlow version (use command below): tensorflow-2.4.1\r\n- Python version: Python 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nThe code I use to enable Hexagon is following:\r\nconst char library_directory_path[] = \"/data/local/tmp/limd\";\r\nTfLiteHexagonInitWithPath(library_directory_path);  // Needed once at startup.\r\nTfLiteHexagonDelegateOptions params = { 2, 0, true, false};\r\nauto* delegate_ptr = TfLiteHexagonDelegateCreate(&params);\r\nif(delegate_ptr == nullptr)\r\n{\r\n    std::cout << \"error! delegate_ptr is NULL !!\" << std::endl;\r\n}\r\ntflite::Interpreter::TfLiteDelegatePtr delegate(delegate_ptr,\r\n  [](TfLiteDelegate* delegate) {\r\n    TfLiteHexagonDelegateDelete(delegate);\r\n    TfLiteHexagonTearDown();\r\n  });\r\ninterpreter->ModifyGraphWithDelegate(delegate.get());\r\n\r\n\r\nadb logcat shows:\r\n04-06 16:15:48.900  3777  3777 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1732: Successfully opened fastrpc_shell_3\r\n04-06 16:15:48.929  3777  3777 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1878: Successfully created user PD on domain 3 (attrs 0x0)\r\n04-06 16:15:48.930  3777  3781 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:277: FastRPC latency thread started for QoS\r\n04-06 16:15:48.992  3777  3777 V ./main  : vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1034: remote_handle64_open: Successfully opened handle 0x2e628a80 for file:///libhexagon_nn_skel.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3\r\n04-06 16:15:48.995  3777  3777 I tflite  : TfLiteHexagonDelegate delegate: 69 nodes delegated out of 71 nodes with 1 partitions.\r\n04-06 16:15:49.282  3777  3777 F libc    : FORTIFY: pthread_mutex_lock called on a destroyed mutex (0x7c2da59f70)\r\n04-06 16:15:49.282  3777  3777 F libc    : Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 3777 (main), pid 3777 (main)\r\n04-06 16:15:49.310  3784  3784 I crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone\r\n04-06 16:15:49.312  1023  1023 I /system/bin/tombstoned: received crash request for pid 3777\r\n04-06 16:15:49.313  3784  3784 I crash_dump64: performing dump of process 3777 (target tid = 3777)\r\n04-06 16:15:49.314  3784  3784 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n04-06 16:15:49.314  3784  3784 F DEBUG   : Build fingerprint: 'EEBBK/H7000/H7000:9.0/PKQ1.190319.001/cp08240552:userdebug/release-keys'\r\n04-06 16:15:49.314  3784  3784 F DEBUG   : Revision: '0'\r\n04-06 16:15:49.314  3784  3784 F DEBUG   : ABI: 'arm64'\r\n04-06 16:15:49.314  3784  3784 F DEBUG   : pid: 3777, tid: 3777, name: main  >>> ./main <<<\r\n04-06 16:15:49.315  3784  3784 F DEBUG   : signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n04-06 16:15:49.315  3784  3784 F DEBUG   : Abort message: 'FORTIFY: pthread_mutex_lock called on a destroyed mutex (0x7c2da59f70)'\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x0  0000000000000000  x1  0000000000000ec1  x2  0000000000000006  x3  0000000000000008\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x4  0000000000008080  x5  0000000000008080  x6  0000000000008080  x7  0000000000000038\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x8  0000000000000083  x9  a0ddb7fa2b51b658  x10 0000000000000000  x11 fffffffc7fffffdf\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x12 0000000000000001  x13 00000000606c18b5  x14 0010363e80ab4a00  x15 00009dbf4cfefd8c\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x16 0000007c2f3ed2b8  x17 0000007c2f30ea50  x18 0000000000000010  x19 0000000000000ec1\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x20 0000000000000ec1  x21 0000007c2e6ccc00  x22 0000007c2da59f58  x23 00000057483c5078\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x24 0000007c2f3f6000  x25 0000005747be4b04  x26 0000007c2f3f6000  x27 0000000000000018\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     x28 0000007c2fd983d0  x29 0000007fc348a9b0\r\n04-06 16:15:49.315  3784  3784 F DEBUG   :     sp  0000007fc348a970  lr  0000007c2f300084  pc  0000007c2f3000ac\r\n04-06 16:15:49.316  3784  3784 I unwind  : Malformed section header found, ignoring...\r\n04-06 16:15:49.317  3784  3784 F DEBUG   : \r\n04-06 16:15:49.317  3784  3784 F DEBUG   : backtrace:\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #00 pc 00000000000220ac  /system/lib64/libc.so (abort+116)\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #01 pc 000000000009377c  /system/lib64/libc.so (__fortify_fatal(char const*, ...)+120)\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #02 pc 0000000000092de4  /system/lib64/libc.so (HandleUsingDestroyedMutex(pthread_mutex_t*, char const*)+52)\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #03 pc 0000000000092c60  /system/lib64/libc.so (pthread_mutex_lock+228)\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #04 pc 000000000000ca00  /data/local/tmp/limd/libhexagon_interface.so\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #05 pc 000000000000b528  /data/local/tmp/limd/libhexagon_interface.so\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #06 pc 000000000001181c  /data/local/tmp/limd/libtensorflowlite_hexagon_jni.so\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #07 pc 000000000001197c  /data/local/tmp/limd/libtensorflowlite_hexagon_jni.so\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #08 pc 000000000010cf68  /data/local/tmp/limd/main\r\n04-06 16:15:49.317  3784  3784 F DEBUG   :     #09 pc 000000000010cc10  /data/local/tmp/limd/main\r\n04-06 16:15:49.354  1436  1584 W NativeCrashListener: Couldn't find ProcessRecord for pid 3777\r\n04-06 16:15:49.316  3784  3784 I chatty  : uid=2000(shell) crash_dump64 identical 1 line\r\n04-06 16:15:49.317  3784  3784 I unwind  : Malformed section header found, ignoring...\r\n04-06 16:15:49.355  1023  1023 E /system/bin/tombstoned: Tombstone written to: /data/tombstones/tombstone_49\r\n04-06 16:15:49.351  3777  3777 W main    : type=1400 audit(0.0:92): avc: denied { search } for name=\"core\" dev=\"mmcblk0p59\" ino=917506 scontext=u:r:shell:s0 tcontext=u:object_r:bee_core_data_file:s0 tclass=dir permissive=0\r\n\r\n\r\nCould anyone help? Or give a advice?\r\nThank you so much~\r\n", "comments": ["@karimnosseir could you take a look?", "Can you share more details about the device that you tried running the delegate on ?\r\nWhat is the SoC on this device ?\r\nAlso, did you try running on another device ? Did you get the same behavior on different devices ?\r\n\r\nI don't have this device, so i am trying to find a way to reproduce this, so that i can see what is going on.\r\n\r\nThanks", "My deviece chip is qualcomm Snapdragon 660, mobile platform.\r\nSorry, I have no other device.", "Thanks @limdlh. \r\n\r\nFew questions:\r\n\r\n- Did the failure happens on multiple models or only one ?\r\n- Can you share the model ? or a sample model that happen to cause this issue ?\r\n- Can you try running the tflite benchmark and see if the issue happens or not, see [instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark) (please don't forget the part about \"install Hexagon libraries on device\"\r\n\r\nThanks", "@karimnosseir\r\nHow could I share code and model with you? Can you share me your E-mail?\r\nI have only one model support DSP now", "Same github username then at google dot com", "I have send you mail, please receive.\r\nI will try running the tflite benchmark soon.", "Hi @limdlh \r\nI got the files and i got a phone with Snapdragon 660. I ran benchmark tool with the model and it ran successfully.\r\n\r\n", "Hi @limdlh \r\n\r\nBased on testing on phones with similar chip. I think the issue comes from your setup/code.\r\nI am going to close the issue. \r\nPlease let me know if you still have questions.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48325\">No</a>\n"]}, {"number": 48324, "title": "MultiWorkerMirroredStrategy:Training is slow with 50 workers", "body": "I am trying to use 50 workers to train my model and made it.However,training is very slow.I'd like to know if there are some tips to \r\nimprove the performance in terms of reducing training time.I would appreciate it if you could give me some advice.", "comments": ["@Mullich123 \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the[ Github new issue template.](https://github.com/tensorflow/tensorflow/issues/new)\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Thanks for you reply,I've got some tips on Tensorflow tutorial and it's sorry to trouble you for my carelessness.I will change next time.", "@Mullich123 \r\nIts absolutely no issues, please let us know the issue faced and share details for us to help.", "Sorry I am a newbie in Tensorflow2.Could you please delete this issue.", "@Mullich123 \r\nPlease confirm if your issue is resolved.", "Good night\r\n\r\nI'm so sorry.It's my default.I raised the issue in a wrong way.By debugging my code\r\nand referencing tutorials,I found I knew too little.In term of the issue,I use 50 ports\r\nof localhost to do experiment and make it ambiguous.So please delete it and I would&nbsp;\r\nraise another questions carefully.\r\n\r\n\r\nYours Sincerely,\r\nMr. Mullich"]}, {"number": 48323, "title": "[TFLite/nnapi delegate] add rank restriction for reshape op", "body": "Android NN API only supported tensor rank of reshape op: up to 4.\r\nhttps://github.com/tensorflow/tensorflow/issues/47546", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48323) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 48322, "title": "fake_quant_with_min_max_vars can only get five significant figures instead of six", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): NO\r\n- TensorFlow version (use command below):  2.4.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): NO\r\n- GCC/Compiler version (if compiling from source): NO \r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla V100-SXM2-32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.4.0-49-22-g85c8b2a817f 2.4.1\r\n\r\n**Describe the current behavior**\r\n\"tf.quantization.fake_quabt_with_min_max_vars\" can only get five significant figures.\r\n\r\n**Describe the expected behavior**\r\n\"tf.quantization.fake_quabt_with_min_max_vars\" should get six significant figures when float32 has six significant figures.\r\nWith regard to pytorch \"torch.fake_quantize_per_tensor_affine\" always has six significant figures.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nimport tensorflow as tf\r\n\r\nmin_var = -10.5268\r\nmax_var = 15.1868\r\n\r\noutput_1 = tf.quantization.fake_quant_with_min_max_vars(0.27, min_var, max_var, num_bits=8, narrow_range=True)\r\nprint(output_1 ) #output_1 is 0.3037033\r\n\r\noutput_2 = tf.quantization.fake_quant_with_min_max_vars(-0.27, min_var, max_var, num_bits=8, narrow_range=True)\r\nprint(output_2 ) #output_2 is -0.30370426\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@xueyedamo521 \r\nThis issue is been worked upon and we will update you at the earliest, apologizes for closing it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48322\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48322\">No</a>\n", "@xueyedamo521 \r\nPlease confirm if this is a duplicate of #48223 and move to close status if its a duplicate.", "Yes, thanks for reply and wait for further solution.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48322\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48322\">No</a>\n"]}, {"number": 48321, "title": "Outdates or Broken Link in  tensorflow/examples documentation ", "body": "While going through  TensorFlow/examples/CONTRIBUTING.md i found out some broken or outdated links in the docs I would like to fix it with the correct once, *please assign the issue to me*", "comments": ["Fixed this issue with [this ](https://github.com/tensorflow/examples/pull/303) PR please review it and merge"]}, {"number": 48320, "title": "Track SignatureDef in TFlite models built using lite.TFLiteConverter.from_concrete_functions()", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.5.0-rc0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Description**\r\n- TF2.5 introduces `get_signature_runner()` API which allows `tensorflow.lite.interpreter` to operate on the model using user provided input an output names.\r\n- SignatureDef is captured in the tflite models in the conversion process using `tensorflow.lite.TFLiteConverter.from_saved_model()` \r\n- However, `tensorflow.lite.TFLiteConverter.from_concrete_functions()` doesn't yet seem to capture the SignatureDef \r\n\r\nReproduced here: https://gist.github.com/avroshk/87f4ad6232118f52500238e75caedba2\r\n\r\nIn reference to the conversation here: https://github.com/tensorflow/tensorflow/issues/32180#issuecomment-813737802\r\n\r\n**Will this change the current api? How?**\r\n- This feature should not need to change anything in the current api. It will help bring TFLite conversion methods at par in terms of signature_def availability \r\n\r\n\r\n", "comments": ["Thank you for filing this feature request :-) @karimnosseir @MeghnaNatraj ", "Looking for the progress of this request as well +1", "We currently track signatureDefs in the SavedModel and concrete functions workflows. Refer to the following snippet:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('tf-version:', tf.__version__)\r\n\r\nclass Test(tf.Module):\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\r\n    def stats(self, sample_input):\r\n        return {\r\n            'mean': tf.math.reduce_mean(sample_input, axis=0),\r\n            'std': tf.math.reduce_std(sample_input, axis=0)\r\n        }\r\n\r\nmodel = Test()\r\n\r\n# Convert the SavedModel using TFLiteConverter\r\nSAVED_MODEL_PATH = 'content/saved_models/coding'\r\ntf.saved_model.save(\r\n    model, SAVED_MODEL_PATH,\r\n    signatures={\r\n      'stats': model.stats.get_concrete_function()\r\n    })\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\r\ntflite_model = converter.convert()\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\nsignatures = interpreter.get_signature_list()\r\nprint(\"[SavedModel] signatures: \", signatures)\r\n\r\n# Convert the concrete functions using TFLiteConverter\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n    [model.stats.get_concrete_function()], model)\r\ntflite_model = converter.convert()\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\nsignatures = interpreter.get_signature_list()\r\nprint(\"[Interpreter] signatures: \", signatures)\r\n```\r\n \r\nOutput:\r\n```\r\ntf-version: 2.8.0\r\n[SavedModel] signatures:  {'stats': {'inputs': ['sample_input'], 'outputs': ['mean', 'std']}}\r\n[Interpreter] signatures:  {'serving_default': {'inputs': ['sample_input'], 'outputs': ['mean', 'std']}}\r\n```"]}, {"number": 48319, "title": "tensorflow illegal instruction error", "body": "hello,\r\nwhen importing tensorflow i am getting an illegal instruction error.\r\nmine is new model CPU ,but AVX instruction are missing?What should i do?plz help", "comments": ["@greeshmapremaraj \r\nCould you please create a new virtual environment and check if you are facing the same issue in that as well? [to check your AVx refer to [link](https://stackoverflow.com/questions/37480071/how-to-tell-if-a-linux-machine-supports-avx-avx2-instructions), [link1](https://www.quora.com/How-can-I-determine-if-my-CPU-supports-AVX-instructions#:~:text=If%20it%20doesn't%20trigger,to%20use%20the%20AVX%20version.&text=If%20you're%20using%20Windows,in%20the%20'Instructions'%20box.)\r\n\r\nYou may refer to these issues as well and let us know: #45744,#46496, [link](https://github.com/tensorflow/tensorflow/issues/47720#issuecomment-797249831)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48318, "title": "Change sample to example in TextVectorization", "body": "I made an issue about this, and Mark Daoust says \"Example\" would be a slightly better fit. \r\n\r\nTherefore I changed all \"sample\" to \"example\" in the docstr of TextVectorization.\r\n\r\nFix #48298 ", "comments": ["@gqqnbig  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned should be fixed. Please check."]}, {"number": 48317, "title": "MultiWorkerMirroredStrategy how to use GPU and something about datashard", "body": "1.When I used MultiWorkerMirroredStrategy to train my model,it works normally but didn't use GPUs,so I supposed if I omit something about the config of GPUs of the Strategy.\r\n2.I wonder if the Strategy would do datashard automatically for me if I do nothing but just send dataset to 'model.fit' without \r\nusing tf.dataset?\r\n\r\n\r\n", "comments": ["@Mullich123,\r\nPlease take a look at the 'Multi-worker training with Keras' tutorial which has information regarding both [MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#overview) and [dataset sharding](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding), and let us know if it helps. Thanks!", "Thanks for your reply.It did a great help.", "@Mullich123,\r\nThank you for the update.\r\n\r\nClosing this issue as it is resolved. Please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48317\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48317\">No</a>\n"]}, {"number": 48316, "title": "Error validating data cardinality when fitting the model", "body": "\r\n**System information**\r\n- I am writing code to create models using tensorflow\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version 2.4.1\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: not installed\r\n- GPU model and memory: GeForce 940MX (256MB)\r\n\r\nSystem information extracted from tf_env_collect.sh\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6260387/tf_env.txt)\r\n\r\n**Current behavior**\r\nWhen fitting the model, inputs and outputs are joined in single tuple before calling the function _check_data_cardinality in tensorflow/python/keras/engine/data_adapter.py. Then, inside the _check_data_cardinality function, a flatten method is called for joined x and y. Since input and output are of different dimension, this results in error still inside _check_data_cardinality function. I have tried input and output manipulation to prevent the error, since this could be caused by invalid input and/or output format, but had no success.\r\n\r\n**Expected behavior**\r\nDo not throw an error, do not mix dimension validation of input and output together.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1qZG82zYoir92D0cIVZYwihvQJtC_L66O?usp=sharing\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-1-1810d9a9a642> in <module>()\r\n     31 \r\n     32 \r\n---> 33 test_concatenated_neural_network()\r\n\r\n4 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _check_data_cardinality(data)\r\n   1527           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\r\n   1528     msg += \"Make sure all arrays contain the same number of samples.\"\r\n-> 1529     raise ValueError(msg)\r\n   1530 \r\n   1531 \r\n\r\nValueError: Data cardinality is ambiguous:\r\n  x sizes: 3, 3, 3, 3, 3, 3, 3, 3\r\n  y sizes: 4\r\nMake sure all arrays contain the same number of samples.\r\n```\r\n", "comments": ["@andregarcia \r\nAfter making few changes to the code it works fine, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/ac1f4db217da2280595e54cce9284dc2/untitled583.ipynb) and let us know.", "Thank you very much @Saduf2019, it works.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48316\">No</a>\n"]}, {"number": 48315, "title": "[r2.5 cherry-pick] Fix Keras Callbacks logs / numpy_logs sync", "body": "This PR cherry-picks #47922 onto the `r2.5` branch which addresses Keras callback issues reported in #41851 and #45895.\r\nWould be great if this fix could make it into the 2.5.", "comments": []}, {"number": 48314, "title": "substantial overhead in tf.GradientTape.jacobian?", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7.9 (Nitrogen)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): conda defaults channel (using \"tensorflow-gpu\", even though only CPUs are being used at the moment)\r\n- TensorFlow version (use command below): unknown 2.4.1\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThere seems to be a substantial overhead in using the tape.jacobian(), compared to running a for-loop with tape.gradient().\r\n\r\n** When \"n_div = 3\" in the code below:\r\n\r\nForward pass took 0.015 [s]\r\njacobian took 3.562 [s]\r\n3 gradient calls took 0.055 [s]\r\n\r\n** When \"n_div = 1001\" in the code below:\r\n\r\nForward pass took 0.014 [s]\r\njacobian took 3.549 [s]\r\n1001 gradient calls took 18.359 [s]\r\n\r\n**Describe the expected behavior**\r\n\r\nThe direct use of a for-loop with tape.gradient() implies that each backpropagation takes almost the same time as the forward pass, and the total running time scales roughly linearly with \"n_div\". This agrees with my expectation.\r\n\r\nI was expecting tape.jacobian() to take, at most, the running time of the tape.gradient() with a for-loop in the worst case scenario of not utilizing any parallelism, and at best, almost the same time as one backpropagation. However, the jacobian's running time appears dominated by some overhead actions.\r\n\r\nThe server I tested the minimal code below has no GPU installed. Eventually I want to run a more complicated version of this on a cluster node with a GPU with the tape.jacobian() taking only one backpropagation time for \"n_div\" ~ 100.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef simple_func(x0, dp0):\r\n\r\n    x1 = x0 / (1 + dp0) * 1.0001\r\n\r\n    return x1\r\n\r\ndef scalar_example(n_repeat, dp0=0.0, show_print=True):\r\n\r\n    x0 = tf.constant(0.0, name='x0')\r\n    dp0 = tf.constant(dp0, name='dp0')\r\n\r\n    t0 = time.time()\r\n    x1, dp1 = x0, dp0\r\n    with tf.GradientTape(persistent=False) as tape:\r\n        tape.watch(x0)\r\n        tape.watch(dp0)\r\n\r\n        for _ in range(n_repeat):\r\n            x1 = simple_func(x1, dp1)\r\n    if show_print:\r\n        print(f'Forward pass took {time.time()-t0:.3f} [s]')\r\n\r\n    t0 = time.time()\r\n    m11 = tape.gradient(x1, x0)\r\n    if show_print:\r\n        print(f'Gradient calc for m11 took {time.time()-t0:.3f} [s]')\r\n\r\n        print('Finished')\r\n\r\n    return m11\r\n\r\ndef vector_example(n_repeat, n_div):\r\n\r\n    x0 = tf.constant(0.0, name='x0')\r\n    dp0 = tf.constant(0.0, name='dp0')\r\n\r\n    dp_offsets = np.linspace(-4e-3, +4e-3, n_div)\r\n    dp0vec = dp0 + dp_offsets\r\n\r\n    t0 = time.time()\r\n    x1, dp1 = x0, dp0vec\r\n    with tf.GradientTape(persistent=False) as tape:\r\n        tape.watch(x0)\r\n        tape.watch(dp0)\r\n\r\n        for _ in range(n_repeat):\r\n            x1 = simple_func(x1, dp1)\r\n    print(f'Forward pass took {time.time()-t0:.3f} [s]')\r\n\r\n    t0 = time.time()\r\n    m11 = tape.jacobian(x1, x0)\r\n    print(f'Gradient calc for m11 took {time.time()-t0:.3f} [s]')\r\n\r\n    print('Finished')\r\n\r\n    return m11\r\n\r\nif __name__ == '__main__':\r\n\r\n    n_repeat = 100\r\n    n_div = 1001\r\n\r\n    print(f'n_repeat = {n_repeat:d}, n_div = {n_div:d}')\r\n    t0 = time.time()\r\n    m11_list_vec = vector_example(n_repeat, n_div)\r\n    tElapsed_vec = time.time()-t0\r\n\r\n    t0 = time.time()\r\n    dp_offsets = np.linspace(-4e-3, +4e-3, n_div, dtype=np.float32)\r\n    m11_list_sca = np.array([scalar_example(n_repeat, dp0=dp0, show_print=False).numpy()\r\n                             for dp0 in dp_offsets])\r\n    tElapsed_sca = time.time()-t0\r\n\r\n    print([n_repeat, n_div, m11_list_vec.numpy(),\r\n           m11_list_sca,\r\n           tElapsed_vec, tElapsed_sca])\r\n```\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@yhidaka \r\nPlease refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/4275c32bc20bc39ec7feba8f98402302/untitled585.ipynb) for the performance on tf 2.4 and tf 2.5 rc0 and let us know.", "@Saduf2019 \r\nThank you for your comment. I ran your gist with 3 and 1001 for \"n_div\" and got the similar results for \"tElapsed_vec\" (using tape.jacobian) and \"tElapsed_sca\" (using tape.gradient) with both tf2.4 & 2.5:\r\n\r\n# tf2.4.1\r\n\r\nn_div = 3\r\nTrial-1: jac = 5.3 [s]; grad = 0.08 [s]\r\nTrial-2: jac = 5.4 [s]; grad = 0.08 [s]\r\n\r\nn_div = 1001\r\nTrial-1: jac = 6.0 [s]; grad = 27.3 [s]\r\nTrial-2: jac = 5.5 [s]; grad = 27.6 [s]\r\n\r\n# tf2.5.0-rc0\r\n\r\nn_div = 3\r\nTrial-1: jac = 5.2 [s]; grad = 0.10 [s]\r\nTrial-2: jac = 5.4 [s]; grad = 0.09 [s]\r\n\r\nn_div = 1001\r\nTrial-1: jac = 5.5 [s]; grad = 29.6 [s]\r\nTrial-2: jac = 5.4 [s]; grad = 29.7 [s]\r\n\r\n#\r\n\r\nSo, my question still remains. The vector case (using tape.jacobian) and scalar case (using tape.gradient) return exactly the same gradient array (both in terms of size [= \"n_div\"] and value), yet the first case takes nearly constant time (and apparently with significant overhead time for setup), while the computation time for the second case scales roughly proportionally to \"n_div\". Is this an expected behavior?", "I'd say that's more-or-less expected for this smaller-scale computation, since constant Python code overhead dominates it.  Using `@tf.fucntion` could give a different picture here."]}, {"number": 48311, "title": "\"A simple graph image\" in \"Introduction to graphs and tf.function\" at TF guides not loading in Safari Browser.", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/intro_to_graphs\r\n\r\nLink to the documentation entry:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/guide/intro_to_graphs.ipynb\r\n\r\n## Description of issue (what needs changing):\r\n\r\n\"A simple graph image\" in [\"Introduction to graphs and tf.function\"](https://www.tensorflow.org/guide/intro_to_graphs) not loading in Safari Browser.\r\n\r\n![screenshot](https://github.com/Suraj1199/test/blob/master/File_000.jpeg)\r\n\r\nAre you planning to also submit a pull request to fix the issue? \r\nYes", "comments": ["Here's my PR (https://github.com/tensorflow/docs/pull/1871)", "This issue will be closed once the pr is merged.", "Thanks. The fix is merged and should be reflected on tf.org after the next publishing run", "Thanks @lamberta for improving my PR. I was going to fix the lints and formatting later today but anyways you did it. @Saduf2019, the [PR](https://github.com/tensorflow/docs/pull/1872) for this issue has been merged, So it can be closed now.", "yet the problem #48311 is still there.", "Looks like the file wasn't imported last night\u2014though the job looks like it ran. cc @yashk2810 ", "Thanks for flagging. I am looking into it. Looks like there is a 48 hour lag which shouldn't happen.", "This looks fixed now. Thanks"]}, {"number": 48310, "title": "Fixed build failure with TF 2.5.0-rc0", "body": "On Linux x86_64, while building TF 2.5.0-rc0 for cuda 10.2, I encountered an error that says \"no such package @local_cuda referenced by cub\" (don't have an exact error). This PR proposes a fix for this.\r\nBut I'm surprised how this is working for everyone as the same code is also present on the master branch. And if it is working fine, then what is wrong with my environment.", "comments": ["@goldiegadde - Could you please review this?", "@npanpaliya  FYR. The `deps = [\"@local_cuda//:cuda_headers\"]` works for for CUDA 11.x because of https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L1143-L1144.  I met this problem when building recent master branch on my Jetson board which doesn't have CUDA 11 yet. This problem was introduced in 7737446695d360efd8e87ca63a26518b8787481c.\r\n\r\ntag @chsigg for 7737446\r\n\r\n", "> @npanpaliya FYR. The `deps = [\"@local_cuda//:cuda_headers\"]` works for for CUDA 11.x because of https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L1143-L1144. I met this problem when building recent master branch on my Jetson board which doesn't have CUDA 11 yet. This problem was introduced in [7737446](https://github.com/tensorflow/tensorflow/commit/7737446695d360efd8e87ca63a26518b8787481c).\r\n> \r\n> tag @chsigg for [7737446](https://github.com/tensorflow/tensorflow/commit/7737446695d360efd8e87ca63a26518b8787481c)\r\n\r\nThanks @freedomtan for the information. Yeah, I'd also found the commit that introduced this problem but was wondering how it has been working for all. Thank you for pointing the change in cuda_configure.bzl which makes it working with cuda 11.x. \r\nBut this breaks TF builds with cuda 10.2. Does it mean that cuda 10.x (specifically 10.2) won't be supported any more?", "TF releases are always tied in with a specific CUDA version.\r\n\r\nThis PR should be first opened on master, merged, wait for inclusion in tf-nightly and only then cherrypicked on the branch.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48310) for more info**.\n\n<!-- need_author_consent -->", "Closing this one and instead created a new one https://github.com/tensorflow/tensorflow/pull/48393 against master which shows clean changes."]}, {"number": 48309, "title": "Documentation for loading `Estimator` and using it to `predict`", "body": "Documentation and examples describing how to use a trained estimator to make predictions exist ([ref:boosted_trees_model_understanding](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding)). \r\n\r\nSimilarly, there are examples on how to save a trained estimator ([ref](https://www.tensorflow.org/guide/estimator))\r\n\r\nHowever, there is a dearth of examples/documentation on how to load a saved estimator, and then use it for predictions. Currently, I am using this [stack-overflow issue](https://stackoverflow.com/questions/58959582/saving-loading-and-predicting-from-a-tensorflow-estimator-model-2-0/60230173#comment118347400_60230173) to help figure out how to do this.\r\n", "comments": ["@avivajpeyi The [guide](https://www.tensorflow.org/guide/estimator#savedmodels_from_estimators) you referenced above also shows how to load an estimator and run predictions. \r\n\r\n\r\n```\r\nYou can also load and run that model, from python:\r\n\r\nimported = tf.saved_model.load(estimator_path)\r\n\r\ndef predict(x):\r\n  example = tf.train.Example()\r\n  example.features.feature[\"x\"].float_list.value.extend([x])\r\n  return imported.signatures[\"predict\"](\r\n    examples=tf.constant([example.SerializeToString()]))\r\n\r\nprint(predict(1.5))\r\nprint(predict(3.5))\r\n```\r\n\r\nIf you think you want to update the guides, please feel free to raise a PR to update those guides. Thanks!", "Hey @jvishnuvardhan, thanks: this works to predict 1 sample. **However, it is unclear from the example/documentation if this is how one is expected to get predictions for N samples.** \r\n\r\nEg. in the [boosted_trees_model_understanding guide](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding#visualizing_model_fitting), there is a predict function that takes an `Estimator` (`est`), and use's its `.predict` function to make predictions from N samples:\r\n\r\n```python\r\ndef predict(est):\r\n  \"\"\"Predictions from a given estimator.\"\"\"\r\n  predict_input_fn = lambda: tf.data.Dataset.from_tensors(dict(df_predict))\r\n  preds = np.array([p['predictions'][0] for p in est.predict(predict_input_fn)])\r\n  return preds.reshape(predict_shape)\r\n  ```\r\nThis is perfect! However, it doesn't work the same when I load the estimator. \r\n\r\n\r\nBased on the example, I tried to write a function to create a `tf.train.Example()` to pass to my loaded-estimator (in `predict_in_fn_autotrackable`). However, the time to make predictions with the loaded-estimator versus the original estimator is significantly different. \r\n\r\n\r\n  \r\n```python\r\n \r\ndef predict_in_fn_autotrackable(input_df):\r\n    examples = []\r\n    for index, row in input_df.iterrows():\r\n        feature = {}\r\n        for col, value in row.iteritems():\r\n            feature[col] = tf.train.Feature(\r\n                float_list=tf.train.FloatList(value=[value]))\r\n        example = tf.train.Example(\r\n            features=tf.train.Features(\r\n                feature=feature\r\n            )\r\n        )\r\n        examples.append(example.SerializeToString())\r\n    return tf.constant(examples)\r\n\r\n\r\ndef predict(model, data: pd.DataFrame):\r\n    if isinstance(model, AutoTrackable):\r\n        pred_fn = model.signatures['serving_default']\r\n        preds = pred_fn(predict_in_fn_autotrackable(data))\r\n        preds = preds['outputs'].numpy().flatten()\r\n    else:\r\n        predict_in_fn = lambda: tf.data.Dataset.from_tensors(dict(data))\r\n        pred_fn = model.predict\r\n        preds = np.array([p['predictions'][0] for p in pred_fn(predict_in_fn)])\r\n    return preds\r\n```\r\n\r\nBelow is a runtime-datasize plot of the original estimator's `.predict` in purple, and the loaded estimator's `.signatures['serving_default']` in red. \r\n![Screen Shot 2021-04-06 at 12 42 19 pm](https://user-images.githubusercontent.com/15642823/113651576-952eba00-96d5-11eb-9c2b-fb2adee82197.png)\r\n\r\n\r\n\r\nI think the issue lies in my `predict_in_fn_autotrackable` function. \r\n\r\n**I would love to help raise a PR to update the guides with an example of how to use a loaded estimator to make N predictions, however, I think currently I am unsure how to format my data to do so!** \r\n\r\nThanks a lot :) ", "Since the estimators follow  v1.session-style code, it is not recommended for new code and this will not receive any fixes other than security vulnerabilities. \r\nCHeck the detailed warning message [here](https://www.tensorflow.org/guide/estimator).\r\nCheck the [migration code](https://www.tensorflow.org/guide/migrate) document for details.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48308, "title": "Keras loss is stuck at the very same value during training after the first epoch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Binary via Python pip\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: Python 3.8.5\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Tesla P100-PCIE with 16 GB memory\r\n\r\n**Describe the current behavior**\r\nI am writing a Keras-based autoencoder, using my own data. That dataset includes about 20k training and about 4k validation images. All of them are very similar, all [show the very same object](https://imgur.com/a/ClbMJ0H). My model looks like this:\r\n\r\n```\r\nModel: \"autoencoder\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 300, 300, 1)]     0\r\n_________________________________________________________________\r\nencoder (Functional)         (None, 16)                5779216\r\n_________________________________________________________________\r\ndecoder (Functional)         (None, 300, 300, 1)       6176065\r\n=================================================================\r\nTotal params: 11,955,281\r\nTrainable params: 11,954,897\r\nNon-trainable params: 384\r\n_________________________________________________________________\r\nModel: \"encoder\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 300, 300, 1)]     0\r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 150, 150, 32)      320\r\n_________________________________________________________________\r\nleaky_re_lu (LeakyReLU)      (None, 150, 150, 32)      0\r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 150, 150, 32)      128\r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 75, 75, 64)        18496\r\n_________________________________________________________________\r\nleaky_re_lu_1 (LeakyReLU)    (None, 75, 75, 64)        0\r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, 75, 75, 64)        256\r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 360000)            0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 16)                5760016\r\n=================================================================\r\nTotal params: 5,779,216\r\nTrainable params: 5,779,024\r\nNon-trainable params: 192\r\n_________________________________________________________________\r\nModel: \"decoder\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_2 (InputLayer)         [(None, 16)]              0\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 360000)            6120000\r\n_________________________________________________________________\r\nreshape (Reshape)            (None, 75, 75, 64)        0\r\n_________________________________________________________________\r\nconv2d_transpose (Conv2DTran (None, 150, 150, 64)      36928\r\n_________________________________________________________________\r\nleaky_re_lu_2 (LeakyReLU)    (None, 150, 150, 64)      0\r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (None, 150, 150, 64)      256\r\n_________________________________________________________________\r\nconv2d_transpose_1 (Conv2DTr (None, 300, 300, 32)      18464\r\n_________________________________________________________________\r\nleaky_re_lu_3 (LeakyReLU)    (None, 300, 300, 32)      0\r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (None, 300, 300, 32)      128\r\n_________________________________________________________________\r\nconv2d_transpose_2 (Conv2DTr (None, 300, 300, 1)       289\r\n_________________________________________________________________\r\nactivation (Activation)      (None, 300, 300, 1)       0\r\n=================================================================\r\nTotal params: 6,176,065\r\nTrainable params: 6,175,873\r\nNon-trainable params: 192\r\n```\r\n\r\nThen I initialize my model like this:\r\n\r\n    IMGSIZE = 300\r\n    EPOCHS = 20\r\n    LR = 0.0001\r\n\r\n    (encoder, decoder, autoencoder) = ConvAutoencoder.build(IMGSIZE, IMGSIZE, 1)\r\n    sched = ExponentialDecay(initial_learning_rate=LR, decay_steps=EPOCHS, decay_rate=LR / EPOCHS)\r\n    autoencoder.compile(loss=\"mean_squared_error\", optimizer=Adam(learning_rate=sched))\r\n\r\nThen I train my model like this:\r\n\r\n    image_generator = ImageDataGenerator(rescale=1.0 / 255)\r\n    train_gen = image_generator.flow_from_directory(\r\n        os.path.join(args.images, \"training\"),\r\n        class_mode=\"input\",\r\n        color_mode=\"grayscale\",\r\n        target_size=(IMGSIZE, IMGSIZE),\r\n        batch_size=BS,\r\n    )\r\n    val_gen = image_generator.flow_from_directory(\r\n        os.path.join(args.images, \"validation\"),\r\n        class_mode=\"input\",\r\n        color_mode=\"grayscale\",\r\n        target_size=(IMGSIZE, IMGSIZE),\r\n        batch_size=BS,\r\n    )\r\n    hist = autoencoder.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, batch_size=BS)\r\n\r\nMy batch size `BS` is 32 and I start with an initial Adam learning rate of 0.001 (but I also tried values like 0.1 down to 0.0001). I also tried to increase the latent dimensionality to something like 1024, but that doesn't solve my issue either.\r\n\r\n**Describe the expected behavior**\r\nDuring training the loss goes down in the first epoch from about 0.5 to about 0.2 - and then beginning from the second epoch that loss sticks at the very same value, e.g. 0.1989, and then it stays there \"forever\", regardless of how many epochs I train and/or the initial learning rate I use.\r\n\r\nI would expect that the loss goes down a bit further, or would change with different learning rates. But as said that didn't help. Even another model layout did not solve my issue - the training loss is stuck to a value and it stays at this value forever.\r\n", "comments": ["Sorry, I found the issue.\r\nProblem was the scheduler, it seems I have miss-used it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48308\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48308\">No</a>\n"]}, {"number": 48307, "title": "TfLite Interpreter fails to execute quantized model, succeeds on non-quantized", "body": "\r\n**System information**\r\n\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): Nightly: 2.6.0-dev20210402. installed using pip install tf-nightly\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nGiven a TensorFlow model, I convert it to TfLite in three ways:\r\n1. Plain conversion without post training quantization \r\n2. Post-integer quantization when only the weights are quantized\r\n3. Full integer quantization. \r\n\r\nWhen running inference using those three models:\r\n1. the non-quantized version works as expected in 21 seconds\r\n2. the version where just the weights are quantized does not terminate even after 10 minutes\r\n3. when using the model with full integer quantization, the interpreter crashes with the following error message:\r\n\"external/ruy/ruy/apply_multiplier.cc:52: RUY_CHECK_LE condition not satisfied:   [ shift <= 7 ]   with values   [ 108 <= 7 ].\"\r\n\r\n**Describe the expected behavior**\r\n\r\nthe interpreter running the model with full integer quantization should terminate with a result similar to the non-quantized version\r\n\r\n**Standalone code to reproduce the issue**\r\nsee linked [zip file](https://drive.google.com/file/d/1c_sKgIbDzROM9DV99lN__GHYuJOlAZBS/view?usp=sharing).\r\n\r\n- quantize.py is the code used to convert and quantize TensorFlow to tflite\r\n- tfliteinf.py is the code used to run inference\r\n- poolnet_small1_tf is the original TensorFlow SavedModel\r\n- trevi_small1.bmp is the input image\r\n- expected.png is the expected output image\r\n- norm_images is a directory containing the representative dataset\r\n\r\n", "comments": ["@xhae could you triage this issue?", "I tested tflites on linux colab, it seems just working but bad result on full-integer. (maybe due to small calibration data?)\r\nI guess this bug is only happens on win10, I'll take a look.", "I've also verified the same behavior persists on Ubuntu 18.04 LTS running in docker 2.5.0. \r\nI've run python tfliteInf.py full_quant.tflite trevi_small1.bmp and got \r\n\r\nexternal/ruy/ruy/apply_multiplier.cc:52: RUY_CHECK_LE condition not satisfied:   [ shift <= 7 ]   with values   [ 108 <= 7 ].\r\n\r\nagain.", "Thanks for your update!\r\n\r\nFrom the TF docker images, tensorflow/tensorflow:2.4.1 has no problem but tensorflow/tensorflow:2.5.0rc0 has the problem what you mentioned. Maybe the changes between them causes the issue.\r\n", "Could you try conversion with `converter.experimental_new_quantizer = False` on the tf 2.5.0rc0 version and find out how it goes? If true, the new quantizer might be the problem.", "I tried conversion as instructed and got the following error during inference:\r\nRuntimeError: tensorflow/lite/kernels/pad.cc:118 op_context.input->type != op_context.constant_values->type (INT8 != FLOAT32)Node number 6 (PADV2) failed to prepare.\r\n\r\nThis was previously solved by using the new quantizer. ", "Hi @talumbau , It fails right after update ruy version here:  f779f42004d1873784cb2b4cb351d4b342ad0816\r\nWould you please take a look?", "The ruy revision bump in f779f42 contains one ruy commit likely to be the culprit here, namely:\r\nhttps://github.com/google/ruy/commit/58e3051707b3a92dab0a72297183114a1d35f483\r\n\r\nThis commit has been rolled back in the ruy repository already, in\r\nhttps://github.com/google/ruy/commit/b0e97e627db281e2f2d79ac84333a93452173ccd\r\n\r\nAnd we have since further simplified and hardened this code in\r\nhttps://github.com/google/ruy/commit/939449243eb36e5b668cc00a1c936f2b1ad4dc27\r\n\r\nIf someone is able to reproduce this issue and willing to help, it would be very helpful if you could try to see if bumping the ruy reference to something containing the above fixes, solves the problem. You can follow f779f42 as a template, just update the ruy commit hashes and the associated sha256 and md5 hashes (just download the .zip from github and run `sha256sum` and `md5sum` on it). Don't worry that the URL under https://storage.googleapis.com/mirror.tensorflow.org/ doesn't exist yet, it's optional and will get auto-created after the fact.\r\n\r\n", "@yakovdan The ruy commit has been updated to the latest one in TF code. Can you update to the latest nightly and check?", "@srjoglekar246 \r\nno change. inference output is still: \r\n\r\nexternal/ruy/ruy/apply_multiplier.cc:52: RUY_CHECK_LE condition not satisfied:   [ shift <= 7 ]   with values   [ 108 <= 7 ].\r\n\r\njust to be on the safe side: the output of `tf.__version__` is\r\n'2.6.0-dev20210408'", "i'm out of the office and not actively working on tflite or ruy. @jdduke could help find an owner.", "Hi @jianlijianli, are there any updates on this?", "Hi yakovdan, we bumped the ruy version in https://github.com/tensorflow/tensorflow/commit/ff72b005814d18aa539b93fdecd9476ec311160e to include the rollbacks bjacob mentioned. Could you please double check that change is reflected in your codebase? I was able to run \" tfliteInf.py full_quant.tflite trevi_small1.bmp\" with that change and produce the following result without any crash:\r\n\r\nrun tflite model on input image\r\nINFO: Initialized TensorFlow Lite runtime.\r\nmin,max:  0.011019315 1.7739744\r\n28.365509510040283", "Sure. Can you instruct me how to check this? is it enough to pip install tf-nightly or do I need to build from latest source?", "Thanks yakovdan, I think pip install tf-nightly should be enough. Is it possible to look at the TF source code in your set up? If there is the \"ruy::ScopedSuppressDenormals suppress_denormals;\" at line 289 in \"${tensorflow_path}/tensorflow/lite/interpreter.cc\" (or any files gets updated in the same PR), it means the version bump https://github.com/tensorflow/tensorflow/commit/ff72b005814d18aa539b93fdecd9476ec311160e is included in your code.", "if I just do pip install, the installation does not contain the source code. Does this mean I need to build from source?", "So, I've verified `ruy::ScopedSuppressDenormals suppress_denormals;` is in interpreter.cc on line 289. \r\nI've also added a print to stdout before this line to verify that it's exectued. The code now reads:\r\n\r\n```\r\n  std::cout << \"YakovDan test\" << std::endl;\r\n  ruy::ScopedSuppressDenormals suppress_denormals;\r\n```\r\n\r\nafter compling the source ,building the pip package and pip installing it, the execution now results in the following:\r\n\r\n>YakovDan test\r\n>external/ruy/ruy/apply_multiplier.cc:52: RUY_CHECK_LE condition not satisfied:   [ shift <= 7 ]   with values   [ 108 <= 7 ]\r\n\r\n@jianlijianli Please assist.", "Hi yakovdan, thanks for the update and sorry for the delay.\r\n\r\nAnd thanks for confirming the ruy version is updated in your environment. We looked deeper into the model and found the culprit. The conv (node id 9; after the first \"split\") in basic_tflite.tflite has inputs that are extremely small (e.g. -1.1937661617583117e-41), which caused large scales in full_quant.tflite. When the model runs on the conv kernel which calls ruy library, the large value (hence big exponent) cause the RUY_CHECK_LE failure.\r\n\r\nWe are looking into several options to fix the issue (and removing the ruy check is one option). Will update this thread soon. Thanks.", "@jianlijianli @liufengdb \r\nHi, are there any updates on this?", "Hi yakovdan, thanks for checking. Since we have pin-pointed the issue, @talumbau (assigned in the bug as well) is looking into the RUY code for a proper solution.\r\n\r\n(In the meantime, may I ask if the small value is expected or not? We need to improve RUY either way but just curious)", "Hi, any progress on this ?\r\n\r\nThe small values on the input to node 9 are not expected. \r\n", "Hi yakovdan, sorry for the delayed reply. We have done internal discussion and submitted https://github.com/google/ruy/commit/70e3b87abb64f94441d00f8cf1824c8cc4db61a0. Could you please try your model with tf nightly? Thanks.", "Hi @yakovdan we removed the check on the Ruy side, as @jianlijianli mentions, and I also just submitted this commit:\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/9d25fe457e7b1fd947bd098f0b84c03967fcd6c7\r\n\r\non the TF side so that TF Lite will build with an updated Ruy that includes the new commit. Just a heads up about performance: if you are running a performance intensive model on an x86 machine, I would highly suggest not using gcc that comes with the TF nightly docker image to build and run TF Lite. The TF Nightly docker image has a default GCC version that is lower than version 9. For current  TF Lite, the best performance on x86 comes from using GCC 9 or above. We hope to update the compiler on the TF nightly docker image soon. Hope that helps!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48307\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48307\">No</a>\n"]}, {"number": 48306, "title": "TFLite: Manipulating the hardware accelerators is not allowed in the Task library currently. Only CPU is allowed", "body": "Hello, \r\n\r\nI am trying to run the image classification code [here](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android). It works fine until I switch to the NNAPI option in the device. When I do that it throws the following error:\r\n```\r\n2021-04-05 15:26:22.086 23313-23349/org.tensorflow.lite.examples.classification E/tensorflow: ClassifierActivity: Failed to create classifier.\r\n    java.lang.IllegalArgumentException: Manipulating the hardware accelerators is not allowed in the Task library currently. Only CPU is allowed.\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:169)\r\n        at org.tensorflow.lite.examples.classification.tflite.ClassifierQuantizedEfficientNet.<init>(ClassifierQuantizedEfficientNet.java:33)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:90)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:147)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.lambda$onInferenceConfigurationChanged$0$ClassifierActivity(ClassifierActivity.java:126)\r\n        at org.tensorflow.lite.examples.classification.-$$Lambda$ClassifierActivity$83lGy2TUjuj0M5n4BhMB9qlLgSY.run(Unknown Source:8)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:233)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)\r\n```\r\n\r\nI am using Xiaomi Mi 11 as the test device (Snapdragon 888, Adreno 660) with the latest updates. Could you please help me with this issue?\r\n\r\nThanks!", "comments": ["@lintian06 could you triage this issue?", "The Java API doesn't support acceleration yet, but we are working actively to enable it. Please stay tuned.", "Hi, I'm also working on using a object detection on the snapdragon 888. Have you already been able to make use of the hexagon dsp ?  Maybe via the hexagon 1.21 delegate or nnapi ? ", "Through Task library, it's not supported yet. But you can leverage the TFLite Interpreter API. See the example [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java). Follow the instruction [here](https://www.tensorflow.org/lite/performance/hexagon_delegate#hexagon_delegate_java_api) to add the hexagon delegate. ", "Thanks for your answer. So if I understand correctly you are using the hexagon 1.21 delegate example from tensorflow ? For me it keeps telling me about incompatibility issues with the version so I thought it might be because of the fact that snapdragon 888 is not yet supported ?!", "@karimnosseir Can you please help to take a look at this issue? Thanks!", "This is similar to https://github.com/tensorflow/tensorflow/issues/47246\r\n888 is a new hardware and different that doesn't work with Hexagon NN.", "@lu-wang-g  do you want to keep the issue for Java API acceleration support.", "Thanks @karimnosseir! I'll close the issue for now. Stay tuned for the upcoming updates about the Java acceleration support. Feel free to reopen this issue for any questions. "]}, {"number": 48305, "title": "OperatorNotAllowedInGraphError", "body": "- TensorFlow version :2.4.0(gpu)\r\n\r\nI am a beginner to tensorflow, when I run the following code,it report error:     OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nimport numpy as np\r\n\r\ndef create_placeholders(n_x):\r\n    x = tf.placeholder(tf.float32,  [None, n_x], name = \"x\")    \r\n    return x\r\n\r\ndef get(x):\r\n    [r, c] = tf.shape(x)\r\n    mat = tf.zeros([r, c])\r\n    return np.array(mat, dtype='float32')\r\n\r\n@tf.function\r\ndef model(h):\r\n    (m, n_x) = h.shape\r\n    x = create_placeholders(n_x) \r\n    with tf.name_scope(\"x_layer\"):\r\n        z = get(x)\r\n\r\n    with tf.Session() as sess:\r\n        v = tf.Variable(tf.ones([2,3]), name=\"v\") \r\n        z = sess.run([z], feed_dict={x: v})           \r\n\r\nh = tf.Variable(tf.ones([3,3]), name=\"h\")\r\nmodel(h)\r\n```\r\n\r\nI don't know what's wrong in code\uff0c how can i solve it?", "comments": ["Error track\uff1a\r\n```\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:\r\n\r\n    /media/D/ly/code/IEEE_TGRS_GCN-master/IEEE_TGRS_GCN-master/practice.py:19 model  *\r\n        z = get(x)\r\n    /media/D/ly/code/IEEE_TGRS_GCN-master/IEEE_TGRS_GCN-master/practice.py:10 get  *\r\n        [r, c] = tf.shape(x)\r\n    /home/ly/anaconda3/envs/tensorflow-gpu2.4.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:505 __iter__\r\n        self._disallow_iteration()\r\n    /home/ly/anaconda3/envs/tensorflow-gpu2.4.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:498 _disallow_iteration\r\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    /home/ly/anaconda3/envs/tensorflow-gpu2.4.0/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:476 _disallow_when_autograph_enabled\r\n        \" indicate you are trying to use an unsupported feature.\".format(task))\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\n```", "@1165628028 \r\n\r\nCan you please upgrade to tf 2.5 and let us know if you face the error, you may also refer to below similar issues:\r\n\r\n#45079, [link](https://stackoverflow.com/questions/62068323/iterating-over-tf-tensor-is-not-allowed-autograph-is-disabled-in-this-function)", "> @1165628028\r\n> \r\n> Can you please upgrade to tf 2.5 and let us know if you face the error, you may also refer to below similar issues:\r\n> \r\n> #45079, [link](https://stackoverflow.com/questions/62068323/iterating-over-tf-tensor-is-not-allowed-autograph-is-disabled-in-this-function)\r\n\r\nOk, i will try it. Thanks", "I try it in tf2.5, but it also report same error: \r\n`    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.`\r\n", "The error lie in : def get(x)\r\n`    [r, c] = tf.shape(x) `, i don't know why this  sentence is wrong", "I am able to replicate the issue reported on tf 2.4,tf 2.5 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/0e09ff8f6996796a9863ec2039e24bdb/untitled585.ipynb).", "> I am able to replicate the issue reported on tf 2.4,tf 2.5 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/0e09ff8f6996796a9863ec2039e24bdb/untitled585.ipynb).\r\n\r\nThank you very much for your help, maybe it's a syntax problem, not a version problem", "@1165628028 \r\nPlease confirm if the issue is resolved, if yes move this issue to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> The error lie in : def get(x)\r\n> ` [r, c] = tf.shape(x)`, i don't know why this sentence is wrong\r\n\r\n`tf.shape` method gives dynamic shape of tensor which fails when tuple unpacking is triggered.\r\n`.shape` method provides static shape of tensor in the form of tuples.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48305\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48305\">No</a>\n"]}, {"number": 48304, "title": "Fix typo in image_ops_impl.py", "body": "radomize -> randomize", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48304) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac", "> We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\n\r\nUnderstood. Is there a simpler/cheaper process to suggest documentation fixes through?", "Please fix all typos in file/directory instead of one by one."]}, {"number": 48303, "title": "Fixes a bug in example code", "body": "The sample implementation of representative_dataset() in the full integer quantization section shows a datatype of tf.float32 instead of np.float32.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48303) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@Nirzak From what I observed `tf.data.Dataset.from_tensor_slices((images)).batch(1).take(100)` returns a list of EagerTensors. Threrefore, when I run the example code it throws:\r\n\r\n`'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'astype'`\r\n\r\nI assume it was a copy & paste error from the second example that operates on numpy arrays.\r\n\r\nBut I just realized that my fix is also incomplete because I changed the iterable from `tf.data.Dataset.from_tensor_slices((images)).batch(1).take(100)` to a numpy array via `images[0:100]` to make it work on my end. \r\n\r\nSo the correct fix replaces `[data.astype(tf.float32)]` with `[tf.dtypes.cast(data, tf.float32)]`. I will update my PR accordingly.", "@Nirzak I have updated my PR accordingly."]}, {"number": 48301, "title": "tensorflow MLIR binaries build fails due to missing dependency declarations", "body": "With the TF git version `a83625aff5add82a150b41922ff99ad0f27fbb35` (as of Apr 5), MLIR binaries fail to build:\r\n\r\n$ bazel build  --config=monolithic --config=noaws  --copt=-UNDEBUG --linkopt='-fuse-ld=lld'  tensorflow/compiler/mlir:all\r\n\r\nERROR: /home/uday/tensorflow-upstream/tensorflow/compiler/mlir/hlo/BUILD:496:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/hlo:hlo_dialect_registration':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/init.cc':\r\n  'bazel-out/k8-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'\r\nINFO: Elapsed time: 2.001s, Critical Path: 1.51s\r\nINFO: 395 processes: 159 remote cache hit, 70 internal, 166 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\ngcc (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5)\r\nbazel 3.7.2\r\nCentOS 8 Linux\r\n\r\nCC: @jurahul @joker-eph ", "comments": ["This initially looked different to the one at  https://github.com/tensorflow/tensorflow/issues/48004 - but I realized this was due to  bazel disk cache staleness at my end.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48301\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48301\">No</a>\n"]}, {"number": 48300, "title": " TypeError: '>' not supported between instances of 'NoneType' and 'float'", "body": "System Information\r\n\r\n- Running in Google Collab\r\n- Python 3.7\r\n- TensorFlow Version: 2.1.0\r\n- Keras Version: 2.3.0\r\n- Notebook is set to run on GPU\r\n![P1](https://user-images.githubusercontent.com/71000110/113538851-85aa5680-9599-11eb-8dfd-e8075f1d0612.PNG)\r\n\r\nWhen I attempted to run the code above I got this error message:\r\n\r\n![P2](https://user-images.githubusercontent.com/71000110/113538936-bee2c680-9599-11eb-8012-a6b2a647d471.PNG)\r\n\r\nAny thoughts on how to solve this?\r\n\r\n\r\n", "comments": ["@AndrewTheReese \r\nPlease share simple stand alone code such that we can replicate the issue reported or a colab gist with the error, the snapshots shared do not help is searching the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48299, "title": "boringssl error : invalid use of incomplete type 'BIO'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: raspberrypi 3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.3, 2.4.1, 2.0.0 (these versions are tried)\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: by bazel\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): local : 7.5.0\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nusing openssl version 1.0.2g, 1.1.1h, 1.1.1k ..., all trial has been failed.\r\nI couldn't complete tensorflow lite lib building using below command, any suggestion would be really helpful regard to this issue.\r\n(I tried boringssl version downgrade / upgrade, but it didn't worked)\r\n![tried_version_boringssl](https://user-images.githubusercontent.com/50652715/113537568-06a63400-9614-11eb-8a31-fcaf4467a3aa.png)\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nsudo bazel build \\\r\n--config=elinux_armhf \\\r\n--config=monolithic \\\r\n--config=noaws \\\r\n--config=nohdfs \\\r\n--config=nonccl \\\r\n--config=nogcp \\\r\n--config=v1 \\\r\n--copt=-O \\\r\n-c opt \\\r\n--verbose_failures \\\r\n--define=tensorflow_mkldnn_contraction_kernel=0 \\\r\n--define=with_xla_support=false \\\r\n--define=with_gcp_support=false \\\r\n--define=tflite_convert_with_select_tf_ops=true \\\r\n--define=with_select_tf_ops=true \\\r\n//tensorflow/lite:libtensorflowlite.so\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n/home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/boringssl/BUILD:137:1: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.1.0-linux-x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages \\\r\n    TF2_BEHAVIOR=0 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/lib/gcc/arm-linux-gnueabihf/8.3.0/include -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/lib/gcc/arm-linux-gnueabihf/8.3.0/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/arm-linux-gnueabihf/include/c++/8.3.0/ -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/arm-linux-gnueabihf/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/bio_ssl.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/bio_ssl.pic.o' -fPIC -iquote external/boringssl -iquote bazel-out/armhf-opt/bin/external/boringssl -isystem external/boringssl/src/include -isystem bazel-out/armhf-opt/bin/external/boringssl/src/include -w -DAUTOLOAD_DYNAMIC_KERNELS -O '-std=c++14' -DOPENSSL_NO_ASM -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/boringssl/src/ssl/bio_ssl.cc -o bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/bio_ssl.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'SSL* get_ssl(BIO*)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:16:37: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n   return reinterpret_cast<SSL *>(bio->ptr);\r\n                                     ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_read(BIO*, char*, int)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:40:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->retry_reason = BIO_RR_ACCEPT;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:45:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->retry_reason = BIO_RR_CONNECT;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_write(BIO*, const char*, int)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:80:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->retry_reason = BIO_RR_CONNECT;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'long int ssl_ctrl(BIO*, int, long int, void*)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:101:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->shutdown = num;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:102:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->ptr = ptr;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:103:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->init = 1;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:107:17: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       return bio->shutdown;\r\n                 ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:110:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n       bio->shutdown = num;\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'int ssl_free(BIO*)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:148:10: error: invalid use of incomplete type 'BIO' {aka 'struct bio_st'}\r\n   if (bio->shutdown) {\r\n          ^~\r\nIn file included from /usr/include/openssl/crypto.h:25,\r\n                 from /usr/include/openssl/comp.h:16,\r\n                 from /usr/include/openssl/ssl.h:17,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\n/usr/include/openssl/ossl_typ.h:79:16: note: forward declaration of 'BIO' {aka 'struct bio_st'}\r\n typedef struct bio_st BIO;\r\n                ^~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc: At global scope:\r\nexternal/boringssl/src/ssl/bio_ssl.cc:170:25: error: variable 'const BIO_METHOD ssl_method' has initializer but incomplete type\r\n static const BIO_METHOD ssl_method = {\r\n                         ^~~~~~~~~~\r\nIn file included from /usr/include/openssl/ssl.h:18,\r\n                 from external/boringssl/src/ssl/bio_ssl.cc:10:\r\nexternal/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected identifier before numeric constant\r\n long BIO_set_ssl(BIO *bio, SSL *ssl, int take_owership) {\r\n      ^~~~~~~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:177:6: error: expected ',' or '...' before numeric constant\r\nexternal/boringssl/src/ssl/bio_ssl.cc: In function 'long int BIO_ctrl(BIO*, int)':\r\nexternal/boringssl/src/ssl/bio_ssl.cc:178:39: error: 'take_owership' was not declared in this scope\r\n   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);\r\n                                       ^~~~~~~~~~~~~\r\nexternal/boringssl/src/ssl/bio_ssl.cc:178:54: error: 'ssl' was not declared in this scope\r\n   return BIO_ctrl(bio, BIO_C_SET_SSL, take_owership, ssl);\r\n                                                      ^~~\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nINFO: Elapsed time: 8014.552s, Critical Path: 118.45s\r\nINFO: 6038 processes: 6038 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["@jaytoone \r\nCan you please try on virtual env and let us know.", "@Saduf2019 \r\nwith virtual env, I got below error\r\n![virtualenv](https://user-images.githubusercontent.com/50652715/113793652-c6fe5a00-9783-11eb-896e-b232cf5169a3.png)\r\n\r\n\r\nerror log changed, but occurs in borringssl too\r\n--------------- error head --------------------\r\n/home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/boringssl/BUILD:137:1: C++ compilation of rule '@boringssl//:ssl' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.1.0-linux-x86_64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/ubuntu/.local/bin/.virtualenvs/tf_bazel/bin/python3 \\\r\n    PYTHON_LIB_PATH=/home/ubuntu/.local/bin/.virtualenvs/tf_bazel/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=0 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_ENABLE_XLA=1 \\\r\n  /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/lib/gcc/arm-linux-gnueabihf/8.3.0/include -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/lib/gcc/arm-linux-gnueabihf/8.3.0/include-fixed -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/arm-linux-gnueabihf/include/c++/8.3.0/ -isystem /home/ubuntu/.cache/bazel/_bazel_root/d207b6c8dd83044074c0d1d7461cfde0/external/armhf_linux_toolchain/arm-linux-gnueabihf/libc/usr/include/ -isystem /usr/include/python3.5 -isystem /usr/include/ -MD -MF bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/dtls_method.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/dtls_method.pic.o' -fPIC -iquote external/boringssl -iquote bazel-out/armhf-opt/bin/external/boringssl -isystem external/boringssl/src/include -isystem bazel-out/armhf-opt/bin/external/boringssl/src/include -w -DAUTOLOAD_DYNAMIC_KERNELS -O '-std=c++14' -DOPENSSL_NO_ASM -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/boringssl/src/ssl/dtls_method.cc -o bazel-out/armhf-opt/bin/external/boringssl/_objs/ssl/dtls_method.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from external/boringssl/src/ssl/../crypto/internal.h:112,\r\n                 from external/boringssl/src/ssl/dtls_method.cc:65:\r\nexternal/boringssl/src/include/openssl/ex_data.h:181:52: error: macro \"CRYPTO_cleanup_all_ex_data\" passed 1 arguments, but takes just 0\r\n OPENSSL_EXPORT void CRYPTO_cleanup_all_ex_data(void);\r\n                                                    ^\r\nIn file included from external/boringssl/src/ssl/../crypto/internal.h:114,\r\n                 from external/boringssl/src/ssl/dtls_method.cc:65:\r\nexternal/boringssl/src/include/openssl/thread.h:120:41: error: macro \"CRYPTO_num_locks\" passed 1 arguments, but takes just 0\r\n OPENSSL_EXPORT int CRYPTO_num_locks(void);\r\n                                         ^\r\nexternal/boringssl/src/include/openssl/thread.h:131:55: error: macro \"CRYPTO_get_locking_callback\" passed 1 arguments, but takes just 0\r\n OPENSSL_EXPORT void (*CRYPTO_get_locking_callback(void))(int mode, int lock_num,\r\n                                                       ^\r\nexternal/boringssl/src/include/openssl/thread.h:176:45: error: macro \"CRYPTO_get_dynlock_create_callback\" passed 1 arguments, but takes just 0\r\n     *CRYPTO_get_dynlock_create_callback(void))(const char *file, int line);\r\n                                             ^\r\nexternal/boringssl/src/include/openssl/thread.h:179:60: error: macro \"CRYPTO_get_dynlock_lock_callback\" passed 1 arguments, but takes just 0\r\n OPENSSL_EXPORT void (*CRYPTO_get_dynlock_lock_callback(void))(\r\n                                                            ^\r\nexternal/boringssl/src/include/openssl/thread.h:183:63: error: macro \"CRYPTO_get_dynlock_destroy_callback\" passed 1 arguments, but takes just 0\r\n OPENSSL_EXPORT void (*CRYPTO_get_dynlock_destroy_callback(void))(\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48299\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48299\">No</a>\n"]}, {"number": 48298, "title": "Sample or example?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nLots of the description use the word _sample_. However, [Google Machine Learning Glossary](https://developers.google.com/machine-learning/glossary) does not have an entry of _sample_. It does have _example_:\r\n\r\n> example\r\n> One row of a dataset. An example contains one or more features and possibly a label. See also labeled example and unlabeled example.\r\n\r\nSince this class works on dataset, I think the description is talking about examples.\r\n\r\nBefore we have samples, there must be a process of sampling. I don't think TextVectorization is doing sampling. Therefore it's unproper to call examples samples.\r\n\r\nShould \"sample\" be replaced by \"example\"? \r\n", "comments": ["Yes. \"Example\" would be a slightly better fit. \"Sample\" is used in the sense \"sample from a probability distribution\", we often think of datasets as sample from a distribution, an \"example\" is a \"sample of size 1\".\r\n\r\nIf you send a PR I'll approve it.\r\n"]}, {"number": 48297, "title": "Add option to reset a tf.data.Iterator / tf.distribute.DistributedIterator", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWe can iterate over a `tf.data.dataset` using `for element in datasetA:`. This automatically \"restarts\" if we run this for multiple epochs, as seen in this code:\r\n```\r\ndatasetA = tf.data.Dataset.from_tensor_slices([1,  2,  3,  4,  5, 6])\r\n\r\nfor epoch in range(10):\r\n  print(f\"Epoch {epoch}\")\r\n  for element in datasetA:\r\n    print(element)\r\n``` \r\nWhen using an iterator `iterB = iter(datasetB)` we can exhaust the iteration, as seen in this code:\r\n\r\n```\r\ndatasetA = tf.data.Dataset.from_tensor_slices([1,  2,  3,  4,  5, 6])\r\ndatasetB = tf.data.Dataset.from_tensor_slices([11, 22, 33, 44])\r\n\r\niterB = iter(datasetB)\r\nepochs = 5\r\n\r\nfor epoch in range(epochs):\r\n  print(f\"Epoch {epoch}\")\r\n  for element in datasetA:\r\n    print(element)\r\n    elementB = iterB.get_next()\r\n    print(elementB)\r\n```\r\n\r\nWe can handle this by catching the error or using `get_next_as_optional()`, and **manually** restart the iteration:\r\n```\r\nfor epoch in range(epochs):\r\n  print(f\"Epoch {epoch}\")\r\n  for element in datasetA:\r\n    print(element)\r\n    elementB = iterB.get_next_as_optional()\r\n    if not elementB.has_value():\r\n      iterB = iter(datasetB) #<--- restarts the iterator manually\r\n      elementB = iterB.get_next_as_optional()\r\n\r\n    print(elementB.get_value())\r\n```\r\nI propose a method like `.reset_iterator()` or `restart_iterator()` that handles this case conveniently. The same holds for distributed iterators.\r\n\r\n**Will this change the current api? How?**\r\nThis won't have any effect\r\n\r\n**Who will benefit with this feature?**\r\n- Users that iterate over a dataset element by element, without using `for elem in dataset:` loops\r\n- Users iterating multiple datasets **pairwise** simultaneously (see this [question](https://stackoverflow.com/questions/66929798/how-to-do-a-pairwise-iteration-over-two-unequal-length-tf-datasets/66931999#66931999))\r\n\r\n**Any Other info.**\r\nAs an alternative, make the `.take(count)` operation advance the internal state, so that repeatedly taking `count` elements yields the next element, rather than the same.\r\n", "comments": ["@Yannik1337 \r\nPlease take a look at [this](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip). ", "@AdityaKane2001 Thanks for this link, I used the `.zip()` method to zip the two datasets like this:\r\n```\r\ndatasetA = tf.data.Dataset.from_tensor_slices([1,  2,  3,  4,  5, 6])\r\ndatasetB = tf.data.Dataset.from_tensor_slices([11, 22, 33, 44])\r\n\r\nzipped_ds = tf.data.Dataset.zip((datasetA, datasetB))\r\n\r\nfor elem in zipped_ds:\r\n  print(elem)\r\n```\r\n\r\nHowever, as is stated in the documentation, this will only contain as many pairs as there are elements in the shorter dataset\r\n\r\nI am looking for a way to restart the iteration over the shorter dataset, e.g. when we have element `5` from `datasetA`, we take `11` from `datasetB` (which is its first element). This is the intent of my feature request, so that one can restart the iterator once it's fully consumed.", "@Yannik1337 \r\nPlease see [repeat](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) in `tf.data.dataset`", "This solved my request, thanks @AdityaKane2001 for helping me out!"]}, {"number": 48295, "title": "Do TensorFlow probability (TFP) layers have", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4 (binary)\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.keras.layers.Layer` -> class from which all layers inherit.\r\n\r\n\r\n\r\n**Who will benefit from this feature?**\r\nUsers of TFP who would like to inherit the base class and add their own layers. \r\n\r\n", "comments": ["@rrklearn2020 [Here](https://github.com/tensorflow/probability/tree/master/tensorflow_probability/python/layers) are the layers in `tfp`. Thanks!\r\n\r\nPlease post `TFP` related issues in [`TFP` repository](https://github.com/tensorflow/probability/issues) so that `TFP` experts will respond and resolve issues. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48295\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48295\">No</a>\n"]}, {"number": 48294, "title": "Supersed pylint_allowlist", "body": "This a first PR to try to supersed `pylint_allowlist` and custom pattern matching scripts for a less critical integration of our `pylint` linting with third party tools (e.g. IDE) and enable a potential pre-commit hook that we want to officially distribute for linting.\r\n\r\nI've upgraded  `.pylintrc` with the info that we currently have in the pattern matching:\r\nhttps://github.com/tensorflow/tensorflow/blob/38f4134e6884fb1d766999486a72810ebae6acf5/tensorflow/tools/ci_build/ci_sanity.sh#L181-L198\r\n\r\nThen I used [pylint-silent](https://pypi.org/project/pylint-silent/) to patch files. ", "comments": ["/cc @mihaimaruseac @angerson ", "We have now 14 residual `pylint` linting errors that I cannot reproduce in our official Docker devel image (for the command see https://github.com/tensorflow/tensorflow/pull/48291).\nWhat is the Ubuntu Sanity image?\n\nDo we want to use the official devel image in the CI to be sure that the user/contributors dev environment is aligned with the CI Sanity pass in github?\n", "@bhack  Can you please address Ubuntu Sanity errors? Thanks!", "I need to understand what Is the reference Sanity Docker Image. \nI prefer to have in CI our official Docker devel image so that we are sure to have the same CI and contributor/developer experience", "@mihaimaruseac What is the Docker image used by CI Ubuntu Sanity?", "In the mean time I've also upgraded the Devel images to run the python linting step at https://github.com/tensorflow/tensorflow/pull/48371", "I cannot reproduce the errors we have in the Sanity CI logs  locally also forcing the same Python version. E.g.\r\nE.g. in the CI log we have:\r\n`tensorflow/python/training/tracking/python_state_test.py:131: [E1123(unexpected-keyword-arg), _NumpyWrapper.deserialize] Unexpected keyword argument 'allow_pickle' in function call`\r\n\r\nBut\r\n\r\n```\r\ndocker run --rm -it -v $PWD:/tensorflow -w /tensorflow python:3.8.9-buster bash -c  \"pip install pylint && pylint tensorflow/python/training/tracking/python_state_test.py\"\r\n```\r\n```\r\n------------------------------------\r\nYour code has been rated at 10.00/10\r\n```\r\n", "I don't think we build using Docker at the moment.", "> I don't think we build using Docker at the moment.\n\nOk but I need to align the env locallly to have the same error as in CI", "I've changed almost every `install numpy==1.14.5` string but the CI is still on `numpy 1.14.5`", "@mihaimaruseac Can you please take a look on the above comments from @bhack. Thanks!", "I've opened https://github.com/tensorflow/build/pull/27 in the SIG-build repository so that they can compare `ci_sanity.sh` VS `pylint` results on this branch with Github Action.", "@mihaimaruseac Can you please take a look on the above comments from @bhack. Thanks!", "@gbaned I've already talked with @mihaimaruseac just yesterday and he his still busy.", "Also we are waiting for a comment at https://github.com/tensorflow/build/pull/27#issuecomment-827819407 for the Ubuntu Sanity VM.", "I checked the environment for the machine that's running ci_sanity (you can actually see it in ci_sanity results for new PRs):\r\n\r\n```\r\nabsl-py==0.8.1\r\nappdirs==1.4.3\r\nastroid==2.5.6\r\nattrs==19.3.0\r\ncachetools==3.1.1\r\ncertifi==2020.12.5\r\nchardet==4.0.0\r\ndistlib==0.3.0\r\nfilelock==3.0.12\r\nfuture==0.18.2\r\ngast==0.2.2\r\ngoogle-auth==1.9.0\r\ngoogle-auth-oauthlib==0.4.1\r\ngrpcio==1.25.0\r\nh5py==2.10.0\r\nidna==2.10\r\nisort==5.8.0\r\njoblib==0.14.1\r\nKeras-Preprocessing==1.1.0\r\nlazy-object-proxy==1.6.0\r\nMarkdown==3.1.1\r\nmccabe==0.6.1\r\nmock==3.0.5\r\nnumpy==1.14.5\r\noauthlib==3.1.0\r\nportpicker==1.3.1\r\nprotobuf==3.11.1\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.7\r\npycurl==7.43.0\r\npygobject==3.20.0\r\npylint==2.7.2\r\npython-apt==1.1.0b1+ubuntu0.16.4.11\r\nPyYAML==3.11\r\nrequests==2.25.1\r\nrequests-oauthlib==1.3.0\r\nrsa==4.0\r\nscikit-learn==0.22\r\nscipy==1.4.0\r\nscreen-resolution-extra==0.0.0\r\nsix==1.12.0\r\ntb-nightly==2.1.0a20191106\r\ntf-estimator-nightly==2.0.0.dev2019121609\r\ntoml==0.10.2\r\ntyping==3.7.4.1\r\nunattended-upgrades==0.1\r\nurllib3==1.26.4\r\nvirtualenv==20.0.5\r\nWerkzeug==0.16.0\r\nwrapt==1.12.1\r\nxkit==0.0.0\r\n\r\ncheck whether pylint is available or not.\r\n\r\npylint 2.7.2\r\nastroid 2.5.6\r\nPython 3.8.9 (default, Apr  3 2021, 01:02:10)\r\n[GCC 5.4.0 20160609]\r\n```", "The last remaining issue for this PR is how we could align a Little bit that numpy version with the version currently in our official Docker devel  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-cpu.Dockerfile#L87\n\nCause I don't know what reference env to suggest to contributors for local linting on par with CI Sanity.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR.\r\n", "Thank you @gbaned. I had to make a number of extra changes internally to land this, but it's in now.", "@angerson Thanks. Let see if we could review https://github.com/tensorflow/tensorflow/pull/48371 so that we have a reference local env for developers to run reproducible liniting.", "It was merged but the Ubuntu sanity VM is still on `numpy==1.14.5`. As I told in https://github.com/tensorflow/tensorflow/pull/48294#issuecomment-843620546 this will fail the sanity check in the CI."]}, {"number": 48293, "title": "Build from source on OSX 10.13.6 wrongly produces .whl package for 10.14", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: v2.4.1, v2.5.0rc0\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: Build from source\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\n\r\nRunning `./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` on OSX 10.13.6 as per instruction produces a .whl package for OSX 10.14 and not 10.13.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\npip install -U --user pip numpy wheel\r\npip install -U --user keras_preprocessing --no-deps\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n./configure\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\n\r\nPackage in /tmp/tensorflow_pkg is named `tensorflow-2.4.1-cp39-cp39-macosx_10_14_x86_64.whl`\r\n\r\n", "comments": ["@hoonkai \r\nCould you please update if this is still an issue with the latest version.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48293\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48293\">No</a>\n"]}]