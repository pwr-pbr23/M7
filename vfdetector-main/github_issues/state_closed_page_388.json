[{"number": 42334, "title": "Fix floating point exception when zero shape tensor passed to tf.reverse", "body": "This PR tries to address the issue raised in #42248 where\r\nfloating point exception was thrown out when zero shape tensor is passed\r\nto tf.reverse.\r\n\r\nThis PR fixes #42248.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 42333, "title": "Fix deprecation warning of keras.backend.random_bernoulli", "body": "`keras.backend.random_binomial` is deprecated in favour of `keras.backend.random_bernoulli`. This PR updates `keras.backend.random_bernoulli` to not throw the same warning.", "comments": []}, {"number": 42332, "title": "tf.tile feature parity with np.tile/torch.tile/jax.tile", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Darwin ghost.lan 18.7.0 Darwin Kernel Version 18.7.0: Mon Apr 27 20:09:39 PDT 2020; root:xnu-4903.278.35~1/RELEASE_X86_64 x86_64`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary (wheel/pip install)\r\n- TensorFlow version (use command below): `v2.2.0-rc4-8-g2b96f3662b 2.2.0`\r\n- Python version: `Python 3.7.5`\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> input = [10, 20]\r\n>>> np.tile(input, (2,))\r\narray([10, 20, 10, 20])\r\n>>> tf.tile(input, (2,))\r\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([10, 20, 10, 20], dtype=int32)>\r\n>>> np.tile(input, (2,2))\r\narray([[10, 20, 10, 20],\r\n       [10, 20, 10, 20]])\r\n>>> tf.tile(input, (2,2))\r\nTraceback (most recent call last):\r\n  File \"/Users/kratsg/.virtualenvs/pyhf-dev/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 11036, in tile\r\n    input, multiples)\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/kratsg/.virtualenvs/pyhf-dev/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 11041, in tile\r\n    input, multiples, name=name, ctx=_ctx)\r\n  File \"/Users/kratsg/.virtualenvs/pyhf-dev/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 11081, in tile_eager_fallback\r\n    ctx=ctx, name=name)\r\n  File \"/Users/kratsg/.virtualenvs/pyhf-dev/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected multiples argument to be a vector of length 1 but got length 2 [Op:Tile]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expected tensorflow to return the same thing that numpy does (or at least has the same behavior).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee above.\r\n\r\n**Other questions**\r\n\r\nIs there a clever workaround that mimics the behavior using `tf.stack` and `tf.repeat` somehow?", "comments": ["Current workaround I have right now looks like:\r\n\r\n```python\r\ndef tile(tensor_in, repeats):\r\n        try:\r\n            return tf.tile(tensor_in, repeats)\r\n        except tf.python.framework.errors_impl.InvalidArgumentError:\r\n            diff = len(repeats) - len(tf.shape(tensor_in))\r\n            if diff < 0:\r\n                raise\r\n            return tf.tile(\r\n                tf.reshape(tensor_in, [1] * diff + tf.shape(tensor_in).numpy().tolist()),\r\n                repeats,\r\n            )\r\n```\r\n\r\nwhich uses `reshape` if this inspires anyone to find a better way.", "@kratsg \r\n\r\nPlease, see workaround as mentioned below.Attached [gist](https://colab.research.google.com/gist/ravikyram/3ea07ed15259b4a18e983543c4eb7992/untitled255.ipynb) for your reference\r\n\r\n```\r\ninput = tf.constant([[10, 20]], tf.int32)\r\ntf.tile(input, (2,2))\r\n```\r\nThanks!\r\n", "@ravikyram This might not have been clear from the original question, but we're not interested in a specific tensor, like the example snippet\r\n\r\n> ```python\r\n> input = tf.constant([[10, 20]], tf.int32)\r\n> tf.tile(input, (2,2))\r\n> ```\r\n\r\nbut rather having a shim that can work with arbitrary tensors. The motivation for this question arose from https://github.com/scikit-hep/pyhf/issues/1025 and we went with @kratsg's example in https://github.com/scikit-hep/pyhf/pull/1028. Given that, the example code, while correct, doesn't seem to be very useful.\r\n\r\nDo you have a more pragmatic solution for arbitrary shape tensors than @kratsg implemented in https://github.com/scikit-hep/pyhf/pull/1028?", "You can use https://www.tensorflow.org/api_docs/python/tf/experimental/numpy/tile\r\n\r\nIn general the tf.experimental.numpy API adheres strictly to numpy spec. We are working to unifying tf.tile to be more unified with numpy as well but that might take a little bit of time.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42332\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42332\">No</a>\n"]}, {"number": 42331, "title": "tf.nest.flatten crashes (abort) when expand_composites's constraints are violated", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.nest.flatten` crashes (abort) when `expand_composites` is 0D boolean is violated.\r\n\r\n\r\n**Describe the expected behavior**\r\nExpect no crash\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nest.flatten(structure = np.zeros((1)), expand_composites=tf.ones((2)))\r\n~~~\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n~~~python\r\nterminate called after throwing an instance of 'pybind11::error_already_set'\r\n  what():  SystemError: <class 'type'> returned a result with an error set\r\n\r\nAt:\r\n  /root/miniconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py(1388): _array_repr_implementation\r\n  /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py(338): flatten\r\n  <stdin>(1): <module>\r\n\r\nAborted (core dumped)\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.1, TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b0a49b8ac355427b554aca7a08e12c75/42331.ipynb). Thanks!", "@DNXie How is this different from https://github.com/tensorflow/tensorflow/issues/42329? If this is duplicate, then close it. We will follow the https://github.com/tensorflow/tensorflow/issues/42329. Thanks!", "@jvishnuvardhan  Is the fix for #42329 also fix the bug for this API? Thanks!", "@DNXie Sorry. I didn't notice the difference between those two issues. Now I understood the difference. Thanks!", "@jvishnuvardhan Could you please also fix this one? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42331\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42331\">No</a>\n"]}, {"number": 42330, "title": "TFLite: reduced duplicated calculation of exp in softmax.h (float)", "body": "This PR improves floating point softmax kernel (similar to the implementation on [GPU version](https://github.com/tensorflow/tensorflow/blob/84d053187cb80d975ef2b9684d4b61981bca0c41/tensorflow/core/kernels/sparse/kernels_gpu.cu.cc#L345)). The original implementation calculates the sum of exponential terms in the first pass and then in the second pass, the exponential terms are recalculated and divided by the sum in the first pass to get the final results.\r\n\r\nThe change reduces duplicated calculations of exponential terms, to be more specific, exponential terms are calculated and stored in the output tensor and accumulate the value to sum in the first pass; on the second pass, the only thing need to be done is to load the value at the output and divided by the sum.\r\n\r\nExperiments on zephyr_riscv platforms (I use Arty A7 running RISC-V soft-CPU) gives great performance improvements in the Softmax alone. The runtime is measured in number of CPU cycles (clock speed is fixed in this platform) not actual time, but improvements are expected to be similar on other devices as well.\r\n\r\n| Input size |    # cycles (k) [Before]    |    # cycles (k) [After]    | Speedup (%) |\r\n|:----------:|:------------:|:------------:|:-----------:|\r\n|      2     |      16      |      5.8     |    75.47%   |\r\n|      5     |      53      |      13      |    75.25%   |\r\n|     10     |      101     |      25      |    50.62%   |\r\n|     16     |      81      |      40      |    50.62%   |\r\n|     32     |      162     |      81      |    50.00%   |\r\n|     64     |      225     |      161     |    28.44%   |\r\n|     128    |      452     |      321     |    28.98%   |\r\n|     256    |      902     |      640     |    29.05%   |\r\n|     512    |     1812     |     1285     |    29.08%   |\r\n|    1024    |     3628     |     2584     |    28.78%   |\r\n\r\n* *Input size* is the dimension of the row vector passed into softmax layer", "comments": []}, {"number": 42329, "title": "tf.nest.assert_same_structure crashes (abort) when some constraints are violated", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.nest.assert_same_structure` crashes (abort, systemerror) when some either `check_types` or `expand_composites` is 0d bool is violated.\r\n\r\n**Describe the expected behavior**\r\nExpect no crash\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nWhen `check_types` is boolean is violated\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nest.assert_same_structure(nest1=np.zeros((1)), nest2=tf.ones((1,1,1)), check_types=tf.ones((2)))\r\n~~~\r\nWhen `expand_composites` is boolean is violated\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.nest.assert_same_structure(nest1=np.zeros((1)), nest2=tf.ones((1,1,1)), expand_composites=tf.ones((2)))\r\n~~~\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n~~~python\r\nterminate called after throwing an instance of 'pybind11::error_already_set'\r\n  what():  SystemError: <class 'type'> returned a result with an error set\r\n\r\nAt:\r\n  /root/miniconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py(1388): _array_repr_implementation\r\n  /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py(397): assert_same_structure\r\n  <stdin>(1): <module>\r\n\r\nAborted (core dumped)\r\n~~~", "comments": ["Was able to reproduce the issue with TF v2.1, TF v2.3 and TF-nightly. \r\n\r\nSession crashes on running the `tf.nest.assert_same_structure` line. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7e2519ef8640e2d849516d0d363ae230/42329.ipynb). Thanks!", "The issue is more or less complicated and is related to pybind's handling of mismatched python type binding. A easy fix might be to enforce the type passed to pybind conforms to bool (for check_types and expand_composites). Created a PR #42397 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42329\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42329\">No</a>\n"]}, {"number": 42328, "title": "Build Error at Building 'tensorflow/core/distributed_runtime/eager:remote_tensor_handle_data'", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution : **Linux UBUNTU 18.04**\r\n- My device (Platform): **Odroid XU4**\r\n- TensorFlow installed from **Source**\r\n- TensorFlow version: **1.15.3** (and this issue appear in 1.15.0, 1.15.1)\r\n- Python version: **3**\r\n- Installed using virtualenv? pip? conda?: No pip package, **Only C++ lib**\r\n- Bazel version : **0.26.1**\r\n- GCC/Compiler version : **7**\r\n- CUDA/cuDNN version: No CUDA Suppot, **CPU-Only**\r\n- GPU model and memory: Armhf with 1GB Ram and 4G SWAP Memory\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm Trying instatal TensorFlow 1.15 on `ODROID XU4`, I'm success in installing TensorFlow 1.13 by Source Compiling on my Hardware but for TF 1.15 after few days and many efforts get an Error that i don't know what caused and how fixes.\r\n\r\nMy Bazel Command:\r\n`bazel --output_base=/media/odroid/ExternalHard/CacheFolder build --config=opt             --config=monolithic --config=noaws      --jobs 1    --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\"  --cpu=armeabi       --local_resources 2024,0.5,1.0 --define tensorflow_mkldnn_contraction_kernel=0    tensorflow:libtensorflow_cc.so        --discard_analysis_cache --verbose_failures\r\n`\r\nBut I get this Error:\r\n`ERROR: /home/odroid/buildFile/tensorflow/tensorflow/tensorflow/core/distributed_runtime/eager/BUILD:148:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/eager:remote_tensor_handle_data' failed (Exit 1): gcc failed: error executing command \r\n  (cd '/media/odroid/ExternalHard/CacheFolder/execroot/org_tensorflow' && \\\r\n  exec env - \\\r\n    PATH=/home/odroid/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=0 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/eager/_objs/remote_tensor_handle_data/remote_tensor_handle_data.pic.d '-frandom-seed=bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/eager/_objs/remote_tensor_handle_data/remote_tensor_handle_data.pic.o' -fPIC '-DPB_FIELD_32BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -DCURL_STATICLIB -iquote . -iquote bazel-out/arm-opt/bin -iquote external/com_google_protobuf -iquote bazel-out/arm-opt/bin/external/com_google_protobuf -iquote external/zlib_archive -iquote bazel-out/arm-opt/bin/external/zlib_archive -iquote external/grpc -iquote bazel-out/arm-opt/bin/external/grpc -iquote external/com_github_nanopb_nanopb -iquote bazel-out/arm-opt/bin/external/com_github_nanopb_nanopb -iquote external/boringssl -iquote bazel-out/arm-opt/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/arm-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/arm-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/arm-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/arm-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/arm-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/arm-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/arm-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/arm-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/arm-opt/bin/external/hwloc -iquote external/curl -iquote bazel-out/arm-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/arm-opt/bin/external/jsoncpp_git -iquote external/local_config_cuda -iquote bazel-out/arm-opt/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/arm-opt/bin/external/local_config_tensorrt -Ibazel-out/arm-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/arm-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/com_google_protobuf/src -isystem bazel-out/arm-opt/bin/external/com_google_protobuf/src -isystem external/zlib_archive -isystem bazel-out/arm-opt/bin/external/zlib_archive -isystem external/grpc/include -isystem bazel-out/arm-opt/bin/external/grpc/include -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/arm-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/arm-opt/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/arm-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/arm-opt/bin/external/eigen_archive -isystem external/gif_archive -isystem bazel-out/arm-opt/bin/external/gif_archive -isystem external/farmhash_archive/src -isystem bazel-out/arm-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/arm-opt/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/arm-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/arm-opt/bin/external/hwloc/include -isystem external/curl/include -isystem bazel-out/arm-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/arm-opt/bin/external/jsoncpp_git/include -isystem external/local_config_cuda/cuda -isystem bazel-out/arm-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/arm-opt/bin/external/local_config_cuda/cuda/cuda/include -w '-march=native' -Wno-sign-compare -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc -o bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/eager/_objs/remote_tensor_handle_data/remote_tensor_handle_data.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc: In function 'void tensorflow::{anonymous}::DestoryRemoteTensorHandle(tensorflow::EagerContext*, tensorflow::eager::EagerClient*, tensorflow::uint64, tensorflow::uint64, int)':\r\ntensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:30:12: error: 'class tensorflow::EagerContext' has no member named 'GetContextId'; did you mean 'NewContextId'?\r\n   if (ctx->GetContextId() != context_id) {\r\n            ^~~~~~~~~~~~\r\n            NewContextId\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 5417.963s, Critical Path: 60.17s\r\nINFO: 1043 processes: 1043 local.\r\nFAILED: Build did NOT complete successfully\r\n`\r\nI think main error log is this LIne:\r\n`tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:30:12: error: 'class tensorflow::EagerContext' has no member named 'GetContextId'; did you mean 'NewContextId'?\r\n   if (ctx->GetContextId() != context_id) {\r\n            ^~~~~~~~~~~~\r\n            NewContextId`\r\n\r\n\r\n![Screenshot from 2020-08-13 21-53-40](https://user-images.githubusercontent.com/38716150/90166519-9f4f2180-ddaf-11ea-96d9-c6455652d559.png)\r\n", "comments": ["@Yasin40 \r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.\r\n\r\nrefer to these resolved issues and let us know:\r\n#22980 #4279 #20500 ", "@Saduf2019 .\r\nThanks for attention. \r\nUnfortunately this links not worked for me and no related to my issue. \r\nI pasted error before screenshot. bu if you complete than:\r\n`INFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /media/odroid/ExternalHard/CacheFolder/sandbox\r\nSUBCOMMAND: # @curl//:curl [action 'Compiling external/curl/lib/llist.c']\r\n(cd '/media/odroid/ExternalHard/CacheFolder/execroot/org_tensorflow' && \\\r\n  exec env - \\\r\n    PATH=/home/odroid/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=0 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/arm-opt/bin/external/curl/_objs/curl/llist.pic.d '-frandom-seed=bazel-out/arm-opt/bin/external/curl/_objs/curl/llist.pic.o' -fPIC -DCURL_STATICLIB -iquote external/curl -iquote bazel-out/arm-opt/bin/external/curl -iquote external/zlib_archive -iquote bazel-out/arm-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/arm-opt/bin/external/boringssl -isystem external/curl/include -isystem bazel-out/arm-opt/bin/external/curl/include -isystem external/zlib_archive -isystem bazel-out/arm-opt/bin/external/zlib_archive -isystem external/boringssl/src/include -isystem bazel-out/arm-opt/bin/external/boringssl/src/include -w '-march=native' -Wno-sign-compare '-std=c++11' '-march=native' '-mfpu=vfp' -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -Iexternal/curl/lib -D_GNU_SOURCE -DBUILDING_LIBCURL -DHAVE_CONFIG_H -DCURL_DISABLE_FTP -DCURL_DISABLE_NTLM -DHAVE_LIBZ -DHAVE_ZLIB_H -Wno-string-plus-int '-DCURL_MAX_WRITE_SIZE=65536' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/curl/lib/llist.c -o bazel-out/arm-opt/bin/external/curl/_objs/curl/llist.pic.o)\r\nSUBCOMMAND: # //tensorflow/core/distributed_runtime/rpc:grpc_server_lib [action 'Compiling tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc']\r\n(cd '/media/odroid/ExternalHard/CacheFolder/execroot/org_tensorflow' && \\\r\n  exec env - \\\r\n    PATH=/home/odroid/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=0 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_server_lib/grpc_server_lib.pic.d '-frandom-seed=bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_server_lib/grpc_server_lib.pic.o' -fPIC '-DGRPC_ARES=0' '-DPB_FIELD_32BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -DCURL_STATICLIB -DSQLITE_OMIT_DEPRECATED -iquote . -iquote bazel-out/arm-opt/bin -iquote external/grpc -iquote bazel-out/arm-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/arm-opt/bin/external/zlib_archive -iquote external/com_github_nanopb_nanopb -iquote bazel-out/arm-opt/bin/external/com_github_nanopb_nanopb -iquote external/boringssl -iquote bazel-out/arm-opt/bin/external/boringssl -iquote external/com_google_protobuf -iquote bazel-out/arm-opt/bin/external/com_google_protobuf -iquote external/com_google_absl -iquote bazel-out/arm-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/arm-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/arm-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/arm-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/arm-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/arm-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/arm-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/arm-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/arm-opt/bin/external/hwloc -iquote external/curl -iquote bazel-out/arm-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/arm-opt/bin/external/jsoncpp_git -iquote external/local_config_cuda -iquote bazel-out/arm-opt/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/arm-opt/bin/external/local_config_tensorrt -iquote external/png_archive -iquote bazel-out/arm-opt/bin/external/png_archive -iquote external/lmdb -iquote bazel-out/arm-opt/bin/external/lmdb -iquote external/icu -iquote bazel-out/arm-opt/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/arm-opt/bin/external/org_sqlite -iquote external/gemmlowp -iquote bazel-out/arm-opt/bin/external/gemmlowp -Ibazel-out/arm-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/arm-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/grpc/include -isystem bazel-out/arm-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/arm-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/arm-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/arm-opt/bin/external/boringssl/src/include -isystem external/com_google_protobuf/src -isystem bazel-out/arm-opt/bin/external/com_google_protobuf/src -isystem external/nsync/public -isystem bazel-out/arm-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/arm-opt/bin/external/eigen_archive -isystem external/gif_archive -isystem bazel-out/arm-opt/bin/external/gif_archive -isystem external/farmhash_archive/src -isystem bazel-out/arm-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/arm-opt/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/arm-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/arm-opt/bin/external/hwloc/include -isystem external/curl/include -isystem bazel-out/arm-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/arm-opt/bin/external/jsoncpp_git/include -isystem external/local_config_cuda/cuda -isystem bazel-out/arm-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/arm-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/png_archive -isystem bazel-out/arm-opt/bin/external/png_archive -isystem external/icu/icu4c/source/common -isystem bazel-out/arm-opt/bin/external/icu/icu4c/source/common -w '-march=native' -Wno-sign-compare '-std=c++11' '-march=native' '-mfpu=vfp' -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc -o bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_server_lib/grpc_server_lib.pic.o)\r\nERROR: /home/odroid/buildFile/tensorflow/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:279:1: C++ compilation of rule '//tensorflow/core/distributed_runtime/rpc:grpc_server_lib' failed (Exit 1): gcc failed: error executing command \r\n  (cd '/media/odroid/ExternalHard/CacheFolder/execroot/org_tensorflow' && \\\r\n  exec env - \\\r\n    PATH=/home/odroid/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=0 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_server_lib/grpc_server_lib.pic.d '-frandom-seed=bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_server_lib/grpc_server_lib.pic.o' -fPIC '-DGRPC_ARES=0' '-DPB_FIELD_32BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -DCURL_STATICLIB -DSQLITE_OMIT_DEPRECATED -iquote . -iquote bazel-out/arm-opt/bin -iquote external/grpc -iquote bazel-out/arm-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/arm-opt/bin/external/zlib_archive -iquote external/com_github_nanopb_nanopb -iquote bazel-out/arm-opt/bin/external/com_github_nanopb_nanopb -iquote external/boringssl -iquote bazel-out/arm-opt/bin/external/boringssl -iquote external/com_google_protobuf -iquote bazel-out/arm-opt/bin/external/com_google_protobuf -iquote external/com_google_absl -iquote bazel-out/arm-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/arm-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/arm-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/arm-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/arm-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/arm-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/arm-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/arm-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/arm-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/arm-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/arm-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/arm-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/arm-opt/bin/external/hwloc -iquote external/curl -iquote bazel-out/arm-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/arm-opt/bin/external/jsoncpp_git -iquote external/local_config_cuda -iquote bazel-out/arm-opt/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/arm-opt/bin/external/local_config_tensorrt -iquote external/png_archive -iquote bazel-out/arm-opt/bin/external/png_archive -iquote external/lmdb -iquote bazel-out/arm-opt/bin/external/lmdb -iquote external/icu -iquote bazel-out/arm-opt/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/arm-opt/bin/external/org_sqlite -iquote external/gemmlowp -iquote bazel-out/arm-opt/bin/external/gemmlowp -Ibazel-out/arm-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/arm-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/grpc/include -isystem bazel-out/arm-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/arm-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/arm-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/arm-opt/bin/external/boringssl/src/include -isystem external/com_google_protobuf/src -isystem bazel-out/arm-opt/bin/external/com_google_protobuf/src -isystem external/nsync/public -isystem bazel-out/arm-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/arm-opt/bin/external/eigen_archive -isystem external/gif_archive -isystem bazel-out/arm-opt/bin/external/gif_archive -isystem external/farmhash_archive/src -isystem bazel-out/arm-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/arm-opt/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/arm-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/arm-opt/bin/external/hwloc/include -isystem external/curl/include -isystem bazel-out/arm-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/arm-opt/bin/external/jsoncpp_git/include -isystem external/local_config_cuda/cuda -isystem bazel-out/arm-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/arm-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/png_archive -isystem bazel-out/arm-opt/bin/external/png_archive -isystem external/icu/icu4c/source/common -isystem bazel-out/arm-opt/bin/external/icu/icu4c/source/common -w '-march=native' -Wno-sign-compare '-std=c++11' '-march=native' '-mfpu=vfp' -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc -o bazel-out/arm-opt/bin/tensorflow/core/distributed_runtime/rpc/_objs/grpc_server_lib/grpc_server_lib.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from ./tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_service_impl.h:22:0,\r\n                 from tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:39:\r\n./tensorflow/core/distributed_runtime/eager/eager_service_impl.h: In member function 'virtual tensorflow::Status tensorflow::eager::EagerServiceImpl::ClientTensorHandleDeleteNode::Run()':\r\n./tensorflow/core/distributed_runtime/eager/eager_service_impl.h:183:35: \r\n**error: 'class tensorflow::EagerContext' has no member named 'RemoteMgr'\r\n       return context_->Context()->RemoteMgr()->DeleteTensorHandle(\r\n                                   ^~~~~~~~~**\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 368.275s, Critical Path: 38.82s\r\nINFO: 1 process: 1 local.\r\n`", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42328\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42328\">No</a>\n"]}, {"number": 42327, "title": "data.Dataset.as_numpy_iterator is non-reentrant compared to tensorflow data iterator.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15.5 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using the numpy iterator returned from tensorflow Dataset, the iterator is not reentrant. This is different compared to the dataset iterator in tensorflow.\r\n\r\n**Describe the expected behavior**\r\nthe numpy iterator should be reentrant which shares the same behaviour as tensorflow\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nds = (\r\n    tf.data.Dataset.from_tensor_slices((np.arange(2))))\r\n\r\niterator = ds\r\nfor elem in iterator:\r\n    print(elem)\r\nfor elem in iterator:\r\n    print(elem)\r\n```\r\ngives \r\n```\r\ntf.Tensor(0, shape=(), dtype=int64)\r\ntf.Tensor(1, shape=(), dtype=int64)\r\ntf.Tensor(0, shape=(), dtype=int64)\r\ntf.Tensor(1, shape=(), dtype=int64)\r\n```\r\nwhereas \r\n```\r\nds = (\r\n    tf.data.Dataset.from_tensor_slices((np.arange(2))))\r\n\r\niterator = ds.as_numpy_iterator()\r\nfor elem in iterator:\r\n    print(elem)\r\nfor elem in iterator:\r\n    print(elem)\r\n```\r\ngives \r\n```\r\n0\r\n1\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nNA", "comments": ["I think the issue is here\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/data/ops/dataset_ops.py#L526\r\n\r\nwhere the _NumpyIterator is used, however in _NumpyIterator's __iter__ method (line 3776), the state for the iterator is not initialized at every iter.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/data/ops/dataset_ops.py#L3770-L3783.\r\n", "I have tried in colab with TF version 2.3, nightly versions(`2.4.0-dev20200813`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2cdbc8759c76dca116fd7ea098ce7a99/untitled254.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Has there been any update on this?", "In tf.data (and in Python in general) iterators can only be iterated through once. `tf.data.Dataset` is an **Iterable**, not an **Iterator**. See https://stackoverflow.com/a/9884259/401884 for more information about the difference. \r\n\r\n[This colab](https://colab.research.google.com/drive/1A19aX1JlvYoRMoShcfs9RyhHDtRnVexB) demonstrates the difference and shows how to create a numpy Iterable that has the behavior you're looking for.\r\n\r\n```python\r\nclass NumpyIterable(object):\r\n  def __init__(self, dataset):\r\n    self.dataset = dataset\r\n\r\n  def __iter__(self):\r\n    return self.dataset.as_numpy_iterator()\r\n\r\nnumpy_iterable = NumpyIterable(ds)\r\nprint(\"\\nIterate numpy iterable first time:\")\r\nfor elem in numpy_iterable:\r\n  print(elem)\r\nprint(\"\\nIterate numpy iterable second time:\")\r\nfor elem in numpy_iterable:\r\n  print(elem)\r\n```", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/2d6b50dbb874c8cdb7d2952238fa6824/untitled69.ipynb)..Thanks ! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42327\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42327\">No</a>\n"]}, {"number": 42326, "title": "[TF:MLIR] Verify and fold tf.Tile", "body": "Verify and fold `tf.Tile`.", "comments": ["@WindQAQ can you please resolve conflicts , thank you ", "@WindQAQ  Can you please resolve conflicts? Thanks!"]}, {"number": 42325, "title": "Error with exporting TF2.2.0 model with tf.lookup.StaticHashTable for Serving", "body": "I'm using `StaticHashTable` as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the `model.output` and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem.\r\n\r\nIssue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with `TF2.X + tf.compat.v1.disable_eager_execution()`\r\n\r\n```python\r\ntf.compat.v1.disable_eager_execution()\r\nversion = 1\r\nname = 'tmp_model'\r\nexport_path = f'/opt/tf_serving/{name}/{version}'\r\nbuilder = saved_model_builder.SavedModelBuilder(export_path)\r\n\r\nmodel_signature = tf.compat.v1.saved_model.predict_signature_def(\r\n    inputs={\r\n        'input': model.input\r\n    }, \r\n    outputs={\r\n        'output': model.output\r\n    }\r\n)\r\n\r\nwith tf.compat.v1.keras.backend.get_session() as sess:\r\n    builder.add_meta_graph_and_variables(\r\n        sess=sess,\r\n        tags=[tf.compat.v1.saved_model.tag_constants.SERVING],\r\n        signature_def_map={\r\n            'predict': model_signature\r\n        },\r\n        # For initializing Hashtables\r\n        main_op=tf.compat.v1.tables_initializer()\r\n    )\r\n    builder.save()\r\n```\r\n\r\nThis will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables. \r\n\r\nHere is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving error as below:\r\n\r\n```python\r\nclass MyModule(tf.Module):\r\n\r\n    def __init__(self, model):\r\n        super(MyModule, self).__init__()\r\n        self.model = model\r\n    \r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 16), dtype=tf.int32, name='input')])\r\n    def predict(self, input):\r\n        result = self.model(input)\r\n        return {\"output\": result}\r\n\r\nversion = 1\r\nname = 'tmp_model'\r\nexport_path = f'/opt/tf_serving/{name}/{version}'\r\n\r\nmodule = MyModule(model)\r\ntf.saved_model.save(module, export_path, signatures={\"predict\": module.predict.get_concrete_function()})\r\n```\r\n\r\n**Error:**\r\n```bash\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"2907:0\", shape=(), dtype=resource).\r\nTensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\n\r\nAny suggestion or am I missing anything on exporting TF2.2 model which is using the `StaticHashTables` in final Lambda layer for TensorFlow Serving? \r\n\r\nThanks!", "comments": ["@spate141,\r\nIssues related to TF Serving are handled in the Serving repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/serving/issues/new/choose), so that we can track the issue there. Thanks!", "@amahendrakar Added a new issue on tensorflow/serving.", "@spate141,\r\nThank you for the update. Marking this issue as close since it is being tacked in the serving repo. Thanks!"]}, {"number": 42324, "title": "Update nn_ops.py", "body": "## Added check for pooling_ratio\r\nIt will checks that `pooling_ratio >= 1.0 `\r\n**If not** then, it will throw error `pooling_ratio should be >= 1.0.` \r\n\r\n**With reference to #42278**", "comments": ["@gbaned @rmlarsen  Will you please check this PR and can anyone tell me that why it is taking so long for to do all checks(Ubuntu CPU, Sanity and copybara) ?", "> @gbaned @rmlarsen Will you please check this PR and can anyone tell me that why it is taking so long for to do all checks(Ubuntu CPU, Sanity and copybara) ?\r\n\r\n@aavishkarmishra Sorry for the delay. This PR is waiting for reviewer approval, once it approved checks will run automatically.  Thank you!", "@aavishkarmishra thanks for the fix.", "@aavishkarmishra  Can you please check @rmlarsen's comments and keep us posted ? Thanks!", "@rmlarsen @gbaned I have upated the changes, please check them !!", "@rmlarsen Please check review this pull reqest", "@gbaned @rmlarsen Hello, is there any updates regarding this pull request as it is taking so long to for review ?\r\n", "@gbaned Is there any other way we can approach someone to review?", "@aavishkarmishra can you please check ubuntu sanity errors ?", "@rthadur I have removed bad-whitespace that may be causing error so please review again. ", "@aavishkarmishra Can you please check below internal errors. Thanks! \r\n\r\nTraceback (most recent call last):\r\n  File \"/tensorflow/python/framework/test_util.py\", line 1313, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/tensorflow/python/kernel_tests/fractional_max_pool_op_test.py\", line 518, in testLargePoolingRatioThroughGradientError\r\n    seed=self._SEED)\r\n  File \"/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/tensorflow/python/ops/nn_ops.py\", line 5417, in fractional_max_pool_v2\r\n    raise ValueError(\"pooling_ratio should be an int or a list of ints.\")\r\nValueError: pooling_ratio should be an int or a list of ints.", "> @aavishkarmishra Can you please check below internal errors. Thanks!\r\n> \r\n> Traceback (most recent call last):\r\n> File \"/tensorflow/python/framework/test_util.py\", line 1313, in decorated\r\n> return f(self, *args, **kwargs)\r\n> File \"/tensorflow/python/kernel_tests/fractional_max_pool_op_test.py\", line 518, in testLargePoolingRatioThroughGradientError\r\n> seed=self._SEED)\r\n> File \"/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n> return target(*args, **kwargs)\r\n> File \"/tensorflow/python/ops/nn_ops.py\", line 5417, in fractional_max_pool_v2\r\n> raise ValueError(\"pooling_ratio should be an int or a list of ints.\")\r\n> ValueError: pooling_ratio should be an int or a list of ints.\r\n\r\n@gbaned Here, we have to update unit tests for this function as I have updated the error messages. This is reason for this error.", "@gbaned I have removed the typos that are causing errors. ", "@rthadur @alextp @rmlarsen Please review this pull request !!\r\n", "> The fact that we need these test changes makes me think this change as-is is backwards incompatible and so not something we can ship.\r\n\r\n@alextp If you look closely in test.py then you will see that there is a minor typo on line 506 which is causing error as it uses `( )` for a list instead of `[ ]`. ", "I am mostly concerned about the naked 1s which became 1.0s. ", "> I am mostly concerned about the naked 1s which became 1.0s.\r\n\r\n@alextp They are converted just because in all the examples the values in list are of float data type, but 1 will work equally well.", "And why are tuples after this change no longer valid as pooling_ratio arguments? Can't we convert tuples to lists inside the function?", "> And why are tuples after this change no longer valid as pooling_ratio arguments? Can't we convert tuples to lists inside the function?\r\n\r\nWe can @alextp  but the description of function says that `pooling_ratio is an int or list of ints that has length 1, 2 or 4`  \r\n [ see here](https://www.tensorflow.org/api_docs/python/tf/nn/fractional_max_pool#args).", "Right but if the current interface accepts tuples as well as lists going to\nlists-only is a backwards incompatible change that is disallowed in\ntensorflow.\n\nOn Wed, Oct 7, 2020 at 7:46 AM Aavishkar Mishra <notifications@github.com>\nwrote:\n\n> And why are tuples after this change no longer valid as pooling_ratio\n> arguments? Can't we convert tuples to lists inside the function?\n>\n> We can @alextp <https://github.com/alextp> but the description of\n> function says that pooling_ratio is an int or list of ints that has\n> length 1, 2 or 4\n> see here\n> <https://www.tensorflow.org/api_docs/python/tf/nn/fractional_max_pool#args>\n> .\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/42324#issuecomment-704986412>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNTCK7FDOQEYI2HI7TSJR5L7ANCNFSM4P6TISIA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Please review this and sorry for the inconvenience \ud83d\ude1e "]}, {"number": 42323, "title": "How to load a tflite model directly from a link or aws s3?", "body": "In order to deploy an aws-lambda function, I want to load a large tflite model from s3 or a web_link using 'tflite.interpreter' function. This function has 'model_path' and 'model_content' arguments. I don't have enough space in the local '/tmp' was lambda folder to store the model there (with 500 MB limitation)  and use the 'model_path' argument. Is it possible to load the model directly from s3 or another web link? For example, can we use file streaming with BytesIO and the 'model_content' argument? \r\n\r\nNote that I'm using python=3.7 \r\n\r\n#s3 #aws-lambda #tflite ", "comments": ["The model needs to be able to fit into memory, or at least be fully (and randomly) addressable in memory. So you can certainly use ByteIO, but you'll have to first read the file into memory via BytesIO and pass that into the `model_content` argument.", "Hi, I solved my issue by importing before unzipping the packages into `/tmp` folder. However, I got some errors when using `BytesIO` and `StringIO` and it wasn't successful. Is it related to the python wrapper?", "Are you using Python 2 or 3?", "I tested with python 3.7", "Apologies for the delay, @MeghnaNatraj can you help with this one?", "@MiladGhorbaniG,\r\n\r\nCan you take a look at this [link](https://stackoverflow.com/questions/59570081/how-to-use-tensorflow-lite-on-aws-lambda) which discuss about a similar question, and let me know if it helps? Thanks!", "Hello @MiladGhorbaniG, you try this with Google drive + Google Colab\r\n\r\nUpload your model to your google drive and authorize access to it from Google colab by running the following code in Google colab:\r\n\r\n```\r\n# 1. Mount (and authorize) Google Drive\r\nfrom google.colab import drive\r\ndrive.mount('/content/drive')\r\n\r\n\r\n# 2. Find your TFLite file\r\n# In the left section of google colab, select the 'Files' tab and refresh the folders. You'll notice a 'gdrive' option. Navigate to your file and drag and drop it into a colab cell ==> it'll automatically copy-paste the file-path\r\n\r\n# Use the file path to load it into the interpreter\r\nconverter = tf.lite.Interpreter(\"/content/drive/My Drive/...<path-to-tflite-model>\")\r\n```\r\n", "> @MiladGhorbaniG,\r\n> \r\n> Can you take a look at this [link](https://stackoverflow.com/questions/59570081/how-to-use-tensorflow-lite-on-aws-lambda) which discuss about a similar question, and let me know if it helps? Thanks!\r\n\r\nHi @sanatmpa1 , \r\nThe link you shared does not address my issue. My problem is I only have a limited amount of space and I want to import tflite models without creating temporary files.", "> Hello @MiladGhorbaniG, you try this with Google drive + Google Colab\r\n> \r\n> Upload your model to your google drive and authorize access to it from Google colab by running the following code in Google colab:\r\n> \r\n> ```\r\n> # 1. Mount (and authorize) Google Drive\r\n> from google.colab import drive\r\n> drive.mount('/content/drive')\r\n> \r\n> \r\n> # 2. Find your TFLite file\r\n> # In the left section of google colab, select the 'Files' tab and refresh the folders. You'll notice a 'gdrive' option. Navigate to your file and drag and drop it into a colab cell ==> it'll automatically copy-paste the file-path\r\n> \r\n> # Use the file path to load it into the interpreter\r\n> converter = tf.lite.Interpreter(\"/content/drive/My Drive/...<path-to-tflite-model>\")\r\n> ```\r\n\r\n@MiladGhorbaniG Does [this comment](https://github.com/tensorflow/tensorflow/issues/42323#issuecomment-946336158) help? \r\n\r\nYou can use this to load TFLite models that have been uploaded to your Google drive (this has a larger limit and you don't have to upload your files repeatedly)\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42323\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42323\">No</a>\n"]}, {"number": 42322, "title": "[ROCm] Enable GenerateBoundingBoxProposals", "body": "Originally enabled by @ekuznetsov139 in this [PR](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/830). It has been working fine in the past half year.\r\n\r\n/cc: @deven-amd @chsigg ", "comments": []}, {"number": 42321, "title": "Optimising functions with Openfermion ", "body": "I am working on some quantum generative adversarial networks code and have a discriminator which is a function of some parameters multiplied by some expectation value measurements. For example (see below the code) the variable parameters which I would like to optimise given a loss function are the disc_weights which are multiplied by openfermion commands which measure the X and Y expectation values on qubit 0 and 1 respectively.\r\n\r\npsi = (disc_weights[0] * QubitOperator('X0') + disc_weights[1] * QubitOperator('Y0') )\r\n\r\nWhen I use a keras optimiser, it recognises disc_weights as a tf.variable but clearly has trouble with the QubitOperator commands. How do I exclude these from the optimisation routine?\r\n", "comments": ["@zohimchandani \r\n\r\nCan you please provide simple standalone code to reproduce  the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42320, "title": "Ragged Tensors in Keras Model Output", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI don't think ragged tensors are supported as the outputs to a model. For example the following code does not work\r\n\r\nimport tensorflow as tf\r\n\r\ny_pred = tf.ragged.constant([[0], [0, 0]])\r\ny_true = tf.ragged.constant([[1], [1,1]])\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Input(shape=(None,), ragged=True)\r\n])\r\n\r\nmodel.compile(loss='MSE', optimizer='adam')\r\nmodel.evaluate(x=y_pred, y=y_true)\r\n\r\nReturns error:\r\n TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(\"sequential_1/Cast_1:0\", shape=(None,), dtype=float32), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n\r\n**Will this change the current api? How?**\r\nyes. implement support for ragged tensors in keras model output\r\n\r\n**Who will benefit with this feature?**\r\nPeople who want to use ragged tensors in output, for example sequence to sequence models with variable lengths.\r\n\r\n**Any Other info.**\r\nNo.\r\n", "comments": ["@ablanch5,\r\nPlease take a look at [this](https://github.com/tensorflow/tensorflow/issues/41276) similar issue and let us know if it helps. Thanks!", "Maybe I'm not reading it properly but it doesn't seem like that issue was actually resolved it was just propogated to a different issue. I agree the same problem is being discussed. Anyway hoping ragged tensors can be supported for labels in the future without the need for custom code on user end.", "Note that supporting RaggedTensors as targets (e.g., in `model.fit`) is a frequent request, see #44988, #44112, #43591, #43093, #42320, #41810.\r\n\r\nSome partial progress is in pull requests #45060 and #45015, which will allow using custom losses in Keras model (but existing losses like MSE or (S)CE will still not work).", "Note that I created a new feature request #45403 to support RaggedTensors in standard Keras loss functions.", "@ablanch5 were  you able to solve this issue. I think it is still not resolved. They have made progress in terms of not having to write standard loss functions for ragged inputs. ", "> \r\n> \r\n> @ablanch5 were you able to solve this issue. I think it is still not resolved. They have made progress in terms of not having to write standard loss functions for ragged inputs.\r\n\r\nI was not able to solve it", "Looks like this was resolved in recent `tf-nightly` (2.8.0.dev2021121008). [Here](https://colab.research.google.com/gist/jvishnuvardhan/6027e225ae010de63ee88cd8fc8ee84d/untitled86.ipynb) is a gist for reference.\r\n\r\nCan you please verify once and close the issue if this was resolved for you.\r\n\r\nIf this was not resolved, can you please open the issue in  [keras-team/keras repo](https://github.com/keras-team/keras/issues) repo as keras development moved to that repo to focus mainly on Keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I had the exact same issue as @ablanch5 and just tried it again with TensorFlow 2.8.0rc1, and confirm that it now works :+1:", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 42782, "title": "Any tutorial that does not use keras or eager execution?", "body": "Hi, first of all, thanks for this amazing tutorial, it really clarifies many concepts.\r\n\r\nThe keras API are obviously convenient for users, except that they hide many details for learners too, which causes extra difficulty when debugging. For example, what happens inside DenseFeatures, what is the argument requirement and how does keras.layers.DenseFeatures differ from old input_layer api?\r\n\r\nLots of existing tutorials elsewhere are still based on tf 1.x, but some of 1.x components(e.g. Session, input_layer) are gone in 2.x API. The detailed usage of 2.x apis are not widely explored and documented, so currently both 1.x and 2.x api are used in my code. I and not sure if it will cause some problems in the future.\r\n\r\nEager execution sounds like great feature, but I've got feedbacks from others that it will cause compatibility and performance issues. I am unaware of the exact details but I guess this feature will not be enabled for a while.\r\n\r\nSo I wonder if there are some 2.x examples that do not heavily rely on keras of eager execution? I'd really appreciate if there are some.\r\n\r\nThanks.", "comments": ["Hi @TheFinalHydra ,\r\nKeras and eager are part of the core TensorFlow product. That said, you don't have to use all the Keras abstractions if you don't want. Check out this guide and the other guides in this section for a more \"from the ground up\" approach: https://www.tensorflow.org/guide/tensor\r\nIn particular, you might find the [tf.Module guide](https://www.tensorflow.org/guide/intro_to_modules) or even the the [tf.numpy guide](https://www.tensorflow.org/guide/tf_numpy) interesting.\r\n"]}, {"number": 42318, "title": "Could not load dynamic library 'libcusparse.so.10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version:  2.4.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?:virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 10/9\r\n- CUDA/cuDNN version: 11/8\r\n- GPU model and memory: nvidia 2060\r\n\r\n\r\n\r\n**Describe the problem**\r\nI can't figure out how to install the module:__Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory__\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices..\r\n", "comments": ["@dark0ghost \r\n\r\nCan you try with tested build configurations from here and try building TensorFlow with CUDA 10.1, and check if it works. \r\nTo use CUDA 10.2  Please build the Tensorflow from source.\r\nFollow the instructions mentioned [here.](https://www.tensorflow.org/install/source) And also take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/38194#issuecomment-609922803).\r\nThanks!", "tensorflow v2.4b  no support  cuda 11?", "@dark0ghost \r\n\r\nPlease, refer https://github.com/tensorflow/tensorflow/issues/42084#issuecomment-670208947 . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42318\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42318\">No</a>\n"]}, {"number": 42317, "title": "Enable depthwise convs in auto_mixed_precision (resubmit)", "body": "Resubmit of https://github.com/tensorflow/tensorflow/pull/42031 with fix for cudnn_version check on internal builds.\r\n\r\ncc @reedwm @nluehr ", "comments": []}, {"number": 42316, "title": "Poor code on Docs", "body": "Code doesn't work, imports aren't listed and K.dot is just too vague\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN", "comments": ["It's a well known convention that you do:\r\n`import tensorflow.keras.backend as K`", "You may use following imports on top of the example:\r\n```python\r\n# Setup modules\r\n  from tensorflow import keras\r\n  from tensorflow.keras.layers import RNN\r\n  from tensorflow.keras import backend as K\r\n# Rest of the example\r\n````", "Thanks ymodak, you should put it on the docs too if you have access."]}, {"number": 42315, "title": "[TensorFlow 1.14] : Use tf.data.TFRecordDataset to obtain hdfs data, and the network can reach a maximum of 600Mb/s. However, the network is dual 10G.", "body": "1. description\uff1a\r\n    **Machine environment**: 4U40C128G.\r\n    When num_parallel_calls is set to 4, 10, 32, and the network  can reach 600Mb/s. The parameter(num_parallel_calls) does not take effect. \r\n    At the same time, the network card is dual 10G, using hadoop fs -copyToLocal, the network can reach 2g/s.\r\n    Using tf.data.TFRecordDataset to load hdfs data, the network did not reach the bottleneck, and cores did not reach the bottleneck(Use less than 5 cores).\r\n2. code\r\ndef input_fn(input_files, num_epochs=1, shuffle=True, batch_size=1000, buffer_size=100000):\r\n    dataset = tf.data.TFRecordDataset(input_files)\r\n    dataset = dataset.batch(batch_size)\r\n    dataset = dataset.prefetch(buffer_size=batch_size * 10)\r\n    dataset = dataset.repeat(num_epochs)\r\n    dataset = dataset.map(decode, num_parallel_calls=32)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    example,lables = iterator.get_next()\r\n    return example,lables\r\n\r\npls why?\r\nIs it a performance problem with TensorFlow loading data, or is it the wrong way to use it? \r\n\r\n", "comments": ["@Mr-Nineteen \r\nI ran the code and did not face any issue, please find [gist here](https://colab.research.google.com/gist/Saduf2019/cf34c7cc45b3bde7a768d516f425728c/untitled370.ipynb).\r\nCan you please provide with a colab gist the error faced for us to analyse. (share error logs if any)", "@Saduf2019 \r\n\r\n       Machine environment: **4U40C128G**, the network card is **dual 10G**.\r\n \r\n       Using hadoop fs -copyToLocal, the network can reach 2g/s. Using tf.data.TFRecordDataset to load hdfs data, the network did not reach the bottleneck, and cores did not reach the bottleneck(Use less than 5 cores).\r\n\r\n       Give you the code, the machine environment and the HDFS data environment are also different.\r\n\r\nPart of the reference code:\r\npart1:\r\ndef input_fn(input_files, num_epochs=1, shuffle=True, batch_size=1000, buffer_size=100000):\r\n  dataset = tf.data.TFRecordDataset(input_files)\r\n  dataset = dataset.batch(batch_size)\r\n  dataset = dataset.prefetch(buffer_size=batch_size * 10)\r\n  dataset = dataset.repeat(num_epochs)\r\n  dataset = dataset.map(decode, num_parallel_calls=32)\r\n  iterator = dataset.make_one_shot_iterator()\r\n  example,lables = iterator.get_next()\r\n  return example,lables\r\n\r\npart2:\r\ndef feed_dict_gen(self, features):        \r\n        feed_dict = {\r\n            self.labels_ctr: features['label_ctr'],\r\n            self.rt_user_cate3id_len: features['user_cat3id_hist_len'],\r\n            Omit.......\r\n        }\r\n        return feed_dict\r\n\r\npart3:\r\nfeatures, labels_ctr  = inputs.input_fn(fnames, num_epochs=1, shuffle=False, batch_size=FLAGS.batch_size)\r\ncur_feed_dict_ = model.feed_dict_gen(features)\r\n\r\npart4:\r\n with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        step = 0\r\n                \r\n        try:\r\n            coord = tf.train.Coordinator()\r\n            threads = tf.train.start_queue_runners(coord=coord)\r\n            \r\n            while not coord.should_stop():\r\n                print(\"load data start: %s step %d\"%(datetime.datetime.now(), step))\r\n                cur_feed_dict = sess.run(cur_feed_dict_)\r\n                print(\"load data end  : %s step %d, batch_size = %d\"%(datetime.datetime.now(), step, FLAGS.batch_size))             \r\n                step = step + 1\r\n        except tf.errors.OutOfRangeError as e:\r\n            print(\"%s except out of range.\" % (datetime.datetime.now()))\r\n            print(e)\r\n        finally:\r\n            coord.request_stop()\r\n            print('all threads are asked to stop!')\r\n\r\n \r\n       \r\n\r\n \r\n", "@Mr-Nineteen \r\nPlease try using 2.1 as this version is not supported, and let us know if you face any error with tf 2.x", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42314, "title": "[TFLu] Update initialization of 'input_dims' in EvalQuantizedInt8() method of Fully Connected kernel (CMSIS-NN)", "body": "This is a new pull request based on the comments of #41285, which aims to fix initialization of `cmsis_nn_dims input_dims`.\r\n\r\nFixes #41816\r\n", "comments": ["@njeffrie Could you have a look at this PR, please? I recently came across an issue for a network where a reshape reduced the dimensions to 2 and caused a crash. Would be good to have this fix in.", "@biagiom  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "Hi @advaitjain,\r\nsorry for my late response but I was very busy in the last two weeks.\r\nAs for your comment, could you please provide more details (and if possible help me) about how to build the unittests ?\r\n\r\nMoreover, I just tested the `hello_world` example (compiled on the latest source code of TFLite) on a [DISCO-L475VG-IOT01A](https://os.mbed.com/platforms/ST-Discovery-L475E-IOT01A/) and it returns the same exception discussed in my previous PR (#41285). However, with the fix introduced in this PR, the example runs without any issue.\r\nI would just point out that the model related to the `hello_world` example uses only 3 fully-connected layers without applying flattening and when the model is executed, the input tensors to the fully-connected layers seem to have only 2 dimensions.\r\nI also tried MobileNet v1 (tf.keras.applications.MobileNet) and I got the same exception when executing the fully-connected layer.\r\nMaybe they can be considered as \"a kind of test cases\" that might help you to further analyze this issue.\r\nIn any case, I think my PR works both when the input is a bi-dimensional tensor and a 4-dim tensor.\r\nFinally, are you able to reproduce the issue on the `hello_world` example ? Please make sure to add the flag `__ARM_FEATURE_DSP=1` when compiling the example in order to use the CMSIS-NN optimized kernels.\r\n\r\nBest regards,\r\nBiagio.", "I'd like to see a new test case in micro/kernels/fully_connected_test.cc that exercises this code.\r\n\r\nApologies for not looking at this closely before -- it would additionally be good to also explain why this new mapping between input_shape and the cmsis-nn input_dims is indeed generic and not model specific.\r\n\r\nTagging @freddan80 for their thoughts.", "Hello @biagiom and thanks for your contribution! As @advaitjain mentions, adding a testcase covering the input that caused the issue would be great (in micro/kernels/fully_connected_test.cc). From https://github.com/tensorflow/tensorflow/issues/41816, I guess the testcase would be similar to an existing one, but have size_=2 according to: _Here input_shape is the input layer (33 neurons) which does not have this dimension (size_ is 2)._. That way we avoid this issue coming back. It's enough to add a test there and run it on host for now I believe. You can try run it using Renode on an emulated Cortex-M, but I don't think it's necessary. We run all these unit tests regularly on Cortex-M models. \r\n\r\nRegarding `__ARM_FEATURE_DSP=1`, it's a predefined macro set by the compiler, so you should not set it. More info here: https://developer.arm.com/documentation/dui0774/g/Other-Compiler-specific-Features/Predefined-macros?lang=en. Basically, as soon as you compile using a Cortex-M processor target, this flag is set automatically. Let me know if that doesn't work properly for you, but perhaps we can take that discussion in a separate Github issue or via email, not to derail this thread.\r\n\r\nBoth me and @felix-johnny are ooo for some days, so I'll loop in @jenselofsson\r\n\r\nHope this helps!\r\n/Fredrik\r\n", "@biagiom Can you please check @freddan80's comments and keep us posted ? Thanks!", "Hi @advaitjain and @freddan80,\r\nfirst of all sorry for my late response and above all thanks for your comments and tips for the issue.\r\n\r\nI have done some further tests and analyzed in more detail my contribution and I would like to point out the following considerations:\r\n\r\n1. First I will start from the @advaitjain comment:\r\n> Apologies for not looking at this closely before -- it would additionally be good to also explain why this new mapping between input_shape and the cmsis-nn input_dims is indeed generic and not model specific.\r\n\r\nWith respect to the current implementation, the new mapping I introduced with my PR is more generic because it is able to manage both 2D and 4D input tensors. In fact, as for 2D input tensors, `input_shape.Dims(2);` and `input_shape.Dims(3);` would generate an exception. In particular, if you look at [lines 196 and 197 of /tensorflow/lite/kernels/internal/types.h](https://github.com/tensorflow/tensorflow/blob/cc30260fe86eb00693dc953655611f068b5c4546/tensorflow/lite/kernels/internal/types.h#L196), they will check if the dimension `i` is greater or equal than 0 and lower than `size_`, the number of dimensions of the input tensor. If any of the two conditions does not hold, both the checks will evaluate to `TFLITE_ASSERT_FALSE`.\r\n\r\n2. As for the test case, I have created a new example called [fully_connected_cmsis_test in my TensorFlow fork (branch cmsis_fc_test)](https://github.com/biagiom/tensorflow/tree/cmsis_fc_test/tensorflow/lite/micro/examples/fully_connected_cmsis_test) that contains the same code you can find in micro/kernels/fully_connected_test.cc as it tests fully-connected op using both 2D and 4D input tensors and several data formats (float, int8, uint8).\r\nI compiled the example with the following commands and tested on my [DISCO-L475VG-IOT01A](https://os.mbed.com/platforms/ST-Discovery-L475E-IOT01A/):\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"cmsis-nn\" generate_fully_connected_cmsis_test_mbed_project\r\n$ cd tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/fully_connected_cmsis_test/mbed\r\n$ mbed config root .\r\n$ mbed deploy\r\n$ mbed config GCC_ARM_PATH \"~/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin\"\r\n$ mbed compile -m DISCO_L475VG_IOT01A -t GCC_ARM\r\n$ cp ./BUILD/DISCO_L475VG_IOT01A/GCC_ARM/mbed.bin /media/biagio/DIS_L4IOT\r\n```\r\nAs for results, all the test cases are passed without any issue with the new mapping (the one described in my PR).\r\nAnother important point is that it still works without any issue also if using the old mapping even for the test cases that make use of a 2D input tensor.\r\nAt that point I was a bit confused as I expected that the test cases using a 2D input tensor would fail.\r\nSo, I investigated in detail the source code and discovered that this strange behavior is given by the use of `NDEBUG` macro:\r\nif you look at [line 30 of /tensorflow/lite/kernels/op_macros.h](https://github.com/tensorflow/tensorflow/blob/23cb51dfb64f863d4e5d470160d6b44f1ffdb992/tensorflow/lite/kernels/op_macros.h#L60), you can note that if `NDEBUG` is defined, then `TFLITE_ASSERT_FALSE` is defined as `(static_cast<void>(0))`. This means that any assertion based on `TFLITE_ASSERT_FALSE` (like `TFLITE_DCHECK_GE` and `TFLITE_DCHECK_LT` used by `Dims()` method) will not have any effect ! So, even though `EvalQuantizedInt8()` method uses `input_shape.Dims(2)` and `input_shape.Dims(3)` with 2D input tensors, there isn't any exception.\r\nIn my TensorFlow fork I have modified the [fully_connected CMSIS-NN implementation](https://github.com/biagiom/tensorflow/blob/cmsis_fc_test/tensorflow/lite/micro/kernels/cmsis-nn/fully_connected.cc) to make some checks at runtime and (above all) test this behavior of `NDEBUG`. I also modified the [op_macros.h](https://github.com/biagiom/tensorflow/blob/cmsis_fc_test/tensorflow/lite/kernels/op_macros.h) file in order to undefine the `NDEBUG` macro (see the notes in the comments).\r\nNow, if you clone my repo and try to compile the `fully_connected_cmsis_test` or `hello_world` example (any example that use fully_connected op), the execution will stop in a loop because now, since `TFLITE_ASSERT_FALSE` is defined with `TFLITE_ABORT`, when `EvalQuantizedInt8()` calls `input_shape.Dims(2)`, `TFLITE_DCHECK_LT(i, size_);` evaluates to `TFLITE_ABORT`.\r\n\r\nAnyway, it seems that it doesn't matter how the fields `h`, `w` and `c` of `input_dims` are initialized because if you look at `arm_fully_connected_s8` to which `input_dims` is passed to, the only field that seems to be used is the `n` field (batch size).\r\nHowever I would prefer that this issue related to the use of `NDEBUG` will be handled in a different way as it may cause unexpected behaviors (see #41816).\r\n\r\nI hope to be clear in my explanation.\r\nMoreover, I think that we don't need any additional test unit in `micro/kernels/fully_connected_test.cc` as all the test cases can be used to test the code I want to merge (maybe we just need to update the [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/BUILD) file to ensure we use the CMSIS-NN implementation instead of the default one).\r\n\r\nWhat do you think about the previous points ? Is there anything of my observations that I missed or is wrong ? \r\n\r\nBest regards,\r\nBiagio.", "Hi @biagiom\r\n\r\nThanks for your detailed analysis! And you are correct. The tests that already exist for fully connected should have caught this. It should have failed on `TFLITE_DCHECK_LT`, as you say: https://github.com/tensorflow/tensorflow/blob/cc30260fe86eb00693dc953655611f068b5c4546/tensorflow/lite/kernels/internal/types.h#L197\r\n@advaitjain , do you know why it doesn't hit the assert? It seems like we can input any number, for example:\r\n```\r\n  const RuntimeShape input_shape = tflite::micro::GetTensorShape(input);\r\n  static int test = input_shape.Dims(15);\r\n```\r\nand it won't hit the assert. For all targets and all tests. I have a quick look and `_size` seem ok. I'd prefer this to be handled in a separate ticket.\r\n\r\n> Moreover, I think that we don't need any additional test unit in micro/kernels/fully_connected_test.cc as all the test cases can be used to test the code I want to merge (maybe we just need to update the BUILD file to ensure we use the CMSIS-NN implementation instead of the default one).\r\n\r\nAgree, we can go ahead and merge this PR imo.", "@biagiom Thanks for the analysis. What we see here is the typical difference between release(-DNDEBUG, asserts are disabled) and debug(-DDEBUG, asserts are enabled) builds. @freddan80 that is why we don't hit the asserts.\r\n\r\n I can understand why a release build option is used as without it, the performance of TFLu ref kernels will fall sharply especially because of the use of asserts(its a comparison statement from the compilers point of view) in the inlined Offset() routine of the core loop as well as in the MultiplyByQuantizedMultiplier() routines of common functions. \r\n\r\n Maybe building it with -DDEBUG for the unit tests only would be an option. But, that is a topic outside the purview of this PR, and I think this PR can be merged as it is.\r\n", "Apologies for the delay here.\r\n\r\nWith d6138980ec98e0634002ca4b23bea6061e46fde4, the default makefile build will have these asserts.\r\n\r\nOne more clarification for the fix from this PR:\r\n\r\nA different TFLM user encountered the same issue and proposed a different fix which involved changing the input_dims assignment to:\r\n```cc\r\ninput_dims.n = batches;\r\ninput_dims.h = input_shape.Dims(1);\r\ninput_dims.w = input_shape.DimensionsCount() >= 3 ? input_shape.Dims(2) : 1;\r\ninput_dims.c = input_shape.DimensionsCount() >= 4 ? input_shape.Dims(3) : 1;\r\n```\r\n\r\nAnd this PR is suggesting:\r\n```\r\ninput_dims.n = batches;\r\ninput_dims.h = 1;\r\ninput_dims.w = 1;\r\ninput_dims.c = accum_depth;\r\n```\r\n\r\nWhat is the preferred solution here? It appears to me that the first solution is semantically better but I'm curious what everyone else thinks.", "@advaitjain While the first solution gives a \"I know what I am doing here\" vibe, I look at the dimension checks as something that has already been done during the model/layer creation and used to update accum_depth(a.k.a filter filter dimension). \r\n\r\n The second solution is a simpler one that builds up on known information and is clearer to read. For those reasons, I prefer solution 2.", "Sounds good, let's merge this PR.", "Hi @advaitjain,\r\nsorry for my late comment.\r\n\r\nEven though I think that the first solution is more elegant since it checks the dimensions of the input tensor, the second one (the one I proposed) it simpler and easier to read so I agree with @felix-johnny that we can merge the commit as it is.\r\n\r\nAnyway, thanks to all for your support @freddan80, @felix-johnny, @advaitjain !\r\n\r\nBest regards,\r\nBiagio."]}, {"number": 42313, "title": "TF 2.3.0 build failure when its conda package is built using Anaconda's toolchain ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 7 ppc64le\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source v2.3.0\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.6/3.7\r\n- Installed using virtualenv? pip? conda?: Building conda package of TF 2.3\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n- CUDA/cuDNN version: 10.2/7\r\n\r\n**Describe the problem**\r\nI'm trying to build conda package of TF 2.3 using Anaconda's toolchain with gcc 7.3 on ppc64le. And with gpu variant build, I'm getting following error -\r\n\r\n```\r\nERROR: /home/builder/.cache/bazel/_bazel_builder/738bada2dae39de8837b3ef244ba3e1e/external/flatbuffers/BUILD.bazel:64:1: Linking of rule '@flatbuffers//:flatc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /home/builder/.cache/bazel/_bazel_builder/738bada2dae39de8837b3ef244ba3e1e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla \\\r\n    GCC_HOST_COMPILER_PATH=/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_build_env/bin/powerpc64le-conda_cos7-linux-gnu-cc \\\r\n    LD_LIBRARY_PATH=::/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/extras/CUPTI/lib64::/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/cuda/lib: \\\r\n    PATH=/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_build_env/bin:/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin:/opt/anaconda3/condabin:/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_build_env:/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_build_env/bin:/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla:/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin:/opt/anaconda3/bin:/opt/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python \\\r\n    PYTHON_LIB_PATH=/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0,7.5 \\\r\n    TF_CUDA_PATHS=/opt/anaconda3/conda-bld/tensorflow-base_1597311086398/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla \\\r\n    TF_CUDA_VERSION=10.2 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_SYSTEM_LIBS=org_sqlite \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/ppc-opt/bin/external/flatbuffers/flatc-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\npowerpc64le-conda_cos7-linux-gnu-cc: error: bazel-out/ppc-opt/bin/external/flatbuffers/flatc: No such file or directory\r\nTarget @flatbuffers//:flatc failed to build\r\n```\r\n\r\nEven on Linux x86_64 with gcc 5.4 I'm seeing similar issue with some or other shared library being generated by build. Also, note that only GPU build is getting affected. CPU builds are working fine. And this error occurs only within conda environment. Standalone build of TF using gcc 7.4 works fine on ppc64le.\r\n\r\n**Any other info / logs**\r\nWhen I digged into the TF commits that went into 2.3.0, I found the change done in this commit https://github.com/tensorflow/tensorflow/commit/e0b19f6ef223af40e2e6d1d21b8464c1b2ebee8f#diff-ee64a9d8638aa0a655dfaac625f72bb2 has been causing the above problem. Since the exact problem is not mentioned in the bazel output, I am unable to find out what has gone wrong in the above commit. Reverting the change done in this cc_toolchain_config.bzl.tpl file fixes the problem for me.\r\n\r\nIf somebody could help me figure out what (specifically from above commit) might caused this error, it would be really appreciated.\r\n", "comments": ["@npanpaliya \r\n\r\nSorry, but we don't provide support for issues with the conda environment.\r\nThis issue is more suitable on Continuum [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).\r\nThanks!", "@ravikyram Thanks for your response. I'll raise the issue with Anaconda too.\r\n\r\nI raised the issue here because conda builds used to work before that particular commit and then it has started giving error due to changes done in cc_toolchain_config.bzl.tpl file. So, just want to know if anyone has any pointers on it. The refactoring done in that file is for sure the cause of this error. ", "I've further figured out the exact linker related changes that are causing the error. Changes done in https://github.com/tensorflow/tensorflow/commit/bb4c751414c3562ab3ab4298f866f47438078c37 have introduced the build issues in conda environment which otherwise work fine.", "@npanpaliya One thing I noted was that you are using CUDA10.2 where as tested [build configuration](https://www.tensorflow.org/install/source) of TF2.3 is as follows.\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.3.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 7.6 | 10.1\r\n\r\nMay be there is a in compatibility issue. I am not sure how TF-Anaconda package was built but pip binaries of TF were built with the above config. Thanks \r\n\r\n", "Thanks @jvishnuvardhan . But we have been building conda package for TF 2.2 too with cuda 10.2 and even TF 2.3 works with cuda 10.2 if I revert the commit I mentioned in my above comment. The build error I'm seeing is due to linker options' ordering changes done in that commit.", "@npanpaliya Unfortunately we don't support or maintain Conda packages. I believe you need to post this issue is Anaconda repository. Thanks!", "I've already posted this issue in Anaconda repository. \r\nAlso, tried building TF 2.3 conda package with cuda 10.1 but the error persists. So, cuda version doesn't seem to be problematic.", "I have only reviewed the said change, but I have no knowledge of the conda toolchain.\r\n@hlopko could you take a look if you have time?", "@npanpaliya https://github.com/tensorflow/tensorflow/issues/41856#issuecomment-706422403 may be helpful for you.", "Thanks @njzjz . I was able to get this working by reverting commit https://github.com/tensorflow/tensorflow/commit/bb4c751414c3562ab3ab4298f866f47438078c37 in my builds. Indeed what you've pointed is also on the same lines. ", "@npanpaliya You may also test my fix from #43951", "@Flamefire - This seems to be correct. I'd noticed `-B` to be alone in bazel's params file which contains linker options for the failing targets but I think I overlooked it. I'll try this fix. Thanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42313\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42313\">No</a>\n"]}, {"number": 42312, "title": "Input and output tensors of converted TFLite model do not accord with trained model in TF", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nHardware : Freescale i.MX6 Quad/DualLite\r\nProcessor: ARMv7 Processor rev 10 (v71)\r\nOS Platform and Distribution: Yocto built Linux distribution (kernel 4.9.4+)\r\nThe tf-lite library was built with flex delegate enabled and using default makefile: https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/lite/tools/make/Makefile\r\nAPI : CPP\r\n\r\n**Describe the problem**\r\nI developed a custom image segmentation model (based on ENet's architecture) that works with 320x240x3 images and outputs 320x240 images. The pixel values range from 0 to 2, corresponding with one of the three classes. During training and evaluation everything went fine. Converting the graph to pb and then to TF Lite format messes up the input and output sizes.\r\n\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI trained the ENet based model locally on a GPU. Everything went fine. I then followed the guidelines and then froze the graph (the inputs and outputs) so ended up with a model in protobuf format (.pb). In the last step I converted the model to .lite (using integer quantization). When examining statistics of the model with the TFLite interpreter on my embedded device, I see that the output corresponds to one float only (instead of the expected 320x240 ints) and that the input tensor is of size 921600. (exactly four times the amount of pixels in 320x240x3 images - because, during training, the batch size was 5 I see no link with the batch size). The same is true for the output tensor (`ENET/logits_to_softmax` - node 286) : \r\n![image](https://user-images.githubusercontent.com/29673343/90166030-df0b0e80-dd99-11ea-9547-a583bbd6da19.png)\r\n\r\n\r\nMy main question is thus why the input and output tensors are so 4 times too big to account for the input and output data? \r\n", "comments": ["Inspecting the input and output in Netron of the .pb model results in : ![image](https://user-images.githubusercontent.com/29673343/90118765-01306c80-dd59-11ea-829b-2f06c9771984.png)\r\n![image](https://user-images.githubusercontent.com/29673343/90119350-bb27d880-dd59-11ea-8181-a71322aa99b7.png)\r\n\r\nInspecting the input (and output) in Netron of the .lite model results in : \r\n![image](https://user-images.githubusercontent.com/29673343/90118856-22915880-dd59-11ea-95b1-d312da31b891.png)\r\n![image](https://user-images.githubusercontent.com/29673343/90119427-da266a80-dd59-11ea-8887-77f15936c425.png)\r\n\r\nPart of the output of the cpp script (just after the pre-invoke interpreter state) looks like this :\r\n![image](https://user-images.githubusercontent.com/29673343/90119025-553b5100-dd59-11ea-82c3-1cbc224e6877.png)\r\n\r\nI also like to mention that picking a number bigger than the input tensor (921602 > 921600) does not result in a segmentation fault (while 2 000 000 does).\r\n\r\n ", "The script I used for conversion to protobuf:\r\nhttps://github.com/tensorflow/tensorflow/commits/master/tensorflow/python/tools/freeze_graph.py\r\n``` \r\npython2 freeze_graph.py \\\r\n--input_graph=./log/enet_cut_16layers_filtersdown_320240/graph.pbtxt \\\r\n--input_checkpoint=./log/enet_cut_16layers_filtersdown_320240/modelfinal.ckpt \\\r\n--output_graph=./log/enet_cut_16layers_filtersdown_320240/enet_cut_16layers_filtersdown_320240.pb \\\r\n--output_node_names=ENet/fullconv/BiasAdd,ENet/logits_to_softmax \\\r\n--input_binary=False\r\n```\r\n(python 2 as the ENet repo I started off with was written in python 2)\r\n\r\nThe python file I invoked to convert .pb to .lite like so: `python3 convert_tf_tflite.py enet_cut_16layers_filtersdown_320240.pb out.lite`\r\nis (this is the one without integer quantization): \r\n```\r\nimport tensorflow as tf\r\nimport sys\r\n\r\n\r\ndef convert(inputpath, outputpath):\r\n    # make a converter object from the saved tensorflow file\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(inputpath, #TensorFlow freezegraph .pb model file\r\n                                                          input_arrays=[\"input\"], before.\r\n                                                          output_arrays=[\"ENet/logits_to_softmax\"]) \r\n                                                          #output_arrays=[\"output/BiasAdd\"],)\r\n    # tell converter which type of optimization techniques to use\r\n    # converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, # to use tf lite builtins\r\n                                           tf.lite.OpsSet.SELECT_TF_OPS] # because some ops are not part of the tf lite builtins\r\n    \r\n    # to view the best option for optimization read documentation of tflite about optimization\r\n    # go to this link https://www.tensorflow.org/lite/guide/get_started#4_optimize_your_model_optional\r\n\r\n    # convert the model\r\n    tf_lite_model = converter.convert()\r\n    # save the converted model\r\n    open(outputpath, 'wb').write(tf_lite_model)\r\n\r\nif __name__==\"__main__\":\r\n    print(sys.argv[1])\r\n    convert(sys.argv[1],sys.argv[2])\r\n\r\n```\r\n\r\nI can also send over the models (they are small in size - about 600kb)", "I just found out that the repo I started off with has the same problem.\r\nhttps://github.com/PINTO0309/TensorFlow-ENet/blob/pinto0309work/checkpoint/semanticsegmentation_enet.pb\r\n\r\nWhen I turn this into TF Lite, the input and output tensors don't accord with the amount of pixels/data", "I've lost interest in ENet for now, but it might be worthwhile to try Integer Quantization on the .pb in the path below with FlexDelegate enabled. \r\n**https://github.com/PINTO0309/TensorflowLite-flexdelegate/tree/master/models/enet**", "Thanks for your reply, the problem is not speed or quantization related. It's about the input and output of the graph that are messed up when converting to protobuf format. Output is an array of 921600 elements according to TFLite but should actually be 320x240. The same can be said for the input (921600 instead of 320x240x3 elements)", "Here goes the protobuf model used in all the code above, and the same model converted to TF Lite. The netron visualisations were carried out with these models. \r\n[pb_and_lite.model.zip](https://github.com/tensorflow/tensorflow/files/5070342/pb_and_lite.model.zip)\r\n", "@abattery can you take a look? This might just be a matter of changing the output tensor name during conversion.", "I updated my comments to make the problem clearer: \r\n\r\n- I'd like to know why the input and output tensor have a shape that is much larger than the input and output images the network has been trained with (912600 in both cases, while I expect input to have an array length of 320x240x3 and output array lenght of 320x240)\r\n- I'd like to know why surpassing this array with length 912600 (e.g. feeding the input 9126002 elements) does not result in a segmentation error - while feeding 2 000 000 elements does\r\n- I'd like to know why netron does not show the correct output sizes (1 float instead of e.g. [1,320,240,1])\r\n\r\nA previous question I had but figured out myself is why the output of the TFLite was also just a float, but that was because I was reading the output wrongly", "> Output is an array of 921600 elements according to TFLite\r\n\r\nHow are you checking this? Note that the *byte* size of the tensor will be 9126000 if the shape is 320x240x3 for a float tensor.\r\n\r\n> e.g. feeding the input 9126002 elements)\r\n\r\nHow exactly are you feeding in the elements? If you're just writing to an array (in C++), then this is undefined behavior and subject to the same issues as out-of-bounds array access with C++ arrays.\r\n\r\n", "After loading both models from https://github.com/tensorflow/tensorflow/issues/42312#issuecomment-673592182,\r\n\r\nBoth TF and TFLite models have the same input/output tensor size in the netron tool.\r\n\r\nFrozen graph: Input name:input, type:float32 (?, 320, 240, 3) -> Output name:ENet/logits_to_softmax type: float32\r\nLite graph: Input name:input type:float32 (1, 320, 240, 3) -> Output name:ENet/logits_to_softmax type:float32.\r\n\r\nThey look fine to me.", "@FlorentijnD  Please check the previous comment and let us know if you have any concern still. Thanks!", "Thanks for coming back on this. @abattery's comment solved my issue and forgot to close this thread. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42312\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42312\">No</a>\n"]}, {"number": 42311, "title": "Lambda layer with custom function gives gradient error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Geforce RTX 2080 \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have created a GAN style model with a weightless transformation layer as the final layer in the generator. This layer is a Lambda layer, and the function it calls uses only Tensorflow ops. When running a training loop using train_on_batch, I get the following error (the layers mentioned in the error are layers of the generator):\r\n```\r\nValueError: No gradients provided for any variable: ['dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0'].\r\n```\r\n\r\nThe model is meant to take in latent noise and produce 4 outputs corresponding to two (x, y) pairs representing the top left and bottom right corners of a square on a canvas. The Lambda layer converts these coordinates into an image of a square which is then fed into the discriminator.\r\n\r\n**Describe the expected behavior**\r\nI expect the model to be able to train using my training loop with no issues and to be able to use the custom Lambda function / layer that I wrote to transform the Dense layer outputs into an image Tensor.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI have included two main GitHub gists with my code. One file is the training file and the other builds the GAN models:\r\n\r\nTrain.py: https://gist.github.com/micahreich/13208980a22be2f53e3f8eba05c0db72\r\nModels.py: https://gist.github.com/micahreich/80e44710417707bf3fb6327cb9946b79\r\n\r\nIt should be noted that the train.py training file loads a numpy file which contains the dataset. If you want to create some data, use the data loader: https://gist.github.com/micahreich/fde2024365ec91c818af6e5203d4f98e (change n_samples to a small number for testing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe full error traceback is below:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 95, in <module>\r\n    TL.train()\r\n  File \"train.py\", line 83, in train\r\n    g_loss = self.gan.train_on_batch(noise, valid)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\", line 1348, in train_on_batch\r\n    logs = train_function(iterator)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/mirrored_strategy.py:770 _call_for_each_replica\r\n        fn, args, kwargs)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/mirrored_strategy.py:201 _call_for_each_replica\r\n        coord.join(threads)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py:389 join\r\n        six.reraise(*self._exc_info_to_raise)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/six.py:703 reraise\r\n        raise value\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\r\n        yield\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/distribute/mirrored_strategy.py:998 run\r\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py:541 train_step  **\r\n        self.trainable_variables)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py:1804 _minimize\r\n        trainable_variables))\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients\r\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\r\n    /nethome/mreich8/.local/lib/python3.5/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads\r\n        ([v.name for _, v in grads_and_vars],))\r\n\r\n    ValueError: No gradients provided for any variable: ['dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0'].\r\n```\r\n\r\nThank you.", "comments": ["I believe that the issue is caused by the fact that the Lambda layer isn't differentiable because of the tf ops used within the layer -- is this correct? If so, how can I either bypass this or find a workaround? ", "@Saduf2019 I'm now able to compile the model and get it to start training, but the loss does not change -- something in the Lambda layer is preventing the gradients from updating or changing... how can I fix this? I tried adding @tf.custom_gradient decorator and setting the gradient to be 1.0 and 0.0, but no change. Any advice?", "@micahreich \r\nPlease provide with latest indented code for me to replicate the issue, or if possible share a colab gist with the error.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42311\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42311\">No</a>\n", "> \r\n> \r\n> @Saduf2019 I'm now able to compile the model and get it to start training, but the loss does not change -- something in the Lambda layer is preventing the gradients from updating or changing... how can I fix this? I tried adding @tf.custom_gradient decorator and setting the gradient to be 1.0 and 0.0, but no change. Any advice?\r\n\r\nHi, what are the changes you have made to start the training?"]}, {"number": 42310, "title": "tensorflow lite inference via c++ : interpreter->output_tensor(0) gives wrong tensor value", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.4 and windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Samsung  Galaxy S8\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.8\r\n- Bazel version (if compiling from source):  3.1.0\r\n- GCC/Compiler version (if compiling from source): c++11\r\n- CUDA/cuDNN version: Non\r\n- GPU model and memory: Tesla V100, 16G memory\r\n\r\n**Describe the current behavior**\r\nI tried to run a TensorFlow (v2.3.0) lite inference of Mobilentnet v2 in various languages, such as python, Android (Java), and C++ (Android JNI). It works well for python and Android(Java), but the output in C++ (Android JNI) is a mess.\r\n\r\nIf I roll back to Tensorflow v2.0.0, it works well in python, Android, and c++.\r\n\r\nThe code in Android JNI c++  is as below:\r\n_model = tflite::FlatBufferModel::BuildFromBuffer(data,numBytesRead );\r\ntflite::ops::builtin::BuiltinOpResolver resolver;  \r\ntflite::InterpreterBuilder(*_model, resolver)(&interpreter);\r\nif (interpreter->AllocateTensors() != kTfLiteOk ){ \r\n        return false;\r\n    }\r\ninterpreter->SetNumThreads(1);\r\n\r\nif (interpreter->outputs().size() != 1) {  \r\n     return false;\r\n}\r\noutputs = interpreter->output_tensor(0);\r\n\r\nI debugged the code and found several issues:\r\na. interpreter->outputs().size() = 18446743945091386451\r\nb. outputs->dims->shape = 4\r\nc. [pseudo code] outputs->dims->data[0,1,2,3] = 1, 224, 224, 3\r\n\r\nI debugged it and found the issue of \r\n**Describe the expected behavior**\r\na. interpreter->outputs().size() = 1\r\nb. outputs->dims->shape = 2\r\nc. [pseudo code] outputs->dims->data[0,1] = 1, 1000\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nMobilenet V2 is from keras_applications.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nNA.", "comments": ["I have a similar situation happening on Arduino_TensorFlowLite, where  \r\n**interpreter->output(0)** and  **interpreter->input(0)**   seem to be sharing values.\r\n\r\n**interpreter->output(0)->dims->size** seems to have the same value as **interpreter->input(0)->dims->size**.\r\n\r\nAlso **interpreter->input(0)->data.f[0]** value can be set before  interpreter->Invoke(); but after  interpreter->Invoke();   **interpreter->input(0)->data.f[0]** seems to have the same value as  **interpreter->output(0)->data.f[0]**\r\n\r\nI have semi-helpful code [here](https://github.com/hpssjellis/my-examples-for-the-arduino-portentaH7/blob/master/m09-Tensoflow/b03_makerML_layers.ino). One file has everything in it. The model has 2 inputs but only one output. The output shows **interpreter->input(0)->dims->size** as 2 when it should be 1.\r\n\r\n\r\n", "Hi, could you please try again with the recent Tensorflow version and let us know if the issue still exists, Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42310\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42310\">No</a>\n"]}, {"number": 42309, "title": "Ubuntu 20.04 libcudnn.so.7", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.2\r\n- CUDA/cuDNN version: 10.2, 8.0.2\r\n- GPU model and memory: nVidia GeForce GT 710, 1GB of memory\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen i import tensorflow with `import tensorflow as tf` everything is fine. But when I try to test the if the gpu is working, this error shows up.\r\n`\r\nCould not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:\r\n`\r\nIt says that the directories don't exist so i thought it was a permission problem. I added the permission with `chmod a+r` but nothing changed.\r\n\r\n**Describe the expected behavior**\r\nI want it to work with my GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n`import tensorflow as tf`\r\n`print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))`\r\n", "comments": ["@PeterGardas,\r\nAs per the [tested build configurations](https://www.tensorflow.org/install/source#gpu), could you please try installing CUDA 10.1 and cuDNN 7.6, and check if you are still facing the same issue. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Also ubuntu 20.04\r\n```\r\n$ find /usr/local -name libcudnn.so\\*\r\n$\r\n\r\n...\r\npciBusID: 0000:01:00.0\r\n2020-10-05 12:25:27.360723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-10-05 12:25:27.361705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-10-05 12:25:27.362587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-10-05 12:25:27.362841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-10-05 12:25:27.363938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-10-05 12:25:27.364783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-10-05 12:25:27.364883: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/targets/x86_64-linux/lib/\r\n```\r\n\r\n"]}, {"number": 42308, "title": "Exception on converting the 'stacked LSTM model' as a 'TF Lite' format.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MUL, RANGE, RESHAPE, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nThe exception was triggered when I try to convert the stacked LSTM model.\r\nThe original source code is provided as following URL.\r\n -> https://github.com/YeongHyeon/FARED_for_Anomaly_Detection\r\nFor making 'tflite' file, I have already trained the above neural network.\r\nAlso, I have prepared the 'pbtxt' and 'pb' files\r\nPlease give me some solution!\r\nThank you and have a nice day.\r\n:)\r\n", "comments": ["@YeongHyeon \r\nCan you please update the issue template with the tensorflow version used for us to replicate the issue.\r\nPlease provide \"simple stand alone code\" for us to replicate the issue or if possible share a colab gist with the error faced.\r\n\r\nWith respect to the error can you please refer to these issues and update us:\r\n#34266 #29117 [link](https://github.com/tensorflow/tensorflow/issues/34868#issuecomment-564180966) [link1](https://stackoverflow.com/questions/57774575/tflite-converter-randomstandardnormal-implemented-for-keras-model-but-not-for)", "The new model, working as the same function, was developed for solving this issue.\r\nIn the LSTM series, it is often a problem that I found via the web searching, so I have replaced the LSTM layer to a convolutional layer.\r\n\r\nIt seems TensorFlow 2 version has solved the above problem.\r\nI will try them in future times and I hope to contact you again.\r\n\r\nThank you for answering.", "Thank you for your update. glad that your issue is resolved, moving this to closed status."]}, {"number": 42307, "title": "Enabling a debug dll build under Windows (without CUDA, at least)", "body": "Modified SpaceToDepthOp and DepthToSpaceOp templated classes not to use a SpaceToDepthOpFunctor/DepthToSpaceOpFunctor structs with a template parameter Device=_GPU_Device in case the class itself is instantiated with Device=_CPU_Device. Added a partial template specialization for Device=GPUDevice to preserve the existing behaviour in all cases.\r\n\r\nThis at least partially (i.e. when bazel is configured not to use CUDA) fixes issue #41118.", "comments": ["@rthadur  Re-opening PR #42268 with master branch this time.", "Please check out the much more concise change version :)\r\n\r\nUPD: Hm. It's more concise alright, but the debug build now does not work again. It used to complain about missing symbols for the () operator of `SpaceToDepthOpFunctor<GPUDevice, ...>`, now I got same linker errors for `SpaceToDepthOpFunctor<CPUDevice, ..., FORMAT_NCHW>` because the CPUDevice-functor is only instantiated for `FORMAT_NHWC` as the last template argument..."]}, {"number": 42306, "title": "InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'", "body": "**System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**  Google Colab (Ubuntu 18.04.3 LTS)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:** N/A\r\n- **TensorFlow installed from (source or binary):** Pre-installed / source\r\n- **TensorFlow version (use command below):** 2.3.0 (and tried tf-nightly with 2.4.0-dev20200812)\r\n- **Python version:** 3.6.9\r\n- **Bazel version (if compiling from source):** N/A\r\n- **GCC/Compiler version (if compiling from source):** N/A\r\n- **CUDA/cuDNN version:** CUDA Version = 10.1\r\n- **GPU model and memory:** Tesla T4 and 12G\r\n\r\n**Describe the problem**\r\nI am trying to implement a custom multi-input and -output model which uses a learning algorithm as proposed in a research paper. The model itself works fine without the custom learning algorithm which I use as a baseline. The problem I encounter is that the code got stuck in ```mc_pred = self.main_classifier([xu, xs], training=True)``` in the train_step function in the DebiasModel class.\r\n\r\nIt did not return an error. After running for an hour, I interrupted the kernel and it returns the error message saying:\r\n\r\n```\r\nInvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.\r\n```\r\nI am not sure what the issue is and I have tried it with ```tf-nighly``` and to use ```persistent=True``` in tf.GradientTape as well instead of declaring two gradientTapes in single watch. But, exactly the same error occurs.\r\n\r\nDoes anyone have any idea what this issue is? And how it can be solved?\r\n\r\n**Source code**\r\n```\r\nimport string\r\nimport nltk\r\nimport time\r\n\r\nnltk.download('punkt')\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\n\r\nfrom keras.layers import Embedding\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.layers import *\r\n\r\n# Load dataset\r\nexample_df = pd.read_csv('example.csv')\r\n\r\n# Sample train/validation/test data\r\nnp.random.seed(100)\r\ntrain, validation, test = np.split(example_df.sample(frac=1), [int(.7*len(example_df)), int(.85*len(example_df))])\r\n\r\n# Class Weight Function\r\ndef compute_sample_weights(df, target, t_expect, weight_name):\r\n  # Setup mitigator_weight\r\n  df[weight_name] = 0\r\n\r\n  # Get frequencies per target level\r\n  targets = df.groupby(target).size()\r\n\r\n  # Compute sample weights\r\n  target_weights = t_expect / (targets / targets.sum())\r\n\r\n  # Convert to dictionary\r\n  target_dict = target_weights.to_dict()\r\n\r\n  # Add sample weights to dataframe\r\n  for i in target_dict:\r\n    df[weight_name] = np.where((df[target] == i), target_dict[i], df[weight_name])\r\n  \r\n  return df\r\n\r\n# Compute Main Class Weights\r\ntrain = compute_sample_weights(df=train, target='target', t_expect=(1/3), weight_name='mainClass_weight')\r\n\r\n# Compute Protect Class Weights\r\ntrain = compute_sample_weights(df=train, target='protect', t_expect=(1/2), weight_name='protectClass_weight')\r\n\r\n# Preprocess Text Data\r\nvocab_size = 25000\r\nmax_length = 300\r\npadding_type = 'post'\r\ntrunc_type = 'post'\r\noov_tok = '<unk>'\r\n\r\ndef Text_to_Seq(train, val, test, vocab_size, max_length, padding_type, trunc_type, oov_tok):\r\n    # Text tokenization\r\n    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\r\n    tokenizer.fit_on_texts(train)\r\n\r\n    # Create word_index\r\n    word_index = tokenizer.word_index\r\n\r\n    # Transforms each text doc to a sequence of integers in train, val and test\r\n    x = tokenizer.texts_to_sequences(train)\r\n    y = tokenizer.texts_to_sequences(val)\r\n    z = tokenizer.texts_to_sequences(test)\r\n\r\n    # Pad sequences to the same length\r\n    x = pad_sequences(x, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n    y = pad_sequences(y, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n    z = pad_sequences(z, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n\r\n    return x, y, z, word_index\r\n\r\ntrain_text = train['reviewText']\r\nval_text = validation['reviewText']\r\ntest_text = test['reviewText']\r\n\r\ntrain_text, val_text, test_text, word_index = Text_to_Seq(train_text, val_text, test_text, vocab_size, max_length, padding_type, trunc_type, oov_tok)\r\n\r\n# Create input, target, protect, sample_weights for Train\r\nxu_train = np.array(train_text, dtype=np.int32)\r\nxs_train = np.array(train.iloc[:, np.r_[0, 2, 10:30]], dtype=np.int32)\r\ny_train = np.array(train[['target_negative', 'target_neutral', 'target_positive']], dtype=np.float32)\r\nz_train = np.array(train['protect_m'], dtype=np.float32).reshape((-1,1))\r\nmainClass_weight = np.array(train['mainClass_weight'], dtype=np.float32).reshape((-1,1))\r\nprotectClass_weight = np.array(train['protectClass_weight'], dtype=np.float32).reshape((-1,1))\r\n\r\n# Create input, target, protect for Validation\r\nxu_val = np.array(val_text, dtype=np.int32)\r\nxs_val = np.array(validation.iloc[:, np.r_[0, 2, 8:28]], dtype=np.int32)\r\ny_val = np.array(validation[['target_negative', 'target_neutral', 'target_positive']], dtype=np.float32)\r\nz_val = np.array(validation['protect_m'], dtype=np.float32).reshape((-1,1))\r\n\r\n# Create input, target, protect for Test\r\nxu_test = np.array(test_text, dtype=np.int32)\r\nxs_test = np.array(test.iloc[:, np.r_[0, 2, 8:28]], dtype=np.int32)\r\ny_test = np.array(test[['target_negative', 'target_neutral', 'target_positive']], dtype=np.float32)\r\nz_test = np.array(test['protect_m'], dtype=np.float32).reshape((-1,1))\r\n```\r\n```\r\n# Setup Pretrained GloVe Embedding\r\ndef load_embedding(file_path):\r\n    # Initialize embeddings_index\r\n    embeddings_index = {}\r\n\r\n    # Store pretrained word vectors in embeddings_index\r\n    with open(file_path) as f:\r\n        for line in f:\r\n            values = line.split(\" \")\r\n            word = values[0]\r\n            coefs = np.asarray(values[1:], dtype='float32')\r\n            embeddings_index[word] = coefs\r\n\r\n    # Print number of word vectors found\r\n    print(\"Found %s word vectors.\" % len(embeddings_index))\r\n\r\n    return embeddings_index\r\n\r\nembeddings_index = load_embedding('glove.840B.300d.txt') # obtained from https://nlp.stanford.edu/projects/glove/\r\n\r\n# Create Embedding matrix\r\nnum_tokens = min(vocab_size, len(word_index))+1\r\nembed_dim = 300\r\n\r\ndef embedding_matrix(word_index, embeddings_index, num_tokens, embed_dim):\r\n    # Initialize embedding_matrix and counters\r\n    hits = 0\r\n    misses = 0\r\n    embedding_matrix = np.zeros((num_tokens, embed_dim))\r\n\r\n    # Create embedding_matrix\r\n    for word, i in word_index.items():\r\n      if i > vocab_size:\r\n        continue\r\n      embedding_vector = embeddings_index.get(word)\r\n      if embedding_vector is not None:\r\n        embedding_matrix[i] = embedding_vector\r\n        hits += 1\r\n      else:\r\n        misses += 1\r\n\r\n    # Print number of hits and misses\r\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\r\n\r\n    return embedding_matrix\r\n\r\nembedding_matrix = embedding_matrix(word_index, embeddings_index, num_tokens, embed_dim)\r\n```\r\n```\r\nclass model_components:\r\n\r\n  def mitigation_expert():\r\n    inputs = Input(shape=(300,), dtype=tf.int32, name=\"me_input\")\r\n    x = Embedding(num_tokens, 300, weights=[embedding_matrix], input_length=max_length, trainable=False, name=\"me_embedding\")(inputs)\r\n    x = LSTM(300, return_sequences=False, name=\"me_lstm\")(x)\r\n\r\n    model = Model(inputs, x)\r\n\r\n    return model\r\n\r\n  def control_expert():\r\n    inputs = Input(shape=(22,), dtype=tf.int32, name=\"ce_input\")\r\n    y = Dense(19, activation='relu', name=\"ce_hidden\")(inputs)\r\n\r\n    model = Model(inputs, y)\r\n\r\n    return model\r\n\r\n  def main_classifier():\r\n    # Expert components\r\n    me = model_components.mitigation_expert()\r\n    ce = model_components.control_expert()\r\n\r\n    # Main classifier\r\n    ensemble = concatenate([me.output, ce.output], name=\"pred_ensemble\")\r\n    pred_output = Dense(319, activation=\"relu\", name=\"pred_hidden\")(ensemble)\r\n    pred_output = Dense(3, activation=\"softmax\", name=\"pred_output\")(pred_output)\r\n\r\n    model = Model(inputs=[me.input, ce.input], outputs=pred_output, name=\"main_classifier\")\r\n\r\n    return model\r\n  \r\n  def adversary_classifier():\r\n    # Mitigation Expert component\r\n    me = model_components.mitigation_expert()\r\n\r\n    # Adversary classifier\r\n    adv_output = Dense(300, activation='relu', name=\"adv_hidden\")(me.output)\r\n    adv_output = Dense(1, activation='sigmoid', name=\"adv_output\")(adv_output)\r\n\r\n    model = Model(inputs=me.input, outputs=adv_output, name=\"adversary_classifier\")\r\n\r\n    return model\r\n\r\ndef tf_normalize(x):\r\n  return x / (tf.norm(x) + np.finfo(np.float32).tiny)\r\n\r\nclass DebiasModel(keras.Model):\r\n    def __init__(self, main_classifier, adversary_classifier):\r\n        super(DebiasModel, self).__init__()\r\n        self.main_classifier = main_classifier\r\n        self.adversary_classifier = adversary_classifier\r\n\r\n    def compile(self, mc_optimizer, adv_optimizer, mc_loss, adv_loss, debias_param):\r\n        super(DebiasModel, self).compile()\r\n        self.mc_optimizer = mc_optimizer\r\n        self.adv_optimizer = adv_optimizer\r\n        self.mc_loss = mc_loss\r\n        self.adv_loss = adv_loss\r\n        self.debias_param = debias_param\r\n\r\n    def train_step(self, data):\r\n      # Unpack data from model.fit()\r\n      x, y, sample_weight = data\r\n\r\n      # Unpack input and output features\r\n      xu, xs = x\r\n      y_mc = y['pred_output']\r\n      z_adv = y['adv_output']\r\n\r\n      # Unpack sample_weights\r\n      mainClass_weights = sample_weight[\"pred_output\"]\r\n      protectClass_weights = sample_weight[\"adv_output\"]\r\n\r\n      # Generate prediction and compute loss for Main_Classifier\r\n      with tf.GradientTape() as mc_tape, tf.GradientTape() as me_mc_tape:\r\n        mc_pred = self.main_classifier([xu, xs], training=True)\r\n        mc_loss = self.mc_loss(y_mc, mc_pred, sample_weight=mainClass_weights)\r\n      \r\n      # Compute and Apply Gradients for CE & Main Classifier\r\n      mc_trainable_vars = self.main_classifier.trainable_weights[3:]\r\n      mc_grads = mc_tape.gradient(mc_loss, mc_trainable_vars)\r\n      self.mc_optimizer.apply_gradients(zip(mc_grads, mc_trainable_vars))\r\n\r\n      # Generate prediction and compute loss for Adversary_Classifier\r\n      with tf.GradientTape() as adv_tape, tf.GradientTape() as me_adv_tape:\r\n        adv_pred = self.adversary_classifier(xu)\r\n        adv_loss = self.adv_loss(z_adv, adv_pred, sample_weight=protectClass_weights)\r\n      \r\n      # Compute and Apply Gradients for CE & Main Classifier\r\n      adv_trainable_vars = self.adversary_classifier.trainable_weights[3:]\r\n      adv_grads = adv_tape.gradient(adv_loss, adv_trainable_vars)\r\n      self.adv_optimizer.apply_gradients(zip(adv_grads, adv_trainable_vars))\r\n\r\n      # Compute and Apply Gradients to debias ME\r\n      me_adv_debias_trainable_vars = self.adversary_classifier.trainable_weights[:3]\r\n      adv_debias_grads = me_adv_tape.gradient(adv_loss, me_adv_debias_trainable_vars)\r\n      adv_debias_dict = tf.lookup.StaticHashTable(\r\n          tf.lookup.KeyValueTensorInitializer(me_adv_debias_trainable_vars, adv_debias_grads), 0)\r\n      \r\n      me_mc_debias_trainable_vars = self.main_classifier.trainable_weights[:3]\r\n      mc_debias_grads = me_mc_tape.gradient(mc_loss, me_mc_debias_trainable_vars)\r\n\r\n      me_grads = []\r\n\r\n      for g, v in zip(mc_debias_grads, me_mc_debias_trainable_vars):\r\n        unit_adv = tf_normalize(adv_debias_dict.lookup(v))\r\n        g -= tf.math.reduce_sum(g * unit_adv) * unit_adv\r\n        g -= self.debias_param * adv_debias_dict.lookup(v)\r\n        me_grads.append(zip(g, v))\r\n      \r\n      self.mc_optimizer.apply_gradients(me_grads)\r\n      \r\n      return {\"pred_loss\": mc_loss, \"adv_loss\": adv_loss}\r\n\r\n# Build and Fit Model\r\nmodel = DebiasModel(model_components.main_classifier(),\r\n                    model_components.adversary_classifier())\r\n\r\nmodel.compile(mc_optimizer=tf.keras.optimizers.Adam(),\r\n              adv_optimizer=tf.keras.optimizers.Adam(),\r\n              mc_loss=tf.keras.losses.CategoricalCrossentropy(),\r\n              adv_loss=tf.keras.losses.BinaryCrossentropy(),\r\n              debias_param=1)\r\n\r\nepoch = 5\r\nsample_weights = {\r\n    \"pred_output\": mainClass_weight,\r\n    \"adv_output\": protectClass_weight,}\r\n\r\nmodel.fit(x=[xu_train, xs_train],\r\n          y={\"pred_output\": y_train, \"adv_output\": z_train},\r\n          validation_data=([xu_val, xs_val], {\"pred_output\": y_val, \"adv_output\": z_val}),\r\n          sample_weight=sample_weights,\tepochs=epoch, batch_size=256, verbose=1)\r\n```\r\n\r\n**Error Traceback Log**\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2485       with c_api_util.tf_buffer() as buf:\r\n-> 2486         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2487         data = pywrap_tf_session.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n54 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2489       # Convert to ValueError for backwards compatibility.\r\n-> 2490       raise ValueError(str(e))\r\n   2491     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'while' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nKeyboardInterrupt                         Traceback (most recent call last)\r\n<ipython-input-43-e00f829ae927> in <module>()\r\n      8           y={\"pred_output\": y_train, \"adv_output\": z_train},\r\n      9           validation_data=([xu_val, xs_val], {\"pred_output\": y_val, \"adv_output\": z_val}),\r\n---> 10           sample_weight=sample_weights,\tepochs=epoch, batch_size=256, verbose=1)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    821       # This is the first call of __call__, so we have to initialize.\r\n    822       initializers = []\r\n--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    824     finally:\r\n    825       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    695     self._concrete_stateful_fn = (\r\n    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 697             *args, **kwds))\r\n    698 \r\n    699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    967                     recursive=True,\r\n    968                     optional_features=autograph_options,\r\n--> 969                     user_requested=True,\r\n    970                 ))\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)\r\n    594     try:\r\n    595       if kwargs is not None:\r\n--> 596         result = converted_f(*effective_args, **kwargs)\r\n    597       else:\r\n    598         result = converted_f(*effective_args)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in tf__train_function(iterator)\r\n     14                 try:\r\n     15                     do_return = True\r\n---> 16                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\r\n     17                 except:\r\n     18                     do_return = False\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)\r\n    530 \r\n    531   if not options.user_requested and conversion.is_whitelisted(f):\r\n--> 532     return _call_unconverted(f, args, kwargs, options)\r\n    533 \r\n    534   # internal_convert_user_code is for example turned off when issuing a dynamic\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)\r\n    338   if kwargs is not None:\r\n    339     return f(*args, **kwargs)\r\n--> 340   return f(*args)\r\n    341 \r\n    342 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in step_function(model, iterator)\r\n    794 \r\n    795       data = next(iterator)\r\n--> 796       outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    797       outputs = reduce_per_replica(\r\n    798           outputs, self.distribute_strategy, reduction='first')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in run(***failed resolving arguments***)\r\n   1209       fn = autograph.tf_convert(\r\n   1210           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\r\n-> 1211       return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n   1212 \r\n   1213   # TODO(b/151224785): Remove deprecated alias.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in call_for_each_replica(self, fn, args, kwargs)\r\n   2583       kwargs = {}\r\n   2584     with self._container_strategy().scope():\r\n-> 2585       return self._call_for_each_replica(fn, args, kwargs)\r\n   2586 \r\n   2587   def _call_for_each_replica(self, fn, args, kwargs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in _call_for_each_replica(self, fn, args, kwargs)\r\n   2943         self._container_strategy(),\r\n   2944         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n-> 2945       return fn(*args, **kwargs)\r\n   2946 \r\n   2947   def _reduce_to(self, reduce_op, value, destinations, experimental_hints):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    253       try:\r\n    254         with conversion_ctx:\r\n--> 255           return converted_call(f, args, kwargs, options=options)\r\n    256       except Exception as e:  # pylint:disable=broad-except\r\n    257         if hasattr(e, 'ag_error_metadata'):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)\r\n    530 \r\n    531   if not options.user_requested and conversion.is_whitelisted(f):\r\n--> 532     return _call_unconverted(f, args, kwargs, options)\r\n    533 \r\n    534   # internal_convert_user_code is for example turned off when issuing a dynamic\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)\r\n    337 \r\n    338   if kwargs is not None:\r\n--> 339     return f(*args, **kwargs)\r\n    340   return f(*args)\r\n    341 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in run_step(data)\r\n    787 \r\n    788       def run_step(data):\r\n--> 789         outputs = model.train_step(data)\r\n    790         # Ensure counter is updated only if `train_step` succeeds.\r\n    791         with ops.control_dependencies(_minimum_control_deps(outputs)):\r\n\r\n<ipython-input-41-7d8c3d718a99> in train_step(self, data)\r\n     39       # Generate prediction and compute loss for Main_Classifier\r\n     40       with tf.GradientTape() as mc_tape, tf.GradientTape() as me_mc_tape:\r\n---> 41         mc_pred = self.main_classifier([xu, xs], training=True)\r\n     42         mc_loss = self.mc_loss(y_mc, mc_pred, sample_weight=mainClass_weights)\r\n     43 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    983 \r\n    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n    986 \r\n    987         if self._activity_regularizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)\r\n    384     \"\"\"\r\n    385     return self._run_internal_graph(\r\n--> 386         inputs, training=training, mask=mask)\r\n    387 \r\n    388   def compute_output_shape(self, input_shape):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)\r\n    506 \r\n    507         args, kwargs = node.map_arguments(tensor_dict)\r\n--> 508         outputs = node.layer(*args, **kwargs)\r\n    509 \r\n    510         # Update tensor_dict.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    661 \r\n    662     if initial_state is None and constants is None:\r\n--> 663       return super(RNN, self).__call__(inputs, **kwargs)\r\n    664 \r\n    665     # If any of `initial_state` or `constants` are specified and are Keras\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    983 \r\n    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n    986 \r\n    987         if self._activity_regularizer:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)\r\n   1181       else:\r\n   1182         (last_output, outputs, new_h, new_c,\r\n-> 1183          runtime) = lstm_with_backend_selection(**normal_lstm_kwargs)\r\n   1184 \r\n   1185       states = [new_h, new_c]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/recurrent_v2.py in lstm_with_backend_selection(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\r\n   1556   # grappler will kick in during session execution to optimize the graph.\r\n   1557   last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(\r\n-> 1558       **params)\r\n   1559   function.register(defun_gpu_lstm, **params)\r\n   1560 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2827     with self._lock:\r\n   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2830 \r\n   2831   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)\r\n   1846                            resource_variable_ops.BaseResourceVariable))],\r\n   1847         captured_inputs=self.captured_inputs,\r\n-> 1848         cancellation_manager=cancellation_manager)\r\n   1849 \r\n   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1927         possible_gradient_type,\r\n   1928         executing_eagerly)\r\n-> 1929     forward_function, args_with_tangents = forward_backward.forward()\r\n   1930     if executing_eagerly:\r\n   1931       flat_outputs = forward_function.call(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self)\r\n   1431     \"\"\"Builds or retrieves a forward function for this call.\"\"\"\r\n   1432     forward_function = self._functions.forward(\r\n-> 1433         self._inference_args, self._input_tangents)\r\n   1434     return forward_function, self._inference_args + self._input_tangents\r\n   1435 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in forward(self, inference_args, input_tangents)\r\n   1187       (self._forward, self._forward_graph, self._backward,\r\n   1188        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (\r\n-> 1189            self._forward_and_backward_functions(inference_args, input_tangents))\r\n   1190     return self._forward\r\n   1191 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)\r\n   1387       outputs = list(self._func_graph.outputs)\r\n   1388       self._build_functions_for_outputs(\r\n-> 1389           outputs, inference_args, input_tangents)\r\n   1390     (forward_function, forward_graph,\r\n   1391      backward_function, output_indices, num_output_tangents) = (\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)\r\n    897             self._func_graph.inputs,\r\n    898             grad_ys=gradients_wrt_outputs,\r\n--> 899             src_graph=self._func_graph)\r\n    900 \r\n    901       captures_from_forward = [\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py in <lambda>()\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py in _WhileGrad(op, *grads)\r\n    395   cond_grad_graph = func_graph_module.func_graph_from_py_func(\r\n    396       grad_cond_name, grad_cond, loop_vars, {},\r\n--> 397       func_graph=util.WhileCondFuncGraph(grad_cond_name))\r\n    398 \r\n    399   _check_num_inputs_outputs(cond_grad_graph, body_grad_graph, len(loop_vars))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    901       kwarg_shapes = None\r\n    902     func_args = _get_defun_inputs_from_args(\r\n--> 903         args, arg_names, flat_shapes=arg_shapes)\r\n    904     func_kwargs = _get_defun_inputs_from_kwargs(\r\n    905         kwargs, flat_shapes=kwarg_shapes)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in _get_defun_inputs_from_args(args, names, flat_shapes)\r\n   1137   \"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\r\n   1138   return _get_defun_inputs(\r\n-> 1139       args, names, structure=args, flat_shapes=flat_shapes)\r\n   1140 \r\n   1141 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in _get_defun_inputs(args, names, structure, flat_shapes)\r\n   1210           placeholder = graph_placeholder(\r\n   1211               arg.dtype, placeholder_shape,\r\n-> 1212               name=requested_name)\r\n   1213         except ValueError:\r\n   1214           # Sometimes parameter names are not valid op names, so fall back to\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/graph_only_ops.py in graph_placeholder(dtype, shape, name)\r\n     38   op = g._create_op_internal(  # pylint: disable=protected-access\r\n     39       \"Placeholder\", [], [dtype], input_types=[],\r\n---> 40       attrs=attrs, name=name)\r\n     41   result, = op.outputs\r\n     42   if op_callbacks.should_invoke_op_callbacks():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n    591     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n    592         op_type, inputs, dtypes, input_types, name, attrs, op_def,\r\n--> 593         compute_device)\r\n    594 \r\n    595   def capture(self, tensor, name=None, shape=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n   3483           input_types=input_types,\r\n   3484           original_op=self._default_original_op,\r\n-> 3485           op_def=op_def)\r\n   3486       self._create_op_helper(ret, compute_device=compute_device)\r\n   3487     return ret\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1985       tf_output = c_api_util.tf_output(self._c_op, i)\r\n   1986       output_type = pywrap_tf_session.TF_OperationOutputType(tf_output)\r\n-> 1987       tensor = Tensor._create_with_tf_output(self, i, output_type, tf_output)  # pylint: disable=protected-access\r\n   1988       self._outputs.append(tensor)\r\n   1989 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_with_tf_output(op, value_index, dtype, tf_output)\r\n    386   @staticmethod\r\n    387   def _create_with_tf_output(op, value_index, dtype, tf_output):\r\n--> 388     ret = Tensor(op, value_index, dtype)\r\n    389     ret._tf_output = tf_output\r\n    390     return ret\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, op, value_index, dtype)\r\n    373     self._op = op\r\n    374     self._value_index = value_index\r\n--> 375     self._dtype = dtypes.as_dtype(dtype)\r\n    376     # This will be set by self._as_tf_output().\r\n    377     self._tf_output = None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py in as_dtype(type_value)\r\n    623     TypeError: If `type_value` cannot be converted to a `DType`.\r\n    624   \"\"\"\r\n--> 625   if isinstance(type_value, DType):\r\n    626     return _INTERN_TABLE[type_value.as_datatype_enum]\r\n    627 \r\n```", "comments": ["@nvrsmeele \r\n\r\nLooks like code is incomplete. `num_tokens`, '`embedding_matrix`' is not defined.\r\n\r\nPlease, share reproducible code .It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\n\r\nI am sorry. Thanks for mentioning how I can improve the reproducible code! \r\n\r\nI have updated the code above. Just for your info, I looked at other similar reported bug issue but nothing worked so far.. I hope you can help me with this.", "@nvrsmeele \r\n\r\nIf possible can you please share `example.csv`file.Thanks!", "@ravikyram \r\n\r\nOf course, I have created a test csv-file (see below). Also, I have created a Google Colab [here](https://colab.research.google.com/drive/1HldF0WOeQvlGYxQOOJEtpa0v7WPW7xYv?usp=sharing) in which you can run it and reproduce the error. If you need anything more, please let me know!\r\n\r\n[example.csv.zip](https://github.com/tensorflow/tensorflow/files/5068745/example.csv.zip)", "@nvrsmeele Can you please simple standalone code to reproduce the issue? It will be too much to debug and find the root-cause if the code is too big. Please try to find minimal reproducible code and share it. \r\n\r\nGutHub is mainly for bugs and performance issues. If this is not a bug, then please post it in Stackoverflow as there is big community who can support you faster. Thanks!\r\n\r\n", "@jvishnuvardhan Thank you for response and help! I have simplified the issue reproduction code in [this](https://colab.research.google.com/drive/1HldF0WOeQvlGYxQOOJEtpa0v7WPW7xYv?usp=sharing) Colab environment. Also, you can use [example.zip](https://github.com/tensorflow/tensorflow/files/5075055/example.zip) to run the code and reproduce the issue.\r\n\r\nThe error occurs in the ```class CustomModel(keras.Model):``` when the learning procedure in ```fit()``` is customized by overriding the ```train_step```. I have tried multiple things but the same issue occurs. From the traceback, I read that the issue is triggered in the following code-block in the ```class CustomModel(keras.Model):```:\r\n\r\n```\r\n    with tf.GradientTape(persistent=True) as tape:\r\n      # Forward pass\r\n      pred = self([xu, xs], training=True) # Issue triggered here (see traceback)\r\n      # Compute losses\r\n      mc_loss = self.mainClass_loss(y_mc, pred[0], sample_weight=mainClass_weights)\r\n      adv_loss = self.advClass_loss(z_adv, pred[1], sample_weight=protectClass_weights)\r\n```\r\n\r\nJust for your information, I have created a multi-input/-output model by using Keras Function API which needs to be trained with a custom learning algorithm.\r\n\r\nFrom the error traceback, my guess is that it could be a back-end error. But, I could also be wrong about this.... To override the ```train_step```, I have used [this](https://keras.io/guides/customizing_what_happens_in_fit/) tutorial in the Keras Developer Guide.\r\n\r\nWith regards to your last comment, I have posted it on Stackoverflow but without success to solve the issue.\r\n\r\nIf you need more information, please let me know. I hope you can help me with this - many thanks!\r\n", "@jvishnuvardhan I just did another attempt by using TF 2.2.0 and Keras 2.3.0 but the same issue occurs again.. However, this time an additional message is printed in the traceback:\r\n\r\n```InvalidArgumentError: Operation 'TensorListPushBack_145' has no attr named '_XlaCompile'.```\r\n\r\nI did some more searching on this and found that there was a similar bug issue reported a month ago. The report bug issue was #41394 - but, it does not seem that the issue is solved.. since the issue has been stalled for some reason..", "@jvishnuvardhan I was wondering if you had the time to look at this issue. Is the simplified reproduction code good enough? I am still facing this issue. If you need anything from my side, please let me know", "Hi @nvrsmeele, thanks for reporting this. Looks like #41394 was closed because the author never provided reproducible code. I was able to reproduce the behavior with the colab and csv you provided. It seems the line in the source code [causing the problem is here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_util.py#L331).\r\nThere is still quite a lot of custom code to unpack in your example, and it would be great if you could help to narrow down the issue. Firstly, have you tried rewriting this code so you're writing your training loop from scratch?  I agree the fact that it's hanging is not great, but the stack trace suggests this has something to do with the model compilation, so I'm wondering if you can get this to work without having to override what happens in fit and just write the whole loop from scratch ([docs example here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42306\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42306\">No</a>\n"]}, {"number": 42305, "title": "tflite after quantization : Didn't find op for builtin opcode 'CONV_2D' version '5'", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 19.04\r\n- TensorFlow installed from (source or binary): Model was generated with TF from binary\r\n- TensorFlow version (or github SHA if from source): Tensorflow version 2.4.0-dev20200715, for conversion to tflite and also in C++ for executing.\r\n\r\n** Error logs **\r\nIts working completely fine, when running in model in float32 format, it is giving error when performing a post-training quantization on the model.\r\n```\r\nERROR: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\nERROR: Registration failed.\r\n```\r\n\r\n** Loading Model **\r\n\r\n```\r\n    ...\r\n    M1 = TfLiteModelCreateFromFile(modelPath.c_str());\r\n    TfLiteInterpreterOptions* options1 = TfLiteInterpreterOptionsCreate();\r\n    TfLiteInterpreterOptionsSetNumThreads(options1, 2);\r\n    \r\n    I1 = TfLiteInterpreterCreate((TfLiteModel*)M1, options1);\r\n    TfLiteInterpreterOptionsDelete(options1);\r\n    TfLiteModelDelete((TfLiteModel*)M1);\r\n    ...\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nA similar issue flutter_tflite has been reported here #41943 . \r\n", "comments": ["@ayush9096,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "@amahendrakar  It worked if I did the conversion of the model, using tensorflow version 2.2 and inference in C++, using the version tf-2.4-dev . I won't be able to provide the complete code, as it is in private for now. The model is similar to a Generator network like in GANs, with Conv2D and ConvTranspose2D. Thanks!", ">I won't be able to provide the complete code, as it is in private for now.\r\n\r\n@ayush9096,\r\nWithout a sample code it would be difficult for us to debug the issue. In this case, could you please provide a dummy model and code to mimic the error so that we can look into this. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> > I won't be able to provide the complete code, as it is in private for now.\r\n> \r\n> @ayush9096,\r\n> Without a sample code it would be difficult for us to debug the issue. In this case, could you please provide a dummy model and code to mimic the error so that we can look into this. Thanks!\r\n\r\nAttaching python script which I used to train the model and facing the same issue.\r\n\r\n\r\n[trainer.py.zip](https://github.com/tensorflow/tensorflow/files/5212455/trainer.py.zip)\r\n\r\n**[[Unable to attach training data.]]**\r\n\r\n**_Code for exporting pb saved model to tflite:_**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('efficientdet_d0_coco17_tpu-32/saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\nopen(\"efficientdet.tflite\", \"wb\").write(tflite_model)\r\n```", "https://1drv.ms/u/s!As1jbBIP1BYU3THC_LhtcvdY8Cjl?e=usmRZq \r\n\r\nData in one drive", "Can you reopen the issue @ayush9096 ", "@dsp1589,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n", "unfortunately the issue is no more reproducible. it automatically resolved for me. \r\n@amahendrakar still you want me to open a new issue? \r\n", "> unfortunately the issue is no more reproducible. it automatically resolved for me.\r\n> @amahendrakar still you want me to open a new issue?\r\n\r\nIt resolved for you? You did not change any of your code for conversion?", "> @amahendrakar still you want me to open a new issue?\r\n\r\n@dsp1589,\r\nYou need not open a new issue if it is resolved. However if you could comment here, the changes you made to fix the error, it would helpful to the community. Thanks!", "@dsp1589 \r\n\r\n> it automatically resolved for me.\r\n\r\nAny idea what changed to get it working?", "@dsp1589 \r\n\r\nyes, could you please post the steps on how you got it to work for you, thank you!", "Any solution?"]}]