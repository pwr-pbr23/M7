[{"number": 5484, "title": "\"bazel build tensorflow/python/tools:freeze_graph\" error", "body": "When I try to build the freeze_graph tool, I see the following error:\r\n\r\n```\r\nERROR: /home/xxxxxx/src/tensorflow/tensorflow/python/BUILD:1907:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored /usr/bin/gcc -shared -o bazel-out/local-fastbuild/bin/tensorflow/python/_pywrap_tensorflow.so -Wl,--version-script ... (remaining 10 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function `tf_git_version()':\r\nversion_info.cc:(.text+0x0): multiple definition of `tf_git_version()'\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0x0): first defined here\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libversion_lib.a(version_info.pic.o): In function `tf_compiler_version()':\r\nversion_info.cc:(.text+0xd): multiple definition of `tf_compiler_version()'\r\nbazel-out/local-fastbuild/bin/tensorflow/core/libframework_internal.lo(version_info.pic.o):version_info.cc:(.text+0xd): first defined here\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/python/tools:freeze_graph failed to build\r\n```\r\n\r\nAny thoughts how to resolve? I thought maybe I cloned the repo at a bad time, so I fetched the latest HEAD, and it solved my issue for one build, but all subsequent builds have failed.\r\n\r\n```\r\n$ bazel version\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n```", "comments": ["I have seen this myself, but I don't have a solid reproduction case. Were you also running a makefile-based build in the same source tree at some point? That's one of my suspicions for a cause.\n", "I don't believe so, but I can't say for certain at this point.\n", "I have the similar issue when building the python package.\n\n```\nroot@ubuntu:~/code/tensorflow# bazel build --spawn_strategy=standalone --verbose_failures --local_resources 2048,0.5,1.0 -c dbg //tensorflow/tools/pip_package:build_pip_package\nINFO: Found 1 target...\nERROR: /root/code/tensorflow/tensorflow/python/BUILD:1903:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: link_dynamic_library.sh failed: error executing command\n  (cd /root/.cache/bazel/_bazel_root/f8df5427e6b4d68e166f1da6eecaa8fb/execroot/tensorflow && \\\n  exec env - \\\n  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored /usr/bin/gcc -shared -o bazel-out/local-dbg/bin/tensorflow/python/_pywrap_tensorflow.so -Wl,--version-script tensorflow/tf_version_script.lds -pthread -Wl,-no-as-needed -B/usr/bin -B/usr/bin -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -Wl,@bazel-out/local-dbg/bin/tensorflow/python/_pywrap_tensorflow.so-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ncollect2: error: ld terminated with signal 9 [Killed]\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 47.299s, Critical Path: 45.82s\n```\n\nIt builds on the 4G memory virtual machine.\n", "But it works one the 12G memory virtual machine with the same command. It may be related to the compute resource and you can try in different servers to confirm.\n\nHope it helps \ud83d\ude03 \n"]}, {"number": 5483, "title": "tf.contrib.learn Quickstart tutorial code doesn't run, no attribute 'load_csv'", "body": "I am trying to run the sample code in the tutorial here (I named the source file 'tf_iris.py'):\r\nhttps://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart\r\n\r\nIt gives the following error:\r\n\r\n```\r\n  File \"tf_iris.py\", line 13, in <module>\r\n    training_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,\r\nAttributeError: 'module' object has no attribute 'load_csv'\r\n```\r\n\r\nI installed TF on OS X using virtualenv, following instructions here:\r\nhttps://www.tensorflow.org/versions/master/get_started/os_setup.html#virtualenv-installation\r\n\r\nThe version of TF installed is OS X / Python 2.7 / CPU, i.e.:\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py2-none-any.whl\r\n", "comments": ["This seems to be related:\nhttps://github.com/tensorflow/tensorflow/pull/4864/files\n\nI tried changing the lines that call load_csv as follows, and it fixed the error (although now there are a bunch of other warnings):\n\n```\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n  filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n  filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)\n```\n", "Your fix is correct -- can you send a PR? The PR you referenced never signed a CLA and unfortunately we cannot accept the change.\n", "This has been fixed at head."]}, {"number": 5482, "title": "TensorFlow crashing on IBM POWER; request debug advice", "body": "We're seeing various crashes while running TensorFlow on IBM POWER systems with Ubuntu 16.04. We'd welcome any debugging advice or comments, especially about debug features or about reducing the size/complexity of TF to simplify debug.\r\n\r\nProblem isolation information:\r\n\r\n- Occurs with any of TF 0.9, 0.10, and recent master (e.g. 2cbb9b52 of Oct 26th)\r\n\r\n- Occurs while training inception model on ILSVRC 2012 dataset. Model is from: https://github.com/tensorflow/models.git  inception/\r\n\r\n- Independent of CUDA / GPU; occurs even if we compile with:\r\n    TF_NEED_CUDA=0\r\n    TF_NEED_GCP=0\r\n    TF_NEED_HDFS=0\r\n\r\n- Occurs after varying run times. Appears to occur sooner with higher thread counts. Very possibly a race of some sort.\r\n\r\n- Does not occur with Ubuntu 15.10's GLIBC 2.21 (or earlier) but does occur with Ubuntu 16.04's GLIBC 2.23 (or later). So appears to be a problem in (or at least exposed by) GLIBC 2.22 or 2.23.\r\n\r\n- TF runs (mostly) clean under valgrind --tool=memcheck. (There are some complaints about bad behavior by python at startup, but then silence. We see similar complaints running other python apps.)\r\n\r\n\r\nTo try to simplify analysis, we're:\r\n\r\n- Building without CUDA, GCP, HDFS support\r\n\r\n- Limiting threading by forcing tensorflow/core/platform/posix/port.cc NumSchedulableCPUs() to return a small value\r\n\r\n- Forcing gcc to perform extra stack checking (updating tf_copts in tensorflow/tensorflow.bzl to include \"-fstack-protector-all\")\r\n\r\n\r\nNature of the crash:\r\n\r\nThe crash is usually a segfault, and often because a pointer was damaged while it was sitting on the stack.\r\n\r\nThe damage is quite consistent. Under linux on POWER, pointers to stack and heap will generally have the form 0x00003nnn nnnnnnnn. The damage always seems to be that the high half-word is changed from 0x0000 to 0x0001 (so looks like a short/half is set or incremented).\r\n\r\nOccasionally the damaged pointer started life as NULL, and so we end up with a dereference of 0x0001000000000000.\r\n\r\nIn one case the pointer that was damaged was the stack pointer (r1), which would not normally be itself stored and retrieved from the stack. So maybe in that damage occurred:\r\na) while the thread was inactive and r1 sitting in a context save area,\r\nb) in some odd case where r1 was saved on the stack, e.g. setjmp() / longjmp().\r\n\r\nThat suggests the crashing thread may not be the thread that's causing the damage.\r\n\r\nI didn't find any obvious matches on stackoverflow (no crash reports with \"stack smashing\" or \"power\"). I did see (unresolved) issue #3174 from July, but can't be sure that's related.", "comments": ["A contributor from IBM is currently working on it. We have a PR at the moment.\n\nhttps://github.com/tensorflow/tensorflow/pull/5450\n\nMaybe you can contact @namrata-ibm for more information.\n\nSherry\n", "Thank you, Sherry.  I'll ping @namrata-ibm, but I guess that other work is unrelated to what we're seeing.  s390 and POWER are separate CPU architectures. It looks like their PR is aimed (at least in part) at adding big-endian support, but our linux on POWER target is little-endian.\n", "@sherrym , Our PR is related to adding Big endian support as @hartb has mentioned. This issue is different.\n", "I'd suggest trying to just run a bunch of our different C++ tests in various directories (e.g., tests in framework/, or graph/, or platform/) repeatedly.  We probably won't have very good insight without a way to reproduce it on our side.\n", "Thank you, @vrv. The problem is easily reproducible on our systems, but there's so much going on the full TF run that the cause of the damaged stack isn't obvious.\n\nWe'll try the component-level tests as you suggest. \n\nIf there are any other debug suggestions, or if anything comes to mind as a bit of code that would be touching sparsely-packed shorts/halves, we'd much appreciate it.  Thank you! \n", "Just wanted to give an update on this.  The root cause appears to be a problem in the libpthread code used by Ubuntu 16.04 on POWER. More information is available at:\n\nhttps://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1641241\nhttps://bugs.launchpad.net/ubuntu/+source/glibc/+bug/1640518\nhttps://sourceware.org/bugzilla/show_bug.cgi?id=20822\n\nThe problem is independent of TF version. It's specific to POWER systems, so shouldn't appear on x86. And won't appear on Ubuntu systems prior to 16.04. IBM folks are working to get a fix into GLIBC/libpthread and then will work with Canonical to get a fix published for Ubuntu.\n\nThank you all for the suggestions!  Closing the issue.\n"]}, {"number": 5481, "title": "How to output .pb file directly instead of .ckpt checkpoint files during training", "body": "Now during training session, the output files are all .ckpt  which are very large. Is there any api can output .pb file directly during training. In fact, .ckpt file is not so useful for us, we only care about the final output.", "comments": ["Hi @civilmanxx,\n\nWe do not have plans to support outputting .pb files directly. If you are worried about too many checkpoint files taking up space, you can limit the max_to_keep to 1.\n\nSherry\n"]}, {"number": 5480, "title": "Creating TensorFlow device issue and perfromance drop", "body": "Now I have 4 GeForce GTX TITAN X on my server. I want to run 4 tensor flow app independently on different GPU. I use the following code to assign GPU for different app, such as the 1st app use gpu:0 and 2nd app use gpu:1.\r\n\r\n**tf.device('/gpu:0')**\r\n\r\nHowever, whenever i run any app individually or all apps at same time, each app CMD window still outputs as below. Each creates 4 devices no matter other one(s) use these device or not.\r\n\r\nAfter starting 3 apps, which are assigned the first 3 GPU (0,1,2) for each of them, then i start the 4th app for GPU:3, which is the 4th GPU, the app has dramatic performance drop.\r\n\r\nNow i am using **Ubuntu 16.04, cuda 8 + cuDNN 5.1.** I do not know why this happen. I am not clear about if each app could run separately on different GPU on the same machine. \r\n\r\n\r\n\r\n**I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0a:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)**\r\n", "comments": ["also, i meet more than once that starting a new app makes other app (process) is running on different GPU out of memory. which means new app impact the current one. This is not what i expected.\n", "Hi @civilmanxx ,\n\nAll these GPUs share the same CPU and memory. Could you please provide the output pf top, htop, nvidia-smi, vmstat while running 1, 2, 3 and 4 apps and see what the difference might be? Thanks.\n\nSherry\n", "let me clarify, out of memory at here is CUDA device out of memory. You mean all apps share GPU memory? each GeForce GTX TITAN X has 12GB GPU memory. Once an app start, it occupies all 4 GPU memory? 12GB \\* 4 = 48 GB. But my app is very small, just run Inception V1 for a single image feature extraction on 1 GPU (inference phase, but not training phase). 4 Apps do the same thing on 4 GPUs. I do not trigger them in multi thread way, but 4 independent processes and assign to 4 GPU separately.\n", "I just use \"Nvidia X Server Settings\" tool to monitor GPU memory usage. I only start one app but all GPU memory on 4 GPUs are occupied. Is this make sense? If start only one small app which needs less than 12GB GPU memory, but tensor flow occupy all 4 GPUs, then **tf.device('/gpu:0')** become useless. This is very inefficient usage. \n", "Now i use the following code to let memory usage growth on demand but not allocate all at once.\n\n**config=tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\nsess=tf.Session(config=config)**\n\nI run one app only and assign current app to use GPU:3 (the 4th one) by the code: **tf.device('/gpu:3'):**\n\nHowever, when i monitor the memory usage by \"Nvidia X Server Settings\" tool. GPU:3 uses 111M memory, which should be. But both GPU:1 and GPU:2 uses 111M as well, which mean even if I do not assign my app to use these GPUs and set config **growth=True**. In fact GPU:0 use 111M as well for my app.\n\nSo this means when we set GPU memory config to growth mode, it growths on all devices at same time, but not growth one after another, though I assigned which GPU to use. As i have 4 GPUs and i only run one app on one GPU, Tenorflow waste 3 copies of GPU memory on other devices.\n\nThis is not a good design, totally.  \n", "So I just want to know if tensor flow support real separation on different GPU for different process.\nFor example, one process is assigned to use GPU:0, so when the session start to run, tensor flow only create one device gpu:0 for use and all the rest GPUs are in the idle status. This is make sense, in every aspect.\n", "@civilmanxx: for weak isolation, use environment variable [CUDA_VISIBLE_DEVICES](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#env-vars).\nFor real isolation, you might want to run TensorFlow in [Docker](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#docker-installation). When using [nvidia-docker](https://github.com/NVIDIA/nvidia-docker) you can set a variable `NV_GPU` to isolate devices.\n", "I found there should be bug there. Now my single process app is assigned to use gpu:3,  but when i monitor the GPU utilization in  \"Nvidia X Server Settings\" tool,  gpu:3 utilization is 0%, but gpu:0 utilization is 5%. That's to say  **tf.device('/gpu:3'):** is not in effect at all.\n", "@flx42 Use **virtual** machine Docker to isolate **physical** hardware GPU... ...\n\nWell, this is an interesting idea. I feel this mechanism is not very humanize. It is a detour. Tensor flow GPU version is build on CUDA. I think it is CUDA framework to decide how GPUs should be used, based on pass in parameter, like device ID assignment. I am not sure this is CUDA's issue or tensor flow's issue. Isolation should be a very fundamental job need to do. Is there any plan to support this natively? I do not know it belongs to whose job, NVidia or Google.\n", "@civilmanxx Docker is not a virtual machine, applications will show up as regular processes on your machine.\n", "@civilmanxx : Similar to what was described in #5066 : TensorFlow will allocate all the memory across all the GPUs accessible to it. If you want to run independent TensorFlow programs, say one on each GPU, then as @flx42 mentioned, you probably want to do something like this:\n\n``` sh\n# Run myprogram.py on GPU 0\nexport CUDA_VISIBLE_DEVICES=\"0\" \nmyprogram.py\n# And another instance on GPU1\nexport CUDA_VISIBLE_DEVICES=\"1\"\nmyprogram.py\n# And another instance on GPU2\nexport CUDA_VISIBLE_DEVICES=\"2\"\n```\n\nNote that the `CUDA_VISIBLE_DEVICES` environment variable restricts what GPUs the TensorFlow runtime \"sees\", and in the setup above, you want to refer to the GPU as `gpu:0` in all the instances.\n\nLet us know if that works out for you, or if I misunderstood.\n", "I set export CUDA_VISIBLE_DEVICES=\"1\", but it uses gpu:2 instead. The index is 0,1,2,3.\n", "after i start the 1st app, which is using gpu:2 (though i set it to 1) and then set device=3 for the 2nd app in another process. The 2nd app use gpu:0 instead.  \n", "@civilmanxx : Could you elaborate on what you mean by \"it uses gpu:2 instead\"? The device names in TensorFlow will correspond to the GPUs accessible to it, while the `CUDA_VISIBLE_DEVICES` environment variable is what the CUDA framework uses to restrict visibility. So, if `CUDA_VISIBLE_DEVICES=\"1\"`, then TensorFlow programs should not be able to see multiple GPUs.\n\nPerhaps you can provide a reproducible example of the program you're using and the output of things like `nvidia-smi` and others like @sherrym mentioned above, to help explain what the problem is?\n", "Maybe @civilmanxx is confused with the output of `nvidia-smi`. Be aware that the device ordering provided by CUDA might be different from the device ordering shown by `nvidia-smi`.\nUsing `CUDA_VISIBLE_DEVICES=0` might actually give you the **latest** device when checking utilization with `nvidia-smi`.\nTo get consistent numbering, use `CUDA_\u200bDEVICE_\u200bORDER=PCI_BUS_ID`, this is documented [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#env-vars)\n", "I am using Ubuntu 16.04. If go to \"Nvidia X server settings\" you can see the list of GPUs. It shows each GPU utilization and used memory. I suppose the CUDA_VISIBLE_DEVICES=\"index\", at here index 0,1,2,3 should be same as the index show in \"Nvidia X server settings\". is that correct?\n", "if base on the post from @flx42 , they are different.\n", "@flx42 you mean i need run export cmd twice as below, is that right?\n\nexport CUDA_\u200bDEVICE_\u200bORDER = PCI_BUS_ID \nexport CUDA_VISIBLE_DEVICES=\"0\" \n", "@civilmanxx: correct, but the `CUDA_DEVICE_ORDER` export needs to be done only once. Whereas you export `CUDA_VISIBLE_DEVICES` before each run.\n", "i get error when i run **export CUDA_\u200bDEVICE_\u200bORDER=PCI_BUS_ID** \n\n`CUDA_\u200bDEVICE_\u200bORDER=PCI_BUS_ID': not a valid identifier\n", "I am using cuda 8.0\n", "Come on, that's obviously not a CUDA problem if you can't `export` a variable. You just messed up your encoding.\n", "I am not clear about why copy paste cmd from browser to terminal does not work. type cmd in terminal directly works.\n", "It turns out, even if i run these 2 export cmds before start my app. the assigned gpu index in export cmd is still different from the true running one. I set gpu:3 but gpu:1 is running...\n", "and I run another app in the 2nd process and set gpu:3 again, but it uses gpu:0.\n", "One point of clarification.  \"/gpu:0\" will always be the first logical GPU available in the process.\n\nSo if you set CUDA_VISIBLE_DEVICES=2, your program should still use \"/gpu:0\", because only one physical GPU is made visible to the process as \"logical GPU id 0\".\n\nIf you set CUDA_VISIBLE_DEVICES=4,2, then \"/gpu:0 would be the 4th physical GPU, and \"/gpu:1\" would be the 2nd physical GPU.\n", "I do not set /gpu:0 or /gpu:3 in side of the python code at this time. I only use CUDA_VISIBLE_DEVICES=\"3\" to set current process see one GPU only. As I already set CUDA_\u200bDEVICE_\u200bORDER=PCI_BUS_ID, I expect that current process only see the visible device, which is 3. But in fact it is not.\n", "in other word, **CUDA_\u200bDEVICE_\u200bORDER=PCI_BUS_ID** is not effective.\n", "the code i use is **tensorflow/tensorflow/models/slim/train_image_classifier.py** There is no device assignment code in the file.\n", "@asimshankar Is more information necessary to debug this?  From my understanding of the thread so far, @civilmanxx says that `CUDA_VISIBLE_DEVICES` doesn't work, but we have no idea why.\n", "I'm not quite sure I still fully understand the problem here.\n\nThis issue began with a question around being able to use one GPU each in 4 simultaneously running programs. And the code being used is [train_image_classifier.py](https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py).\n\nThe suggestion was to use `CUDA_VISIBLE_DEVICES` to restrict visibility of each instance of the program to a single GPU.\n\nWhat isn't clear to me is what happened after this. @civilmanxx : Is it that after setting `CUDA_VISIBILE_DEVICES`, each instance of the program still uses multiple GPUs or is it that you can't stabilize on _which_ one of the GPUs is being used?\n\nIt will be really helpful if you can provide detailed instructions on reproducing the problem - like what steps you're running, which release of TensorFlow etc. (all the details asked for in the new issue template).\n", "The thread 1 use **tf.device('/gpu:0')** in python code to assign GPU.  But to simplify the issue for easier investigation,  just try **https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py** is enough.\n\nrun these 2 cmd:\n\nexport CUDA_\u200bDEVICE_\u200bORDER = PCI_BUS_ID \nexport CUDA_VISIBLE_DEVICES=3 \n\nand then run **train_image_classifier.py**, you will notice that the true running GPU is not (PCI) 3.\n", "But it is running on a single GPU?\nIf so, then this is perhaps an issue you want to investigate with the CUDA library folks as my understanding is that TensorFlow is simply working with the single GPU device that CUDA has made accessible to it.\n", "I do not want to spend more time on this now. Now i already do not care which is which.\n"]}, {"number": 5479, "title": "theano.clone feature", "body": "Are there some plans to do extension like a **theano.clone** ?\r\n\r\n[Description from a original API:](http://deeplearning.net/software/theano/library/#theano.clone)\r\n\r\n`theano.clone(output, replace=None, strict=True, share_inputs=True)`\r\n\r\nFunction that allows replacing subgraphs of a computational graph.\r\nIt returns a copy of the initial subgraph with the corresponding substitutions.\r\n\r\n**Parameters:**\r\n\r\n- **output** (Theano Variables (or Theano expressions)) \u2013 Theano expression that represents the computational graph.\r\n\r\n- **replace** (dict) \u2013 Dictionary describing which subgraphs should be replaced by what.\r\n\r\n- **share_inputs** (bool) \u2013 If True, use the same inputs (and shared variables) as the original graph. If False, clone them. Note that cloned shared variables still use the same underlying storage, so they will always have the same value.\r\n\r\nIt would be great if we can do similar things in **tensorflow** with easy.\r\n", "comments": ["Closing since this seems to be fixed with pull request #557.\n"]}, {"number": 5478, "title": "ImportError: No module named tensorflow - Can't install Tensorflow", "body": "I am trying to install tensorflow on mac and it's giving me this error.\r\n\r\nImportError: No module named tensorflow\r\n\r\nHere is what I have done in the terminal\r\n\r\n`sudo easy_install pip \r\n\r\nsudo easy_install --upgrade six\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py3-none-any.whl\r\nsudo -H pip3 install --upgrade $TF_BINARY_URL`\r\n\r\n\r\nAfter that I try to run python and tensorflow to check my installation. It doesn't work. I have spent 3 hours on the problem.\r\n\r\nHere is another post about it. http://stackoverflow.com/questions/40472144/importerror-no-module-named-tensorflow-cant-install-tensorflow?noredirect=1#comment68200554_40472144", "comments": ["Please consider these things:\n- What OS? We support El Capitan right now. Sierra hopefully soon.\n- Try a newer version than 0.9 which is quite old, perhaps 0.11rc2\n- Are there any errors on the pip install?\n- What version of python. Perhaps try the python 2 version if you have that as well. Show exactly what version of python you are actually running and what it spits out. For example...\n\n```\n$ python\nPython 2.7.10 (default, Jul 30 2016, 18:31:42) \n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named tensorflow\n>>> \n\n```\n\nI understand it is frustrating to spend 3 hours having trouble, but it is hard for us to help you when you do not provide detailed information. Thanks.\n", "Hi @aselle ,\r\n\r\nTHIS IS SOLVED, SEE BOTTOM COMMENT\r\nI am having the same error too.\r\nI am using EI Capitan 10.11.2\r\n\r\nHere is what i did in my installation, no error during the installation:\r\n```\r\n$ sudo easy_install pip\r\n$ sudo easy_install --upgrade six\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py2-none-any.whl\r\n$ sudo pip install --upgrade $TF_BINARY_URL\r\n\r\n```\r\nHere is my tensor flow information:\r\n```\r\n$ pip show tensorflow\r\nName: tensorflow\r\nVersion: 0.9.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: protobuf, numpy, wheel, six\r\n```\r\n\r\nhere is my protobuf information:\r\n```\r\n$ pip show protobuf\r\nName: protobuf\r\nVersion: 3.0.0b2\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: New BSD License\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: setuptools, six\r\n```\r\nhere is my six information:\r\n```\r\n$ pip show six\r\nName: six\r\nVersion: 1.10.0\r\nSummary: Python 2 and 3 compatibility utilities\r\nHome-page: http://pypi.python.org/pypi/six/\r\nAuthor: Benjamin Peterson\r\nAuthor-email: benjamin@python.org\r\nLicense: MIT\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires:\r\n```\r\n\r\nFollowing is my python version:\r\n```\r\n$ python\r\nPython 2.7.10 (default, Oct 23 2015, 18:05:06)\r\n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensor flow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensor flow\r\n>>>\r\n```\r\n\r\nUPDATE: It seems that my pip is link to python 2.7.12 (previously installed via Homebrew). I uninstall everything, and reinstall to solve the issue", "Closing.", "I had the same issue with mac os.. this fixed the problem:\r\ninstall brew\r\nbrew install python\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py2-none-any.whl\r\nsudo pip install --upgrade $TF_BINARY_URL\r\n\r\n", "If you use brew:\r\n- brew install python - installs 2.7\r\n- brew install python3 - installs 3.6\r\nin any case, you are not running optimal version for TensorFlow 3.5\r\n\r\nAnyway,\r\n$ python --version\r\nPython 3.5.2 :: Continuum Analytics, Inc.\r\n\r\n$ echo $TF_BINARY_URL\r\nhttps://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.1-py3-none-any.whl\r\n\r\nWhen running:\r\n $ sudo  pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\n\r\nit looks like it installs everything 100%\r\n$ conda list\r\n...\r\ntensorflow-gpu            0.12.1                    <pip>\r\n...\r\n\r\nbut when running $ jupyter notebook\r\nimport tensorflow as tf\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nFor Jupyter kernel to reflect changes in conda env (tensorflow), I had to set up the following:\r\n\r\n```\r\n$ source activate tensorflow\r\n$ sudo pip install ipykernel\r\n$ python -m ipykernel install --user --name tensorflow --display-name \"conda env tensorflow\"\r\n```\r\n[my blog](http://ukitech.blogspot.com/2017/02/kernel.html)", "i had the same issue. and here is why it happen and how i fixed it in my issue:\r\nenv: anaconda (python 3.6)+mac ,\r\nfirst ,i used :\r\n$ pip3 install tensorflow \r\n$ python\r\nPython 3.6.0 |Anaconda 4.3.0 (x86_64)| (default, Dec 23 2016, 13:19:00) \r\n\r\n>>> import tensorflow as tf\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nand then \r\n$pip install tensorflow\r\n$python\r\nPython 3.6.0 |Anaconda 4.3.0 (x86_64)| (default, Dec 23 2016, 13:19:00) \r\n>>>  import tensorflow as tf\r\n>>> \r\n\r\n", "@LeongKH thanks this works for me!", "importing tensorflow using python3 instead of python 2.7 worked for me.", "> pip3 install tensorflow\r\n> ...\r\n> pip install tensorflow\r\n\r\nthank you, it is works for me", "some computer scientist lady recommended miniconda - and I haven't looked back.\r\nForget using pip/pip3 / use conda + conda environments. I can't recommend it more.\r\nhttps://gist.github.com/johndpope/187b0dd996d16152ace2f842d43e3990", "@UkiDLucas  Thanks very much.  That fixed it for me.  I believe it's that you need iPython to make Python 2.7 work with Anaconda.  \r\n\r\nThis fixed it for me:\r\n$source activate tensorflow\r\n$ sudo pip install ipykernel\r\n$ python -m ipykernel install --user --name tensorflow --display-name \"conda env tensorflow\"", "chunchuria\r\numbrella", "a si\r\ncomo \r\nno\r\nme\r\ninco\r\n", "Hi i have the same issue when trying to run tensorflow in my windows 10 \r\nwhen I run pip show cmd this is what is displayed \r\nC:\\>pip show tensorflow\r\nName: tensorflow\r\nVersion: 1.3.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: c:\\program files\\python36\\lib\\site-packages\r\nRequires: six, wheel, tensorflow-tensorboard, numpy, protobuf\r\n\r\nwhen prompt my python shell and try to import tensorflow \r\n\r\n\r\nit gives me this error \r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\nkindly assist\r\n", "i had the same issue.\r\ni installed two version of python 2.7 and 3.5\r\ntensorflow installs in python2.7\r\n\r\nmy default python version is 3.5 \r\nso if I type: python\r\nPython 3.5.3 (default, Dec  7 2017, 09:41:57) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named 'tensorflow'\r\n\r\n\r\nbut if I type: python2.7\r\nPython 2.7.5 (default, Aug  4 2017, 00:39:18) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n\r\nIt`s OK \r\n\r\n\r\nso check your python version.\r\nIt will help", "@Afrogeek254 I also stumbled on this problem, and fixed it by changing python version. (Win10 x64)\r\nFunny thing is that the first version I tried is the one \"supported\" by TF as said in [the documentation](https://www.tensorflow.org/install/install_windows#installing_with_native_pip) (used the link in the page for 3.6.x 64 which was 3.6.2).\r\nDidn't want to find a workaround, and ended up installing 3.5.2 (also linked in the installation docs)", "\u5367\u69fd \u89e3\u51b3\u4e86 \u5f88\u7ed9\u529b\uff01", "\u308f\u3042\u3001\u89e3\u6c7a\u3057\u3001\u3059\u3054\u3044", "I create a virtual environment, when I run some codes ,give the same Error. And I reinstall tensorflow , but it doesn't work", "I have another stituation, I have succssfully installed tensorflow on my mac, and a function which uses tensorflow's model can run by its own. However, when I import this fuction in Django, the first line is 'import tensorflow as tf', error: ModuleNotFoundError: No module named 'tensorflow'.. wtf", "In a new environment, maybe you need reinstall it. If it doesn't work , try to change the way installing.\r\nAt first, I used pip installing, got the error. then I changed to use conda installing, the error gone.\r\n\r\n\u53d1\u81ea\u575a\u679c Pro\r\n\r\nWeiwen Chen <notifications@github.com> \u4e8e 2018\u5e746\u670815\u65e5 \u4e0b\u53487:31\u5199\u9053\uff1a\r\n\r\n\r\nI have another stituation, I have succssfully installed tensorflow on my mac, and a function which uses tensorflow's model can run by its own. However, when I import this fuction in Django, the first line is 'import tensorflow as tf', error: ModuleNotFoundError: No module named 'tensorflow'.. wtf\r\n\r\n\u2015\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/5478#issuecomment-397592685>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Afm4KmH2mQP6VFDyUSQPDZmpzS8q-aRGks5t85sfgaJpZM4Ksj8X>.\r\n", "Although it's a long time after this question is asked since I was searching so much for the same problem and couldn't find the extant solutions helpful, I write what fixed my trouble for anyone with the same issue:\r\nThe point is, Jupyter should be installed in your virtual environment, meaning, after activating the tensorflow environment, run the following in the command prompt (in virtual environment):\r\n- conda install jupyter\r\n- jupyter notebook\r\n\r\nand then the jupyter norebook will pop up\r\n"]}, {"number": 5477, "title": "Deal with _control_flow_context when copying op", "body": "In the current implementation of copying ops (both `tf.contrib.copy_graph` and `tf.contrib.graph_editor`)\r\nThe code of copying an op looks like this\r\n```python\r\n# copy inputs\r\ninputs_ = copy_func(op.inputs)\r\n# copy control_inputs\r\ncontrol_inputs_ = copy_func(control_inputs)\r\n# copy _node_def, _op_def\r\nnode_def_ = deepcopy(op._node_def)\r\nop_def_ = deepcopy(op._op_def)\r\noutput_types_ = op._output_types[:]\r\ninput_types_ = op._input_types[:]\r\n# copy name\r\nname_ = copy_func(op.name)\r\n# init the new op with above copies\r\nnew_op = tf_ops.Operation(node_def_, ...)\r\n# ... copy shape and add to graph\r\n```\r\nBut the `op._control_flow_context` is not copied at all. This causes problems when trying to compute gradients on a copied subgraph with control flow op like `tf.cond`. The error looks like\r\n```python\r\n    grads = optimizer.compute_gradients(-lower_bound)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 461, in gradients\r\n    out_grads[i] = control_flow_ops.ZerosLikeOutsideLoop(op, i)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1310, in ZerosLikeOutsideLoop\r\n    pred = op_ctxt.pred\r\nAttributeError: 'NoneType' object has no attribute 'pred'\r\n```\r\nThis is because the function `ZerosLikeOutsideLoop` uses `_control_flow_context` when the op is `tf.switch`\r\n```\r\ndef ZerosLikeOutsideLoop(op, index):\r\n  \"\"\"Create zeros_like for the specified output of an op.\"\"\"\r\n  val = op.outputs[index]\r\n  if not IsSwitch(op):\r\n    return array_ops.zeros_like(val, optimize=False)\r\n  else:\r\n    op_ctxt = op._get_control_flow_context()\r\n    pred = op_ctxt.pred\r\n    branch = op_ctxt.branch\r\n    switch_val = switch(op.inputs[0], pred)[1 - branch]\r\n    zeros_shape = array_ops.shape_internal(switch_val, optimize=False)\r\n    return array_ops.zeros(zeros_shape, dtype=val.dtype)\r\n```\r\nI tried setting `new_op. _control_flow_context` as `op._control_flow_context`\r\nNow the error step passed. But I'm not sure whether this is right for dealing with _control_flow_context copy. Do you have some advice?", "comments": ["You can use copy_scoped_meta_graph() in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph.py.\n\nYou can find examples of how to import, export and copy scoped meta_graph in\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/meta_graph_test.py\nand\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver_test.py.\n\nPlease let me know if that works for you.\n\nSherry\n", "@sherrym Thanks. This is very similar to what I want. But I feel there are some difficulties when using it to copy ops in a subgraph.\n\nFirst these operations are not always in the same scope, so I have to copy them one by one. What I do is to traverse the subgraph and get those ops. However, some operations share the same name with their uproot scopes, e.g. `tf.random_normal` creates a name_scope named \"random_normal\" while the `tf.add` operation in it shares the same name. This causes problems when I'm trying to copy this `tf.add` using its name because `copy_scoped_meta_graph()` will copy the outer name_scope.\n\nDo you have some workaround for this? Maybe a copy function accepting operation objects other than scope names could be more suitable for my case.\n", "@sherrym After spending more time figuring this out, I now think it should be of not that much effort to implement a copy_op_meta_graph(). I think I can work on it and will submit a PR.\nDo you think it's okay to serve as an api in tensorflow main library? Because I'm writing some high-level library based on this and hope to keep the base utilities as stable as an api in tensorflow main library.\n", "@thjashin You are always welcome to submit a PR for contrib.\n", "Leaving this open despite being old because I don't think it's fixed. In addition, I don't think importing a metagraph will work either (I haven't actually tested but have been looking at this code recently).", "thjashin@ do you know if this issue is still happening/relevant?", "@annarev Sorry I missed this comment. Yes, it's still happening.", "@annarev - Hi, any updates on this ?", "Sorry for delay, I missed it somehow. I am actually not the right owner since I am unfamiliar with copy_graph and graph_editor code.\r\n\r\nSo, I am reassigning to @frankchn . Frank, I saw that you might own moving tf.contrib.copy_graph out of contrib. But feel free to reassign if there is a better owner.", "Hmm I am probably not the right person for this (the internal CL was from a sync pull) -- @skye do you know who might know about this?", "We're currently reimplementing control flow in TF such that it no longer uses control flow contexts. It's still under construction, but you can set the environment variables TF_ENABLE_COND_V2=1 TF_ENABLE_WHILE_V2=1 to try it out. Please feel free to report any errors you see using the new control flow.", "@thjashin Is this issue resolved? Please close If it was resolved already. Thanks!", "@jvishnuvardhan  I'm not sure because I don't use copy anymore. Feel free to close the issue.", "@thjashin Thanks! I am closing the issue. Please feel free to open a new ticket if the bug persists in latest version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=5477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=5477\">No</a>\n"]}, {"number": 5476, "title": "CUDNN_STATUS_BAD_PARAM in version 0.11.0rc1", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFollowing steps i did:\r\n1. Install tensorflow\r\n2. Execute the source example to make sure everything is fine\r\n3. Run srcnn.py in  https://github.com/liliumao/Tensorflow-srcnn/blob/master/srcnn.py\r\n\r\nThen I encounter the error message\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:444] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\r\n\r\nWhat is more strange is that the program did trained 160 times and then show the error message.\r\nAnd it only happens after iteration 160 times.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n#2033 #1787\r\n\r\n### Environment info\r\nOperating System : CentOS 7.2.1511+ tensorflow 0.11.0rc1\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0 & cuDNN 5.1\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0rc1\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nhttps://github.com/liliumao/Tensorflow-srcnn/blob/master/srcnn.py\r\n\r\n### What other attempted solutions have you tried?\r\n1. Transfer the cuda version to 7.5, however the tf.0.11.0rc1 needs cuda 8.0.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties:\r\nname: GeForce GTX 980\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.3925\r\npciBusID 0000:03:00.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.82GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:03:00.0)\r\n2016-11-08 06:08:00.274964: step 0, duration = 0.457\r\n2016-11-08 06:08:00.705598: step 10, duration = 0.020\r\n2016-11-08 06:08:01.106745: step 20, duration = 0.019\r\n2016-11-08 06:08:01.492362: step 30, duration = 0.019\r\n2016-11-08 06:08:01.849530: step 40, duration = 0.018\r\n2016-11-08 06:08:02.245288: step 50, duration = 0.019\r\n2016-11-08 06:08:02.629120: step 60, duration = 0.019\r\n2016-11-08 06:08:02.979037: step 70, duration = 0.018\r\n2016-11-08 06:08:03.341024: step 80, duration = 0.018\r\n2016-11-08 06:08:03.706578: step 90, duration = 0.019\r\n2016-11-08 06:08:04.098436: step 100, duration = 0.019\r\n2016-11-08 06:08:04.476913: step 110, duration = 0.019\r\n2016-11-08 06:08:04.895861: step 120, duration = 0.019\r\n2016-11-08 06:08:05.262154: step 130, duration = 0.019\r\n2016-11-08 06:08:05.670712: step 140, duration = 0.019\r\n2016-11-08 06:08:06.018086: step 150, duration = 0.018\r\n2016-11-08 06:08:06.387210: step 160, duration = 0.018\r\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:444] could not set cudnn tensor descriptor: CUDNN_STATUS_BAD_PARAM\r\n[1]    3640 abort (core dumped)  python3 srcnn.py\r\n\r\n\r\n", "comments": ["@zheng-xq  please take a look and suggest fixes.\n", "I found the bug.\nIt was because the input data array dead at 170 iteration.\nIt did not have a traditional python error, but a tensorflow ones.\n\nThanks guys a lot.\n", "@Goolley Hi, what do you mean by `the input data array dead`? How to find it?  I also encounter the error message now.", "@barrykui In my case, it is when I feed an empty array in a placeholder.", "@barrykui My input array is not an legal array since i have some mistake on  the index."]}, {"number": 5475, "title": "Wait for jvm to shutdown completely before remove bazel install directory", "body": "@gunan ", "comments": ["@meteorcloudy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan to be a potential reviewer.\n", "I am not really happy with the amount of bazel magic we are calling here.\nUltimately, are we going to be able to simply run the following in this script?\n\n```\n$ echo \"\" | configure\n$ bazel test tensorflow/...\n```\n", "@gunan Yes, I believe so, but of course the tests still need to be fixed first.\nAnd the bazel magics are only for working around `bazel clean --expunge` problem, they can go away after we fix it. https://github.com/bazelbuild/bazel/issues/1586\n", "OK, sounds good.\nThe test failure is unrelated, therefore merging.\n"]}, {"number": 5474, "title": "Set TF_NEED_OPENCL=0 by default on Windows", "body": "@gunan ", "comments": ["@meteorcloudy, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @lukeiwanski and @keveman to be potential reviewers.\n", "Failing test is a flake.\nMerging.\n"]}, {"number": 5473, "title": "modprobe: ERROR: ../libkmod/libkmod-module.c:832 kmod_module_insert_module() could not find module by name='nvidia_367_uvm'", "body": "here is the details:\r\nmodprobe: ERROR: ../libkmod/libkmod-module.c:832 kmod_module_insert_module() could not find module by name='nvidia_367_uvm'\r\nmodprobe: ERROR: could not insert 'nvidia_367_uvm': Unknown symbol in module, or unknown parameter (see dmesg)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_UNKNOWN\r\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:140] kernel driver does not appear to be running on this host (gzhao-b1208): /proc/driver/nvidia/version does not exist\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\r\n\r\nit worked fine yesterday.", "comments": ["Hi @KeithYin  I have exactly the same issue, how did you fix it?\n", "i just reinstall the nvidia driver, then that issue is gone.@WenchenLi\n"]}, {"number": 5472, "title": "Fix a typo in export meta graph sample code.", "body": "The sample code in How-tos \"Exporting and Importing a MetaGraph\" cause NameError: name 'saver0_ckpt' is not defined.\r\nI think it should be `'my-save-dir/my-model-10000'` to be restored in the subsequent sample code.\r\n\r\nI've signed the CLA before for other Google's project (https://github.com/GoogleCloudPlatform/DataflowJavaSDK). Do I have to make another agreement?", "comments": ["Can one of the admins verify this patch?\n", "@nagachika, thanks for your PR! By analyzing the history of the files in this pull request, we identified @sherrym, @ilblackdragon and @danmane to be potential reviewers.\n", "Jenkins, test this please.\n"]}, {"number": 5471, "title": "Remove a premature checkpoint existence test in Saver.restore().", "body": "The correct way to perform such a check is inside the Op kernel.\r\nChange: 138212174\r\n\r\n(cherry picked from commit a2f0f07f59b318d255ba86c8ab3486466dece5a8)", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @concretevitamin to be potential reviewers.\n", "Well, this has not fixed it. I do not know what's wrong. @sguada, any idea?\n", "In `...slim/learning_test.py` ~Line 197 and other places, try replacing\n\n```\n    self._logdir = os.path.join(self.get_temp_dir(), 'tmp_bnlogs/')\n```\n\nwith a process-unique tmp dir.  For instance `... + uuid.uuid4().hex`.  \n", "@concretevitamin Actually, that also won't work.\nFor example, assume 2 mac test runs for 2 different PRs.\nIf these end up running on the same machine, they can end up with the same paths.\nso we need something more like this:\n\n```\nself._logdir = tempfile.mkdtemp(prefix=os.path.join(self.get_temp_dir(), 'tmp_bnlogs/'))\n```\n", "We are removing this test to unblock the release. Meanwhile @cshallue is working on a fix. Closing this PR.\n"]}, {"number": 5470, "title": "Cannot import graph_def for 8-bit Quantized cnn model", "body": "Following steps I did:\r\n1. Train a CNN model on MNIST using tensorflow tutorials(Deep MNIST for Experts)(CONV+CONV+FC). Saved this model as binary protobuf(.pb).\r\n2. Using 8-bit Quantization api, converted previously saved model to 8-bit model as binary proto(.pb). \r\n3. Read 8-bit binary-proto to tensorflow, by first creating graph_def and importing graph def to tf.Session()\r\n```python\r\nwith sess.as_default() :\r\n   with tf.Graph().as_default():\r\n       graph_def = tf.GraphDef()\r\n             with open(input_file_name, 'rb') as f:\r\n                 proto_b = f.read()\r\n                 graph_def.ParseFromString(proto_b)\t\r\n                 _ = tf.import_graph_def(graph_def, name=\"\")\r\n``` \r\n\r\nHowever, this gives error on tf.import_graph_def as\r\n`ValueError: graph_def is invalid at node u'concat_eightbit_reshape_concat/values_0': Input tensor 'concat/values_0:0' Cannot convert a tensor of type int32 to an input of type float32.`\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04 + tensorflow 0.11.0rc2\r\n\r\nInstalled version of CUDA and cuDNN: No\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl\r\n\r\n\r\n\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n1. I am able to correctly import quantized graph def for pre-trained inception model(.pb).\r\n2. When I define Multi-layer perceptron model for MNIST, it can import quantized graph_def. \r\n3. All the quantization headers are imported correctly as\r\n``` python\r\nfrom tensorflow.contrib.quantization import load_quantized_ops_so\r\nfrom tensorflow.contrib.quantization.kernels import load_quantized_kernels_so\r\nload_quantized_ops_so.Load()\r\nload_quantized_kernels_so.Load()\r\n``` \r\n\r\n", "comments": ["Which 8-bit Quantization API is this? Could you please send me a link? Sounds like it didn't change the data type when performing the conversion.\n", "I followed this [How-to](https://www.tensorflow.org/versions/r0.11/how_tos/quantization/index.html).\n", "Andrew, could you please take a look? Thanks.\n\nSherry\n", "Added @cwhipkey as well.\n", "Can you share the file that you pass to quantize_graph ?\n\n@petewarden as well\n\nOn Tue, Nov 8, 2016 at 6:15 PM, Sherry Moore notifications@github.com\nwrote:\n\n> Added @cwhipkey https://github.com/cwhipkey as well.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5470#issuecomment-259318209,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AQw4wXWOO48OsYmN3heHtMDOhW_zNrQcks5q8SzTgaJpZM4Kr9Qg\n> .\n", "This is binary proto before quantization. \n[tf_mnist_cnn_graph.pb.zip](https://github.com/tensorflow/tensorflow/files/585009/tf_mnist_cnn_graph.pb.zip)\n\nThanks. \n", "@petewarden\n\nThanks for the info.  Yes, the problem is that it is choosing to quantize a\nConcat node, but can't actually quantize its inputs.  I have a fix I will\npush out.\n\nOn Thu, Nov 10, 2016 at 8:25 PM, Abhinav Dadhich notifications@github.com\nwrote:\n\n> This is binary proto before quantization.\n> tf_mnist_cnn_graph.pb.zip\n> https://github.com/tensorflow/tensorflow/files/585009/tf_mnist_cnn_graph.pb.zip\n> \n> Thanks.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5470#issuecomment-259878163,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AQw4wT6I7QxDkbxD9zxaS_6mnrhE_B68ks5q8-4hgaJpZM4Kr9Qg\n> .\n", "@cwhipkey \nThank you, Please share the info after the fix.  \n", "This change was pushed as this commit:\nhttps://github.com/tensorflow/tensorflow/commit/662533b85c66f198b779bea147397e1441f3e482\n\nOn Thu, Nov 17, 2016 at 9:23 PM, Abhinav Dadhich notifications@github.com\nwrote:\n\n> @cwhipkey https://github.com/cwhipkey\n> Thank you, Please share the info after the fix.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5470#issuecomment-261451376,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AQw4wa_QZXagg-ukN_mi_L5ILvKh9VVfks5q_TZMgaJpZM4Kr9Qg\n> .\n"]}, {"number": 5469, "title": "Branch 138426641", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @charlesnicholson, @keveman and @andrewharp to be potential reviewers.\n"]}, {"number": 5468, "title": "Any plans for TF to support FP16&INT8?", "body": "nVidia releases P100, P4, and P40 GPUs, and it also releases TensorRT for inference to support FP16 and INT8. Any plans for TensorFlow?", "comments": ["Benoit, could you please comment? Thanks.\n", "Any update on this?", "dup of https://github.com/tensorflow/tensorflow/issues/4589", "@yaroslavvb AFAIK, there is no full support of even non-native INT8 operations, and https://github.com/tensorflow/tensorflow/issues/4589 talks only about FP16", " I am tracking #4589 and will provide updates until FP16 is in decent shape.  With out Volta it is hard to test real performance but that will happen soon."]}, {"number": 5467, "title": "Add ci scripts for cmake build and test on windows.", "body": "Retrying without virtualenv.\r\n@guschmue  PTAL.\r\nWhen it comes to batch files, I have no idea what I am doing.", "comments": ["http://ci.tensorflow.org/job/tensorflow-pr-win-cmake-pytest/13/console\n\n1 test failed out of 141!\n\nwill merge and start builds.\n", "cool, we're getting there. This test works on our Jenkins fine, but I can take a look tomorrow what it complains about.\n\nStart  83: D:/Jenkins_Workspace/TensorFlow-Google-Master-CPU/tensorflow/python/kernel_tests/reader_ops_test.py\n 83/141 Test  #83: D:/Jenkins_Workspace/TensorFlow-Google-Master-CPU/tensorflow/python/kernel_tests/reader_ops_test.py ..........................   Passed    4.34 sec\n"]}, {"number": 5466, "title": "Updates download_progress_hook comment", "body": "Updates download_progress_hook comment to match the behavior of reporting every 5% of change.", "comments": ["@rmcnair, thanks for your PR! By analyzing the history of the files in this pull request, we identified @Jakobovski, @vincentvanhoucke and @lukas-krecan to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "(Please feel free to ping this thread or send a new PR once the CLA is signed, thanks for the PR!)\n"]}, {"number": 5465, "title": "Exploration of cmake on Ubuntu: /tensorflow/stream_executor/gcuda.h", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\n### Environment info\r\nOperating System: Ubuntu14.04\r\n\r\nInstalled version of CUDA and cuDNN: 8.0/5.1.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`)\r\nf870373756f8017cf89c7a6e3889539aba834265 \r\n\r\nI'm exploring TensorFlow build with cmake on Ubuntu14.04\r\n\r\nWhile trying building, I'm stuck with the following error.\r\n\r\nIt seems to me ..\r\n1. device_functions.h header is not included? \r\n2. add/remove definitions such as -D__NVCC__? \r\n3. tf_stream_executor.cmake file looks incomplete. what do I need to add to the file? \r\n\r\n```\r\n> In file included from /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc:16:0:\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:234:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ clock_t __gcuda_nvcc_clock() { return clock(); }\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:235:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __gcuda_nvcc__clz(int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:238:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __gcuda_nvcc__clzll(long long int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:241:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ float __gcuda_nvcc__fdividef(float a, float b) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:244:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __gcuda_nvcc__ffsll(long long int x) { // NOLINT\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:247:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __gcuda_nvcc__popc(unsigned int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:250:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ float __gcuda_nvcc__powf(float a, float b) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:253:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ void __gcuda_nvcc__sincosf(\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:257:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ unsigned int __gcuda_nvcc__umulhi(\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:263:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ unsigned int __gcuda_nvcc__ballot(int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:274:9: error: \u2018::abs\u2019 has not been declared\r\n>  using ::abs;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:275:9: error: \u2018::atomicAdd\u2019 has not been declared\r\n>  using ::atomicAdd;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:276:9: error: \u2018::atomicCAS\u2019 has not been declared\r\n>  using ::atomicCAS;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:277:9: error: \u2018::ceil\u2019 has not been declared\r\n>  using ::ceil;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:278:9: error: \u2018::ceilf\u2019 has not been declared\r\n>  using ::ceilf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:279:9: error: \u2018::cos\u2019 has not been declared\r\n>  using ::cos;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:280:9: error: \u2018::cosf\u2019 has not been declared\r\n>  using ::cosf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:281:9: error: \u2018::erfcinv\u2019 has not been declared\r\n>  using ::erfcinv;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:282:9: error: \u2018::erfcinvf\u2019 has not been declared\r\n>  using ::erfcinvf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:283:9: error: \u2018::exp\u2019 has not been declared\r\n>  using ::exp;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:284:9: error: \u2018::expf\u2019 has not been declared\r\n>  using ::expf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:285:9: error: \u2018::fabs\u2019 has not been declared\r\n>  using ::fabs;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:286:9: error: \u2018::fabsf\u2019 has not been declared\r\n>  using ::fabsf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:287:9: error: \u2018::floor\u2019 has not been declared\r\n>  using ::floor;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:288:9: error: \u2018::floorf\u2019 has not been declared\r\n>  using ::floorf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:289:9: error: \u2018::fabs\u2019 has not been declared\r\n>  using ::fabs;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:290:9: error: \u2018::fabsf\u2019 has not been declared\r\n>  using ::fabsf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:291:9: error: \u2018::fma\u2019 has not been declared\r\n>  using ::fma;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:292:9: error: \u2018::fmaf\u2019 has not been declared\r\n>  using ::fmaf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:293:9: error: \u2018::fmax\u2019 has not been declared\r\n>  using ::fmax;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:294:9: error: \u2018::fmaxf\u2019 has not been declared\r\n>  using ::fmaxf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:295:9: error: \u2018::fmin\u2019 has not been declared\r\n>  using ::fmin;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:296:9: error: \u2018::fminf\u2019 has not been declared\r\n>  using ::fminf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:297:9: error: \u2018::log\u2019 has not been declared\r\n>  using ::log;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:298:9: error: \u2018::log1p\u2019 has not been declared\r\n>  using ::log1p;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:299:9: error: \u2018::log1pf\u2019 has not been declared\r\n>  using ::log1pf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:300:9: error: \u2018::logf\u2019 has not been declared\r\n>  using ::logf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:301:9: error: \u2018::max\u2019 has not been declared\r\n>  using ::max;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:302:9: error: \u2018::min\u2019 has not been declared\r\n>  using ::min;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:303:9: error: \u2018::powf\u2019 has not been declared\r\n>  using ::powf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:305:9: error: \u2018::sin\u2019 has not been declared\r\n>  using ::sin;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:306:9: error: \u2018::sinf\u2019 has not been declared\r\n>  using ::sinf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:307:9: error: \u2018::sincos\u2019 has not been declared\r\n>  using ::sincos;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:308:9: error: \u2018::sincosf\u2019 has not been declared\r\n>  using ::sincosf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:309:9: error: \u2018::sincospi\u2019 has not been declared\r\n>  using ::sincospi;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:310:9: error: \u2018::sincospif\u2019 has not been declared\r\n>  using ::sincospif;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:311:9: error: \u2018::sqrt\u2019 has not been declared\r\n>  using ::sqrt;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:312:9: error: \u2018::sqrtf\u2019 has not been declared\r\n>  using ::sqrtf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:313:9: error: \u2018::tanh\u2019 has not been declared\r\n>  using ::tanh;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:314:9: error: \u2018::trunc\u2019 has not been declared\r\n>  using ::trunc;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:315:9: error: \u2018::truncf\u2019 has not been declared\r\n>  using ::truncf;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:316:9: error: \u2018::trunc\u2019 has not been declared\r\n>  using ::trunc;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:326:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __host__ __device__ float rsqrtf(float x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:329:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __host__ __device__ double rsqrt(double x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:334:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int clock() { return __gcuda_nvcc_clock(); }\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:336:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __clz(int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:340:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __clz(long long int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:344:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ float __fdividef(float a, float b) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:348:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __ffsll(long long int x) { // NOLINT\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:352:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ int __popc(unsigned int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:356:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ float __powf(float a, float b) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:360:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ void __sincosf(float x, float *sptr, float *cptr) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:364:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ unsigned int __umulhi(unsigned int x,\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:378:1: error: \u2018__forceinline__\u2019 does not name a type\r\n>  __forceinline__ __device__ unsigned int __ballot(int x) {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:384:9: error: \u2018::__shfl\u2019 has not been declared\r\n>  using ::__shfl;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:385:9: error: \u2018::__shfl_down\u2019 has not been declared\r\n>  using ::__shfl_down;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:386:9: error: \u2018::__shfl_up\u2019 has not been declared\r\n>  using ::__shfl_up;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:387:9: error: \u2018::__shfl_xor\u2019 has not been declared\r\n>  using ::__shfl_xor;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:391:9: error: \u2018::__ldg\u2019 has not been declared\r\n>  using ::__ldg;\r\n>          ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:402:8: error: \u2018__device__\u2019 does not name a type\r\n>  inline __device__ int isfinite(float x) {\r\n>         ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:405:8: error: \u2018__device__\u2019 does not name a type\r\n>  inline __device__ int isfinite(double x) {\r\n>         ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.h:408:8: error: \u2018__device__\u2019 does not name a type\r\n>  inline __device__ int isfinite(long double x) {\r\n>         ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc:23:1: error: \u2018map\u2019 in namespace \u2018std\u2019 does not name a type\r\n>  std::map<void *, KernelCacheConfig> &GetGcudaccStubToCacheConfigMap() {\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc:29:5: error: \u2018StreamExecutor\u2019 was not declared in this scope\r\n>      StreamExecutor *stream_exec) {\r\n>      ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc:29:21: error: \u2018stream_exec\u2019 was not declared in this scope\r\n>      StreamExecutor *stream_exec) {\r\n>                      ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc:29:34: error: expected \u2018,\u2019 or \u2018;\u2019 before \u2018{\u2019 token\r\n>      StreamExecutor *stream_exec) {\r\n>                                   ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc:102:1: error: expected \u2018}\u2019 at end of input\r\n>  }  // namespace perftools\r\n>  ^\r\n> /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc:102:1: error: expected \u2018}\u2019 at end of input\r\n> make[2]: *** [CMakeFiles/tf_stream_executor.dir/home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc.o] Error 1\r\n> make[1]: *** [CMakeFiles/tf_stream_executor.dir/all] Error 2\r\n> make: *** [all] Error 2\r\n```", "comments": ["OK, it looks like you are using cmake. Could you clarify what command you used to build?\n\n@mrry for CC.\n", "Yes, using cmake, i am exploring and  trying to build tf.  I had to modify CMakeList such as cuda versions for cuda_config.h and comments on Windows' dependencies or macro because current cmakelist was set up for wondows env. After some edits, commands I entered are pretty much like cmake commands:  cmake .. (with cudnn_home flag and gpu enabled), then sudo make\nI figured out serveral build errors previously, but am stuck with this issue as of now.\n", "From what I can tell, the cmake only works on GPU on windows. So if you could get those fixes as PR once you're done that would be terrific.\n\nCould you show how the compiler is called? IIRC, it's make VERBOSE=1. Then we can cross-check against the blaze command-line.\n", "Hi again, as you mentioned, as of now TF works on GPU + Windows. That's why it's in my experimental stage. In 'CMakeList.txt', there are options such as \"(tensorflow_VERBOSE \"Enable for verbose output\" ON)\" which you can pass options with :cmake\" (e.g. cmake  ... -Dxxx). The command \"cmake ...\" generates 'Makefile', then you can put 'sudo make'. As another build option apart from bazel, I think CMake is a cool feature. BTW, tf_stream_executor is blocking my progress with the error above, do you have any insight about this ? \nThanks!\n", "Seemingly, **NVCC** shouldn't be used  for gcuda.cc? \n[91%] Building CXX object CMakeFiles/tf_stream_executor.dir/home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc.o\n/usr/bin/c++  \n-DEIGEN_AVOID_STL_ARRAY \n-DEIGEN_HAS_C99_MATH \n-DLANG_CXX11 \n-DTENSORFLOW_USE_EIGEN_THREADPOOL \n-D__NVCC__ \n-I/home/ubuntu/workspace/tensorflow \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2 \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/zlib_archive \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/gif_archive/giflib-5.1.4 \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/png_archive \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/jpeg_archive \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/eigen_archive \n-I/home/ubuntu/workspace/tensorflow/third_party/eigen3 \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/gemmlowp/src/gemmlowp \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/jsoncpp/src/jsoncpp \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/farmhash_archive \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/farmhash_archive/util \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/external/highwayhash \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/protobuf/src/protobuf/src \n-I/home/ubuntu/workspace/tensorflow/tensorflow/contrib/cmake/build2/grpc/src/grpc/include \n-I/usr/local/cuda \n-I/usr/local/cuda/extras/CUPTI/include \n-I/usr/local/cuda/include \n-I/home/ubuntu/workspace/tensorflow/third_party/gpus  \n-fno-exceptions \n-std=c++11 -fPIC  \n-o CMakeFiles/tf_stream_executor.dir/home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc.o \n-c /home/ubuntu/workspace/tensorflow/tensorflow/stream_executor/gcuda.cc\n", "The file has ifdefs for CUDA. What error are you getting?\n", "While figuring out several issues, I added 'add_definitions(-D__NVCC__) to tf_core_kernel , which caused that error.  Finally, I built tf with cmake and got .so and .a libraries, but I need to test and validate them. \nPlus, I'm also exploring how to build .whl package after cmake build. \n", "Woohoo!\n\nNice! The CMake should have a rule to build the pip tool:\n\n```\ntensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n```\n", "Note that building the PIP package with CMake is slightly different: you would do `make tf_python_build_pip_package` to generate the wheel file.\n", "Thanks for your info, I will try. BTW, Actually, I was looking at 'tf_python.cmake' \n\n```\nadd_custom_command(TARGET tf_python_build_pip_package POST_BUILD\n  COMMAND ${CMAKE_COMMAND} -E copy ${tensorflow_source_dir}/tensorflow/contrib/cmake/setup.py\n                                   ${CMAKE_CURRENT_BINARY_DIR}/tf_python/)\nif(WIN32 OR UNIX)\n  add_custom_command(TARGET tf_python_build_pip_package POST_BUILD\n    COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_CURRENT_BINARY_DIR}/${CMAKE_BUILD_TYPE}/pywrap_tensorflow.dll\n                                     ${CMAKE_CURRENT_BINARY_DIR}/tf_python/tensorflow/python/_pywrap_tensorflow.pyd)\nelse()\n  add_custom_command(TARGET tf_python_build_pip_package POST_BUILD\n    COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_CURRENT_BINARY_DIR}/libpywrap_tensorflow.so\n                                     ${CMAKE_CURRENT_BINARY_DIR}/tf_python/tensorflow/python/_pywrap_tensorflow.so)\nendif()\nadd_custom_command(TARGET tf_python_build_pip_package POST_BUILD\n  COMMAND ${CMAKE_COMMAND} -E copy ${tensorflow_source_dir}/tensorflow/tools/pip_package/README\n                                   ${CMAKE_CURRENT_BINARY_DIR}/tf_python/)\nadd_custom_command(TARGET tf_python_build_pip_package POST_BUILD\n  COMMAND ${CMAKE_COMMAND} -E copy ${tensorflow_source_dir}/tensorflow/tools/pip_package/MANIFEST.in\n                                   ${CMAKE_CURRENT_BINARY_DIR}/tf_python/)\nadd_custom_command(TARGET tf_python_build_pip_package POST_BUILD\n  COMMAND ${PYTHON_EXECUTABLE} ${CMAKE_CURRENT_BINARY_DIR}/tf_python/setup.py bdist_wheel\n  WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/tf_python)\n```\n", "Did you get this to work? It would be fabulous if you could make a PR out of it.\n", "The line `if(WIN32 OR UNIX)` in the CMake file you quoted looks wrong. That branch should only execute on `WIN32` (and it does [at head](https://github.com/tensorflow/tensorflow/blame/20c3d37ecc9bef0e106002b9d01914efd548e66b/tensorflow/contrib/cmake/tf_python.cmake#L371)) because otherwise when running on Ubuntu it'll look for a `.dll` file that doesn't exist.\n", "@mrry, Thans you for your feedback. it was fixed in my CMake file. I could build TF and create whl, it seems working wen I ran ~/workspace/tensorflow$ ls tensorflow/models/image/mnist/convolutional.py, but it's CPU only.  still working on this as of now. \n", "I can leave summary here; \nIn CMakeList.txt \n1. a wrong version number for cuda_config.h for unix/linux\n2. no \"add_definitions\" are for unix/linux (currently, many -D flags are for Windows)\n3.  cuda header path (include_directories) is needed \n4.  cudnn so libs is needed \nlet me update this soon again. \nThanks\n", "Thanks @joshsuihn , very helpful.\n", "@drpngx What's the status of this bug?\n", "User found a resolution.\n"]}, {"number": 5464, "title": "Can't find input_layer after freezing graph for Cifar10 model (it uses shuffle queue)", "body": "I've run [cifar10_train.py]( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_train.py\r\n) example for some time and saved graph and checkpoint files. After that I used freeze_graph.py from python/tools and it worked fine, no errors. Later I've built [label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image) example, it worked fine with it's predefined values, but when I tried to use frozen graph and provided different flags, it failed because I couldn't find proper --input_layer value. I've tried many different values but nothing worked.\r\n\r\nI've tried most of operations as --input_layer in this code snippet from Cifar10 example but none worked for me:\r\n\r\n```\r\ndef read_cifar10(filename_queue):\r\n  \"\"\"Reads and parses examples from CIFAR10 data files.\r\n\r\n  Recommendation: if you want N-way read parallelism, call this function\r\n  N times.  This will give you N independent Readers reading different\r\n  files & positions within those files, which will give better mixing of\r\n  examples.\r\n\r\n  Args:\r\n    filename_queue: A queue of strings with the filenames to read from.\r\n\r\n  Returns:\r\n    An object representing a single example, with the following fields:\r\n      height: number of rows in the result (32)\r\n      width: number of columns in the result (32)\r\n      depth: number of color channels in the result (3)\r\n      key: a scalar string Tensor describing the filename & record number\r\n        for this example.\r\n      label: an int32 Tensor with the label in the range 0..9.\r\n      uint8image: a [height, width, depth] uint8 Tensor with the image data\r\n  \"\"\"\r\n\r\n  class CIFAR10Record(object):\r\n    pass\r\n  result = CIFAR10Record()\r\n\r\n  # Dimensions of the images in the CIFAR-10 dataset.\r\n  # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\r\n  # input format.\r\n  label_bytes = 1  # 2 for CIFAR-100\r\n  result.height = 32\r\n  result.width = 32\r\n  result.depth = 3\r\n  image_bytes = result.height * result.width * result.depth\r\n  # Every record consists of a label followed by the image, with a\r\n  # fixed number of bytes for each.\r\n  record_bytes = label_bytes + image_bytes\r\n\r\n  # Read a record, getting filenames from the filename_queue.  No\r\n  # header or footer in the CIFAR-10 format, so we leave header_bytes\r\n  # and footer_bytes at their default of 0.\r\n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\r\n  result.key, value = reader.read(filename_queue)\r\n\r\n  # Convert from a string to a vector of uint8 that is record_bytes long.\r\n  record_bytes = tf.decode_raw(value, tf.uint8, name=\"input_layer_node\")\r\n\r\n  # The first bytes represent the label, which we convert from uint8->int32.\r\n  result.label = tf.cast(\r\n      tf.slice(record_bytes, [0], [label_bytes]), tf.int32)\r\n\r\n  # The remaining bytes after the label represent the image, which we reshape\r\n  # from [depth * height * width] to [depth, height, width].\r\n  depth_major = tf.reshape(tf.slice(record_bytes, [label_bytes], [image_bytes]),\r\n                           [result.depth, result.height, result.width])\r\n  # Convert from [depth, height, width] to [height, width, depth].\r\n  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\r\n\r\n  return result\r\n```\r\n\r\nAnyone had success with this one?\r\n\r\n>### Environment info\r\n>Operating System: Ubuntu 14.04\r\n>CUDA: 8.0\r\n>cuDNN: 5.0\r\n>tf.__version__\r\n'0.11.0rc2'\r\n\r\n", "comments": ["By using first convolutional layer as input (conv1/Conv2D) I get an interesting error:\n\n> E tensorflow/examples/label_image/main.cc:306] Running model failed: Invalid argument: Must provide as many biases as the last dimension of the input tensor: [64] vs. [1,32,32,3]\n>          [[Node: conv1/BiasAdd = BiasAdd[T=DT_FLOAT, data_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_conv1/Conv2D_0, conv1/biases/read)]]\n\nIt is a bit strange to me, I've checked label_image example and it uses convention for image ops [batch_size, img_width, img_height, channels] and Cifar10 example also uses the same. Now, if Cifar10 is training and running properly, why do we get this error here? It doesn't make sense to me. \n\nHere is code from label_image example that reads image as a tensor:\n\n```\n// Given an image file name, read in the data, try to decode it as an image,\n// resize it to the requested size, and then scale the values as desired.\nStatus ReadTensorFromImageFile(string file_name, const int input_height,\n                               const int input_width, const float input_mean,\n                               const float input_std,\n                               std::vector<Tensor>* out_tensors) {\n  auto root = tensorflow::Scope::NewRootScope();\n  using namespace ::tensorflow::ops;  // NOLINT(build/namespaces)\n\n  string input_name = \"file_reader\";\n  string output_name = \"normalized\";\n  auto file_reader = ReadFile(root.WithOpName(input_name), file_name);\n  // Now try to figure out what kind of file it is and decode it.\n  const int wanted_channels = 3;\n  Output image_reader;\n  if (tensorflow::StringPiece(file_name).ends_with(\".png\")) {\n    image_reader = DecodePng(root.WithOpName(\"png_reader\"), file_reader,\n                             DecodePng::Channels(wanted_channels));\n  } else if (tensorflow::StringPiece(file_name).ends_with(\".gif\")) {\n    image_reader = DecodeGif(root.WithOpName(\"gif_reader\"), file_reader);\n  } else {\n    // Assume if it's neither a PNG nor a GIF then it must be a JPEG.\n    image_reader = DecodeJpeg(root.WithOpName(\"jpeg_reader\"), file_reader,\n                              DecodeJpeg::Channels(wanted_channels));\n  }\n  // Now cast the image data to float so we can do normal math on it.\n  auto float_caster =\n      Cast(root.WithOpName(\"float_caster\"), image_reader, tensorflow::DT_FLOAT);\n  // The convention for image ops in TensorFlow is that all images are expected\n  // to be in batches, so that they're four-dimensional arrays with indices of\n  // [batch, height, width, channel]. Because we only have a single image, we\n  // have to add a batch dimension of 1 to the start with ExpandDims().\n  auto dims_expander = ExpandDims(root, float_caster, 0);\n  // Bilinearly resize the image to fit the required dimensions.\n  auto resized = ResizeBilinear(\n      root, dims_expander,\n      Const(root.WithOpName(\"size\"), {input_height, input_width}));\n  // Subtract the mean and divide by the scale.\n  Div(root.WithOpName(output_name), Sub(root, resized, {input_mean}),\n      {input_std});\n\n  // This runs the GraphDef network definition that we've just constructed, and\n  // returns the results in the output tensor.\n  tensorflow::GraphDef graph;\n  TF_RETURN_IF_ERROR(root.ToGraphDef(&graph));\n\n  std::unique_ptr<tensorflow::Session> session(\n      tensorflow::NewSession(tensorflow::SessionOptions()));\n  TF_RETURN_IF_ERROR(session->Create(graph));\n  TF_RETURN_IF_ERROR(session->Run({}, {output_name}, {}, out_tensors));\n  return Status::OK();\n}\n```\n", "Could you please include the exact steps, and the exact code if you have written any?\n\nThanks,\nSherry\n", "@sherrym - ok, the thing was that queue returns images in batches of 128 images at a time so CNN expects tensor of shape [batch_size, height, width, channels]. When I changed batch_size to 1 and trained it like that, everything worked fine. I'm wondering still if there is a way to train network with batch_size != 1 and then later freeze graph and reuse it by classifying images 1 by 1? Maybe somehow tell CNN to receive tensors of shape [None, height, width, channels] so when I freeze graph and run it from C++ it will work with only 1 example. My current solution could be to train Cifar10 model with batch_size = 128 and then in C++ create tensor with shape [128, height, width, channels], only first image would be real image and other \"images\" would be just random numbers, but that seems like waste of processing time and resources.\n", "@petewarden Can you take a look at this graph feezing issue?\n", "Was there any progress on this?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5463, "title": "Add dynamic_pad to shuffle_batch", "body": "Add 'dynamic_pad' flag to `shuffle_batch` as implemented in `batch`\r\nAdded the c++ implementation of padding_random_shuffle_queue \r\n#5147 ", "comments": ["Can one of the admins verify this patch?\n", "@SriramRamesh, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @ebrevdo to be potential reviewers.\n", "Derek, do we want the added complexity of a padded shuffle queue?  PTAL.\n", "According to #5147, it's a contribution we'd welcome. I assume there's a use case?\n\n(I'm not thrilled by adding another queue implementation, but it's compatible with the current way things are implemented. It looks like there's a lot of repeated logic in the new shuffle queue... perhaps the padding and shuffling could be implemented orthogonally so there isn't as much repetitive code?)\n", "+1 Sriram, is there a way to refactor the two queues in question (shuffle + paddingshuffle) to use one common base set of functions for shuffling?\n", "RandomShuffleQueue can take dynamic_pad as a default argument and then implement padding if it is true  \n", "@mrry are you ok merging the logic for dynamic_padding and w/o into a single kernel impl?  if so, we could do the same for PaddingFIFOQueue and regular FIFOQueue.\n", "@ebrevdo: I don't think we want a single kernel (at the very least, we need separate kernels for backwards compatibility, but I would also prefer it aesthetically)... but the aim is to cut down on repeated code: so we might want to refactor the queue implementation so that (e.g.) the padding (output preparation) logic is separate from the coordination and the element selection code.\n", "Sriram, want to try to factor the c++ implementations of the two ops to\nshare a common codebase for everything but the padded copies?\n\nOn Nov 15, 2016 12:31 PM, \"Derek Murray\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo: I don't think we want a single\n> kernel (at the very least, we need separate kernels for backwards\n> compatibility, but I would also prefer it aesthetically)... but the aim is\n> to cut down on repeated code: so we might want to refactor the queue\n> implementation so that (e.g.) the padding (output preparation) logic is\n> separate from the coordination and the element selection code.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5463#issuecomment-260758479,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim1FB53Fi5miz4RoKumduNajAWwsZks5q-haRgaJpZM4KroQo\n> .\n", "Sorry,I didn't get you @ebrevdo \nShould i create a common base for PaddingRandomShuffleQueue and PaddingFIFOQueue?\n", "This seems reasonable, yes.\n\nOn Fri, Nov 18, 2016 at 1:58 PM, Sriram notifications@github.com wrote:\n\n> Sorry,I didn't get you @ebrevdo https://github.com/ebrevdo\n> Should i create a common base for PaddingRandomShuffleQueue and\n> PaddingFIFOQueue?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/5463#issuecomment-261652873,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim3QmBpFhQFFMn9P0f78bpGTLAI52ks5q_h-igaJpZM4KroQo\n> .\n", "@SriramRamesh Please also rebase when you make the changes ebrevdo suggested.", "Friendly ping. Can you rebase? "]}, {"number": 5462, "title": "dist_test local k8s cluster setup is broken, use kubernetes/minikube", "body": "**Feature Request/Documentation Request**\r\n\r\nlocal k8s cluster dist_test broken, use [kubernetes/minikube](https://github.com/kubernetes/minikube) instead\r\n\r\n\r\nAs of commit 21a7ae05e04f4f060938db08015cb47896970dd1, the dockerfile for setting up a local cluster for dist_test is broken.  This dockerfile imports start_local_k8s_service.sh and other scripts that were deleted with this merge.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/local/Dockerfile#L18\r\n```\r\n$ docker build .\r\n...\r\nStep 9 : ADD start_local_k8s_cluster.sh /var/k8s/start_local_k8s_cluster.sh\r\nlstat start_local_k8s_cluster.sh: no such file or directory\r\n...\r\n```\r\n\r\nInstead of this dockerfile, this local cluster setup should be switched over to Minikube, the officially supported method for non-k8s-development local clusters.\r\n\r\nhttp://kubernetes.io/docs/getting-started-guides/minikube/\r\n\r\nAs a maintainer for minikube, I would happily create a PR with instructions and a tutorial on how to run dist_test locally.  If this sounds good to you, please assign this issue to me.      \r\n", "comments": ["cc @elibixby @dlorenc\n", "@mrry Can you comment?\n", "My guess is this should be \"contributions welcome\", although we should check with @caisq, who created the test in the first place.\n", "This is definitely the way to go long term IMO. I'm wondering if it would be worth having a separate repo to store all the third-party (including k8s) tooling around running distributed TF. Useful configs, etc.\n", "@elibixby Perhaps https://github.com/tensorflow/ecosystem would be the right place? There's already [some k8s content](https://github.com/tensorflow/ecosystem/tree/master/kubernetes) in that repo.\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 5461, "title": "Branch 138409704", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @petewarden and @tensorflower-gardener to be potential reviewers.\n", "Jenkins, test this please.\n"]}, {"number": 5460, "title": "reduce_sum extremely slow on gpu for complex dtypes", "body": "## Description\r\n`tf.reduce_sum` takes an order of magnitude longer to compute when the tensor is a complex dtype and the computation takes place on a gpu. I've tried to investigate the source of the problem and it's not clear to me whether this is a Tensorflow issue or an Eigen issue, but I thought I'd raise it here first.\r\n\r\n## Environment\r\nUbuntu 16.04 LTS\r\nTensorflow: 1fcd6d1294564066c6f92b121a3aaf4ed186dc1a\r\nGPU: Titan X\r\nPython 2.7.11\r\n\r\n## Diagnostics/Reproducibility:\r\nI produced a tensorflow timeline trace. The trace shows that reductions for a real and complex tensor on the cpu, and for a real tensor on the gpu, takes no longer than 1.3 ms. The reduction of a complex tensor of the same size takes about 196 ms on the gpu.  In particular this time is spent in a dedicated GPU stream, where the input arguments are marked as undefined in the trace.\r\n\r\nMore specifically, here's a table summarizing the relevant parts of the trace:\r\n\r\n### CPU, Complex Reduction\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n0.076 ms | 1.174 ms | 1.174 ms | \"cplx_var/read\" | \"cplx_reduction/range\" | \"cplx_reduction/Sum\" | \"Sum\"\r\n\r\n### GPU, Real Reduction\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n0.087 ms | 1.267 ms | 1.267 ms | \"real_var/read\" | \"real_reduction_1/range\" | \"real_reduction_1/Sum\" | \"Sum\"\r\n1.355 ms | 0.043 ms | 0.043 ms | undefined | undefined | \"real_reduction_1/Sum\" | \"Sum\"\r\n\r\n### CPU, Real Reduction\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n4.010 ms | 0.465 ms | 0.465 ms | \"real_var/read/_7\" | \"real_reduction/range\" | \"real_reduction/Sum\" | \"Sum\"\r\n\r\n### GPU, Complex Reduction (Slow)\r\nStart | Wall Duration (ms) | Self time (ms) | Arg: input0 | Arg: input1 | Arg: name | Arg: op\r\n-----|--------------------|---------------|------------|------------|------------|--------\r\n4.019 ms | 0.070 ms | 0.070 ms | \"cplx_var/read/_5\" | \"cplx_reduction_1/range\" | \"cplx_reduction_1/Sum\" | \"Sum\"\r\n4.078 ms | 195.994 ms | 195.994 ms | undefined | undefined | \"cplx_reduction_1/Sum\" | \"Sum\"\r\n\r\nThe trace and a screenshot are attached as a text file and pdf file, respectively. The code that generated the trace is provided below:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.client import timeline\r\n\r\ndef reduction_tester(real_var, cplx_var):\r\n    with tf.name_scope('real_reduction'):\r\n        reduced_real = tf.reduce_sum(real_var)\r\n    with tf.name_scope('cplx_reduction'):\r\n        reduced_cplx = tf.reduce_sum(cplx_var)\r\n    return reduced_real, reduced_cplx\r\n\r\ndef main():\r\n    R_DTYPE = np.float32\r\n    C_DTYPE = np.complex64\r\n    r = np.random.randn(1000, 2000).astype(R_DTYPE)\r\n    i = np.random.randn(1000, 2000).astype(R_DTYPE)\r\n    real_var = tf.get_variable('real_var', initializer=r,\r\n                               dtype=tf.as_dtype(R_DTYPE))\r\n    cplx_var = tf.get_variable('cplx_var', initializer=r+1j*i,\r\n                               dtype=tf.as_dtype(C_DTYPE))\r\n\r\n    with tf.device('/cpu'):\r\n        real_reduc_cpu, cplx_reduc_cpu = reduction_tester(real_var, cplx_var)\r\n\r\n    with tf.device('/gpu'):\r\n        real_reduc_gpu, cplx_reduc_gpu = reduction_tester(real_var, cplx_var)\r\n\r\n    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True,\r\n                                            allow_soft_placement=False))\r\n    init = tf.initialize_all_variables()\r\n    sess.run(init)\r\n\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    ops = [real_reduc_cpu, cplx_reduc_cpu, real_reduc_gpu, cplx_reduc_gpu]\r\n    ret = sess.run(ops, options=run_options, run_metadata=run_metadata)\r\n\r\n    real_np_sum = np.sum(r.astype(R_DTYPE))\r\n    cplx_np_sum = np.sum((r+1j*i).astype(C_DTYPE))\r\n    assert np.allclose(ret[0], real_np_sum, rtol=1e-4)\r\n    assert np.allclose(ret[1], cplx_np_sum, rtol=1e-4)\r\n    assert np.allclose(ret[2], real_np_sum, rtol=1e-4)\r\n    assert np.allclose(ret[3], cplx_np_sum, rtol=1e-4)\r\n\r\n    tl = timeline.Timeline(run_metadata.step_stats)\r\n    with open('/home/sarroff/tmp/timeline.trace', 'w') as f:\r\n        f.write(tl.generate_chrome_trace_format(show_memory=True))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n## Partial Workaround\r\nIt's easy to avoid placing explicit calls to `tf.reduce_sum` by flattening the tensor into a row and performing a matrix multiply with a tensor column of 1s. The performance on complex tensors using the gpu is reasonable using this approach. However Tensorflow uses reductions internally for many gradient computations and these are harder to avoid.\r\n\r\n[timeline.trace.txt](https://github.com/tensorflow/tensorflow/files/576345/timeline.trace.txt)\r\n[chrome_tracing.pdf](https://github.com/tensorflow/tensorflow/files/576234/chrome_tracing.pdf)\r\n", "comments": ["This issue is quite old and hasn't had recent activity. If it is still an issue in the latest version of TensorFlow, and please create a new bug. We've made many performance improvements since then, and it's quite possible that this has been covered by one of them.", "This was still an issue, but it is now actually fixed.", "@ekelsen I came across the same problem. How to solve it?", "It should be solved.  Are you using head or a release?"]}, {"number": 5459, "title": "Typo, causing AttributeError", "body": "$ ~/mnist $ python convolutional.py\r\nTraceback (most recent call last):\r\n  File \"convolutional.py\", line 316, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"convolutional.py\", line 123, in main\r\n    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\r\n  File \"convolutional.py\", line 62, in maybe_download\r\n    size = f.Size()\r\nAttributeError: 'GFile' object has no attribute 'Size'", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Can one of the admins verify this patch?\n", "@normanyahq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @vincentvanhoucke to be potential reviewers.\n", "The existing code is correct: the function name is Size (capital S) in branch 0.7 from 9+ months ago.  Check out the code from newer branches if you are using a newer binary.\n"]}, {"number": 5458, "title": "Tensorflow on android: directly build app in python?", "body": "The sample app given by google for tensorflow on android is written in C++.\r\n\r\nI have a tensorflow application written in python. This application currently runs on desktop. I want to move the application to android platform. Can I use bazel to build the application that is written in python directly for android? Thanks.", "comments": ["@michaelhuang74 No, the short answer is that it is not possible (or at least supported) to run any TensorFlow python code on Android. However, you may of course reuse your trained GraphDef proto (after running [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) on it to bake in the checkpoint weights), and if your interaction with TensorFlow is not too complex you should be able to fairly easily hook up your inputs and outputs to achieve the same effect in either Java or C++.\n\nCan you clarify a bit more exactly what you're trying to achieve? Do you have an existing app you wanted to hook the python code into, or did you want to run it as a standalone program? In the former case, you'll need to convert it to use TensorFlowInferenceInterface.java. In the latter, you'll want to convert to C++ as seen in tensorflow/tools/benchmark:benchmark_model.\n", "**@andrewharp**  Thank you very much for your reply.\n\nThe python application that I want to move to android is evaluate.py at https://github.com/lengstrom/fast-style-transfer. \n\nevaluate.py will take a model file (checkpoint file) generated by the style.py and stylize an input image.  Style.py will stay on desktop computer. I intend to move the evaluate.py to android.\n\nI am new to both tensorflow and application development on android. I appreciate if you can provide some specifics how to modify the sample tensorflow on android demo to port the evaluate.py to android platform. Thanks.\n", "While convert python code to C++ code, we find that the python code uses several python APIs such as tf.nn.tanh, tf.nn.conv2d, tf.nn.relu, tf.nn.moment, tf.nn.conv2d_transpose.  \r\n\r\nWhere in the tensorflow repository can I find the C++ implementation for the above python APIs? Thanks.", "TF should find these operations automatically when loading your trained GraphDef, which you should use freeze_graph.py to create in conjunction with your checkpoint file. You do not need to refer to them by name in the Android app, you just need to set the input and output nodes similar to how the demo does it in [ClassifierActivity](https://github.com/tensorflow/tensorflow/blob/a2eba827afa870f7db0e0a582c630125a622e725/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java#L51)\r\n\r\nApologies for the delayed response! This is not a TF issue though -- you may have better luck on with these sorts of queries on stackoverlow.\r\n", "@andrewharp I have a slightly more complex architecture in TF which is ConvNet + LSTMs with visual attention. Similar to [Im2Text](https://github.com/tensorflow/models/tree/master/im2txt) rather than the classifier models. Say I checkpoint my graph and rewrite ClassifierActivity as instructed in your previous comment, will I be able to make it work on an Android device? If not, what other changes might I have to make?\r\n\r\nAlso,\r\n\r\n>  if your interaction with TensorFlow is not too complex\r\n\r\ncould you explain how complex is \"too complex\" and what issues might arise because of it?\r\n\r\nThanks.", "Is is possible to execute tf.gradients in android environment.\r\nI have to take gradients in Android application.\r\nWhat is the best way to do so.\r\n\r\nThanks"]}, {"number": 5457, "title": "Ignore tools/git/gen", "body": "Here you go @vrv.  For some reason my previous edit to `tools/git/.gitignore` just vanished; this adds it to the right place.", "comments": ["@girving, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @rasbt and @davidzchen to be potential reviewers.\n"]}, {"number": 5456, "title": "[Windows/CMake] Remove dependency on zlib.dll.", "body": "Instead, use the static `zlibstatic.lib`, as originally intended.\r\n\r\nFixes #5275.", "comments": ["@tensorflow-jenkins test this please.\n"]}, {"number": 5455, "title": "Unable to save the wide and deep model in a tensorflow session ", "body": "I am running the wide and deep model in TensorFlow serving and to export the trained model I am using the piece of code\r\n\r\n![image](https://cloud.githubusercontent.com/assets/17990840/20066180/b18823aa-a4de-11e6-9c88-455b0df31c91.png)\r\n But while using the command ` saver = tf.train.Saver()` the error ` ValueError: No variable to save is displayed`\r\n\r\n![image](https://cloud.githubusercontent.com/assets/17990840/20066325/2b4db204-a4df-11e6-8572-5bc26bd63e55.png)\r\n\r\nHow can I save the model, so that a servable is created which is required while loading the exported model in tensorflow standard server? Any help is appreciated.", "comments": ["Could you please include the complete code? I do not see any a model being built? Typically you need to build a model that contains variables.\n\nCC'ing Noah in any case.\n\nSherry\n", "This is the entire piece of code\n\n```\n\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom tensorflow.contrib.session_bundle import exporter\nimport tempfile\nimport sys\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\"model_dir\", \"/tmp/model_dir\", \"Base directory for output models.\")\nflags.DEFINE_string(\"model_type\", \"wide_n_deep\",\n                    \"Valid model types: {'wide', 'deep', 'wide_n_deep', 'regressor'}.\")\nflags.DEFINE_integer(\"train_steps\", 100, \"Number of training steps.\")\nflags.DEFINE_string(\n    \"train_data\",\n    \"\",\n    \"Path to the training data.\")\nflags.DEFINE_string(\n    \"test_data\",\n    \"\",\n    \"Path to the test data.\")\nflags.DEFINE_string(\n    \"predict_data\",\n    \"\",\n    \"Path to the prediction data.\")\n\nCOLUMNS = [\"bid\", \"bidrequesttime\", \"crid\", \"domain\", \"lid\",\n           \"floor\", \"ip\", \"device\", \"os\", \"pubid\",\n           \"userid\", \"winprice\",\"impressionflag\"]\nLABEL_COLUMN = \"impressionflag\"\nCATEGORICAL_COLUMNS = [\"bid\",\"crid\",\"domain\",\"lid\",\"ip\",\"device\",\"os\",\"userid\"]\nCONTINUOUS_COLUMNS = [\"bidrequesttime\",\"floor\",\"pubid\",\"winprice\"]\n\n\ndef maybe_download():\n  \"\"\"May be downloads training data and returns train and test file names.\"\"\"\n  if FLAGS.train_data:\n    train_file_name = FLAGS.train_data\n  else:\n    #train_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)  # pylint: disable=line-too-long\n    #train_file_name = train_file.name\n    #train_file.close()\n    train_file_name=pd.read_csv('TrainingData_v1.0.csv', skipinitialspace=True,\n                                skiprows=1, names=COLUMNS)\n\n\n  if FLAGS.test_data:\n    test_file_name = FLAGS.test_data\n  else:\n    #test_file = tempfile.NamedTemporaryFile(delete=False)\n    #urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)  # pylint: disable=line-too-long\n    #test_file_name = test_file.name\n    #test_file.close()\n    test_file_name=pd.read_csv('TestData_v1.0.csv', skipinitialspace=True,\n                               skiprows=1, names=COLUMNS)\n\n  if FLAGS.predict_data:\n    predict_file_name = FLAGS.predict_data\n  else:\n    predict_file_name=pd.read_csv('PredictData_v1.0.csv', skipinitialspace=True,\n                                  skiprows=1, names=COLUMNS)\n\n  return train_file_name, test_file_name,predict_file_name\n\n\ndef build_estimator(model_dir):\n  \"\"\"Build an estimator.\"\"\"\n  # Sparse base columns.\n  bid=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"bid\", hash_bucket_size=10000)\n  crid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"crid\", hash_bucket_size=1000)\n  domain=tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"domain\", hash_bucket_size=1000)\n  lid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"lid\", hash_bucket_size=1000)\n  ip = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"ip\", hash_bucket_size=1000)\n  device = tf.contrib.layers.sparse_column_with_keys(\n    column_name=\"device\", keys=[\"Personal Computer\",\"Mobile/Tablet\",\"None\"])\n  os = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"os\", hash_bucket_size=1000)   \n  userid = tf.contrib.layers.sparse_column_with_hash_bucket(\n      \"userid\", hash_bucket_size=1000)\n\n\n  # Continuous base columns.\n  bidrequesttime = tf.contrib.layers.real_valued_column(\"bidrequesttime\")\n  floor = tf.contrib.layers.real_valued_column(\"floor\")\n  pubid = tf.contrib.layers.real_valued_column(\"pubid\")\n  winprice = tf.contrib.layers.real_valued_column(\"winprice\")\n\n\n  # Wide columns and deep columns.\n  wide_columns = [bid,crid,domain,lid,ip,device,os,userid]\n\n  deep_columns = [\n      tf.contrib.layers.embedding_column(bid, dimension=8),\n      tf.contrib.layers.embedding_column(crid, dimension=8),\n      tf.contrib.layers.embedding_column(domain,dimension=8),\n      tf.contrib.layers.embedding_column(lid, dimension=8),\n      tf.contrib.layers.embedding_column(ip, dimension=8),\n      tf.contrib.layers.embedding_column(device, dimension=8),\n      tf.contrib.layers.embedding_column(os,dimension=8),\n      tf.contrib.layers.embedding_column(userid, dimension=8),\n      bidrequesttime,\n      floor,\n      pubid,\n      winprice,\n  ]\n\n  if FLAGS.model_type == \"wide\":\n    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n                                          feature_columns=wide_columns,\n                                          optimizer=tf.train.FtrlOptimizer(\n                                            learning_rate=0.1,\n                                            l1_regularization_strength=0.001\n                                          ))\n\n  elif FLAGS.model_type == \"deep\":\n    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n                                       feature_columns=deep_columns,\n                                       hidden_units=[10,10],\n                                       optimizer=tf.train.ProximalAdagradOptimizer(\n                                         learning_rate=0.1,\n                                         l1_regularization_strength=0.1\n                                      ) )\n\n  else:\n    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n        model_dir=model_dir,\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=[10,20,20,10])\n\n\n  return m\n\n\ndef input_fn(df):\n  \"\"\"Input builder function.\"\"\"\n  # Creates a dictionary mapping from each continuous feature column name (k) to\n  # the values of that column stored in a constant Tensor.\n  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n  # Creates a dictionary mapping from each categorical feature column name (k)\n  # to the values of that column stored in a tf.SparseTensor.\n  categorical_cols = {k: tf.SparseTensor(\n      indices=[[i, 0] for i in range(df[k].size)],\n      values=df[k].values,\n      shape=[df[k].size, 1])\n                      for k in CATEGORICAL_COLUMNS}\n  # Merges the two dictionaries into one.\n  feature_cols = dict(continuous_cols)\n  feature_cols.update(categorical_cols)\n  # Converts the label column into a constant Tensor.\n  label = tf.constant(df[LABEL_COLUMN].values)\n  print(\"Label was created successfully\")\n  # Returns the feature columns and the label.\n  return feature_cols, label\n\n\ndef train_and_eval():\n  \"\"\"Train and evaluate the model.\"\"\"\n\n  train_file_name, test_file_name, predict_file_name = maybe_download()\n  df_train=train_file_name\n  df_test=test_file_name\n  df_predict=predict_file_name\n  model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n  print(\"model directory = %s\" % model_dir)\n  m = build_estimator(model_dir)\n  print('model successfully build!!')\n\n  with tf.Session() as sess:\n    init_op=tf.initialize_all_variables()\n    saver=tf.train.Saver()\n    m.fit(input_fn=lambda: input_fn(df_train), steps=FLAGS.train_steps)\n    print('model successfully fit!!')\n    results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n    for key in sorted(results):\n      print(\"%s: %s\" % (key, results[key]))\n    model_exporter = exporter.Exporter(saver)\n    model_exporter.init(\n    sess.graph.as_graph_def(),\n    named_graph_signatures={\n        'inputs': exporter.generic_signature({'input': df_train}),\n        'outputs': exporter.generic_signature({'output': df_train[impressionflag]})})\n    model_exporter.export(export_path, tf.constant(FLAGS.export_version), sess)\n    y = m.predict(input_fn=lambda: input_fn(df_predict))\n    print ('Predictions: {}'.format(str(y)))\n\ndef main(_):\n  train_and_eval()\n\n\nif __name__ == \"__main__\":\n  tf.app.run()\n\n```\n", "Hi @vasantivmahajan ,\n\nYour build_estimator() function does not construct a graph. It builds a bunch of Python objects. As a result there's no graph, and no Variables to save.\n\nSherry\n", "How can I construct a graph from this? I followed the instructions from the below-mentioned link \"https://www.tensorflow.org/versions/r0.11/tutorials/wide_and_deep/index.html\" to build my model. I would really appreciate if you help me with forming a graph from these objects?\n", "@hengtze one of the original authors of the code and tutorial should be able to help you out.\n", "@hengtze Can you guide me with this?\n", "Can someone guide me on this issue?\nStill looking for a resolution.\n", "@vasantivmahajan have you got the solution\uff1f", "@vasantivmahajan Was a solution provided via DM? Did you ever solve this on your own? ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Please check out [this sample for Tensorflow Wide and Deep Local prediction and saving to CSV](https://github.com/AshutoshDongare/Tensorflow-Wide-Deep-Local-Prediction)\r\n"]}]