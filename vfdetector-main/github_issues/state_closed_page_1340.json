[{"number": 12887, "title": "Rearranging statements", "body": "https://github.com/tensorflow/tensorflow/issues/12857\r\n\r\nHighlights:-\r\nIt generates informative error.", "comments": ["Can one of the admins verify this patch?", "@printdhruv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @drpngx to be potential reviewers.", "@tensorflow-jenkins test this please", "@yifeif Any suggestions for `tensorflow/contrib/layers/python/layers/optimizers_test.py`? My local machine is passing all test cases for that. ", "@honkentuber could you help taking a look? Thanks!\r\n", "+wicke,ispir\n\nI'm not on ML any more.\n\nOn Sep 7, 2017 2:54 PM, \"Yifei Feng\" <notifications@github.com> wrote:\n\n> @honkentuber <https://github.com/honkentuber> could you help taking a\n> look? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12887#issuecomment-327937062>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AS8QxX_YCj6SjAqda669FFrumKgv5U8Gks5sgGYxgaJpZM4PQXM->\n> .\n>\n", "It appears that you changed the behavior, probably by changing the order of ops. @aselle do you see why that might be? It may be a question if simply updating the tests. The second failure looks a little dangerous though.", "@martinwicke Yes.I am changing the flow. ", "Essentially, the order the graph is constructed will change the random number seeding, even if the number of ops and connectivity is the same. That test should be uneatable in the baseline.\r\n", "@aselle what should be your guidelines to achieve tf.int32 error message as expected ?\r\nBy default, mean is invoked hence I changed the order.\r\nShould I check it in truncated_normal() function itself and raise error for invalid dtype?", "Why does this change generates informative error?", "Should we close this PR?", "@gunan As,it goes against the random number seeding pointed out by aselle , this fix won't work. The simplest thing would be to brute force checking for this function or to change error message which will be affected to the all connected components.", "Ok, so close it? Fix the tests? @aselle? @yifeif?", "I am closing this PR, as now it is clear that this is not the way we would like to fix this.\r\nWe can explore the other approach in another PR."]}, {"number": 12886, "title": "Fixed #12797: uint8 now has a length", "body": "This small change allows tensors of uint8 to be constructed from the Java API. I am leaving the new test case of it over in the phase2 PR (#11535) because that test case relies on other changes to Tensor.create.", "comments": ["Can one of the admins verify this patch?", "@andrewcmyers, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @tensorflower-gardener and @jhseu to be potential reviewers.", "Jenkins, test this please"]}, {"number": 12885, "title": "mmap file", "body": "I added a memory mapped file option for the model file.  Since the model files have to be copied out of the apk due to assets directory readonly status and inability to create the FileChannel directly, it is assumed that you have done so to internal storage with the same context that is passed into the overloaded constructor.  Internal storage eliminates problems of SD card not being present or M runtime permissions requests but is also in danger of being full etc.  Expansion files are the only other option, but this requires a download from the play store, which is an extra step.  I therefore did the most simple thing.", "comments": ["Can one of the admins verify this patch?", "@cloudbank, thanks for your PR! By analyzing the history of the files in this pull request, we identified @asimshankar, @andrewharp and @jhseu to be potential reviewers.", "I could not squash my commits so I created a new request with a clean history.", "@andrewharp could you take a look or reassign? Thank you!", "@andrewharp if you want some code that uses it and copies files initially,  I am working on a library  which is using the TensorFlowImageClassifier. I see the comment:\r\nTODO(andrewharp): make this handle non-assets.\r\nIf you have ideas I have not mentioned about where to hold the model files, I would be flexible about working on some other solutions too.", "@cloudbank That TODO refers to the label file for the demo specifically. Since that's just example app code (under examples/android) rather than library code (under contrib/android) it felt less urgent to generalize it.\r\n\r\nI would not be opposed to having the TFII class do a check and copy the file from assets to a specified tmp dir if not already present, if we can show that this sufficiently speeds up subsequent runs.", "@andrewharp  The worry point is when to delete the file that was copied. Although this does not affect the buffer, obviously the mapped file won't be there forever either. Ideally, this heavy create operation involving the big graph file is done off heap avoiding IO exactly once and the classifier object persisted for further usage. Of course all of this is taken care of outside of the TFII in my work.   I certainly could add copy and delete methods if you insist but would not want to encourage repeated copy, or for that matter repeated create.  I would love to work on adding persistence and feel safe to copy/delete once and then null out the buffer.  \r\nI am fixing the typo and whitespace edits.  Tell me what you think about the delete."]}, {"number": 12884, "title": "Fix typos in contrib.estimator.extenders.py", "body": "", "comments": []}, {"number": 12883, "title": "Additional arguments for summary.image", "body": "### System information\r\nN/A \r\n\r\n### Describe the problem\r\n\r\nIt'd be nice to have more control over the color map applied to the image summaries. In particular having `vmin` and `vmax` arguments as in matplotlib [imshow](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.imshow.html) function.\r\nIf no value is given the default behaviour could be kept, but if specified, black would correspond to vmin and white to vmax.\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["You can simply do:\r\n```\r\nimg = (img - vmin) / (vmax - vmin) * 255\r\ntf.summary.image('img', tf.cast(img, tf.uint8))\r\n```\r\nto get what you want.", "I wasn't asking for a workaround. I'm trying to make the point that it's common enough that it makes sense to have that code integrated in the summary.image() function as it has been done with the matplotlib equivalent.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@alextp May I submit a PR for this feature?", "Please!\n\nOn Tue, Aug 21, 2018 at 11:14 AM Fei Hu <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> May I submit a PR for this feature?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12883#issuecomment-414769241>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxRV_TYJx0PrWUNlM8Otlkm_u0yi6ks5uTE3ngaJpZM4PQHIi>\n> .\n>\n\n\n-- \n - Alex\n", "An update - after some discussion, our plans for the `tf.summary.image()` op are actually to keep it narrowly focused on the minimum necessary for summary export (i.e. PNG conversion), and thus not add further normalization controls.\r\n\r\nIn fact, for TF 2.0, we are planning on reducing the normalization provided by the op to just do [`tf.image.convert_image_dtype(tensor, tf.uint8, saturate=True)`](https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype).  With that change, float tensors will always be scaled according to an expected range of `[0.0, 1.0)`, so it will no longer adjust the scaling range based on the input data max and min values.  This brings it into consistency with all the various `tf.image.*` ops which only support `tf.image.convert_image_dtype()` as an input conversion.\r\n\r\nI understand that it adds some convenience to be able to use the op's built-in normalization for a wider range of numerical inputs, but ultimately it doesn't scale well to parameterize the op so that the normalization can be fine-tuned.  So for TF 2.0 we're doing only the most minimal conversion and recommending that users who need more than that do their own explicit normalization."]}, {"number": 12882, "title": "[OpenCL SYCL] Add support for triSYCL in TensorFlow", "body": "Add the ability to use triSYCL as the SYCL implementation in TensorFlow when OpenCL SYCL support is enabled, relates to #22 \r\n\r\nNote that triSYCL is still work in progress and many features are missing, currently only the host device can be used (triSYCL/triSYCL#51).\r\n\r\nComputeCPP is a much more reliable way of using SYCL with TensorFlow.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Can you reformat tensorflow/core/platform/default/build_config/BUILD to fix the errors below ?\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n181c181\r\n<         \"-Wl,-rpath,../local_config_sycl/sycl/lib\"\r\n---\r\n>         \"-Wl,-rpath,../local_config_sycl/sycl/lib\",\r\n183,184c183,186\r\n<     deps = if_ccpp([\"@local_config_sycl//sycl:syclrt\"],\r\n<                    [\"@local_config_sycl//sycl:sycl_headers\"]),\r\n---\r\n>     deps = if_ccpp(\r\n>         [\"@local_config_sycl//sycl:syclrt\"],\r\n>         [\"@local_config_sycl//sycl:sycl_headers\"],\r\n>     ),\r\n209c211\r\n< )\r\n\\ No newline at end of file\r\n---\r\n> )", "Jenkins, test this please.", "Buildifier is complaining about the following remaining issue:\r\n\r\ntensorflow/core/platform/default/build_config/BUILD # reformat \r\nbuildifier suggested the following changes:\r\n183,184c183,185\r\n<     deps = if_ccpp([\"@local_config_sycl//sycl:syclrt\"],\r\n<                    [\"@local_config_sycl//sycl:sycl_headers\"]\r\n---\r\n>     deps = if_ccpp(\r\n>         [\"@local_config_sycl//sycl:syclrt\"],\r\n>         [\"@local_config_sycl//sycl:sycl_headers\"],\r\nPlease fix manually or run buildifier <file> to auto-fix.", "@a-doumoulakis can you fix the merge conflict and take a look at the tests", "Jenkins, test this please.", "@a-doumoulakis Unfortunately, we have another sanity check failure:\r\n\r\n`FAIL:`\r\n `buildifier found errors and/or warnings in above BUILD files.`\r\n`buildifier suggested the following changes:`\r\n`204,205c204,206`\r\n`<     deps = if_ccpp([\"@local_config_sycl//sycl:syclrt\"],`\r\n`<                    [\"@local_config_sycl//sycl:sycl_headers\"],`\r\n`---`\r\n`>     deps = if_ccpp(`\r\n`>         [\"@local_config_sycl//sycl:syclrt\"],`\r\n`>         [\"@local_config_sycl//sycl:sycl_headers\"],`\r\n`Please fix manually or run buildifier <file> to auto-fix.`\r\n\r\nMaybe you could run [buildifier](https://github.com/bazelbuild/buildtools) on all the BUILD file to knock all these failures in one go ?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 12881, "title": "adding missing dep on jdk for gen_ops.bzl (again)", "body": "* Redoing changes in https://github.com/tensorflow/tensorflow/pull/12827\r\n(undid by merge)", "comments": ["Can one of the admins verify this patch?", "Closing as change has been submitted internally."]}, {"number": 12880, "title": "tf.map_fn handles elems differently if it's a list or tuple", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'unknown' 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 6\r\n- **GPU model and memory**: GTX 1080, 8GB\r\n- **Exact command to reproduce**: Just run the script\r\n\r\n\r\n### Describe the problem\r\nIt seems from some simple experiments that `tf.map_fn` behaves differently from what documented according to the specific type of its `elems` parameter. If the parameter is a tuple of tensors, the code works. If it's a list of tensors, however, the function is applied to the list itself instead than its elements (thus raising an error in the example code I put below)\r\n\r\n### Code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nelems = tf.constant([1,2,3],dtype=tf.int64)\r\nlist_elems = [elems]\r\ntuple_elems = (elems)\r\n\r\nres = tf.map_fn(lambda x: x+1, tuple_elems, dtype=tf.int64)\r\nwith tf.Session() as sess:\r\n    print(sess.run(res))\r\n \r\nres = tf.map_fn(lambda x: x+1, list_elems, dtype=tf.int64)\r\nwith tf.Session() as sess:\r\n    print(sess.run(res))\r\n```\r\n### Traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-45-e7852b9d4ba6>\", line 13, in <module>\r\n    res = tf.map_fn(lambda x: x+1, list_elems, dtype=tf.int64)\r\n\r\n  File \"C:\\Users\\1\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 389, in map_fn\r\n    swap_memory=swap_memory)\r\n\r\n  File \"C:\\Users\\1\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2775, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n\r\n  File \"C:\\Users\\1\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2604, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n\r\n  File \"C:\\Users\\1\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2554, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n\r\n  File \"C:\\Users\\1\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 379, in compute\r\n    packed_fn_values = fn(packed_values)\r\n\r\n  File \"<ipython-input-45-e7852b9d4ba6>\", line 13, in <lambda>\r\n    res = tf.map_fn(lambda x: x+1, list_elems, dtype=tf.int64)\r\n\r\nTypeError: can only concatenate list (not \"int\") to list\r\n```", "comments": []}, {"number": 12879, "title": "Is it a bug of tf.map_fn?", "body": "Look at the code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nelems = [tf.constant([1,2,3],dtype=tf.int64)]\r\nalternates = tf.map_fn(lambda x: x, elems, dtype=tf.int64)\r\nwith tf.Session() as sess:\r\n    print(sess.run(alternates))\r\n```\r\n\r\nIt will raise a error, but when I use tuple instead of list, it works well.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nelems = (tf.constant([1,2,3],dtype=tf.int64))\r\nalternates = tf.map_fn(lambda x: x, elems, dtype=tf.int64)\r\nwith tf.Session() as sess:\r\n    print(sess.run(alternates))\r\n```\r\nIs it a bug?\r\nIf you want to know more, llo at the discussion on [stackoverflow](https://stackoverflow.com/questions/46096767/how-to-explain-the-result-of-tf-map-fn).\r\n\r\n", "comments": ["@gauss-clb I think to specify a tuple with one element you will need comma like `elems = (tf.constant([1,2,3],dtype=tf.int64),)`?\r\n\r\n", "@yongtang Yes, you are right, but I don't know why it will raise a error. I think it can work according to [api](https://www.tensorflow.org/versions/master/api_docs/python/tf/map_fn).", "The doc said:\r\nfn: The callable to be performed. __It accepts one argument, which will have the same (possibly nested) structure as elems.__ Its output must have the same structure as dtype if one is provided, otherwise it must have the same structure as elems.\r\n\r\n\r\nSo the fix to the first snippet would be:\r\n```python\r\nelems = [tf.constant([1,2,3],dtype=tf.int64)]\r\nalternates = tf.map_fn(lambda x: x[0], elems, dtype=tf.int64)\r\nwith tf.Session() as sess:\r\n    print(sess.run(alternates))  # [1,2,3]\r\n```"]}, {"number": 12878, "title": "Tutorial code in \"Logging and Monitoring Basics with tf.contrib.learn\" with multiple errors", "body": "I've tried to run the setup code in the tutorial, but I met some confusing running errors. I'm new to tensorflow, so I have no idea how to deal with it. Could anybody help me fix this problem? Thanks a lot!\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Data sets\r\nIRIS_TRAINING = os.path.join(os.path.dirname(__file__), \"iris_training.csv\")\r\nIRIS_TEST = os.path.join(os.path.dirname(__file__), \"iris_test.csv\")\r\n\r\ndef main(unused_argv):\r\n    # Load datasets.\r\n    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n        filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)\r\n    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n        filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)\r\n\r\n    # Specify that all features have real-value data\r\n    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\r\n\r\n    # Build 3 layer DNN with 10, 20, 10 units respectively.\r\n    classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\r\n                                                hidden_units=[10, 20, 10],\r\n                                                n_classes=3,\r\n                                                model_dir=\"/tmp/iris_model\")\r\n\r\n    # Fit model.\r\n    classifier.fit(x=training_set.data,\r\n                   y=training_set.target,\r\n                   steps=2000)\r\n\r\n    # Evaluate accuracy.\r\n    accuracy_score = classifier.evaluate(x=test_set.data,\r\n                                         y=test_set.target)[\"accuracy\"]\r\n    print('Accuracy: {0:f}'.format(accuracy_score))\r\n\r\n    # Classify two new flower samples.\r\n    new_samples = np.array(\r\n        [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\r\n    y = list(classifier.predict(new_samples, as_iterable=True))\r\n    print('Predictions: {}'.format(str(y)))\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n```\r\n\r\nHere is what I saw:\r\nWARNING:tensorflow:From .\\logging_monitoring.py:33: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From .\\logging_monitoring.py:33: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:642: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\n2017-09-07 20:00:50.306221: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-07 20:00:50.306424: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-09-07 20:00:50.447012: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/biases not found in checkpoint\r\n2017-09-07 20:00:50.447738: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/weights not found in checkpoint\r\n2017-09-07 20:00:50.448265: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/biases/denlayer_0/biases/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.451033: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/multi_class_head/dnn/learning_rate not found in checkpoint\r\n2017-09-07 20:00:50.451078: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/biases not found in checkpoint\r\n2017-09-07 20:00:50.451715: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_0/weights/enlayer_0/weights/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.452925: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/biases/denlayer_1/biases/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.454670: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/logits/weights/nn/logits/weights/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.455734: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/weights/enlayer_1/weights/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.455777: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_1/weights not found in checkpoint\r\n2017-09-07 20:00:50.460821: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/biases not found in checkpoint\r\n2017-09-07 20:00:50.463001: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/logits/weights not found in checkpoint\r\n2017-09-07 20:00:50.464352: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/biases/denlayer_2/biases/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.464794: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/weights not found in checkpoint\r\n2017-09-07 20:00:50.467010: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/hiddenlayer_2/weights/enlayer_2/weights/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.467803: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/logits/biases/dnn/logits/biases/part_0/Adagrad not found in checkpoint\r\n2017-09-07 20:00:50.472935: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Not found: Key dnn/logits/biases not found in checkpoint\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key dnn/hiddenlayer_0/biases not found in checkpoint\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".\\logging_monitoring.py\", line 47, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \".\\logging_monitoring.py\", line 33, in main\r\n    steps=2000)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 296, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 442, in fit\r\n    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1353, in fit\r\n    monitors=all_monitors)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 296, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 458, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1006, in _train_model\r\n    config=self._session_config\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 365, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 668, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 490, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 842, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 847, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 551, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 425, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\", line 273, in prepare_session\r\n    config=config)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\session_manager.py\", line 205, in _restore_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1560, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key dnn/hiddenlayer_0/biases not found in checkpoint\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nCaused by op 'save/RestoreV2', defined at:\r\n  File \".\\logging_monitoring.py\", line 47, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \".\\logging_monitoring.py\", line 33, in main\r\n    steps=2000)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 296, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 442, in fit\r\n    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1353, in fit\r\n    monitors=all_monitors)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 296, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 458, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\", line 1006, in _train_model\r\n    config=self._session_config\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 365, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 668, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 490, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 842, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 847, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 551, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 416, in create_session\r\n    self._scaffold.finalize()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 209, in finalize\r\n    self._saver.build()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1172, in build\r\n    filename=self._filename)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 684, in build\r\n    restore_sequentially, reshape)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 450, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 407, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 247, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 663, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Key dnn/hiddenlayer_0/biases not found in checkpoint\r\n         [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\r\n\r\nERROR:tensorflow:==================================\r\nObject was never used (type <class 'tensorflow.python.framework.ops.Tensor'>):\r\n<tf.Tensor 'report_uninitialized_variables_1/boolean_mask/Gather:0' shape=(?,) dtype=string>\r\nIf you want to mark it as used call its \"mark_used()\" method.\r\nIt was originally created here:\r\n['File \".\\\\logging_monitoring.py\", line 47, in <module>\\n    tf.app.run()', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\platform\\\\app.py\", line 48, in run\\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))', 'File \".\\\\logging_monitoring.py\", line 33, in main\\n    steps=2000)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\deprecation.py\", line 296, in new_func\\n    return func(*args, **kwargs)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\learn\\\\python\\\\learn\\\\estimators\\\\estimator.py\", line 442, in fit\\n    SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\learn\\\\python\\\\learn\\\\estimators\\\\estimator.py\", line 1353, in fit\\n    monitors=all_monitors)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\deprecation.py\", line 296, in new_func\\n    return func(*args, **kwargs)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\learn\\\\python\\\\learn\\\\estimators\\\\estimator.py\", line 458, in fit\\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\learn\\\\python\\\\learn\\\\estimators\\\\estimator.py\", line 1006, in _train_model\\n    config=self._session_config', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 365, in MonitoredTrainingSession\\n    stop_grace_period_secs=stop_grace_period_secs)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 668, in __init__\\n    stop_grace_period_secs=stop_grace_period_secs)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 490, in __init__\\n    self._sess = _RecoverableSession(self._coordinated_creator)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 842, in __init__\\n    _WrappedSession.__init__(self, self._create_session())', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 847, in _create_session\\n    return self._sess_creator.create_session()', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 551, in create_session\\n    self.tf_sess = self._session_creator.create_session()', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 416, in create_session\\n    self._scaffold.finalize()', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 196, in finalize\\n    default_ready_for_local_init_op)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 258, in get_or_default\\n    op = default_constructor()', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\training\\\\monitored_session.py\", line 193, in default_ready_for_local_init_op\\n    variables.global_variables())', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\tf_should_use.py\", line 175, in wrapped\\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\tf_should_use.py\", line 144, in _add_should_use_warning\\n    wrapped = TFShouldUseWarningWrapper(x)', 'File \"C:\\\\Program Files\\\\Python36\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\tf_should_use.py\", line 101, in __init__\\n    stack = [s.strip() for s in traceback.format_stack()]']", "comments": ["Looks like its trying to restore from some malformed checkpoint in /tmp/iris_model. Can you try either clearing out that directory or using another empty dir?", "@rohan100jain Yes, it really works! Thanks a lot!"]}, {"number": 12877, "title": "Are there any predefined image format, image size etc for training using your tutorial. I found some error for several times, such as : \"No image found in category validation\" & \"division by zero problem\"", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thanks a lot for  your suggestion."]}, {"number": 12876, "title": "Reading float value from tfrecord, return a not same value.", "body": "### System information\r\n- OS Platform and Distribution: Linux Ubuntu 16.04)\r\n- TensorFlow installed from: [binary](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp36-cp36m-linux_x86_64.whl)\r\n- TensorFlow version: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- Python version: Python 3.6.2 :: Continuum Analytics, Inc.\r\n\r\n### Describe the problem\r\nWhen i read feature `label` with `float` type from tfrecord, it return not same value. Example:\r\n- `3.3 -> 3.29999995`\r\n- `3.7 -> 3.70000005`\r\n- `4.9 -> 4.9000001`\r\n\r\nHere, code make `Example`:\r\n```\r\ndef make_example(s0, s1, label):\r\n    ex = tf.train.SequenceExample()\r\n    ex.context.feature[\"len_s0\"].int64_list.value.append(len(s0))\r\n    ex.context.feature[\"len_s1\"].int64_list.value.append(len(s1))\r\n    ex.context.feature[\"label\"].float_list.value.append(label)# HERE: the feature are `float` type\r\n\r\n    for w in s0:\r\n        ex.feature_lists.feature_list[\"s0\"].feature.add().int64_list.value.append(w)\r\n    for w in s1:\r\n        ex.feature_lists.feature_list[\"s1\"].feature.add().int64_list.value.append(w)\r\n\r\n    return ex\r\n```\r\nParsing function:\r\n```\r\ndef _parse_function(example_proto):\r\n    context_features = {\r\n        \"len_s0\": tf.FixedLenFeature((), dtype=tf.int64),\r\n        \"len_s1\": tf.FixedLenFeature((), dtype=tf.int64),\r\n        \"label\": tf.FixedLenFeature((), dtype=tf.float32) # HERE: the feature are `float` type\r\n    }\r\n    sequence_features = {\r\n        \"s0\": tf.FixedLenSequenceFeature((), dtype=tf.int64),\r\n        \"s1\": tf.FixedLenSequenceFeature((), dtype=tf.int64)\r\n    }\r\n\r\n    context_parsed, sequence_parsed = tf.parse_single_sequence_example(\r\n        serialized=example_proto,\r\n        context_features=context_features,\r\n        sequence_features=sequence_features\r\n    )\r\n\r\n    len_s0 = tf.cast(context_parsed['len_s0'], dtype=tf.int8)\r\n    len_s1 = tf.cast(context_parsed['len_s1'], dtype=tf.int8)\r\n    # label = tf.convert_to_tensor(context_parsed['label'], dtype=tf.float32)\r\n    # label = tf.round(context_parsed['label'])\r\n    label = context_parsed['label']\r\n    s0 = tf.cast(sequence_parsed['s0'], dtype=tf.int32)\r\n    s1 = tf.cast(sequence_parsed['s1'], dtype=tf.int32)\r\n\r\n    return {\"len_s0\": len_s0, \"s0\": s0, \"len_s1\": len_s1, \"s1\": s1}, label\r\n```\r\nWhen i call `result = tf.contrib.learn.run_n({\"label\": label}) `, it return:\r\n```\r\n{'label': array([ 3.29999995], dtype=float32)} # right value: 3.3\r\n{'label': array([ 3.70000005], dtype=float32)} # right value: 3.7\r\n{'label': array([ 4.9000001], dtype=float32)} # right value: 4.9\r\n```\r\n\r\nSome one can help me? Plzz told me some ways to fix it.", "comments": ["This is a problem at the ProtocolBuffer layer. (@martinwicke i don't know who to tag about this)\r\n\r\nIf i do the following using your code:\r\n```\r\ndef main(unused_argv):\r\n\texample = make_example([3, 5, 7], [4, 6, 8], 4.3)\r\n\tprint example\r\n\t\r\n\tserialized = example.SerializeToString()\r\n\r\n\treconstituted = tf.train.SequenceExample()\r\n\treconstituted.ParseFromString(serialized)\r\n\tprint reconstituted\r\n\t\r\n\tsomething, label = _parse_function(serialized)\r\n\tresult = tf.contrib.learn.run_n({\"label\": label})\r\n\tprint result\r\n```\r\n\r\nI'll get:\r\n```\r\ncontext {\r\n  feature {\r\n    key: \"label\"\r\n    value {\r\n      float_list {\r\n        value: 4.3\r\n      }\r\n    }\r\n  }\r\n...\r\n```\r\nfor `example`, and\r\n```\r\ncontext {\r\n  feature {\r\n    key: \"label\"\r\n    value {\r\n      float_list {\r\n        value: 4.30000019073\r\n      }\r\n    }\r\n  }\r\n...\r\n```\r\nfor `reconstituted` (and ultimately `[{'label': 4.3000002}]` for `result`)\r\n\r\nI ran this against Python 2.7.10, and TF built from source from the head of master 5 days ago.\r\n", "To me, this looks like a bug in protobuf. I'm wondering:\r\n\r\n1. is this reproducible with a simpler proto, say a message containing only a single float?\r\n2. is this reproducible using a language other than python? I.e., is the bug in the python binding, or deeper?\r\n", "Addressing (1.) first. Yes, this happens with a simple message:\r\n```\r\n(tensorflow_20170902) autogenic:12876 loki$ more simple_float.proto \r\nsyntax = \"proto3\";\r\n\r\noption cc_enable_arenas = true;\r\noption java_outer_classname = \"SimpleFloatProtos\";\r\noption java_multiple_files = true;\r\noption java_package = \"org.tensorflow.ldq.skratch\";\r\n\r\npackage tensorflow;\r\n\r\nmessage SimpleFloat {\r\n        float value = 1;\r\n}\r\n(tensorflow_20170902) autogenic:12876 loki$ ~/arbeit/worg/projects/tensorflow/bazel-out/host/bin/external/protobuf_archive/protoc --python_out=. simple_float.proto \r\n(tensorflow_20170902) autogenic:12876 loki$ python\r\nPython 2.7.10 (default, Feb  7 2017, 00:08:15) \r\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import simple_float_pb2\r\n>>> sf = simple_float_pb2.SimpleFloat()\r\n>>> sf.value = 12.17\r\n>>> print sf\r\nvalue: 12.17\r\n\r\n>>> serialized = sf.SerializeToString()\r\n>>> reconstitution = simple_float_pb2.SimpleFloat()\r\n>>> reconstitution.ParseFromString(serialized)\r\n>>> print reconstitution\r\nvalue: 12.1700000763\r\n\r\n>>> quit()\r\n(tensorflow_20170902) autogenic:12876 loki$ \r\n```\r\nWill futz around for (2.) tonight or this weekend.\r\n", "Just to be replete, i have verified that (1.) occurs in the official pip releases of TF 1.3.0 and 1.2.0", "It's pretty clearly a protobuf issue. The question is why. Allegedly it's\nonly in the C++ backend. Another thing that could be tried.\n\nEither way, a workaround is to avoid Protobuf.\n\nOn Sep 8, 2017 19:32, \"loki der quaeler\" <notifications@github.com> wrote:\n\n> Just to be replete, i have verified that (1.) occurs in the Python\n> releases of TF 1.3.0 and 1.2.0\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12876#issuecomment-328248835>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_S8t9wOrGL0si9dsHEVrznWkJZmTks5sgfi4gaJpZM4PPpyr>\n> .\n>\n", "Agreed on both cases; i'm still ignorant enough of the codebase to know at what points (if any) protobuf is unavoidably used in the TF code and so could not practically be avoided by TF consumers.\r\n\r\nWill still spend some time this weekend in probing and gedankenexperiment WRT (2.)", "Actually, it's all much simpler. Python uses float64 internally (Python \"float\" is really double). Once you cast 4.3 to 4.3f you get 4.30000019073486.\r\n\r\nProof:\r\n```\r\ncat > foo.cpp <<EOF\r\n#include <iostream>\r\n#include <iomanip>\r\nint main(){\r\n  double a = 4.3;\r\n  float b = (float) a;\r\n  double c = (double) b;\r\n  std::cout<<std::setprecision(15)<<a<<\" \"<<b<<\" \"<<c<<std::endl;\r\n}\r\nEOF\r\ng++ foo.cpp && ./a.out\r\n```\r\nyields\r\n```\r\n4.3 4.30000019073486 4.30000019073486\r\n```\r\n\r\nThanks @hawkinsp, @aselle!\r\n\r\nCasting explicitly to `np.float32` and printing that before serialization should yield the same. The confusing part is that default print options will prevent you from seeing anything useful.\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> np.set_printoptions(precision=15)\r\n>>> x = 4.3\r\n>>> y = np.float32(x)\r\n>>> print y\r\n4.3\r\n>>> y \r\n4.3000002\r\n>>> x\r\n4.3\r\n```", "Interesting - thanks. ... but something is still rotten here; this isn't really TF's problem, but it seems like a real problem that will affect Python users of TF and so maybe that at least warrants some note in a read-me somewhere?\r\n\r\nThere's a use case problem which highlights the data representation problem.\r\n\r\nThe use case problem is: i should be able to do math operations on the values in my protobuf object prior to serialization, and have them be numerically equivalent to the result were i to perform those same operations on a deserialized version of that object.\r\n\r\nThe related data representation problem is: once it comes out of deserialization, that true original value isn't easily reclaimable (beyond some truly hacky schlock.)\r\n\r\nIllustrated using the above simple float proto object & r1.3:\r\n\r\n```python\r\n>>> sf = simple_float_pb2.SimpleFloat()\r\n>>> sf.value = 12.17\r\n>>> cubedSF = math.pow(sf.value, 3)\r\n>>> cubedSF\r\n1802.485313\r\n>>> npSF = np.float32(sf.value)\r\n>>> npSF\r\n12.17\r\n>>> print npSF\r\n12.17\r\n>>> cubedNPSF = math.pow(npSF, 3)\r\n>>> cubedNPSF  # now it is revealed that numpy was holding on to precision-schmutz\r\n1802.4853468994372\r\n>>> serialized = sf.SerializeToString()\r\n>>> reconstitution = simple_float_pb2.SimpleFloat()\r\n>>> reconstitution.ParseFromString(serialized)\r\n>>> print reconstitution.value\r\n12.1700000763\r\n>>> reconstitution.value\r\n12.170000076293945\r\n>>> sf.value == reconstitution.value   # non-trivial problem\r\nFalse\r\n>>> np.float32(sf.value) == reconstitution.value  # but i could always write a custom comparator for pre & post serialization objects\r\nTrue\r\n>>> np.equal(sf.value, reconstitution.value)   # maybe using Numpy will solve everything\r\nFalse\r\n>>> cubedRe = math.pow(reconstitution.value, 3)\r\n>>> cubedRe\r\n1802.4853468994372\r\n>>> npRe = np.float32(reconstitution.value)\r\n>>> npRe   # actually unexpected - ?thank you Numpy?\r\n12.17\r\n>>> print npRe\r\n12.17\r\n>>> sf.value == npRe   # i can always dream\r\nFalse\r\n>>> np.equal(sf.value, npRe)  # really dream\r\nFalse\r\n>>> cubedNPRe = math.pow(npRe, 3)\r\n>>> cubedNPRe\r\n1802.4853468994372\r\n>>> import StringIO  # were i intent on reclaiming the original value...\r\n>>> hack = StringIO.StringIO()\r\n>>> print >>hack, npRe\r\n>>> hack.getvalue()\r\n'12.17\\n'\r\n>>> originalValue = float(hack.getvalue())\r\n>>> originalValue\r\n12.17\r\n>>> sf.value == originalValue\r\nTrue\r\n>>> np.equal(sf.value, originalValue)\r\nTrue\r\n>>> math.pow(originalValue, 3)\r\n1802.485313\r\n```\r\nTo get *consistent* behaviour, i should cast sf.value via np.float32() to do operations; then i will have consistent values from operations on pre and post serialization versions of the object - of course, those values would be wrong all along.\r\nTo get *correct* behaviour, i need to do a string parsing hack on my deserialized float prior to operations; then i will be able to have consistent results from operations on pre and post serialization versions of the object, and will have correct logic function results as well.\r\n\r\nAm i cuckoo-banana to think this situation is a valid problem?\r\n", "I just ran into this, too. I wanted to make sure my importer code is correct, so I also implemented an exporter. Surprisingly, the results were not the same as the inputs. For my use case, the loss in precision that is described in this issue actually matters. My examples are drawn from a problem with a fractal result space. So deviating in the problem space by losing some precision actually makes my examples wrong, as @quaeler noted. This means that beyond 32 bit precision, my estimator would be learning garbage. However, I found that the work around of storing the data as strings and casting them back and forth is actually common.\r\n\r\nHere is a blog post from the future (May 2019), encoding floats as bytes:\r\n\r\nhttp://jrmeyer.github.io/machinelearning/2019/05/29/tensorflow-dataset-estimator-api.html\r\n\r\nThe money bit for serialization:\r\n\r\n```python\r\ndef _float64_feature(float64_value):\r\n    float64_bytes = [str(float64_value).encode()]\r\n    bytes_list = tf.train.BytesList(value=float64_bytes)\r\n    bytes_list_feature = tf.train.Feature(bytes_list=bytes_list)\r\n\r\n    return bytes_list_feature\r\n```\r\n\r\n... and for deserialization:\r\n\r\n```python\r\ndef deserialize(serialized_example):\r\n    features = {\r\n        'float_value': tf.FixedLenFeature([], tf.string),\r\n    }\r\n    features = tf.parse_single_example(\r\n        serialized_example,\r\n        features=features\r\n    )\r\n\r\n    features['float_value'] = tf.strings.to_number(string_tensor=features['float_value'], out_type=tf.float64)\r\n\r\n    return features['float_value']\r\n```", "@Bengt Did you ever found a solution around this without casting? I guess the protocol buffer looses all it's advantage of storing floats if we store them as strings :(", "I'm still seeing this issue with a simple serialization of a `tf.Tensor` (shape=(1, 512), dtype=float32) which is stored as a `list of floats` when the `TFRecord` example is created all the values are what is expected, but when reading the data back through the parse, the value of the `tf.Tensor` that is all wrong, all values are rounded up even if we are going from `float32` to `float32` ."]}, {"number": 12875, "title": "python wrapper around libtensorflow", "body": "I wish to use tensorflow as a standalone library. I  have  `libtensorflow.so`. How do I bridge it to python so I can use it for standalone purpose?or is there any github repo for the same", "comments": ["If you have the tensorflow source checked out locally, you could build a pip installer package from that source and install it into a virtualenv as discussed here: https://github.com/tensorflow/tensorflow/issues/12722#issuecomment-326769780"]}, {"number": 12874, "title": "error in Building tensor flow. [sun.security.validator.ValidatorException:]", "body": "== cat /etc/issue ===============================================\r\nLinux ravi 4.4.0-93-generic #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ravi 4.4.0-93-generic #116-Ubuntu SMP Fri Aug 11 21:17:51 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nHello Below is compilation error i am getting.\r\n\r\n$~/bin/bazel build -c opt --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\n...........\r\nERROR: /home/ravi.spatil/LinuxShare/caffe_opecv_resources/mobilenet/tensorflow_ori/tensorflow/tools/pip_package/BUILD:101:1: no such package '@nsync//': Error downloading [https://github.com/google/nsync/archive/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz] to /home/ravi.spatil/.cache/bazel/_bazel_ravi.spatil/39b7c1709071717822a0ecd1200753ba/external/nsync/ad722c76c6e6653f66be2e1f69521b7f7517da55.tar.gz: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\nINFO: Elapsed time: 28.497s\r\nFAILED: Build did NOT complete successfully (106 packages loaded)\r\n\r\nBelow is system information.\r\n--------------------------------------\r\n\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.5)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n../tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n", "comments": ["It looks like the current nsync package has been signed in a manner which Bazel can't verify (is that correct @jart?)", "Hello, i solved it by changing branch r1.3 ->./configure -> build.\r\nMay be issue with latest master code."]}, {"number": 12873, "title": "Efficient way to load a model over 2GiB with C++ interface?", "body": "Hi,all\r\nWhen applying TF model with its C++ interface, freeze_graph operations, for the graph file and the cpk file, are required. This operation will then generate a dumped binary protobuffer file.\r\n\r\nHowever, protobuffer does not support to dump a file that is larger than 2GiB.\r\n\r\nIn this case, what's the right way to load a big model in TF, with its C++ interface? Any clues will be appriciated.\r\n\r\nThank you", "comments": ["The checkpoint should not be limited to 2GB for saving. You don't need to freeze the graph. You can create a saver in the graph, which you can use with `session.Run`.  Something like this:\r\n```\r\n  const tensorflow::Status status = session->Run(\r\n      {{\"save/Const\", \"/path/to/checkpoint\"}}, {} /* no output */,\r\n      {\"save/restore_all\"}, nullptr /* no output */);\r\n```", "@drpngx  I have tried, it works, thank you!", "Closing since issue is resolved."]}, {"number": 12872, "title": "PREP: migrate ErfGrad to c++ side", "body": "see #12686.\r\n\r\n### How was this patch tested?\r\n\r\n+ [x] add unit tests.\r\n+ [x] pass all tests.\r\n", "comments": ["Can one of the admins verify this patch?", "Hi, @kbsriram @suharshs , may you take a look? Thanks.", "I believe `Scope grad_scope = scope.WithControlDependencies(grad); ` is useless here, hence remove it in both `ErfGradient` and `LgmmaGradient`.", "Hi @facaiy - just adding some notes:\r\n1. the control dependencies are only an optimization - in the Erf grad for instance it would ensure the Conjugate operation is never scheduled unless something forces grad_input[0] to be evaluated. That said, at this point perhaps you could also choose to leave this particular optimization as part of an existing TODO that already exists. So more a note to consider if useful later.\r\n2. When you merge/rebase to HEAD, there's a newer version of the gradient_checker that includes support for complex checking - you can move the Erf/lgamma test cases into a TestCWiseUnaryGrad form - hopefully making the test case simpler as well.\r\n3. Notwithstanding, do we actually have complex support for [lgamma](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_lgamma.cc#L19) and [erf](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_erf.cc#L19)? If not, it might still be ok to leave the grad implementations as-is for now; and just enable tests on the real-valued points you've identified.", "Thank you, @kbsriram . I am not sure whether I understand this correctly, could you clarify what I should do later:\r\n\r\n1. Do you mean to add a comment `# TODO xxxxx` after the `grad_scope` dependency is removed?\r\n2. I found there exist `CWiseUnaryGradTest`, `MathGradTest` and `NaryGradTest`. Which one is the best tool for test?\r\n3. Yes, I believe we have complex support for lgamma and erf, so I'm glad to add test cases for complex type.\r\n", "1. There's [currently one](https://github.com/tensorflow/tensorflow/blob/74a31949b164a9bd501886c4b0b5a574ce7fbfba/tensorflow/cc/gradients/math_grad.cc#L50), so perhaps no need to add another one.\r\n2. [CWiseUnaryGradTest](https://github.com/tensorflow/tensorflow/blob/74a31949b164a9bd501886c4b0b5a574ce7fbfba/tensorflow/cc/gradients/math_grad_test.cc#L34) would be my suggestion here, as it is a unary operator, and convenient to test using a simple list of input test values. ", "Hi, test cases has been modified, might you take a test? Thanks.", "Jenkins, test this please.\n", "Thanks for your help, @kbsriram .\r\n\r\nTest cases has been revised, and both passed. By the way, since `complex` kernel is not implemented yet, hence I leave a todo comment.\r\n\r\n```bash\r\nINFO: Elapsed time: 35.033s, Critical Path: 28.07s\r\n//tensorflow/cc:gradients_math_grad_test                                 PASSED in 2.5s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```", "Looks fine to me, pending any remaining comments KB may have. Thanks for the changes!", "Thanks for the changes, also lgtm!", "Jenkins, test this please.", "Jenkins, test this please.", "unrelated failure.\r\n\r\n```\r\nrm: cannot remove '/workspace/tensorflow/contrib/makefile/gen/proto_text/tensorflow/core/protobuf/rewriter_config.pb_text-impl.h': Permission denied\r\nmake: *** [clean] Error 1\r\nBuild step 'Execute shell' marked build as failure\r\n[Set GitHub commit status (universal)] ERROR on repos [] (sha:8d6c928) with context:tensorflow-pull-requests-makefile\r\nUnable to get pull request builder trigger!!\r\nSetting status of d9d567785a3e97edbb5163a505a7b4bcd67b3597 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/10840/ and message: 'FAILURE\r\n```", "Jenkins, test this please.", "Hi, @kbsriram @suharshs . The failure is beyond my scope, could you help me? Thanks.\r\n\r\n```\r\n14:22:04   transform_utils.cc\r\n14:22:04 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\tools\\graph_transforms\\sparsify_gather.cc) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_tools_transform_graph_lib.vcxproj]\r\n14:22:04   C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\tools\\graph_transforms\\sparsify_gather.cc)\r\n14:22:15   tf_tools_transform_graph_lib.vcxproj -> C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_tools_transform_graph_lib.dir\\Release\\tf_tools_transform_graph_lib.lib\r\n14:22:15 \r\n14:22:15 c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build>if 1 NEQ 0 exit /b 1 \r\n14:22:15 \r\n14:22:15 c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build>exit 1 \r\n14:22:15 Build step 'Execute Windows batch command' marked build as failure\r\n14:22:16 [Set GitHub commit status (universal)] ERROR on repos [] (sha:99b665c) with context:tensorflow-pr-win-cmake-py\r\n14:22:17 Unable to get pull request builder trigger!!\r\n```", "That is a flake, re-running tests now Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Same failure on Windows Cmake Tests. \r\n\r\n```\r\n17:39:53   transform_utils.cc\r\n17:39:53 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow/c/c_api.h(999): warning C4190: 'TF_NewWhile' has C-linkage specified, but returns UDT 'TF_WhileParams' which is incompatible with C (compiling source file C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\tools\\graph_transforms\\sparsify_gather.cc) [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_tools_transform_graph_lib.vcxproj]\r\n17:39:53   C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow/c/c_api.h(955): note: see declaration of 'TF_WhileParams' (compiling source file C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\tools\\graph_transforms\\sparsify_gather.cc)\r\n17:40:04   tf_tools_transform_graph_lib.vcxproj -> C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_tools_transform_graph_lib.dir\\Release\\tf_tools_transform_graph_lib.lib\r\n17:40:04 \r\n17:40:04 c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build>if 1 NEQ 0 exit /b 1 \r\n17:40:04 \r\n17:40:04 c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build>exit 1 \r\n17:40:04 Build step 'Execute Windows batch command' marked build as failure\r\n```\r\n\r\nDoes anyone know how to deal with it?", "@facaiy here is the actual error:\r\n\r\n```17:25:05 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(703): error C2065: 'M_PI': undeclared identifier [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_cc.vcxproj]\r\n17:25:05 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(703): error C2660: 'tensorflow::ops::Const': function does not take 1 arguments [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_cc.vcxproj]\r\n17:25:05 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(704): error C2440: '<function-style-cast>': cannot convert from 'initializer list' to 'tensorflow::ops::Cast' [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_cc.vcxproj]\r\n17:25:05   C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(704): note: No constructor could take the source type, or constructor overload resolution was ambiguous\r\n17:25:05 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(709): error C3536: 'two_over_root_pi': cannot be used before it is initialized [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_cc.vcxproj]```", "Thank you very much, @frankchn , Really helpful!\r\nIt seems that `M_PI` is not supported on Windows, right? That's what we discussed above. Hi, @suharshs , do you have any idea why `M_PI` is missing? Thanks.\r\n\r\n```\r\n17:25:05 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(703): error C2065: 'M_PI': undeclared identifier [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_cc.vcxproj]\r\n17:25:05 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(703): error C2660: 'tensorflow::ops::Const': function does not take 1 arguments [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_cc.vcxproj]\r\n17:25:05 C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\cc\\gradients\\math_grad.cc(704): error C2440: '<function-style-cast>': cannot convert from 'initializer list' to 'tensorflow::ops::Cast' [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\tf_cc.vcxproj]\r\n```\r\n\r\n", "`cmath` is imported explicitly by referred to [Math Constants - Visual C++](https://docs.microsoft.com/en-us/cpp/c-runtime-library/math-constants). Since I'm on holiday, hence the change is not tested by myself. Could anyone help retest the PR to see whether it works? Thanks.", "Jenkins, test this please.", "Can one of the admins verify this patch?"]}, {"number": 12871, "title": "About Deterministic Behaviour of GPU implementation of tensorflow", "body": "- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian\r\n- **TensorFlow version (use command below)**:('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GeForce GTX 950\r\n\r\n\r\nDear experts,\r\n\r\nI have just written a python code that implements a CNN with tensorflow. Despite using tf.set_random_seed(0) throughout my code, I get different results for each different run using a GPU, while this is not happening when switching to CPU. I read different threads about stochastic behaviour of GPUs and I could find that the stochastic behaviour is happening when using AdamOptimizer. My code now uses a CPU just for the optimizer, and a GPU for all the others operations, so now the time performance of the GPU is 25% better than CPU despite the 700% of the full GPU implementation. My question is: is this avoidable? Should I really give up in having a deterministic code when using GPUs? How can I tune my hyperparameters and use a full GPU code (some forums say to make an average of different runs.. but this kills the time advantage of using GPUs).\r\n\r\nThanks in advance for all your kind support and for the wonderful job\r\n", "comments": ["There was some discussion previously #10636. Does that help?", "Hello, thank you for your reply. yes, indeed the document is useful, but I see that in the example there's no a minimization of the loss through the metod tf.minimize. Indeed I fount that the stochastic component of my code is in optimizer = tf.train.AdamOptimizer(rate).minimize(loss). Now, my question is, is there a way to use an optimizer and to get a deterministic result at the same time?\r\n\r\nThank you very much again\r\n\r\n", "@ekelsen do you know when determinism will be available?", "Reductions are already deterministic as well as l2loss.  softmax will be soon,  Cross entropy following.", "The gradient calculation of convolutions is not likely to be deterministic at anywhere close to the same performance unless NVIDIA decides to change the implementation of cuDNN.", "@ekelsen, as far as I understand, Theano also uses cuDNN, but they still have settings (in .theanorc):\r\n```\r\n[dnn.conv]\r\n\r\nalgo_bwd_data=deterministic\r\nalgo_bwd_filter=deterministic\r\n```\r\nthat [work with convolutions in a slower but deterministic way](http://deeplearning.net/software/theano/library/sandbox/cuda/dnn.html). \r\n\r\n", "That's entirely correct and what I was referring to.", "Closing since it sounds like @ekelsen solved the problem.", "@drpngx, the original issue with non-deterministic behaviour of TensorFlow still persists.\r\n\r\nThe root cause is in cuDNN, not Tensorflow, but it is still should be possible to make the behavior deterministic in the same way like Theano developers did with\r\n\r\n```\r\n[dnn.conv]\r\n\r\nalgo_bwd_data=deterministic\r\nalgo_bwd_filter=deterministic\r\n```\r\n\r\n@ekelsen said:\r\n\r\n>The gradient calculation of convolutions is not likely to be deterministic at anywhere close to the same performance unless NVIDIA decides to change the implementation of cuDNN.\r\n\r\nBut in fact the performance *is* almost the same.\r\n\r\nAccording to my own experiments on Geforce gtx 1050 the performance is almost the same for Tensorflow using cuDNN vs. Theano using cuDNN with non-deterministic config vs. the same with deteministic config (about 4s/epoch all all three cases).  \r\n\r\nOther people also report relatively the same performance with Theano/cuDNN and with or without `algo_bwd_data` and `algo_bwd_filter` configuration:\r\n\r\n* https://github.com/keras-team/keras/issues/2479#issuecomment-213987747\r\n* https://www.kaggle.com/c/ultrasound-nerve-segmentation/discussion/22014#125961\r\n\r\nSo I believe it still makes sense to reopen the issue and implement something like that in TensorFlow.\r\n", "/CC @protoget ", "You can try set TF_CUDNN_USE_AUTOTUNE = '0' and try again. Though prepare to experience degenerated performance. Determinism + High performance isn't what we'd support.\r\n\r\n/cc @yzhwang ", "TF_CUDNN_USE_AUTOTUNE='0' will not work, it will use cuDNN function to retrieve the best algorithm. One solution I found was to support a new environment variable TF_CUDNN_CONVOLUTION_BWD_FILTER_ALGO_DETERMINISTIC='1' such that we only use cuDNN deterministic algorithms.\r\n\r\nI did not measure the performance of this solution but I'm getting deterministic results for gradient computation of convolutions, something that I was not getting previously\r\n\r\nThe code I used to test the determinism of the backward filter of the convolution is the following:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(0)\r\nnp.set_printoptions(precision=4)\r\n\r\ndef run_convolution_test(device, N=1):\r\n  tf.reset_default_graph()\r\n  tf.set_random_seed(42)\r\n  with tf.device(device):\r\n    input = tf.random_normal([1, 2048, 2048, 1], name='input')\r\n    kernel = tf.get_variable('kernel', shape = [3,3,1,1])\r\n    conv = tf.nn.conv2d(name='conv', input=input,\r\n                        filter=kernel, strides=[1,1,1,1], padding='SAME',\r\n                        use_cudnn_on_gpu=True)\r\n    grad = tf.gradients(conv, kernel)\r\n  grad_val = None\r\n  a_val = None\r\n  config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n  with tf.Session(config=config) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    conv_val, grad_val = sess.run([tf.squeeze(conv), tf.squeeze(grad)])\r\n    return conv_val, grad_val\r\n\r\ndef compute_error_matrix(matrices):\r\n    total_absolute_error = np.zeros((len(matrices), len(matrices)))\r\n    for i in range(len(matrices)):\r\n        for j in range(len(matrices)):\r\n            if i != j:\r\n                dif=matrices[i] - matrices[j]\r\n                total_absolute_error[i][j] = np.sum(np.absolute(dif))\r\n    print(total_absolute_error)\r\n\r\nprint(\"Running on GPU\")\r\nconv_gpu, grad_gpu = zip(*[run_convolution_test('/device:GPU:0', i) for i in range(10)])\r\ncompute_error_matrix(conv_gpu)\r\ncompute_error_matrix(grad_gpu)\r\n\r\nprint(\"Running on CPU\")\r\nconv_cpu, grad_cpu = zip(*[run_convolution_test('/device:CPU:0', i) for i in range(10,20)])\r\ncompute_error_matrix(conv_cpu)\r\ncompute_error_matrix(grad_cpu)\r\n```\r\n\r\n", "+1 to @lucb - the optional enabling of deterministic cudNN algo's would be very useful. Even if a performance trade-off exists for determinism, users should have the ability to enable it.", "But it seems what the real problem is Adam optimizer is not deterministic. Is there any possible flag to make it deterministic for Adam?\r\n\r\nThanks so much.", "I second this. My computation graph does not have any convolution operation but it still suffers from non-determinism. By sending the compute_gradients operation of AdamOptimizer to the CPU, the problem disappears. I'm running tf 1.8. ", "@chithangduong What kind of differences do you see? Could they be due to multi-threading?", "@drpngx I have figured it out. The non-deterministic part is the [clip_by_value ](https://www.tensorflow.org/api_docs/python/tf/clip_by_value) operation. By sending it to CPU and setting `colocate_gradients_with_ops `for the AdamOptimizer, the results between two different runs are the same. Do you have any idea why this is the case? ", "Could you clarify which one you're using? Unary, binary (left or right) or ternary? Thanks.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "> TF_CUDNN_USE_AUTOTUNE='0' will not work, it will use cuDNN function to retrieve the best algorithm. One solution I found was to support a new environment variable TF_CUDNN_CONVOLUTION_BWD_FILTER_ALGO_DETERMINISTIC='1' such that we only use cuDNN deterministic algorithms.\r\n> \r\nUnfortunately I cannot reproduce the deterministic computation you're claiming to achieve :(\r\nFor some reason, this FLAG is not supported (yet?).\r\n\r\nI am using Python `3.6` and TensorFlow `1.11` compiled from source on `CUDA 10.0` + `CuDnn 7.3`.\r\n\r\nAny clue? ", "Is there a solution to this?", "@chithangduong How are you able to send just `clip_by_value` to the CPU and keep everything else the same? I'm using Keras to do my training. Is there a way to still do it or do I need to switch into tf?", "in march 2019 the lack of deterministic behaviour of the adam optimizer is still a problem !\r\n I don't want to force the optimizer to run on CPU (i don't want to wait for the rest of my life) ....\r\n\r\nIs there any new proposal ?\r\nOr is the \"clip_by_value\" on cpu a real answer ? if so, can someone detail it a little because i didn't catch this.", "> We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!\r\n\r\nyes still an issue for me at least :+1: ", "Still an issue! ", "I'm an educator trying to teach students Tensorflow. This is a significant issue for me as the non-deterministic nature of Tensorflow on GPU makes it extremely difficult to automate the checking of students' projects. +1 @OutSorcerer, having a setting like in Theano to allow for deterministic behavior would be much appreciated.", "Hello,\r\n\r\nThe non determinstic behaviour of cudnn is caused by the non deterministic algorithms that are being used. Those are sometimes faster - so the trade-off is between mathematical accuracy and training time. Those algorithms compute sometimes gradients that are leading to better model accuracy and sometimes they don't.\r\n\r\nI agree - it would have been very nice if we had a feature to choose the algorithms we would like to use. Adding an argument for specifying convolution algorithms to the signature of the Conv2D function and passing that argument to the c++ layer and then to the cuda_dnn.cc should not be that much of an effort.\r\n\r\nHere is a workaround that works for me:\r\n1. Download tensorflow source code\r\n2. Find and go to the c++ class **cuda_dnn.cc**\r\n3. Go to the function **ToConvBackwardFilterAlgo**\r\n4. Make it return **cudnnConvolutionBwdFilterAlgo_t::CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1;** or any other of the following algorithms that is deterministic and implemented:\r\n    **CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0**  (non-deterministic)\r\n    **CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1**\r\n    **CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT**\r\n    **CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3** (non-deterministic)\r\n    **CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD** (not implemented)\r\n    **CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED**\r\n    **CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING**\r\n\r\n(Tensorflow is using mainly ALGO_3 because this is almost always estimated by cudnn as the fastest one)\r\n\r\n5. Build and compile the source code with bazel and install with pip.\r\n\r\nDon't forget that this is only the computation of the convolution gradients. In order to have an absolute determinstic behaviour all other parts of the model such as Loss, Optimizer and other Layers being used should be deterministic too.\r\n\r\nGreetings.", "Check out the tf-deterministic package. From tensorflow 1.14 onwards it should mitigate quite a lot of GPU related non determinism.", "Yes - this looks like it could be the solution for some people, although it didn't work for me:\r\n https://github.com/NVIDIA/framework-determinism\r\n\r\n"]}, {"number": 12870, "title": "init weights from pre_train model resnet_v2_101 something wrong :    Assign requires shapes of both tensors to match. lhs shape= [19] rhs shape= [256]", "body": " **SSD generate feature map by VGG16, I replaced VGG16 with ResNet_V2_101,the code as follows:**\r\n`  with slim.arg_scope(resnet_utils.resnet_arg_scope()):\r\n         net , end_points =resnet_v2.resnet_v2_101(inputs,reuse=reuse,global_pool=False,is_training=True,scope='resnet_v2_101')\r\n`\r\n           \r\n**when i restore the pretrain_weights from the checkpoint_path, something wrong as follows:**\r\n`InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [19] rhs shape= [256]\r\n\t [[Node: save_1/Assign_294 = Assign[T=DT_FLOAT, _class=[\"loc:@resnet_v2_101/block3/unit_18/bottleneck_v2/conv2/BatchNorm/gamma\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](resnet_v2_101/block3/unit_18/bottleneck_v2/conv2/BatchNorm/gamma, save_1/RestoreV2_294)]]\r\n`\r\n**## I do not konw why, Hope for your help!**", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12869, "title": "Update documentation", "body": "Made explicit HTTPS calls", "comments": ["Can one of the admins verify this patch?", "Thanks @alanyee!\r\n", "@tensorflow-jenkins test this please"]}, {"number": 12868, "title": "Fix minor typo in Programmer's Guide docs", "body": "Change `my_row_vetor` to `my_row_vector` in the Tensors guide.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12867, "title": "Branch 167812735", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 12866, "title": "R1.3", "body": "update", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "commitment", "Looks like this PR is unintentional. Closing."]}, {"number": 12865, "title": "Fix issue in batch_norm", "body": "Fix issue in batch_norm where center=False, data_format='NCHW' and zero_debias_moving_mean=True, and add a test case for it.\r\n\r\nThis fix closes #11673.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 12864, "title": "No gradient defined for operation '..' (op type: QueueDequeueUpToV2)", "body": "When I tried to compute the gradient of the loss with 'grads = opt.compute_gradients(total_loss, update_gradient_vars)', there was an error said 'No gradient defined for operation '..' (op type: QueueDequeueUpToV2)' ", "comments": ["We can't backpropagate through queues; @vrv suggests that you probably have a queue between loss and update. \r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12863, "title": "[BUG] unexpect results when run init with assign", "body": "```python\r\nimport tensorflow as tf\r\nsess = tf.InteractiveSession()\r\na =tf.Variable([1])\r\nb = tf.assign_add(a,[1])\r\ninit = tf.global_variables_initializer()\r\ninit_a=tf.variables_initializer([a])\r\nsess.run([init])\r\nfor i in range(10):\r\n    print(sess.run([a,b,init_a]))\r\n\r\n# GET\r\n[array([1], dtype=int32), array([1], dtype=int32), None]\r\n[array([1], dtype=int32), array([1], dtype=int32), None]\r\n[array([1], dtype=int32), array([1], dtype=int32), None]\r\n[array([1], dtype=int32), array([1], dtype=int32), None]\r\n[array([2], dtype=int32), array([2], dtype=int32), None]\r\n[array([1], dtype=int32), array([1], dtype=int32), None]\r\n[array([2], dtype=int32), array([2], dtype=int32), None]\r\n[array([2], dtype=int32), array([2], dtype=int32), None]\r\n[array([1], dtype=int32), array([1], dtype=int32), None]\r\n[array([2], dtype=int32), array([2], dtype=int32), None]\r\n\r\n# this make the metrics and stream metrics hard to use \r\n```\r\n\r\n", "comments": ["Looks like the init is sometimes performed first and sometimes the assign.\r\n\r\nI don't know if this is a bug, but one could argue that you are expected to init all variables with a separate sess.run call before performing any operations. \r\nOn the other hand could sess.run always perform init operations first. I don't know if it would be hard to check for this", "This is not a bug. Similar problem was discussed in #10860"]}, {"number": 12862, "title": "Branch 167800256", "body": "", "comments": []}, {"number": 12861, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Looks like this PR was created unintentionally. Closing for now."]}, {"number": 12860, "title": "cuda/cuda_config.h missing when compiling custom ops with nvcc", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, see below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nv1.3.0-0-g9e76bf324 1.3.0\r\n- **Python version**: \r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n- **CUDA/cuDNN version**:\r\n8.0.44 / 5.1.5\r\n- **GPU model and memory**:\r\nAny.\r\n- **Exact command to reproduce**:\r\nSee below.\r\n\r\n### Describe the problem\r\n\r\nWhen compiling a custom op using nvcc, which includes `tensorflow/core/util/cuda_kernel_helper.h`, I get the following error:\r\n\r\n```\r\n/usr/local/cuda-8.0/bin/nvcc -c -o ~/Code/libspn/build/ops/gather_columns_functor_gpu.cu.cc.o ~/Code/libspn/libspn/ops/gather_columns_functor_gpu.cu.cc -std=c++11 -x=cu -Xcompiler -fPIC -DGOOGLE_CUDA=1 --expt-relaxed-constexpr -I ~/.local/lib/python3.5/site-packages/tensorflow/include -gencode=arch=compute_35,\"code=sm_35,compute_35\" -gencode=arch=compute_52,\"code=sm_52,compute_52\" -gencode=arch=compute_61,\"code=sm_61,compute_61\"\r\nIn file included from ~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/platform/default/stream_executor.h:26:0,\r\n                 from ~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/platform/stream_executor.h:24,\r\n                 from ~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:26,\r\n                 from ~/Code/libspn/libspn/ops/gather_columns_functor_gpu.cu.h:11,\r\n                 from ~/Code/libspn/libspn/ops/gather_columns_functor_gpu.cu.cc:5:\r\n~/.local/lib/python3.5/site-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory\r\ncompilation terminated.\r\n```\r\n\r\nCopying `cuda_config.h` to `/site-packages/tensorflow/include/tensorflow/stream_executor/cuda` solves the problem.\r\n\r\nThe same issue has been observed by several other users in #6602 (see the comments added after the issue was closed).\r\n\r\n", "comments": ["the same problem, I want to know if there is any better way to solve this problem", "@allenlavoie, have you looked at this as part of your library cleanup?", "I haven't. I may have run into it when I was following our custom op documentation. It sounds like worst case we could add a copy in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/build_pip_package.sh ? Or maybe a genrule before that. ", "I have the same problem while compiling with nvcc. Also is because `tensorflow/core/util/cuda_kernel_helper.h` was included.\r\n\r\nBut when I copy the file, i get the following error:\r\n\r\n```\r\n../site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:28:80: fatal error: cuda/include/cuda.h: No such file or directory\r\ncompilation terminated.\r\n```", "The commit 2c598e874e6a7b6b3185846ce9bac97a7d5d0169 destroys my makefile, as well. This commit consistly changes in several places the includes\r\n\r\n```diff\r\n-#include \"third_party/gpus/cuda/include/cuda.h\"\r\n+#include \"cuda/include/cuda.h\"\r\n```\r\n\r\nWhich gives me the error message:\r\n\r\n```\r\n../site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:28:80: fatal error: cuda/include/cuda.h: No such file or directory\r\ncompilation terminated.\r\n```\r\n\r\ncompiling user-ops with GPU does not work since 1.3.1.\r\n\r\nWhere lives that guy, tensorflower-gardener? He is the one who usually introduces those breaking changes!", "@PatWie +1\r\nI encounter this problem when build android tensorflow_demo, giving the following error message:\r\n```bash\r\ntensorflow/core/kernels/lrn_op.cc:34:10: fatal error: 'cuda/include/cuda.h' file not found\r\n```", "With TF 1.3, I only find `cuda_config.h` missing, but when compiling against current master 1.4-rc0/1, I also additionally get:\r\n```\r\ntensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:24:31: fatal error: cuda/include/cuda.h: No such file or directory\r\ncompilation terminated.\r\n```", "I also encounter the same problem in TF 1.3.0.  This error is introduced with  `#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"`\r\n`/usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory`", "I also encounter this problem in TF 1.4.0, any clue ?", "Also stucking on this problem in TF 1.4.0...", "@CR-Ko Downgrading to 1.2.0 \"solved\" the issue for me", "I have a similar issue, resulting in:\r\n```.tensorflow-venv/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:24:31: fatal error: cuda/include/cuda.h: No such file or directory```\r\nAny other workarounds for this besides downgrading?\r\n", "Downgrading \"solution\" will cause incompatibility with cudnn. You also need to downgrade cudnn.@nikste\r\nCould installing from source solve this problem?@pronobis @PatWie", "No change here. I currently stick to TF1.2 compiled from source (I always compile the library from source). It is frustrating. @allenlavoie or @aselle can you give us at least a hint?\r\n\r\nedit: I tracked it down to commit  2c598e874e6a7b6b3185846ce9bac97a7d5d0169.\r\n\r\nThis is a [MWE for v1.2](https://github.com/cgtuebingen/tf_custom_op/tree/master/matrix_add) for reproducing the error. It works under TF1.2 but not TF1.4.", "@Queequeg92 installing from source did not fix the problem, ended up manually adding paths to .h files as additional include directories and copying `cuda_config.h` from source to my project.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Hello. I also have the problem with T.F 1.4 although it worked in T.F 1.2. Have anyone fix it with T.F 1.4", "Folding all the duplicates into #15002", "@Queequeg92 I have it working under v1.5rc0 now, see https://github.com/cgtuebingen/tf_custom_op which requires installing from source or copying the `cuda_config.h` from #15002.", "@PatWie Thank you! I'll have a try.", "@PatWie I have installed v1.5 from source successfully. No cuda_config.h error, but still cuda.h error. :tired_face:", "I solved this problem by commenting the line *** #include \"cuda/cuda_config.h\" *** in the \"dso_loader.h\" file. And the custom op works normally.", "@Queequeg92\r\nThe cuda.h is part of the cuda toolkit or whatever Nvidia ships. It is not part of TensorFlow, that is what the lines\r\n\r\n```cmake\r\n# use cuda\r\nfind_package(CUDA 9.0 EXACT REQUIRED)\r\nset(CUDA_SAMPLE_INC \"${CUDA_INCLUDE_DIRS}/../samples/common/inc\")\r\nmessage(STATUS \"CUDA_INCLUDE_DIRS: ${CUDA_INCLUDE_DIRS}\")\r\ninclude_directories(SYSTEM \"${CUDA_INCLUDE_DIRS}/../../\")\r\n```\r\n\r\nare for. But the latest version of TF is broken again (https://github.com/tensorflow/tensorflow/issues/15002#issuecomment-362663624)\r\n\r\n@xysmlx\r\nWhen writing cuda ops, you probably want to use the TensorFlow cuda_config part."]}, {"number": 12859, "title": "FailedPreconditionError when restoring initializable_iterator with Scaffold in a MonitoredTrainingSession for the second time.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: Python 3.5.1\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: - \r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n\r\n### Context:\r\nUsing `initializable_iterator` with `MonitoredTrainingSession` because there are stateful `lookup_ops.index_table_from_tensor()` lookup tables that don't work with `one_shot_iterator`.\r\n\r\n`initializable_iterator` is initialized with a tf.train.Scaffold():\r\n```\r\nScaffold = tf.train.Scaffold(\r\n        init_op=control_flow_ops.group(variables.global_variables_initializer(),\r\n                                       resources.initialize_resources(resources.shared_resources()),\r\n                                       iter_init_op))\r\n\r\nwith tf.train.MonitoredTrainingSession(\r\n    master=server.target,\r\n    is_chief=hps.is_chief,\r\n    scaffold=Scaffold,\r\n    config=config,\r\n    checkpoint_dir=hps.checkpoint_dir,\r\n    hooks=hooks\r\n) as mon_sess:\r\n                ...\r\n```\r\nWhere `iter_init_op` is equivalent to `iterator.initializer`.\r\n\r\n### Problem\r\nUpper mentioned initialization works properly when the model is initialized and created for the **first** time and some initial training can be done without problems.\r\n\r\nIf chief worker crashes or is shut down purposefully, after **restarting** `MonitoredTrainingSession` shows following error as if iterator is not initialized:\r\n```\r\nFailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element...\r\n```\r\n\r\n### Workaround\r\nRight now the only solution that works for is to run initialization internally using `_coordinated_creator.tf_sess.run`:\r\n```\r\nmon_sess._coordinated_creator.tf_sess.run(iter_init_op)\r\n```\r\nThis doesn't look like an intended use.\r\n\r\n### Statement:\r\nThis doesn't seem as an intended behaviour.\r\nWhat is a better way to use `initializable_iterator` with `MonitoredTrainingSession` or `lookup_ops.index_table_from_tensor` with `one_shot_iterator`?", "comments": ["This happens because the `init_op` is not run when the worker restarts from a checkpoint. The relevant implementation is [in `SessionManager.prepare_session()`](https://github.com/tensorflow/tensorflow/blob/4a24db28868e3ec9c9bc7408b9f72862116d6d39/tensorflow/python/training/session_manager.py#L274).\r\n\r\nI think for the purposes of the `MonitoredSession`, an initializable iterator is more like a \"local variable\" (which are reinitialized on each worker when they start) than a \"global variable\" (which are initialized once by the chief, and then restored from checkpoints). Could you try moving the initializer to the `Scaffold.local_init_op` and see if that fixes things?\r\n\r\n(This is clearly \"not great\". We're still figuring out a more elegant way to integrate Datasets with the `MonitoredSession` and `Estimator` APIs. Hopefully this suggestion works in the meantime.)\r\n\r\n/cc @ispirmustafa for `MonitoredSession` wisdom.", "`Scaffold.local_init_op` works as intended, thanks for the suggestion.\r\nHere is a working example for future reference:\r\n```\r\nScaffold = tf.train.Scaffold(\r\n    local_init_op=control_flow_ops.group(variables.local_variables_initializer(),\r\n                                         lookup_ops.tables_initializer(),\r\n                                         iter_init_op)\r\n    )\r\n```\r\nDatasets is a pleasant API, thanks for the effort.", "Thanks for confirming that that works! We're definitely still looking for a way to make Datasets work more naturally with `MonitoredSession`, though :).", "@mrry Similarly, what would be the best workaround to make feedable iterators work along with MonitoredTrainingSession? \r\nI am looking for a way to run training and validation dataset simultaneously and also get the convenience of MonitoredTrainingSession. Your suggestion would be helpful.\r\n\r\n[Equivalent stackoverflow post](https://stackoverflow.com/questions/46111072/how-to-use-feedable-iterator-from-tensorflow-dataset-api-along-with-monitoredtra)", "For feedable iterators, writing a SessionRunHook may work. ", "@ispirmustafa \r\nfeedable iterators require the handle to be passed as placeholders which should be run through the same session as training.\r\n\r\nAccording the [documentation](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook#begin), SessionRunHook has \r\n* a `begin()` method which is invoked before creating a session.\r\n* a `after_session_create(session, coord)` method - by this time the graph is already finalized.\r\n\r\nTo create the string handles, a session is required without graph being finalized. Am I missing something here!? How to overcome this issue?", "handle is an output of session.run. it is not changing the graph. it is executing it. So i don't expect there will be an issue there. @mrry to verify?\r\nHaving said that, we're working on a better solution. ", "Right, the handles could potentially be collected in the `after_session_create()` method. Annoyingly, you'll need to call `iterator.string_handle()` on each of the iterators before creating the `MonitoredSession`, and pass the resulting string-valued `tf.Tensor` objects to the hook.", "@mrry Thanks, It works without any hooks \ud83d\udc4d. So the solution is to invoke the `iterator.string_handle()` before creating the `MonitoredSession`.\r\n\r\n```\r\n# Iterator Handle and MonitoredTrainingSession\r\n\r\ndataset_train = Dataset.range(10)\r\ndataset_val = Dataset.range(90, 100)\r\n\r\niter_train_handle = dataset_train.make_one_shot_iterator().string_handle()\r\niter_val_handle = dataset_val.make_one_shot_iterator().string_handle()\r\n\r\nhandle = tf.placeholder(tf.string, shape=[])\r\niterator = Iterator.from_string_handle(\r\n    handle, dataset_train.output_types, dataset_train.output_shapes)\r\nnext_batch = iterator.get_next()\r\n\r\nwith tf.train.MonitoredTrainingSession() as sess:\r\n    handle_train, handle_val = sess.run([iter_train_handle, iter_val_handle])\r\n    \r\n    for step in range(10):\r\n        print('train', sess.run(next_batch, feed_dict={handle: handle_train}))\r\n        \r\n        if step % 3 == 0:\r\n            print('val', sess.run(next_batch, feed_dict={handle: handle_val}))\r\n\r\nOutput:\r\n('train', 0)\r\n('val', 90)\r\n('train', 1)\r\n('train', 2)\r\n('val', 91)\r\n('train', 3)\r\n```\r\n", "I'm having issues with the one_shot_iterator from a Dataset when using in a monitoredTrainingSession on Google cloudML. Issue is that the dataset is iterated through multiple times when running on multiple workers. Also, tensorboard complains:\r\n\r\n> WARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\r\n\r\nI'm guessing a similar registration method as provided by @gmichaeljaison might solve this issue for single_shot_iterator as well. However, Iterator has no string_handle method in tf 1.2 (the latest and greatest Google cloud ML version).\r\n\r\nAny hints how to make @gmichaeljaison solution work on tf 1.2?", "see https://stackoverflow.com/questions/45945881/tf-train-monitoredtrainingsession-and-reinitializable-iterator-from-dataset", "@gmichaeljaison \r\nI tried the proposed solution for MonitoredTrainingSession and it did not work. I got an error message asking me to pass a feed_dict for the handle placeholder when trying\r\n`handle_train, handle_val = sess.run([iter_train_handle, iter_val_handle])`\r\nI tried passing  it an empty string with \r\n`feed_dict={handle:''}`\r\nand that got me the following error:\r\n\r\n> InvalidArgumentError (see above for traceback): Trying to access resource located in device  from device /job:localhost/replica:0/task:0/cpu:0\r\n         [[Node: IteratorFromStringHandle = IteratorFromStringHandle[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_Placeholder_0_1)]]\r\n\r\nThe only thing that worked (from above, no idea why...) was to replace the above line with:\r\n`mon_sess._coordinated_creator.tf_sess.run([iter_train_handle, iter_val_handle])`\r\n", "the Scaffold code is working, but in further step I want add feed_dict(like feed tfrecord file names) to iterator initializer, that will not work, I only see `init_feed_dict`, no `local_init_feed_dict` in Scaffold.", "@dongjk you can use a hook to do all you need. Following is a code example.\r\n```\r\ninitializer_hook = DatasetInitializerHook(dataset.make_initializable_iterator())\r\nwith MonitoredTrainingSession(hooks=[], ...\r\n\r\nclass _DatasetInitializerHook(tf.train.SessionRunHook):\r\n  def __init__(self, iterator):\r\n    self._iterator = iterator\r\n  def begin(self):\r\n    self._initializer = self._iterator.initializer\r\n  def after_create_session(self, session, coord):\r\n    del coord\r\n    session.run(self._initializer, your-feed-dict)\r\n```", "I extended @gmichaeljaison 's example with a SummarySaverHook and voila, the error is back:\r\n```\r\nimport tensorflow as tf\r\n\r\ndataset_train = tf.data.Dataset.range(10)\r\ndataset_val = tf.data.Dataset.range(90, 100)\r\n\r\niter_train_handle = dataset_train.make_one_shot_iterator().string_handle()\r\niter_val_handle = dataset_val.make_one_shot_iterator().string_handle()\r\n\r\nhandle = tf.placeholder(tf.string, shape=[])\r\niterator = tf.data.Iterator.from_string_handle(\r\n    handle, dataset_train.output_types, dataset_train.output_shapes)\r\nfeature = iterator.get_next()\r\n\r\npred = feature * feature\r\ntf.summary.scalar('pred', pred)\r\nglobal_step = tf.train.create_global_step()\r\n\r\nsummary_hook = tf.train.SummarySaverHook(save_steps=5,\r\n                                         output_dir=\"summaries\", summary_op=tf.summary.merge_all())\r\n\r\nwith tf.train.MonitoredTrainingSession(hooks=[summary_hook]) as sess: \r\n    handle_train, handle_val = sess.run([iter_train_handle, iter_val_handle])\r\n\r\n    for step in range(10):\r\n        feat = sess.run(feature, feed_dict={handle: handle_train})\r\n        pred_ = sess.run(pred, feed_dict={handle: handle_train})\r\n        print('train: ', feat)\r\n        print('pred: ', pred_)\r\n\r\n        if step % 3 == 0:\r\n            print('val', sess.run(feature, feed_dict={handle: handle_val}))\r\n```\r\n\r\nGives error:\r\n```\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype string\r\n\t [[Node: Placeholder = Placeholder[dtype=DT_STRING, shape=[], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\t [[Node: cond/Switch_1/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_18_cond/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```", "while it works if you add a CheckpointSaverHook instead (?)\r\n", "@maxfiedler could you please let us know the exception stack trace? It will be useful to know which session.run command created this error.\r\n\r\nBTW, `MonitoredTrainingSession` has an argument ` save_summaries_steps`. you can use that instead to handle summaries.\r\n", "Full Trace\r\n```\r\n/home/mfiedler/.tensorflow-venv/bin/python3.6 /home/mfiedler/perception/T5168/try_stuff/stuff.py\r\n2017-11-30 19:59:48.788525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-30 19:59:48.788872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.72GiB\r\n2017-11-30 19:59:48.788883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2017-11-30 19:59:48.854709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2017-11-30 19:59:48.871052: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Resource localhost/_1_OneShotIterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\nTraceback (most recent call last):\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_1_OneShotIterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: cond/Switch_1/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_cond/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mfiedler/perception/T5168/try_stuff/stuff.py\", line 31, in <module>\r\n    feat = sess.run(feature, feed_dict={handle: handle_train})\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 521, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 892, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 967, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/lib/python3/dist-packages/six.py\", line 686, in reraise\r\n    raise value\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1024, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 827, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_1_OneShotIterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: cond/Switch_1/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_cond/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op 'IteratorFromStringHandle', defined at:\r\n  File \"/home/mfiedler/perception/T5168/try_stuff/stuff.py\", line 11, in <module>\r\n    handle, dataset_train.output_types, dataset_train.output_shapes)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 189, in from_string_handle\r\n    output_shapes=nest.flatten(output_shapes))\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 662, in iterator_from_string_handle\r\n    output_types=output_types, output_shapes=output_shapes, name=name)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Resource localhost/_1_OneShotIterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: cond/Switch_1/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_cond/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nError in sys.excepthook:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/apport_python_hook.py\", line 63, in apport_excepthook\r\n    from apport.fileutils import likely_packaged, get_recent_crashes\r\n  File \"/usr/lib/python3/dist-packages/apport/__init__.py\", line 5, in <module>\r\n    from apport.report import Report\r\n  File \"/usr/lib/python3/dist-packages/apport/report.py\", line 30, in <module>\r\n    import apport.fileutils\r\n  File \"/usr/lib/python3/dist-packages/apport/fileutils.py\", line 23, in <module>\r\n    from apport.packaging_impl import impl as packaging\r\n  File \"/usr/lib/python3/dist-packages/apport/packaging_impl.py\", line 23, in <module>\r\n    import apt\r\n  File \"/usr/lib/python3/dist-packages/apt/__init__.py\", line 23, in <module>\r\n    import apt_pkg\r\nModuleNotFoundError: No module named 'apt_pkg'\r\n\r\nOriginal exception was:\r\nTraceback (most recent call last):\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_1_OneShotIterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: cond/Switch_1/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_cond/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/mfiedler/perception/T5168/try_stuff/stuff.py\", line 31, in <module>\r\n    feat = sess.run(feature, feed_dict={handle: handle_train})\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 521, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 892, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 967, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/lib/python3/dist-packages/six.py\", line 686, in reraise\r\n    raise value\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1024, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 827, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_1_OneShotIterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: cond/Switch_1/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_cond/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op 'IteratorFromStringHandle', defined at:\r\n  File \"/home/mfiedler/perception/T5168/try_stuff/stuff.py\", line 11, in <module>\r\n    handle, dataset_train.output_types, dataset_train.output_shapes)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 189, in from_string_handle\r\n    output_shapes=nest.flatten(output_shapes))\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 662, in iterator_from_string_handle\r\n    output_types=output_types, output_shapes=output_shapes, name=name)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/mfiedler/.tensorflow-venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Resource localhost/_1_OneShotIterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_Placeholder_0_0)]]\r\n\t [[Node: cond/Switch_1/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_cond/Switch_1\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n", "StopAtStepHook also works. \r\nLoggingTensorHook doesn't\r\nI guess it's about which Hook actually needs to evaluate certain ops in the graph for its functionality.\r\nThen, when they try to evaluate part of the graph on the first session.run() call that produces the handle instances\r\n`    handle_train, handle_val = sess.run([iter_train_handle, iter_val_handle]) `\r\nwhere the handle obviously cannot be fed through the feed_dict yet, it causes the error\r\nA simple (hacky) fix could be to tell the hooks to ignore this very first session.run call (i.e. do not execute their before_run and after_run function bodies in this case). I will try this to confirm my hypothesis.", "Yes this does the trick. Below the hack in the source code of SummarySaverHook that will make the example above work (starting in line 628 of basic_session_run_hooks.py of TF r1.4):\r\n```\r\n  def begin(self):\r\n    if self._summary_writer is None and self._output_dir:\r\n      self._summary_writer = SummaryWriterCache.get(self._output_dir)\r\n    self._next_step = None\r\n    self._global_step_tensor = training_util._get_or_create_global_step_read()  # pylint: disable=protected-access\r\n    self._start = False\r\n    if self._global_step_tensor is None:\r\n      raise RuntimeError(\r\n          \"Global step should be created to use SummarySaverHook.\")\r\n\r\n  def before_run(self, run_context):  # pylint: disable=unused-argument\r\n    if self._start:\r\n      self._request_summary = (\r\n          self._next_step is None or\r\n          self._timer.should_trigger_for_step(self._next_step))\r\n      requests = {\"global_step\": self._global_step_tensor}\r\n      if self._request_summary:\r\n        if self._get_summary_op() is not None:\r\n          requests[\"summary\"] = self._get_summary_op()\r\n\r\n      return SessionRunArgs(requests)\r\n\r\n  def after_run(self, run_context, run_values):\r\n    if self._start:\r\n      _ = run_context\r\n      if not self._summary_writer:\r\n        return\r\n\r\n      stale_global_step = run_values.results[\"global_step\"]\r\n      global_step = stale_global_step + 1\r\n      if self._next_step is None or self._request_summary:\r\n        global_step = run_context.session.run(self._global_step_tensor)\r\n\r\n      if self._next_step is None:\r\n        self._summary_writer.add_session_log(\r\n            SessionLog(status=SessionLog.START), global_step)\r\n\r\n      if self._request_summary:\r\n        self._timer.update_last_triggered_step(global_step)\r\n        if \"summary\" in run_values.results:\r\n          for summary in run_values.results[\"summary\"]:\r\n            self._summary_writer.add_summary(summary, global_step)\r\n\r\n      self._next_step = global_step + 1\r\n    self._start = True\r\n```\r\nOf course this shouldn't be the official fix ;)\r\nAlternatively one could check the fetches of a session run call and don't execute the before_run and after_run off a hook if the fetches contain a `Tensor(\"IteratorToStringHandle:0\", shape=(), dtype=string)`\r\nBut this also seems rather roundabout ...\r\n", "@maxfiedler thanks for the code. I see the problem now. First `session.run` invokes summaries. I can think of a couple workaround. One of the is using MonitoredSession.run_step_fn(...) (@isaprykin ). This is a new utility. You can find it at head. \r\n```\r\ndef step_fn(step_context):\r\n  if handle_train is None:\r\n    handle_train, handle_val = sess.run([iter_train_handle, iter_val_handle])\r\n  return step_context.run_with_hooks(fetches=..., feed_dict=...)\r\n```", "@ispirmustafa I outlined another simple solution in a new feature request:\r\nhttps://github.com/tensorflow/tensorflow/issues/15158", "We think allowing different hook sets to be used in each run complicates the usage. MonitoredSession.run_step_fn should be used instead.", "Another thing that I noticed is that after introducing the new Dataset API (and Multi-GPU implementation following largely the example from the tutorial) to our Resnet implementation:\r\nThe very first session.run() call in the MonitoredSession block (the one evaluating the iterator.handle) is hanging for about 30 seconds (using Cifar10) / about 80 seconds (using imagenet) before evaluating the subsequent sess.run() calls IF using a CheckpointSaverHook\r\nTurns out that writing the 'graph.pbtxt' file is what is taking so long, specifically the `file_io.atomic_write_string_to_file(path, text_format.MessageToString(graph_def))` command in `graph.io` is taking very long\r\nAny ideas, why this could be?", "So, the problem is that the graph_def contains the **content** of the tf.Dataset instances that I build with the `.from_tensor_slices(array of file-path-strings)` method.\r\nI assume, this is not the intended behavior of the tf.Dataset and Iterator API. Is this a bug or is there something to watch out for while constructing the tf.Dataset instances? (e.g. Do Not directly pass numpy arrays to `from_tensor_slices()` but create `tf.constants` from them first?)\r\nBelow, as an example a few lines of the `graph.pbtxt` file storing my ResNet implementation using Imagenet\r\n```\r\nnode {\r\n  name: \"tensors/component_0\"\r\n  op: \"Const\"\r\n  device: \"/device:CPU:0\"\r\n  attr {\r\n    key: \"_output_shapes\"\r\n    value {\r\n      list {\r\n        shape {\r\n          dim {\r\n            size: 1281167\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_STRING\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_STRING\r\n        tensor_shape {\r\n          dim {\r\n            size: 1281167\r\n          }\r\n        }\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03042490/n03042490_4061.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02110063/n02110063_5748.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02113624/n02113624_695.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02804610/n02804610_19564.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02979186/n02979186_4284.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n01641577/n01641577_17952.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02692877/n02692877_28505.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03777568/n03777568_9134.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n01530575/n01530575_1008.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03733281/n03733281_42923.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03345487/n03345487_16827.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03697007/n03697007_15122.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02088466/n02088466_9046.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02655020/n02655020_8269.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n01984695/n01984695_11124.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03781244/n03781244_12596.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03109150/n03109150_25698.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02119022/n02119022_12055.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03680355/n03680355_4881.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03742115/n03742115_12877.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02128925/n02128925_3865.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n13044778/n13044778_1545.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02396427/n02396427_29936.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02104365/n02104365_7290.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03637318/n03637318_7028.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n01774384/n01774384_17819.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02782093/n02782093_2885.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02804414/n02804414_4313.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n04026417/n04026417_9113.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02110341/n02110341_2431.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02442845/n02442845_5329.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n07579787/n07579787_3848.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n07753113/n07753113_4663.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n01914609/n01914609_17098.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n07711569/n07711569_3906.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03956157/n03956157_13205.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n02443114/n02443114_38892.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03908714/n03908714_2272.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03394916/n03394916_54636.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n01601694/n01601694_519.JPEG\"\r\n        string_val: \"/mnt/extra_storage/datasets/ILSVRC/Data/CLS-LOC/train/n03450230/n03450230_11239.JPEG\"\r\n```", "@maxfiedler `Dataset.from_tensor_slices()` will always convert its arguments to tensors, so if you have a large input, it will always be encoded in the graph. The best way to work around this is to replace the list/array of inputs with a TensorFlow op that calculates the same list (e.g. [`tf.matching_files(pattern)`](https://www.tensorflow.org/api_docs/python/tf/matching_files)). If you need arbitrary Python logic to construct the list, you could instead use `Dataset.from_generator(lambda: list_of_file_path_strings)`, which will not encode the strings into the graph.\r\n\r\n(Note however that using `tf.matching_files()` is preferable to `Dataset.from_generator()` because the former allows you to save the graph and restore it. The Python functions used in `Dataset.from_generator()` will not be saved when you save the graph.)", "Thanks, @mrry \r\nI will try that. (Maybe, include a sentence on this in the 'Importing Data' documentation)", "Related to @dongjk's question/problem, I have an explicit example of loading tfrecords with the `Dataset` API on a modified version of the [tensorflow/models/research/resnet](https://github.com/tensorflow/models/tree/master/research/resnet) example, with @ispirmustafa's suggestion of using a hook (thank you for that suggestion btw, it solved a very long headache \ud83d\ude03). Hopefully it helps \"connect the dots\" in case anyone like me is having trouble piecing all of this together and making a working example with their own data. I really like the `Dataset` version of doing things, and am looking forward to see how it develops further. Thanks!\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n...\r\n\r\ntrain_tfrecord_path = '/path/to/my/train-000.tfrecord'\r\ntrain_tfrecord_filenames = [train_tfrecord_path] # Add more to the list if you need\r\nval_tfrecord_path = '/path/to/my/val-000.tfrecord'\r\nval_tfrecord_filenames = [val_tfrecord_path] # Add more to the list if you need\r\n\r\n#Make a placeholder, we will use a feed_dict to fill this with the filenames of the tfrecords\r\nfilenames_placeholder = tf.placeholder(tf.string, shape=[None])\r\n\r\n# Create the `Dataset` and apply some preprocessing\r\ndataset = tf.data.TFRecordDataset(filenames_placeholder)\r\ndataset = dataset.map(_my_parse_function, num_parallel_calls=4)\r\ndataset = dataset.repeat()\r\n# This method of batching assures a fixed batch size and avoids problems\r\n# of unknown shapes [?, num_classes] (for labels), otherwise can use `dataset.batch(batch_size)`\r\ndataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(FLAGS.batch_size))\r\ndataset = dataset.prefetch(1)\r\n\r\n# Create the `Iterator`, `Initializer` and get the images and labels for building the model\r\niterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\r\ninit_train = iterator.make_initializer(dataset)\r\nimages, labels = iterator.get_next()\r\n\r\n...\r\n\r\n# Create the hook to initialize the Iterator with the filenames_list, credit to @ispirmustafa\r\nclass _DatasetInitializerHook(tf.train.SessionRunHook):\r\n    def __init__(self, initializer, filenames_list):\r\n        self._initializer = initializer\r\n        self._filenames_list = filenames_list\r\n    def begin(self):\r\n        pass\r\n    def after_create_session(self, session, coord):\r\n        del coord\r\n        session.run(self._initializer, feed_dict={filenames_placeholder: self._filenames_list})\r\n\r\n...\r\n\r\n# define the training function. Modified version of wideresnet cifar example from <https://github.com/tensorflow/models/tree/master/research/resnet>\r\ndef train(hps):\r\n    # `images` and `labels` are the previous output from our call to `iterator.get_next()`\r\n    model = resnet_model.ResNet(hps, images, labels, FLAGS.mode, batch_size=FLAGS.batch_size)\r\n    model.build_graph()\r\n\r\n    # Make instance of `_DatasetInitializerHook`\r\n    initializer_hook = _DatasetInitializerHook(init_train, train_tfrecord_filenames)\r\n    ...\r\n    # After everything else is prepared, prepend the `initializer_hook` to the hooks in `MonitoredTrainingSession`\r\n    with tf.train.MonitoredTrainingSession(\r\n            checkpoint_dir=FLAGS.log_root,\r\n            hooks=[initializer_hook, logging_hook, _LearningRateSetterHook()],\r\n            chief_only_hooks=[summary_hook],\r\n            save_summaries_steps=0,\r\n            config=tf.ConfigProto(allow_soft_placement=True)) as mon_sess:\r\n        while not mon_sess.should_stop():\r\n            mon_sess.run(model.train_op)\r\n\r\n....\r\n\r\ndef eval(hps):\r\n    # Modify `eval` the same way, but use an initializer for the validation dataset, or feed `val_filenames`\r\n\r\n    ...\r\n\r\n...\r\n\r\n# For completeness, the rest of the framework from the tf example\r\n# Note: I removed the gpu and batch_size handling from the example, modify the network accordingly.\r\ndef main(_):\r\n    hps = resnet_model.HParams(batch_size=FLAGS.batch_size,\r\n                               num_classes=FLAGS.num_classes,\r\n                               min_lrn_rate=0.0001,\r\n                               lrn_rate=0.1,\r\n                               num_residual_units=5,\r\n                               use_bottleneck=False,\r\n                               weight_decay_rate=0.0002,\r\n                               relu_leakiness=0.1,\r\n                               optimizer='mom')\r\n\r\n    if FLAGS.mode == 'train':\r\n        train(hps)\r\n    elif FLAGS.mode == 'eval':\r\n        evaluate(hps)\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run()\r\n\r\n```", "Hi, guys:\r\nThere is a demo for using placeholder in mot_session with SessionRunHook.\r\n[placeholder_mot_session](https://github.com/jke-zq/tensorflow120/blob/master/dataset_switch.py)", "A combination of saveable iterator state,\r\n\r\n```python\r\n    # Create dataset with placeholder.\r\n    placeholder = tf.placeholder(tf.float32, [None, 5])\r\n    dataset = tf.data.Dataset.from_tensor_slices(placeholder)\r\n    if shuffle:\r\n      dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(100, seed=0))\r\n    else:\r\n      dataset = dataset.repeat()\r\n    dataset = dataset.batch(32)\r\n\r\n    # Construct initialization function and a minibatch.\r\n    iterator = dataset.make_initializable_iterator()\r\n    x = iterator.get_next()\r\n\r\n    # Add dataset serialization ops to graph.\r\n    saveable = tf.contrib.data.make_saveable_from_iterator(iterator)\r\n    tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\r\n```\r\n\r\nand `init_fn`\r\n\r\n```python\r\n  def init_fn(scaffold, session):\r\n    session.run(iterator.initializer, feed_dict={placeholder: np.random.randn(10, 5)})\r\n\r\n  with tf.MonitoredTrainingSession(\r\n      checkpoint_dir=...,\r\n      scaffold=tf.train.Scaffold(init_fn=init_fn)) as sess:\r\n        sess.run(x)\r\n```\r\n\r\nworks for me. It guarantees the iterator is initialized before the first checkpoint is created, and ensures state is restored from disk afterwards. \r\n\r\nPay special attention to the use of `tf.contrib.data.shuffle_and_repeat()` instead of `Dataset.shuffle()`. Only the former is guaranteed to be serializable.", "> @dongjk you can use a hook to do all you need. Following is a code example.\r\n> \r\n> ```\r\n> initializer_hook = DatasetInitializerHook(dataset.make_initializable_iterator())\r\n> with MonitoredTrainingSession(hooks=[], ...\r\n> \r\n> class _DatasetInitializerHook(tf.train.SessionRunHook):\r\n>   def __init__(self, iterator):\r\n>     self._iterator = iterator\r\n>   def begin(self):\r\n>     self._initializer = self._iterator.initializer\r\n>   def after_create_session(self, session, coord):\r\n>     del coord\r\n>     session.run(self._initializer, your-feed-dict)\r\n> ```\r\n\r\ngood!", "> Related to @dongjk's question/problem, I have an explicit example of loading tfrecords with the `Dataset` API on a modified version of the [tensorflow/models/research/resnet](https://github.com/tensorflow/models/tree/master/research/resnet) example, with @ispirmustafa's suggestion of using a hook (thank you for that suggestion btw, it solved a very long headache \ud83d\ude03). Hopefully it helps \"connect the dots\" in case anyone like me is having trouble piecing all of this together and making a working example with their own data. I really like the `Dataset` version of doing things, and am looking forward to see how it develops further. Thanks!\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> \r\n> ...\r\n> \r\n> train_tfrecord_path = '/path/to/my/train-000.tfrecord'\r\n> train_tfrecord_filenames = [train_tfrecord_path] # Add more to the list if you need\r\n> val_tfrecord_path = '/path/to/my/val-000.tfrecord'\r\n> val_tfrecord_filenames = [val_tfrecord_path] # Add more to the list if you need\r\n> \r\n> #Make a placeholder, we will use a feed_dict to fill this with the filenames of the tfrecords\r\n> filenames_placeholder = tf.placeholder(tf.string, shape=[None])\r\n> \r\n> # Create the `Dataset` and apply some preprocessing\r\n> dataset = tf.data.TFRecordDataset(filenames_placeholder)\r\n> dataset = dataset.map(_my_parse_function, num_parallel_calls=4)\r\n> dataset = dataset.repeat()\r\n> # This method of batching assures a fixed batch size and avoids problems\r\n> # of unknown shapes [?, num_classes] (for labels), otherwise can use `dataset.batch(batch_size)`\r\n> dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(FLAGS.batch_size))\r\n> dataset = dataset.prefetch(1)\r\n> \r\n> # Create the `Iterator`, `Initializer` and get the images and labels for building the model\r\n> iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\r\n> init_train = iterator.make_initializer(dataset)\r\n> images, labels = iterator.get_next()\r\n> \r\n> ...\r\n> \r\n> # Create the hook to initialize the Iterator with the filenames_list, credit to @ispirmustafa\r\n> class _DatasetInitializerHook(tf.train.SessionRunHook):\r\n>     def __init__(self, initializer, filenames_list):\r\n>         self._initializer = initializer\r\n>         self._filenames_list = filenames_list\r\n>     def begin(self):\r\n>         pass\r\n>     def after_create_session(self, session, coord):\r\n>         del coord\r\n>         session.run(self._initializer, feed_dict={filenames_placeholder: self._filenames_list})\r\n> \r\n> ...\r\n> \r\n> # define the training function. Modified version of wideresnet cifar example from <https://github.com/tensorflow/models/tree/master/research/resnet>\r\n> def train(hps):\r\n>     # `images` and `labels` are the previous output from our call to `iterator.get_next()`\r\n>     model = resnet_model.ResNet(hps, images, labels, FLAGS.mode, batch_size=FLAGS.batch_size)\r\n>     model.build_graph()\r\n> \r\n>     # Make instance of `_DatasetInitializerHook`\r\n>     initializer_hook = _DatasetInitializerHook(init_train, train_tfrecord_filenames)\r\n>     ...\r\n>     # After everything else is prepared, prepend the `initializer_hook` to the hooks in `MonitoredTrainingSession`\r\n>     with tf.train.MonitoredTrainingSession(\r\n>             checkpoint_dir=FLAGS.log_root,\r\n>             hooks=[initializer_hook, logging_hook, _LearningRateSetterHook()],\r\n>             chief_only_hooks=[summary_hook],\r\n>             save_summaries_steps=0,\r\n>             config=tf.ConfigProto(allow_soft_placement=True)) as mon_sess:\r\n>         while not mon_sess.should_stop():\r\n>             mon_sess.run(model.train_op)\r\n> \r\n> ....\r\n> \r\n> def eval(hps):\r\n>     # Modify `eval` the same way, but use an initializer for the validation dataset, or feed `val_filenames`\r\n> \r\n>     ...\r\n> \r\n> ...\r\n> \r\n> # For completeness, the rest of the framework from the tf example\r\n> # Note: I removed the gpu and batch_size handling from the example, modify the network accordingly.\r\n> def main(_):\r\n>     hps = resnet_model.HParams(batch_size=FLAGS.batch_size,\r\n>                                num_classes=FLAGS.num_classes,\r\n>                                min_lrn_rate=0.0001,\r\n>                                lrn_rate=0.1,\r\n>                                num_residual_units=5,\r\n>                                use_bottleneck=False,\r\n>                                weight_decay_rate=0.0002,\r\n>                                relu_leakiness=0.1,\r\n>                                optimizer='mom')\r\n> \r\n>     if FLAGS.mode == 'train':\r\n>         train(hps)\r\n>     elif FLAGS.mode == 'eval':\r\n>         evaluate(hps)\r\n> \r\n> if __name__ == '__main__':\r\n>     tf.logging.set_verbosity(tf.logging.INFO)\r\n>     tf.app.run()\r\n> ```\r\n\r\nyou are very good!"]}, {"number": 12858, "title": "Fix a path in README.md", "body": "", "comments": ["@Keno42, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @martinwicke and @petewarden to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "Thanks for the PR @Keno42!"]}]