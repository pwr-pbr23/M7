[{"number": 5544, "title": "lack of up-to-date files after upgrading to tensorflow 0.11", "body": "I just upgraded tensorflow to 0.11 by using `conda install -c conda-forge tensorflow` , but still lack of files in `anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/examples/tutorials`, such as `mnist_softmax.py`. The current files I have in this folder are:\r\n```\r\n__init__.py \r\n__init__.pyc\r\ninput_data.py\r\ninput_data.pyc\r\nmnist.py\r\nmnist.pyc\r\n```\r\n", "comments": ["Hi @kopxiong ,\n\n0.11 was built a month ago. If you would like the latest greatest of everything, please install nightly.\n\nSherry\n", "One stupid question: anyone can give a hint how to install tensorflow nightly? Because I am using python 2.7 in Anaconda environment, is Anaconda not a good choice for tensorflow? Thanks a lot!\n", "@martinwicke Should a page about the tensorflow nightly builds show up on the first page of https://www.google.com/search?sclient=psy-ab&espv=2&biw=1440&bih=801&btnG=Search&q=tensorflow+nightly&oq=&gs_l=&pbx=1#q=tensorflow+nightly+builds?\n", "Ideally, but since we're completely replacing our website, SEO is not\nuseful at this point.\n\nOn Thu, Nov 17, 2016 at 8:56 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke Should a page about the\n> tensorflow nightly builds show up on the first page of\n> https://www.google.com/search?sclient=psy-ab&espv=2&biw=\n> 1440&bih=801&btnG=Search&q=tensorflow+nightly&oq=&gs_l=&\n> pbx=1#q=tensorflow+nightly+builds?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5544#issuecomment-261302620,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_ZV4rR0iGHbSnkZ8EQsVhlxFutYdks5q_IcmgaJpZM4Kv46S\n> .\n", "@girving @sherrym \r\n\r\nI've uninstalled tensorflow and build it from nightly binaries, it is now `0.11.0` version, but still doesn't contain other examples except for mnist tutorials. \r\n\r\n```\r\n/tensorflow\r\n    /examples\r\n        /__pycache__\r\n        /tutortials\r\n            /mnist\r\n                /input_data.py\r\n                /mnist.py\r\n                /__init__.py\r\n                /__pycache__\r\n            /__pycache__\r\n            /__init__.py\r\n        /__init__.py\r\n```", "That's the only example that named in the pip_package/BUILD file. I have no objection to including other things from the examples/ folder."]}, {"number": 5543, "title": "Constant folding doesn't remove control edges", "body": "I believe that when constant folding takes place, and a section of a graph is replaced by a constant, that only the data output edge of the replaced node is removed.  \r\n\r\nI believe that I can see that a graph of nodes ending up in a Div (part of the gradient generation bit) is replaced by a Const.  The output of the Div goes to a Mul, and this is changed to the new const correctly.\r\n\r\nHowever, there is a control output from the Div going to a Const (not sure why, but it is).  This is not changed to the Div replacement.  Consequently the dead node pruning doesn't remove the original Div.  \r\n\r\nHere is some trace:\r\n\r\nDuring the constant folding:\r\n\r\nGraph Before #nodes 67 #edges 109\r\nGraph Constant graph #nodes 32 #edges 42\r\nConstant foldable 32 : 67\r\n\r\nReplacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:ipu:0\"](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant\r\nReplacing edge to gradients/Square_grad/mul_1:0\r\n\r\nDuring the post constant folding pruning:\r\n\r\nPruneForReverseReachability: gradients/Square_grad/mul_1 <- gradients/Mean_grad/truediv/_0__cf__1\r\nPruneForReverseReachability: gradients/Square_grad/mul/x <- gradients/Mean_grad/truediv\r\n\r\nAfter the pruning:\r\n\r\nGraph ConstFolding #nodes 68 #edges 110\r\n\r\n\r\nWhen the graph is passed to the device, it contains:\r\n\r\nNode: gradients/Square_grad/mul/x = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 2>, _device=\"/job:localhost/replica:0/task:0/device:ipu:0\"](^gradients/Mean_grad/truediv)\r\n\r\n", "comments": ["Example:\n\n```\nimport tensorflow as tf\n\n# Creates a graph.\na = tf.placeholder(tf.float32, [2,2])\nb = tf.placeholder(tf.float32, [2,2])\nc = tf.constant([[2.0,2.0],[2.0,2.0]])\nd = tf.constant([[2.0,2.0],[2.0,2.0]])\ne = tf.add(c, d)\nwith tf.control_dependencies([e]):\n  f = tf.mul(a, a)\n  g = tf.mul(b, e)\n  o = tf.add(f, g)\n\nsess = tf.Session()\n\nfd = {\n  a: [[1.0,2.0],[3.0,4.0]],\n  b: [[1.0,0.0],[0.0,1.0]],\n}\n\nsess.run(o, feed_dict=fd)\n```\n\nDuring constant folding this leads to:\n\n```\nGraph Before #nodes 11 #edges 16\n|| \n|| () -> () {\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_16__recv_Placeholder_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_17__recv_Placeholder_1_0\", tensor_type=float]()\n||   n4 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()\n||   n5 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()\n||   n6 = Add[T=float](n4, n5)\n||   n7 = Mul[T=float](n2, n2) @ n6\n||   n8 = Mul[T=float](n3, n6)\n||   n9 = Add[T=float](n7, n8)\n||   n10 = _Send[T=float, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device_incarnation=1, tensor_name=\"edge_5_Add_1\"](n9)\n|| }\n|| \n\n\nGraph After #nodes 12 #edges 17\n|| \n|| () -> () {\n||   n11 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 4 4 4...>]()\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_16__recv_Placeholder_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_17__recv_Placeholder_1_0\", tensor_type=float]()\n||   n4 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()\n||   n5 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()\n||   n8 = Mul[T=float](n3, n11)\n||   n6 = Add[T=float](n4, n5)\n||   n7 = Mul[T=float](n2, n2) @ n6\n||   n9 = Add[T=float](n7, n8)\n||   n10 = _Send[T=float, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device_incarnation=1, tensor_name=\"edge_5_Add_1\"](n9)\n|| }\n|| \n```\n\nYou can see that there is a constant replacing the Add, but that the Add remains, because it is referred to by the control edge.\n\nI think that the output should be:\n\n```\nGraph After #nodes 12 #edges 17\n|| \n|| () -> () {\n||   n11 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 4 4 4...>]()\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_16__recv_Placeholder_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_17__recv_Placeholder_1_0\", tensor_type=float]()\n||   n8 = Mul[T=float](n3, n11)\n||   n7 = Mul[T=float](n2, n2) @ n11\n||   n9 = Add[T=float](n7, n8)\n||   n10 = _Send[T=float, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device_incarnation=1, tensor_name=\"edge_5_Add_1\"](n9)\n|| }\n|| \n```\n", "One more example.  This is the 'getting_started.py' that is in the TF documentation.  I have modified it so that it uses placeholders, rather than allowing the X and Y data to be constants.\n\n```\nGraph Before #nodes 67 #edges 109\n|| \n|| () -> () {\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()\n||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()\n||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n28, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n\nReplacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:ipu:0\"](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant\n\nGraph After #nodes 68 #edges 110\n|| \n|| () -> () {\n||   n67 = Const[dtype=float, value=Tensor<type: float shape: [100] values: 0.01 0.01 0.01...>]()\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()\n||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()\n||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n67, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n```\n\nNode n29, which has a control dependency on n28 is the problem.  n28 was the Div which was replaced.\n", "@keveman would you please comment? Thanks.\n", "I should point out that if I bind the nodes to the CPU, then the results are the same.\n\n(these examples are for my own device back end)\n", "It seems that constant_folding.cc, ReplaceTensorWithConstant, assumes that the node that will be replaced will have multiple outputs, and only replaces those edges that are fed by the one replaced output. \n\nBroadcastGradientArgs is a good example.  It really needs to be replaced by 2 constants, and the code, as it stands, works well for that.\n\nPerhaps it is worth considering what it means to have a node that only has a control output assigned to it?  Is this valid?  If not, then the control output should be eliminated, then then the dead code pruning algorithm will kill off that node and it's upstream nodes.\n\nI've not played with the control flow (loops, conditionals), so I don't know if nodes with only control outputs are useful.   I would expect that loops and conditonals are done with Boolean edges, not control edges.  If this is the case, then I think that adding the control edge removal thing is the correct thing to do.\n", "I am also wondering what is the rationale behind the memory constraints in the ReplaceTensorWithConstant:\n\n```\n  // 1) If the destination tensor is not an int32 tensor, and has HOST_MEMORY\n  // constraint, do not replace it.\n  // 2) If the destination tensor is an int32 tensor, but has DEVICE_MEMORY\n  // constraint, do not replace it.\n```\n\nWhy would you want to keep nodes that perform arithmetic on a device unnecessarily?  I can imagine that you might consider that it is important to do floating point arithmetic on the target platform because of differences in rounding (although on a neural network I would hope that the result would work around that).\n\nIn the case of integers, I can't decide why constant folding would always take place. Could you describe why it would be limited to HOST_MEMORY only?  \n", "Here is a diff for trying out 'remove control outputs from nodes that have only a control output after constant folding' change:\n\n[diff.txt](https://github.com/tensorflow/tensorflow/files/589083/diff.txt)\n", "Sorry for the late response. Constant folding is conservative (perhaps overly) in what it replaces. In your case, since the original node had an outgoing control dependence, it didn't get replaced. Note that the analysis to decide whether that is safe to do is somewhat non-trivial. We have to determine whether that node is control dependent on a stateful node. We chose to let it be conservative. If you have a real example where the performance is poor because of this conservativeness, I'll be happy to take a look.\n", "I see the problem.  My case isn't really a performance issue, per se. I am trying to implement a device on TF for our new hardware.  The extra nodes of integer math are getting in the way of the initial bring-up.\r\n\r\nIf there are stateful nodes in the graph, then why does the constant tree finder consider them acceptable to be within the constant tree?  Isn't the constant that is generated (there is one generated) is not valid after the 1st run through the graph?  In which case, the constant should not replace that section of the graph at all? \r\n\r\n", "I realize i failed to write sensible language in that last message.\r\n\r\nWhat I intended to say is that replacing a stateful node with a constant doesn't seem right.  If the stateful node changes state, then the constant is invalid. Either the stateful node should not be included in the constant sub-graph, or the constant sub-graph should be re-evaluated whenever the stateful node changes, in which case there doesn't seem to be a lot of point in having it.\r\n", "Hi,\r\n\r\nIn fact, I'm pretty certain that the following code taken from constant_folding.cc will prevent the constant sub-graph from containing any stateful nodes.  In which case, it should be ok to apply the control edge removal.\r\n\r\n```\r\nbool IsConstantFoldable(const FunctionLibraryDefinition* flib_def,\r\n                        const Node* n,\r\n                        std::function<bool(const Node*)> consider) {\r\n  if (n->op_def().is_stateful()) {\r\n    return false;\r\n  }\r\n```", "Started looking at this. Will have a fix soon.", "Now that my own device is an XLA device, I am no longer concerned about this.   \r\n\r\nHowever this is still a problem as far as I can see.  unless the dead code removal is removing nodes that only have a control output."]}, {"number": 5542, "title": "word2vec: Create directories for saving summaries, if necessary.", "body": "Simple change to allow passing a directory that doesn't exist to `--save_path` in the word2vec examples.", "comments": ["Can one of the admins verify this patch?\n", "@darrengarvey, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @benoitsteiner to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "@darrengarvey Please fix indentation:\n\nFAIL: Found 2 non-whitelited pylint errors:\ntensorflow/models/embedding/word2vec_optimized.py:130: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\n\ntensorflow/models/embedding/word2vec.py:151: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6\n", "@rmlarsen - Fixed. Sorry about that.\n", "@tensorflow-jenkins test this please\n", "@darrengarvey No worries. Thanks for the fix.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5541, "title": "tensorflow/examples/learn/text_classification_builtin_rnn_model.py references removed TensorFlowRNNClassifier", "body": "### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n$ ls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root 560184 May 19 01:44 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 May 19 01:47 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 May 19 01:47 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root 394472 May 19 01:44 /usr/local/cuda/lib64/libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root 737516 May 19 01:44 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root     37 Sep 12 12:36 /usr/local/cuda/lib64/libcudnn.so -> /usr/lib/x86_64-linux-gnu/libcudnn.so\r\nlrwxrwxrwx 1 root root     39 Sep 12 12:36 /usr/local/cuda/lib64/libcudnn.so.5 -> /usr/lib/x86_64-linux-gnu/libcudnn.so.5\r\nlrwxrwxrwx 1 root root     43 Sep 12 12:36 /usr/local/cuda/lib64/libcudnn.so.5.1.5 -> /usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.5\r\nlrwxrwxrwx 1 root root     43 Sep 12 12:38 /usr/local/cuda/lib64/libcudnn_static.a -> /usr/lib/x86_64-linux-gnu/libcudnn_static.a\r\nlrwxrwxrwx 1 root root     46 Sep 12 12:38 /usr/local/cuda/lib64/libcudnn_static_v5.a -> /usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\r\n\r\n\r\nIf installed from source, provide \r\n\r\n1. git rev-parse HEAD = 2ac978d2532dd359dd7ebbd27ac13dfa147d755d\r\n2. $ bazel version\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n\r\nThe file `tensorflow/examples/learn/text_classification_builtin_rnn_model.py` references `TensorFlowRNNClassifier`, which was removed in dfb1bfe6aec58700547f6525289090c737bfe453 so presumably this example should also be removed.\r\n\r\nThat commit doesn't mention why the class was removed and it looks quite useful. What is the replacement for it? `get_rnn_model()`? I can provide a PR to remove the example but wanted to check first if actually there is another word-level RNN example for `tf.contrib.learn` to point to?\r\n", "comments": ["Are there any updates on this?", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 5540, "title": "help with finetune process", "body": "Hi there,\r\nI have run the finetune_inception_v3_on_flowers.sh slim script example, and I have obtained the graph.pbtxt file. How can I use this file in a Python script for image classification? (e.g., https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py)\r\nThanks.", "comments": ["Hi @daddym85 ,\n\nYou should be able to follow instructions here:\n\nhttps://github.com/tensorflow/models/blob/master/slim/README.md\n\nThanks,\nSherry\n", "Hi there,\nI have run the command:\n\npython train_image_classifier.py \\\n  --train_dir=${TRAIN_DIR} \\\n  --dataset_name=flowers \\\n  --dataset_split_name=train \\\n  --dataset_dir=${DATASET_DIR} \\\n  --model_name=inception_v3 \\\n  --checkpoint_path=${PRETRAINED_CHECKPOINT_DIR}/inception_v3.ckpt \\\n  --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \\\n  --trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \\\n  --learning_rate=0.01 \\\n  --learning_rate_decay_type=fixed \\\n  --save_interval_secs=60 \\\n  --save_summaries_secs=60 \\\n  --log_every_n_steps=100 \\\n  --weight_decay=0.00004\n\nto generate model.ckpt-XXXX file.\nHow can I use such a file in the freeze_graph script in order to build a pb file?\nThanks.\n", "Questions like this are better asked on StackOverflow.  Github issues are for bug reports and feature requests.\n"]}, {"number": 5539, "title": "Sticky  \"No OpKernel was registered to support Op 'Conv2D' with these attrs.\"", "body": "There are two issues here:\r\n\r\n1: tf.nn.conv2d does not support float64 tensors on CPU although the documentation states it should\r\n2: after the first occurrence of the situation above, tf.nn.conv2d keeps crashing with the same error even when we provide float32 tensors.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/4929\r\n\r\n### Environment info\r\nOperating System: Linux\r\n\r\nInstalled version of CUDA and cuDNN: None\r\n\r\nIf installed from binary pip package, provide:\r\n1. A link to the pip package you installed:\r\n-> https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp35-cp35m-linux_x86_64.whl\r\n(not 100% sure though)\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n-> 0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\n# issue 1: this never works although the documentation states that it should\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput = tf.constant(np.zeros([1,5,5,1]))\r\nfilter = tf.constant(np.zeros([3,3,1,1]))\r\nop = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')\r\ntf.Session().run(op)\r\n\r\n# issue 2: \r\n# - this works if executed *before* the part above\r\n# - this does not work if exectuted *after* the part above\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput2 = tf.constant(np.zeros([1,5,5,1]), dtype=np.float32)\r\nfilter2 = tf.constant(np.zeros([3,3,1,1]), dtype=np.float32)\r\nop2 = tf.nn.conv2d(input2, filter2, strides=[1, 1, 1, 1], padding='VALID')\r\ntf.Session().run(op2)\r\n```", "comments": ["Our doc is incorrect in this case: we currently don't support 64-bit conv2d. We will have the doc updated shortly. Thanks.\n\nWe do have plans to support 64-bit in the near future.\n", "@sherrym Is a near term doc update likely?\n"]}, {"number": 5538, "title": "Compiling error when adding -c dbg parameter", "body": "I try to compile tensorflow in Docker machine:\r\n./configure   // all options is default\r\nbazel build -c dbg --local_resources 1500,1,1 --verbose_failures  tensorflow/tools/pip_package:build_pip_package --ignore_unsupported_sandboxing\r\n\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\nthe output is here:\r\n\r\nFri Nov 11 04:23:09 UTC 2016 : === Using tmpdir: /tmp/tmp.EhXibedOJY\r\n/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles /tensorflow\r\n/tensorflow\r\n/tmp/tmp.EhXibedOJY /tensorflow\r\nFri Nov 11 04:23:15 UTC 2016 : === Building wheel\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 201, in <module>\r\n    keywords='tensorflow tensor machine learning',\r\n  File \"/usr/lib/python2.7/distutils/core.py\", line 151, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python2.7/distutils/dist.py\", line 953, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python2.7/distutils/dist.py\", line 972, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/wheel/bdist_wheel.py\", line 241, in run\r\n    wheel_name = archive_wheelfile(pseudoinstall_root, archive_root)\r\n  File \"/usr/local/lib/python2.7/dist-packages/wheel/archive.py\", line 22, in archive_wheelfile\r\n    return make_wheelfile_inner(base_name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/wheel/archive.py\", line 72, in make_wheelfile_inner\r\n    writefile(path, date_time)\r\n  File \"/usr/local/lib/python2.7/dist-packages/wheel/archive.py\", line 61, in writefile\r\n    zip.writestr(zinfo, fp.read())\r\n  File \"/usr/lib/python2.7/zipfile.py\", line 1222, in writestr\r\n    self._writecheck(zinfo)\r\n  File \"/usr/lib/python2.7/zipfile.py\", line 1106, in _writecheck\r\n    raise LargeZipFile(\"Filesize would require ZIP64 extensions\")\r\nzipfile.LargeZipFile: Filesize would require ZIP64 extensions\r\n\r\n\r\n\r\n", "comments": ["@alephman This looks like a bug in pip, not tensorflow\n\n@martinwicke Do you know any workarounds?  Is this something we can fix on our end?\n", "It looks like it's somehow trying to build a truly huge zip file. I have never seen this before, but I'm not building debug pip packages -- can you do some sanity checks to see how big it would be?\n", "yeah, the whole folder in /tmp has more than 5G size.  \nBTW,  I try to re-compile the tensorflow, but always failed due to downloading dependences errors. It seems github unstable. Any ways to work around this?\n", "@alephman I don't think we can fix this on our end, since it's a bug in pip.  I would suggest building and running in place without going through pip.\n", "Just got this error too. I always did this steps and I wasn't getting it before ", "Can you try building with --spawn_strategy=sandboxed? I just fixed the sandboxed build, and that should cut down on the size of the data in the folder used for building the package.", "Still getting this issue when compiled with 'dbg' option. Any workaround?\r\n\r\nMy workaround/hack has been to change `make_wheelfile_inner` of '/python2.7/dist-packages/wheel/archive.py' to add `allowZip64=True` parameter to `zipfile.ZipFile` class as:\r\n\r\n`\r\nzip = zipfile.ZipFile(open(zip_filename, \"wb+\"), \"w\",                        \r\n                          compression=zipfile.ZIP_DEFLATED, _allowZip64=True_) \r\n`\r\n\r\n", "@conqer This sounds like it would be a great PR for wheel.", "solved by modifying /usr/lib/python2.7/site-packages/wheel/archive.py:46\r\n```diff\r\n    # XXX support bz2, xz when available\r\n    zip = zipfile.ZipFile(open(zip_filename, \"wb+\"), \"w\",\r\n-                         compression=zipfile.ZIP_DEFLATED)\r\n+                         compression=zipfile.ZIP_DEFLATED, allowZip64=True)\r\n\r\n    score = {'WHEEL': 1, 'METADATA': 2, 'RECORD': 3}\r\n    deferred = []\r\n\r\n    def writefile(path, date_time):\r\n        st = os.stat(path)\r\n        if date_time is None:\r\n            mtime = time.gmtime(st.st_mtime)\r\n            date_time = mtime[0:6]\r\n        zinfo = zipfile.ZipInfo(path, date_time)\r\n        zinfo.external_attr = st.st_mode << 16\r\n        zinfo.compress_type = zipfile.ZIP_DEFLATED\r\n-       with open(path, 'rb') as fp:\r\n-            zip.writestr(zinfo, fp.read())\r\n+       zip.write(path, arcname=zinfo.filename, compress_type=zipfile.ZIP_DEFLATED)\r\n        log.info(\"adding '%s'\" % path)\r\n```"]}, {"number": 5537, "title": "Disable display \u201csuccessfully opened CUDA library ****\u201d when import tensorflow", "body": "Hello, \r\n  when ```import tensorflow``` it will output some info which is boring to me , how to disable this dispaly without compile again? set CUDA_VISIBLE_DEVICES seems no work.  \r\n```\r\nIn [1]: import tensorflow as tf;\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally\r\n```\r\n\r\nthanks\r\n", "comments": ["Duplicate of https://github.com/tensorflow/tensorflow/issues/566.\n", "@girving  i'm so sorry cannot found a solution there.\n", "Does anyone know of a solution to this problem?  Specifically for those using Python?  ", "Do this before importing TensorFlow\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='3'\r\nimport tensorflow\r\n\r\n```", "@yaroslavvb  thanks.  it indeed work, but is not succinct enough, we need set it everytime.  \r\ni just put `export TF_CPP_MIN_LOG_LEVEL=3``` in my env, but why it does't work?", "@tornadomeet it doest't work by the @yaroslavvb suggestion. the version is `0.11.0rc1`"]}, {"number": 5536, "title": "I am trying to build tensorflow from source code and error happed", "body": "after :bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\nThings happend like this:\r\n\r\nWARNING: /home/lixinyu/tensorflow/tensorflow/tensorflow.bzl:477:26: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /home/lixinyu/tensorflow/tensorflow/tensorflow.bzl:490:24: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /home/lixinyu/.cache/bazel/_bazel_lixinyu/824013fbb9b22e08d948a2226491e595/external/protobuf/protobuf.bzl:90:19: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /home/lixinyu/.cache/bazel/_bazel_lixinyu/824013fbb9b22e08d948a2226491e595/external/protobuf/protobuf.bzl:96:28: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nERROR: /home/lixinyu/tensorflow/tensorflow/python/BUILD:1044:1: in cc_library rule //tensorflow/python:tf_session_helper: non-test target '//tensorflow/python:tf_session_helper' depends on testonly target '//tensorflow/python:construction_fails_op' and doesn't have testonly attribute set.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\nINFO: Elapsed time: 0.209s\r\n\r\n\r\nI googled this but there is not a trustworthy answer to my issue", "comments": ["Running to the same thing error:\nERROR: /home/ubuntu/tensorflow/tensorflow/python/BUILD:1728:1: in cc_library rule //tensorflow/python:tf_session_helper: non-test target '//tensorflow/python:tf_session_helper' depends on testonly target '//tensorflow/python:construction_fails_op' and doesn't have testonly attribute set.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 2.693s\n\nUbuntu 14.04\nUsing: tf 11 (tagged commit)\nBazel Info\n\n```\nbazel version\nBuild label: 0.4.0\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\nBuild timestamp: 1478109254\nBuild timestamp as int: 1478109254\n```\n", "FYI - I just pulled master down and was able to build, \nFer ref:\ntag v0.11.0 failed\ntag v0.11.0rc2 failed\n", "@lixinyutfd  we can't really help you if you don't specify what operating system and version you are trying to build. Thanks @sono-bfio. \n", "@aselle I am using ubantu 14.04 and 0-11-0 version.\n", "@sono-bfio @lixinyutfd which bazel version are you using?\nI think this can be caused by a bazel update.\nThere has been a non backwards compatible change in bazel.\nCould you try with bazel 0.3.2 ?\n", "@gunan  I am using 0.40 for bazel.\n", "Then I am pretty certain it is with the bazel version.\nCould you try with bazel 0.3.2?\n", "I did successfully just now .\nWhat I did is just pull the new master of tensorflow and it worked.\nI dont think this is caused by bazel @gunan \nAthough there are lots of info and warning during compiling. Is such warning normal ones or shows something wrong?\n", "@lixinyutfd It is caused by an update in bazel.\nThey used to not care about \"testonly\" attributes in BUILD files.\nWith 0.4, they now do. Due to them not caring about testonly attributes before, our code evolved in such a way that we imported our test helper libraries into our pip package, which is obviously not test only.\nOn master, we did make the fixes and removed most testonly libs from pip package, so it builds successfully with the latest bazel version. In older releases, we did not make that fix, so they do not build with bazel 0.4.\n\nThe compilation warnings are expected. INFO and WARNING messages are mostly benign.\n", "Will try with bazel 0.3.2 and let you know how it goes. \n", "Hi everyone,\r\nI am looking for some help regarding the same error message. I haven't found the way to downgrade bazel to the version 0.3.2,  I am doing :\r\nroot@tensorflow:~# sudo apt-get install bazel=0.3.2\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Version '0.3.2' for 'bazel' was not found\r\n\r\nBelow is my system configuration:\r\n\r\nTensorflow version:\r\n>>> tf.__version__\r\n'1.0.0'\r\n\r\nARNING: /root/.cache/bazel/_bazel_root/ff219f214582e62419b9171e5b5ba9a0/external/protobuf/protobuf.bzl:90:19: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /root/.cache/bazel/_bazel_root/ff219f214582e62419b9171e5b5ba9a0/external/protobuf/protobuf.bzl:96:28: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\n/usr/local/lib/python2.7/dist-packages\r\n\r\nUbuntu version:\r\nDescription:\tUbuntu 16.04.2 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n\r\n\r\nBazel version:\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n\r\nThanks", "I realized I might be looking at the wrong error message above.\r\nIf you are trying to build 1.0 branch, any bazel version above 0.4.2 should work.\r\nIf you are trying to build an old branch, you can download an old version of bazel and install manually using the packages found here:\r\nhttps://github.com/bazelbuild/bazel/releases"]}, {"number": 5535, "title": "Fix inconsequential compiler warnings", "body": "Fixes many compiler warnings due to integer signedness issues, unused variables, missing return values and multi-line comments.", "comments": ["Can one of the admins verify this patch?\n", "@benbarsdell, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @mrry and @andrewharp to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n\n(Thanks!)\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5534, "title": "Error while creating apk in TensorFlow Android Camera Demo", "body": "I followed  instruction for TensorFlow Android Camera Demo, I changed the path for NDK and SDK in WORKSPACE and now when I'm trying to build APK, using command:\r\n\r\n`$ bazel build //tensorflow/examples/android:tensorflow_demo`\r\n\r\nI got such errors:\r\n\r\n![screen shot 2016-11-10 at 8 00 20 am](https://cloud.githubusercontent.com/assets/10096433/20200940/18a39390-a7bd-11e6-939a-7a0f9132be11.png)\r\n\r\nIf I'm trying to build APK in Android Studio I get such Errors:\r\n\r\n```\r\n`Error:/private/var/tmp/_bazel_Iryna/c541cf0f6f349cf0d7c8d3692096116a/external/protobuf/BUILD:73:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: sandbox-exec failed: error executing command /usr/bin/sandbox-exec -f /private/var/tmp/_bazel_Iryna/c541cf0f6f349cf0d7c8d3692096116a/bazel-sandbox/a0350092-17a0-44fc-a63c-6321732c52f4-2/sandbox.sb /bin/false -MD -MF ... (remaining 27 argument(s) skipped).`\r\nError:Execution failed for task ':buildNative'.\r\n> Process 'command '/usr/local/bin/bazel'' finished with non-zero exit value 1\r\n```\r\n\r\nMy question in [StackOverflow.](http://stackoverflow.com/questions/40520927/error-while-trying-to-build-apk-of-tensorflow-android-camera-demo)", "comments": ["What is the bazel version, tensorflow version, OS?\n", "@Luckygirlllll \nCan you verify that the entries have also been uncommented, the paths are correct, and the specified API levels have been installed?\n", "@drpngx  tensorflow==0.11.0, Bazel: build label: 0.4.0-homebrew,  Mac OS 10.11.6 \n", "@andrewharp \n\nYou are right, I changed path in WORKSPACE, but didn't uncomment it, now I uncommented it:\n![screen shot 2016-11-11 at 4 31 55 am](https://cloud.githubusercontent.com/assets/10096433/20202498/357bd102-a7c8-11e6-9b5e-9b9b1a1bb20a.png)\n\n and get ERRORS: \n\n![screen shot 2016-11-11 at 4 34 27 am](https://cloud.githubusercontent.com/assets/10096433/20202502/3db8a700-a7c8-11e6-8afa-f0fe04bc5711.png)\n", "It looks like you have \"smart\" quotes around the SDK path, which could be causing problems.\n", "@andrewharp which kind of quotes should I use there? \n", "The regular ascii kind you have elsewhere :)\n\nSome text editors, especially on Mac, will \"helpfully\" replace straight quotes (\"\") with the curly unicode ones: (\u201c\u201d). Can lead to some pretty devious issues... you might try and find the setting that caused this so it doesn't happen again.\n\nhttps://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html\n", "@andrewharp \n\nThat was a tricky one! I changed quotes,  now command \n\n`$ bazel build //tensorflow/examples/android:tensorflow_demo`\n\nworks without errors. \n\nIt finished, but I didn't see installed app in my phone. Or it generate apk file in some directory in the  laptop? and then I manually should install it? \n\n![screen shot 2016-11-11 at 6 14 27 am](https://cloud.githubusercontent.com/assets/10096433/20204278/0c2b8dc4-a7d7-11e6-9902-9895fc69eb11.png)\n", "Woohoo! It looks like, from the output, is that you have a choice of two apks to install:\n`bazel-bin/tensorflow/examples/android/tensorflow_demo_unsigned.apk` or the signed one. To install it on your phone, if memory serves, you need to upload it `adb install bazel-bin/tensorflow/examples/android/tensorflow_demo_unsigned.apk`.\n", "Command not found, if I use command:\n\n`adb install bazel-bin/tensorflow/examples/android/tensorflow_demo_unsigned.apk`\n\n![Uploading Screen Shot 2016-11-11 at 6.34.01 AM.png\u2026]()\n", "It's somewhere in the android sdk, `/Users/Iryna/Library/Android/sdk/`. There may be a binary directory.\n\nYou can use `find /Users/Iryna/Library/Android/sdk -type f -name adb` to locate it.\n", "Okay, found it, now I got:\n\n`Failure [INSTALL_PARSE_FAILED_NO_CERTIFICATES]`\n\n![screen shot 2016-11-11 at 6 43 34 am](https://cloud.githubusercontent.com/assets/10096433/20204636/64dd45fe-a7da-11e6-8bad-df4a32c5ed44.png\n", "OK, try the signed one maybe?\n\nAre you in developer mode? On the phone, you should find \"About\" \"version\", and click something like 10 times on the version to enable the developer mode.\n", "Yeah, I'm in developer mode. I tried signed apk, it installed app on my phone. But when I open the app, it crashes immideately. \n", "What does `adb logcat` say?\n", "ClassifierActivity Exception.\n java.io.FileNotFoundException: imagenet_comp_graph_label_strings.txt \n\n![screen shot 2016-11-11 at 7 30 31 am](https://cloud.githubusercontent.com/assets/10096433/20205367/da651dbe-a7e0-11e6-80a0-20d45cde980a.png)\n", "If you `unzip -v bazel-bin/tensorflow/examples/android/tensorflow_demo.apk`, do you see the file somewhere there?\n", "No\n\n![screen shot 2016-11-11 at 7 47 53 am](https://cloud.githubusercontent.com/assets/10096433/20205653/3cdb8986-a7e3-11e6-9ef8-4529a778839c.png)\n", "OK, I'm not sure where it's supposed to come from. Maybe it's from the assets and bazel failed to fetch them.\n\nCan you look at `tensorflow/examples/android/assets`? Is it there?\n\nIf you download the inception file:\n\n```\n$ curl -L https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip -o /tmp/inception5h.zip\n$ unzip -v /tmp/inception5h.zip\n```\n\nIs it there?\n", "There is no directory assets in android directory: \n\n![screen shot 2016-11-11 at 8 17 01 am](https://cloud.githubusercontent.com/assets/10096433/20206279/e0b3021a-a7e7-11e6-9bbe-7143eeffe65b.png)\n\nIf I'm doing this: \n\n```\n$ curl -L https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip -o /tmp/inception5h.zip\n$ unzip -v /tmp/inception5h.zip\n```\n\nYeah, in this zip exists file: imagenet_comp_graph_label_strings.txt\n", "Where should I put this file (imagenet_comp_graph_label_strings.txt) in the project in order not to get Error? \n", "It should have been fetched automatically. \nTry\n\n```\nunzip /tmp/inception5h.zip -d tensorflow/examples/android/assets/\n```\n\nThen rebuild, the apk, `unzip -v` to make sure it's there. Then `adb install -r` to re-install on your phone.\n", "Okay,  so I did it, installed app once again, but still have the same error. \n\n![screen shot 2016-11-11 at 8 42 46 am](https://cloud.githubusercontent.com/assets/10096433/20206729/1c273fac-a7eb-11e6-91a8-edd4d6c2a713.png)\n", "@Luckygirlllll You need to rebuild the APK after putting the files in the assets dir.\n\nAlso, if you git pull you should be able to get the updated version that auto-fetches the assets so you can skip the manual step altogether.\n", "@andrewharp Okay, and how to rebuild it? \n\nIf I'm using `bazel build //tensorflow/examples/android:tensorflow_demo`, then I get errors \n\n![screen shot 2016-11-11 at 8 51 11 am](https://cloud.githubusercontent.com/assets/10096433/20206889/40846f36-a7ec-11e6-8146-2a4e93f59971.png)\n", "Delete the .DS_store file it's complaining about -- that's an OSX artifact that doesn't need to be there.\n", "@andrewharp @drpngx Bingo! Finally it works! Thanks a lot! \n", "Woohoo!\n", "I follow what @Luckygirlllll did, but I still unable to install the apk . help please . \r\n\r\n__Nico-MBP:tensorflow nicomortel$ adb install -r bazel-bin/tensorflow/examples/android/tensorflow_demo.apk\r\nbazel-bin/tensorflow/examples/android/tenso...shed. 51.5 MB/s (175398653 bytes in 3.245s)\r\n        pkg: /data/local/tmp/tensorflow_demo.apk\r\nFailure [INSTALL_FAILED_NO_MATCHING_ABIS]__\r\n", "what this conflict means?\r\n\r\n_CONFLICT: asset:tensorflow_inception_graph.pb is provided with ambiguous priority from:\r\n        tensorflow/examples/android/assets/tensorflow_inception_graph.pb\r\n        external/inception_v1/tensorflow_inception_graph.pb\r\n\r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n        external/speech_commands/WORKSPACE\r\n        external/mobile_ssd/WORKSPACE\r\n\r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n        external/mobile_ssd/WORKSPACE\r\n        external/inception_v1/WORKSPACE\r\n\r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n        external/stylize/WORKSPACE\r\n        external/speech_commands/WORKSPACE_\r\n"]}, {"number": 5533, "title": "no modules in tensorflow/python ... ", "body": "1. The commit hash (`git rev-parse HEAD`) \r\n    20c3d37ecc9bef0e106002b9d01914efd548e66b\r\n\r\nUsing CMake, I got and installed python whl, but, 'import tensorflow' causes no module error. \r\nIt seems these two files are needed. \r\n\r\n1. tensorflow/python/ops/gen_resource_variable_ops.py this is already mentioned here: https://github.com/tensorflow/tensorflow/pull/5410) \r\n2. tensorflow/python/summary/writer/__init__.py \r\n", "comments": ["what happens when you do `dir(tensorflow)`?\n", "Thanks for reporting this!\n\nIssue (1) should already be fixed, and it looks like the fix is included in 20c3d37. Perhaps this is the result of a stale build?\n\nIssue (2) should be fixed by #5531, which fixes the `tf_python.cmake` to account for some of the new modules added recently.\n", "@joshsuihn   please verify @mrry's changes fix your problem and report back.\n", "Closing due to lack of activity."]}, {"number": 5532, "title": "[Windows/CMake] Enable avgpooling_op.cc.", "body": "Fixes #5517.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @guschmue, @zheng-xq and @tensorflower-gardener to be potential reviewers.\n", "Looks like the cmake build fails?\nis the failure caused by this change?\n", "This one looks like it builds successfully but after installing the PIP package, it can't find the `tensorflow` module. Not clear why a C++ change would affect this\u2026 could the Jenkins worker be in a bad state? (Looks like the build has been red all day, even with this morning's fix\u2026.)\n", "hmm, looks like pip install flaked?\nLet's retry this.\nJenkins, test this please.\n", "Everything but `reader_ops_test.py` passed this time... which is a bit odd since it passes locally for me :(, but it shouldn't be affected by the addition of `avgpooling_op.cc` to the build. \n", "That test has been flaking wildly in our jenkins.\nWe can debug that separately.\n", "Ah, I know why I did not enable avgpooling_op.cc: breaks the gpu build. If I remember correctly a bunch on changes needed to the .cu.cc files. \nI can take care of it.\n\n avgpooling_op.obj : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SpatialAvgPooling\n"]}, {"number": 5531, "title": "[Windows/CMake] Add support for tf.contrib.", "body": "This change enables all libraries apart from tf.contrib.tfprof to be used from the CMake-built PIP package.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane, @andrewharp and @ageron to be potential reviewers.\n", "/cc @guschmue \n", "I see a lot of test failures which look like this:\n\n00:15:14.624 AttributeError: module 'tensorflow.python.ops.gen_array_ops' has no attribute '_list_diff'\n\nThey do not seem to happen in master branch.\nAre we excluding gen_array_ops from cmake build in windows?\n", "The previous Jenkins build is showing the same failures: https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/32/console\n\nSo I don't think it's caused by this CL, but something is messed up. We're blowing away the `cmake_build` directory between runs, right? (Otherwise maybe the Python codegen for `gen_array_ops` isn't rerunning for some reason, and thus we aren't picking up the new op setdiff1d or whatever it's called?)\n", "if it helps: I have a jenkins for cpu+gpu that track master and they are both green all day, including tests.\n", "@tensorflow-jenkins test this please.\n", "Now we also have avgpooling ops change in.\nDerek, do we need to rebase?\n", "Jenkins, test this please.\n", "I think we shouldn't need to rebase, as this touches a disjoint set of files.\n", "Now it looks like we are having a problem with gen_array_ops:\n\n```\n00:16:42.127     import tensorflow as tf\n00:16:42.127   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\n00:16:42.127     from tensorflow.python import *\n00:16:42.127   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 71, in <module>\n00:16:42.127     from tensorflow.python.framework.framework_lib import *\n00:16:42.127   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\framework_lib.py\", line 99, in <module>\n00:16:42.127     from tensorflow.python.framework.subscribe import subscribe\n00:16:42.127   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\subscribe.py\", line 23, in <module>\n00:16:42.127     from tensorflow.python.ops import array_ops\n00:16:42.127   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 136, in <module>\n00:16:42.127     listdiff.__doc__ = gen_array_ops._list_diff.__doc__ + \"\\n\" + listdiff.__doc__\n00:16:42.127 AttributeError: module 'tensorflow.python.ops.gen_array_ops' has no attribute '_list_diff'\n```\n", "@tensorflow-jenkins test this please.\n", "reader_op_test is fixed, and there seems to be a flake, so:\nJenkins, test this please.\n", "@tensorflow-jenkins test this please.\n", "For everything else, we run contrib tests together with test for core.\nI would like to change that, but for now having them run at the same location seems like the best approach.\n"]}, {"number": 5530, "title": "How to allow GPU memory growth under TF slim?", "body": "Under TF library, we can set how GPU memory is used by creating a config as below and then assign to a session.\r\n\r\n**config = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.Session(config=config)**\r\n\r\nHowever, I can't find any such config to let **TF slim** library to set memory usage mode. Directly put such config to **slim.learning.train()** method?", "comments": ["I just tried, at least this **train()** method does not accept **config** as parameter.\n", "The parameter is `session_config`.\n\nSee here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py#L595\n", "yes, the code below works. Thanks!\n\n**session_config = tf.ConfigProto()\nsession_config.gpu_options.allow_growth = True\nsession = tf.Session(session_config=session_config)**\n", "to make it clear, the right code in tf.slim to allow memory grow is:\r\n```\r\nsession_config = tf.ConfigProto()\r\nsession_config.gpu_options.allow_growth = True\r\nslim.learning.train(..., session_config=session_config)\r\n```"]}, {"number": 5529, "title": "Branch 138793949", "body": "", "comments": ["@benoitsteiner, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @charlesnicholson and @vrv to be potential reviewers.\n", "Jenkins, test this please.\n", "@ispirmustafa Is there a reason why we are deprecating monitors? SessionRunHook doesn't offer all functionalities monitors have and it might be hard for users to do implement their own.\n", "cc: @martinwicke @ilblackdragon I understand that most of these changes are more for internal compatibility and flexibility, but is there any roadmap that you can share by any chance? \n", "The problem with `Monitors` was that they were too specific to the `Estimator` class. We wanted a similar solution for training loops not written as an `Estimator`, so we pulled `Monitor`s further down to attach them more directly to the run calls in a session.\n\nThey should be able to do everything the Monitors did (in fact, they should be able to do more). What functionality is missing?\n", "Thanks for the clarification! That actually makes sense. I missed some functionalities that actually exist so yeah they are able to do more than monitors. Perhaps we need to update the tutorials as well. \n", "yes. A lot of the docs are at least somewhat stale.\n"]}, {"number": 5528, "title": "Add maxout op to tf.contrib.layers", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@gokceneraslan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @zhangyaobit and @martinwicke to be potential reviewers.\n", "Small demo: \n\n![image](https://cloud.githubusercontent.com/assets/1140359/20194275/fb51abb8-a791-11e6-905c-68e53ff7849f.png)\n", "add a reference to the paper? https://arxiv.org/abs/1302.4389 \n", "@ilblackdragon, can you review? ", "@gokceneraslan any luck with this?", "Closing due to inactivity. Please re-open when updates are available.", "@gokceneraslan Would be interested in this feature! Would be cool, if you would finish this PR! :)", "Hi,\r\n\r\nI'm interested in this feature.\r\n\r\nIs it just a matter of implement some test units and improve the documentation?\r\n", "Yes, basically the work to do is to address @ilblackdragon's comments. \r\n\r\nI saw you made a PR, but it was closed, not sure what happened.", "I don't know what happened to; I'll reopen it"]}, {"number": 5527, "title": "Inaccuracies in tf.reduce_sum", "body": "I've been trying to normalize some vectors by dividing by their sum, but have noticed that this doesn't always result in vectors that sum to one. \r\n\r\nFor example:\r\n\r\n```\r\nfoo = np.random.rand(100)*100 #random floats between 0-100\r\nprint(sess.run(tf.reduce_sum(foo/tf.reduce_sum(foo)))) #should return 1\r\nprint((foo/foo.sum()).sum()) #does return 1\r\n```\r\n\r\nWhile I realize that this is possible with finite precision floating point operations, I find it odd how readily it occurs, especially when the same operation in numpy doesn't encounter this issue. Is this a known issue? I've also noticed that tf.nn.softmax doesn't appear to suffer from this problem, is there a different op I need to use to ensure proper normalization?", "comments": ["Met similar problem recently. the following code:\n\n``` python\ndata = np.random.rand(1e4, 1e4) + np.random.rand(1e4, 1)\ndata = data.astype('float32')\nvec = tf.placeholder(tf.float32, data.shape)\ns = tf.reduce_sum(vec)\nwith tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1)) as sess:\n    np_sum = np.sum(data)\n    tf_sum = sess.run(s, feed_dict={vec: data})\n    diff = abs(tf_sum - np_sum)\n    print np_sum, diff\n```\n\ngives an error of sometimes 2e5, which is about 2e-3 error per element. This seems a bit large. \nThe difference is not very large on GPU though. I wonder how the reduction is implemented and what's its difference from numpy.\n", "NumPy uses pairwise summation in some cases:\nhttps://en.wikipedia.org/wiki/Pairwise_summation\n\nNot sure what TensorFlow does on the CPU.\n", "@asimshankar could you please have someone look into this? Thanks!\n", "@ppwwyyxx Thanks for the quick responses! Just wanted to mention that the inaccuracy is still much higher than numpy when running on the GPU, even though it sounds like its less so than TF CPU.\n", "@zergylord @ppwwyyxx I cannot reproduce either of these snippets myself.  In the first snippet, both Numpy and TF return 1.0.  In the second, the error seems to be of the magnitude in O(100), not as drastic as reported.\n\nCould you give some indications on how you built your program and other environment/compiler info?\n", "@zergylord : To add to what @concretevitamin said - if you're seeing this on GPU a lot, do let us know about the CUDA version and the driver version. Mismatches between them often lead to messed up computations.\n", "@concretevitamin Running on GPU my snippet got the error of O(100). But on CPU I have O(1e5).\n\nI'm using the binary at https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc2-cp27-none-linux_x86_64.whl. Tested on both i7-5930K+ubuntu16.04 and i7-6700HQ+Archlinux. My `numpy.__version__ == '1.11.2'`. \n", "One weird thing is that this seems reproducible only non-deterministically.  Sometimes I see negligible error, sometimes I see 1%.\n\nIt seems this is working as intended.  When ~1e8 numbers are being accumulated, at some point the accumulator value becomes >> next value being accumulated (in [0,1]).  On GPU the Eigen impl has\nmore accumulators so this is mitigated a bit.  \"Fixing\" this probably requires significant algo revamp in https://bitbucket.org/eigen/eigen/src/caedc0b002df3c53c265b603c0f537511a88b15f/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h?at=default&fileviewer=file-view-default.\n", "@benoitsteiner, are you planning to improve the numerical accuracy of reduce_sum, or should we mark it contributions welcome. The naive implementation is certainly not \"incorrect\"  or a \"bug\" but it would be better for it to use multiple accumulators, magnitude ordering partial sums or whatever.", "Duplicate of https://github.com/tensorflow/tensorflow/issues/2625.", "Met same problem, any solution?"]}, {"number": 5526, "title": "Add support for broadcasting shapes.", "body": "This PR adds functionality to `TensorShape` to allow for broadcasting. In particular,\r\n\r\n* `TensorShape.broadcast_with(other)` returns the shape resulting from broadcasting a tensor of the given shape with a tensor of shape `other`. It raises a `ValueError` if the shapes are not broadcastable.\r\n* `TensorShape.is_broadcastable_with(other)` determines whether two shapes are broadcastable\r\n* `TensorShape.assert_is_broadcastable_with(other)` asserts that two shapes are broadcastable", "comments": ["@tillahoffmann, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @mrry to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/common_shapes.py#L501\n\nDo you know why this is different from that?\n", "No, they are in fact identical (up to the implementation).\n"]}, {"number": 5525, "title": "AttributeError: 'module' object has no attribute 'dtypes'", "body": "version r0.11\r\n\r\npython 2\r\n\r\nMac OS\r\n\r\ngender = tf.contrib.layers.sparse_column_with_integerized_feature(column_name=\"sex\",\r\n                                                                      bucket_size=2, dtype=tf.dtypes.int16)\r\n\r\n\r\n  File \"/Users/sh1ng/Projects/santander/nn/simple.py\", line 220, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/sh1ng/Projects/santander/nn/simple.py\", line 216, in main\r\n    train_and_eval()\r\n  File \"/Users/sh1ng/Projects/santander/nn/simple.py\", line 208, in train_and_eval\r\n    m = build_estimator(model_dir)\r\n  File \"/Users/sh1ng/Projects/santander/nn/simple.py\", line 75, in build_estimator\r\n    bucket_size=2, dtype=tf.dtypes.int16)\r\nAttributeError: 'module' object has no attribute 'dtypes'", "comments": ["You can leave out the `dtypes`: \n\n``` python\ngender = tf.contrib.layers.sparse_column_with_integerized_feature(\n                    column_name=\"sex\", bucket_size=2, dtype=tf.int16)\n```\n", "@sh1ng please let me know if using tf.int16 fixes your problem.\n", "it's fixed thanks \n", "I encountered with this error about lambda function.\r\n\r\nAttributeError: 'Lambda' object has no attribute 'dtype'\r\n\r\nwhat do i do to solve this problems?", "I enountered error :  `AttributeError: 'DataFrame' object has no attribute 'dtype'` on calling python code in R using `reticulate` package in R. \r\n\r\nThe code in python runs correctly. I am not sure from where this error is coming. I am using pvlib python library to call some in build databases. \r\n\r\n\r\nMy code is for R is: \r\n```\r\nlibrary('reticulate')\r\npd <- import('pandas')\r\npvlib <- import('pvlib')\r\nnp <- import('numpy') \r\nsandia_modules = pvlib$pvsystem$retrieve_sam('SandiaMod')\r\ncec_inverters = pvlib$pvsystem$retrieve_sam(\"CECInverter\")\r\n```\r\n\r\nI have issue with `cec_inverters = pvlib$pvsystem$retrieve_sam(\"CECInverter\")` when I the code in python it's working but running same commands in R is giving me error. I am not sure what the issue is. Please help me resolve this issue. \r\n\r\nThe similar code in python is:\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport pvlib\r\n\r\nsandia_modules = pvlib.pvsystem.retrieve_sam('SandiaMod')\r\ncec_inverters = pvlib.pvsystem.retrieve_sam('cecinverter')\r\n```\r\n"]}, {"number": 5524, "title": "Added Student T CDF as per 5413", "body": "See Issue [5413](https://github.com/tensorflow/tensorflow/issues/5413#issuecomment-259452262). I implemented & tested CDF for Student's T distribution. ", "comments": ["Can one of the admins verify this patch?\n", "@finbarrtimbers, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jvdillon and @langmore to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the review- I made the requested changes!\n", "Jenkins, test this please.\n", "I think I screwed up Jenkins by accidentally pushing to the wrong branch- can you run that again? \n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n", "@finbarrtimbers it looks like there are multiple failing tests, please take a look.\n", "Some of the tests are failing -- can you fix this? If I don't hear from @finbarrtimbers I will close this as stalled soon.", "Sorry, I've been busy with work. I'll fix the failing tests over the weekend, if that's alright.", "Jenkins, test this please.", "Jenkins, test this please.", "Sorry for all the commits- I'm struggling with how to access the `select` and `less` functions from the [Control Flow subsection of the Python API](https://www.tensorflow.org/api_docs/python/control_flow_ops/). I think I have it figured out with the last commit.\r\n\r\nThe cdf test passes on my computer when run outside of Bazel, and I'm having issues running Bazel locally (it's killing my MacBook Air's memory after running for several hours). ", "I understand, building full tensorflow is usually heavy for ultrabooks.\r\nNo problem, I am happy to rerun the tests as needed.\r\nJenkins, test this please.", "When I try to use select, I get this:\r\n```\r\nWARNING:tensorflow:From <stdin>:1 in <module>.: select (from tensorflow.python.ops.math_ops) is deprecated and will be removed after 2016-12-07.\r\n```\r\nSo maybe it is automatically hidden now?\r\nCould you replace select with where, and see what happens?", "Done. ", "Jenkins, test this please.", "Thanks for the help, @gunan and @jvdillon! Super excited to have a commit merged into Tensorflow. "]}, {"number": 5523, "title": "XLA's Dot should follow broadcast semantics from np.matmul, not np.dot", "body": "I notice that the [XLA Dot operation](https://www.tensorflow.org/versions/master/resources/xla_prerelease.html#dot) copies \"outer-product style\" broadcast semantics from `numpy.dot`:\r\n\r\n| Input                                     | Output                | Semantics                      |\r\n|-------------------------------------------|-----------------------|--------------------------------|\r\n| array [p x q x r] `dot` array [s x r x t] | array [p x q x s x t] | array dot product (read below) |\r\n \r\nIn brief, I think this is a mistake. It would be better to follow the \"[matmul style](http://legacy.python.org/dev/peps/pep-0465/#semantics)\" style broadcasting semantics of Python's `@` operation and NumPy's `matmul`.\r\n\r\nmatmul's broadcasting is much more general, and in my opinion, also easier to understand. For example, it can do batch matrix-multiplication, but also can still do outer product style broadcasting if you insert dummy dimensions of length 1 (the axes do end up in a different order), e.g.,\r\nbatch matmul: [p x q x r] `matmul` [p x r x t] -> [p x q x t]\r\nouter product matmul: [p x 1 x q x r] `matmul` [1 x s x r x t] -> [p x s x q x t]\r\n\r\nIf we could go back in time as NumPy developers, we assuredly would change `dot` to work this way (now we cannot, because of backwards compatibility concerns). So it would be nice to change this for XLA before we lock in this behavior.", "comments": ["@cwhipkey, @andydavis1 and @prb12 , could you please comment on this? Thanks.\n", "@shoyer thanks for the suggestion -- we'll think about it\n", "Based on this input and other considerations, we've decided to restrict the semantics of XLA's Dot operation to 1D and 2D arrays in the initial release. We may consider expanding it to higher dimensions in the future, and at that point we'll be considering different possible semantics. For the time being, however, this issue can be closed.\r\n\r\nThanks for the feedback!"]}, {"number": 5522, "title": "Extracting Bazel installation", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nAfter I run `./configure`, why it stagnate in the step of `Extracting Bazel installation...` ? \r\n\r\n### Environment info\r\nOperating System:\r\n`Linux k20-1 2.6.32-358.el6.x86_64 #1 SMP Fri Feb 22 00:31:26 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux`\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\ncuda 7.5\r\ncuDNN 5", "comments": ["@gunan, could you please comment? Thanks.\n", "Doe it get stuck, or does it simply wait a while and then continue?\nDuring configure, we run a bunch of bazel commands, and that is the first thing bazel prints if you run a bazel command.\n", "@guman It remains staying at the step of *_Extracting Bazel installation... *_ now. Continuing about 11 hours. Does it get a stuck? How to confirm it?\n", "@guman During configure, would the bazel download something from internet?\n", "`bazel` is the last thing which runs at the end of `configure`. Could you `strace bazel clean --expunge` and tell us what the last thing it's trying to do?\n", "I think during configure there should not be a download.\nDid you try killing the process and rerunning ./configure?\n\nThe command @drpngx provided might help us get some more information.\nI think there are multiple steps where it can print \"Extracting bazel installation\".\nCould you copy your full temrinal output to pastebin, or a similar site and share that with us?\n", "Ping!\n@linrio Could you try the suggestions above?\n", "Closing the issue due to lack of debug information, and lack of activity.", "FWIW, I have seen similar issues w/ a custom built bazel 0.5.1 on Rhel 6, and the issue was a dangling bazel process. If I make sure no bazel process is alive before I do configure, the issue goes away. Unfortunately, doing configure seems to always keep one dangling bazel process :(\r\n\r\nPlatforms details:\r\n\r\n* rhel 6.5 on x86_64\r\n* bazel 0.5.1 built against rhel 6's openjdk 1.8.0\r\n* configure run as follows: `yes \"\" | ./configure`\r\n* the operation hanging is always bazel clean.\r\n\r\nIt always appear when trying to configure a second time, and one can fixes the hang of bazel clean by either  `rm -rf ~/.cache/bazel` or killing the hanging bazel process.\r\n\r\nDoing an strance on bazel clean shows that otherwise, bazel clean tries to connect to localhost, and hangs around there (I can get a full trace if that is deemed useful)", "Mine is stuck as well, any fix?\r\n\r\nI'm on Ubuntu Subsystem for Windows if that effects anything."]}, {"number": 5521, "title": "Do not require execution of ./configure when changing branches", "body": "It would be great to be able to recompile tensorflow without having to call `configure` after changing branches. Currently, I get the following error message:\r\n\r\n```\r\nRuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/<my branch name>'\r\n```\r\n\r\nThe main problem with running `configure` again is that a clean build is performed afterwards--which takes some time.", "comments": ["@jart had a nice writeup at #5253 regarding bazel clean. I suspect it's related and required. You can probably still use `ccache`. \"Clean\" build time should be reduce to under 2 minutes.\n", "Closing due to lack of activity."]}, {"number": 5520, "title": "Make sure to uninstall the old pip package and install the newly built", "body": "one when running windows tests on TF CI.", "comments": ["both failures seem to be flakes.\nThe consistently failing 4 targets in cmake tests are fixed.\nMerging.\n"]}, {"number": 5519, "title": "Fine tune inception without TFRecords.", "body": "I wonder if I can fine-tune the inception model without using TFRecords format for my dataset in the inception_train script. I have a placeholder for images and a placeholder for labels and during training I feed them in sess.run(). I get low precision on a binary classification problem so I wonder if this approach is wrong. In stackoverflow noone answered. Thanks in advance!", "comments": ["Hi @chrisrn ,\n\nYou should be able to as long as you provide the images in the same format and distribution as expected by the pre-trained model. By \"as expected\" I meant if the values used for training were in the range of 0-255, your dataset will need to be in the same range.\n\nSherry\n", "I feed the images with size batch_size x 299 x 299 x 3 and values in range [-1, 1] like the image_processing script. Am I right? I need to confirm that I don't feed them wrong in order to proceed in next steps of training.\n", "Questions like this do belong on Stackoverflow, since this isn't a bug report or feature request.\n"]}, {"number": 5518, "title": "Op type not registered 'SqrtGrad'", "body": "I am unable to create the Tensorflow graph from an imported model on Android using JNI. When attempting to load the file with `tensorflow::Status s = session->Create(graph_def);`, I receive the error, `Not found: Op type not registered 'SqrtGrad'`.\r\n\r\nI checked `jni-build/jni/include/tensorflow/python/ops/math_ops.py` and `SqrtGrad` does exist. As running the model on Python does not return this error, could this be a problem when I generated my protobuf file with `tf.train.write_graph(sess.graph_def, 'location', 'trained_model.pb', as_text=False)`?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/3619\r\nhttps://github.com/tensorflow/tensorflow/issues/3533\r\n\r\n### Environment info\r\nOperating System:\r\nMac OS 10.12\r\nAndroid Cyanogenmod 12\r\nTensorflow 0.10.0 on Jupyter\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThis section of code is from [TensorFlowAndroidMNIST](https://github.com/miyosuda/TensorFlowAndroidMNIST/tree/master/jni-build/jni)\r\n\r\n[trained_model.pb.zip](https://github.com/tensorflow/tensorflow/files/583485/trained_model.pb.zip)\r\n```\r\nstatic std::unique_ptr<tensorflow::Session> session;\r\n\r\nstatic bool g_compute_graph_initialized = false;\r\nusing namespace tensorflow;\r\n\r\nJNIEXPORT jfloat JNICALL\r\nTENSORFLOW_METHOD(init)(JNIEnv* env, jobject thiz, jobject java_asset_manager, jstring model) {\r\n\r\n\tconst char* const model_cstr = env->GetStringUTFChars(model, NULL);\r\n\r\n\ttensorflow::SessionOptions options;\r\n\ttensorflow::ConfigProto& config = options.config;\r\n\r\n\tsession.reset(tensorflow::NewSession(options));\r\n\r\n\ttensorflow::GraphDef graph_def;\r\n\tAAssetManager* const asset_manager = AAssetManager_fromJava(env, java_asset_manager);\r\n\r\n\tReadFileToProto(asset_manager, model_cstr, &graph_def);\r\n\ttensorflow::Status s = session->Create(graph_def);\r\n\r\n\tif (!s.ok()) {\r\n\t\tLOG(ERROR) << \"Could not create Tensorflow Graph: \" << s;\r\n\t\treturn -1;\r\n\t}\r\n}\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nI have tried adding `tensorflow/contrib/*`to `tf_op_files.txt` and `\"//tensorflow/contrib/all_files\"` to the tensorflow/BUILD file.\r\n", "comments": ["Hi @leslietso ,\n\nBy default we only have inference ops on Android, however there's a way that you can add training ops. @petewarden will address this for you.\n\nSherry\n", "Hi @leslietso,\n\nIf you're using Bazel, you should add cwise_op_sqrt.cc to the srcs list of :android_extended_ops_group_2:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L2490\n\nIf you're using the makefile, add this line to tensorflow/contrib/makefile/tf_op_files.txt:\ntensorflow/core/kernels/cwise_op_sqrt.cc\n\nThe reason that this isn't present by default is that most applications don't do training on mobile devices, and removing the implementations of those ops shrinks the binary size. If you're not running training, you might want to run some tools like optimize_for_inference to strip out training ops. I've attached some draft documentation on that process.\n[deploying.txt](https://github.com/tensorflow/tensorflow/files/584688/deploying.txt)\n", "You are correct in saying that I do not want to train on my Android device. \n\nIn order to run `optimize_for_inference`, I wanted to first run `freeze_graph` mentioned in the \"deploying.txt\" file you posted. However, I get the error `UnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in position 331: invalid start byte` when attempting to run the bash script:\n\n```\n#!/bin/bash\nbazel-bin/tensorflow/python/tools/freeze_graph \\\n--input_graph=/Users/leslie/Downloads/trained_model.pb \\\n--input_checkpoint=/Users/leslie/Downloads/Y6_1478303913_Leslie \\\n--output_graph=/tmp/frozen_graph.pb --output_node_names=Y_GroundTruth\n```\n\nCould this be a problem in the way I created my graph and checkpoint? I created the input_graph via `tf.train.write_graph(sess.graph_def, location, 'trained_model.pb', as_text=False)` and the checkpoint is created via `saver.save(sess, chkpointpath)`. Answers from StackOverflow say that the python script has non-ascii characters and that I should just simply strip them from the python script but I do not think that is such a great idea.\n\n**Full traceback:**\n\n```\nTraceback (most recent call last):\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 135, in <module>\n    tf.app.run()\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 132, in main\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 98, in freeze_graph\n    text_format.Merge(f.read().decode(\"utf-8\"), input_graph_def)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.py\", line 16, in decode\n    return codecs.utf_8_decode(input, errors, True)\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x96 in position 331: invalid start byte\n```\n\n**UPDATE:**\nI generated my protobuf file with `as_text = True` and the error above did not show up. However, I only got the following output.\n\n```\nConverted 0 variables to const ops.\n1 ops in the final graph.\n```\n\nComplete contents of \"frozen_graph.pb\"\n\n```\n6\n\nY_GroundTruth\u0012\u000bPlaceholder*\u000b\n\u0005dtype\u0012\u00020\u0001*\u000b\n\u0005shape\u0012\u0002:\n```\n\nFor completeness sake I tried `freeze_graph` with my metagraph, but I received a similar error.\n\n```\nTraceback (most recent call last):\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 135, in <module>\n    tf.app.run()\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 43, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 132, in main\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\n  File \"/Users/leslie/tensorflow-master/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 98, in freeze_graph\n    text_format.Merge(f.read().decode(\"utf-8\"), input_graph_def)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/encodings/utf_8.py\", line 16, in decode\n    return codecs.utf_8_decode(input, errors, True)\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xc4 in position 1: invalid continuation byte\n```\n", "Just wanted to say that it was a problem with how the protobuf file was generated (the variables in the protobuf file were not correctly initiated).\n\nHowever, I am getting another error when attempting to load the protobuf file on Android but I will open that as another issue.  \n", "Closing due to lack of recent activity. Since this issue is quite old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thanks."]}, {"number": 5517, "title": "Don't support avgpooling op on Windows ?", "body": "## Environment info\r\nOperating System: Windows 10, 64bit, Visual Studio 2015\r\n\r\n## Issue\r\nI try compile tensorflow label image sample on windows 10 64bit with Visual Studio 2015.\r\nI got error 'AvgPool' op kernel not found message while loading tensorflow_inception_graph.pb graph.\r\nI check tf_core_kernels.cmake and found follow line.\r\n\r\n```\r\nif(WIN32)\r\n  file(GLOB_RECURSE tf_core_kernels_windows_exclude_srcs\r\n      # not working on windows yet\r\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/depthwise_conv_op.cc\"  # Cannot find symbol: tensorflow::LaunchConv2DOp<struct Eigen::ThreadPoolDevice, double>::launch(...).\r\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/fact_op.cc\"\r\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/meta_support.*\"\r\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/*quantiz*.h\"\r\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/*quantiz*.cc\"\r\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/svd*.cc\"\r\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/avgpooling_op.*\"\r\n  )\r\n  list(REMOVE_ITEM tf_core_kernels_srcs ${tf_core_kernels_windows_exclude_srcs})\r\nendif(WIN32)\r\n```\r\n\r\nIs this mean don't support above ops on windows?\r\nHow can I running graph that using these ops?", "comments": ["At least in the initial version, `avgpooling_op.cc` did not compile with MSVC. I'm not at a computer right now, but could you try commenting out the line \n\n```\n      \"${tensorflow_source_dir}/tensorflow/core/kernels/avgpooling_op.*\"\n```\n\nfrom the `tf_core_kernels_windows_exclude_srcs` definition you pasted above, and report the error (if any)?\n", "I had try the add avgpooling_op.cc into tf_core_kernels project manually and build it.\nBut I got the last error\n`\n1>------ Build started: Project: tf_core_kernels, Configuration: Debug x64 ------\n\n...\n\n1>tf_core_kernels.dir\\Debug\\tf_core_kernels.lib : fatal error LNK1248: image size (1017EA054) exceeds maximum allowable size (FFFFFFFF)\n`\nHow can I build it?\n", "The best bet is to build it in the`Release` or `RelWithDebInfo` configurations, which are the only two we support. (`Debug` has some compatibility issues with the behavior of STL iterators in MSVC... and apparently some image-size problems too!)\n\nI've just sent out PR #5532, which adds the CPU version of `avgpooling_op.cc` to the Windows build, and the tests are passing.\n"]}, {"number": 5516, "title": "TensorFlow is 1.3-7 times slower than Theano for small models", "body": "I created a small benchmark (https://github.com/wjaskowski/tensorflow-vs-theano-benchmark) which shows that, depending on the model architecture, Tensorflow is 1.3-7 times slower than Theano depending on the model architecture. The question is whether this effect is due to a design decision or a performance bug, which may be solved in the future?\r\n\r\nRelated: https://github.com/fchollet/keras/issues/4287 , #5422", "comments": ["@zhangyaobit, please take a look.\n", "One first thing I'd check is that for convolution if NCHW (faster) format or NHWC format is used. To find out the causes of performance discrepancies, you will need to profile Tensorflow and Theano and compare the two in terms of where the time is spent. For TensorFlow, you can use Timeline (http://stackoverflow.com/questions/36123740/is-there-a-way-of-determining-how-much-gpu-memory-is-in-use-by-tensorflow/37931964#37931964) to find out the time breakdown for all nodes/operations in a TensorFlow graph.\n", "I use `tf.contrib.layers.convolution2d(input,...)`, where input is `[batch_size, height, width, channels]` according to the documentation. Do you suggest that this is wrong, and I should use [batch_size, channels, height, width]?\n\nThe code for tensorflow is dead simple:\n\n``` python\ndef create_tf(image_shape, output_dim, layers_conf):\n    from tensorflow.contrib import layers\n\n    x = tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], 1], name=\"input\")\n    t = tf.placeholder(tf.float32, shape=[None, output_dim], name=\"target\")\n\n    net = x\n    for num_filters in layers_conf[:-1]:\n        net = layers.conv2d(net, num_outputs=num_filters, kernel_size=3, stride=3, activation_fn=tf.nn.relu,\n                weights_initializer=layers.xavier_initializer_conv2d(), biases_initializer=tf.constant_initializer(0.1))\n    net = layers.flatten(net)\n    net = layers.fully_connected(net, num_outputs=layers_conf[-1], activation_fn=tf.nn.relu, weights_initializer=layers.xavier_initializer(),\n            biases_initializer=tf.constant_initializer(0.1))\n    y = layers.fully_connected(net, num_outputs=output_dim, activation_fn=None, weights_initializer=layers.xavier_initializer(),\n            biases_initializer=tf.constant_initializer(0.1))\n    mean_square_error = 0.5*tf.reduce_mean((y - t)**2)\n    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss=mean_square_error)\n\n    def backprop(x_batch, y_batch):\n        optimizer.run({x: x_batch, t: y_batch})\n\n    def fwd_pass(x_batch):\n        return y.eval({x: x_batch})\n\n    return fwd_pass, backprop\n```\n\nI am willing to profile it I this could help you. I would not spend time on it, however, if you acknowledge that such performance gap is expected. So is this expected or not?\n", "NHWC is not wrong. I'm just saying it might be a slower option than NCHW. You can try HCHW by adding data_format=\"NCHW\" to layers.conv2d, and change the shape accordingly, and also make sure other ops can accept this format.\n\nThis kind of slow down (1.3-7x) is common when it comes to comparing NCHW and NHWC, and might be the reason why it is slower than Theano.\n\nWhile we are working on hiding this kind of manual tuning from users (e.g. selecting a better data_format for a op and a device), users are still expected to spend time to understand and tune the  performance, and the first basic step towards that is profiling.\n", "Btw. TF is ca. 3 times slower than Theano also where the model involves only dense layers (no convolutions).\n", "Could you post the code and the Timeline profiling results?\n", "HCHW did not help. The benchmark code is here: https://github.com/wjaskowski/tensorflow-vs-theano-benchmark. Here are the Timeline results:\n**1. 128x128 images, 3 conv layers 32 filters each + 128 neurons [Theano 2.5 times faster]**\n<img width=\"1166\" alt=\"128x128\" src=\"https://cloud.githubusercontent.com/assets/4952605/20236490/fc13f594-a8b6-11e6-98b3-440fd65d305d.png\">\n[timeline-128 128 32 32 32 128.txt](https://github.com/tensorflow/tensorflow/files/587125/timeline-128.128.32.32.32.128.txt)\n**2. 32x32 images, 3 conv layers 32 filters each + 512 neurons [Theano 5 times faster]**\n<img width=\"1153\" alt=\"32x32\" src=\"https://cloud.githubusercontent.com/assets/4952605/20236491/08ed25b0-a8b7-11e6-9e15-f2389686b81d.png\">\n\n[timeline-32 32 32 32 32 512.txt](https://github.com/tensorflow/tensorflow/files/587126/timeline-32.32.32.32.32.512.txt)\n", "@wjaskowski, could you make sure you exclude the first few iterations from the benchmark? This is how convnet-benchmark does it. \n\nhttps://github.com/soumith/convnet-benchmarks/blob/master/tensorflow/benchmark_alexnet.py#L139\n\nIn the first round, TensorFlow does autotuning to find out the best algorithm, so it might have more overhead. Following up rounds should not have them. For convnet benchmarks, important things are: \n- NCHW format\n- Excluding first few rounds, typically 10. \n- Make sure your session run didn't fetch anything unnecessarily.\n", "@zheng-xq:\n- I exclude the first 50 rounds (https://github.com/wjaskowski/tensorflow-vs-theano-benchmark/blob/master/benchmark.py#L74). \n- I have checked that NCHW improves the results only slightly (as a margin note, I find it strange that you recommend NCHW while NHWC is the default one).\n- I cannot imagine what unnecessary could I possibly fetch in this simple example.\n", "Looks like your Timeline is a profiling run that tries multiple conv2d implementations to find the best one, rather than a later run that uses the previously found best implementation. \n\nIt might be that somehow the benchmark is not written in a standard way, and as a result it is always running the profiling runs.\n\nAs you see, each conv layers actually invoked multiple runs:\n\n![apy0wpg5toh](https://cloud.githubusercontent.com/assets/1034716/20285252/b46c6896-aa75-11e6-815d-25be068acf6c.png)\n\nAfter this is fixed, you can try NCHW again. Because I'm thinking your previous comparison might be between  NCHW and NHWC in the profiling mode, rather than in the regular mode.\n", "@wjaskowski there's a fixed per-run overhead in TensorFlow which is higher than in Theano, and when your session.run only takes 1.6ms, this overhead is significant. One source of overhead is \"crossing TensorFlow/Python\" boundary which happens in TF that has C++ runtime, but not Theano which has Python runtime. For your TF experiment, every 1.6ms you release GIL, copy data from Python runtime to C++ TensorFlow runtime, then copy the result back and reacquire GIL. For an op that does no memory transfers and sees no graph modifications , each eval call will add 0.2ms overhead. \n\nTF has been optimized for cases when session run call takes 20ms-2000ms, so these tiny run call scenarios didn't get as much attention, however there are some tricks you can do. For instance to remove the extra Python->TF copy, you can keep the data in tensorflow runtime using variables or queues. To eliminate extra TF->Python copies, you can keep data in tensorflow runtime by doing `sess.run(optimizer_node.op)` instead of `sess.run(optimizer_node)` in your loop\n", "I run the benchmark and found out your profiling results is fine, which is an actually a regular run, instead of a profiling run, which I thought earlier it is. \n\nThis now indeed looks like a TensorFlow internal issue; the CPU overhead seems high for smaller shapes (32x32) and we will investigate that. We will also look at the performance for NCHW format.\n", "@yaroslavvb I see. Indeed, my first profiling results shows the overhead of fetching the data. When I removed it, TF/TH time ratio decreased to 2-2.5. I also confirm that when the model is larger (e.g. 512 filters, the difference between TH and TF shrinks. E.g., for \"128x128 + conv[512,512,256,128] + 512\") I got a ratio of 1.5 for forward pass and 0.85 for backprop (i.e. TF is actually faster).\n\nBut still, it would be nice to make TF also quick for smaller models. In reinforcement learning, I cannot afford huge models. It is also not so easy to prefetch data to TF since the data are generated on-the-fly (but maybe with the queues?).\n", "@yaroslavvb @wjaskowski +1 regarding the fact that one can't dismiss poor performance for small models just because no one cares about MNIST and CIFAR-10 (and their corresponding small convnets) anymore. The state-of-the-art RL models are rather small and so the importance of adequate performance for small models is still an issue to address for current research and production use cases.\n", "I did more CPU profiling and found that the majority of the CPU overhead is actually from the host code inside the conv op, rather than from the executor. The CPU time is distributed across pre- and post- processing steps such as padding and filter transform, as well as in the cuDNN kernel invocation. There doesn't seem to be a single \"performance bug\" that could be fixed here, but the overall CPU side of the implementation of conv could be improved.\r\n\r\nConv actually internally converts NHWC to NCHW, so using NCHW directly would avoid the conversion. However, for small models, as the performance is bottlenecked by the CPU code, using NCHW makes little performance difference. \r\n\r\nI marked the issue as contribution welcome for interested people to take a deeper look.", "@benoitsteiner, here is an interesting example the performance of NCHW and NHWC is about the same, as the CPU is the bottleneck. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I am using Tensorflow 1.3 and it is %25 slower than MatConvNet, an alternative deep learning library for Matlab. I can fit more batches of data(2x) into GPU in the \"MatConvNet\" implementation and i think this is one of the most important factor in the performance difference.\r\n\r\nIn tensorflow implementation, queue runners, multi tower gpu implementation and all best practices are used for getting the best performance.\r\n\r\nTried many different scenarios to improve results such as using different data formats, NHWC and NCHW, allow_growth etc..\r\n\r\nI expect Tensorflow as way much faster according to the cumbersome \"Matlab\" architecture. Currently, TF with python, consumes much more memory and uses GPU less efficient than Matlab alternative. This is very interesting.\r\n\r\nTraining Data : 2165x5x5x145\r\nModel :\r\nconv2d([1x1, 1200])\r\nrelu\r\nconv2d([3x3, 600])\r\nrelu\r\nconv2d([5x5, 300])\r\nrelu\r\nfc\r\n\r\nThis issue should be reopened as TF performs worse many other libraries out there regardless of their platforms", "There are many cases when TensorFlow is slower than something else. Sometimes it's due to user error, sometimes not. A self-contained benchmark would be needed to troubleshoot this correctly.\r\n\r\nBTW, queue runners don't have great performance, tf.data should be better", "@yaroslavvb if you think the tf.data should close the %25 performance gap between other frameworks, i can try it. But how could you explain the 2x batch size difference between two frameworks ? Is there any hints, data pinning or something like that ?\r\n\r\nExcuse me but, my comparison is not between top-competitors(Caffe, Caffe2, CNTK, Theano etc..) of the arena. This is a Matlab based third party library not backed by an large scale company or community. Also Matlab is another computation platform which is heavier and slower according other native or managed platforms. I think this issue should be considered more seriously and any inspection regarding such a critical performance issue should not be handled case case but on the overall implementation of the TF library.", "@aligokalppeker I do not find it surprising that TensorFlow code may run slower than Matlab for a particular example, TensorFlow gives a lot more opportunities for the user shooting themselves in the foot", "@yaroslavvb I think, the your latest comment has been written with no development experience/knowledge on Matlab(or you write such a meaningless thing intentionaly). With such a unacceptable approach, decline on TF's performance and quality will continue. This bug is an evidence of such a bad progress; https://github.com/tensorflow/tensorflow/issues/14107\r\n\r\nAt least they accepted the problem in the bug, they did not bury their heads in to the sand...", "\"Low-performance\" is a bit of an over-simplification. Improvements in TensorFlow performance translates directly to datacenter cost savings, so Google spends significant resources in this area. What's happening is that some kinds of computations  are optimized, while others, especially ones small enough to run in Matlab, are not. I think eventually, most computations will be possible to do in TensorFlow without incurring high performance/human cost, but what can help in getting there faster is providing self-contained benchmarks that demonstrate the problem."]}, {"number": 5515, "title": "Supervisor should_stop not working in TF distributed", "body": "Running TensorFlow 0.11.0rc2 and following the example (\"Putting it all together: example trainer program\"): https://www.tensorflow.org/versions/r0.11/how_tos/distributed/index.html\r\n\r\nThe workers does not get a should_stop() signal from the master.\r\n\r\nRepro:\r\n\r\ntrainer.py: https://gist.github.com/hholst80/492cfeaad041db7580fa6ddf4480dce1\r\nstart: https://gist.github.com/hholst80/892ae7ed3d8db560647a03253c972de1", "comments": ["@hholst80 Please include the full error message when you file issues.\n", "I updated the trainer.py and start script to make it more clear what is _not_ happening. As we see task 1 never gets a `sv.should_stop()` signal as I would expect.\n\n```\n(tensorflow) hholst@fb-hholst3:/tmp/bug$ cat worker0.log \nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2000}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3000, 1 -> localhost:3001}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:3000\nI tensorflow/core/distributed_runtime/master_session.cc:928] Start master session c878251f6d23eadc with config: \n\ntask 0 should_stop = True\ntask 0 calling sv.stop()\n(tensorflow) hholst@fb-hholst3:/tmp/bug$ \n```\n\n```\n(tensorflow) hholst@fb-hholst3:/tmp/bug$ cat worker1.log \nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2000}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3000, 1 -> localhost:3001}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:3001\nI tensorflow/core/distributed_runtime/master_session.cc:928] Start master session 1b67ef1b14af632b with config: \n\n(tensorflow) hholst@fb-hholst3:/tmp/bug$ \n```\n\n```\n(tensorflow) hholst@fb-hholst3:/tmp/bug$ cat ps0.log \nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2000}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3000, 1 -> localhost:3001}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:206] Started server with target: grpc://localhost:2000\n(tensorflow) hholst@fb-hholst3:/tmp/bug$ \n```\n", "I tried with TensorFlow 0.11 just released and the problem is the same.\n", "@sherrym Can you take a look?\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}]