[{"number": 20570, "title": "Slow tf.hessians slicing", "body": "Hi, I'd like to calculate the diagonal matrix of some hessian:\r\n\r\nI've got some variables `a, b` of shape `(5, <num_features>)` and I'd need for each feature the hessian's diagonal:\r\n\r\n```python3\r\n>>> [self.a, self.b]\r\n[<tf.Tensor 'Linear_Batch_Model/a/concat:0' shape=(5, 10000) dtype=float32>,\r\n <tf.Tensor 'Linear_Batch_Model/b/concat:0' shape=(5, 10000) dtype=float32>]\r\n\r\n>>> tf.hessians(self.full_loss, [self.a, self.b])\r\n[<tf.Tensor 'Reshape_1:0' shape=(5, 10000, 5, 10000) dtype=float32>,\r\n <tf.Tensor 'Reshape_3:0' shape=(5, 10000, 5, 10000) dtype=float32>]\r\n\r\n>>> [tf.diag_part(t) for t in tf.hessians(self.full_loss, [self.a, self.b])]\r\n[<tf.Tensor 'DiagPart_3:0' shape=(5, 10000) dtype=float32>,\r\n <tf.Tensor 'DiagPart_4:0' shape=(5, 10000) dtype=float32>]\r\n```\r\n\r\n**However, this calculation seems to run infinitely:**\r\nThe full hessian matrices are huge, but since I only take the diagonal elements, they should never be calculated explicitly.\r\n\r\n- I can calculate the diagonal by hand (`gradient(gradient(loss, [a,b]), [a,b])`), but I always expected tensorflow to optimize such operations implicitely...\r\n- Also, this trick does not work if I'd need other parts of the Hessian matrix, e.g. if I need for each independent gene the `parm x parm`-shaped hessian:\r\nTF does not allow to differentiate only slices of a and b (e.g. `tf.hessians(loss, [a[:, 0], b[:,0]])`)\r\n\r\n.\r\n\r\n### The problem\r\nTensorflow seems to explicitly calculate the full hessian matrix\r\n\r\n### What I'd expect\r\nFast calculation since I only request the diagonal\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: none", "comments": ["@Hoeze,\r\nSorry for the delayed response. Please find the example code for [Diagonal Hessian](https://www.tensorflow.org/probability/api_docs/python/tfp/math/diag_jacobian#example) and let us know if this is what you are looking for. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20569, "title": "Tensorflow 1.8.0 ALWAYS looking for libcublas.so.9.0 with Cuda 9.1, Cudnn 7.1.12 (and libcublas.so.9.1)", "body": " **System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary (via virtualenv)\r\n- **TensorFlow version (use command below)**: tensorflow-gpu 1.8.0\r\n- **Python version**: Python 3.6.5\r\n- **CUDA/cuDNN version**: CUDA 9.1/CuDNN 7.1.12\r\n- **GPU model and memory**: NVIDIA TITAN X (Pascal)/64 GB DDR4\r\n- **GPU driver**: 390.48\r\n- **Exact command to reproduce**:  import tensorflow as tf\r\n\r\n### Describe the problem\r\nTensorflow is looking for libcublas.so.9.0. When I run \"import tensorflow as tf\" inside my virtual environment, I get the following error message:\r\n\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\nFailed to load the native TensorFlow runtime.\r\n\r\nOutputs of: Idconfig -v\r\n\r\n/sbin/ldconfig.real: Can't stat /usr/local/lib/i386-linux-gnu: No such file or directory\r\n/sbin/ldconfig.real: Can't stat /usr/local/lib/i686-linux-gnu: No such file or directory\r\n/sbin/ldconfig.real: Can't stat /lib/i686-linux-gnu: No such file or directory\r\n/sbin/ldconfig.real: Can't stat /usr/lib/i686-linux-gnu: No such file or directory\r\n/sbin/ldconfig.real: Can't stat /usr/local/lib/x86_64-linux-gnu: No such file or directory\r\n/sbin/ldconfig.real: Can't stat /usr/local/cuda-9.0/lib64: No such file or directory\r\n/sbin/ldconfig.real: Path `/lib/x86_64-linux-gnu' given more than once\r\n/sbin/ldconfig.real: Path `/usr/lib/x86_64-linux-gnu' given more than once\r\n/usr/lib/x86_64-linux-gnu/libfakeroot:\r\n\tlibfakeroot-0.so -> libfakeroot-tcp.so\r\n/lib/i386-linux-gnu:\r\n\tlibgcc_s.so.1 -> libgcc_s.so.1\r\n\tlibnsl.so.1 -> libnsl-2.27.so\r\n\tlibresolv.so.2 -> libresolv-2.27.so\r\n\tlibnss_files.so.2 -> libnss_files-2.27.so\r\n\tlibSegFault.so -> libSegFault.so\r\n\tlibdl.so.2 -> libdl-2.27.so\r\n\tlibutil.so.1 -> libutil-2.27.so\r\n\tlibnss_nis.so.2 -> libnss_nis-2.27.so\r\n\tlibrt.so.1 -> librt-2.27.so\r\n\tlibcidn.so.1 -> libcidn-2.27.so\r\n\tlibnss_nisplus.so.2 -> libnss_nisplus-2.27.so\r\n\tlibc.so.6 -> libc-2.27.so\r\n\tlibBrokenLocale.so.1 -> libBrokenLocale-2.27.so\r\n\tlibnss_dns.so.2 -> libnss_dns-2.27.so\r\n/sbin/ldconfig.real: /lib/i386-linux-gnu/ld-2.27.so is the dynamic linker, ignoring\r\n\r\n\r\n/usr/local/lib:\r\n\tlibcublas.so.9.1 -> libcublas.so.9.1\r\n\r\n/usr/lib/x86_64-linux-gnu:\r\n/sbin/ldconfig.real: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2 is for unknown machine 21.\r\n\r\n/sbin/ldconfig.real: /usr/lib/x86_64-linux-gnu/libcudnn.so.7 is for unknown machine 21.\r\n\r\n/sbin/ldconfig.real: /usr/lib/x86_64-linux-gnu/libcudnn.so is for unknown machine 21.\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/meriemlio/tensorflow/venv/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n", "comments": ["Maybe those builds could help you: https://github.com/hadim/docker-tensorflow-builder#builds\r\n\r\nThere is different builds according to CUDA/cuDNN.", "Thanks Hadim. So in my case, I am using Ubuntu 18.04 with CUDA 9.1 and cuDNN 7.1. I do not see builds for Ubuntu 18.04 there. ", "The one for ubuntu 16.04 should work.", "This is still an issue. I still have not tested the above-mentioned proposed solution.", "I have the same problem with tensorflow 1.9 and cuda 9.2, and the repo that was suggested by @hadim does not list anything for my system (Ubuntu 18.04, AMD Threadripper). ", "The same issue; \r\nUbuntu 18.04; Intel i7-7820X .\r\ncuda 9.2\r\ntensorflow-gpu 1.9.0\r\ncudnn 7.2 (also tried 7.1).", "Same issue Ubuntu 18.04\r\nCUDA9.2\r\ncuDNN v7.2.1\r\ntensorflow-gpu 1.10, also tested with 1.8\r\nAll libraries expected by tensorflow are 9.0 instead of 9.2, (e.g. libcublas.so.9.0, libcurand.so.9.0, libcufft.so.9.0, libcufft.so.9.0, etc.\r\n\r\nI'll appreciate your guide to know how to solve this issue.\r\n", "same issue with CUDA 9.2, cuDNN 7.2.1, Ubuntu 18.04", "I finally soved it with compiling Tensorflow1.8 from sources.\r\nI can share my procedure for you to try it.", "Oh yes please do !", "Here it is! Just provide your feedback in my Gist. I hope it is usefule for you.  Let me know. \r\nhttps://gist.github.com/ljaraque/d18d3dd198dcff3bc40cbe91889564d0  \r\nThanks!", "So I had the same problem too, especially I already installed CUDA 9.0. But I finally got what I did wrong. What we should do is to add these two lines to ~/.bashrc:\r\n\r\n> So Tensorflow can find your CUDA installation and use it properly, you need to add these lines to the end of you ~/.bashrc or ~/.zshrc.\r\n> \r\n> export PATH=/usr/local/cuda-9.0/bin${PATH:+:${PATH}}\r\n> export LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n> \r\nAnd then restart the computer. WOLA.\r\nThe entire process to install CUDA, cuDNN and TF is given by this article: https://medium.com/@taylordenouden/installing-tensorflow-gpu-on-ubuntu-18-04-89a142325138\r\nI spotted what I did wrong by watching this video: https://www.youtube.com/watch?v=vxjbL5iN1XY&list=PLAw9d7BrrnER1BZc7upmxwqD8WBbLTRsD&index=7&t=740s", "Yep good. The issue from my previous post arises when you want to use CUDA9.2", "True; the issue is related to the latest versions (as of the date of this\npost); CUDA 9.0 and previous cuDNNs work fine.\n\nOn Fri, 31 Aug 2018 at 20:06, Luis Jaraquemada <notifications@github.com>\nwrote:\n\n> Yep good. The issue from my previous post arises when you want to use\n> CUDA9.2\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20569#issuecomment-417761995>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEF2-XZB91WG1Ms0Li1vCMUXhuvq2W83ks5uWYlGgaJpZM4VEGly>\n> .\n>\n-- \n*---------------------------------------------*\n*This is a private email account. Do not send messages to it that contain\npatient identifiable data or if your organisation has concerns regarding\nthe use of data.*\n", "I am facing the same errors with CUDA-9.1 and cannot install a newer package or an older package since I dont have admin privileges, how can one fix the tensorflow installation issues?", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As our installation pages list, prebuilt TF pip packages on pypi are built with CUDA 9.0 support only:\r\nhttps://www.tensorflow.org/install/gpu\r\n\r\nFor TF with other CUDA versions, you can either build from sources, or look into community supported packages built by Anaconda, or other members of the community.", "According to Nvidia website (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html ), Ubuntu 18.04 requires CUDA version 10.0 (with its compatible cuDNN library). This version of CUDA is not supported by tensorflow-gpu version 1.8.0\r\n\r\nDo I need to install tensorflow for source? \r\n\r\nPlease advice", "What was the actual solution here?", "Following the GPU install instructions here:\r\n\r\nhttps://www.tensorflow.org/install/gpu\r\n\r\nworks fine on \"Ubuntu 18.04.1 LTS\"\r\n", "You can stick to this step by step procedure I composed if you need to make tensorflow-gpu and cuda work if the out of the box versions don't work for you:\r\nhttps://gist.github.com/ljaraque/d18d3dd198dcff3bc40cbe91889564d0 ", "Thank you both! @jacob-jarick  your link from section \"Install CUDA with apt\" worked like a charm.", "What about arch linux?", "Having \r\n`http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1704/x86_64` \r\nin source.list and issuing\r\n`sudo apt install cuda-toolkit-9-0`\r\nwill solve the issue to 18.04 installation too (with outdated lib of course)."]}, {"number": 20568, "title": "CMake: Build break occurs when making tf_python_build_pip_package using cmake.", "body": "### Problem\r\nBuild break occurs when making __tf_python_build_pip_package__ using __cmake__ instead of Bazel.\r\n\r\n### System information\r\n* Have I written custom code: N/A\r\n* OS Platform and Distribution: Ubuntu 16.04.4 LTS\r\n* TensorFlow installed from: Not installed\r\n* TensorFlow version: N/A\r\n* Python version: 2.7.12\r\n* Bazel version: Not used since CMake build\r\n* CMake version: 3.5.1\r\n* GCC/Compiler version: 5.4.0\r\n* CUDA/cuDNN version: 9.0 (But not important in this case)\r\n* GPU model and memory: Nvidia 1080ti / 11G\r\n* Exact command to reproduce: refer the below 'How to reproduce'\r\n\r\n### Source code information\r\n* Branch: r1.9\r\n* Commit ID: e3f2b5903c56ec09fa501dd8e95dd0e4336843fc (latest when I built)\r\n* Just clone without modification\r\n\r\n### How to reproduce \r\n```bash\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\n$ cd tensorflow\r\n$ git checkout -b cmake_build_test e3f2b5903c56ec09fa501dd8e95dd0e4336843fc\r\n$ mkdir build\r\n$ cd build\r\n$ cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\n$ make VERBOSE=1 -j4  tf_python_build_pip_package &> cmake_build_log.txt\r\n```\r\n\r\n### Error message\r\n* ImportError: No module named tensorflow.python.util\r\n* Download the full build log message: [link](https://www.dropbox.com/s/5hfgqugzbwwu3i2/cmake_build_log.txt?dl=0)\r\n```bash\r\nmake[3]: Entering directory '/home/again4you/tensorflow/build'                  \r\n[100%] Generating __init__.py files for Python API.                             \r\ncd /home/again4you/tensorflow/build/tf_python && /usr/bin/cmake -E remove -f /home/again4you/tensorflow/build/tf_python/tensorflow/__init__.py\r\ncd /home/again4you/tensorflow/build/tf_python && /usr/bin/cmake -E env PYTHONPATH=/home/again4you/tensorflow/build/tf_python /usr/bin/python /home/again4you/tensorflow/build/tf_python/tensorflow/tools/api/generator/create_python_api.py --root_init_template=/home/again4you/tensorflow/build/tf_python/tensorflow/api_template.__init__.py --apidir=/home/again4you/tensorflow/build/tf_python/tensorflow /home/again4you/tensorflow/api_init_files_list.txt\r\nTraceback (most recent call last):                                              \r\n  File \"/home/again4you/tensorflow/build/tf_python/tensorflow/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.util import tf_decorator                             \r\nImportError: No module named tensorflow.python.util                             \r\nCMakeFiles/tf_python_api.dir/build.make:57: recipe for target 'tf_python/tensorflow/__init__.py' failed\r\nmake[3]: *** [tf_python/tensorflow/__init__.py] Error 1                         \r\nmake[3]: Leaving directory '/home/again4you/tensorflow/build'\r\n```\r\n\r\n### Detailed reason that I found\r\n* __tf_python/tensorflow/\\_\\_init.py\\_\\___ does not exist as below.\r\n```bash\r\n$ ls tf_python/tensorflow/__init__.py\r\nls: cannot access 'tf_python/tensorflow/__init__.py': No such file or directory\r\n```\r\n\r\n* ~~When building tensorflow using __python 3__, it does not matter since python3 supports [Implicit Namespace Packages](https://www.python.org/dev/peps/pep-0420/). So `create_python_api.py` runs without errors. However, __ImportError__ occurs when using __python 2__ if tf_python/tensorflow/\\_\\_init\\_\\_.py does not exist.~~\r\n* The similar error occurs when using __python 3__. Refer my below comment.\r\n\r\n* After checking the build log with 'VERBOSE' option, I found out the below message.\r\nBecause of 'tf_python/tensorflow/**app**' module, 'tf_python/tensorflow/\\_\\_init\\_\\_.py' is deleted. But I can't find any 'app' module in tf_python.\r\n\r\n```bash\r\n...\r\n[100%] Built target tf_extension_ops                                            \r\nmake -f CMakeFiles/tf_python_api.dir/build.make CMakeFiles/tf_python_api.dir/depend\r\nmake[3]: Entering directory '/home/again4you/tensorflow/build'                  \r\ncd /home/again4you/tensorflow/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/again4you/tensorflow/tensorflow/contrib/cmake /home/again4you/tensorflow/tensorflow/contrib/cmake /home/again4you/tensorflow/build /home/again4you/tensorflow/build /home/again4you/tensorflow/build/CMakeFiles/tf_python_api.dir/DependInfo.cmake --color=\r\nDeleting primary custom command output \"/home/again4you/tensorflow/build/tf_python/tensorflow/__init__.py\" because another output \"/home/again4you/tensorflow/build/tf_python/tensorflow/app/__init__.py\" does not exist.\r\nDependee \"/home/again4you/tensorflow/build/CMakeFiles/tf_python_api.dir/DependInfo.cmake\" is newer than depender \"/home/again4you/tensorflow/build/CMakeFiles/tf_python_api.dir/depend.internal\".\r\n...\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nCUDA/cuDNN version\nExact command to reproduce", "@tensorflowbutler bot. Thanks for quick reply  :)\r\nI just added some omitted information that you requested but i think it is not important. \r\n\r\n@angersson IMHO, it looks like a kind bug. Feel free to ask me if you have any question about this issue. Thanks in advance.", "In case of __Python 3__, the same error also occurs. My test result is as below.\r\n\r\n### System information\r\n* Python version: 3.5.3\r\n* Everything is the same except the python version\r\n\r\n### Source code information\r\n* Branch: r1.9\r\n* Commit ID: e34327d766016bf4901d847c6ce9eb671db7abb9 (latest at that time)\r\n* Just clone without modification\r\n\r\n\r\n### How to reproduce\r\n```bash\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\n$ cd tensorflow\r\n$ git checkout -b cmake_build_test e34327d766016bf4901d847c6ce9eb671db7abb9\r\n$ mkdir build\r\n$ cd build\r\n$ cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\n$ make VERBOSE=1 -j4  tf_python_build_pip_package &> cmake_build_py3_log.txt\r\n```\r\n\r\n\r\n### Error message\r\n* Download the full build log message: [link](https://www.dropbox.com/s/jfbrqrw7omha6gv/cmake_build_py3_log.txt)\r\n```bash\r\n...\r\n[100%] Built target tf_extension_ops\r\nmake -f CMakeFiles/tf_python_api.dir/build.make CMakeFiles/tf_python_api.dir/depend\r\nmake[3]: Entering directory '/home/again4you/github_work/tensorflow/build'\r\ncd /home/again4you/github_work/tensorflow/build && /usr/bin/cmake -E cmake_depends \"Unix Makefiles\" /home/again4you/github_work/tensorflow/tensorflow/contrib/cmake /home/again4you/github_work/tensorflow/tensorflow/contrib/cmake /home/again4you/github_work/tensorflow/build /home/again4you/github_work/tensorflow/build /home/again4you/github_work/tensorflow/build/CMakeFiles/tf_python_api.dir/DependInfo.cmake --color=\r\nDeleting primary custom command output \"/home/again4you/github_work/tensorflow/build/tf_python/tensorflow/__init__.py\" because another output \"/home/again4you/github_work/tensorflow/build/tf_python/tensorflow/app/__init__.py\" does not exist.\r\nDependee \"/home/again4you/github_work/tensorflow/build/CMakeFiles/tf_python_api.dir/DependInfo.cmake\" is newer than depender \"/home/again4you/github_work/tensorflow/build/CMakeFiles/tf_python_api.dir/depend.internal\".\r\nDependee \"/home/again4you/github_work/tensorflow/build/CMakeFiles/CMakeDirectoryInformation.cmake\" is newer than depender \"/home/again4you/github_work/tensorflow/build/CMakeFiles/tf_python_api.dir/depend.internal\".\r\nScanning dependencies of target tf_python_api\r\nmake[3]: Leaving directory '/home/again4you/github_work/tensorflow/build'\r\nmake -f CMakeFiles/tf_python_api.dir/build.make CMakeFiles/tf_python_api.dir/build\r\nmake[3]: Entering directory '/home/again4you/github_work/tensorflow/build'\r\n[100%] Generating __init__.py files for Python API.\r\ncd /home/again4you/github_work/tensorflow/build/tf_python && /usr/bin/cmake -E remove -f /home/again4you/github_work/tensorflow/build/tf_python/tensorflow/__init__.py\r\ncd /home/again4you/github_work/tensorflow/build/tf_python && /usr/bin/cmake -E env PYTHONPATH=/home/again4you/github_work/tensorflow/build/tf_python /usr/bin/python /home/again4you/github_work/tensorflow/build/tf_python/tensorflow/tools/api/generator/create_python_api.py --root_init_template=/home/again4you/github_work/tensorflow/build/tf_python/tensorflow/api_template.__init__.py --apidir=/home/again4you/github_work/tensorflow/build/tf_python/tensorflow /home/again4you/github_work/tensorflow/api_init_files_list.txt\r\nTraceback (most recent call last):\r\n  File \"/home/again4you/github_work/tensorflow/build/tf_python/tensorflow/tools/api/generator/create_python_api.py\", line 28, in <module>\r\n    from tensorflow.tools.api.generator import doc_srcs\r\nImportError: No module named 'tensorflow.tools.api'\r\n...\r\n```\r\n", "@annarev, I faced 'ImportError' when running __create_python_api.py__. I am still fighting to resolve this issue but I can't find any doubtful codes until now. Please look and see this issue and let me know the problem if I made a mistake.", "Hi again4you,\r\nIs it possible you have an old version of TensorFlow installed on your system?\r\nIt might be that create_python_api.py ends up importing system-installed tensorflow package instead of the one in the build directory.", "@annarev\r\nThanks for quick reply. \r\n\r\nYou means that I __have to__ install an old version of Tensorflow on my system to build the Tensorflow from the scratch using cmake. right? If then, I thinks that is not a good point. Is there any alternative solution to build it without installing an old version of Tensorflow?\r\n\r\nMoreover, as you can see the build log that I submit, `create_python_api.py` script runs with __PYTHONPATH=/home/again4you/github_work/tensorflow/build/tf_python__ option so it does not refer system-installed python environment.\r\n\r\n```bash\r\ncd /home/again4you/github_work/tensorflow/build/tf_python && /usr/bin/cmake -E env PYTHONPATH=/home/again4you/github_work/tensorflow/build/tf_python /usr/bin/python /home/again4you/github_work/tensorflow/build/tf_python/tensorflow/tools/api/generator/create_python_api.py --root_init_template=/home/again4you/github_work/tensorflow/build/tf_python/tensorflow/api_template.__init__.py --apidir=/home/again4you/github_work/tensorflow/build/tf_python/tensorflow /home/again4you/github_work/tensorflow/api_init_files_list.txt\r\n```", "@annarev, Even installing a previous tensorflow, I faced the same error message.\r\nMy test environment is as below.\r\n* python: 3.5.3\r\n* tensorflow: 1.6.0\r\n```python\r\n$ python\r\nPython 3.5.3 (default, Nov 23 2017, 11:34:05) \r\n[GCC 6.3.0 20170406] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> tensorflow.__version__\r\n'1.6.0'\r\n```", "The TF team only officially supports Bazel builds on Linux, so I'm marking this issue as Community-Supported. Can you try using Bazel instead?\r\n\r\nSorry for the inconvenience.", "@again4you, I meant that if you have an old version installed without that module available, you might see the error. If that is the case, you can create and activate virtualenv before running the build. This way you can make sure system-installed TF does not get picked up by the import in create_python_api.py.", "@again4you You can firstly uninstall `tensorflow` or `tensorflow-gpu`, and `grpcio`, `protobuf`, which you installed via pip in the system.\r\n\r\nFor detailed reasons, the cmake script that uses python to generate some stuffs attempts to import system modules first, rather than your built version.\r\n\r\nIn addition, I also found a bug or a missing step when imports google.protobuf, I am trying to work it out later.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20566, "title": "Cannot use warm startup of an estimator with MirroredStrategy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:  using nvidia containter: https://docs.nvidia.com/deeplearning/dgx/tensorflow-release-notes/rel_18.06.html#rel_18.06 \r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**: 9.0.176\r\n- **GPU model and memory**: nvidia tesla v100\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\nI'm training an network using tf.estimator where I load a set of variables from an existing checkpoint. This is done using the warm start-up option from the estimator class. While the code works when running on a single GPU, It generates an error when I use the MirroredStrategy.\r\n\r\n### Source code / logs\r\nThe code:\r\n```\r\nws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=\"init_checkpoint.ckpt\"),\r\n                                                vars_to_warm_start=\"^((?!Logits).)*$\"\r\n                                                )\r\nself.trainer = tf.estimator.Estimator(\r\n            model_fn=self.get_model_fn,  # First-class function\r\n            params=None,  # HParams are passed using the config file\r\n            config=run_config,\r\n            warm_start_from=ws\r\n        )\r\n```\r\n\r\nThis generate the following trace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 37, in <module>\r\n    experimenter.run_training_experiment(config)\r\n  File \"/media/local/BDA_tf_framework/neuralnetwork/trainingexperimenter.py\", line 39, in run_training_experiment\r\n    tf.estimator.train_and_evaluate(self.trainer, self.training_specs, self.eval_specs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 439, in train_and_evaluate\r\n    executor.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 518, in run\r\n    self.run_local()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 650, in run_local\r\n    hooks=train_hooks)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 977, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 986, in _train_with_estimator_spec\r\n    warm_starting_util.warm_start(*self._warm_start_settings)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/warm_starting_util.py\", line 342, in warm_start\r\n    _warm_start_var(variable, ckpt_to_initialize_from, prev_var_name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/warm_starting_util.py\", line 139, in _warm_start_var\r\n    \"PartitionedVariable, but is {}\".format(type(var)))\r\nTypeError: var MUST be one of the following: a Variable, list of Variable or PartitionedVariable, but is <class 'tensorflow.contrib.distribute.python.values.MirroredVariable'>\r\nMakefile:19: recipe for target 'train_gpu' failed\r\nmake: *** [train_gpu] Error 1\r\n```", "comments": ["Thanks for forwarding, a fix for this is in progress. Please see duplicate issue #19958 . ", "The fix has been merged: https://github.com/tensorflow/tensorflow/commit/f1de0ddd55dcae6237ea7d21ccddcc6467a6cf8b\r\n\r\nYou should be able to test this out in a nightly build soon. \r\n\r\n"]}, {"number": 20565, "title": "while_loop on GPU doesn't iterate", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nno\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\n\"16.04.4 LTS (Xenial Xerus)\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\ntensorflow-gpu binary\r\n\r\ntf.VERSION = 1.9.0-rc2\r\ntf.GIT_VERSION = v1.9.0-rc1-48-ge3f2b5903c\r\ntf.COMPILER_VERSION = v1.9.0-rc1-48-ge3f2b5903c\r\n\r\n- **Python version**: \r\n\r\nPython 2.7.12\r\n\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nN/A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nCUDA 8.0\r\ncuDNN 6.0\r\n\r\n- **GPU model and memory**:\r\n\r\nGeForce GTX 960M 4GB RAM\r\nNVIDIA Driver Version: 384.130\r\n\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    with tf.device('/gpu:0'):\r\n        N = tf.constant(10, dtype=tf.int64)\r\n\r\n        def cond(i, s):\r\n            return tf.less(i, N)\r\n\r\n        def body(i, s):\r\n            n = (i+1)*2\r\n            return i+1, n\r\n\r\n        loop_vars = [tf.constant(0, dtype=tf.int64)]*2\r\n        loop = tf.while_loop(cond, body, loop_vars)\r\n\r\n    init = tf.global_variables_initializer()\r\n\r\n    graph.finalize()\r\n\r\nconfig = tf.ConfigProto(log_device_placement=True)\r\n\r\nwith tf.Session(graph=graph, config=config) as S:\r\n    S.run(init)\r\n\r\n    print(S.run(loop))\r\n\r\n```\r\n\r\n### Describe the problem\r\n\r\nWhen the above script is run, it produces `(0,0)` instead of the expected `(10,20)`.\r\n\r\nIf one changes the device to `/cpu:0` the expected result is produced.\r\n\r\nI've tested that the GPU works with the following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    A = tf.ones([100,100])\r\n    B = A*2\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as S:\r\n    S.run(init)\r\n\r\n    result = S.run(B)\r\n    print(result.shape, result)\r\n```\r\n\r\n### Source code / logs\r\n\r\nN/A", "comments": ["This also seems to fail on CUDA 9.0 and cuDNN 7.1", "@skye can you take a look into this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20565\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20565\">No</a>\n", "Closing out this issue because it hasn't been updated in the last year.  Please reopen if this is still relevant."]}, {"number": 20564, "title": "How tf.contrib.seq2seq.AttentionWrapper . working ?", "body": "I am trying to understand tf.contrib.seq2seq.AttentionWrapper  working , I have created simple program to understand the working of AttentionWrapper : \r\n\r\n\r\n```\r\nnum_units=32\r\n\r\nbatch_size = 12\r\nmax_time = 10\r\nnum_dim = 11\r\n\r\n\r\n\r\ninput_data = np.random.randn(batch_size,max_time,num_dim).astype(np.float32)\r\n\r\n\r\n# atch_size = np.random.randn(batch_size,num_dim).astype(np.float32)\r\n\r\nsequence_length =[len(i) for i in input_data]\r\n\r\nlstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_units)\r\n\r\noutput,last_state = tf.nn.dynamic_rnn(lstm_cell,input_data,sequence_length=sequence_length,dtype=tf.float32)\r\n\r\nprint(output,last_state)\r\n\r\n\r\n\r\n#now let's use attention\r\n\r\n#rnn_no_units , encoder output , sequence_length , dtype , name\r\n\r\nattention_Bahdanau = tf.contrib.seq2seq.BahdanauAttention(num_units=num_units,memory=output,memory_sequence_length=sequence_length,dtype=tf.float32,name='BahdanauAttention')\r\n\r\nbatch_size = attention_Bahdanau.batch_size\r\nmemory_layer = attention_Bahdanau.memory_layer\r\nkeys=attention_Bahdanau.keys\r\nvalues=attention_Bahdanau.values\r\nalignment_size= attention_Bahdanau.alignments_size\r\nstate_size = attention_Bahdanau.state_size\r\n\r\n\r\n\r\nquery_ = tf.get_variable(name='query_dta_1',\r\n                         shape=[batch_size,num_units],\r\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\r\n\r\nstate_ = tf.get_variable(name='state__dta_l',\r\n                         shape=[batch_size,alignment_size],\r\n                         dtype=tf.float32,initializer=tf.random_uniform_initializer(-0.01,0.01))\r\n\r\n\r\n\r\n\r\ninitial_alignments = attention_Bahdanau.initial_alignments(batch_size,dtype=tf.float32)\r\n\r\ninitial_state   =    attention_Bahdanau.initial_state(batch_size,dtype=tf.float32)\r\n\r\nfg=attention_Bahdanau.__call__(query_,state_)\r\n\r\n#(12, 10)\r\n#(12, 10)\r\n\r\n#lstm_cell , attention_mech , rnn_num_units\r\n\r\nattention_wrapper = tf.contrib.seq2seq.AttentionWrapper(lstm_cell,attention_Bahdanau,num_units)\r\n\r\nzero_s= attention_wrapper.zero_state(batch_size=batch_size,dtype=tf.float32)\r\nprint(zero_s)\r\n\r\nprint(attention_wrapper.__call__(query_,zero_s))\r\n```\r\n\r\nThen i am getting this error:\r\n\r\n```\r\nAttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state:0' shape=(12, 32) dtype=float32>, h=<tf.Tensor 'AttentionWrapperZeroState/checked_cell_state_1:0' shape=(12, 32) dtype=float32>), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(12, 32) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=(12, 10) dtype=float32>, alignment_history=(), attention_state=<tf.Tensor 'AttentionWrapperZeroState/zeros_3:0' shape=(12, 10) dtype=float32>)\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1588   try:\r\n-> 1589     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1590   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-80-0926d476036c> in <module>()\r\n      5 print(zero_s)\r\n      6 \r\n----> 7 print(attention_wrapper.__call__(query_,zero_s))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    230         setattr(self, scope_attrname, scope)\r\n    231       with scope:\r\n--> 232         return super(RNNCell, self).__call__(inputs, state)\r\n    233 \r\n    234   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    327 \r\n    328       # Actually call layer\r\n--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n    330 \r\n    331     if not context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    701 \r\n    702       if not in_deferred_mode:\r\n--> 703         outputs = self.call(inputs, *args, **kwargs)\r\n    704         if outputs is None:\r\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\r\n   1410     cell_inputs = self._cell_input_fn(inputs, state.attention)\r\n   1411     cell_state = state.cell_state\r\n-> 1412     cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n   1413 \r\n   1414     cell_batch_size = (\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope, *args, **kwargs)\r\n    337     # method.  See the class docstring for more details.\r\n    338     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\r\n--> 339                                      *args, **kwargs)\r\n    340 \r\n    341 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    327 \r\n    328       # Actually call layer\r\n--> 329       outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n    330 \r\n    331     if not context.executing_eagerly():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    701 \r\n    702       if not in_deferred_mode:\r\n--> 703         outputs = self.call(inputs, *args, **kwargs)\r\n    704         if outputs is None:\r\n    705           raise ValueError('A layer\\'s `call` method should return a Tensor '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)\r\n    855     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n    856     lstm_matrix = math_ops.matmul(\r\n--> 857         array_ops.concat([inputs, m_prev], 1), self._kernel)\r\n    858     lstm_matrix = nn_ops.bias_add(lstm_matrix, self._bias)\r\n    859 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\r\n   2012     else:\r\n   2013       return gen_math_ops.mat_mul(\r\n-> 2014           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n   2015 \r\n   2016 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name)\r\n   4277     _, _, _op = _op_def_lib._apply_op_helper(\r\n   4278         \"MatMul\", a=a, b=b, transpose_a=transpose_a, transpose_b=transpose_b,\r\n-> 4279         name=name)\r\n   4280     _result = _op.outputs[:]\r\n   4281     _inputs_flat = _op.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   3412           input_types=input_types,\r\n   3413           original_op=self._default_original_op,\r\n-> 3414           op_def=op_def)\r\n   3415 \r\n   3416       # Note: shapes are lazily computed with the C API enabled.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1754           op_def, inputs, node_def.attr)\r\n   1755       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1756                                 control_input_ops)\r\n   1757     else:\r\n   1758       self._c_op = None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1590   except errors.InvalidArgumentError as e:\r\n   1591     # Convert to ValueError for backwards compatibility.\r\n-> 1592     raise ValueError(str(e))\r\n   1593 \r\n   1594   return c_op\r\n\r\nValueError: Dimensions must be equal, but are 96 and 43 for 'rnn/while/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [12,96], [43,128].\r\n```\r\n\r\nI am not getting from where this [43,128] coming from and how to compute_attention successfully using this wrapper ?\r\n\r\nFor live experiment i have created google colab notebook , You can run and check code there : \r\n\r\nhttps://colab.research.google.com/drive/1oR039KJgtpNsJFGh8VtQt-NRcAtXi4NR", "comments": ["After some research i found how it is working \r\n\r\nif hyperparameters are \r\n\r\n```\r\n# num_units =6 (lstm hidden_units )\r\n# batch_size = 4 \r\n# max_time = 90 \r\n# num_dim = 12  (input_size or embedding dim )\r\n```\r\n\r\nNow  input will be \r\n\r\n`input_size = . Batch_size x max_time x num_dim   #4x90x12\r\n`\r\nit will pass through dynamic_rnn  : \r\n\r\n`lstm_output =dynamic_rnn_function ( input_size )  \r\n`\r\n```\r\nwhich will return  [Batch_size x  max_time x rnn_dim ] and last state [Batch_size x rnn_dim ] \r\n#4x90x6 and 4x 6\r\n```\r\n\r\n\r\nNow what attention is doing , it's creating two matrix \r\n`rnn_output is 4x6 \r\n`\r\nIn first matrix it is adding rnn_dim three times  so  matrix will be . [4,(6x3) ]\r\n\r\nsecond matrix it is adding num_dim + num_units and then reduce_max across rnn_dim\r\n`num_dim + num_units ==>6+12 = 18\r\n\r\n\r\n``reduce_max across rnn_dim ==> (num_units x batch_size ) ==> ((6+12) x (6x4) ) . ==> [18,24]\r\n`\r\n`now matmul ( [4,18] , [18,24] ) . ==> [4,24]\r\n`\r\nMy confusion is why it is adding rnn_dim three times every time , From where three is coming ?\r\n\r\nworking code is here  https://colab.research.google.com/drive/1Ifb3Tg--xxI17CoqtaMEAMpDpWVgiv5A\r\n\r\n", "Nagging Assignee @tatatodd: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20563, "title": "Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow)\r\nOS Platform and Distribution (window 7 64b):\r\nTensorFlow installed from (binary) 1.6 later:\r\nTensorFlow version NA:\r\nPython version NA: \r\nBazel version NA:\r\nGCC/Compiler version NA:\r\nCUDA/cuDNN version NA:\r\nGPU model and memory NA:\r\nExact command to reproduce NA:\r\n\r\n### Describe the problem\r\nwhen i  get start  with [https://www.tensorflow.org/install/install_java](https://www.tensorflow.org/install/install_java]) I found a problem.\r\nif the tensorflow_jni.dll version Higher than 1.5.1 then  will receive the error\uff1a\r\n\r\n`Exception in thread \"main\" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:78)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:66)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)\r\n\tat org.tensorflow.Graph.<clinit>(Graph.java:258)\r\n\tat HelloTF.main(HelloTF.java:25)`\r\n\r\n1. 1.3.0  \r\n2. 1.4.1\r\n3. 1.5.1\r\n\r\nthese versions lower than **1.6.0** is good.\r\n\r\n1.  1.6.0\r\n2.  1.7.0\r\n3.  1.8.0\r\n4.  1.9.0.rc2\r\n\r\nthese versions receive this error.\r\n\r\n\r\n\r\n### Source code / logs\r\n[https://www.tensorflow.org/install/install_java](https://www.tensorflow.org/install/install_java)   HelloTF.java\r\n", "comments": ["I thank I find the problem.\r\n![ll8t 2kmlixk 5wn x 1d](https://user-images.githubusercontent.com/15864647/42314072-c809550c-8076-11e8-8abe-f770926ad088.png)\r\n\r\n![xh hs4ubyeux5 fh z2g6](https://user-images.githubusercontent.com/15864647/42314163-03dc1ea2-8077-11e8-8871-2b19c4044fda.png)\r\n![odmjppa2zsrc n7t ay](https://user-images.githubusercontent.com/15864647/42314310-54adb82c-8077-11e8-90d8-15a5c4b20f7e.png)\r\nMy computer not  Contained  avx instructions.", "@yifeif  can you please take a look or assign it to some one more appropriate? Thanks.", "Could you please try with the latest `Tensorflow` version(2.6 as of now).\r\nFeel free to reopen the issue if the issue still persist."]}, {"number": 20562, "title": "gcc: error trying to exec 'cc1plus': execvp: No such file or directory : while installing tensorflow from source on ubuntu 14.04", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:source \r\n- **TensorFlow version (use command below)**:latest \r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:gcc version 5.5.0 (Homebrew gcc 5.5.0_4)\r\n- **CUDA/cuDNN version**:8.0/5\r\n- **GPU model and memory**:K5200 quadro\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda te //tensorflow/tools\r\n/pip_package:build_pip_package \r\n-**Have i written custom code**: N/A\r\nI am unable to execute the command above. I get the following error, I have searched around but unable to find the exact solution matching my problem. \r\n`WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:356:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/kfac/python/ops/BUILD:80:1: in py_library rule //tensorflow/contrib/kfac/python/ops:loss_functions: target '//tensorflow/contrib/kfac/python/ops:loss_functions' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/navidme/tensorflow/tensorflow/contrib/BUILD:14:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/navidme/.cache/bazel/_bazel_navidme/ae0be448885ac013f0d1c5e70ad8620c/external/double_conversion/BUILD.bazel:7:1: C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit 1)\r\ngcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/navidme/tensorflow/tensorflow/python/BUILD:1486:1 C++ compilation of rule '@double_conversion//:double-conversion' failed (Exit 1)\r\nINFO: Elapsed time: 3.624s, Critical Path: 3.17s\r\nINFO: 20 processes: 20 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n`\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@navidpanchi it's not clear to me that this is a TF problem. \r\n\r\nDoes this StackOverflow post help: https://stackoverflow.com/questions/22414109/g-error-trying-to-exec-cc1plus-execvp-no-such-file-or-directory", "Thanks. I used docker image and the problem was solved! Thanks a lot for the reply"]}, {"number": 20561, "title": "failed to embed python code that import tensorflow in c++ ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Tesla K40c,  11440 MBytes\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nI want the embedded python in my C++ code to import tensorflow, but the line \"import tensorflow\" gives an error at run time. That's no happening while importing other python modules. There is no problem with using tensorflow from my python shell/scripts, I can also compile and use custom ops written in C++, so that proofs that the installation worked without problems. I don't know if it's relevant, but my C++ library already uses tensorflow as third part shared library.\r\n\r\nHere my very easy C++ code that calls python:\r\n\r\n            PyObject* pInt;\r\n\r\n            Py_Initialize();\r\n\r\n            PyRun_SimpleString(\"import tensorflow \\n\"\r\n                               \"print('Hello TF!!!')\");\r\n\r\n            Py_Finalize();\r\n\r\n\r\nHere is the error I get at runtime:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK6google8protobuf7Message11GetTypeNameEv\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZNK6google8protobuf7Message11GetTypeNameEv\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["@allenlavoie this looks like a linkage issue. Can you take a look?", "I believe this is because _pywrap_tensorflow_internal.so can't find libtensorflow_framework.so (presumably there's a linker warning for that just before the error?).\r\n\r\nOn my machine, the linking goes like:\r\n```\r\n> ldd ~/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\tlibtensorflow_framework.so => ~/.local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n```\r\n\r\nlibtensorflow_framework.so has the protocol buffer symbols. We have some rpaths set in _pywrap_tensorflow_internal.so which are relative to its installation location (thus the .. above):\r\n\r\n```\r\nobjdump -x ~/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so  | grep PATH\r\n  RUNPATH              $ORIGIN/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:$ORIGIN/:$ORIGIN/..\r\n```\r\n(The first one is added by Bazel and makes no sense here)\r\n\r\nBut I'm guessing libtensorflow_framework.so is not in any of those locations if `$ORIGIN` points to your C++ executable instead of _pywrap_tensorflow_internal.so. Why this is different between the Python C API and regular Python (where this linking works fine) I don't know, and it may not even be a TF bug since it seems like $ORIGIN should always be the shared object doing the loading.\r\n\r\nCould you confirm that this works if you set `LD_LIBRARY_PATH` to include the location of libtensorflow_framework.so on your system? May be a reasonable workaround. I can dig further, but it'll be a bit before I have the time.", "Thank you for your reply, any help is really appreciated. \r\nI don't have a tensorflow repository in ~/.local/lib/python3.5/site-packages, but in usr/local/lib/python3.5/dist-packages. \r\nIt seems to work fine:\r\n\r\n    ldd usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n\t    libtensorflow_framework.so => /usr/local/lib/python3.5/dist-packages/tensorflow/python/../libtensorflow_framework.so (0x00007fec7ee3a000)\r\n\r\n    objdump -x usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so  | grep PATH\r\n    RPATH                $ORIGIN/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccublas___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccusolver___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudnn___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:$ORIGIN/../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib:$ORIGIN/:$ORIGIN/..:../local_config_cuda/cuda/lib64:../local_config_cuda/cuda/extras/CUPTI/lib64\r\n\r\n\r\nI added the location of libtensorflow_framework.so to LD_LIBRARY_PATH but the error is still there. Even if I try the binary installation nothing changes. There is also no linker warning before the error.\r\nMaybe I can link tensorflow to my c++ project explicitly in the CMakeList.txt? But for other python packages (like numpy) I don't need to...  Is there something similar to \r\n    find_package(PythonLibs REQUIRED) \r\n    find_package(CUDA) \r\nfor tensorflow (for either source or binary)?\r\n  ", "We have some [ways to get linker flags for custom ops](https://www.tensorflow.org/api_docs/python/tf/sysconfig). For example `tf.sysconfig.get_link_flags()` prints `['-L~/.local/lib/python2.7/site-packages/tensorflow', '-ltensorflow_framework']` for me. Maybe if you link against that and your C++ binary loads it successfully it'll find it by name without _pywrap_tensorflow_internal.so having to search for it. (Ideally you could just dlopen it, but that seems like it'd require importing TensorFlow in C++ first.)\r\n\r\nThat symbol is in libtensorflow_framework.so (`nm -g libtensorflow_framework.so | grep _ZNK6google8protobuf7Message11GetTypeNameEv` finds the symbol), so I'm still convinced that it not finding that .so is the issue. You could try `strace` to figure out what it's doing.", "Actually I can't find the symbol _ZNK6google8protobuf7Message11GetTypeNameEv in libtensorflow_framework.so, but I can find it in _pywrap_tensorflow_internal.so. Also including libtensorflow_framework.so and dlopen did't bring any results. In the meanwhile I switched to release 1.9 but the issue is still there. What if the problem is in _pywrap_tensorflow_internal.so? The error starts in \r\n`  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n        return _load(spec)`\r\n ", "Actually I can't find the symbol _ZNK6google8protobuf7Message11GetTypeNameEv in libtensorflow_framework.so, but I can find it in _pywrap_tensorflow_internal.so. Also including libtensorflow_framework.so and dlopen did't bring any results. In the meanwhile I switched to release 1.9 but the issue is still there. What if the problem is in _pywrap_tensorflow_internal.so? The error starts in \r\n`  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n        return _load(spec)` when we want to  `from tensorflow.python.pywrap_tensorflow_internal import *`  in /tensorflow/python/pywrap_tensorflow.py.\r\n\r\n ", "I face a similar issue. When libraries tensorflow_cc.so and tensorflow_framework.so are linked into a python module. There's a singleton construction error.\r\nI can either import python module or execute ```import tensorflow```.\r\nMaking both results in an error. Are there some limitations on the usage of tensorflow at both C++ and python side. Python and C++ are relying on the same precompiled binaries.\r\n```\r\nName            : tensorflow\r\nVersion         : 1.14.0-2\r\nDescription     : Library for computation using data flow graphs for scalable machine learning\r\nArchitecture    : x86_64\r\nURL             : https://www.tensorflow.org/\r\nLicenses        : APACHE\r\nGroups          : None\r\nProvides        : None\r\nDepends On      : c-ares\r\nOptional Deps   : tensorboard: Tensorflow visualization toolkit\r\nRequired By     : python-tensorflow\r\nOptional For    : None\r\nConflicts With  : None\r\nReplaces        : None\r\nInstalled Size  : 538.08 MiB\r\nPackager        : Konstantin Gizdov <arch@kge.pw>\r\nBuild Date      : Thu 27 Jun 2019 10:22:30 AM UTC\r\nInstall Date    : Fri 16 Aug 2019 07:04:38 PM UTC\r\nInstall Reason  : Explicitly installed\r\nInstall Script  : No\r\nValidated By    : Signature\r\n\r\nName            : python-tensorflow\r\nVersion         : 1.14.0-2\r\nDescription     : Library for computation using data flow graphs for scalable machine learning\r\nArchitecture    : x86_64\r\nURL             : https://www.tensorflow.org/\r\nLicenses        : APACHE\r\nGroups          : None\r\nProvides        : None\r\nDepends On      : c-ares  tensorflow  python-termcolor  python-astor  python-gast  python-numpy  python-protobuf  absl-py  python-h5py  python-keras-applications  python-keras-preprocessing  python-tensorflow-estimator\r\nOptional Deps   : tensorboard: Tensorflow visualization toolkit\r\nRequired By     : None\r\nOptional For    : None\r\nConflicts With  : None\r\nReplaces        : None\r\nInstalled Size  : 323.26 MiB\r\nPackager        : Konstantin Gizdov <arch@kge.pw>\r\nBuild Date      : Thu 27 Jun 2019 10:22:30 AM UTC\r\nInstall Date    : Fri 16 Aug 2019 07:04:57 PM UTC\r\nInstall Reason  : Explicitly installed\r\nInstall Script  : No\r\nValidated By    : Signature\r\n```\r\n\r\n```\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/profiler/tfprof_log.proto\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1370] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\n```", "You cannot link both of these in the same binary as they contain duplicated\nsymbols. Pick one (in your case tensorflow_cc which includes all symbols in\nframework afaict).\n\nOn Sun, Aug 18, 2019 at 10:11 AM Siarhei Siniak <notifications@github.com>\nwrote:\n\n> I face a similar issue. When libraries tensorflow_cc.so and\n> tensorflow_framework.so are linked into a python module. There's a\n> singleton construction error.\n> I can either import python module or execute import tensorflow.\n> Making both results in an error. Are there some limitations on the usage\n> of tensorflow at both C++ and python side. Python and C++ are relying on\n> the same precompiled binaries.\n>\n> Name            : tensorflow\n> Version         : 1.14.0-2\n> Description     : Library for computation using data flow graphs for scalable machine learning\n> Architecture    : x86_64\n> URL             : https://www.tensorflow.org/\n> Licenses        : APACHE\n> Groups          : None\n> Provides        : None\n> Depends On      : c-ares\n> Optional Deps   : tensorboard: Tensorflow visualization toolkit\n> Required By     : python-tensorflow\n> Optional For    : None\n> Conflicts With  : None\n> Replaces        : None\n> Installed Size  : 538.08 MiB\n> Packager        : Konstantin Gizdov <arch@kge.pw>\n> Build Date      : Thu 27 Jun 2019 10:22:30 AM UTC\n> Install Date    : Fri 16 Aug 2019 07:04:38 PM UTC\n> Install Reason  : Explicitly installed\n> Install Script  : No\n> Validated By    : Signature\n>\n> Name            : python-tensorflow\n> Version         : 1.14.0-2\n> Description     : Library for computation using data flow graphs for scalable machine learning\n> Architecture    : x86_64\n> URL             : https://www.tensorflow.org/\n> Licenses        : APACHE\n> Groups          : None\n> Provides        : None\n> Depends On      : c-ares  tensorflow  python-termcolor  python-astor  python-gast  python-numpy  python-protobuf  absl-py  python-h5py  python-keras-applications  python-keras-preprocessing  python-tensorflow-estimator\n> Optional Deps   : tensorboard: Tensorflow visualization toolkit\n> Required By     : None\n> Optional For    : None\n> Conflicts With  : None\n> Replaces        : None\n> Installed Size  : 323.26 MiB\n> Packager        : Konstantin Gizdov <arch@kge.pw>\n> Build Date      : Thu 27 Jun 2019 10:22:30 AM UTC\n> Install Date    : Fri 16 Aug 2019 07:04:57 PM UTC\n> Install Reason  : Explicitly installed\n> Install Script  : No\n> Validated By    : Signature\n>\n> [libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/profiler/tfprof_log.proto\n> [libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1370] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):\n> terminate called after throwing an instance of 'google::protobuf::FatalException'\n>   what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20561?email_source=notifications&email_token=AAABHRKOHAQPZRCLIAJM4STQFF7FBA5CNFSM4FINL4J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD4REBCY#issuecomment-522338443>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLPGWI2KYFJPSIMQZDQFF7FBANCNFSM4FINL4JQ>\n> .\n>\n\n\n-- \n - Alex\n", "The error occurs even when I link the module with tensorflow_cc.so only.\r\n```#1``` Test Case\r\n- load the module with a tensorflow_cc API usage\r\n- import tensorflow\r\n\r\n```\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/protobuf/master.proto\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1370] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\n```\r\n```\r\nThread 1 \"ipython\" received signal SIGABRT, Aborted.\r\n0x00007f5108585755 in raise () from /usr/lib/libc.so.6\r\n(gdb) where\r\n#0  0x00007f5108585755 in raise () from /usr/lib/libc.so.6\r\n#1  0x00007f5108570851 in abort () from /usr/lib/libc.so.6\r\n#2  0x00007f50f46a681f in __gnu_cxx::__verbose_terminate_handler () at /build/gcc/src/gcc/libstdc++-v3/libsupc++/vterminate.cc:95\r\n#3  0x00007f50f46b330a in __cxxabiv1::__terminate (handler=<optimized out>) at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:47\r\n#4  0x00007f50f46b3367 in std::terminate () at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:57\r\n#5  0x00007f50f46b35bd in __cxxabiv1::__cxa_throw (obj=<optimized out>, tinfo=0x7f50f678f8f0 <typeinfo for google::protobuf::FatalException>, \r\n    dest=0x7f50f5fa7d00 <google::protobuf::FatalException::~FatalException()>) at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_throw.cc:95\r\n#6  0x00007f50f54a1740 in ?? () from /usr/lib/libtensorflow_framework.so.1\r\n#7  0x00007f50f5e69d90 in google::protobuf::DescriptorPool::InternalAddGeneratedFile(void const*, int) () from /usr/lib/libtensorflow_framework.so.1\r\n#8  0x00007f50f5ec6cd2 in google::protobuf::internal::AddDescriptors(google::protobuf::internal::DescriptorTable*, void (* const*)(), int) () from /usr/lib/libtensorflow_framework.so.1\r\n#9  0x00007f50f5ec6cba in google::protobuf::internal::AddDescriptors(google::protobuf::internal::DescriptorTable*, void (* const*)(), int) () from /usr/lib/libtensorflow_framework.so.1\r\n#10 0x00007f510872b79a in call_init.part () from /lib64/ld-linux-x86-64.so.2\r\n#11 0x00007f510872b8a1 in _dl_init () from /lib64/ld-linux-x86-64.so.2\r\n#12 0x00007f510872f683 in dl_open_worker () from /lib64/ld-linux-x86-64.so.2\r\n#13 0x00007f51086823d9 in _dl_catch_exception () from /usr/lib/libc.so.6\r\n#14 0x00007f510872ef5e in _dl_open () from /lib64/ld-linux-x86-64.so.2\r\n#15 0x00007f51081de34c in ?? () from /usr/lib/libdl.so.2\r\n#16 0x00007f51086823d9 in _dl_catch_exception () from /usr/lib/libc.so.6\r\n#17 0x00007f5108682473 in _dl_catch_error () from /usr/lib/libc.so.6\r\n#18 0x00007f51081deab9 in ?? () from /usr/lib/libdl.so.2\r\n#19 0x00007f51081de3da in dlopen () from /usr/lib/libdl.so.2\r\n#20 0x00007f51083ca451 in _PyImport_FindSharedFuncptr () from /usr/lib/libpython3.7m.so.1.0\r\n#21 0x00007f510840356c in _PyImport_LoadDynamicModuleWithSpec () from /usr/lib/libpython3.7m.so.1.0\r\n#22 0x00007f51084038e5 in ?? () from /usr/lib/libpython3.7m.so.1.0\r\n#23 0x00007f51083090b5 in _PyMethodDef_RawFastCallDict () from /usr/lib/libpython3.7m.so.1.0\r\n```\r\n\r\n```\r\n(gdb) info sharedlibrary \r\nFrom                To                  Syms Read   Shared Object Library\r\n0x00007f5108570630  0x00007f51086b889f  Yes (*)     /usr/lib/libc.so.6\r\n0x00007f5108264020  0x00007f51084080d2  Yes (*)     /usr/lib/libpython3.7m.so.1.0\r\n0x00007f510871d100  0x00007f510873bc64  Yes (*)     /lib64/ld-linux-x86-64.so.2\r\n0x00007f51081e9b20  0x00007f51081f7d85  Yes (*)     /usr/lib/libpthread.so.0\r\n0x00007f51081de210  0x00007f51081defe9  Yes (*)     /usr/lib/libdl.so.2\r\n0x00007f51081d93d0  0x00007f51081d9d60  Yes (*)     /usr/lib/libutil.so.1\r\n0x00007f51080a1370  0x00007f510813ccb8  Yes (*)     /usr/lib/libm.so.6\r\n0x00007f5108715020  0x00007f5108716b39  Yes (*)     /usr/lib/python3.7/lib-dynload/_heapq.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5107a7b020  0x00007f5107a7e441  Yes (*)     /usr/lib/python3.7/lib-dynload/zlib.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51078591d0  0x00007f51078662a7  Yes (*)     /usr/lib/libz.so.1\r\n0x00007f5107a74020  0x00007f5107a7569a  Yes (*)     /usr/lib/python3.7/lib-dynload/_bz2.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f510783b020  0x00007f5107847b92  Yes (*)     /usr/lib/libbz2.so.1.0\r\n0x00007f510784e020  0x00007f51078512a1  Yes (*)     /usr/lib/python3.7/lib-dynload/_lzma.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51075cacc0  0x00007f51075e1983  Yes (*)     /usr/lib/liblzma.so.5\r\n0x00007f51077f5020  0x00007f51077f5876  Yes (*)     /usr/lib/python3.7/lib-dynload/grp.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5107c65020  0x00007f5107c65222  Yes (*)     /usr/lib/python3.7/lib-dynload/_opcode.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5107c59020  0x00007f5107c5d421  Yes (*)     /usr/lib/python3.7/lib-dynload/_struct.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51074ae020  0x00007f51074ba7ae  Yes (*)     /usr/lib/python3.7/lib-dynload/_json.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5107c4c020  0x00007f5107c5054b  Yes (*)     /usr/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51073e5020  0x00007f51073e7db6  Yes (*)     /usr/lib/python3.7/lib-dynload/_hashlib.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f510717a000  0x00007f5107318a61  Yes (*)     /usr/lib/libcrypto.so.1.1\r\n0x00007f51070f8020  0x00007f51070ff818  Yes (*)     /usr/lib/python3.7/lib-dynload/_blake2.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51070de020  0x00007f51070f0450  Yes (*)     /usr/lib/python3.7/lib-dynload/_sha3.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5107c45020  0x00007f5107c45732  Yes (*)     /usr/lib/python3.7/lib-dynload/_bisect.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51077ef020  0x00007f51077f09c6  Yes (*)     /usr/lib/python3.7/lib-dynload/_random.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51073de020  0x00007f51073df448  Yes (*)     /usr/lib/python3.7/lib-dynload/_posixsubprocess.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5107014020  0x00007f5107017875  Yes (*)     /usr/lib/python3.7/lib-dynload/select.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106f4d020  0x00007f5106f4db0b  Yes (*)     /usr/lib/python3.7/lib-dynload/termios.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106f45020  0x00007f5106f4652e  Yes (*)     /usr/lib/python3.7/lib-dynload/fcntl.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106f40020  0x00007f5106f40a11  Yes (*)     /usr/lib/python3.7/lib-dynload/resource.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5107541020  0x00007f5107543aab  Yes (*)     /usr/lib/python3.7/lib-dynload/binascii.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106dd2020  0x00007f5106de3f16  Yes (*)     /usr/lib/python3.7/lib-dynload/_datetime.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106c70020  0x00007f5106c80c43  Yes (*)     /usr/lib/python3.7/lib-dynload/_socket.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106c14020  0x00007f5106c1ea04  Yes (*)     /usr/lib/python3.7/lib-dynload/_ssl.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106b8d020  0x00007f5106bd8b07  Yes (*)     /usr/lib/libssl.so.1.1\r\n0x00007f5106c07020  0x00007f5106c071ff  Yes (*)     /usr/lib/python3.7/lib-dynload/_contextvars.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106b1f020  0x00007f5106b27895  Yes (*)     /usr/lib/python3.7/lib-dynload/_asyncio.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106a3e020  0x00007f5106a51ac1  Yes (*)     /usr/lib/python3.7/lib-dynload/_pickle.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f51069a9020  0x00007f51069b177b  Yes (*)     /usr/lib/python3.7/lib-dynload/_sqlite3.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f510688d060  0x00007f51069574d4  Yes (*)     /usr/lib/libsqlite3.so.0\r\n0x00007f5106c02020  0x00007f5106c02ab9  Yes (*)     /usr/lib/python3.7/lib-dynload/_queue.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f510655d020  0x00007f510656326d  Yes (*)     /usr/lib/python3.7/lib-dynload/array.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106357020  0x00007f510635ad58  Yes (*)     /usr/lib/python3.7/lib-dynload/unicodedata.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5106e79020  0x00007f5106e7a5b3  Yes (*)     /usr/lib/python3.7/lib-dynload/_lsprof.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5104279020  0x00007f5104289525  Yes         /usr/lib/libgcc_s.so.1\r\n0x00007f5104105020  0x00007f510412c370  Yes (*)     /usr/lib/libfmt.so.5\r\n0x00007f50f46a6040  0x00007f50f479dd12  Yes         /usr/lib/libstdc++.so.6\r\n0x00007f50f42f7020  0x00007f50f454be0e  Yes (*)     /usr/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50f40b9a70  0x00007f50f40ca0e0  Yes (*)     /usr/lib/libcblas.so.3\r\n0x00007f50d45a73f0  0x00007f50d45f46c4  Yes (*)     /usr/lib/libblas.so.3\r\n0x00007f50d4324e30  0x00007f50d4570a7d  Yes         /usr/lib/libgfortran.so.5\r\n0x00007f51040af020  0x00007f51040dc27f  Yes         /usr/lib/../lib/libquadmath.so.0\r\n0x00007f5104018020  0x00007f5104024849  Yes (*)     /usr/lib/python3.7/site-packages/numpy/core/_multiarray_tests.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50f401a020  0x00007f50f40281f5  Yes (*)     /usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f5104007040  0x00007f510400b92c  Yes (*)     /usr/lib/libffi.so.6\r\n0x00007f5106e72020  0x00007f5106e73271  Yes (*)     /usr/lib/python3.7/site-packages/numpy/linalg/lapack_lite.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b7741bd0  0x00007f50b7d07e2c  Yes (*)     /usr/lib/liblapack.so.3\r\n0x00007f50d4269020  0x00007f50d4280aee  Yes (*)     /usr/lib/python3.7/site-packages/numpy/linalg/_umath_linalg.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50f4001020  0x00007f50f400e188  Yes (*)     /usr/lib/python3.7/site-packages/numpy/fft/pocketfft_internal.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50d4040020  0x00007f50d4091600  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/mtrand.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b76de020  0x00007f50b7716ee8  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/common.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b7678020  0x00007f50b76cd423  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/bounded_integers.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50d4018020  0x00007f50d4027be2  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/mt19937.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b7646020  0x00007f50b7664468  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/bit_generator.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b760f020  0x00007f50b7633f43  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/entropy.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b7551020  0x00007f50b75b6d63  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/generator.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50d4004020  0x00007f50d400df58  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/pcg64.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b74f0020  0x00007f50b74fd4f1  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/philox.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f50b74df020  0x00007f50b74e6275  Yes (*)     /usr/lib/python3.7/site-packages/numpy/random/sfc64.cpython-37m-x86_64-linux-gnu.so\r\n0x00007f509ea2e5c0  0x00007f50a5f544a3  Yes (*)     /usr/lib/libtensorflow_cc.so.1\r\n0x00007f50f5416900  0x00007f50f6380740  Yes (*)     /usr/lib/libtensorflow_framework.so.1\r\n0x00007f51049a86f0  0x00007f51049ab950  Yes (*)     /usr/lib/librt.so.1\r\n0x00007f509357b0c0  0x00007f509ac7c703  Yes (*)     /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n(*): Shared library is missing debugging information.\r\n```\r\n\r\n```#2``` Test Case\r\n- import tensorflow\r\n- load the module with a tensorflow_cc API usage\r\n```\r\n[libprotobuf ERROR external/protobuf_archive/src/google/protobuf/descriptor_database.cc:58] File already exists in database: tensorflow/core/protobuf/master.proto\r\n[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/descriptor.cc:1370] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\n\r\n```\r\n```\r\n(gdb) where\r\n#0  0x00007ff213553755 in raise () from /usr/lib/libc.so.6\r\n#1  0x00007ff21353e851 in abort () from /usr/lib/libc.so.6\r\n#2  0x00007ff20aeab81f in __gnu_cxx::__verbose_terminate_handler () at /build/gcc/src/gcc/libstdc++-v3/libsupc++/vterminate.cc:95\r\n#3  0x00007ff20aeb830a in __cxxabiv1::__terminate (handler=<optimized out>) at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:47\r\n#4  0x00007ff20aeb8367 in std::terminate () at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_terminate.cc:57\r\n#5  0x00007ff20aeb85bd in __cxxabiv1::__cxa_throw (obj=<optimized out>, tinfo=0x7ff1eb7918f0 <typeinfo for google::protobuf::FatalException>, \r\n    dest=0x7ff1eafa8d00 <google::protobuf::FatalException::~FatalException()>) at /build/gcc/src/gcc/libstdc++-v3/libsupc++/eh_throw.cc:95\r\n#6  0x00007ff1ea4a2740 in ?? () from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1\r\n#7  0x00007ff1eae6ad90 in google::protobuf::DescriptorPool::InternalAddGeneratedFile(void const*, int) () from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1\r\n#8  0x00007ff1eaec7cd2 in google::protobuf::internal::AddDescriptors(google::protobuf::internal::DescriptorTable*, void (* const*)(), int) ()\r\n   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1\r\n#9  0x00007ff1eaec7cba in google::protobuf::internal::AddDescriptors(google::protobuf::internal::DescriptorTable*, void (* const*)(), int) ()\r\n   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1\r\n#10 0x00007ff2136f979a in call_init.part () from /lib64/ld-linux-x86-64.so.2\r\n#11 0x00007ff2136f98a1 in _dl_init () from /lib64/ld-linux-x86-64.so.2\r\n#12 0x00007ff2136fd683 in dl_open_worker () from /lib64/ld-linux-x86-64.so.2\r\n#13 0x00007ff2136503d9 in _dl_catch_exception () from /usr/lib/libc.so.6\r\n#14 0x00007ff2136fcf5e in _dl_open () from /lib64/ld-linux-x86-64.so.2\r\n#15 0x00007ff2131ac34c in ?? () from /usr/lib/libdl.so.2\r\n#16 0x00007ff2136503d9 in _dl_catch_exception () from /usr/lib/libc.so.6\r\n#17 0x00007ff213650473 in _dl_catch_error () from /usr/lib/libc.so.6\r\n#18 0x00007ff2131acab9 in ?? () from /usr/lib/libdl.so.2\r\n#19 0x00007ff2131ac3da in dlopen () from /usr/lib/libdl.so.2\r\n#20 0x00007ff213398451 in _PyImport_FindSharedFuncptr () from /usr/lib/libpython3.7m.so.1.0\r\n#21 0x00007ff2133d156c in _PyImport_LoadDynamicModuleWithSpec () from /usr/lib/libpython3.7m.so.1.0\r\n```\r\n\r\n\r\n\r\n\r\n\r\n```\r\nldd /usr/lib/libtensorflow_cc.so\r\n        linux-vdso.so.1 (0x00007ffc9d365000)\r\n        libstdc++.so.6 => /usr/lib/libstdc++.so.6 (0x00007fd3bf89f000)\r\n        libm.so.6 => /usr/lib/libm.so.6 (0x00007fd3bf759000)\r\n        libtensorflow_framework.so.1 => /usr/lib/libtensorflow_framework.so.1 (0x00007fd3bdca1000)\r\n        libdl.so.2 => /usr/lib/libdl.so.2 (0x00007fd3bdc9c000)\r\n        libpthread.so.0 => /usr/lib/libpthread.so.0 (0x00007fd3bdc7b000)\r\n        librt.so.1 => /usr/lib/librt.so.1 (0x00007fd3bdc70000)\r\n        libgcc_s.so.1 => /usr/lib/libgcc_s.so.1 (0x00007fd3bdc54000)\r\n        libc.so.6 => /usr/lib/libc.so.6 (0x00007fd3bda91000)\r\n        /usr/lib64/ld-linux-x86-64.so.2 (0x00007fd3cacda000)\r\n```\r\n```\r\nldd <custom_python_module_with_tensorflow_cc_api_usage.so>\r\n        linux-vdso.so.1 (0x00007ffc2436a000)\r\n        libpython3.7m.so.1.0 => /usr/lib/libpython3.7m.so.1.0 (0x00007f4ae3acf000)\r\n        libpthread.so.0 => /usr/lib/libpthread.so.0 (0x00007f4ae3aae000)\r\n        libtensorflow_cc.so.1 => /usr/lib/libtensorflow_cc.so.1 (0x00007f4ad885d000)\r\n        libfmt.so.5 => /usr/lib/libfmt.so.5 (0x00007f4ad881f000)\r\n        libtensorflow_framework.so.1 => /usr/lib/libtensorflow_framework.so.1 (0x00007f4ad6d65000)\r\n        libstdc++.so.6 => /usr/lib/libstdc++.so.6 (0x00007f4ad6b7d000)\r\n        libm.so.6 => /usr/lib/libm.so.6 (0x00007f4ad6a37000)\r\n        libgcc_s.so.1 => /usr/lib/libgcc_s.so.1 (0x00007f4ad6a1d000)\r\n        libc.so.6 => /usr/lib/libc.so.6 (0x00007f4ad685a000)\r\n        libdl.so.2 => /usr/lib/libdl.so.2 (0x00007f4ad6855000)\r\n        libutil.so.1 => /usr/lib/libutil.so.1 (0x00007f4ad684e000)\r\n        /usr/lib64/ld-linux-x86-64.so.2 (0x00007f4ae3ec5000)\r\n        librt.so.1 => /usr/lib/librt.so.1 (0x00007f4ad6843000)\r\n```", "1. Are there some compile time definitions, so as to not declare protobuf singletons twice?\r\nIt looks like ```import tensorflow``` doesn't verify the presence of protobuf declarations. And treats everything as if only ```import tensorflow``` can initialize tensorflow framework.\r\nI definitely can't control it from python. Since ```import tensorflow``` has no arguments.\r\n2. What are the ways to make it at C++ side when a python module is being compiled?\r\n3. Could you please point to some examples within tensorflow codebase?", "Python module can be like [c++ api example](https://www.tensorflow.org/guide/extend/cc).\r\nBut it will be just ```void f()``` with the c calling convention compiled into a <path.so> file.\r\nAnd being used from python via ctypes.\r\nIn such a case it's not clear how to use python tensorflow API. Since ```libtensorflow_framework.so``` attempts to register protobufs twice.", "Everything uses libtensorflow_framework.so, so just adding that to a Python program is safe. That's how custom ops are language-agnostic. But using another language binding (e.g. libtensorflow.so) with Python will give you duplicate registration issues at the moment. That includes statically linking TensorFlow into your C++ program and loading that in a Python process.", "> Everything uses libtensorflow_framework.so, so just adding that to a Python program is safe. That's how custom ops are language-agnostic. But using another language binding (e.g. libtensorflow.so) with Python will give you duplicate registration issues at the moment. That includes **statically** linking TensorFlow into your C++ program and loading that in a Python process.\r\n\r\nIn the mentioned library libtensorflow_cc.so is being use dynamically. It's not a static linkage.\r\n\r\n>will give you duplicate registration issues at the moment\r\n\r\nIs there an issue on github regarding it?\r\n", "Yep, just noting that static is included. Dynamic linking is too. Not sure if there's an existing issue; the solution will likely be splitting up TF's shared objects further, so e.g. there'd just be one that has protobufs and it's shared between all of the language binding DSOs.", "Current alternatives are:\r\n- add ```tensorflow modules``` as in linux kernel;\r\n- recompile tensorflow from scratch with custom patches;\r\n- use inter-process communication with C++ bindings being executed as a separate process.", "I have a similar problem.\r\n```\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/fan/cleverhans-master/examples/adversarial_asr/lingvo/core/ops/x_ops.so: undefined symbol: _ZNK6google8protobuf7Message11GetTypeNameEv\r\n```\r\nCheck out the list of shared libraries that the library file depends on.\r\n```\r\n# ldd /home/fan/cleverhans-master/examples/adversarial_asr/lingvo/core/ops/x_ops.so\r\n\tlinux-vdso.so.1 =>  (0x00007fffb00f1000)\r\n\tlibtensorflow_framework.so => not found\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f357c7b6000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f357c4ad000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f357c297000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f357becd000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f357cb38000)\r\n```\r\n\r\nI used **bazel build --config=opt --config=cuda --config=monolithic //tensorflow:libtensorflow_cc.so**\r\nto build libtensorflow_framework.so.It is located in /usr/local/lib .\r\n\r\nIs this tensorflow not calling libtensorflow_framework.so? What is the problem? How to solve this problem?\r\nSimilar to the original question, I am using ubuntu16.04, tensorflow1.13.1, python2.7.", "I solved this problem. This is because the protobuf version does not match the version when the os file was generated. The same or newer version should be used.", "> I solved this problem. This is because the protobuf version does not match the version when the os file was generated. The same or newer version should be used.\r\n\r\nHi, I am facing the same bug, could you please tell me which protobuf version you are using? I am using tf 1.15.0 and protobuf 3.11.3, but it doesn't work for me.", "@Ceveloper We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. check [**`link`**](https://stackoverflow.com/questions/42183255/importing-tensorflow-when-embedding-python-in-c-returns-null) and https://github.com/tensorflow/tensorflow/issues/20561#issuecomment-534900390 .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20561\">No</a>\n"]}, {"number": 20560, "title": "R1.8", "body": "I signed Contributor License Agreement (CLA).\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 20559, "title": "gfile.Glob is Case Sensitive for file extensions in Unix but not in Windows", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and macOS High Sierra 10.13.4\r\n- **TensorFlow installed from (source or binary)**: `pip3 install --upgrade tensorflow`\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: In source code\r\n\r\n### Describe the problem\r\ngfile.Glob() distinguishes between \".case\" and \".CASE\" in file extension in Unix-based systems, but not in Windows.\r\nIdeally, they should work the same way.\r\n\r\n### Source code\r\nStand in a root folder, add a image.jpg in the folder.\r\nThen start python and run:\r\n```\r\nfrom tensorflow.python.platform import gfile\r\nprint(gfile.Glob(\"./*.jpg\"))\r\nprint(gfile.Glob(\"./*.JPG\"))\r\n```\r\nIn Windows, you will get:\r\n```\r\n[\".\\\\image.jpg\"\"]\r\n[\".\\\\image.jpg\"\"]\r\n```\r\nIn Unix, you will get:\r\n```\r\n[\"./image.jpg\"]\r\n[]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "See also #19378 . This is an area where contributions would be welcome, if you are interested.", "I would be willing to work on this, but what is the expected behaviour? Case sensitive or insensitive?\r\n\r\nAlso, regardless of the answer, this issue is caused by Windows being case insensitive, and Unix being case sensitive (OS-wide) is it really desirable to change the conventions?", "@mrry -- any thoughts on the what is the expected behavior in this case?", "I'd probably expect it to match the local filesystem conventions. I can't find anything in the documentation for the Win32 API about case sensitivity, but I believe NTFS is case sensitive, so matching the Linux behavior seems good. ", "After digging through the code, the function in charge of the string comparison is taken straight from the Win32 API ([PathMatchSpecW](https://docs.microsoft.com/en-us/windows/desktop/api/shlwapi/nf-shlwapi-pathmatchspecw)). So the case insensitiveness is not on TensorFlow's side.\r\n\r\nAs a possible solution we could implement another check on TensorFlow side in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system_helper.cc\r\n\r\nBut really, we could also do nothing. Since this is a limitation from the underlying OS I would consider it \"expected behavior\".\r\n\r\n", "Thanks, @Belval , that is really helpful. Maybe the change here is to add a comment to the docstrings warning/explaining about the OS-specific behavior?", "Hi ! Sorry the delay.\r\nA comment in the docs would be helpful.\r\nI understand your point, but I think that if we want Tensorflow to be multiplatform it would be nice to have one behaviour for all, because if you have a team where half are using one platform and half the other, they won't be able to work with the same code, or they would have to add a conditional based on the platform.", "I can still see this as an issue in Unix in the latest Tensorflow version 2.6, please find the [Gist](https://colab.research.google.com/gist/sachinprasadhs/11834063223b177c6f8b8a7bbc30cc42/20557.ipynb) here for reference.", "This is WAI. We are obeying local filesystem rules."]}, {"number": 20558, "title": "Resolve issue 17698: Session JNI interface has memory leak", "body": "Quick fix https://github.com/tensorflow/tensorflow/issues/17698", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20557, "title": "gfile.Glob - Recursive in Windows, not in Unix", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 and macOS High Sierra 10.13.4\r\n- **TensorFlow installed from (source or binary)**: `pip3 install --upgrade tensorflow`\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: In source code\r\n\r\n### Describe the problem\r\ngfile.Glob() works recursively in Windows, but not in Unix-based systems.\r\nIdeally, they should work the same way.\r\n\r\n### Source code\r\nStand in a root folder, create subfolder and add a image.jpg in the subfolder.\r\nThen start python and run:\r\n```\r\nfrom tensorflow.python.platform import gfile\r\nprint(gfile.Glob(\"./*.jpg\"))\r\n```\r\nIn Windows, you will get `[\".\\\\subfolder\\\\image.jpg\"]`.\r\nIn Unix, you will get `[]`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tatatodd , I think this is a duplicate of #19378 (although I think this is a better name for the issue, because I couldn't find the other one, @karmel pointed this issue out in another issue - #20559 ).", "I can now see the output, and `gfile.Glob` has been moved under [tf.io.gfile.glob](https://www.tensorflow.org/api_docs/python/tf/io/gfile/glob), check the working gist [here](https://colab.research.google.com/gist/sachinprasadhs/11834063223b177c6f8b8a7bbc30cc42/20557.ipynb) for reference.\r\n"]}, {"number": 20556, "title": "Update resnet_v1.py", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 20555, "title": "Tensorflow-mkl does not work on 3D input data", "body": "------------------------\r\n\r\n### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): tensorflow-mkl installed from anaconda\r\nTensorFlow version: 1.8\r\nPython version: Python 3.6\r\nCUDA/cuDNN version: Both 9.2\r\nGPU model and memory: 1080 TI 11 GB\r\nExact command to reproduce: Any code that takes 3D data (my code is too long to paste in here)\r\n\r\nI am training a model to perform volumetric segmentation (3D data). I am training on CPU (two Xeon E5 v4 2699) due to the size of the input data that will not fit in vram. When I train the model, I get an\u00a0error\r\n\"tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'data_format' of \"NDHWC\" is not in the list of allowed values: \"NHWC\", \"NCHW\"\r\n\r\nHowever, on Intel's github it says it works on volumetric segmentation (https://github.com/intel/mkl-dnn). How may I resolve this issue so I can train my 3D-Unet with\u00a0mkl?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nGPU model and memory", "Added labels to my answers", "@AlbertPun Is your issue similar to this? https://github.com/kcct-fujimotolab/3DCNN/issues/12", "That solved it. I didn\u2019t know I had to specify the latest version.\r\n\r\nThanks", "Good to know. We have optimizations for 3D ops that should land here soon. Perf with Intel CPUs will improve significantly at that point.", "Anybody got this to work at all as it seems like this should be supported?", "@abrys This should be working on the newer versions of Tensorflow. Optimizations for 3D ops have also been upstreamed. E.g., https://github.com/tensorflow/tensorflow/pull/21324", "Thanks for the reply.\n\nAny specific version? As I was testing with Tensorflow 1.8.0, 1.9.0 and\n1.10.0 (conda version tensorflow-mkl 1.10.0 to be more specific). Otherwise\nI will try to compile myself.\n\n\n\nOn Tue, Oct 2, 2018 at 4:46 PM Jayaram Bobba <notifications@github.com>\nwrote:\n\n> @abrys <https://github.com/abrys> This should be working on the newer\n> versions of Tensorflow. Optimizations for 3D ops have also been upstreamed.\n> E.g., #21324 <https://github.com/tensorflow/tensorflow/pull/21324>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20555#issuecomment-426297550>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APQmuQ0mXAn-XYPts_M0TcQN03KWi6_qks5ug3w8gaJpZM4VDMvE>\n> .\n>\n\n\n-- \n*Arne Brys**, MSc*\nSoftware Engineer / Division lead R&D\n\nico*metrix*\nTransforming patient care through imaging AI\n\nLeuven, Belgium | Chicago, IL, USA\nT: +32 16 369 000\n\n<http://www.icometrix.com/>\n e-mail and all attachments are confidential.\n", "Please try 1.11.0. ", "**Hi, I have updated my tf version to 1.9, but this error still occurs.**\r\n\r\n**Have I written custom code:** \r\nNot very sure what custom code means here, I am trying to replicate this work: https://github.com/harvitronix/five-video-classification-methods. This error occurs only when I try model \"Conv3d\" or \"c3d\" both of which call keras.layers.Conv3D().\r\n**OS Platform and Distribution:** \r\nUbuntu--16-0 with AWS DLAMI\r\n**TensorFlow installed from:**\r\ntensorflow                1.9.0           mkl_py36h6d6ce78_1\r\n**GPU model and memory:**\r\nNo GPU, 4G\r\nI am using AWS EC2 c5.large\r\n", "**Hi, I have updated my tf version to 1.9, but this error still occurs.**\r\n\r\n**Have I written custom code:**\r\nNot very sure what custom code means here, I am trying to replicate this work: https://github.com/harvitronix/five-video-classification-methods. This error occurs only when I try model \"Conv3d\" or \"c3d\" both of which call keras.layers.Conv3D().\r\n**OS Platform and Distribution:**\r\nUbuntu--16-0 with AWS DLAMI\r\n**TensorFlow installed from:**\r\ntensorflow 1.9.0 mkl_py36h6d6ce78_1\r\n**Bazel version:**\r\nbazel                     0.15.0                        0    conda-forge\r\n**GPU model and memory:**\r\nNo GPU, 4G\r\nI am using AWS EC2 c5.large", "Nagging Assignee @jart: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue since its resolved for the original author. Please post a new issue by providing all the information asked by the template. Thanks!"]}, {"number": 20554, "title": "Expose `tf.contrib.layers.conv1d` like conv2d/conv3d", "body": "This fix tries to address the issue raised in #20551 where `tf.contrib.layers.conv1d` is not exposed.\r\nIn tf.contrib.layers even though conv1d/conv2d/conv3d are all available, the conv1d was not exposed like conv2d/conv3d.\r\n\r\nThis fix exposed `tf.contrib.layers.conv1d` so that it is consistent with `tf.contrib.layers.conv2d` and `tf.contrib.layers.conv3d`.\r\n\r\nThis fix fixes #20551.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["LGTM. Adding Alex for second set of eyes.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20553, "title": "tensorflow cuda build configure hanging(after NCCL)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 x64\r\n- **TensorFlow installed from (source or binary)**:  source master and source r1.9\r\n- **TensorFlow version (use command below)**: 1.9.0rc0 and 1.9.0rc2\r\n- **Python version**:  3.6.6\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.2/7.1\r\n- **GPU model and memory**: GTX1080Ti 11GB  X 2 \r\n- **Exact command to reproduce**:\r\n\r\ncheckout to r1.9 or master and ./configure\r\n\r\nafter configure cuda and cudnn\r\n\r\nand configure NCCL ( default 1.3 or 2.2, whatever I choose, same )\r\n\r\nthe process hangs forever", "comments": ["one other machine works fine with CUDA9.2, so this could be a configuration bug with CUDA and NCCL", "Can you post the exact commands you're running and where it's hanging?", "@skye \r\n./ configure\r\n\r\nenable XLA, cuda, cuda 9.2, cudnn 7.1.5, and then NCCL setting \r\n\r\nand then I tried 2 different \r\n1. just enter ( default NCCL 1.3 )\r\n2. install NCCL 2.2 and type 2.2\r\n\r\nresults were same hanging\r\n\r\n\r\nI think this could be conflict with the geforce driver, I can try with newest driver, so I will try again and post ther result", "Thanks @alanpurple \r\n\r\n@yifeif do you know who deals with ./configure issues?", "Nagging Assignee @skye: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hey @alanpurple , did you manage to fix this issue?", "I don't currently have a machine to try repro this. @alanpurple do you still see this with the latest TF version?", "It didnt occur with the latest nvidia driver so this can be closed\n\nSorry for long last conclusion\n\nOn Tue, Sep 11, 2018, 4:35 AM Yifei Feng <notifications@github.com> wrote:\n\n> I don't currently have a machine to try repro this. @alanpurple\n> <https://github.com/alanpurple> do you still see this with the latest TF\n> version?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20553#issuecomment-420032353>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AETlMPvEi8Ip6QWV1rt-XnbML3-j5S_aks5uZr8RgaJpZM4VDIio>\n> .\n>\n", "Sorry for long last conclusion, this didnt occur with recent drivers", "No worries @alanpurple. Glad it's fixed now!", "I'm actually having the same issue on the current master branch (just pulled)\r\n\r\nSystem information\r\n**OS Platform and Distribution** (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4 x64\r\n**TensorFlow installed from (source or binary):** trying to install from source master and source r1.9\r\n**TensorFlow version (use command below):** none yet...\r\n**Python version:** 3.5\r\n**Bazel version (if compiling from source):** 0.18.0\r\n**GCC/Compiler version (if compiling from source):** 5.4.0\r\n**CUDA/cuDNN version:** 10.0/7.4\r\n**GPU model and memory:** GTX1060Ti\r\n**GPU driver version:** 410.48\r\n**Exact command to reproduce:**\r\n./configure \r\n\r\n**The output looks like:**\r\nWARNING: Duplicate rc file: /home/scott/bin/tensorflow/tools/bazel.rc is read multiple times, most recently imported from /home/scott/bin/tensorflow/.bazelrc\r\nWARNING: Processed legacy workspace file /home/scott/bin/tensorflow/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/scott/.cache/bazel/_bazel_scott/install/f1e11885a5cc7ba9947679cffb18bf94/_embedded_binaries/A-server.jar) to field java.lang.String.value\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.18.0 installed.\r\nPlease specify the location of python. [Default is /home/scott/anaconda3/envs/tensorflow/bin/python]: \r\nFound possible Python library paths:\r\n  /home/scott/anaconda3/envs/tensorflow/lib/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/scott/anaconda3/envs/tensorflow/lib/python3.5/site-packages]\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: Y\r\nCUDA support will be enabled for TensorFlow.\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.4\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\nPlease specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: 2.3\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nAssuming NCCL header path is /usr/local/cuda-10.0/lib/../include/nccl.h\r\n\r\n....\r\n\r\nThen it hangs. I've also tried letting it use the default or 1.3, and it still hangs there.\r\n\r\nThanks for your help!", "The exact command that's doing it is:\r\n\"/usr/local/cuda/extras/demo_suite/deviceQuery\"\r\nWhen run on it's own, it seems to print to screen just fine, but this command just hangs and never releases even when called outside of the config script - must be a cuda issue? Here's what my output looks like for that:\r\n\r\n/usr/local/cuda/extras/demo_suite/deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\nDetected 1 CUDA Capable device(s)\r\n\r\nDevice 0: \"GeForce GTX 1060 6GB\"\r\n  CUDA Driver Version / Runtime Version          10.0 / 10.0\r\n  CUDA Capability Major/Minor version number:    6.1\r\n  Total amount of global memory:                 6075 MBytes (6370295808 bytes)\r\n  (10) Multiprocessors, (128) CUDA Cores/MP:     1280 CUDA Cores\r\n  GPU Max Clock rate:                            1709 MHz (1.71 GHz)\r\n  Memory Clock rate:                             4004 Mhz\r\n  Memory Bus Width:                              192-bit\r\n  L2 Cache Size:                                 1572864 bytes\r\n  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\r\n  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\r\n  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\r\n  Total amount of constant memory:               65536 bytes\r\n  Total amount of shared memory per block:       49152 bytes\r\n  Total number of registers available per block: 65536\r\n  Warp size:                                     32\r\n  Maximum number of threads per multiprocessor:  2048\r\n  Maximum number of threads per block:           1024\r\n  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\r\n  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\r\n  Maximum memory pitch:                          2147483647 bytes\r\n  Texture alignment:                             512 bytes\r\n  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\r\n  Run time limit on kernels:                     Yes\r\n  Integrated GPU sharing Host Memory:            No\r\n  Support host page-locked memory mapping:       Yes\r\n  Alignment requirement for Surfaces:            Yes\r\n  Device has ECC support:                        Disabled\r\n  Device supports Unified Addressing (UVA):      Yes\r\n  Device supports Compute Preemption:            Yes\r\n  Supports Cooperative Kernel Launch:            Yes\r\n  Supports MultiDevice Co-op Kernel Launch:      Yes\r\n  Device PCI Domain ID / Bus ID / location ID:   0 / 5 / 0\r\n  Compute Mode:\r\n     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\r\n\r\ndeviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.0, CUDA Runtime Version = 10.0, NumDevs = 1, Device0 = GeForce GTX 1060 6GB\r\nResult = PASS\r\n\r\n\r\n"]}, {"number": 20552, "title": "tf.keras.estimator.model_to_estimator giving error in TensorFlow >= 1.8.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0 and 1.9.0-rc2\r\n- **Python version**: Python3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n`estimator = tf.keras.estimator.model_to_estimator(model, model_dir=MODEL_DIR)`\r\n\r\n### Describe the problem\r\nWhen calling tf.keras.estimator.model_to_estimator, I got following error. However, it works fine with TensorFlow == 1.7.0.\r\n\r\n```\r\nINFO:tensorflow:Using the Keras model provided.\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f61f0ede908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-176d5ff06eb1> in <module>()\r\n     13 \r\n     14 model = get_keras_model()\r\n---> 15 estimator = tf.keras.estimator.model_to_estimator(model, model_dir='./model')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py in model_to_estimator(keras_model, keras_model_path, custom_objects, model_dir, config)\r\n    502                            estimator,\r\n    503                            custom_objects,\r\n--> 504                            keras_weights)\r\n    505   elif keras_model.built:\r\n    506     logging.warning('You are creating an Estimator from a Keras model '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/estimator.py in _save_first_checkpoint(keras_model, estimator, custom_objects, keras_weights)\r\n    413           model.set_weights(keras_weights)\r\n    414         # Make update ops and initialize all variables.\r\n--> 415         if not model.train_function:\r\n    416           # pylint: disable=protected-access\r\n    417           model._make_train_function()\r\n\r\nAttributeError: 'Sequential' object has no attribute 'train_function'\r\n```\r\n\r\n### Source code / logs\r\nHere is the code to reproduce the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef get_keras_model():\r\n  model = tf.keras.Sequential()\r\n  model.add(tf.keras.layers.InputLayer(input_shape=[28*28]))\r\n  model.add(tf.keras.layers.Dense(300, activation='relu'))\r\n  model.add(tf.keras.layers.Dense(100, activation='relu'))\r\n  model.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n  model.compile(loss='categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.SGD(lr=0.005),\r\n                metrics=['accuracy'])\r\n  return model\r\n\r\nmodel = get_keras_model()\r\nestimator = tf.keras.estimator.model_to_estimator(model, model_dir='./model')\r\n```", "comments": ["The bug can be reproduced. It seems that `input_tensor` gets lost when cloning model here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b2fe2a874bade4782aaca5c44bf29e7ff6c39200/tensorflow/python/estimator/keras.py#L310", "/cc @tanzhenyu", "/cc @facaiy See https://github.com/tensorflow/tensorflow/issues/17950#issuecomment-401988200", "@bhack Thanks for cc me. The analysis of @tanzhenyu is really useful. I'll look into the issue at weekend.", "@Youki Hi, could you test tf.keras.Model and see if the same error is raised ? Thanks.", "@facaiy Found that the problem can't be reproducible now. Both with/without tf.keras.Model are now working with TF >= 1.8.0. Is there any update that fix this problem???\r\n\r\n**In TensorFlow >= 1.8.0 (tested with 1.8.0 and 1.9.0-rc2) without tf.keras.Model**\r\n\r\n```\r\ndef get_keras_model():\r\n  model = tf.keras.Sequential()\r\n  model.add(tf.keras.layers.InputLayer(input_shape=[28*28]))\r\n  model.add(tf.keras.layers.Dense(300, activation='relu'))\r\n  model.add(tf.keras.layers.Dense(100, activation='relu'))\r\n  model.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n  model.compile(loss='categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.SGD(lr=0.005),\r\n                metrics=['accuracy'])\r\n  return model\r\n\r\nmodel = get_keras_model()\r\nestimator = tf.keras.estimator.model_to_estimator(model, model_dir='./model')\r\n```\r\n\r\n```\r\nINFO:tensorflow:Using the Keras model provided.\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fed725a5828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\n```\r\n\r\n**In TensorFlow >= 1.8.0 (tested with 1.8.0 and 1.9.0-rc2) with tf.keras.Model**\r\n\r\n```\r\ndef get_keras_model():\r\n  _model = tf.keras.Sequential()\r\n  _model.add(tf.keras.layers.InputLayer(input_shape=[28*28]))\r\n  _model.add(tf.keras.layers.Dense(300, activation='relu'))\r\n  _model.add(tf.keras.layers.Dense(100, activation='relu'))\r\n  _model.add(tf.keras.layers.Dense(10, activation='softmax'))\r\n  model = tf.keras.Model(inputs=_model.input, outputs=_model.output)\r\n  model.compile(loss='categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.SGD(lr=0.005),\r\n                metrics=['accuracy'])\r\n  return model\r\n\r\nmodel = get_keras_model()\r\nestimator = tf.keras.estimator.model_to_estimator(model, model_dir='./model')\r\n```\r\n\r\n```\r\nINFO:tensorflow:Using the Keras model provided.\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_model_dir': './model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fed6d9ceb38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\r\n```\r\n", "@Youki Thank you for feedback. I run both examples you provided with tf1.9-rc2, and find that Sequence still raises the exception, while Model works well.", "@facaiy You're right. The information I provided above was incorrect. I still have the same issue without tf.keras.Model. So sorry to confuse you. ", "@Youki Hi, according to [official document](https://keras.io/getting-started/sequential-model-guide/), it seems better to remove `InputLayer` and pass an input_shape argument to the second layer. Is the workaround below useful for you?\r\n\r\n```python\r\ndef get_keras_model():\r\n  model = tf.keras.Sequential()\r\n  model.add(tf.keras.layers.Dense(300, activation='relu', input_shape=[28*28]))\r\n  model.add(tf.keras.layers.Dense(100, activation='relu'))\r\n```\r\n\r\nAnd I also check the [unit test case](https://github.com/tensorflow/tensorflow/blob/f351e5ef6b36e96e633be2ebd9e1286b500fbce4/tensorflow/python/estimator/keras_test.py#L56), and find that they don't test Sequential model with `InputLayer`. \r\ncc @fchollet what do you think? Is it a bug?", "Nagging Assignees @fchollet, @pavithrasv: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@facaiy \r\nSorry for the very late reply. The workaround you suggested is very useful.", "I am also getting an error model_to_estimator() got an unexpected keyword argument 'checkpoint_format' with the newly changed model_to_estimator function in keras. \r\nTF version - 2.0.0b1\r\nKeras - 2.2\r\nI tried reproducing the example given in https://github.com/kashif/tf-keras-tutorial/blob/master/7-estimators-multi-gpus.ipynb and the line\r\nestimator = tf.keras.estimator.model_to_estimator(keras_model=model, config=config) throws the above error", "Adding Kathy who is working on the model to estimator API."]}, {"number": 20551, "title": "contrib.layers in 1.9.0-rc2 doesn't expose 1-d convolutions anymore", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n  yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version**: 1.9.0-rc2\r\n- **Python version**: Python 3.5.2 (and Python 2.7.13)\r\n- **CUDA/cuDNN version**: 9.0, V9.0.176\r\n- **GPU model and memory**: NVidia Tesla V100\r\n- **Exact command to reproduce**:\r\n```\r\n#!/usr/bin/env python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.layers as layers\r\nx = tf.zeros((1,10,1))\r\ny = layers.conv2d(x, kernel_size=1, num_outputs=1)      # Crashes with dimension error,\r\n# layers.convolution is not exposed.  layers.conv1d doesn't exist and layers.convolution1d isn't exposed.\r\n# But the below sleazy trick works:\r\nfrom tensorflow.contrib.layers.python.layers.layers import convolution\r\ny = convolution(x, kernel_size=1, num_outputs=1) \r\n```\r\n*From Environment Capture Script:\r\n== cat /etc/issue ===============================================\r\nLinux bn-gpuwkstn01 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nDGX_SWBUILD_VERSION=\"3.1.3\"\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 6.2.1-7ubuntu1~16.04.york0) 6.2.1 20161215\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux bn-gpuwkstn01 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy           1.14.5\r\nprotobuf        3.6.0\r\ntensorflow      1.9.0rc2\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.9.0-rc2\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Jul  4 17:05:57 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\r\n| N/A   32C    P0    60W / 300W |     28MiB / 16149MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\r\n| N/A   30C    P0    37W / 300W |     10MiB / 16149MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\r\n| N/A   30C    P0    35W / 300W |     10MiB / 16149MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\r\n| N/A   30C    P0    36W / 300W |     10MiB / 16149MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1755      G   /usr/lib/xorg/Xorg                            17MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n", "comments": ["`tf.contrib.layers.conv1d` is actually available, just not exposed. Added a PR #20554 for the fix to expose `tf.contrib.layers.conv1d`.", "Reassigning to a PR reviewer.", "Nagging Assignee @alextp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20550, "title": "Training speed much slower", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nThe code used is adapted from [Keras FastText](https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py)  || [Exact Code](https://github.com/frunkad/sent-aog/blob/master/imdb_fasttext.py)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 18.04 \r\n- **TensorFlow installed from (source or binary)**: Anaconda\r\n- **Bacel Version**: Not applicable\r\n- **TensorFlow version**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.4 :: Anaconda custom (64-bit)\r\n- **CUDA/cuDNN version**: 9.0, V9.0.176 \r\n- **GPU model and memory**: GeForce GTX 980M totalMemory: 7.93GiB\r\n- **Exact command to reproduce**: device specific\r\n\r\nFrom Environment Capture Script:\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux dracarys 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"18.04 LTS (Bionic Beaver)\"\r\nVERSION_ID=\"18.04\"\r\nVERSION_CODENAME=bionic\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux dracarys 4.15.0-23-generic #25-Ubuntu SMP Wed May 23 18:02:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.14.4     \r\nnumpydoc                           0.7.0      \r\nprotobuf                           3.5.2.post1\r\ntensorflow-gpu                     1.8.0      \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n/home/frunkad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Jul  5 02:05:17 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980M    Off  | 00000000:01:00.0  On |                  N/A |\r\n| N/A   48C    P0    31W /  N/A |    588MiB /  8118MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1530      G   /usr/lib/xorg/Xorg                            16MiB |\r\n|    0      1579      G   /usr/bin/gnome-shell                          48MiB |\r\n|    0      1762      G   /usr/lib/xorg/Xorg                           212MiB |\r\n|    0      1907      G   /usr/bin/gnome-shell                          88MiB |\r\n|    0      2394      G   ...-token=8EBF6E3FCA82B57AEE9186BDD7AFEAF3   108MiB |\r\n|    0      2902      G   ...-token=FA087B07103596ED40954ABE2071373B   105MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/lib64/libcudart_static.a\r\n/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n\r\n```\r\n### Describe the problem\r\nFrom the keras code the GTX 980M should have taken 2s for each epoch, but it takes around 58s. Also the same code runs within 10s on colab. The possible reason as per me is that I am using it with Ubuntu 18. Should I give it a try on Windows on same machine?\r\n\r\n### Source code / logs\r\n\r\n```\r\n/home/frunkad/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nLoading data...\r\n25000 train sequences\r\n25000 test sequences\r\nAverage train sequence length: 238\r\nAverage test sequence length: 230\r\nAdding 2-gram features\r\nAverage train sequence length: 476\r\nAverage test sequence length: 428\r\nPad sequences (samples x time)\r\nx_train shape: (25000, 400)\r\nx_test shape: (25000, 400)\r\nBuild model...\r\nTrain on 25000 samples, validate on 25000 samples\r\nEpoch 1/5\r\n2018-07-05 01:26:20.525149: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-07-05 01:26:20.601376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-05 01:26:20.601794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.32GiB\r\n2018-07-05 01:26:20.601813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-07-05 01:26:20.751183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-05 01:26:20.751214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0\r\n2018-07-05 01:26:20.751220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N\r\n2018-07-05 01:26:20.751487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7064 MB memory)-> physical GPU (device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n25000/25000 [==============================] - 58s 2ms/step - loss: 0.6651 - acc: 0.6945 - val_loss: 0.6278 - val_acc: 0.7829\r\nEpoch 2/5\r\n25000/25000 [==============================] - 57s 2ms/step - loss: 0.5572 - acc: 0.8524 - val_loss: 0.5116 - val_acc: 0.8378\r\nEpoch 3/5\r\n25000/25000 [==============================] - 57s 2ms/step - loss: 0.4034 - acc: 0.9102 - val_loss: 0.4015 - val_acc: 0.8692\r\nEpoch 4/5\r\n25000/25000 [==============================] - 58s 2ms/step - loss: 0.2781 - acc: 0.9420 - val_loss: 0.3359 - val_acc: 0.8827\r\nEpoch 5/5\r\n25000/25000 [==============================] - 58s 2ms/step - loss: 0.1966 - acc: 0.9619 - val_loss: 0.2986 - val_acc: 0.8923\r\n``` ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "Bazel version: not applicable", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20549, "title": "Check parameters before creating gRPC server", "body": "When creating gRPC server with ```task_index=-1```, TF just aborts (core dumped):\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> cluster = tf.train.ClusterSpec({\"ps\": [\"localhost:2222\"]}) \r\n>>> server = tf.train.Server(cluster, job_name=\"ps\", task_index=2)\r\n// error msg...\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Task 2 was not defined in job \"ps\"\r\n\r\n>>> server = tf.train.Server(cluster, job_name=\"ps\", task_index=-1)\r\n2018-07-05 00:59:49.609445: F tensorflow/core/common_runtime/device.cc:28] Check failed: DeviceNameUtils::ParseFullName(name(), &parsed_name_) Invalid device name: /job:ps/replica:0/task:-1/device:CPU:0\r\nAborted (core dumped)\r\n```\r\n\r\nBoth -1 and 2 are invalid task indices, but the results are totally different. I think it's better to give the same message as when task_index=2\r\n\r\nThis can be solved by moving the parameter checking part to earlier place, just before adding devices. \r\n\r\nAlthough the parameter checking part seems not intended to check parameters in the original place, it actually filters all invalid parameters.\r\n", "comments": ["Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20548, "title": " Use abs() rather than sqrt() for mfcc mel filter to prepare the spectrum", "body": "It's more sensible to use power spectrum than amplitude spectrum.\r\n\r\nAnd we can use \"magnitude_squared=False\" for amplitude spectrum.", "comments": ["We cannot change the behavior of this unless this is a straightforward bug. What does the documentation say about this?", "According to [Wikipedia](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), \r\n\r\n> MFCCs are commonly derived as follows:\r\n> 1. Take the Fourier transform of (a windowed excerpt of) a signal.\r\n> 2. Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.\r\n> 3. Take the logs of the powers at each of the mel frequencies.\r\n> 4. Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\r\n> 5. The MFCCs are the amplitudes of the resulting spectrum.\r\n\r\nBut the [standardised MFCC algorithm](https://www.etsi.org/deliver/etsi_es/201100_201199/201108/01.01.03_60/es_201108v010103p.pdf)  defined by ETSI seems to use amplitude spectrum.\r\n\r\nI did more research online, and found out that some paper use power spectrum[1][2][3] while the other use amplitude spectrum[4][5]. Some ppts from classes in university show that professors prefer power spectrum.\r\n\r\nFor me, it's weird to square the \"magnitude_squared=True\" spectrum to get MFCCs from power spectrum. And if I want to use some different spectrum (diff(FFT(x)) for example), I'll have to square the spectrum before MFCCs feature extract.\r\n\r\n\r\n[1]: A. Dev, \u201cRobust Features for Noisy Speech Recognition using MFCC Computation from Magnitude Spectrum of Higher Order Autocorrelation Coefficients,\u201d Int. J. Comput. Appl., vol. 10, no. 8, pp. 36\u201338, 2010.\r\n[2]: A. Mohammed et al., \u201cTechniques for Feature Extraction In Speech Recognition System: A Comparative Study,\u201d Int. J. Comput. Appl. Eng. Technol. Sci., vol. 2, no. 1, pp. 1\u20139, 2013. Time-frequency representations Speech Processing Tom B\u00e4ckstr\u00f6m Aalto University October 2015\r\n[3]: S. G. Koolagudi, D. Rastogi, and K. S. Rao, \u201cIdentification of language using mel-frequency cepstral coefficients (MFCC),\u201d in Procedia Engineering, 2012, vol. 38, pp. 3391\u20133398.\r\n[4]: B. Logan, \u201cMel Frequency Cepstral Coefficients for Music Modeling,\u201d Int. Symp. Music Inf. Retr., vol. 28, p. 11p., 2000.\r\n[5]: S. K. Kopparapu and M. Laxminarayana, \u201cChoice of Mel filter bank in computing MFCC of a resampled speech,\u201d in 10th International Conference on Information Sciences, Signal Processing and their Applications, ISSPA 2010, 2010, pp. 121\u2013124.", "It's my understanding that the choice is task dependent. I would suggest that people who want MFCCs use [`tf.contrib.signal.mfccs_from_log_mel_spectrograms`](https://www.tensorflow.org/api_docs/python/tf/contrib/signal/mfccs_from_log_mel_spectrograms), which gives you the choice of whether to use amplitude or power (or power in decibels, as librosa does). \r\n\r\nThis also gets you GPU and TPU support, which this C++ op doesn't :).", "Nagging Assignee @martinwicke: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We cannot change the behavior of the stable API. I think we should consider deprecating this function and instead find a good place for contrib/signal, so users can get it from there. "]}, {"number": 20547, "title": "can't import tensorflow", "body": "when i try to import tensorflow, i got this traceback:\r\n################################\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 903, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 903, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n################################\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nI'm using Python3.5 and tensorflow 1.8.0\r\nAny help would be very appreciated!\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have you tried the suggestion on this stack overflow page? It suggest that the issue may be related to CUDA/cudnn. \r\n\r\n> Try to rename cudnn64_6.dll to cudnn64_5.dll in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin.\r\n\r\nhttps://stackoverflow.com/questions/43577923/cannot-import-tensorflow-for-gpu-on-windows-10.", "Os platform: window 7 64bit sp1\r\nTensorflow installed from pip install tensorflow\r\nTensorflow version: 1.8.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model: N/A", "Have the same issue with current Tensorflow 1.9.0 CPU - Win10 64bit, Python 3.6.\r\nOlder Tensorflow 1.5.1 CPU - works ok. \r\n", "Can you use the DependencyWalker on the tensorflow DLL to see what you are missing?", "I found out I have legacy core i5 CPU without AVX support that caused the issue. You may ignore my comment. Thanks for help.", "sorry my bad. i totally forgot. how can i remove the tag?", "Is that still a problem? Did you use the DependencyWalker to check? Usually it's that the nvidia libs are not installed, the wrong version, or not in the right place.", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @drpngx: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 20546, "title": "Fix a small compilation failed bug #20545", "body": "Fixed: #20545\r\nLine 86: `details.name += \":\" + string(profiling_string);` should be `details.name += \":\" + std::string(profiling_string);`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi Nandi Wang (@slobber),\r\nWe are working on identifying redundant development and duplicate pull requests. We have found there is a pull request: https://github.com/tensorflow/tensorflow/pull/20330 which might be duplicate to this one and already merged into main stream. So maybe this pull request could be closed.\r\nThank you very much for your time!\r\n", "That looks like a correct duplicate detection, I'll close this PR. Thanks!"]}, {"number": 20545, "title": "Bug in tflite benchmark_model", "body": "```\r\nbazel build -c opt   --config=android_arm   --cxxopt='--std=c++11'   tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n```\r\n\r\nThe compilation output is:\r\n```\r\nINFO: Analysed target //tensorflow/contrib/lite/tools/benchmark:benchmark_model (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/tensorflow/tensorflow/contrib/lite/profiling/BUILD:44:1: C++ compilation of rule '//tensorflow/contrib/lite/profiling:profile_summarizer' failed (Exit 1): clang failed: error executing command \r\n  (cd /usr/local/google/home/nandiw/.cache/bazel/_bazel_nandiw/caad1bccb22acfdb24a81d2c3569fb6e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=28.0.0 \\\r\n    ANDROID_NDK_API_LEVEL=16 \\\r\n    ANDROID_NDK_HOME=/usr/local/google/home/nandiw/Developments/android-ndk-r16b \\\r\n    ANDROID_SDK_API_LEVEL=28 \\\r\n    ANDROID_SDK_HOME=/usr/local/google/home/nandiw/Developments/android-sdk \\\r\n    PATH=/usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/venv/bin:/usr/local/google/home/nandiw/Developments/google-cloud-sdk/bin:/usr/lib/google-golang/bin:/usr/local/buildtools/java/jdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/google/home/nandiw/Developments/android-sdk/platform-tools:/usr/local/google/home/nandiw/Developments/nodejs/npm/lib/node_modules/@angular/cli/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/venv/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/google/home/nandiw/Projects/tensorflow/tflite_tools/venv/lib/python3.6/site-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin/clang '-D__ANDROID_API__=16' -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/profiling/_objs/profile_summarizer/tensorflow/contrib/lite/profiling/profile_summarizer.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/profiling/_objs/profile_summarizer/tensorflow/contrib/lite/profiling/profile_summarizer.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/armeabi-v7a-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-opt/genfiles/external/gemmlowp -iquote external/flatbuffers -iquote bazel-out/armeabi-v7a-opt/genfiles/external/flatbuffers -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-v7a-opt/bin/external/eigen_archive -isystem tensorflow/contrib/lite/schema -isystem bazel-out/armeabi-v7a-opt/genfiles/tensorflow/contrib/lite/schema -isystem bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/armeabi-v7a-opt/genfiles/external/flatbuffers/include -isystem bazel-out/armeabi-v7a-opt/bin/external/flatbuffers/include '--std=c++11' -Wall -DFARMHASH_NO_CXX_STRING '-mfpu=neon' '-mfloat-abi=softfp' '-std=c++11' -O3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '--sysroot=external/androidndk/ndk/platforms/android-16/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -isystemexternal/androidndk/ndk/sysroot/usr/include -c tensorflow/contrib/lite/profiling/profile_summarizer.cc -o bazel-out/armeabi-v7a-opt/bin/tensorflow/contrib/lite/profiling/_objs/profile_summarizer/tensorflow/contrib/lite/profiling/profile_summarizer.o)\r\ntensorflow/contrib/lite/profiling/profile_summarizer.cc:86:27: error: use of undeclared identifier 'string'\r\n    details.name += \":\" + string(profiling_string);\r\n                          ^\r\n1 error generated.\r\nTarget //tensorflow/contrib/lite/tools/benchmark:benchmark_model failed to build\r\n```\r\n\r\nThe problem is in:\r\nhttps://github.com/tensorflow/tensorflow/blob/b2fe2a874bade4782aaca5c44bf29e7ff6c39200/tensorflow/contrib/lite/profiling/profile_summarizer.cc#L86\r\n\r\n`details.name += \":\" + string(profiling_string);` should be `details.name += \":\" + std::string(profiling_string);` \r\n", "comments": ["I sent a PR https://github.com/tensorflow/tensorflow/pull/20330 for this couple days ago", "@freedomtan Thank you for fixing the bug."]}, {"number": 20544, "title": "Changing minor tolerance to pass linalg_grad_test on ppc64le", "body": "Fixes: https://github.com/tensorflow/tensorflow/issues/19935\r\n\r\nThanks!", "comments": ["@martinwicke  Could you please review this PR.", "@rmlarsen As per your suggestion I have updated the PR , please review. Thanks !", "@rmlarsen, Can you please review the updated pull request ? ", "Nagging Assignee @martinwicke: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixed in https://github.com/tensorflow/tensorflow/commit/422158776bcd9ffbde485610fdd3af498a2d5669, hence closing."]}, {"number": 20543, "title": "how to run multiple models parallel in different graph?", "body": "hi, all:\r\n  I have a machine with 2 GPUs. How could I train 2 models parallel with 2 sub-processes in seperable graphs please?\r\n  I tried this:\r\n\r\n```python\r\nloaded_data = load_data(dir_name)\r\nGPU_NUM = 2\r\nfor i in range(GPU_NUM):\r\n    with tf.device('/gpu:{}'.format(i)):\r\n        with tf.Graph().as_default():\r\n            sub_process = multiprocessing.Process(target=func, args=(loaded_data))\r\n```\r\n \r\n  I called nvidia-smi, but only one GPU is running.\r\n  Any suggestions would be appreciated. \r\n  Thanks a lot:)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 20542, "title": "[MSVC] Disable C++ exceptions", "body": "In MSVC, the flag to disable exception is `/EHs-c-`. `/EHsc` will enable exception (i.e. Tensorflow have been built with exception on Windows for years).\r\n\r\nCMake add `/EHsc` for MSVC by default. We use regex hack to replace it with `/EHs-c-`.\r\n\r\nBazel's `CROSSTOOL` also add `/EHsc` for MSVC by default, currently there is no way remove it unless we use `nocopts` for every `cc_library`. Wait for a fix from Bazel's side. Those who use custom `CROSSTOOL` can do this themselves.\r\n\r\nDrive-by fix: remove some unused flags in CMake.", "comments": ["@meteorcloudy Can you help triggering the CI test? Thanks.", "All tests passed. Windows CMake is giving tons of warning: `'noexcept' used with no exception handling mode specified; termination on exception is not guaranteed`, so we suppress it with `/wd4577`."]}, {"number": 20541, "title": "[Bazel/MSVC] Make Kafka builds on Windows", "body": "Working towards enabling `//tensorflow/contrib/kafka` for Bazel on Windows.", "comments": ["@rongjiecomputer Is this PR ready to be reviewed?", "Yes. The actual change to enable `//tensorflow/contrib/kafka` will be another PR.", "Windows Bazel builds all failed at link step: `LINK : fatal error LNK1000: unknown error at ...`. Probably just flake.\r\n\r\nMacOS Contrib also failed with mysterious \"Internal CI infrastructure error\".\r\n\r\nRebased against master, please trigger CI test again."]}, {"number": 20540, "title": "[Bazel] Bump boringssl version", "body": "Update boringssl from 18 Oct 2017 to 4 July 2018. Contain a few fixes for Bazel on Windows.\r\n\r\n/cc @yifeif ", "comments": ["Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}]