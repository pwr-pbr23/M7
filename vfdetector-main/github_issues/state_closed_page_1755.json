[{"number": 236, "title": "Getting Started variable name mismatch", "body": "In http://tensorflow.org/get_started/basic_usage.md#variables\n\nthe variable in the example is initialized as `var`, then subsequently referred to as `state`\nleading to an obvious NameError:\n\n```\nTraceback (most recent call last):\n  File \"./variables.py\", line 8, in <module>\n    new_value = tf.add(state, one)\nNameError: name 'state' is not defined\n```\n", "comments": ["It's updated in our repo, but not on our website yet.  We're going to update the website to reflect the current state of the repo soon.\n\n(We get this question a lot, has it been hard to search for in our existing issues list?)\n", "apologies @vrv -- just found a previous issue.\n"]}, {"number": 235, "title": "the tutorial Sequence to Sequence Models has errors", "body": "first: the bazel downloading statement is missing a -- after the -c opt:\nsecond : the directory translate does not exist in the directory rnn, neither the file translate.py\n", "comments": ["i installed tensorflow through pip and it did not install the translate directory that I found on git\n"]}, {"number": 234, "title": "translate example is missing '--' in its 'run' command", "body": "There should be a '--' after 'opt'.\nbazel run -c opt -- tensorflow/models/rnn/translate:translate --data_dir <DIR>\n", "comments": ["bazel run [options] [target] -- [--commandlineflags]\n", "Shouldn't we update the tutorial?  In the translate example, the command is:\n\nbazel run -c opt <...>/models/rnn/translate:translate\n  --data_dir [your_data_directory] --train_dir [checkpoints_directory]\n  --en_vocab_size=40000 --fr_vocab_size=40000\n\nwhich doesn't work because it's missing '--'  before arguments\n", "We've already fixed it in our internal repo :)  We need to push out the change early this week to both git and the website -- thanks for the feedback!\n"]}, {"number": 233, "title": "Need a way to ask users what version of tensorflow they are running", "body": "And ideally where they got it from.  Something like `tf.version`?\n", "comments": ["This would help diagnose stuff likehttps://github.com/tensorflow/tensorflow/issues/229\n", "This would help diagnose stuff like https://github.com/tensorflow/tensorflow/issues/229\n", "https://github.com/numpy/numpy/blob/626eb3748d067ea6e4c4dd308252bfad97df186f/setup.py#L123 is how numpy does it \n", "Excellent, that looks like a good thing to copy.\n", "Done: `tf.__version__`.\n"]}, {"number": 232, "title": "NameError: name 'init' is not defined", "body": "I am getting the following error while trying MNIST tutorial\n\nsessi.run(init,init)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'init' is not defined\n", "comments": ["That looks like a typo on your end (I can't find that snippet of code anywhere) -- please re-open with a way for us to reproduce the problem if there's a bug in our tutorial.  Thanks!\n", "I see typo in the post above, 'sessi' instead of 'sess'. \r\n\r\nI have run into same issue though, And here's my code:\r\n\r\nnum_neurons = 200\r\nnum_layers = 3\r\ndropout = tf.placeholder(tf.float32)\r\n\r\ncell = rnn_cell.GRUCell(num_neurons)  # Or LSTMCell(num_neurons)\r\ncell = rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\r\ncell = rnn_cell.MultiRNNCell([cell] * num_layers)\r\n\r\n#Simulating Time Steps\r\nmax_length = Sz_Max_GST #Sz_Max_LLD\r\n#Batch size x time steps x features.\r\ndata = tf.placeholder(tf.float32, [None, max_length, Num_F_L])\r\ntarget = tf.placeholder(tf.float32, [None, max_length, Num_G_L])\r\noutput, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32,sequence_length=mylength(data))\r\nyLogits=tf.contrib.layers.linear(output,1)\r\n\r\nprediction=yLogits\r\ncross_entropy = -tf.reduce_sum(target * tf.log(prediction))\r\n\r\ntrain_step=tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\r\nsess=tf.Session()\r\nsess.run(init)", "No worries, I figured it out. :)\r\n\r\nOne should declare init as \r\ninit=tf.global_variables_initializer()"]}, {"number": 231, "title": "cannot enable peer access from device ordinal 0 to device ordinal 1", "body": "I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties: \nname: GeForce GTX 680\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0585\npciBusID 0000:02:00.0\nTotal memory: 2.00GiB\nFree memory: 1.95GiB\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 1 with properties: \nname: GeForce GTX 480\nmajor: 2 minor: 0 memoryClockRate (GHz) 1.401\npciBusID 0000:01:00.0\nTotal memory: 1.50GiB\nFree memory: 929.24MiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:611] Ignoring gpu device (device: 0, name: GeForce GTX 680, pci bus id: 0000:02:00.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:611] Ignoring gpu device (device: 1, name: GeForce GTX 480, pci bus id: 0000:01:00.0) with Cuda compute capability 2.0. The minimum required Cuda capability is 3.5.\nI tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 4\nInitialized!\nEpoch 0.00\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nEpoch 0.12\nMinibatch loss: 3.285, learning rate: 0.010000\nMinibatch error: 6.2%\nValidation error: 7.0%\n## Epoch 0.23\n\nI have gtx680 and gtx480(I know the two GPUs do not have peer accesses),when I run the example mnist,I got these problems and it  runs very slowly.\nAm I running without GPUs or GPU??\nCan I run with only one GPU,  e.g. GTX680?\nHow? Thanks~\n", "comments": ["Your GPUs are a little too old to be used in our 0.5.0 release.  See https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658 for how to build from source if you want to try to get cuda 2.0/3.0 cards working.\n\nFrom there, you can also use the environment variable [CUDA_VISIBLE_DEVICES](http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/) to control which GPUs are exposed to the process.\n", "@vrv  is this common? :)\n\n```\n[./simpleP2P] - Starting...\nChecking for multiple GPUs...\nCUDA-capable device count: 16\n> GPU0 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU1 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU2 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU3 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU4 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU5 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU6 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU7 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU8 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU9 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU10 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU11 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU12 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU13 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU14 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n> GPU15 = \"      Tesla K80\" IS  capable of Peer-to-Peer (P2P)\n\nChecking GPU(s) for support of peer to peer memory access...\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU1) : Yes\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU2) : Yes\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU3) : Yes\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU4) : Yes\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU5) : Yes\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU6) : Yes\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU7) : Yes\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU0) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU0) : Yes\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU2) : Yes\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU3) : Yes\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU4) : Yes\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU5) : Yes\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU6) : Yes\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU7) : Yes\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU1) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU0) : Yes\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU1) : Yes\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU3) : Yes\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU4) : Yes\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU5) : Yes\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU6) : Yes\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU7) : Yes\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU2) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU0) : Yes\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU1) : Yes\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU2) : Yes\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU4) : Yes\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU5) : Yes\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU6) : Yes\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU7) : Yes\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU3) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU0) : Yes\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU1) : Yes\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU2) : Yes\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU3) : Yes\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU5) : Yes\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU6) : Yes\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU7) : Yes\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU4) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU0) : Yes\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU1) : Yes\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU2) : Yes\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU3) : Yes\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU4) : Yes\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU6) : Yes\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU7) : Yes\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU5) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU0) : Yes\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU1) : Yes\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU2) : Yes\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU3) : Yes\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU4) : Yes\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU5) : Yes\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU7) : Yes\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU6) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU0) : Yes\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU1) : Yes\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU2) : Yes\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU3) : Yes\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU4) : Yes\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU5) : Yes\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU6) : Yes\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU8) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU9) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU10) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU11) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU12) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU13) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU14) : No\n> Peer access from Tesla K80 (GPU7) -> Tesla K80 (GPU15) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU9) : Yes\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU10) : Yes\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU11) : Yes\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU12) : Yes\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU13) : Yes\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU14) : Yes\n> Peer access from Tesla K80 (GPU8) -> Tesla K80 (GPU15) : Yes\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU8) : Yes\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU10) : Yes\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU11) : Yes\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU12) : Yes\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU13) : Yes\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU14) : Yes\n> Peer access from Tesla K80 (GPU9) -> Tesla K80 (GPU15) : Yes\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU8) : Yes\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU9) : Yes\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU11) : Yes\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU12) : Yes\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU13) : Yes\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU14) : Yes\n> Peer access from Tesla K80 (GPU10) -> Tesla K80 (GPU15) : Yes\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU8) : Yes\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU9) : Yes\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU10) : Yes\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU12) : Yes\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU13) : Yes\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU14) : Yes\n> Peer access from Tesla K80 (GPU11) -> Tesla K80 (GPU15) : Yes\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU8) : Yes\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU9) : Yes\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU10) : Yes\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU11) : Yes\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU13) : Yes\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU14) : Yes\n> Peer access from Tesla K80 (GPU12) -> Tesla K80 (GPU15) : Yes\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU8) : Yes\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU9) : Yes\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU10) : Yes\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU11) : Yes\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU12) : Yes\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU14) : Yes\n> Peer access from Tesla K80 (GPU13) -> Tesla K80 (GPU15) : Yes\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU8) : Yes\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU9) : Yes\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU10) : Yes\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU11) : Yes\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU12) : Yes\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU13) : Yes\n> Peer access from Tesla K80 (GPU14) -> Tesla K80 (GPU15) : Yes\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU0) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU1) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU2) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU3) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU4) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU5) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU6) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU7) : No\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU8) : Yes\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU9) : Yes\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU10) : Yes\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU11) : Yes\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU12) : Yes\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU13) : Yes\n> Peer access from Tesla K80 (GPU15) -> Tesla K80 (GPU14) : Yes\n```\n", "I'm not sure what you mean by common, but that looks like a reasonable peering topology...\n", "@GaofengCheng  Are you using P2 instance of AWS? How did you configure to result that?"]}, {"number": 230, "title": "document error", "body": "I was following the getting started tutorial and found one issue in this http://tensorflow.org/get_started/basic_usage.md#overview. \n\nthere is undefined name \"state\" in the example of Variables section. I think it should be \"var\" but I am new to tensorflow. Please confirm and update the doc so that others don't get confused.  \n", "comments": ["Thanks for the report -- this has been updated in our repository but not yet pushed to the website.  Will probably be updated this week!\n"]}, {"number": 229, "title": "from __future__ import division gives error in word2vec_basic when dividing tensors.Just comment out \"from __future__....\"", "body": "", "comments": ["Are you running a version of the tutorials from HEAD against tensorflow installed via pip or similar?  We just added `__truediv__` support as part of #1. \n", "Was running python 2.7......\n", "Yep, you're running 2.7, but how did you install tensorflow and where did you get the tutorial?  I think you might be running a newer version of the tutorial than tensorflow itself. \n", "I pip installed it but I run the tutorial from the repository after git fetched\nit.\n\nSent with Mixmax\n\nOn Sun, Nov 15, 2015 at 7:44 PM, Geoffrey Irving < notifications@github.com > wrote:\nYep, you're running 2.7, but how did you install tensorflow and where did you\nget the tutorial? I think you might be running a newer version of the tutorial\nthan tensorflow itself.\n\n\u2014\nReply to this email directly or view it on GitHub .\n", "@arieltci: For now, deleting that import is the right thing for you to do.  @vrv: Not sure if we should close or remove that division from the tutorial so that it's more backwards compatible. \n", "I'm going to fix the tutorial in git to be compatible back to 0.5.0.\n", "Fixed, will be part of the next git commit.  Thanks for reporting!\n"]}, {"number": 228, "title": "AttributeError in Tensor", "body": "While running workd2vec_basic.py as is, an AttributeError rises when dividing tesors. \n\nAttributeError: type object 'Tensor' has no attribute '**truediv**'\n\nDividing arbitrary tensors inside the console in a python session would not give such error. Maybe multiple class definitions for Tensor?\n", "comments": []}, {"number": 227, "title": "Compute capability < 3.5", "body": "I have a little bit old GPU card with compute capability 2.0 (GeForce GTX 560).\nIs there a way to get TensorFlow work with this card?\n", "comments": ["If you're comfortable building from source, https://github.com/tensorflow/tensorflow/issues/25#issuecomment-156234658 should have an answer for you, though we haven't tested it on anything below 3.0.\n\nWill de-dupe this bug with that one, thanks!\n", "@marker68 I know 2 years have passed, but did you finally managed to get it running?\r\nI just got myself 5x Tesla m2075 6GB VRAM and wonder if I can do anything productive with it in terms of deep learning.", "I also have the same card, 2.0 computational capacity, looking for a solution. ", "I have 2.1 , it is only after downloading everything that  I go this :( I hope there will be a build for this (Windows)", "You can have a look at [this](https://stackoverflow.com/questions/38542763/how-can-i-make-tensorflow-run-on-a-gpu-with-capability-2-0), but what I got were errors  during attempt to obtain memory buffer size, which ended in an InternalError"]}, {"number": 226, "title": "`from tensorflow.g3doc...` is Broken", "body": "There are a dozen places in `g3doc/how_tos` and `g3doc/tutorials` where things are imported from the `g3doc` directory. \n\nThe `g3doc` directory isn't included in the installation, and `tensorflow`, intentionally, can't be imported from the source directory, so those are likely all broken.\n", "comments": ["Any workarounds in the meantime?\n", "Yes, just remove the path name before the import. Change:\nfrom tensorflow.g3doc.tutorials.mnist import input_data\nfrom tensorflow.g3doc.tutorials.mnist import mnist\nto :\nimport input_data\nimport mnist\n", "Should be fixed in 0.6.0\n"]}, {"number": 225, "title": "Cannot get TensorBoard example working", "body": "Here's my code:\n\n```\nmerged_summary_op = tf.merge_all_summaries()\nsummary_writer = tf.train.SummaryWriter('/tmp/mnist_logs', sess.graph_def)\nfor i in range(200):\n    batch = mnist.train.next_batch(50)\n    sess.run(train_step, feed_dict={x: batch[0], y_: batch[1])\n    if i % 100 == 0:\n        train_accuracy = sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})\n        summary_str = sess.run(merged_summary_op, feed_dict={x: batch[0], y_: batch[1]})\n        summary_writer.add_summary(summary_str, i)\n```\n\nbut got this error at line `sess.run(merged_summary_op, ...)`\n\n```\nInvalidArgumentError                      Traceback (most recent call last)\n<ipython-input-39-f56ad86fef0b> in <module>()\n----> sess.run(merged_summary_op, feed_dict={x: batch[0], y_: batch[1]})\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict)\n    343 \n    344     # Run request and get response.\n--> 345     results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n    346 \n    347     # User may have fetched the same tensor multiple times, but we\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, target_list, fetch_list, feed_dict)\n    417         # pylint: disable=protected-access\n    418         raise errors._make_specific_exception(node_def, op, e.error_message,\n--> 419                                               e.code)\n    420         # pylint: enable=protected-access\n    421       raise e_type, e_value, e_traceback\n\nInvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float\n```\n\nThe error message `you must feed a value for placeholder tensor` doesn't make much sense. The line above that with similar structure ran just fine.\n", "comments": ["Hey @FabHan, can you post the entire code example? Will be easier for me to help debug. You could use a https://gist.github.com/ \n", "I'm having trouble reproducing your error, but here's a working example in which the entire mnist_softmax tutorial has been modified into interact well with TensorBoard. (I'll merge this code example into the TensorBoard tutorial to keep people from running into your issue in the future.)\n\n``` python\n\"\"\"A very simple MNIST classifer, modified to display data in TensorBoard\n\nSee extensive documentation for the original model at\nhttp://tensorflow.org/tutorials/mnist/beginners/index.md\n\nSee documentaion on the TensorBoard specific pieces at\nhttp://tensorflow.org/how_tos/summaries_and_tensorboard/index.md\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# Import data\nimport input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\n# Create the model\nx = tf.placeholder(\"float\", [None, 784], name=\"x-input\")\nW = tf.Variable(tf.zeros([784,10]), name=\"weights\")\nw_hist = tf.histogram_summary(\"weights\", W)\nb = tf.Variable(tf.zeros([10], name=\"bias\"))\nb_hist = tf.histogram_summary(\"biases\", b)\nwith tf.name_scope(\"Wx_b\") as scope:\n  y = tf.nn.softmax(tf.matmul(x,W) + b)\ny_hist = tf.histogram_summary(\"y\", y)\n\n# Define loss and optimizer\ny_ = tf.placeholder(\"float\", [None,10], name=\"y-input\")\nwith tf.name_scope(\"xent\") as scope:\n  cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n  ce_summ = tf.scalar_summary(\"cross entropy\", cross_entropy)\nwith tf.name_scope(\"train\") as scope:\n  train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\nwith tf.name_scope(\"test\") as scope:\n  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n  accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n\nmerged = tf.merge_all_summaries()\nwriter = tf.train.SummaryWriter(\"/tmp/mnist_logs\", sess.graph_def)\ntf.initialize_all_variables().run()\n\n# Test trained model\n\nfor i in range(1000):\n  if i % 10 == 0:  # Record summary data, and the accuracy\n    feed = {x: mnist.test.images, y_: mnist.test.labels}\n    result = sess.run([merged, accuracy], feed_dict=feed)\n    summary_str = result[0]\n    acc = result[1]\n    writer.add_summary(summary_str, i)\n    print(\"Accuracy at step %s: %s\" % (i, acc))\n  else:\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    feed = {x: batch_xs, y_: batch_ys}\n    sess.run(train_step, feed_dict=feed)\n\nprint(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\n```\n\nIf you post your full code so I can run it and repro your error, I'll try to help figure out why TensorFlow was throwing that error :)\n", "Thanks! I don't know yet what I did wrong but your example works.\n", "Wow!  Tensorboard is so amazing.   \n\nRan into some issues using the example you've provided with the docker install, not sure if I'm the only one.  but fwiw, here they are:\n\nWeb server couldn't find some icons:\nNo such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/lib/svg/summary-icon.svg' on path /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/lib/svg/summary-icon.svg\n\nAlso, when I click on the various icons, I'm not seeing much in the way of data.  Is that right? Maybe it is :D  For example, test namespace -> \n\ntest\u200b/ScalarSummary\n\nOperation: ScalarSummary\nAttributes (1)\nT\n{\"type\":\"DT_FLOAT\"}\nInputs (0)\nOutputs (0)\n\nor on Wx_b namespace and then Histogram:\n\nHistogramSummary_2\n\nOperation: HistogramSummary\nAttributes (0)\nInputs (0)\nOutputs (0)\n\nThe histograms and events tabs seem to be showing up properly.  Cheers\n", "I was getting the same error although my code looked similar to the one above. I'm using the interactive Jupyter Notebook environment, and I figured out that shutting down and restarting the notebook somehow solved my problem. \n", "same error, can confirm mine was also a jupyter notebook problem. I think this arises when running the graph definition more than once (there are _all sorts_ of problems with this!).\n", "I had similar issues with jupyter notebooks and discovered that explicitly flushing and closing the summary writer helped. \n\n```\ntw = tf.train.SummaryWriter('./logDir',sess.graph)\n\n# Train your network\n\ntw.flush()\ntw.close()\n```\n", "I had the same error when I run the script in ipython shell. Interestingly, it worked fine in python shell. This was my script: https://gist.github.com/sergulaydore/e9d768356ab004c2bf97ca171312f43d\n", "when I use:\r\nfor i in range(1000):\r\n    batch_xs, batch_ys = mnist.train.next_batch(100)\r\n    train_step.run({x: batch_xs, y: batch_ys})\r\n\r\nI got \r\nInvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float\r\n\r\nand it runs as blow:\r\nfor i in range(1000):\r\n    feed = {x: mnist.train.images, y_: mnist.train.labels}\r\n    train_step.run(feed)", "Can confirm that this issue seems to still appear - I'd second @achristensen56 and @serhannn in that noting when running graph definitions multiple times causes it to ragequit, and restarting Jupyter fixes it.\r\n\r\nIt's a corker of an error to run into as you end up questioning your own understanding of maths :p", "i have the same error in my jupyter notebook.\r\n\r\nfirst running,it works well.but after that,it just goes wrong like you.i have no idea about how to fix it. just restart jupyter.\r\n\r\nmaybe because the jupyter will store the value for us .\r\n\r\ni have no idea.\r\n\r\nmaybe jupyter is not suitable for tensorboard . i  give up jupyter.just using python shell", "while running the above example, it ran successfully , but while calling tensorborad it gives the below error. Please help.\r\n\r\nAccuracy at step 980: 0.9151\r\nAccuracy at step 990: 0.9156\r\n0.9147\r\nAbhisheks-MacBook-Air:POC abhi$ tensorboard --logdir=/tmp/mnist_logs\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Library not loaded: @rpath/libcublas.8.0.dylib\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n  Reason: image not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py\", line 242, in load_module", "i am using python 2.7", "Hello, I am using these versions :--\r\n\r\n\r\nAbhisheks-MacBook-Air:tensorflow abhi$ pip show tensorflow\r\nName: tensorflow\r\nVersion: 1.1.0\r\nSummary: TensorFlow helps the tensors flow\r\nHome-page: http://tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: wheel, protobuf, numpy, mock, werkzeug, six\r\nAbhisheks-MacBook-Air:tensorflow abhi$ pip show protobuf\r\nName: protobuf\r\nVersion: 3.3.0\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: 3-Clause BSD License\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: setuptools, six\r\nAbhisheks-MacBook-Air:tensorflow abhi$ pip show six\r\nName: six\r\nVersion: 1.10.0\r\nSummary: Python 2 and 3 compatibility utilities\r\nHome-page: http://pypi.python.org/pypi/six/\r\nAuthor: Benjamin Peterson\r\nAuthor-email: benjamin@python.org\r\nLicense: MIT\r\nLocation: /usr/local/lib/python2.7/site-packages\r\nRequires: \r\nAbhisheks-MacBook-Air:tensorflow abhi$ python\r\nPython 2.7.13 (default, Apr  4 2017, 08:47:57) \r\n[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> ", "I faced same problem with Jupyter notebook.\r\nThe reason is not clear yet. But I agree with achristensen56-san's comment.\r\n\r\nIf I run scripts more than 2 times, including # placeholder, I faced same issue.\r\n\r\nUnfortunatelly. hblasins-san's method didn't work well in my case.\r\n\r\n You don't need use Jupyter's restart. (it is stressful)\r\n With executing tf.reset_default_graph(), you can start your session from graph definition.\r\n\r\n This was written by Guillermo-san in stackoverflow.\r\nhttps://stackoverflow.com/questions/39356714/using-tensorboard-with-jupyter-notebooks", "I have exactly the same problem with jupyter notebook for this example.  No solution yet.\r\nAlso there is an exception message here. Wonder if this has anything to do with  nested naming scope resolution.  \r\n\r\nException AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f6ee0371a50>> ignored", "i have this error \r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-52-ddc18530c4f6> in <module>()\r\n     13             # Calculate batch loss\r\n     14             loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\r\n---> 15             print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \"                   + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\").format(acc)\r\n     16         step += 1\r\n     17     print (\"Optimization Finished!\")\r\n\r\nTypeError: unsupported format string passed to numpy.ndarray.__format__\r\ncan you help me please ", "Just Kernel:Restart & Run All"]}, {"number": 224, "title": "TensorFlow for Jetson TK1 (ARM + Cuda)", "body": "I have a gpu-enabled build for Jetson TK1 using CUDA 6.5 runtime and CUDNN 6.5.\nThis is a link to download the binary for the pip install if someone wants to try it:\nhttps://drive.google.com/file/d/0B1uGKNpQ7xNqeTB4RFE2MXdNWUU/view?usp=sharing\n\nJetson  TK1 has a limited amount of memory ( CPU+GPU <2GB) so most of the examples will not run without modifications but aside from  Out Of Memory errors, TensorFlow runs just fine.\n", "comments": ["Thank you for your contribution!  In general we encourage users with nonstandard setup to build their own pip packages from source.  And we are specifically encouraging the CUDA 7 runtime with CUDNN 6.5 because it's one less variable to consider when debugging (and this is the version we use and test with).  Since it doesn't seem like there's any issue, I'm closing / archiving this for now.\n"]}, {"number": 223, "title": "extra cpus not recognized when using docker on windows 10", "body": "Not sure if this is a windows 10 specific issue, a virtual box issue, a docker issue or a tensorflow/docker integration issue, but I'm getting \n\n\"tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'random_normal/stddev': Could not satisfy explicit device specification '/cpu:1'\"\n\nFor the following code:\n\nwith tf.device(\"/cpu:1\"):\n    sess = tf.Session()\n    op = tf.matrix_inverse(tf.random_normal([100,100]))\n    sess.run(op)\n\nNote that /proc/cpuinfo is reporting that I have cpu cores       : 8\n\nAlso, when I do run  tf.matrix_inverse(tf.random_normal([3000,3000])) my CPUs are barely used. \n\nAnother thing I tried was  \n\n\"cat cifar10_multi_gpu_train.py | sed -e 's:gpu:cpu:g' > cifar10_multi_cpu_train.py\"\npython cifar10_multi_cpu_train.py --num_cpus=7\n\nThat didn't seem to work at all.\n", "comments": ["We should probably clear this up somewhere, but at the moment there is only one 'CPU device' per process.  So cpu:0 has access to all 8 of your cores via threading.\n\n(We thought it would be useful to have the ability to specify multiple CPUs in the future if it turns out that NUMA-aware assignment was helpful)\n", "@vrv -- presumably the fix here is to just document this?\n", "https://www.tensorflow.org/versions/r0.7/how_tos/using_gpu/index.html description of devices is reasonable but could be better.  Happy to accept a PR to make this even more clear.  Closing to reduce the size of our enormous open issues list.\n", "C:\\Users\\Desktop>docker run -it b.gcr.io/tensorflow/tensorflow\n\nUnable to find image 'b.gcr.io/tensorflow/tensorflow:latest' locally\n\ndocker: Error response from daemon: unable to ping registry endpoint https://b.g\ncr.io/v0/\nv2 ping attempt failed with error: Get https://b.gcr.io/v2/: dial tcp 64.233.188\n.82:443: i/o timeout\n", "@zkl99999 this sounds like a connectivity problem; are you still seeing this?\n", "@craigcitro  thank you, in China,you  know  break wall  haha\n", "@craigcitro  i install TF ok  in docker,thank you\n"]}, {"number": 222, "title": "Error in api_docs/images/Gather.png ", "body": "The last arrow should originate from p_5. I'd photoshop it, but I imagine there's an source illustrator file somewhere which would be better to change.\n\nhttp://www.tensorflow.org/api_docs/python/array_ops.md#gather\nhttps://github.com/tensorflow/tensorflow/blob/a9ca5173b2252b0de5dd754147b275e85298e522/tensorflow/g3doc/api_docs/images/Gather.png\n", "comments": ["Thanks for pointing it out. We will fix it at the source.\n", "@colah, can you take a look at this?\n", "was fixed ages ago\n"]}, {"number": 221, "title": "tf.matrix_inverse() is slow compared to numpy.linalg.inv", "body": "Running this snippet\n\n```\nimport numpy as np\nimport scipy as sp\nfrom datetime import datetime\nimport tensorflow as tf\n\ns = tf.Session()\ndim = 3000\nmat = tf.random_uniform((dim,dim))\ns.run(tf.initialize_all_variables())\nmatinv = tf.matrix_inverse(mat)\nst = datetime.now()\ns.run(matinv)\nprint \"time elapsed tensorflow:\", datetime.now() - st\n\nst = datetime.now()\nx = np.random.rand(dim,dim)\nsp.linalg.inv(x)\nprint \"time elapsed scipy:     \", datetime.now() - st\n```\n\nyields this output\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:643] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)\ntime elapsed tensorflow: 0:00:18.078232\ntime elapsed scipy:      0:00:01.613825\n```\n\n**1.6 sec (Scipy)  vs. ~18 sec (tensorflow), that's a huge difference.**\n- Is this expected behavior?\n- Why is TensorFlow so slow when inverting a matrix? I was hoping it leverages the GPU.\n- Is there a way to improve the TensorFlow runtime duration?\n", "comments": ["matrix_inverse unfortunately is not yet implemented on GPU, but some of the other operations are, so I suspect there's some amount of memory copying.  MatrixInverse on CPU is just calling Eigen::FullPivLU: https://github.com/tensorflow/tensorflow/blob/d6357a5849db980df51d00d8a9ff874cda2faeb3/tensorflow/core/kernels/matrix_inverse_op.cc#L50, so the optimizations probably have to be made at that level.\n\n(We also don't distribute our binary package using avx/avx2 optimizations so they can run on more platforms, so it's possible things might run faster when running from source and building with more optimizations for your platform).\n", "Thanks for the quick reply. I guess I just have to wait a bit.\n\nIs there a road map link to see which issues will be resolved / new features will added first the next weeks?\n", "We're still going through all of these issues and prioritizing, so we don't have a roadmap ready yet.\n\nOnce we are more able to accept external contributions, improvements like these from the community are welcome!  A quick search suggests that cublas has a matrix inversion function that might be appropriate.  Eager contributors should discuss with us here or on the discussion mailing list to figure out the best way to integrate such a change.\n\nRenaming the title of this issue accordingly!\n", "I'm not sure the CPU vs. GPU aspect is the only issue, since I'd be surprised if scipy used the GPU for matrix inverse.\n", "Indeed, even on CPU it's slow.  Should this bug just be the catch-all for now?\n", "That (slowness on CPU) was my observation too. But I'm not sure if this a bug or an enhancement until we have some profile data.\n", "I guess this was fixed by 04f1932f053dd7865b191719b33860270461943a, I get now the following output to your benchmark:\n\n```\ntime elapsed tensorflow: 0:00:04.545141\ntime elapsed scipy:      0:00:07.803832\n```\n", "This is great. I can confirm this too from my latest build.\n\nBefore:\n\n```\ntime elapsed tensorflow: 0:01:13.776929\ntime elapsed scipy:      0:00:00.913324\n```\n\nAfter git pull & rebuild:\n\n```\ntime elapsed tensorflow: 0:00:02.835981\ntime elapsed scipy:      0:00:00.844610\n```\n", "Hmm, one of the measurements says tensorflow is twice as fast, the other says it is three times as slow.  @delip: Is it still consistently 3x slower for you?\n", "@girving  I ran this 10 times making sure no other compute heavy jobs were running. I see this consistently 3x slower than scipy, but still a dramatic improvement from the earlier slowness (>1min). My env is OSX El Capitan on iMac 5K. Running this in CPU only mode (GPU is not supported in my env). I'm pasting the timing from individual runs here:\n\nhttps://gist.github.com/delip/b4b1ae1c273f2895a78a\n", "Thanks for confirming @delip!  We should leave the bug open until the gap is better understood (and ideally closed).   Could I have your script to reproduce?\n", "@girving, It's the same script as one used by the OP (@kzk2000) -- see first post in the issue. Is it possible that this gap exists only in OSX? Running this on other CPU-only environments (that I don't have access to at the moment), could further narrow the scope of this bug.\n", "@girving: are we still looking at this? worth keeping open?\n", "I'll check this once I get tensorflow building on my Mac.\n", "@rmlarsen: Vijay and I are getting around 40% slower than scipy with this benchmark.  Turning off the symmetry checking made no difference.  We're dramatically closer to good than we were at the beginning of this bug, but I'll leave it up to you whether we should close it or do further investigation. \n", "Closing this since there's been no activity on this for a while -- 40% slower on OS X for matrix_inverse isn't great, but suffices for now.  Looks like we're faster on other platforms so, there's always that.\n", "Has this op been implemented for the GPU?\n", "Had this been implemented on GPU?\n", "Hello! Any news on this regarding GPU implementation?", "+1 for GPU implementation", "I think the tf.matrix_inverse() is already implemeted on GPU.\r\n\r\nI run this benchmark on my laptop with 1060 Ti 6GB and get the following results:\r\n\r\ntime elapsed tf.matrix_inverse: 0:00:00.561071\r\ntime elapsed sp.linalg.inv:     0:00:02.610363\r\n", "Hello guys,\r\nMy simple question is that why the Matlab running on my laptop (Intel, core i5, 8G ram) computes the inverse faster as compared to the Google TPU. Code attached...\r\nMatlab code:\r\n    A = rand(3000,3000);\r\n    tic\r\n    inv(A);\r\n    t = toc;\r\n    disp(t)\r\n**Output:**\r\n    **1.1297**\r\n\r\nPython code:\r\nimport numpy as np\r\nimport scipy as sp\r\nimport timeit\r\nimport tensorflow as tf\r\n\r\ndim = 3000\r\nmat = tf.random.uniform(shape=(dim,dim))\r\n\r\nst = timeit.default_timer()\r\ntf.linalg.inv(mat)\r\nprint(\"time elapsed tensorflow:\", timeit.default_timer() - st)\r\n\r\nx = np.random.rand(dim,dim)\r\nst = timeit.default_timer()\r\nnp.linalg.inv(x)\r\nprint(\"time elapsed scipy:     \", timeit.default_timer() - st)\r\n\r\n**Output:**\r\ntime elapsed tensorflow: 1.3009283419999065\r\ntime elapsed scipy:      2.618305011000075"]}, {"number": 220, "title": "how can I export android-tensorflow into my android stuido", "body": "Is it possible for me to import the android example into android studio? I am confused because I haven't found the libtensorflowdemo.so lib (in the code ,this lib is loaded) about tensorflow in the folders\n", "comments": ["Currently, the android demo is built with Bazel - one way to go is to put your android app under bazel, and follow a similar build rule as highlighted by the Android example.\n\nIf that is not possible, you can manually copy the header files and the pre-built .so file to your Android Studio project. The libtensorflow_demo.so can be found in the bazel cache. If you use Linux, it is usually located at ~/.cache/bazel. Build the target first using\n\n``` shell\nbazel build //tensorflow/examples/android:tensorflow_demo -c opt --copt=-mfpu=neon\n```\n\n, and find the file with:\n\n``` shell\nfind ~/.cache/bazel -name \"libtensorflow_demo.so\"\n```\n\n(Another way is to rename the *.apk to *.apk.zip and unzip it. The resulting .so file is the same as the one found above.)\n", "Assuming the above addresses your problem, I will close this issue - feel free to reopen if you encounter problems further.\n", "@Yangqing do you guys happen to have example WORKSPACE and BUILD files for using TensorFlow in an Android app that is separately managed (i.e. the Android project is not located inside the TensorFlow project like your example is)?  bazel has a huge learning curve to incorporate a large dependency like TensorFlow and I'm not crazy about manually copying over the .h and .so files.\n", "@Yangqing I had been facing the same problem so I located the .so file as you mentioned. After this, where do we have to place the file? \n", "You can find a project android demo done using Android studio here:\n[https://github.com/miyosuda/TensorFlowAndroidDemo](url) \n\nbut it only works with armebi v7 only. If any one can tell me how to convert this to run on x86 and x86_64 devices it would be a great help.\n", "@CalmWaves Check this out https://github.com/tensorflow/tensorflow/issues/1019\n", "Hi renats, \n                 Thank you for your reply. I already checked your post, but I want to know the **commands that you used to build the project using bazel**.\n\n And the other thing is you have used bazel to build the project, But I need to **use gradle in Android Studio.**\n\nPlease help me on this\n", "Instructions for building the Android demo APK with Android Studio (Bazel still required for native libs): https://github.com/tensorflow/tensorflow/issues/3444\n", "@CalmWaves check this out https://github.com/cesardelgadof/TensorFlowAndroidMNIST\n", "Thanks @cesardelgadof  I will check it out.\n"]}, {"number": 219, "title": "ImportError:No module named setuptools after exectued the step for bazel-bin", "body": "I setup tensorflow according to http://tensorflow.org/tutorials/seq2seq/index.md. When I do the step as below:\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg,but the below error was throw:\nSat XXXXXXXXXXXXX CST 2015: ===Building wheel\nTraceback(most recent call last):\n  File \u201csetup.py\u201d,line 3,in <module>\n    from setuptools import find_packages,setup,Extension\nImportError:No module named setuptools\nBTW, I had no do the step for (Optional) Enable GPU Support\n", "comments": ["Are you able to install the binary wheel via the getting started guide,\nwithout going through the pip build process?\nOn Nov 14, 2015 6:53 AM, \"Yi Leung\" notifications@github.com wrote:\n\n> I setup tensorflow according to\n> http://tensorflow.org/tutorials/seq2seq/index.md. When I do the step as\n> below:\n> bazel-bin/tensorflow/tools/pip_package/build_pip_package\n> /tmp/tensorflow_pkg,but the below error was throw:\n> Sat XXXXXXXXXXXXX CST 2015: ===Building wheel\n> Traceback(most recent call last):\n> File \u201csetup.py\u201d,line 3,in\n> from setuptools import find_packages,setup,Extension\n> ImportError:No module named setuptools\n> BTW, I had no do the step for (Optional) Enable GPU Support\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/219.\n", "You need [python setuptools](https://pypi.python.org/pypi/setuptools) installed. Either `sudo apt-get install python-setuptools` or `sudo pip install setuptools` should install that on a Ubuntu machine.\n", "Let us know if that isn't sufficient.  Thanks!\n", "@vrv it worked as the above suggesion. What's more I apt-get install python-pip,the step was executed successfully\n"]}, {"number": 218, "title": "bazel run error", "body": "I am trying to run the code from this tutorial: http://tensorflow.org/tutorials/seq2seq/index.md\n\nWhen I try to run this command:\n\n`bazel run -c opt tensorflow/models/rnn/translate/translate.py -- -- data_dir ../data/translate/`\nI get the following error:\n\n```\n...................\nERROR: Cannot run target //tensorflow/models/rnn/translate:translate.py: Not   executable.\nINFO: Elapsed time: 1.537s\nERROR: Build failed. Not running target.\n```\n\nAny ideas how to resolve?\n", "comments": ["`bazel run -c opt tensorflow/models/rnn/translate:translate`\n\nin bazel you have to build / run the target as defined in the BUILD file, rather than the source file.\n", "Maybe update the tutorial with the right command then?\n", "Good call -- sent out the fix internally and we'll push the change out soon.  Thanks!\n", "Great thanks!\n"]}, {"number": 217, "title": "Is pep8 compatibility necessary right now for tensorflow?", "body": "On running the pep8 test, I found out around 18k warnings. I think we should strict to some guideline, so that as the project grows, a proper standard is maintained. We can use flake8 for this.\n\nI can work on this and submit a PR, if pep8 compatibility is required.\n", "comments": ["Thanks for the report -- we'll look into this at some point!  Hopefully our code is at least internally consistent :)\n", "ok, then i work on this later on.\n", "@martinwicke: Do we have a plan in terms of linting and/or pep8 compliance?  Should this be closed?\n", "Plans, yes, time to do it, no. I would like to make much more of the pylint\nwe're already running, and ideally add cpplint as well.\n\nOn Mon, Jun 6, 2016 at 10:30 AM Geoffrey Irving notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke: Do we have a plan in terms\n> of linting and/or pep8 compliance? Should this be closed?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-224028856,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_VoOya6ZrBTvi7Gs1zoghoDgvTGvks5qJFkvgaJpZM4GiS_S\n> .\n", "@martinwicke: I'll leave it assigned to you for now, and you can reassign to other testing folk as appropriate. \n", "I can work on this.\n", "@staranjeet: I think we may need to figure out a policy of just how pep8 compliant we want to be first.  For example, I'd hate to give up our 2 space indent paradise.\n", "Definitely not while we still maintain a 80-character hard limit.\n\nOn Mon, Jun 6, 2016 at 10:13 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> @staranjeet https://github.com/staranjeet: I think we may need to\n> figure out a policy of just how pep8 compliant we want to be first. For\n> example, I'd hate to give up our 2 space indent paradise.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-224179082,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_TqzEtKeX6LZw0zbNhLTi5PsFRKdks5qJP4OgaJpZM4GiS_S\n> .\n", "@staranjeet We'd like to ignore some thing (in particular, the 4 space indent), but otherwise, proper linting would be great. We already run pylint as part of the build, but we disabled all but the most terrible errors (syntax errors, unknown names) in order to make it blocking without damaging productivity.\n\nIt would be nice to have a stricter pylintrc that we can use for informational purposes. Especially if we can generate statistics on how many lint issues were added (and make that a blocking error). \n\nI haven't used flake8 miself, if it supports both python 2 and python 3 and is sufficiently flexible we could use that as well.\n", "Closing this -- if there are specific violations, or a nicer linter for our tests, I'd love more concrete issues or PRs.\n", "I ran a pep8 test and found that most of the warnings were for spaces as 2\nand 80 char limit. As for the spaces, I think that as per the above\nconversation we are going with 2 spaces convention. Char limit can be\ndiscussed.\n\nAlso let me know if there are any other linters. I was a little busy, will\nalso look myself today.\n", "We currently use pylint: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh\n\nWe can talk about changing options to make it more powerful -- it will need some experimentation to get the right balance between false positives (which we have to whitelist) and enforcing the rules.\n", "Ok I will look this into today and update accordingly.\nOn 11-Jun-2016 11:46 AM, \"Martin Wicke\" notifications@github.com wrote:\n\n> Reopened #217 https://github.com/tensorflow/tensorflow/issues/217.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/217#event-689325940, or mute\n> the thread\n> https://github.com/notifications/unsubscribe/AEGlvNy9GLO8C0ri2ce1P8qi9aOJtYdzks5qKlLAgaJpZM4GiS_S\n> .\n", "The errors which we are planning to ignore\n\n| Error | Error meaning |\n| --- | --- |\n| E501 | line too long |\n| E111 | indentation is not a multiple of four |\n| E114 | indentation is not a multiple of four(comment) |\n| E701 | multiple statements on one line (colon) |\n\nNow the stats while running pep8(ignoring the above mentioned erros) tests are as follows:\n\n| Error | Error meaning | Count |\n| --- | --- | --- |\n| E101 | indentation contains mixed spaces and tabs | 2 |\n| E115 | expected an indented block (comment) | 4 |\n| E116 | unexpected indentation (comment) | 1 |\n| E121 | continuation line under-indented for hanging indent | 27 |\n| E123 | closing bracket does not match indentation of opening bracket's line | 18 |\n| E124 | closing bracket does not match visual indentation | 15 |\n| E125 | continuation line with same indent as next logical line | 195 |\n| E126 | continuation line over-indented for hanging indent | 22 |\n| E127 | continuation line over-indented for visual indent | 102 |\n| E128 | continuation line under-indented for visual indent | 145 |\n| E129 | visually indented line with same indent as next logical line | 106 |\n| E131 | continuation line unaligned for hanging indent | 7 |\n| E201 | whitespace after '[' | 14 |\n| E202 | whitespace before ']' | 1 |\n| E203 | whitespace before ':' | 3 |\n| E222 | multiple spaces after operator | 1 |\n| E225 | missing whitespace around operator | 21 |\n| E226 | missing whitespace around arithmetic operator | 300 |\n| E227 | missing whitespace around bitwise or shift operator | 2 |\n| E231 | missing whitespace after ',' | 58 |\n| E241 | multiple spaces after ',' | 1 |\n| E251 | unexpected spaces around keyword / parameter equals | 15 |\n| E261 | at least two spaces before inline comment | 6 |\n| E262 | inline comment should start with '# ' | 1 |\n| E265 | block comment should start with '# ' | 57 |\n| E266 | too many leading '#' for block comment | 78 |\n| E271 | multiple spaces after keyword | 5 |\n| E272 | multiple spaces before keyword | 2 |\n| E301 | expected 1 blank line, found 0 | 98 |\n| E302 | expected 2 blank lines, found 1 | 55 |\n| E303 | too many blank lines (2) | 7 |\n| E304 | blank lines found after function decorator | 1 |\n| E402 | module level import not at top of file | 43 |\n| E502 | the backslash is redundant between brackets | 3 |\n| E704 | multiple statements on one line (def) | 1 |\n| E711 | comparison to None should be 'if cond is None:' | 4 |\n| E731 | do not assign a lambda expression, use a def | 155 |\n| W191 | indentation contains tabs | 2 |\n| W291 | trailing whitespace | 43 |\n| W293 | blank line contains whitespace | 15 |\n| W391 | blank line at end of file | 15 |\n| W503 | line break before binary operator | 58 |\n\nI think that these should be removed. Let me know your opinion\n", "E402 has valid uses (conditional imports to avoid breaking on missing soft\ndependencies) and we should probably ignore those if they are used for such\na purpose.\n\nE129 is a bit iffy, i have to check how these look.\n\nI'd like to find a way to enforce these going forward (=add a stricter\nlinter test), but we have to make sure that we don't conflict with our\ninternal tools. Otherwise I agree, these should be fixed.\n\nOn Sun, Jun 12, 2016 at 01:27 Taranjeet Singh notifications@github.com\nwrote:\n\n> The errors which we are planning to ignore\n> Error Error meaning\n> E501 line too long\n> E111 indentation is not a multiple of four\n> E114 indentation is not a multiple of four(comment)\n> E701 multiple statements on one line (colon)\n> \n> Now the stats while running pep8(ignoring the above mentioned erros) tests\n> are as follows:\n> Error Error meaning Count\n> E101 indentation contains mixed spaces and tabs 2\n> E115 expected an indented block (comment) 4\n> E116 unexpected indentation (comment) 1\n> E121 continuation line under-indented for hanging indent 27\n> E123 closing bracket does not match indentation of opening bracket's line\n> 18\n> E124 closing bracket does not match visual indentation 15\n> E125 continuation line with same indent as next logical line 195\n> E126 continuation line over-indented for hanging indent 22\n> E127 continuation line over-indented for visual indent 102\n> E128 continuation line under-indented for visual indent 145\n> E129 visually indented line with same indent as next logical line 106\n> E131 continuation line unaligned for hanging indent 7\n> E201 whitespace after '[' 14\n> E202 whitespace before ']' 1\n> E203 whitespace before ':' 3\n> E222 multiple spaces after operator 1\n> E225 missing whitespace around operator 21\n> E226 missing whitespace around arithmetic operator 300\n> E227 missing whitespace around bitwise or shift operator 2\n> E231 missing whitespace after ',' 58\n> E241 multiple spaces after ',' 1\n> E251 unexpected spaces around keyword / parameter equals 15\n> E261 at least two spaces before inline comment 6\n> E262 inline comment should start with '# ' 1\n> E265 block comment should start with '# ' 57\n> E266 too many leading '#' for block comment 78\n> E271 multiple spaces after keyword 5\n> E272 multiple spaces before keyword 2\n> E301 expected 1 blank line, found 0 98\n> E302 expected 2 blank lines, found 1 55\n> E303 too many blank lines (2) 7\n> E304 blank lines found after function decorator 1\n> E402 module level import not at top of file 43\n> E502 the backslash is redundant between brackets 3\n> E704 multiple statements on one line (def) 1\n> E711 comparison to None should be 'if cond is None:' 4\n> E731 do not assign a lambda expression, use a def 155\n> W191 indentation contains tabs 2\n> W291 trailing whitespace 43\n> W293 blank line contains whitespace 15\n> W391 blank line at end of file 15\n> W503 line break before binary operator 58\n> \n> I think that these should be removed. Let me know your opinion\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-225416784,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_ZJoZzSpTLcHP507AoFlMlkzJCFBks5qK8L1gaJpZM4GiS_S\n> .\n", "Actually, we want to enforce line-too-long, but with some exceptions\n(import statements for instance can make unavoidably long lines)\nOn Sun, Jun 12, 2016 at 10:00 Martin Wicke wicke@google.com wrote:\n\n> E402 has valid uses (conditional imports to avoid breaking on missing soft\n> dependencies) and we should probably ignore those if they are used for such\n> a purpose.\n> \n> E129 is a bit iffy, i have to check how these look.\n> \n> I'd like to find a way to enforce these going forward (=add a stricter\n> linter test), but we have to make sure that we don't conflict with our\n> internal tools. Otherwise I agree, these should be fixed.\n> \n> On Sun, Jun 12, 2016 at 01:27 Taranjeet Singh notifications@github.com\n> wrote:\n> \n> > The errors which we are planning to ignore\n> > Error Error meaning\n> > E501 line too long\n> > E111 indentation is not a multiple of four\n> > E114 indentation is not a multiple of four(comment)\n> > E701 multiple statements on one line (colon)\n> > \n> > Now the stats while running pep8(ignoring the above mentioned erros)\n> > tests are as follows:\n> > Error Error meaning Count\n> > E101 indentation contains mixed spaces and tabs 2\n> > E115 expected an indented block (comment) 4\n> > E116 unexpected indentation (comment) 1\n> > E121 continuation line under-indented for hanging indent 27\n> > E123 closing bracket does not match indentation of opening bracket's line\n> > 18\n> > E124 closing bracket does not match visual indentation 15\n> > E125 continuation line with same indent as next logical line 195\n> > E126 continuation line over-indented for hanging indent 22\n> > E127 continuation line over-indented for visual indent 102\n> > E128 continuation line under-indented for visual indent 145\n> > E129 visually indented line with same indent as next logical line 106\n> > E131 continuation line unaligned for hanging indent 7\n> > E201 whitespace after '[' 14\n> > E202 whitespace before ']' 1\n> > E203 whitespace before ':' 3\n> > E222 multiple spaces after operator 1\n> > E225 missing whitespace around operator 21\n> > E226 missing whitespace around arithmetic operator 300\n> > E227 missing whitespace around bitwise or shift operator 2\n> > E231 missing whitespace after ',' 58\n> > E241 multiple spaces after ',' 1\n> > E251 unexpected spaces around keyword / parameter equals 15\n> > E261 at least two spaces before inline comment 6\n> > E262 inline comment should start with '# ' 1\n> > E265 block comment should start with '# ' 57\n> > E266 too many leading '#' for block comment 78\n> > E271 multiple spaces after keyword 5\n> > E272 multiple spaces before keyword 2\n> > E301 expected 1 blank line, found 0 98\n> > E302 expected 2 blank lines, found 1 55\n> > E303 too many blank lines (2) 7\n> > E304 blank lines found after function decorator 1\n> > E402 module level import not at top of file 43\n> > E502 the backslash is redundant between brackets 3\n> > E704 multiple statements on one line (def) 1\n> > E711 comparison to None should be 'if cond is None:' 4\n> > E731 do not assign a lambda expression, use a def 155\n> > W191 indentation contains tabs 2\n> > W291 trailing whitespace 43\n> > W293 blank line contains whitespace 15\n> > W391 blank line at end of file 15\n> > W503 line break before binary operator 58\n> > \n> > I think that these should be removed. Let me know your opinion\n> > \n> > \u2014\n> > You are receiving this because you modified the open/close state.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/issues/217#issuecomment-225416784,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe/AAjO_ZJoZzSpTLcHP507AoFlMlkzJCFBks5qK8L1gaJpZM4GiS_S\n> > .\n", "So I should start fixing pep8 warning or add clauses first in existing pylintrc?\n", "We can't really add the clauses until the code is clean, as currently set\nup, pylint errors are a test failure blocking a commit.\nOn Fri, Jun 17, 2016 at 00:32 Taranjeet Singh notifications@github.com\nwrote:\n\n> So I should start fixing pep8 warning or add clauses first in existing\n> pylintrc?\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/217#issuecomment-226700696,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_RQmNv_b1B85iA5QO86TrPj3kw9iks5qMk2igaJpZM4GiS_S\n> .\n", "@staranjeet I don't think we should ignore any pep8 errors, `E501,E111,E114,E701`.\n\nEvery developer should config his/her Python editor with a full PEP8 checker.\n\nLike the [coding style](http://docs.openstack.org/developer/hacking/) of OpenStack, the largest open source project in Python, says,\n\n> - Step 1: Read pep8\n> - Step 2: Read pep8 again\n> - Step 3: Read on\n\nI know that TensorFlow has made some declaration about `E111` in [Coding Guide](https://www.tensorflow.org/versions/r0.9/how_tos/style_guide.html), but, do you want every developer to change his/her Python editor configuration to ignore the PEP8 E111?\n\nTensorFlow's coding style should be corrected, not everyone's PEP8 rules.\n", "You can use a tool [autopep8](https://pypi.python.org/pypi/autopep8) to fix all of the PEP8 things. Though this may effect the `git blame` now, it will take a good effect to the future.\n", "So I can start working on this?. What should be the line length that we are following for E501?\n", "@ohmystack @martinwicke : any comments?\n", "@martinwicke any direction here for @TroJan who is excited to work on this.\n", "E501 should be 80 characters. I encourage you to do this, I'm not terribly concerned about git blame per se, it's the nature of things.\n", "Any news on this? We have autopep8 set up now, but it's doing fairly minimal things so far (and @yifeif hacked a version to do 2 space indent).\n", "@martinwicke @yifeif \n\n(Sorry for my misunderstanding about your hacking `autopep8` before this edition of this comment.)\n\nIf the TF uses 2 space indent, which is a special case for most of the Python developers, please add a comment line at the beginning of all the files for vim users.\n\nLike this,\n\n``` python\n#!/usr/bin/env python\n# vim: tabstop=2 shiftwidth=2 softtabstop=2\n```\n\n> Why? Please see: https://wiki.python.org/moin/Vim\n\nIndentaion is very important for Python scripts as we all known.\nTell it explicitly in code.\n", "Yes, 2 space indent is a bit of an accident, but here we are, and changing it is not a priority (and would invalidate 100% of our git blame, which is too valuable to lose). \n\nI would also prefer an annotation for each file, but I don't believe that linter tools will respect vim comments, and vim will likely not respect pylint directives (which we'd be more likely to use).\n", "The 2 space indent breaks  consistency for a python developer when he starts working on tensorflow. I would like to see it changed to what Python recommends, if not in immediate future, at least in any planned major release.\nSince we don't use punch cards any more, it is far better to break the 80char limit than change the indentation in a language that has significant white space.  Please :)\n", "I'd like to see this addressed too, my vim goes all red when I open TF code. Either adding a decent flake8 config (like in https://github.com/tensorflow/tensorflow/pull/4511#event-804342012) and vim header or fixing the 2 space thing would be fine with me.\n\nFor the time being, I'm just putting a `.flake8` file in my local tree, that solves everything being red and I still have some descent linting. Not sure about the vim-config, anyone know how to configure vim to have project based overrides? For example to use the config suggested above, but only when starting vim in a specific directory or set of files?\n\nAlso, you should check out: https://www.youtube.com/watch?v=wf-BqAjZb8M (just auto-pepping the entire code-base is not a solution)\n", "@yifeif is working on this. I'll reassign.\n", "I'd like to +1 migrating to the standard 4-space indentation. I've seen quite a few places in the tensorflow codebase with a mix of 2-space and 4-space indentations in the same file. This is really not great.\r\n\r\nAlso like to +1 the comment above to extend the 80char limit ", "also on the point about `git blame`, we can always just use `git blame -w` which will ignore whitespace only changes. So we can still get the desired information fairly easily\r\n\r\n```\r\n-w\r\n           Ignore whitespace when comparing the parent's version and the child's to find where the lines came from.\r\n```", "In some of these checks, we are bound by google's style. Therefore, I think the amount of enforcement for pep8 we have in place right now will stay.\r\nClosing this issue, please reopen if you think there needs to be more discussion on this."]}, {"number": 216, "title": "MatMul Broadcasting / tensordot", "body": "It would really help if `matmul()` and element-wise `mul()` were broadcastable, like in Numpy.  Otherwise you're writing a bunch of boilerplate reshaping code.\n\nFor example, suppose I have a `T x n x k` and want to multiply it by a `k x k2`, and then to a max pool over `T` and then a mean pool over `n`.  To do this now, I think you need to reshape, do the `matmul()` and then undo the reshape and then do the pooling.\n", "comments": ["element-wise mul should do broadcasting already, but feature requested noted for matmul.  Thanks!\n", "I was going to open my own issue on this, but it seems that this one is being used for everything related to batch matrix-vector ops, `numpy.dot`, etc. For those looking for a workaround, I have two implementations of batch vector x matrix multiplication:\n\n``` python\ndef batch_vm(v, m):\n  shape = tf.shape(v)\n  rank = shape.get_shape()[0].value\n  v = tf.expand_dims(v, rank)\n\n  vm = tf.mul(v, m)\n\n  return tf.reduce_sum(vm, rank-1)\n\ndef batch_vm2(x, m):\n  [input_size, output_size] = m.get_shape().as_list()\n\n  input_shape = tf.shape(x)\n  batch_rank = input_shape.get_shape()[0].value - 1\n  batch_shape = input_shape[:batch_rank]\n  output_shape = tf.concat(0, [batch_shape, [output_size]])\n\n  x = tf.reshape(x, [-1, input_size])\n  y = tf.matmul(x, m)\n\n  y = tf.reshape(y, output_shape)\n\n  return y\n```\n\nThe first is based on the broadcasting behavior of `tf.mul`, while the second relies on reshaping all but the last dimension of the input to reduce the operation down to matrix-matrix multiplication. I suspect the second method is more efficient, but it might rely on the tensors being in the right memory order (although I think tf, unlike numpy, only uses C order).\n", "This issue appears to be covered by the more specific #1062.  @vladfi1: Does #1062 do what you want, or does `tf.batch_matmul` work now?  We can reopen if not, but we should try to make the issue more specific if so. \n", "Hi, I had a similar problem with which I struggled.\n\nThe details of my problem and the solution can be found at http://stackoverflow.com/questions/38051143/no-broadcasting-for-tf-matmul-in-tensorflow/38056381.\n\nBriefly, I had to multiply vectors of a 4D tensor by a matrix and I wanted the output as 4D tensor.\nThis is trickier when one uses placeholders with undefined dimension sizes. The solution on stack overflow essentially shows how to restore the original tensor rank after using `tf.reshape(X, [-1, N])`.\n", "I don't think this should be closed -- the matmul does not broadcast in tensorflow 1.3.0. Here is a minimal example: \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport numpy.random as rnd\r\n\r\nA = rnd.randint(10, size = (5, 2, 2)).astype(np.float)\r\nB = rnd.randint(10, size = (1, 2, 2)).astype(np.float)\r\n\r\nprint(\"Numpy broadcasts the first index as expected:\")\r\nprint( np.matmul(A, B))\r\n\r\nprint(\"Tensorflow throws an exception:\")\r\nprint( tf.matmul(A, B))\r\n```", "I am surprised that the example above fails. @vrv I would like to contribute a fix to support this. Would a PR for supporting this be considered? Or is there some ambiguity/complexity I'm not seeing?", "I am surprised that so few people have this problem. I suppose it doesn't crop up often if you only do neural networks.\r\n\r\nAs a temporary workaround, I'm using this:\r\n```python\r\ndef broadcast_matmul(A, B):\r\n    \"Compute A @ B, broadcasting over the first `N-2` ranks\"\r\n    with tf.variable_scope(\"broadcast_matmul\"):\r\n        return tf.reduce_sum(A[..., tf.newaxis] * B[..., tf.newaxis, :, :],\r\n                             axis=-2)\r\n```\r\n\r\nwhich is probably not very efficient but it works. Are you implementing it in the proper `matmul` operation, @PeterMitrano ?\r\n", "I found tensordot() useful when broadcasting is needed.\r\n\r\n\r\n```\r\n#a : tensor of shape [2, 3, 3]  \r\na = tf.constant([[[1, 2, 3], [2, 3, 4], [1, 4, 5]], [[5, 6, 7], [6, 3, 4], [9, 0, 1]]])  \r\n#b : tensor of shape [3, 4]  \r\nb = tf.constant([[2, 3, 4, 5], [8, 9, 1, 5], [0, 9, 0, 2]])  \r\n#c : tensor of shape [2, 3, 4] where c[i, ...] = dot(a[i, ...], b)  \r\nc = tf.tensordot(a, b, axes=1)  \r\narray([[[ 18,  48,   6,  21],\r\n        [ 28,  69,  11,  33],\r\n        [ 34,  84,   8,  35]],\r\n\r\n       [[ 58, 132,  26,  69],\r\n        [ 36,  81,  27,  53],\r\n        [ 18,  36,  36,  47]]])\r\n```\r\n", "`tensordot` only does batch matrix multiplication when one of the two is a rank-2 tensor, though.", "I am also quite bothered by this. The same problem arises in the `tf.matrix_triangular_solve` as well. Currently, I am using `tf.einsum` as a workaround.", "I would also like `tf.matmul` to broadcast properly. It becomes a problem when trying to add a fully connected or softmax layer in an RNN:\r\n\r\n```\r\noutputs = tf.stack(outputs, axis=1, name='Outputs')  # Shape is [batch, sequence, rnn_cell.output_size]\r\nwith tf.name_scope('Softmax'):\r\n    w = tf.get_variable(name='Weight', initializer=tf.truncated_normal([rnn_cell.output_size, embedding_size], stddev=0.01))\r\n    b = tf.get_variable(name='Bias', initializer=tf.zeros(embedding_size))\r\n\r\n    scores = tf.matmul(outputs, w) + b  # Fails with ValueError: Shape must be rank 2 but is rank 3 for 'Softmax/MatMul' (op: 'MatMul') with input shapes: [?,50,256], [256,2569].\r\n\r\n    softmax = tf.nn.softmax(scores, name='Softmax')\r\n```\r\n\r\nIdeally it would broadcast and produce a tensor of shape [?,50,2569]", "I think proper broadcasting support for `tf.matmul()` would pretty clearly be a welcome addition to TensorFlow. There is certainly plenty of interest in it, and it is backwards compatible with how `tf.matmul()` currently works.\r\n\r\nI suspect full support for the semantics of `@` as described in https://github.com/tensorflow/tensorflow/issues/1062#issuecomment-225882004 would also be welcome, but supporting matrix-vector, vector-matrix and vector-vector multiplication with `tf.matmul()` would warrant more discussion given the potential for confusion.", "I would also welcome broadcasting for matmul, because without broadcasting it is necessary to use tiling instead. Profiling shows this tiling is what I (and probably anyone else in need of broadcasting) lose the largest part of my (/our) processing power on. It seems plausible that a clever matmul broadcasting implementation can prevent any actual copying and thus speed up all our code by potentially eliminating the rate limiting step.\r\n\r\nNote that capsule and matrix capsule networks (two new networks released by Hinton) could potentially benefit a lot from a broadcasting matmul; see https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb (specifically section Compute the Predicted Output Vectors)", "I am using this until an official implementation comes around (inspired by @talhasaruhan):\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef batch_matmul(A, B, transpose_a=False, transpose_b=False):\r\n    '''Batch support for matrix matrix product.\r\n\r\n    Args:\r\n        A: General matrix of size (A_Batch, M, X).\r\n        B: General matrix of size (B_Batch, X, N).\r\n        transpose_a: Whether A is transposed (A_Batch, X, M).\r\n        transpose_b: Whether B is transposed (B_Batch, N, X).\r\n\r\n    Returns:\r\n        The result of multiplying A with B (A_Batch, B_Batch, M, N).\r\n        Works more efficiently if B_Batch is empty.\r\n    '''\r\n    Andim = len(A.shape)\r\n    Bndim = len(B.shape)\r\n    if Andim == Bndim:\r\n        return tf.matmul(A, B, transpose_a=transpose_a,\r\n                         transpose_b=transpose_b)  # faster than tensordot\r\n    with tf.name_scope('matmul'):\r\n        a_index = Andim - (2 if transpose_a else 1)\r\n        b_index = Bndim - (1 if transpose_b else 2)\r\n        AB = tf.tensordot(A, B, axes=[a_index, b_index])\r\n        if Bndim > 2:  # only if B is batched, rearrange the axes\r\n            A_Batch = np.arange(Andim - 2)\r\n            M = len(A_Batch)\r\n            B_Batch = (M + 1) + np.arange(Bndim - 2)\r\n            N = (M + 1) + len(B_Batch)\r\n            perm = np.concatenate((A_Batch, B_Batch, [M, N]))\r\n            AB = tf.transpose(AB, perm)\r\n    return AB\r\n```\r\nThis code would still work even if both A_batch and B_batch had multiple axes or none at all. The only caveat here is when B_Batch is not empty; the shape of the output tensor would be `(A_Batch, M, B_Batch, N)`, which requires calling `tf.transpose()` that [copies the full tensor](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/transpose) instead of creating an efficient view (on TensorFlow version 1.9 and older). I believe an efficient impelmentation of reshaping the inner-most axes of a tensor as a view would solve this issue but I am not aware of such function.", "I understand that any fully connected NN layer requires matmul. Also, if you want performance then you *need* batching. So does this not mean that pretty much *any* neural network implementation has a need for batched matmul? What's the recommended way to do this? Many solutions are proposed here or on the web but they all seem to rely on copying lots of data (tile, concat, squeeze, arrange, reshape) causing loss of performance.\r\n\r\nTiling causes another specific problem for me: With large batch sizes this created very high memory usage for intermediate tensors.", "Hi @GSPP , \r\n\r\n>I understand that any fully connected NN layer requires matmul\r\n\r\nYep!\r\n\r\n>So does this not mean that pretty much any neural network implementation has a need for batched matmul?\r\n\r\nActually, no. If you had a single example, x which is a vector, and you wanted to apply the weights and compute Wx, you need a matrix-vector multiplication. If you have a minibatch of inputs, then X is a matrix, and you compute X@W which is a matrix-matrix multiplication. At no point you need a batched matrix multiplication.\r\n\r\nBatched matmul would be needed if, for example, you need to do a matrix-matrix multiplication for every training example and then you need to minibatch them.\r\n\r\nAdditionally, batched matmul _is_ currently supported. What isn't supported is _broadcasted_ batch matmul, that is, when you want to do W@X and then W is of shape [1, a, b] and X is of shape [c, b, d].\r\n", "@rhaps0dy you are right and this explains, why my implementation is so complicated :grin:. Thanks for clarifying.", "Has there been any progress with this? I too find this to be a highly desirable feature. \r\n\r\nCouldn't it easily be implemented with tensordot and a transposition?   Like this:\r\n\r\n```python\r\ndef broadcasted_matmul(A, B):\r\n    \"\"\" Matmul with fast broadcasting\r\n\r\n    Args:\r\n      A - [n, m]\r\n      B - [batch_size, m, k]\r\n\r\n    Returns:\r\n      AB - [batch_size, n, k]\r\n    \"\"\"\r\n    # [n, batch_size, k]\r\n    AB = tf.tensordot(A, B, axes=[[1], [1]])\r\n    # [batch_size, n, k]\r\n    return tf.tranpose(AB, [1, 0, 2])\r\n```", "Hi, I'm new to tensorflow and was trying out matrix multiplication and ran into this error when broadcasting is desired. I would like to contribute to this, if its decided to implement broadcasting in this particular function (tf.matmul). If so can one of you please help me get started?", "You could also do this using a conv1d layer with a kernel size of 1 and stride of 1. For instance in @TheButlah examples above one could use (dims etc from above): \r\n\r\n```\r\noutputs = tf.stack(outputs, axis=1, name='Outputs')\r\nscores = tf.layer.conv1d(inputs=outputs, kernel_size=1, filters=2569,  kernel_initializer=layers.xavier_initializer())\r\nsoftmax = tf.nn.softmax(scores, name='Softmax')\r\n```\r\nThis is an equivalent operation and produces the desired output of [?,50,2569]. This is obviously only practical if one of the matrices is a parameter of the model and not calculated from the data.", "Dreams coming true:\r\nhttps://github.com/tensorflow/tensorflow/commit/47ab68d265a96b6e7be06afd1b4b47e0114c0ee9", "Going to close this issue since matmul now broadcasts, and the forward compatibility window has passed. This means if you are on tf-nightly, tf.linalg.matmul will broadcast (and users of LinearOperator will also feel the performance improvements of this).", "Sorry to bring back the issue, but matmul documentation still says that\r\n\r\n> the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size\r\n\r\nThe word *matching* implicitly excludes broadcast in my understanding.\r\n\r\nI fear many people lost and lose time not knowing of the \"secret\" broadcast capability of matmul. (I did \ud83d\ude2d)\r\n\r\nI feel that a couple of words on this should really be added to the documentation."]}, {"number": 215, "title": "Question:  Example on how can TensorFlow be used for Text classification?", "body": "", "comments": ["Hi, this is probably a better question for either the discussion mailing list or StackOverflow -- can you please repost at one of those [venues](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md#community-)?\n", "Got a redirect back on that \nhttp://stackoverflow.com/questions/33705284/tensorflow-text-classification-using-neural-networks?noredirect=1#\n\nI think it will be helpful to have a tutorial as community is just getting started on this. \n", "(Discuss mailing list is probably the best for this)\n"]}, {"number": 214, "title": "Beam Search", "body": "Is there anything in place supporting this yet? Wanted to ask before I implement my own.\n", "comments": ["I'd probably encourage you to write a C++-based beam search instead of trying to do it with graph ops.  That said, we will likely provide some sort of specialized self-contained beam search for CTC with the CTC loss.  Additionally there's a PriorityQueue in the works which, when completed, will provide the ability to sort groups of Tensors.  Not sure if this would help you.  Another good project to look at for implementing a beam search is [OpenFST](http://www.openfst.org/), which was coincidentally partly developed within Google Research :)\n", "so what's the verdict? I am using the python API now, should I do it in python?\n\ni.e. is beam-search even possible in the graph ops, if it's not possible then clearly there's no option, but if it is possible is the issue of being inefficient or more difficult to implement than its worth?\n", "@evanthebouncy, see #654 \n", "Marking this as a duplicate of #654  cause it seems like that's what you want to do.\n", "@ebrevdo Can you provide a pointer to how to do beam search using the C++ API?\n", "Do you mean CTC beam search specifically?\n", "@ebrevdo I'd like to see an example of a CTC beam search\n", "I've frozen a model that has as last node ctc beam search. Now I want to use this .pb file in C++ but I don't know exactly how to process the output tensor in order to get the final sequence of labels. Anyone with previous experience in this kind of issue?"]}, {"number": 213, "title": "tensorflow howto for sharing variable is confusing", "body": "at here:\nhttp://tensorflow.org/how_tos/variable_scope/index.md\n\nThe tutorial talk about using \"two sets of variables\" and talk about their duplication, however the network itself uses 2 layers of convolution. So there's 2 different things that can be the quantity of 2. \n1) The number of layers is 2, being conv1 and conv2, these 2 should be distinct\n2) The number of image filters, applied once to each image, these 2 should be identical\n\nI was having a hard time understanding at first and thought we're attempting to use the same weights for both of the conv layers. It didn't help that the definition for \"variables_dict\" only explicitly states the existence for the weights for the first conv layer, leading me to believe that the weights of the 2 layers are actually shared as well.\n\nI think a simple fix is to make the network have say 5 conv layers instead of 2, this way you get to demonstrate variable_scope as well in the later section\n\nJust my 2 cents, I starred at this for much longer than I should have because of this confusion.\n", "comments": ["Thanks, we'll take a look to see if we can reduce the confusion!\n", "just as a follow up:\nI think it'll be useful to include a discussion on variable creation in places where it is not explicit.\n\nFor instance, \n    cell1 = rnn.lstm( ... ) \nactually does not create any variables, and it's only when you call it\n    out, state = cell1(x, state)\ndoes the variables gets created. Since the variable creating is not explicit in both case, one (such as me) can get very confused on where to use variable sharing and when to not use it.\n", "@vrv: Did this ever happen?\n", "I doubt it -- assigning to Lukasz\n", "Improved the tutorial to clarify which sets of variables are meant in a recent CL, is now live on master.\n", "Hi,\r\n\r\nMy question may be a little bit different than what is already asked.\r\nIf I define a class like below:\r\n\r\n```\r\n\r\nclass Dense:\r\n    def __init__(self,Dsize,OSize):\r\n        self.variables_dict = {\r\n            \"FirstLayer\": tf.Variable(tf.random_normal(Dsize),  name=\"weights\"),\r\n            \"FirstLayer_b\": tf.Variable(tf.zeros(OSize), name=\"biases\")\r\n        }\r\n    def apply(self, input, equation = 'ijk,kd->ijd'):\r\n        output = tf.einsum(equation, input, self.variables_dict['FirstLayer']) + self.variables_dict['FirstLayer_b']\r\n        return output\r\n```\r\n\r\n\r\nAnd get **one** instance of this class in my code and use it multiple times by using its `apply` function. Does it mean that I have shared the same weight matrix (they have equal values in them) in different parts of my graph? Or it will make different matrices and update the variables differently?\r\n\r\nI am not using the `reuse = True` option.\r\n\r\nI would appreciate if someone helps me with this issue. It will help me a lot to understand the logic behind generating a graph in Tensorflow and also writing a clean and readable code.\r\n\r\n\r\n", "Please use tf.get_variable. Reuse will *not* work with tf.Variable and you should basically *never* use it, always go for tf.get_variable instead.", "@lukaszkaiser  Thanks a lot for your prompt response and the good tip.\r\n\r\nActually, my main concern is about using object oriented style programming in TF. \r\n\r\nIf I implement a costume layer as a class (Dense layer in my toy example above), can I do weight sharing by just getting one instance of that class and use it multiple times? Or the TF engine will internally make different ones and update them completely different and that layer won't have same weights with other ones?\r\n\r\nThanks", "It is a little tricky to do the OO-style right, but we're working to provide a convenient way to do it as follows.\r\n\r\nFirst of all, let your class be a sub-class of Layer from tensorflow/python/layers/base.py: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/base.py\r\n\r\nThis will handle everything related to scopes. It will open new scope if you provide a name for your layer, it will reuse if you're referring to the same object, but create new variables when you create a new object. If you subclass from Layer, the only thing you need to implement are the \"build\" and \"call\" functions, similar to Keras. You can see how we do it for the core Dense layer here:\r\n  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/core.py#L106\r\n\r\nI think the easiest way to do OO is to do the same, you can even use our OO layers already. They are not part of the public API yet because we want to test them properly and let them mature, but I think they're far enough to give them a try. If you want to have a look, the expected behaviors in terms of interaction with reuse have their unit tests here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/core_test.py\r\n\r\nHope that helps, we want to make a great OO interface too, so let us know if you try the core OO stuff and have troubles."]}, {"number": 212, "title": "Multiple models in one session", "body": "Is this possible? I'm doing something where I need the outputs of multiple distinct models to be compared. To do that, I'm batching up inputs and running them over each model. It's unclear to me how I can do this in one session.\n", "comments": ["If you create both of your graphs within the same session, and as long as they are distinct subgraphs within the graph, you should be able to run both models within the same session.\n\npseudo-example:\n\n```\nwith tf.Session() as sess:\n  # Build graph 1\n  model1_output_node = build_model_1()\n  model2_output_node = build_model_2()\n\n  model1_output = sess.run(model1_output, feed_dict={...})\n  model2_output = sess.run(model2_output, feed_dict={...})\n\n  .. compare the result ...\n```\n\nLet me know if that doesn't work for some reason.\n", "Thanks! Will try this and report back.\n", "I'm using a function very similar to `create_model` in models/rnn/translate/translate.py (reproduced below) to load two RNN encoder-decoders like this:\n\n```\nwith tf.Session() as session:                                                                    \n    print 'Creating model two from directory %s.' % FLAGS.model_dir_one                         \n    model_two = create_model(session, forward_only=True, model_dir=FLAGS.model_dir_one)                            \n\n    print 'Creating model one from directory %s.' % FLAGS.model_dir_two                          \n    model_two = create_model(session, forward_only=True, model_dir=FLAGS.model_dir_two)                            \n```\n\nAnd I'm running into an error: `tensorflow.python.framework.errors.NotFoundError: Tensor name \"Variable_2\" not found in checkpoint files <...>/model.ckpt-<###>` (more extensive at bottom)\n\nBoth of the models were built at a previous time. They were made with the same parameters (50,000 vocab size, 400 size layer, 3 layers) and if I run them in an interactive decoding mode, they both work just fine. \n\nIn addition, the first model that I specify loads just fine, regardless of which one I put first. I know this because if I try running it as is, then the 1st one loads and the second one chokes. If I then remove the second one, the program runs without a hiccup. And if I switch them so that the second model comes before the first one, the same thing happens but in reverse.\n\nThis leads me to think that there is something up with the graph loading but I'm quite sure what.\n\n```\ndef create_model(session, forward_only, model_dir=None):                                                             \n  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"                       \n  model = seq2seq_model.Seq2SeqModel(                                                                \n      FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,                                            \n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,                       \n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,                                         \n      forward_only=forward_only)\n\n  ckpt = tf.train.get_checkpoint_state(model_dir)\n  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)                           \n    model.saver.restore(session, ckpt.model_checkpoint_path)\n  else:\n    print(\"Created model with fresh parameters.\")                                                    \n    session.run(tf.variables.initialize_all_variables())\n  return model\n```\n\n```\n...\n File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 867, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 349, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 423, in _do_run\n    e.code)\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"Variable_2\" not found in checkpoint files /home/ubuntu/tensorflow/research/lm/iwslt-train-tags-de-50000/translate.ckpt-59200  \n         [[Node: save_1/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_1/Const_0, save_1/restore_slice_2/tensor_name, save_1/restore_slice_2/shape_and_slice)]]  \n         [[Node: save_1/restore_slice_12/_99 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_3360_save_1/restore_slice_12\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nCaused by op u'save_1/restore_slice_2', defined at:\n\n...\n\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 691, in __init__\n    restore_sequentially=restore_sequentially)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 411, in build\n    filename_tensor, vars_to_save, restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 170, in _AddRestoreOps\n    values = self.restore_op(filename_tensor, vs, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 87, in restore_op\n    preferred_shard=preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/io_ops.py\", line 173, in _restore_slice\n    preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 271, in _restore_slice\n    preferred_shard=preferred_shard, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 638, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1733, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1008, in __init__\n    self._traceback = _extract_stack()\n```\n", "@lukaszkaiser: is this a matter of wrapping the model creation inside a name or var scope?\n", "I'm not 100% sure, but I think that the problem might be that the Seq2SeqModel class has some non-sharable things, like global_step and a separate saver. So if you just make 2 of them, you'll have 2 global_steps, 2 savers, and so on. I think you need to move the common things out of the class. And yes - best wrap the 2 models you're creating in different tf.variable_scope to not share parameters (or the same variable_scope with reuse=True to share them).\n\nLet us know if that helps or if there are more problems!\n", "Thanks for the fast replies guys.\n\nI changed it so that the model is created under a separate scope when I build it.\n\n```\n    with tf.variable_scope(model_name):\n      model = Seq2SeqModel(...)\n\n    ...\n\n    Seq2SeqModel():\n        def __init__(...):\n            ...\n            self.learning_rate = tf.get_variable('learning_rate', shape=[], initializer=tf.constant_initializer(float(learning_rate)), trainable=False)\n            self.global_step = tf.get_variable('global_step', shape=[], initializer=tf.constant_initializer(0), trainable=False)\n            ...\n```\n\nThen I made a couple of test models to a few hundred steps. Afterwards, I tried loading them again. They still load fine individually and decode, etc. \n\nSo I tried loading them both at the same time:\n\n```\nprint 'Creating model one from directory %s with scope %s.' % (                              \n    FLAGS.model_one_dir, FLAGS.model_one_scope)                                                    \nmodel_one = get_rnn_model(session, model_dir=FLAGS.model_one_dir,                                  \n                          model_name=FLAGS.model_one_scope)                                  \nmodel_one.batch_size = bridge_batch_size                                                     \n\nprint 'Creating model two from directory %s with scope %s.' % (                              \n    FLAGS.model_two_dir, FLAGS.model_two_scope)                                                    \nmodel_two = get_rnn_model(session, model_dir=FLAGS.model_two_dir,                                  \n                          model_name=FLAGS.model_two_scope)                                  \nmodel_two.batch_size = bridge_batch_size   \n```\n\nThe call to `get_rnn_model` just encapsulates the model restoration in a `with tf.variable_scope(model_name)`. However, when loaded together, I'm running into the following error where the scope for the first model is used in place of the scope for the second model (...Tensor name \"**de-10000-256-2-10000**/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint files **model-en-10000-256-2-10000**/translate.ckpt-300)\n\nI thought this had to be just a coding error, but debugging it shows that the directory model it's restoring from and the model_name it's using for scope are aligned. \n\nWhat am I missing? I don't think it's to do with the saver because it is saving and restoring fine when using just one model.\n\n```\nCreating model one from directory <...>/model-de-10000-256-2-10000 with scope de-10000-256-2-10000.\nReading model parameters from model-de-10000-256-2-10000/translate.ckpt-400\n\nCreating model two from directory <...>/model-en-10000-256-2-10000 with scope en-10000-256-2-10000.\nReading model parameters from model-en-10000-256-2-10000/translate.ckpt-300\n\nW tensorflow/core/common_runtime/executor.cc:1052] 0xbec32b0 Compute status: Not found: Tensor name \"de-10000-256-2-10000/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint\n files model-en-10000-256-2-10000/translate.ckpt-300\n```\n", "Any thoughts on this? I looked through the embedding_rnn_seq2seq function and it looks like it builds from get_variables and variable_scope throughout. Even though there's a reuse_variables in, for example, the \"RNN\" scope, I would think that this would be in a different domain given that the top level scope is different (\"en-10000-256-2-10000\" vs \"de-10000-256-2-10000\")\n", "Hi cinjon,\n\nSorry for the delay. I was thinking a bit about what you're doing and it took me a while to wrap my mind about it, but I think I can see why the first model loads ok but the second does not.\n\nHere is how I see it. Both model1 and model2 have a saver, right? Created like this:\n\n``` python\n  self.saver = tf.train.Saver(tf.all_variables())\n```\n\nThis is ok for the first model, as it saves only the variables belonging to it. But -- because of the use of tf.all_variables() -- the second model with save both itself and model1. Does that make sense?\n\nI see 2 ways to correct that. For one, you could filter the variables saved in model2 to start with the prefix you're giving it. Another, and I think better way, is to remove the saver from the 2 models entirely, and have it only once in your main loop - after you've created both models. Then you'll have only a single checkpoint file and you can still use tf.all_variables() without the risk of forgetting anything, and it should all work.\n\nHope that helps!\n\nLukasz\n", "Thanks Lukasz! I had a chance to try it tonight and I still can't get this right.\n\nAre you suggesting that I have one file containing all of the models? What I'm trying utilizes two models, M_1 and M_2, and then a third, J_12, to join them. But there could be a lot more M_k and, for each two of them, there would be a J. Ideally, I would put each of the M_k in their own model directory so that I can build J_pq by loading only M_p and M_q.\n\nIf I had just one checkpoint file, then I'd be breaking a lot of the modularity involved. It would also be difficult to keep track of experiments using different hyper-parameters.\n\nTaking your advice, I removed the saver from the Seq2Seq model. The training for the M_k now looks like this:\n\n```\ndef create_model(..., do_restore=True):\n  ...\n  with tf.variable_scope(model_name):\n    model = Seq2SeqModel(\n      vocab_size, vocab_size, bucket_sizes, layer_size, num_layers,\n      max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor,\n      forward_only=forward_only)\n\n  ...\n  saver = tf.train.Saver(tf.all_variables())\n  if model_checkpoint_path:\n      if do_restore:\n          saver.restore(session, model_checkpoint_path)\n      else:\n          return model_checkpoint_path, model, saver\n  else:\n      session.run(tf.variables.initialize_all_variables())\n  return None, model, saver\n\nckpt_path, model, saver = create_model(..., model_name=model_name, do_restore=True)\n<Train Train Train>\nif current_step % steps_per_checkpoint == 0:\n    saver.save(session, ckpt_path, global_step=model.global_step)\n```\n\nAnd for the J_pq it looks like this:\n\n```\nckpt_path_one, model_one, _ = get_rnn(\n            ..., model_dir=model_dir_one, model_name=model_one_scope, do_restore=False)\nckpt_path_two, model_two, _ = get_rnn(\n            ..., model_dir=model_dir_two, model_name=model_two_scope, do_restore=False)\n\nwith tf.variable_scope('j_pq'):\n    j_pq = ...\n\nsaver = tf.train.Saver(tf.all_variables())\nsaver.restore(session, ckpt_path_one)\nsaver.restore(session, ckpt_path_two)\n```\n\nThis didn't work and failed with similar errors (lots of them):\n\n```\nTensor name \"j_pq/weights\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300\nTensor name \"model-en-10000-256-2-10000/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/MultiRNNCell/Cell1/GRUCell/Candidate/Linear/Matrix\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300\n```\n", "I got it to work! (I think)\n\nBy following your other suggestion to set the variables explicitly, it seems to be working:\n\n```\nall_vars = tf.all_variables()\nmodel_one_vars = [k for k in all_vars if k.name.startswith(FLAGS.model_one_scope)]\nmodel_two_vars = [k for k in all_vars if k.name.startswith(FLAGS.model_two_scope)]\nj_pq_vars    = [k for k in all_vars if k.name.startswith('j_pq')]\n\ntf.train.Saver(model_one_vars).restore(sess, model_one_checkpoint)\ntf.train.Saver(model_two_vars).restore(sess, model_two_checkpoint)\n\nsaver = tf.train.Saver(j_pq_vars)\n```\n\nThanks so much for your help guys!\n", "I am running into a similar error. I have trained two models. en-fr and fr-en models. The fr-en model has different vocab size compared to the en-fr model. The first model is being loaded properly. It crashes while loading the second one. I have trained both the models separately. Please check the log file attached to see the error logs. kindly tell me the right way to go ahead.\n\n```\n`global en_fr_sess\n\nif en_fr_sess==None:\n  en_fr_sess = tf.Session()\n\nglobal en_fr_model\nglobal en_fr_en_vocab_path\nglobal en_fr_fr_vocab_path\n\n  en_fr_model = create_model(en_fr_sess, True)\n  en_fr_model.batch_size = 1  \n  fr_en_model = fr_en_create_model(en_fr_sess, True)\n  fr_en_model.batch_size = 1`\n```\n\nbelow are the two methods create_model and fr_en_create_model\n\n```\n`def create_model(session, forward_only):\n\n    model = seq2seq_model.Seq2SeqModel(\n    FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\n    FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n    FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,\n    forward_only=forward_only)\n    ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n      print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n      model.saver.restore(session, ckpt.model_checkpoint_path)\n    else:\n       print(\"Created model with fresh parameters.\")\n    session.run(tf.initialize_all_variables())\n    return model\n\n def fr_en_create_model(session, forward_only):\n\n      model = seq2seq_model.Seq2SeqModel(\n      380, 380, _buckets,\n      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,\n      forward_only=forward_only)\n      ckpt = tf.train.get_checkpoint_state(\"./password_data/train/\")\n      if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n        model.saver.restore(session, ckpt.model_checkpoint_path)\n      else:\n        print(\"Created model with fresh parameters.\")\n      session.run(tf.initialize_all_variables())\n      return model`\n```\n\n[out.txt](https://github.com/tensorflow/tensorflow/files/356916/out.txt)\n", "You're creating both models so that they share variables, which is not possible when vocabulary sizes differ. If you want separate variables for each model, use, e.g., \"with tf.variable_scope('enfr'):\" when creating one and \"with tf.variable_scope('fren'):\" when creating the other.\n"]}, {"number": 211, "title": "Add gpu support for LRN", "body": "When I launch a network that uses local response normalization, it works perfectly on a CPU, but it appears to not have a gpu implementation and results in the following error when I switch to a gpu device:\n\n`tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'LRN': Could not satisfy explicit device specification '/gpu:0'\n     [[Node: LRN = LRN[alpha=0.0005, beta=0.75, bias=2, depth_radius=5, _device=\"/gpu:0\"](conv1/conv1)]]`\n\nI could probably explicitly deploy this operation on the cpu, but it since this is a sliding window algorithm I'm surprised it doesn't have a gpu implementation.\n", "comments": ["Thanks for this feature request! In the meantime, if you want to test your model without explicitly specifying the device for each op, you can do:\n\n``` python\nsess = tf.Session(tf.ConfigProto(allow_soft_placement=True))\n```\n\n...when constructing your session. This allows you to request a GPU device for any of your ops, and it will fall back to running on a CPU if there is no GPU kernel available.\n", "Just in case anyone is using this code, theres a typo. It should be:\n\n```\nsess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n```\n", "just ran into this, anyone working on it?\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dnn.h#L822 does exist, if someone wants to plumb that call through, I think we'd have LRN support for GPU :)\n", "Any updates on this? not having this on GPU really slows things down.\n", "Right, cudnn has an LRN implementation, so the point is to have it use that one.\n", "FYI to anyone looking at contributing a fix for this -- there is some movement internally.\n", "Thanks, removing the contributions welcome tag, since this is being worked on.\n", "@vrv: Does the person working on it have a Github username?\n", "I don't know, @rryan do you know?\n", "Once Stream Executor support is finished I was planning to work on it -- so you can assign me if you'd like :).\n", "@rryan: Assigned, thanks!\n", "Is there any news on this bug? Should I better compute it with different operations? \n", "Sorry for the delay, I carved out some time to work on it this weekend and will be sending it out for review as soon as the GPU tests pass.\n", "Thanks @rryan , looking forward!\n", "Added in 35df3ed43edabbc4ad1b2439bbc7de8917026d6e.\n", "Yay!\n"]}, {"number": 210, "title": "feature request: softmax target axes / multi-dimensional softmax", "body": "The current softmax implementation operates only on 2d tensors, and computes the normalization over the second dimension.\n\nIt would be useful to have a softmax operation that can sum over arbitrary dimensions in the same way that the `reduce_*` operations do.\n\nAs a concrete example application, one might have a convolutional network that outputs a (soft) 1-of-k prediction for each position.\n", "comments": ["Thanks for the request!  For the purposes of prototyping, you could probably implement this using more primitive operations (e.g., using tf.exp, tf.reduce_sum , etc) -- let us know if there's something missing from the set of primitive operations.\n", "> Thanks for the request! For the purposes of prototyping, you could probably implement this using more primitive operations (e.g., using tf.exp, tf.reduce_sum , etc) -- let us know if there's something missing from the set of primitive operations.\n\nThanks!  And yeah, I've already done that for now.  It's slow and not numerically stable, but it works for now.\n", "You should have enough ops to make it numerically stable as well. Just subtract off the result of tf.reduce_max before exponentiating.\n", "I think this is best implemented with `reshape` and `transpose` followed by the existing functionality.\n", "I just had this need, and here is mine: https://gist.github.com/raingo/a5808fe356b8da031837\n"]}, {"number": 209, "title": "Yann LeCun's web page is not available - cannot download data from there is there a mirror?", "body": "Yan Lecun''s web page is not available - cannot download data from there is there a mirror?\n\npython tensorflow/models/image/mnist/convolutional.py attempts to download from http://yann.lecun.com/exdb/mnist/\n\nFor the past hour or so yann.lecun.com is not responding...\n", "comments": ["The port on the address 'yann.lecun.com' is not listening. Either someone stopped the webserver (unlikely) or it died because of overload (much more likely). After more research I have found that no known port is responding and it might very well have been the whole server machine which have gone down. The domain is owned by 'PERFECT PRIVACY LLC' and reverse IP domain check reports that only one domain is hosted at the same location.\n\nPersonally, to me this sounds like it is some ones private server and I think that the data located there used by TensorFlow should be copied or transferred over to a more public mirror.\n", "Thanks!\n- Damian\n\n> On Nov 13, 2015, at 14:59, Max Fax\u00e4lv notifications@github.com wrote:\n> \n> The port on the address 'yann.lecun.com' is not listening. Either someone stopped the webserver (unlikely) or it died because of overload (much more likely). After more research I have found that no known port is responding and it might very well have been the whole server machine which have gone down. The domain is owned by 'PERFECT PRIVACY LLC' and reverse IP domain check reports that only one domain is hosted at the same location.\n> \n> Personally, to me this sounds like it is some ones private server and I think that the data located there related should be copied or transferred over to a more public mirror\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "@damiand I have uploaded the training data to a Github repository which can be used as a mirror until the old website comes back up or a TensorFlow official announces a new location.\n\nThe **temporary** mirror can be found (with hash file checksums) here:\nhttps://github.com/Foorack/tensorflow-trainingdata\n(yes I know I spelled 'upload' wrong in the initial commit...)\n\nKind Regards,\n~Foorack\n", "Cool. Thanks!\n- Damian\n\n@DamianDanowski\n(703) 716-7474\n\n> On Nov 13, 2015, at 15:21, Max Fax\u00e4lv notifications@github.com wrote:\n> \n> @damiand I have uploaded the training data to a Github repository which can be used as a mirror until the old website comes back up or a TensorFlow official announces a new location.\n> \n> The temporary mirror can be found (with hash file checksums) here:\n> https://github.com/Foorack/tensorflow-trainingdata\n> \n> Kind Regards,\n> ~Foorack\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n", "Page looks to be back up, so I'm closing this for now.\n", "The page seems to be down again from here...\n", "@jeremija I have unfortunately deleted my mirror as the problem was solved. Please tell me if you want me to upload it again. \n", "@jeremija Actually wayback-machine should work! Tell me how it went if you decide to try it. :) \n", "@Foorack wayback-machine [doesn't seem to work](https://web.archive.org/web/*/https://github.com/Foorack/tensorflow-trainingdata), but thanks anyway! You don't have to reupload for me, I just wanted to raise the issue.\n", "Like @jeremija, I am also unable to access the data set from either http://yann.lecun.com/exdb/mnist/ or wayback-machine.  Just FYI!\n", "There are actually snapshots on the wayback machine available [here](https://web.archive.org/web/20160117040036/http://yann.lecun.com/exdb/mnist/). I was only looking at Foorack's repository at first...\n", "Great, yes that link seems to work.  Thanks!\n", "OK @jeremija davebiagioni, great that you got it working :+1:\n", "Same for me.\n", "It seems that the page is not available again...", "@RuiZhang1993 It's still not available now. But [this link](https://web.archive.org/web/20160117040036/http://yann.lecun.com/exdb/mnist/) is still available, and you can download the data from there. Thanks a lot @jeremija!"]}, {"number": 208, "title": "Symbolic loops (like \"scan\" in Theano)", "body": "I've been wondering if there are plans to add symbolic loops to TensorFlow because I feel like this is a major feature when it comes to variable length sequences. Finite unfolding (with bucketing) seems like a dirty hack to me and since (as far as I understand) TensorFlow is meant for deployment, too, how do you envision using it for seq2seq translation given that you don't know ahead of time how long the generated sequence will be?\n\nThanks,\nSigurd\n", "comments": ["Our white paper mentions a number of control flow operations that we've experimented with -- I think once we're happy  with its API and confident in its implementation we will try to make it available through the public API -- we're just not quite there yet.  It's still early days for us :)\n", "I see. Great news that that's in the pipeline. Can't wait until it's publicly available. ;-)\n", "+1\n", "See the highly alpha and unsupported control_flow_ops.While, the\nTensorArray python class, and nn.dynamic_rnn.  All at HEAD.\nOn Feb 16, 2016 7:48 AM, \"Dmitrij Koniajev\" notifications@github.com\nwrote:\n\n> +1\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/208#issuecomment-184739842\n> .\n", "Any update or progress schedule?\n", "You can now use control_flow_ops.{map, foldl, foldr}  with forward and backprop, and you can call these functions from inside each others' lambdas.  For RNN, you can use dynamic_rnn which does the same.  If you have more complex eneds you can comment them here or use control_flow_ops.While and TensorArray.  I'm marking this as fixed.\n", "@ebrevdo: are those functions:\n\n1) public\n2) documented?\n", "Indeed not, but I believe {map_fn, foldl, foldr} are ready to be added by\nreferencing them in the header.  We'll get a CL out soon.\n\nOn Wed, Mar 9, 2016 at 9:21 AM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> @ebrevdo https://github.com/ebrevdo: are those functions:\n> \n> 1) public\n> 2) documented?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/208#issuecomment-194410139\n> .\n", "A toy RNN example with these functions would go a long way; especially since they do not directly correspond to the `theano.scan` functionality.\n", "See the implementation of dynamic_rnn for a comprehensive example.\nOn Mar 9, 2016 6:59 PM, \"rakeshvar\" notifications@github.com wrote:\n\n> A toy RNN example with these functions would go a long way; especially\n> since they do not directly correspond to the theano.scan functionality.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/208#issuecomment-194637554\n> .\n", "Apart from `map` and `fold`s, do you plan to implement also `scan` (as the original question mentions)?\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py#L256 !\n", "@rakeshvar here is a vanilla-RNN example: https://nbviewer.jupyter.org/github/rdipietro/tensorflow-notebooks/blob/master/tensorflow_scan_examples/tensorflow_scan_examples.ipynb\n\n(`dynamic_rnn` currently uses the lower-level `While` and `TensorArray` instead of the higher-level `scan`.)\n", "Scan uses the same approach as dynamic_rnn.  The latter was made first.\nWhereas in theano, scan was the fundamental loop method, in tensorflow that\nbehavior is subsumed my while loop and tensorarray.\nOn Apr 7, 2016 2:45 PM, \"Robert DiPietro\" notifications@github.com wrote:\n\n@rakeshvar https://github.com/rakeshvar here is a vanilla-RNN example:\nhttps://nbviewer.jupyter.org/github/rdipietro/tensorflow-notebooks/blob/master/tensorflow_scan_examples/tensorflow_scan_examples.ipynb\n\n(dynamic_rnn currently uses While and TensorArray, not scan.)\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/tensorflow/tensorflow/issues/208#issuecomment-207100390\n", "@rdipietro  The link to that notebook does not work for me :(\n", "Oh I'm sorry. Reorganized a bit.\n\nhttp://rdipietro.github.io/tensorflow-scan-examples/\nOn May 15, 2016 9:20 PM, \"22csnyder\" notifications@github.com wrote:\n\n> @rdipietro https://github.com/rdipietro The link to that notebook does\n> not work for me :(\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/208#issuecomment-219328359\n", "@rdipietro I tried to extend your code to a vanilla GRU. Also removed the clipping therefore and replaced Gradient Descent with Adagrad.\nConsequently convergence is now much faster.\nCan you please verify the code ?\nhttp://paste.ubuntu.com/16534925/\n(I know it's much dirty!)\n", "How to use map and scan together??????? Anyone have any idea?\n\nx=tf.constant(\n[[[1,2,3],[10,20,30]],\n[[1,2,3],[10,20,30]],\n[[1,2,3],[10,20,30]], \n[[1,2,3],[10,20,30]]])\n\ndef sum(x): return  tf.scan(lambda y, z : tf.add(y,z), x)\ntf.map_fn(sum ,a )\n\nThen errors pop up!!!! Is there any way using these together?\n@rdipietro I have been through your example notebook but how to use it for batch????\n", "The implementation of both scan and map_fn uses TensorArray. Nesting scan inside map_fn is essentially nested while loops. Unfortunately, there is a known bug when TensorArray and nested while loops are used together.  We have been working on a fix. \n", "Ok thanks.\n", "@KnHuq Not sure if this serves your problem for batches, but you can still see my [attempt](https://github.com/rajarsheem/playing-with-rnns/blob/master/rnn.ipynb) . There may be some issues, though.\n", "I am using scan to deal with different sequence length. But having problem to deal with batch! @rajarsheem can you write your solution for my above  problem??? If I use for loop then its same like online learning! No need to use batch then! So no need to use GPU then!\n\nPlease Google solve this nested while loops. ...\n", "You can see [here](https://github.com/rdipietro/miccai-2016-surgical-activity-rec/blob/master/train_and_eval.ipynb) for a more complex example, with a batch size > 1. But you'll need to either a) write not-so-clean code that's efficient (as they do in TensorFlow officially) or b) write clean code that is less efficient (this is what I do; I just wrap shorter sequences in time until all sequences are the same length, which simultaneously makes short sequences \"count\" just as much loss wise as long sequences).\n", "@rdipietro Thanks a lot. I will go through the your example!\n", "@rdipietro can you verify my graph, whether it is set up right ? (it is a very very simple [example](https://github.com/rajarsheem/playing-with-rnns/blob/master/rnn.ipynb))\n", "Finally I have figured out how to use scan with batch. So I have written some simple code for dynamic  [RNN](http://knhuq.github.io/RNN.html) and [GRU](http://knhuq.github.io/GRU.html). Have conducted a little hack as Tensorflow was raising low level error.  The github repo is [here](https://github.com/KnHuq/Dynamic_RNN_Tensorflow)\n", "The problem of scan and map together is solved by the latest release of Tensorflow. \n"]}, {"number": 207, "title": "confused by device placement on Amazon AWS", "body": "First - loving tensorflow.  The tutorials were really great fun to read.   Tensorflow API architecture is clear and amazing.   Hope to do pull requests once I get up to speed.\n\nHowever, I'm trying to do device placement to do some basic benchmarks to see if it's worth using Amazon at all versus my own computers.  So far, not so promising, but maybe I'm doing something wrong.\n\nFrom the example here:  http://tensorflow.org/how_tos/using_gpu/index.md\n\nI get the following results.\n\n<pre>\n\n>>> with tf.device('/cpu:0'):\n...   a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n...   b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n...   sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n...   print sess.run(c)\n...\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus\\\n id: 0000:00:03.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GRID K520, pci bus id: 0000:00:03.0\nI tensorflow/core/common_runtime/direct_session.cc:111] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GRID K520, pci bus id: 0000:00:03.0\n\nb: /job:localhost/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:289] b: /job:localhost/replica:0/task:0/cpu:0\na: /job:localhost/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:289] a: /job:localhost/replica:0/task:0/cpu:0\nMatMul_1: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:289] MatMul_1: /job:localhost/replica:0/task:0/gpu:0\n[[ 22.  28.]\n [ 49.  64.]]\n</pre>\n\nLooking at that, it looks like it's placing the matmul on the gpu.   I want it on the cpu.  Is that possible?  \n\nAlso, is matrix_inverse or matmul of say a 5000x5000 matrix a good (but very rough!) benchmark to help evaluate the value of Amazon GPU/CPU large instances?\n\nThanks again\n", "comments": ["In your code, it doesn't look like you wrapped the declaration of 'c' within the 'with tf.device()' block, so it doesn't have a device assigned to it.  Because we have an implementation of matmul on GPU, we automatically place the op on GPU if the op isn't hardcoded to a device.\n\nI bet if you moved  c = tf.matmul(a, b) into the `with` block,it would probably do wht you expect.\n\nAs for the benchmark: GPUs are generally good for large matrix multiplies, so it's not a bad benchmark, but it depends on what you intend to use TensorFlow for.  The best benchmark is a real application :)\n", "Great stuff, vrv.  That was the issue.  \n\nSo, following that matmul only works for matricies up to 3000x3000 size or so.  I guess that's fine.  However, it doesn't seem to work at all for Matrix Inverse.  Is this just not supported yet?  https://developer.nvidia.com/sites/default/files/akamai/cuda/files/Misc/mygpu.pdf\n\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'MatrixInverse_9': Could not satisfy explicit device specification '/gpu:0'\n", "Yeah, matrix inverse has not yet been implemented for GPU, so when you say 'run this on GPU', we can't find an implementation for it and we return that error.\n", "Ok, awesome!   Thanks.   I assume I just need to look at ~/tensorflow/tensorflow/core/kernels/_gpu_ for a list of the gpu supported operations.   I'll work with that.  List looks really exciting. \n\nCorrection, that doesn't seem to be right.  Let me look a bit more.\n", "Ok, I think this is the list.  \n\n$ ls _gpu_ | grep -v cwise\nadjust_contrast_op_gpu.cu.cc\naggregate_ops_gpu.cu.cc\nargmax_op_gpu.cu.cc\navgpooling_op_gpu.cu.cc\nbatch_norm_op_gpu.cu.cc\nbias_op_gpu.cu.cc\ncast_op_gpu.cu.cc\ncheck_numerics_op_gpu.cu.cc\nconcat_op_gpu.cu.cc\nconstant_op_gpu.cu.cc\nconv_ops_gpu.cu.cc\nconv_ops_gpu_2.cu.cc\nconv_ops_gpu_3.cu.cc\nconv_ops_gpu_matmul.cu.cc\ndense_update_ops_gpu.cu.cc\nl2loss_op_gpu.cu.cc\nmatmul_op_gpu.cu.cc\nmaxpooling_op_gpu.cu.cc\nmaxpooling_op_gpu.h\npad_op_gpu.cu.cc\npooling_ops_common_gpu.h\nrandom_op_gpu.cu.cc\nreduction_ops_gpu.cu.cc\nrelu_op_gpu.cu.cc\nreverse_op_gpu.cu.cc\nreverse_sequence_op_gpu.cu.cc\nslice_op_gpu.cu.cc\nsoftmax_op_gpu.cu.cc\nsoftplus_op_gpu.cu.cc\nsplit_op_gpu.cu.cc\ntile_ops_gpu.cu.cc\ntraining_ops_gpu.cu.cc\ntranspose_op_gpu.cu.cc\nxent_op_gpu.cu.cc\n\nLet me know if you want some assist with the docs.  Happy to do a pull request in whatever format you'd like.  Cheers\n", "Those are the list of the files, but some of those files have multiple op implementations in them.\n\nI would love to be able to automatically generate the list of implemented ops for each platform so they don't go out of date.  Maybe one day soon I'll get to doing this :)\n", "(should have a separate feature request for this)\n"]}]