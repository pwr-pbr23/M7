[{"number": 43629, "title": "Fail to build TF 1.15 on Cuda 11.1", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 1.15\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.25.3\r\n- GCC/Compiler version (if compiling from source): MSVC 2017\r\n- CUDA/cuDNN version: 11.1 / 7.6.0\r\n- GPU model and memory: RTX 2080 TI\r\n\r\n**Describe the problem**\r\n\r\nunable to build TF 1.15 on Cuda 11.1\r\n\r\n**Any other info / logs**\r\n\r\n```\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseSgtsv': undec\r\nlared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseDgtsv': undec\r\nlared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseCgtsv': undec\r\nlared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(212): error C2065: 'cusparseZgtsv': undec\r\nlared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseSgtsv_nopivot\r\n': undeclared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseDgtsv_nopivot\r\n': undeclared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseCgtsv_nopivot\r\n': undeclared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(224): error C2065: 'cusparseZgtsv_nopivot\r\n': undeclared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseSgtsvStridedB\r\natch': undeclared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseDgtsvStridedB\r\natch': undeclared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseCgtsvStridedB\r\natch': undeclared identifier\r\ntensorflow/core/kernels/cuda_sparse.cc(250): error C2065: 'cusparseZgtsvStridedB\r\natch': undeclared identifier\r\n```", "comments": ["You need master for Cuda 11", "@bhack that is tf 1.15-master", "I meant the master branch.", "My big app is created using TF 1\r\n\r\nI cannot upgrade it to TF 2 API, because it requires a lot of modifications and testings from scratch, which is time and money consuming task.\r\n\r\nSeems like Tensorflow(TM) cannot provide backward compatibility for new CUDA versions.\r\nSo even 2 years old app will not support new cards.\r\nIt is serious impact to business and companies which are using TF.\r\nWhere is the guarantee that it will not happen again?\r\n\r\nI am already very sorry that I did not choose pytorch at first.\r\nBurn in hell google, die tensorflow.\r\n", "@iperov You can express your opinion and also your frustration but please respect the perimeter of our code of conduct https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md /cc @theadactyl \r\n\r\nI think you can probably explore to use TF 1.15 in a Docker container.", "why there docker, if source code is just cannot handle cuda 11.1?\r\n\r\nRTX 3080 does not work with CUDA < 11.1", "@iperov \r\nPlease refer to these [configurations](\r\nhttps://www.tensorflow.org/install/source#tested_build_configurations).\r\n\r\nPlease note as per process we do not have support for tf 1.x you have to upgrade to 2.x. [You may try with cuda toolkit 9 and see if you face any issues as 1.x would not support cuda 9+]", "@Saduf2019 \r\n\r\nThe main issue is not about support 1.x.\r\nIssue is 1.x does not support new cards starts from RTX 3000 series.\r\n\r\nIs this problem with CUDA ( NVIDIA breaks backward compatibility ? ) or the source code of TF 1.x has bugs ?\r\n\r\nThis is serious reason not work with TF and/or CUDA anymore.\r\n\r\nPlease discuss this topic with your devteam.", "@iperov \r\nAs tf  1.x is not actively maintained anymore, there is no work on 1.x, hence please upgrade to 2.x ans there is support for it only now and any issues and up-gradation is performed on 2.x only.", "@Saduf2019 \r\nI don't need actively maintain 1.x \r\nJust fix support RTX 3000 for 1.15", "> @iperov\r\n> Please refer to these [configurations](https://www.tensorflow.org/install/source#tested_build_configurations).\r\n> \r\n> Please note as per process we do not have support for tf 1.x you have to upgrade to 2.x. [You may try with cuda toolkit 9 and see if you face any issues as 1.x would not support cuda 9+]\r\n\r\nas explained above there is no support for 1.x now, no fixes/changes will be made to 1.x. \r\nplease upgrade to 2.x and let us know if you face any issues. kindly move this issue to closed status if there are no issues with 2.x", "Please follow our migration guide https://www.tensorflow.org/guide/migrate", "Hi @iperov \r\n\r\nIn order to obtain the maximum performance, code is tied to the current version of CUDA. Hence, each branch can only build with exactly one version of CUDA. It will be extremely costly to upgrade an old branch to a new CUDA version and very risky, so we don't do that at all.\r\n\r\nTF is not alone here. Every software moves forward and every user will have to upgrade at one time or another.", "@mihaimaruseac \r\nok. Is there any way to use any version of tensorflow with RTX 3k on Windows ?", "If you want CUDA 11 you can use nightly. If you want to keep 1.x APIs then you can only use CUDA 10.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43629\">No</a>\n", "To use tensorflow 1.1x on CUDA11.x, I think you should use nvidia-tensorflow. \r\nThe installation is quite simple.\r\n(https://developer.nvidia.com/blog/accelerating-tensorflow-on-a100-gpus/)", "> I think you should use nvidia-tensorflow.\r\n> (https://developer.nvidia.com/blog/accelerating-tensorflow-on-a100-gpus/)\r\n\r\nAny possible ways to compile working libtensorflow.dll with it? I installed cuda 10.0 to system with 3070 and tensorflow.dll 1.15 that i'm currently using doesn't give me any errors, but initialization and inference are extremely slow.", "Install NVIDIA drivers(455.23). After installing it check the status of GPU using `nvidia-smi`. Then install tf-1.15 as follows:\r\n\r\n```\r\nsudo apt update\r\nsudo apt install -y python3-dev python3-pip git\r\npip3 install --upgrade pip setuptools requests\r\n\r\npip install -U virtualenv\r\nvirtualenv --system-site-packages -p python3 /venv\r\nsource /venv/bin/activate\r\n\r\npip install nvidia-pyindex\r\npip install nvidia-tensorflow[horovod]\r\n\r\n```\r\nThis should install tf-1.15 with cuda 11.1 support. \r\nTest it as follows:\r\n`python -c 'import tensorflow as tf; print(tf.__version__)'`\r\n\r\n`python -c \"import tensorflow as tf; print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\"`", "@anshkumar are you bot or spammer?", "@iperov why do you think so ?\r\nThis worked for me. See (https://developer.nvidia.com/blog/accelerating-tensorflow-on-a100-gpus/)", "> > I think you should use nvidia-tensorflow.\r\n> > (https://developer.nvidia.com/blog/accelerating-tensorflow-on-a100-gpus/)\r\n> \r\n> Any possible ways to compile working libtensorflow.dll with it? I installed cuda 10.0 to system with 3070 and tensorflow.dll 1.15 that i'm currently using doesn't give me any errors, but initialization and inference are extremely slow.\r\n\r\nHi @VladislavAD ,\r\n\r\nI have a similar question here. \r\nI tried to run a self-built tensorflow.dll(1.13.1) with cuda 10.0 and cudnn7.4.2 on RTX3080, it can run but the result is **totally different** from the one I ran on RTX2080Ti. Initialization took about 20 mins in my case, but the inference time is similar with the time I ran on RTX2080Ti. I'm wondering is the inference result being as expected in your case?\r\n\r\nAnother question here, how did you deal with using TF1's tensorflow.dll on RTX30 series' GPU card?\r\n(Or did you switch to TF2 to solve this issue?)\r\n\r\nThanks in advance for any advice, I'm looking forward to your reply!\r\n"]}, {"number": 43628, "title": "tf.feature_column.embedding_column: wrong dtype in lookup variable", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n\r\nYes\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nUbuntu 18.10\r\n\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n\r\nNo\r\n\r\n-   **TensorFlow installed from (source or binary)**:\r\n\r\npip install -U --force-reinstall tf-nightly\r\n\r\n-   **TensorFlow version (use command below)**:\r\n\r\n'2.4.0-dev20200712'\r\n\r\n-   **Python version**:\r\n\r\npython --version\r\nPython 3.7.7\r\n\r\n\r\n-   **Bazel version (if compiling from source)**:\r\n\r\nNO\r\n\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n\r\nNO\r\n\r\n-   **CUDA/cuDNN version**:\r\n\r\nReproducable on CPU\r\n\r\n-   **GPU model and memory**:\r\n\r\nReproducable on CPU\r\n\r\n-   **Exact command to reproduce**:\r\n\r\nhttps://gist.github.com/dennisjay/9482041ba5e037b0cc8fd12dcd2292c8\r\n\r\n### Describe the problem\r\nThe tf.feature_column.embedding_column of my tensorflow Version is internally working with int32 instead of int64 and thus there are errors when exporting the model to saved model and compiling it with i.e. triton server.\r\n\r\n![image](https://user-images.githubusercontent.com/1181660/94471941-481fd780-01ca-11eb-9693-8b7b8537bf0d.png)\r\n\r\nThe strided slice operation outputs a int64 but the following operation is accepting int32 only. Reproducable is the error with the use of the jupyter notebook above.\r\n\r\n### Source code / logs\r\n\r\nInferenceServerException: [_Derived_]{{function_node __inference_signature_wrapper_3345}} {{function_node __inference_signature_wrapper_3345}} input segment_ids[0] expected type int32 != int64, the type of sequential_1/dense_features_1/test_col_embedding/test_col_embedding_weights/embedding_lookup_sparse/strided_slice:output:0[0]\r\n\tIn {{node sequential_1/dense_features_1/test_col_embedding/test_col_embedding_weights/embedding_lookup_sparse}}\r\n\t [[StatefulPartitionedCall]]\r\n\t [[StatefulPartitionedCall_1]]\r\n", "comments": ["@dennisjay \r\n\r\nLooks like you are using a custom module named \"tritonhttpclient\" in your code. In order to reproduce the issue, could you please provide the complete reproducible code including all the dependencies? Thanks!", "@ravikyram the full code for installation and running is provided in the gist. The third party module for nvidia triton server can be downloaded here: https://github.com/triton-inference-server/server/releases/tag/v2.2.0 .\r\n\r\nThanks for looking at this.", "@dennisjay \r\n\r\nI tried in colab and seeing the error message as `ConnectionRefusedError: [Errno 111] Connection refused`.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c8b0d56d7b13f632b5aa67f8c0e56186/untitled402.ipynb). Thanks!", "The notebook requires the nvidia triton docker container to be running. With this command from cell 17:\r\n\r\ndocker run --rm -p8030:8000 -p8031:8001 -p8032:8002 -v`pwd`/test_repo:/models nvcr.io/nvidia/tritonserver:20.08-py3 tritonserver --model-repository=/models\r\n\r\nPlease be shure to execute it from within the notebook path.\r\nSorry for the little bit complicated reproduction. \r\n\r\n\r\n\r\n", "@ravikyram were you able to reproduce the error?", "I see you're already using TF 2.4. Please try to use [Keras Preprocessing Layer](https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md) which gives you much more flexible control for dtypes.", "We fixed the error with the Keras preprocessing layer. Ty for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43628\">No</a>\n", "@dennisjay  now I encounter the same problem, can you tell me how to solve it specifically\uff1f\r\n\r\nthank you very much"]}, {"number": 43627, "title": "TF2 Memory Leak when fitting multiple models in Docker", "body": "When fitting multiple models within same tuning process, the consumption of RAM grows until all of the memory is consumed, which ultimately leads to failure of the complete training process.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): docker pull tensorflow/tensorflow:2.0.0-py3-jupyter\r\n- TensorFlow version (use command below): tensorflow:2.0.0  and every one up!\r\n- Python version: 3.6 \r\n\r\n**Describe the current behavior**\r\nWhen fitting multiple keras models RAM is consumed until there is no more free RAM and training process fails.\r\n\r\n**Describe the expected behavior**\r\ntf.keras.backend.clear_session() should reset the memory consumption after each tuning iteration.\r\n\r\n**Standalone code to reproduce the issue**\r\nfrom tensorflow import keras\r\nimport psutil\r\n\r\nfor i in range(0, 200):\r\n    keras.backend.clear_session()\r\n    a = keras.layers.Input(shape=(32,))\r\n    b = keras.layers.Dense(32)(a)\r\n    model = keras.Model(inputs=a, outputs=b)\r\n    print('memory usesd: ' + str(psutil.virtual_memory().used // 1e6))", "comments": ["Can you try with TF 2.3.1?", "@bhack that TF image is not available on Docker Hub.", "2.3.0 is ok too..", "@bhack yes pls take a look at: https://github.com/tensorflow/tensorflow/issues/31312\r\n", "#31312 is closed already. Let's use this issue thread to discuss the memory leak problem.\r\n\r\n@markodjordjic Could you verify [this comment](https://github.com/tensorflow/tensorflow/issues/43627#issuecomment-700225043) first?\r\n", "@bhack yes TF2.3.0 does not help", "It seems to me that it isn't  growing in colab with 2.3.0", "@bhack can we please acknowledge that there is a problem. Numerous issues have been opened and there is no progress.", "On TF 1.3.0 on Colab\r\n```python\r\nfrom tensorflow import keras\r\nfrom statistics import mean\r\nimport psutil\r\nmem_stats=[]\r\nfor i in range(0, 10001):\r\n  keras.backend.clear_session()\r\n  a = keras.layers.Input(shape=(32,))\r\n  b = keras.layers.Dense(32)(a)\r\n  model = keras.Model(inputs=a, outputs=b)\r\n  mem_stats.append(psutil.virtual_memory().used)\r\n  if (i % 1000 == 0):\r\n    print('memory -> step %5d Max %d Min %d Mean %d ' % (i, \r\n                                                         max(mem_stats)/(1024), \r\n                                                         min(mem_stats)/(1024), \r\n                                                         mean(mem_stats)/(1024)\r\n                                                         )\r\n    )\r\n    mem_stats.clear()\r\n```\r\n\r\n```\r\nmemory -> step     0 Max 1262812 Min 1262812 Mean 1262812 \r\nmemory -> step  1000 Max 1265760 Min 1261540 Mean 1262152 \r\nmemory -> step  2000 Max 1265484 Min 1261376 Mean 1261946 \r\nmemory -> step  3000 Max 1270308 Min 1261772 Mean 1264956 \r\nmemory -> step  4000 Max 1269436 Min 1264424 Mean 1265338 \r\nmemory -> step  5000 Max 1268056 Min 1264468 Mean 1265161 \r\nmemory -> step  6000 Max 1269956 Min 1264492 Mean 1265200 \r\nmemory -> step  7000 Max 1268564 Min 1264476 Mean 1265354 \r\nmemory -> step  8000 Max 1268628 Min 1264376 Mean 1265071 \r\nmemory -> step  9000 Max 1268000 Min 1263448 Mean 1264206 \r\nmemory -> step 10000 Max 1267624 Min 1263276 Mean 1263966 \r\n```", "@bhack yes. On lower versions of TF it works. I had to refactor the code to lower versions.", "It was just a typo.. is 2.3.0", "@bhack I have utilized the docker pull tensorflow/tensorflow:2.3.1-gpu-jupyter and I can confirm that the issue is _not_ resolved. Can you please take into account that the training is happening inside docker container on AWS and that I could be that keras.backend.clear_session() cannot release the RAM occupied by tf.keras,model in this setting.", "Can you run my example unmodified on your  setup and paste the output here?", "@bhack here you go. TF docker image tensorflow/tensorflow:2.3.1-gpu-jupyter.\r\n\r\nalgo-1-7b5eo_1  | 2020-09-29 14:14:14.820908: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nalgo-1-7b5eo_1  | memory -> step     0 Max 806536 Min 806536 Mean 806536 \r\nalgo-1-7b5eo_1  | memory -> step  1000 Max 813376 Min 806332 Mean 811834 \r\nalgo-1-7b5eo_1  | memory -> step  2000 Max 816452 Min 813352 Mean 815939 \r\nalgo-1-7b5eo_1  | memory -> step  3000 Max 816444 Min 816436 Mean 816440 \r\nalgo-1-7b5eo_1  | memory -> step  4000 Max 816676 Min 816436 Mean 816590 \r\nalgo-1-7b5eo_1  | memory -> step  5000 Max 816916 Min 816676 Mean 816773 \r\nalgo-1-7b5eo_1  | memory -> step  6000 Max 820372 Min 816784 Mean 817007 \r\nalgo-1-7b5eo_1  | memory -> step  7000 Max 817528 Min 817288 Mean 817397 \r\nalgo-1-7b5eo_1  | memory -> step  8000 Max 817512 Min 817280 Mean 817384 \r\nalgo-1-7b5eo_1  | memory -> step  9000 Max 817504 Min 817376 Mean 817381 \r\nalgo-1-7b5eo_1  | memory -> step 10000 Max 817748 Min 817368 Mean 817393 \r\nalgo-1-7b5eo_1  | 2020-09-29 14:15:41,387 sagemaker-training-toolkit INFO     Reporting training SUCCESS\r\ntmpri_3aih3_algo-1-7b5eo_1 exited with code 0\r\nAborting on container exit...\r\n===== Job Complete =====", "@bhack Thanks.", "@bhack it revealed the problem.", "I've try in the same docker image that you have used but on a machine without cuda devices and it doesn't seems to me growing monotonically. At 10k steps it is less then 3k steps:\r\n```\r\nmemory -> step     0 Max 3873720 Min 3873720 Mean 3873720 \r\nmemory -> step  1000 Max 3883720 Min 3873720 Mean 3880353 \r\nmemory -> step  2000 Max 3884048 Min 3882772 Mean 3883289 \r\nmemory -> step  3000 Max 3885432 Min 3883848 Mean 3884593 \r\nmemory -> step  4000 Max 3884192 Min 3883912 Mean 3884009 \r\nmemory -> step  5000 Max 3887120 Min 3883696 Mean 3885244 \r\nmemory -> step  6000 Max 3885896 Min 3885248 Mean 3885504 \r\nmemory -> step  7000 Max 3885656 Min 3884416 Mean 3884744 \r\nmemory -> step  8000 Max 3885172 Min 3884440 Mean 3884854 \r\nmemory -> step  9000 Max 3885676 Min 3884280 Mean 3884742 \r\nmemory -> step 10000 Max 3885228 Min 3884152 Mean 3884441 \r\n\r\n```\r\n\r\nCan you verify if your problem is on GPU only? Run the same with this before the loop\r\n```\r\n tf.config.set_visible_devices([], 'GPU')\r\n```", "@bhack the AWS instance on which I ran the script was CPU only.", "@bhack any ideas how to circumvent this in CPU only setting? Btw, thanks for your efforts.", "I cannot reproduce this. \r\nI've runned the script with mprof for 50k steps in the Docker container:\r\n`mprof run --include-children --multiprocess tf_memory_test.py`\r\n\r\n![memory](https://user-images.githubusercontent.com/1710528/94676566-d5267600-031b-11eb-8de4-1eebde3c403c.png)\r\n", "@bhack fit method of the keras.model object is the main problem. If fit (and predict) methods are added there is a memory leak.", "So do you have another minimal example with fit/predict to reproduce this?", "@bhack I will make one tomorrow. I have also a work-around for this issue. I will also post it tomorrow. It has been a long day here. Thank you.", "@bhack here is an example with fit method.\r\nfor i in range(0, 10001):\r\n  keras.backend.clear_session()\r\n  a = keras.layers.Input(shape=(2,))\r\n  b = keras.layers.Dense(32)(a)\r\n  output = keras.layers.Dense(1)(b)\r\n  model = keras.Model(inputs=a, outputs=output)\r\n  model.compile(\r\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n    loss='mse',\r\n    metrics=None,\r\n  )\r\n  model.fit(\r\n    x=x,\r\n    y=y,\r\n    batch_size=128,\r\n    epochs=2,\r\n    verbose=0,\r\n    callbacks=None,\r\n    shuffle=False    \r\n  )\r\n  print('memory uses: ' + str(psutil.virtual_memory().used // 1e6))", "This code has missing x,y can you share something runnable?", "@bhack how did that happen. Here is an update.\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom statistics import mean\r\nimport psutil\r\n\r\n# Generate data.\r\nsamples = 10000\r\nmean = [0, 0]\r\ncov = [[1, 0], [0, 100]]  # diagonal covariance\r\nx = np.random.multivariate_normal(mean, cov, samples)\r\ny = np.random.normal(0, 1, (samples, 1))\r\nfor i in range(0, 10001):\r\n  keras.backend.clear_session()\r\n  a = keras.layers.Input(shape=(2,))\r\n  b = keras.layers.Dense(32)(a)\r\n  output = keras.layers.Dense(1)(b)\r\n  model = keras.Model(inputs=a, outputs=output)\r\n  model.compile(\r\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n    loss='mse',\r\n    metrics=None,\r\n  )\r\n  model.fit(\r\n    x=x,\r\n    y=y,\r\n    batch_size=128,\r\n    epochs=2,\r\n    verbose=0,\r\n    callbacks=None,\r\n    shuffle=False    \r\n  )\r\n  print('Memory used: ' + str(psutil.virtual_memory().used // 1e6))", "Can you edit a final version that we can just copy and run? \r\nPlease put it formatted in a code block as I've done with my examples.", "```\r\nsamples = 10000\r\nmean = [0, 0]\r\ncov = [[1, 0], [0, 100]]  # diagonal covariance\r\nx = np.random.multivariate_normal(mean, cov, samples)\r\ny = np.random.normal(0, 1, (samples, 1))\r\nfor i in range(0, 10001):\r\n  keras.backend.clear_session()\r\n  a = keras.layers.Input(shape=(2,))\r\n  b = keras.layers.Dense(32)(a)\r\n  output = keras.layers.Dense(1)(b)\r\n  model = keras.Model(inputs=a, outputs=output)\r\n  model.compile(\r\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\r\n    loss='mse',\r\n    metrics=None,\r\n  )\r\n  model.fit(\r\n    x=x,\r\n    y=y,\r\n    batch_size=128,\r\n    epochs=2,\r\n    verbose=0,\r\n    callbacks=None,\r\n    shuffle=False\r\n  )\r\n  print('Memory used: ' + str(psutil.virtual_memory().used // 1e6))\r\n```", "@bhack there you go", "@bhack @yhliang2018 there appears to be more of this: https://github.com/tensorflow/tensorflow/issues/43702", "Hi, yes, #43702 seems very related, also because that code over there fails to reproduce the issue when there is no `model.fit`.\r\n\r\nIn addition, #43702 has a complete example with all imports and data that reproduces the issue on a CPU-only system and on Colab, as well as on Linux and Windows, and independent of Docker. (I fail the repro the problem with the code provided in this issue.) Finally, that example reports process memory consumption rather than system memory consumption. In summary, I feel the example in #43702 may be better suited to repro the issue.", "@bersbersbers @bhack funny enough docker pull tensorflow/tensorflow:nightly-gpu-jupyter seems to help. I need to run a more comprehensive training process, to confirm, but on the example with fit method, the memory consumption stays fairly managable. Just to confirm this is AWS linux setup, on a ml.t3.medium instance and the training is done within the docker container mentioned above.", "@bhack @yhliang2018 Hi. Thank you for your efforts. I would like to confirm that the issue still persists on AWS regardless of docker pull tensorflow/tensorflow:nightly-gpu-jupyter. The workaround is to implement custom training process and circumvent the usage of keras.model.fit method. Here is an example which can be customized to particular needs:\r\n\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\nIs there any way that the existence of this issue can be acknowledged and addressed in future releases of the TF?\r\n", "Have you tried to compile with eager like in https://github.com/tensorflow/tensorflow/issues/43702#issuecomment-702681590", "@bhack Thank you for your reply. I have run a minimalistic example, and I can confirm that `run_eagerly=True` reduces the memory leak, however the problem still exists. After 30 cv folds the consumption of RAM has increased from 891 MB to 940 MB. Therefore, approximately 50 MB per single training cycle. In case of 300 models, this will lead to consumption of approximately 15 Gb over the complete training process. Also, it is unclear if this will scale up with the increase of the size of the training set (larger training set, larger memory leak).\r\nTherefore, the problem is ameliorated, however it still exists and it will exhibit it self in more complex training processes, involving more models, or maybe larger training sets.", "Can you graph with `mprun` like in https://github.com/tensorflow/tensorflow/issues/43702#issuecomment-703510259 ?", "@markodjordjic Is this still an issue for you? Can you please check with recent TF versions (`TF2.7` anmd `tf-nightly`). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43624, "title": "Converted model gives shape error during inference in C++ API.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.4.0-dev20200630\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\n\"\"\"TensorflowTTS-TFLite.ipynb\r\n\r\nAutomatically generated by Colaboratory.\r\n\r\nOriginal file is located at\r\n    https://colab.research.google.com/drive/1HudLLpT9CQdh2k04c06bHUwLubhGTWxA\r\n\r\nCopyright 2020 The TensorFlow Authors. All Rights Reserved.\r\n\r\nAuthor : [jaeyoo@](https://github.com/jaeyoo),\r\n[khanhlvg@](https://github.com/khanhlvg),\r\n[abattery@](https://github.com/abattery), [thaink@](https://github.com/thaink)\r\n(Google Research) and [dathudeptrai@](https://github.com/dathudeptrai) (owner\r\nTensorflowTTS)\r\n\r\n# TensorflowTTS real TFLite Conversion demonstration\r\n\r\nThis notebook provides a demo to convert models from TensorflowTTS to TFlite\r\n\r\n- Github: https://github.com/TensorSpeech/TensorflowTTS\r\n- Audio samples: https://tensorspeech.github.io/TensorflowTTS/\r\n\"\"\"\r\n\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\nimport os\r\nimport sys\r\nimport time\r\n\r\nimport yaml\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow_tts.configs import FastSpeech2Config, FastSpeechConfig, Tacotron2Config\r\nfrom tensorflow_tts.models import TFFastSpeech, TFFastSpeech2, TFTacotron2\r\nfrom tensorflow_tts.processor import LJSpeechProcessor\r\nfrom tensorflow_tts.processor.ljspeech import LJSPEECH_SYMBOLS\r\nfrom tensorflow_tts.configs import MultiBandMelGANGeneratorConfig\r\n\r\nfrom tensorflow_tts.models import TFMelGANGenerator\r\nfrom tensorflow_tts.models import TFPQMF\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nsys.path.append(\"tensorflowtts/\")\r\ntf.__version__\r\n\r\n\r\ndef load_tacotron2():\r\n    # Tacotron2\r\n    with open(\"tacotron2_config.yml\") as f:\r\n        config = yaml.load(f, Loader=yaml.Loader)\r\n\r\n    config = Tacotron2Config(**config[\"tacotron2_params\"])\r\n    tacotron2 = TFTacotron2(\r\n        config=config,\r\n        training=False,\r\n        name=\"tacotron2v2\",\r\n        enable_tflite_convertible=True,\r\n    )\r\n\r\n    # Newly added :\r\n    tacotron2.setup_window(win_front=6, win_back=6)\r\n    tacotron2.setup_maximum_iterations(1000)  # be careful\r\n\r\n    tacotron2._build()\r\n    tacotron2.load_weights(\"tacotron2-120k.h5\")\r\n    return tacotron2\r\n\r\n\r\ndef load_fastspeech():\r\n    # FastSpeech\r\n\r\n    with open(\"fastspeech_config.yml\") as f:\r\n        config = yaml.load(f, Loader=yaml.Loader)\r\n\r\n    config = FastSpeechConfig(**config[\"fastspeech_params\"])\r\n    fastspeech = TFFastSpeech(\r\n        config=config, enable_tflite_convertible=True, name=\"fastspeech\"\r\n    )\r\n    fastspeech._build()\r\n    fastspeech.load_weights(\"fastspeech-150k.h5\")\r\n    return fastspeech\r\n\r\n\r\ndef load_fastspeech2():\r\n    # FastSpeech2\r\n    with open(\"fastspeech2_config.yml\") as f:\r\n        config = yaml.load(f, Loader=yaml.Loader)\r\n\r\n    config = FastSpeech2Config(**config[\"fastspeech_params\"])\r\n    fastspeech2 = TFFastSpeech2(\r\n        config=config, enable_tflite_convertible=True, name=\"fastspeech2\"\r\n    )\r\n    fastspeech2._build()\r\n    fastspeech2.load_weights(\"fastspeech2-150k.h5\")\r\n    return fastspeech2\r\n\r\n\r\ndef convert_tacotron2(tacotron2):\r\n    \"\"\"## Convert to TF Lite\r\n\r\n    ### Tacotron-2\r\n    \"\"\"\r\n\r\n    # Concrete Function\r\n    tacotron2_concrete_function = tacotron2.inference_tflite.get_concrete_function()\r\n\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n        [tacotron2_concrete_function]\r\n    )\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the TF Lite model.\r\n    with open(\"tacotron2.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n\r\n    print(\"Model size is %f MBs.\" % (len(tflite_model) / 1024 / 1024.0))\r\n\r\n\r\ndef convert_fastspeech(fastspeech):\r\n    \"\"\"### FastSpeech\"\"\"\r\n\r\n    # Concrete Function\r\n    fastspeech_concrete_function = fastspeech.inference_tflite.get_concrete_function()\r\n\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n        [fastspeech_concrete_function]\r\n    )\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the TF Lite model.\r\n    with open(\"fastspeech_quant.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n\r\n    print(\"Model size is %f MBs.\" % (len(tflite_model) / 1024 / 1024.0))\r\n\r\n\r\ndef convert_fastspeech2(fastspeech2):\r\n    \"\"\"### FastSpeech2\"\"\"\r\n\r\n    # Concrete Function\r\n    fastspeech2_concrete_function = fastspeech2.inference_tflite.get_concrete_function()\r\n\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n        [fastspeech2_concrete_function]\r\n    )\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the TF Lite model.\r\n    with open(\"fastspeech2_quant.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n\r\n    print(\"Model size is %f MBs.\" % (len(tflite_model) / 1024 / 1024.0))\r\n\r\n\r\n\"\"\"# Inference\"\"\"\r\n\r\n\r\ndef visualize_mel_spectrogram(mels):\r\n    mels = tf.reshape(mels, [-1, 80]).numpy()\r\n    fig = plt.figure(figsize=(10, 8))\r\n    ax1 = fig.add_subplot(311)\r\n    ax1.set_title(\"Predicted Mel-after-Spectrogram\")\r\n    im = ax1.imshow(np.rot90(mels), aspect=\"auto\", interpolation=\"none\")\r\n    fig.colorbar(mappable=im, shrink=0.65, orientation=\"horizontal\", ax=ax1)\r\n    plt.show()\r\n    plt.close()\r\n\r\n\r\ndef infer_tacotron2():\r\n    \"\"\"## Tacotron-2\"\"\"\r\n    # Load the TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=\"tacotron2.tflite\")\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Prepare input data.\r\n    def prepare_input(input_ids):\r\n        return (\r\n            tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),\r\n            tf.convert_to_tensor([len(input_ids)], tf.int32),\r\n            tf.convert_to_tensor([0], dtype=tf.int32),\r\n        )\r\n\r\n    # Test the model on random input data.\r\n    def infer(input_text):\r\n        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)\r\n        input_ids = processor.text_to_sequence(input_text.lower())\r\n        print(\"Input ids shape:\", input_ids.shape)\r\n        interpreter.resize_tensor_input(input_details[0][\"index\"], [1, len(input_ids)])\r\n        interpreter.allocate_tensors()\r\n        input_data = prepare_input(input_ids)\r\n        for i, detail in enumerate(input_details):\r\n            print(\"Input tensor %d detail:\" % i, detail)\r\n            input_shape = detail[\"shape\"]\r\n            print(\"Shape of tensor %d:\" % i, input_data[i].shape)\r\n            interpreter.set_tensor(detail[\"index\"], input_data[i])\r\n\r\n        interpreter.invoke()\r\n\r\n        # The function `get_tensor()` returns a copy of the tensor data.\r\n        # Use `tensor()` in order to get a pointer to the tensor.\r\n        return (\r\n            interpreter.get_tensor(output_details[0][\"index\"]),\r\n            interpreter.get_tensor(output_details[1][\"index\"]),\r\n        )\r\n\r\n    input_text = \"Recent research at Harvard has shown meditating\\\r\n    for as little as 8 weeks, can actually increase the grey matter in the \\\r\n    parts of the brain responsible for emotional regulation, and learning.\"\r\n\r\n    decoder_output_tflite, mel_output_tflite = infer(input_text)\r\n\r\n    return decoder_output_tflite, mel_output_tflite\r\n\r\n\r\ndef infer_fastspeech():\r\n    \"\"\"## FastSpeech\"\"\"\r\n    # Load the TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=\"fastspeech_quant.tflite\")\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Prepare input data.\r\n    def prepare_input(input_ids):\r\n        input_ids = tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0)\r\n        return (\r\n            input_ids,\r\n            tf.convert_to_tensor([0], tf.int32),\r\n            tf.convert_to_tensor([1.0], dtype=tf.float32),\r\n        )\r\n\r\n    # Test the model on random input data.\r\n    def infer(input_text):\r\n        for x in input_details:\r\n            print(x)\r\n        for x in output_details:\r\n            print(x)\r\n        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)\r\n        input_ids = processor.text_to_sequence(input_text.lower())\r\n        interpreter.resize_tensor_input(input_details[0][\"index\"], [1, len(input_ids)])\r\n        interpreter.resize_tensor_input(input_details[1][\"index\"], [1])\r\n        interpreter.resize_tensor_input(input_details[2][\"index\"], [1])\r\n        interpreter.allocate_tensors()\r\n        input_data = prepare_input(input_ids)\r\n        for i, detail in enumerate(input_details):\r\n            input_shape = detail[\"shape\"]\r\n            interpreter.set_tensor(detail[\"index\"], input_data[i])\r\n\r\n        interpreter.invoke()\r\n\r\n        # The function `get_tensor()` returns a copy of the tensor data.\r\n        # Use `tensor()` in order to get a pointer to the tensor.\r\n        return (\r\n            interpreter.get_tensor(output_details[0][\"index\"]),\r\n            interpreter.get_tensor(output_details[1][\"index\"]),\r\n        )\r\n\r\n    input_text = \"Recent research at Harvard has shown meditating\\\r\n    for as little as 8 weeks, can actually increase the grey matter in the \\\r\n    parts of the brain responsible for emotional regulation, and learning.\"\r\n\r\n    decoder_output_tflite, mel_output_tflite = infer(input_text)\r\n\r\n\r\ndef infer_fastspeech2():\r\n    \"\"\"## FastSpeech2\"\"\"\r\n    # Load the TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=\"fastspeech2_quant.tflite\")\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Prepare input data.\r\n    def prepare_input(input_ids):\r\n        input_ids = tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0)\r\n        return (\r\n            input_ids,\r\n            tf.convert_to_tensor([0], tf.int32),\r\n            tf.convert_to_tensor([1.0], dtype=tf.float32),\r\n            tf.convert_to_tensor([1.0], dtype=tf.float32),\r\n            tf.convert_to_tensor([1.0], dtype=tf.float32),\r\n        )\r\n\r\n    # Test the model on random input data.\r\n    def infer(input_text):\r\n        for x in input_details:\r\n            print(x)\r\n        for x in output_details:\r\n            print(x)\r\n        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)\r\n        input_ids = processor.text_to_sequence(input_text.lower())\r\n        interpreter.resize_tensor_input(input_details[0][\"index\"], [1, len(input_ids)])\r\n        interpreter.resize_tensor_input(input_details[1][\"index\"], [1])\r\n        interpreter.resize_tensor_input(input_details[2][\"index\"], [1])\r\n        interpreter.resize_tensor_input(input_details[3][\"index\"], [1])\r\n        interpreter.resize_tensor_input(input_details[4][\"index\"], [1])\r\n        interpreter.allocate_tensors()\r\n        input_data = prepare_input(input_ids)\r\n        for i, detail in enumerate(input_details):\r\n            input_shape = detail[\"shape\"]\r\n            interpreter.set_tensor(detail[\"index\"], input_data[i])\r\n\r\n        interpreter.invoke()\r\n\r\n        # The function `get_tensor()` returns a copy of the tensor data.\r\n        # Use `tensor()` in order to get a pointer to the tensor.\r\n        return (\r\n            interpreter.get_tensor(output_details[0][\"index\"]),\r\n            interpreter.get_tensor(output_details[1][\"index\"]),\r\n        )\r\n\r\n    input_text = \"Recent research at Harvard has shown meditating\\\r\n    for as little as 8 weeks, can actually increase the grey matter in the \\\r\n    parts of the brain responsible for emotional regulation, and learning.\"\r\n\r\n    decoder_output_tflite, mel_output_tflite = infer(input_text)\r\n\r\n\r\ndef convert_vocoder():\r\n    # Convert the vocoder as a TFLite model.\r\n    with open(\r\n        \"tensorflowtts/examples/multiband_melgan/conf/multiband_melgan.v1.yaml\"\r\n    ) as f:\r\n        config = yaml.load(f, Loader=yaml.Loader)\r\n\r\n    config = MultiBandMelGANGeneratorConfig(\r\n        **config[\"multiband_melgan_generator_params\"]\r\n    )\r\n\r\n    class TFMBMelGANGenerator(TFMelGANGenerator):\r\n        def __init__(self, config, **kwargs):\r\n            super().__init__(config, **kwargs)\r\n            self.pqmf = TFPQMF(config=config, name=\"pqmf\")\r\n\r\n        def build(self, input_shape):\r\n            self.pqmf.build(input_shape)\r\n            self.built = True\r\n\r\n        @tf.function(\r\n            experimental_relax_shapes=True,\r\n            input_signature=[tf.TensorSpec(shape=[None, None, 80], dtype=tf.float32)],\r\n        )\r\n        def call(self, mels):\r\n            mb_audios = super().call(mels)\r\n            audios = self.pqmf.synthesis(mb_audios)\r\n            return audios\r\n\r\n    mb_melgan = TFMBMelGANGenerator(config, name=\"mb_melgan\")\r\n    fake_mels = tf.random.uniform(shape=[1, 256, 80], dtype=tf.float32)\r\n    audios = mb_melgan(fake_mels)[0, :, 0]\r\n    mb_melgan.summary()\r\n    mb_melgan.load_weights(\"generator-940000.h5\")\r\n    concrete_func = mb_melgan.call.get_concrete_function()\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    mbmlite = converter.convert()\r\n    # Save the TF Lite model.\r\n    with tf.io.gfile.GFile(\"mbm_quant.tflite\", \"wb\") as f:\r\n        f.write(mbmlite)\r\n    print(\"Model saved to mbm_quant.tflite file\")\r\n    print(\"Model size is %f MBs.\" % (len(mbmlite) / 1024 / 1024.0))\r\n\r\n\r\ndef infer_mbmelgan(mel_output_tflite):\r\n    class MBMelganLite:\r\n        def __init__(self, path):\r\n            # Load TFLite model and allocate tensors.\r\n            self.interpreter = tf.lite.Interpreter(model_path=path)\r\n            # Get input and output tensors.\r\n            self.inputs = self.interpreter.get_input_details()\r\n            self.outputs = self.interpreter.get_output_details()\r\n            self.current_shape = self.inputs[0][\"shape\"]\r\n\r\n        def printDetails(self):\r\n            for x in self.inputs:\r\n                print(x)\r\n            for x in self.outputs:\r\n                print(x)\r\n\r\n        def infer(self, mel):\r\n            # Resize if input length different, assuming batch size is always 1.\r\n            if self.current_shape[1] is not mel.shape[1]:\r\n                print(\r\n                    f\"Input shape: {mel.shape} , interpreter shape: {self.current_shape}\"\r\n                )\r\n                print(\"Warning: Latency might be affected due to change in input shape\")\r\n                self.interpreter.resize_tensor_input(\r\n                    self.inputs[0][\"index\"], list(mel.shape)\r\n                )\r\n                self.interpreter.allocate_tensors()\r\n                self.current_shape = mel.shape\r\n            self.interpreter.set_tensor(self.inputs[0][\"index\"], mel)\r\n            self.interpreter.invoke()\r\n            tflite_results = self.interpreter.get_tensor(self.outputs[0][\"index\"])\r\n            return tflite_results\r\n\r\n    mbmelganlite = MBMelganLite(path=\"mbm_quant.tflite\")\r\n\r\n    start = time.time()\r\n    audio_mblite = mbmelganlite.infer(mel_output_tflite)[0, :, 0]\r\n    print(f\"Time taken: {time.time() - start}s\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # Load the synthesizer and vocoder models, convert them to TFLite, and save to disk.\r\n    tacotron2 = load_tacotron2()\r\n    convert_tacotron2(tacotron2)\r\n    convert_vocoder()\r\n\r\n    # TFLite inference.\r\n    decoder_output_tflite, mel_output_tflite = infer_tacotron2()\r\n    print(\"Type of ``mel_output_tflite``:\", type(mel_output_tflite))\r\n    print(\"Value of ``mel_output_tflite``:\", mel_output_tflite)\r\n    infer_mbmelgan(mel_output_tflite)\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-09-28 11:49:35.986313: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-09-28 11:49:35.986361: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-09-28 11:49:39.385944: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-09-28 11:49:39.385986: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-09-28 11:49:39.386014: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host: /proc/driver/nvidia/version does not exist\r\n2020-09-28 11:49:39.386244: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-28 11:49:39.424376: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199955000 Hz\r\n2020-09-28 11:49:39.428501: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5610b8b69120 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-28 11:49:39.428546: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-28 11:49:47.039592: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-09-28 11:49:47.039824: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-09-28 11:49:47.201050: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-09-28 11:49:47.201094: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 882 nodes (146), 970 edges (166), time = 16.951ms.\r\n2020-09-28 11:49:47.201102: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 882 nodes (0), 970 edges (0), time = 15.414ms.\r\n2020-09-28 11:49:47.201108: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: decoder_while_cond_6632\r\n2020-09-28 11:49:47.201115: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-09-28 11:49:47.201120: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-09-28 11:49:47.201126: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: decoder_while_body_6633\r\n2020-09-28 11:49:47.201132: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-09-28 11:49:47.201138: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-09-28 11:49:47.201144: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_body_4839\r\n2020-09-28 11:49:47.201150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-09-28 11:49:47.201155: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-09-28 11:49:47.201161: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_cond_3160\r\n2020-09-28 11:49:47.201167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-09-28 11:49:47.201186: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-09-28 11:49:47.201193: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_cond_4838\r\n2020-09-28 11:49:47.201198: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-09-28 11:49:47.201204: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-09-28 11:49:47.201210: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: while_body_3161\r\n2020-09-28 11:49:47.201215: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-09-28 11:49:47.201221: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-09-28 11:49:50.042865: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-09-28 11:49:50.042923: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2020-09-28 11:49:51.573781: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor LocationSensitiveAttention/memory_layer/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.573831: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor LocationSensitiveAttention/memory_layer/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.573842: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor boolean_mask/Reshape because it has fewer than 1024 elements (1).\r\n2020-09-28 11:49:51.573852: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor residual_projection/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.573857: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor residual_projection/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.573863: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor boolean_mask_1/Reshape because it has fewer than 1024 elements (1).\r\n2020-09-28 11:49:51.683054: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg6 that is not type float.\r\n2020-09-28 11:49:51.683096: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor arg7 because it has fewer than 1024 elements (512).\r\n2020-09-28 11:49:51.686444: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg6 that is not type float.\r\n2020-09-28 11:49:51.686455: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor arg7 because it has fewer than 1024 elements (512).\r\n2020-09-28 11:49:51.689772: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor decoder/while/decoder_cell/location_conv/conv1d1 because it has fewer than 1024 elements (992).\r\n2020-09-28 11:49:51.689783: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/location_layer/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.689788: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/location_layer/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.689798: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/scores_attention/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.689803: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor decoder/while/decoder_cell/scores_attention/Tensordot/Shape that is not type float.\r\n2020-09-28 11:49:51.689819: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor decoder/while/decoder_cell/scores_attention/Tensordot/MatMul because it has fewer than 1024 elements (128).\r\n2020-09-28 11:49:54.531714: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-09-28 11:49:54.531884: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-09-28 11:49:54.613588: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\r\n2020-09-28 11:49:54.613620: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 1200 nodes (1114), 1264 edges (1178), time = 43.114ms.\r\n2020-09-28 11:49:54.613628: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.771ms.\r\n2020-09-28 11:49:56.158239: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-09-28 11:49:56.158292: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2020-09-28 11:49:56.439344: I tensorflow/lite/tools/optimize/quantize_weights.cc:219] Skipping quantization of tensor conv1d;PartitionedCall/conv1d1 because it has fewer than 1024 elements (252).\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 218 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 31 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 31 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 1 nodes delegated out of 4 nodes with 1 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 139 nodes with 3 partitions.\r\n\r\n/home/brendan/conda/envs/py37/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:44: UserWarning: You are currently using a nightly version of TensorFlow (2.4.0-dev20200630). \r\nTensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \r\nIf you encounter a bug, do not file an issue on GitHub.\r\n  UserWarning,\r\nModel size is 32.616089 MBs.\r\nModel: \"mb_melgan\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nsequential (Sequential)      (None, None, 4)           2534356   \r\n_________________________________________________________________\r\npqmf (TFPQMF)                multiple                  0         \r\n=================================================================\r\nTotal params: 2,534,356\r\nTrainable params: 2,534,356\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nModel saved to mbm_quant.tflite file\r\nModel size is 6.481171 MBs.\r\nType of seq_symbols: <class 'str'>\r\nValue of seq_symbols: recent research at harvard has shown meditating for as little as eight weeks, can actually increase the grey matter in the parts of the brain responsible for emotional regulation, and learning.\r\nInput tensor 0 detail: {'name': 'input_ids', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\nShape of tensor 0: (1, 194)\r\nInput tensor 1 detail: {'name': 'input_lengths', 'index': 1, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\nShape of tensor 1: (1,)\r\nInput tensor 2 detail: {'name': 'speaker_ids', 'index': 2, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\nShape of tensor 2: (1,)\r\nType of ``mel_output_tflite``: <class 'numpy.ndarray'>\r\nValue of ``mel_output_tflite``: [[[-1.1824543  -0.73486066 -0.6587808  ... -1.4803959  -1.6538712\r\n   -1.7523229 ]\r\n  [-0.61233926 -0.1465351   0.26401407 ... -1.4384888  -1.5623676\r\n   -1.6496788 ]\r\n  [ 0.00431968  0.206805    0.68081516 ... -1.3629606  -1.5245864\r\n   -1.6202413 ]\r\n  ...\r\n  [ 0.21240304 -0.1798481   1.0284772  ... -0.42555726 -0.47812086\r\n   -0.3530782 ]\r\n  [ 0.179356   -0.16748641  1.031336   ... -0.7199661  -0.76034176\r\n   -0.6377524 ]\r\n  [ 0.21463726 -0.1416688   0.9862344  ... -0.8474455  -0.8514927\r\n   -0.7260057 ]]]\r\nInput shape: (1, 1000, 80) , interpreter shape: [ 1  1 80]\r\nWarning: Latency might be affected due to change in input shape\r\nTime taken: 46.32718467712402s\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n[tacotron2.tflite.zip](https://github.com/tensorflow/tensorflow/files/5293557/tacotron2.tflite.zip)\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\nThe conversion is successful, but running inference in C++ is failing. I suspect it is because I am loading the input tensors in incorrectly. The code and output is below:\r\n```\r\n/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n#include <cstdio>\r\n#include <bits/stdc++.h>\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/optional_debug_tools.h\"\r\n//#include \"ljspeech_functional.cc\"\r\n\r\n// BEGIN LJSPEECH_FUNCTIONAL CODE\r\n\r\n\r\n\r\nstd::map<std::string, int> symbol_to_id = {\r\n    {\"pad\", 0},\r\n\t{\"-\", 1},\r\n\t{\"!\", 2},\r\n\t{\"'\", 3},\r\n\t{\"(\", 4},\r\n\t{\")\", 5},\r\n\t{\"\", 6},\r\n\t{\".\", 7},\r\n\t{\":\", 8},\r\n\t{\";\", 9},\r\n\t{\"?\", 10},\r\n\t{\" \", 11},\r\n\t{\"A\", 12},\r\n\t{\"B\", 13},\r\n\t{\"C\", 14},\r\n\t{\"D\", 15},\r\n\t{\"E\", 16},\r\n\t{\"F\", 17},\r\n\t{\"G\", 18},\r\n\t{\"H\", 19},\r\n\t{\"I\", 20},\r\n\t{\"J\", 21},\r\n\t{\"K\", 22},\r\n\t{\"L\", 23},\r\n\t{\"M\", 24},\r\n\t{\"N\", 25},\r\n\t{\"O\", 26},\r\n\t{\"P\", 27},\r\n\t{\"Q\", 28},\r\n\t{\"R\", 29},\r\n\t{\"S\", 30},\r\n\t{\"T\", 31},\r\n\t{\"U\", 32},\r\n\t{\"V\", 33},\r\n\t{\"W\", 34},\r\n\t{\"X\", 35},\r\n\t{\"Y\", 36},\r\n\t{\"Z\", 37},\r\n\t{\"a\", 38},\r\n\t{\"b\", 39},\r\n\t{\"c\", 40},\r\n\t{\"d\", 41},\r\n\t{\"e\", 42},\r\n\t{\"f\", 43},\r\n\t{\"g\", 44},\r\n\t{\"h\", 45},\r\n\t{\"i\", 46},\r\n\t{\"j\", 47},\r\n\t{\"k\", 48},\r\n\t{\"l\", 49},\r\n\t{\"m\", 50},\r\n\t{\"n\", 51},\r\n\t{\"o\", 52},\r\n\t{\"p\", 53},\r\n\t{\"q\", 54},\r\n\t{\"r\", 55},\r\n\t{\"s\", 56},\r\n\t{\"t\", 57},\r\n\t{\"u\", 58},\r\n\t{\"v\", 59},\r\n\t{\"w\", 60},\r\n\t{\"x\", 61},\r\n\t{\"y\", 62},\r\n\t{\"z\", 63},\r\n\t{\"@AA\", 64},\r\n\t{\"@AA0\", 65},\r\n\t{\"@AA1\", 66},\r\n\t{\"@AA2\", 67},\r\n\t{\"@AE\", 68},\r\n\t{\"@AE0\", 69},\r\n\t{\"@AE1\", 70},\r\n\t{\"@AE2\", 71},\r\n\t{\"@AH\", 72},\r\n\t{\"@AH0\", 73},\r\n\t{\"@AH1\", 74},\r\n\t{\"@AH2\", 75},\r\n\t{\"@AO\", 76},\r\n\t{\"@AO0\", 77},\r\n\t{\"@AO1\", 78},\r\n\t{\"@AO2\", 79},\r\n\t{\"@AW\", 80},\r\n\t{\"@AW0\", 81},\r\n\t{\"@AW1\", 82},\r\n\t{\"@AW2\", 83},\r\n\t{\"@AY\", 84},\r\n\t{\"@AY0\", 85},\r\n\t{\"@AY1\", 86},\r\n\t{\"@AY2\", 87},\r\n\t{\"@B\", 88},\r\n\t{\"@CH\", 89},\r\n\t{\"@D\", 90},\r\n\t{\"@DH\", 91},\r\n\t{\"@EH\", 92},\r\n\t{\"@EH0\", 93},\r\n\t{\"@EH1\", 94},\r\n\t{\"@EH2\", 95},\r\n\t{\"@ER\", 96},\r\n\t{\"@ER0\", 97},\r\n\t{\"@ER1\", 98},\r\n\t{\"@ER2\", 99},\r\n\t{\"@EY\", 100},\r\n\t{\"@EY0\", 101},\r\n\t{\"@EY1\", 102},\r\n\t{\"@EY2\", 103},\r\n\t{\"@F\", 104},\r\n\t{\"@G\", 105},\r\n\t{\"@HH\", 106},\r\n\t{\"@IH\", 107},\r\n\t{\"@IH0\", 108},\r\n\t{\"@IH1\", 109},\r\n\t{\"@IH2\", 110},\r\n\t{\"@IY\", 111},\r\n\t{\"@IY0\", 112},\r\n\t{\"@IY1\", 113},\r\n\t{\"@IY2\", 114},\r\n\t{\"@JH\", 115},\r\n\t{\"@K\", 116},\r\n\t{\"@L\", 117},\r\n\t{\"@M\", 118},\r\n\t{\"@N\", 119},\r\n\t{\"@NG\", 120},\r\n\t{\"@OW\", 121},\r\n\t{\"@OW0\", 122},\r\n\t{\"@OW1\", 123},\r\n\t{\"@OW2\", 124},\r\n\t{\"@OY\", 125},\r\n\t{\"@OY0\", 126},\r\n\t{\"@OY1\", 127},\r\n\t{\"@OY2\", 128},\r\n\t{\"@P\", 129},\r\n\t{\"@R\", 130},\r\n\t{\"@S\", 131},\r\n\t{\"@SH\", 132},\r\n\t{\"@T\", 133},\r\n\t{\"@TH\", 134},\r\n\t{\"@UH\", 135},\r\n\t{\"@UH0\", 136},\r\n\t{\"@UH1\", 137},\r\n\t{\"@UH2\", 138},\r\n\t{\"@UW\", 139},\r\n\t{\"@UW0\", 140},\r\n\t{\"@UW1\", 141},\r\n\t{\"@UW2\", 142},\r\n\t{\"@V\", 143},\r\n\t{\"@W\", 144},\r\n\t{\"@Y\", 145},\r\n\t{\"@Z\", 146},\r\n\t{\"@ZH\", 147},\r\n\t{\"eos\", 148},\r\n};\r\n\r\nstd::string _eos = \"eos\";\r\nstd::string cleaner_names = \"english_cleaners\";\r\nint eos_id = 148;\r\n\r\n/* Returns lowercase version of input string.*/\r\nstd::string lowercase(std::string text) {\r\n    std::transform(text.begin(), text.end(), text.begin(),\r\n                   [](unsigned char c){return std::tolower(c);});\r\n    return text;\r\n}\r\n\r\n/* Removes leading, trailing and extra consecutive spaces from text.*/\r\nvoid collapse_whitespace(std::string &text) {\r\n    int n = text.length();\r\n    bool space = false;\r\n    int k = 0;\r\n    for (int i = 0; i < n; ++i) {\r\n        // Handle leading spaces\r\n        while (k == 0 && i < n && text[i] == ' ') {\r\n            ++i;\r\n        }\r\n        // Handle all other spaces (remove > 1 consecutive)\r\n        if (text[i] == ' ') {\r\n            if (!space) {\r\n                text[k++] = text[i];\r\n                space = true;\r\n            }\r\n        }\r\n        else if (ispunct(text[i])) {\r\n            if (k > 0 && text[k - 1] == ' ') {\r\n                text[k - 1] = text[i];\r\n            }\r\n            else {\r\n                text[k++] = text[i];\r\n            }\r\n            space = false;\r\n        }\r\n        else {\r\n            text[k++] = text[i];\r\n            space = false;\r\n        }\r\n    }\r\n    // Handle trailing spaces\r\n    text.erase(text.begin() + k - 1, text.end());\r\n}\r\n\r\n/* Returns input string lowercase with extra spaces collapsed.*/\r\nstd::string english_cleaners(std::string &text) {\r\n    text = lowercase(text);\r\n    collapse_whitespace(text);\r\n    return text;\r\n}\r\n\r\n/* Returns true if symbol is valid and should be kept in text.*/\r\nbool _should_keep_symbol(std::string s) {\r\n    bool in_dict = symbol_to_id.find(s) != symbol_to_id.end();\r\n    return in_dict && s != \"_\" && s != \"~\";\r\n}\r\n\r\n/* Caller must free returned vector.*/\r\nstd::vector<int>* _symbols_to_sequence(std::string symbols) {\r\n    std::vector<int>* sequence = new std::vector<int>;\r\n    int n = symbols.length();\r\n    for (int i = 0; i < n; ++i) {\r\n\r\n        // Cast to std::string.\r\n        std::string symbol(1, symbols[i]);\r\n\r\n        if (_should_keep_symbol(symbol)) {\r\n            sequence->push_back(symbol_to_id[symbol]);\r\n        }\r\n    }\r\n    return sequence;\r\n}\r\n\r\n/* Caller must free returned vector.*/\r\n\r\n/*\r\nstd::vector<int>* _arpabet_to_sequence(std::string text) {\r\n    // Build symbols vector from text\r\n    std::vector<char> v(text.begin(), text.end());\r\n    std::vector<std::string> symbols;\r\n    int n = v.size();\r\n    for (int i = 0; i < n; ++i) {\r\n        symbols.push_back(\"@\" + v[i]);\r\n    }\r\n\r\n    // Get sequence vector from symbols vector\r\n    return _symbols_to_sequence(symbols);\r\n}\r\n*/\r\n\r\n/* Translates raw text into token IDs.*/\r\nstd::vector<int>* text_to_sequence(std::string text) {\r\n    std::string clean_text = english_cleaners(text);\r\n    std::vector<int>* sequence = _symbols_to_sequence(clean_text);\r\n    sequence->push_back(eos_id);\r\n    return sequence;\r\n}\r\n\r\n// END LJSPEECH_FUNCTIONAL.CC CODE\r\n\r\n// This is an example that is minimal to read a model\r\n// from disk and perform inference. There is no data being loaded\r\n// that is up to you to add as a user.\r\n//\r\n// NOTE: Do not add any dependencies to this that cannot be built with\r\n// the minimal makefile. This example must remain trivial to build with\r\n// the minimal build tool.\r\n//\r\n// Usage: minimal <tflite model>\r\n\r\n#define TFLITE_MINIMAL_CHECK(x)                              \\\r\n  if (!(x)) {                                                \\\r\n    fprintf(stderr, \"Error at %s:%d\\n\", __FILE__, __LINE__); \\\r\n    exit(1);                                                 \\\r\n  }\r\n\r\n\r\nint main(int argc, char* argv[]) {\r\n  if (argc != 2) {\r\n    fprintf(stderr, \"minimal <tflite model>\\n\");\r\n    return 1;\r\n  }\r\n  const char* filename = argv[1];\r\n\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter with the InterpreterBuilder.\r\n  // Note: all Interpreters should be built with the InterpreterBuilder,\r\n  // which allocates memory for the Intrepter and does various set up\r\n  // tasks so that the Interpreter can read the provided model.\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  tflite::InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<tflite::Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Define test input string.\r\n  std::string in = \"hello.\";\r\n\r\n  // Convert to input ids.\r\n  std::vector<int>* input_ids = text_to_sequence(in);\r\n  int n = input_ids->size();\r\n\r\n  // Fill input buffers\r\n  // TODO(user): Insert code to fill input tensors.\r\n  // Note: The buffer of the input tensor with index `i` of type T can\r\n  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`\r\n  int* input = interpreter->typed_input_tensor<int>(0);\r\n\r\n  // Resize input tensors.\r\n  int t0 = interpreter->inputs()[0];\r\n  int t1 = interpreter->inputs()[1];\r\n  int t2 = interpreter->inputs()[2];\r\n  interpreter->ResizeInputTensor(t0, {1, n});\r\n  interpreter->ResizeInputTensor(t1, {1});\r\n  interpreter->ResizeInputTensor(t2, {1});\r\n\r\n  // Allocate tensors again?\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n\r\n  for (int i = 0; i < n; i++) {\r\n    input[i] = input_ids->data()[i];\r\n  }\r\n\r\n  // Run inference - uncomment invoke when input filled\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n  printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n\r\n  // Read output buffers\r\n  // TODO(user): Insert getting data out code.\r\n  // Note: The buffer of the output tensor with index `i` of type T can\r\n  // be accessed with `T* output = interpreter->typed_output_tensor<T>(i);`\r\n  float* output = interpreter->typed_output_tensor<float>(0);\r\n  return 0;\r\n}\r\n```\r\n\r\n```\r\n=== Pre-invoke Interpreter State ===\r\nInterpreter has 445 tensors and 218 nodes\r\nInputs: 0 1 2\r\nOutputs: 297 352 353\r\n\r\nTensor   0 input_ids            kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor   1 input_lengths        kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor   2 speaker_ids          kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor   3 Const_2              kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor   4 LocationSensitiveAttention/memory_layer/Tensordot/Const_2 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor   5 boolean_mask_1/Reshape_1/shape kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor   6 decoder/Tile/input   kTfLiteBool   kTfLiteMmapRo          1 bytes ( 0.0 MB)  1\r\nTensor   7 decoder/Tile_1/input kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1 1\r\nTensor   8 decoder/Tile_1/multiples/1 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor   9 decoder/maximum_iterations kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  10 decoder/LessEqual    kTfLiteBool   kTfLiteMmapRo          1 bytes ( 0.0 MB) \r\nTensor  11 decoder/while/input_18 kTfLiteFloat32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256\r\nTensor  12 decoder/while/input_20 kTfLiteFloat32   kTfLiteMmapRo       1024 bytes ( 0.0 MB)  256\r\nTensor  13 decoder/while/input_23 kTfLiteFloat32   kTfLiteMmapRo      16384 bytes ( 0.0 MB)  4096\r\nTensor  14 decoder/while/input_29 kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  15 decoder/while/input_37 kTfLiteFloat32   kTfLiteMmapRo      16384 bytes ( 0.0 MB)  4096\r\nTensor  16 decoder/while/input_39 kTfLiteFloat32   kTfLiteMmapRo        320 bytes ( 0.0 MB)  80\r\nTensor  17 decoder/while/input_41 kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  18 encoder/bilstm/backward_lstm/Read_2/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024\r\nTensor  19 encoder/bilstm/concat/axis kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  20 transpose_2/perm;encoder/bilstm/forward_lstm/PartitionedCall/transpose_2/perm kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor  21 encoder/bilstm/forward_lstm/Read_2/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       4096 bytes ( 0.0 MB)  1024\r\nTensor  22 encoder/bilstm/forward_lstm/zeros_1/packed/1 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  23 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  24 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  25 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  26 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  27 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  28 encoder/embeddings/Gather/resource kTfLiteInt8   kTfLiteMmapRo      76288 bytes ( 0.1 MB)  149 512\r\nTensor  29 encoder/embeddings/LayerNorm/batchnorm/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  30 encoder/embeddings/LayerNorm/batchnorm/add/y kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  31 encoder/embeddings/LayerNorm/batchnorm/mul/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  32 input_sequence_masks/ExpandDims/dim kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  33 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  34 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  35 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  36 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  37 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  38 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d/ExpandDims/dim kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  39 range/delta          kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  40 residual_projection/BiasAdd/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo        320 bytes ( 0.0 MB)  80\r\nTensor  41 residual_projection/Tensordot/Const_2 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  42 residual_projection/Tensordot/free kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2\r\nTensor  43 zeros_3/packed/1     kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  44 zeros_4/packed/1     kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  45 decoder/TensorArrayV2 kTfLiteFloat32   kTfLiteMmapRo     320000 bytes ( 0.3 MB)  1000 80\r\nTensor  46 decoder/TensorArrayV2_1 kTfLiteFloat32   kTfLiteMmapRo       4000 bytes ( 0.0 MB)  1000 1\r\nTensor  47 decoder/TensorArrayV2_2 kTfLiteInt32   kTfLiteMmapRo       4000 bytes ( 0.0 MB)  1000 1\r\nTensor  48 TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2\r\nTensor  49 TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_11 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  50 TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_12 kTfLiteFloat32   kTfLiteMmapRo          4 bytes ( 0.0 MB) \r\nTensor  51 LocationSensitiveAttention/memory_layer/Tensordot/MatMul kTfLiteInt8   kTfLiteMmapRo      65536 bytes ( 0.1 MB)  128 512\r\nTensor  52 MatMul;encoder/bilstm/backward_lstm/PartitionedCall/MatMul kTfLiteInt8   kTfLiteMmapRo     524288 bytes ( 0.5 MB)  1024 512\r\nTensor  53 MatMul_1;encoder/bilstm/backward_lstm/PartitionedCall/MatMul_1 kTfLiteInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  1024 256\r\nTensor  54 MatMul;encoder/bilstm/forward_lstm/PartitionedCall/MatMul kTfLiteInt8   kTfLiteMmapRo     524288 bytes ( 0.5 MB)  1024 512\r\nTensor  55 MatMul_1;encoder/bilstm/forward_lstm/PartitionedCall/MatMul_1 kTfLiteInt8   kTfLiteMmapRo     262144 bytes ( 0.2 MB)  1024 256\r\nTensor  56 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  57 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  58 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  59 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  60 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  61 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  62 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  63 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  64 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  65 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  66 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  67 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  68 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  69 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  70 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  71 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  72 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  73 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  74 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/mul kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  75 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/sub kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  76 residual_projection/Tensordot/MatMul kTfLiteInt8   kTfLiteMmapRo      40960 bytes ( 0.0 MB)  80 512\r\nTensor  77 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  78 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  79 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  80 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  81 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  82 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d kTfLiteInt8   kTfLiteMmapRo     204800 bytes ( 0.2 MB)  512 1 5 80\r\nTensor  83 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  84 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  85 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  86 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d kTfLiteFloat32   kTfLiteMmapRo       2048 bytes ( 0.0 MB)  512\r\nTensor  87 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d1 kTfLiteInt8   kTfLiteMmapRo    1310720 bytes ( 1.2 MB)  512 1 5 512\r\nTensor  88 strided_slice_1;encoder/bilstm/forward_lstm/PartitionedCall/strided_slice_1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor  89 strided_slice_1;encoder/bilstm/forward_lstm/PartitionedCall/strided_slice_11 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor  90 strided_slice_1;encoder/bilstm/forward_lstm/PartitionedCall/strided_slice_12 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor  91 LocationSensitiveAttention/SequenceMask/ExpandDims kTfLiteInt32   kTfLiteMmapRo          8 bytes ( 0.0 MB)  2\r\nTensor  92 boolean_mask_1/strided_slice_2 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  93 boolean_mask_1/strided_slice_21 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  94 boolean_mask_1/strided_slice_22 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor  95 encoder/embeddings/Gather;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 512\r\nTensor  96 encoder/embeddings/LayerNorm/moments/mean kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor  97 encoder/embeddings/LayerNorm/moments/SquaredDifference kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor  98 encoder/embeddings/LayerNorm/moments/variance kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor  99 encoder/embeddings/LayerNorm/batchnorm/add kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 100 encoder/embeddings/LayerNorm/batchnorm/Rsqrt kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 101 encoder/embeddings/LayerNorm/batchnorm/mul kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 102 encoder/embeddings/LayerNorm/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 103 encoder/embeddings/LayerNorm/batchnorm/mul_2 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 104 encoder/embeddings/LayerNorm/batchnorm/sub kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 105 encoder/embeddings/LayerNorm/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 106 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 107 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 108 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 109 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/conv_._0/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 110 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 111 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm/batch_norm_._0/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 112 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 113 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 114 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 115 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/conv_._1/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 116 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 117 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_1/batch_norm_._1/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 118 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 119 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 120 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 121 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/conv_._2/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 122 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 123 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_2/batch_norm_._2/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 124 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 125 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 126 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 127 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/conv_._3/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 128 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 129 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_3/batch_norm_._3/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 130 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 131 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 132 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 133 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/conv_._4/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 134 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 135 encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/Relu;encoder/conv_batch_norm/tf_tacotron_conv_batch_norm_4/batch_norm_._4/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 136 transpose;encoder/bilstm/backward_lstm/PartitionedCall/transpose kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 137 ReverseV2;encoder/bilstm/backward_lstm/PartitionedCall/ReverseV2 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 138 Shape;encoder/bilstm/backward_lstm/PartitionedCall/Shape kTfLiteInt32 kTfLitePersistentRo         12 bytes ( 0.0 MB)  3\r\nTensor 139 strided_slice;encoder/bilstm/backward_lstm/PartitionedCall/strided_slice kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 140 TensorArrayV2_1;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 141 TensorArrayV2_1;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2_11 kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 142 TensorArrayV2_1;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2_12 kTfLiteFloat32  kTfLiteDynamic          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 143 strided_slice_1;encoder/bilstm/backward_lstm/PartitionedCall/strided_slice_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 512\r\nTensor 144 MatMul;encoder/bilstm/backward_lstm/PartitionedCall/MatMul1 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 145 MatMul;encoder/bilstm/forward_lstm/PartitionedCall/MatMul1 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 146 encoder/bilstm/backward_lstm/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 147 encoder/bilstm/backward_lstm/strided_slice kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 148 encoder/bilstm/backward_lstm/zeros/packed kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 149 encoder/bilstm/backward_lstm/zeros kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 150 MatMul_1;encoder/bilstm/backward_lstm/PartitionedCall/MatMul_11 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 151 add;encoder/bilstm/backward_lstm/PartitionedCall/add kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 152 BiasAdd;encoder/bilstm/backward_lstm/PartitionedCall/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 153 split;encoder/bilstm/backward_lstm/PartitionedCall/split kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 154 split;encoder/bilstm/backward_lstm/PartitionedCall/split1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 155 split;encoder/bilstm/backward_lstm/PartitionedCall/split2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 156 split;encoder/bilstm/backward_lstm/PartitionedCall/split3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 157 Sigmoid;encoder/bilstm/backward_lstm/PartitionedCall/Sigmoid kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 158 Sigmoid_1;encoder/bilstm/backward_lstm/PartitionedCall/Sigmoid_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 159 Sigmoid_2;encoder/bilstm/backward_lstm/PartitionedCall/Sigmoid_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 160 Tanh;encoder/bilstm/backward_lstm/PartitionedCall/Tanh kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 161 mul_1;encoder/bilstm/backward_lstm/PartitionedCall/mul_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 162 mul;encoder/bilstm/backward_lstm/PartitionedCall/mul kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 163 add_1;encoder/bilstm/backward_lstm/PartitionedCall/add_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 164 Tanh_1;encoder/bilstm/backward_lstm/PartitionedCall/Tanh_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 165 mul_2;encoder/bilstm/backward_lstm/PartitionedCall/mul_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 166 zeros_like;encoder/bilstm/backward_lstm/PartitionedCall/zeros_like kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 167 MatMul_1;encoder/bilstm/forward_lstm/PartitionedCall/MatMul_11 kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 168 add;encoder/bilstm/forward_lstm/PartitionedCall/add kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 169 BiasAdd;encoder/bilstm/forward_lstm/PartitionedCall/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 170 split;encoder/bilstm/forward_lstm/PartitionedCall/split kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 171 split;encoder/bilstm/forward_lstm/PartitionedCall/split1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 172 split;encoder/bilstm/forward_lstm/PartitionedCall/split2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 173 split;encoder/bilstm/forward_lstm/PartitionedCall/split3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 174 Sigmoid;encoder/bilstm/forward_lstm/PartitionedCall/Sigmoid kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 175 Sigmoid_1;encoder/bilstm/forward_lstm/PartitionedCall/Sigmoid_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 176 Sigmoid_2;encoder/bilstm/forward_lstm/PartitionedCall/Sigmoid_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 177 Tanh;encoder/bilstm/forward_lstm/PartitionedCall/Tanh kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 178 mul_1;encoder/bilstm/forward_lstm/PartitionedCall/mul_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 179 mul;encoder/bilstm/forward_lstm/PartitionedCall/mul kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 180 add_1;encoder/bilstm/forward_lstm/PartitionedCall/add_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 181 Tanh_1;encoder/bilstm/forward_lstm/PartitionedCall/Tanh_1 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 182 mul_2;encoder/bilstm/forward_lstm/PartitionedCall/mul_2 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 183 zeros_like;encoder/bilstm/forward_lstm/PartitionedCall/zeros_like kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 184 LocationSensitiveAttention/SequenceMask/ExpandDims1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 185 Max                  kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 186 input_sequence_masks/Range kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 187 input_sequence_masks/Less kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1\r\nTensor 188 ExpandDims;encoder/bilstm/backward_lstm/PartitionedCall/ExpandDims kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1\r\nTensor 189 transpose_1;encoder/bilstm/backward_lstm/PartitionedCall/transpose_1 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1\r\nTensor 190 ReverseV2_1;encoder/bilstm/backward_lstm/PartitionedCall/ReverseV2_1 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1\r\nTensor 191 while;encoder/bilstm/backward_lstm/PartitionedCall/while kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 192 while;encoder/bilstm/backward_lstm/PartitionedCall/while1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 193 while;encoder/bilstm/backward_lstm/PartitionedCall/while2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 194 while;encoder/bilstm/backward_lstm/PartitionedCall/while3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 195 while;encoder/bilstm/backward_lstm/PartitionedCall/while4 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 196 while;encoder/bilstm/backward_lstm/PartitionedCall/while5 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 197 while;encoder/bilstm/backward_lstm/PartitionedCall/while6 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1\r\nTensor 198 while;encoder/bilstm/backward_lstm/PartitionedCall/while7 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 199 while;encoder/bilstm/backward_lstm/PartitionedCall/while8 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 200 TensorArrayV2Stack/TensorListStack;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 201 TensorArrayV2Stack/TensorListStack;encoder/bilstm/backward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 202 transpose_2;encoder/bilstm/backward_lstm/PartitionedCall/transpose_2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 203 encoder/bilstm/ReverseV2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 204 while;encoder/bilstm/forward_lstm/PartitionedCall/while kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 205 while;encoder/bilstm/forward_lstm/PartitionedCall/while1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 206 while;encoder/bilstm/forward_lstm/PartitionedCall/while2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 207 while;encoder/bilstm/forward_lstm/PartitionedCall/while3 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 208 while;encoder/bilstm/forward_lstm/PartitionedCall/while4 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 209 while;encoder/bilstm/forward_lstm/PartitionedCall/while5 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 210 while;encoder/bilstm/forward_lstm/PartitionedCall/while6 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1 1\r\nTensor 211 while;encoder/bilstm/forward_lstm/PartitionedCall/while7 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 212 while;encoder/bilstm/forward_lstm/PartitionedCall/while8 kTfLiteFloat32  kTfLiteArenaRw       1024 bytes ( 0.0 MB)  1 256\r\nTensor 213 TensorArrayV2Stack/TensorListStack;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 214 TensorArrayV2Stack/TensorListStack;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2Stack/TensorListStack1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 215 transpose_2;encoder/bilstm/forward_lstm/PartitionedCall/transpose_2 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 216 encoder/bilstm/concat kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 217 LocationSensitiveAttention/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 218 LocationSensitiveAttention/strided_slice kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 219 LocationSensitiveAttention/SequenceMask/Range kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 220 LocationSensitiveAttention/SequenceMask/Less kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1\r\nTensor 221 LocationSensitiveAttention/SequenceMask/Cast_1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 222 LocationSensitiveAttention/Shape_1 kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 223 LocationSensitiveAttention/concat kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 224 LocationSensitiveAttention/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 225 LocationSensitiveAttention/mul kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 226 LocationSensitiveAttention/memory_layer/Tensordot/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 227 LocationSensitiveAttention/memory_layer/Tensordot/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 228 LocationSensitiveAttention/memory_layer/Tensordot/Prod kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 229 LocationSensitiveAttention/memory_layer/Tensordot/concat_1 kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 230 LocationSensitiveAttention/memory_layer/Tensordot/GatherV2_1;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 231 LocationSensitiveAttention/memory_layer/Tensordot/Prod_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 232 LocationSensitiveAttention/memory_layer/Tensordot/stack kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 233 LocationSensitiveAttention/memory_layer/Tensordot/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 234 LocationSensitiveAttention/memory_layer/Tensordot/MatMul1 kTfLiteFloat32  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 128\r\nTensor 235 LocationSensitiveAttention/memory_layer/Tensordot kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 236 Shape_2              kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 237 strided_slice        kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 238 Reshape/shape        kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 239 Reshape_1/shape      kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 240 decoder/Tile/multiples kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 241 decoder/Tile         kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1\r\nTensor 242 decoder/LogicalOr    kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1\r\nTensor 243 decoder/zeros_like/Shape kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 244 decoder/zeros_like   kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 245 decoder/Tile_1/multiples kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 246 decoder/Tile_1       kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 80\r\nTensor 247 zeros_5/packed       kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 248 strided_slice_2      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 249 range                kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 250 ExpandDims           kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 251 strided_slice_3      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 252 Tile/multiples       kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 253 Tile                 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 254 zeros/packed         kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 255 zeros                kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 256 zeros_4/packed       kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 257 zeros_4              kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 512\r\nTensor 258 zeros_5              kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 259 zeros_7              kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 260 decoder/while        kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 261 decoder/while1       kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 262 decoder/while2       kTfLiteFloat32  kTfLiteArenaRw     320000 bytes ( 0.3 MB)  1000 80\r\nTensor 263 decoder/while3       kTfLiteFloat32  kTfLiteArenaRw       4000 bytes ( 0.0 MB)  1000 1\r\nTensor 264 decoder/while4       kTfLiteInt32  kTfLiteArenaRw       4000 bytes ( 0.0 MB)  1000 1\r\nTensor 265 decoder/while5       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 266 decoder/while6       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 267 decoder/while7       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 268 decoder/while8       kTfLiteFloat32  kTfLiteArenaRw       4096 bytes ( 0.0 MB)  1 1024\r\nTensor 269 decoder/while9       kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 512\r\nTensor 270 decoder/while10      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 271 decoder/while11      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 272 decoder/while12      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 273 decoder/while13      kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 80\r\nTensor 274 decoder/while14      kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1\r\nTensor 275 decoder/while15      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 276 decoder/while16      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 277 decoder/while17      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 278 decoder/while18      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 279 decoder/while19      kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 280 decoder/while20      kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 281 Reshape              kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80\r\nTensor 282 Abs                  kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80\r\nTensor 283 Sum                  kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 284 Cast                 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 285 NotEqual             kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1 1\r\nTensor 286 boolean_mask/Reshape_1 kTfLiteBool  kTfLiteArenaRw          1 bytes ( 0.0 MB)  1\r\nTensor 287 boolean_mask/Where   kTfLiteInt64  kTfLiteArenaRw          8 bytes ( 0.0 MB)  1 1\r\nTensor 288 boolean_mask/Squeeze kTfLiteInt64  kTfLiteArenaRw          8 bytes ( 0.0 MB)  1\r\nTensor 289 boolean_mask/Shape   kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 290 boolean_mask/strided_slice kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 291 boolean_mask/Prod    kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 292 boolean_mask/concat/values_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 293 boolean_mask/strided_slice_2 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 294 boolean_mask/concat  kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 295 boolean_mask/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 296 boolean_mask/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 297 Identity             kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 298 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 1 80\r\nTensor 299 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 300 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 301 post_net/tf_tacotron_conv_batch_norm_5/conv_._0/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 302 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 303 post_net/tf_tacotron_conv_batch_norm_5/batch_norm_._0/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 304 post_net/tf_tacotron_conv_batch_norm_5/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 305 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 306 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 307 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 308 post_net/tf_tacotron_conv_batch_norm_6/conv_._1/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 309 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 310 post_net/tf_tacotron_conv_batch_norm_6/batch_norm_._1/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 311 post_net/tf_tacotron_conv_batch_norm_6/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 312 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 313 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 314 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 315 post_net/tf_tacotron_conv_batch_norm_7/conv_._2/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 316 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 317 post_net/tf_tacotron_conv_batch_norm_7/batch_norm_._2/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 318 post_net/tf_tacotron_conv_batch_norm_7/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 319 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 320 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 321 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 322 post_net/tf_tacotron_conv_batch_norm_8/conv_._3/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 323 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 324 post_net/tf_tacotron_conv_batch_norm_8/batch_norm_._3/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 325 post_net/tf_tacotron_conv_batch_norm_8/activation_7/Tanh kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 326 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d/ExpandDims kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 327 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d2 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 328 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/conv1d/Squeeze kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 329 post_net/tf_tacotron_conv_batch_norm_9/conv_._4/BiasAdd kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 330 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/mul_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 331 post_net/tf_tacotron_conv_batch_norm_9/batch_norm_._4/batchnorm/add_1 kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 332 residual_projection/Tensordot/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 333 residual_projection/Tensordot/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 334 residual_projection/Tensordot/Prod kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 335 residual_projection/Tensordot/concat_1 kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 336 residual_projection/Tensordot/GatherV2_1;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 337 residual_projection/Tensordot/Prod_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 338 residual_projection/Tensordot/stack kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 339 residual_projection/Tensordot/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 340 residual_projection/Tensordot/MatMul2 kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 80\r\nTensor 341 residual_projection/Tensordot kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 342 residual_projection/BiasAdd kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80\r\nTensor 343 add                  kTfLiteFloat32  kTfLiteArenaRw        320 bytes ( 0.0 MB)  1 1 80\r\nTensor 344 boolean_mask_1/Shape kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 345 boolean_mask_1/strided_slice kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 346 boolean_mask_1/Prod  kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB) \r\nTensor 347 boolean_mask_1/concat/values_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 348 boolean_mask_1/strided_slice_23 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 349 boolean_mask_1/concat kTfLiteInt32  kTfLiteArenaRw          8 bytes ( 0.0 MB)  2\r\nTensor 350 boolean_mask_1/Reshape kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 351 boolean_mask_1/GatherV2;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1 kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 352 Identity_1           kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1 1\r\nTensor 353 Identity_2           kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1 1\r\nTensor 354 encoder/embeddings/Gather;TensorArrayV2_1;encoder/bilstm/forward_lstm/PartitionedCall/TensorArrayV2_1_dequantize kTfLiteFloat32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  1 1 512\r\nTensor 355 (null)               kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 356 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 357 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 358 (null)               kTfLiteInt32  kTfLiteArenaRw         12 bytes ( 0.0 MB)  3\r\nTensor 359 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 360 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 361 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 362 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 363 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 364 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 365 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 366 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 367 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 368 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 369 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 370 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 371 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 372 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 373 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 374 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 375 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 376 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 377 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 378 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 379 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 380 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 381 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 382 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 383 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 384 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 385 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 386 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 387 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 388 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 389 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 390 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 391 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 392 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 393 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 394 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 395 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 396 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 397 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 398 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 399 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 400 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 401 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 402 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 403 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 404 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 405 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 406 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 407 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 408 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 409 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 410 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 411 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 412 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 413 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 414 (null)               kTfLiteNoType  kTfLiteMemNone          0 bytes ( 0.0 MB)  (null)\r\nTensor 415 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560\r\nTensor 416 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 417 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 418 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1\r\nTensor 419 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 420 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512\r\nTensor 421 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560\r\nTensor 422 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 423 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 424 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1\r\nTensor 425 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 426 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512\r\nTensor 427 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560\r\nTensor 428 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 429 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 430 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1\r\nTensor 431 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 432 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512\r\nTensor 433 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560\r\nTensor 434 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 435 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 436 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1\r\nTensor 437 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 438 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512\r\nTensor 439 (null)               kTfLiteInt8  kTfLiteArenaRw       2560 bytes ( 0.0 MB)  1 1 1 2560\r\nTensor 440 (null)               kTfLiteInt8  kTfLiteArenaRw        512 bytes ( 0.0 MB)  1 1 1 512\r\nTensor 441 (null)               kTfLiteFloat32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 442 (null)               kTfLiteInt32  kTfLiteArenaRw       2048 bytes ( 0.0 MB)  512 1\r\nTensor 443 (null)               kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor 444 (null)               kTfLiteInt32 kTfLiteArenaRwPersistent       2048 bytes ( 0.0 MB)  512\r\n\r\nNode   0 Operator Builtin Code  36 GATHER\r\n  Inputs: 28 0\r\n  Outputs: 95\r\nNode   1 Operator Builtin Code   6 DEQUANTIZE\r\n  Inputs: 95\r\n  Outputs: 354\r\nNode   2 Operator Builtin Code  40 MEAN\r\n  Inputs: 354 92\r\n  Outputs: 96\r\n  Temporaries: 355 356 357\r\nNode   3 Operator Builtin Code  99 SQUARED_DIFFERENCE\r\n  Inputs: 354 96\r\n  Outputs: 97\r\nNode   4 Operator Builtin Code  40 MEAN\r\n  Inputs: 97 92\r\n  Outputs: 98\r\n  Temporaries: 358 359 360\r\nNode   5 Operator Builtin Code   0 ADD\r\n  Inputs: 98 30\r\n  Outputs: 99\r\nNode   6 Operator Builtin Code  76 RSQRT\r\n  Inputs: 99\r\n  Outputs: 100\r\nNode   7 Operator Builtin Code  18 MUL\r\n  Inputs: 100 31\r\n  Outputs: 101\r\nNode   8 Operator Builtin Code  18 MUL\r\n  Inputs: 354 101\r\n  Outputs: 102\r\nNode   9 Operator Builtin Code  18 MUL\r\n  Inputs: 96 101\r\n  Outputs: 103\r\nNode  10 Operator Builtin Code  41 SUB\r\n  Inputs: 29 103\r\n  Outputs: 104\r\nNode  11 Operator Builtin Code   0 ADD\r\n  Inputs: 102 104\r\n  Outputs: 105\r\nNode  12 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 105 38\r\n  Outputs: 106\r\nNode  13 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 106 77 86\r\n  Outputs: 107\r\n  Temporaries: 415 416 417 418 419 420\r\nNode  14 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 107\r\n  Outputs: 108\r\nNode  15 Operator Builtin Code   0 ADD\r\n  Inputs: 108 23\r\n  Outputs: 109\r\nNode  16 Operator Builtin Code  18 MUL\r\n  Inputs: 109 56\r\n  Outputs: 110\r\nNode  17 Operator Builtin Code   0 ADD\r\n  Inputs: 110 57\r\n  Outputs: 111\r\nNode  18 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 111 38\r\n  Outputs: 112\r\nNode  19 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 112 78 86\r\n  Outputs: 113\r\n  Temporaries: 421 422 423 424 425 426\r\nNode  20 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 113\r\n  Outputs: 114\r\nNode  21 Operator Builtin Code   0 ADD\r\n  Inputs: 114 24\r\n  Outputs: 115\r\nNode  22 Operator Builtin Code  18 MUL\r\n  Inputs: 115 58\r\n  Outputs: 116\r\nNode  23 Operator Builtin Code   0 ADD\r\n  Inputs: 116 59\r\n  Outputs: 117\r\nNode  24 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 117 38\r\n  Outputs: 118\r\nNode  25 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 118 79 86\r\n  Outputs: 119\r\n  Temporaries: 427 428 429 430 431 432\r\nNode  26 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 119\r\n  Outputs: 120\r\nNode  27 Operator Builtin Code   0 ADD\r\n  Inputs: 120 25\r\n  Outputs: 121\r\nNode  28 Operator Builtin Code  18 MUL\r\n  Inputs: 121 60\r\n  Outputs: 122\r\nNode  29 Operator Builtin Code   0 ADD\r\n  Inputs: 122 61\r\n  Outputs: 123\r\nNode  30 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 123 38\r\n  Outputs: 124\r\nNode  31 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 124 80 86\r\n  Outputs: 125\r\n  Temporaries: 433 434 435 436 437 438\r\nNode  32 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 125\r\n  Outputs: 126\r\nNode  33 Operator Builtin Code   0 ADD\r\n  Inputs: 126 26\r\n  Outputs: 127\r\nNode  34 Operator Builtin Code  18 MUL\r\n  Inputs: 127 62\r\n  Outputs: 128\r\nNode  35 Operator Builtin Code   0 ADD\r\n  Inputs: 128 63\r\n  Outputs: 129\r\nNode  36 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 129 38\r\n  Outputs: 130\r\nNode  37 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 130 81 86\r\n  Outputs: 131\r\n  Temporaries: 439 440 441 442 443 444\r\nNode  38 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 131\r\n  Outputs: 132\r\nNode  39 Operator Builtin Code   0 ADD\r\n  Inputs: 132 27\r\n  Outputs: 133\r\nNode  40 Operator Builtin Code  18 MUL\r\n  Inputs: 133 64\r\n  Outputs: 134\r\nNode  41 Operator Builtin Code   0 ADD\r\n  Inputs: 134 65\r\n  Outputs: 135\r\nNode  42 Operator Builtin Code  39 TRANSPOSE\r\n  Inputs: 135 20\r\n  Outputs: 136\r\nNode  43 Operator Builtin Code 105 REVERSE_V2\r\n  Inputs: 136 93\r\n  Outputs: 137\r\nNode  44 Operator Builtin Code  77 SHAPE\r\n  Inputs: 136\r\n  Outputs: 138\r\nNode  45 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 138 93 94 94\r\n  Outputs: 139\r\nNode  46 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 139 94\r\n  Outputs: 140\r\nNode  47 Operator Builtin Code   2 CONCATENATION\r\n  Inputs: 140 48\r\n  Outputs: 141\r\nNode  48 Operator Builtin Code  94 FILL\r\n  Inputs: 141 50\r\n  Outputs: 142\r\nNode  49 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 136 88 89 90\r\n  Outputs: 143\r\nNode  50 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 143 52 -1\r\n  Outputs: 144\r\nNode  51 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 143 54 -1\r\n  Outputs: 145\r\nNode  52 Operator Builtin Code  77 SHAPE\r\n  Inputs: 135\r\n  Outputs: 146\r\nNode  53 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 146 93 94 94\r\n  Outputs: 147\r\nNode  54 Operator Builtin Code  83 PACK\r\n  Inputs: 147 22\r\n  Outputs: 148\r\nNode  55 Operator Builtin Code  94 FILL\r\n  Inputs: 148 50\r\n  Outputs: 149\r\nNode  56 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 149 53 -1\r\n  Outputs: 150\r\nNode  57 Operator Builtin Code   0 ADD\r\n  Inputs: 144 150\r\n  Outputs: 151\r\nNode  58 Operator Builtin Code   0 ADD\r\n  Inputs: 151 18\r\n  Outputs: 152\r\nNode  59 Operator Builtin Code  49 SPLIT\r\n  Inputs: 39 152\r\n  Outputs: 153 154 155 156\r\nNode  60 Operator Builtin Code  14 LOGISTIC\r\n  Inputs: 153\r\n  Outputs: 157\r\nNode  61 Operator Builtin Code  14 LOGISTIC\r\n  Inputs: 154\r\n  Outputs: 158\r\nNode  62 Operator Builtin Code  14 LOGISTIC\r\n  Inputs: 156\r\n  Outputs: 159\r\nNode  63 Operator Builtin Code  28 TANH\r\n  Inputs: 155\r\n  Outputs: 160\r\nNode  64 Operator Builtin Code  18 MUL\r\n  Inputs: 157 160\r\n  Outputs: 161\r\nNode  65 Operator Builtin Code  18 MUL\r\n  Inputs: 158 149\r\n  Outputs: 162\r\nNode  66 Operator Builtin Code   0 ADD\r\n  Inputs: 162 161\r\n  Outputs: 163\r\nNode  67 Operator Builtin Code  28 TANH\r\n  Inputs: 163\r\n  Outputs: 164\r\nNode  68 Operator Builtin Code  18 MUL\r\n  Inputs: 159 164\r\n  Outputs: 165\r\nNode  69 Operator Builtin Code  93 ZEROS_LIKE\r\n  Inputs: 165\r\n  Outputs: 166\r\nNode  70 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 149 55 -1\r\n  Outputs: 167\r\nNode  71 Operator Builtin Code   0 ADD\r\n  Inputs: 145 167\r\n  Outputs: 168\r\nNode  72 Operator Builtin Code   0 ADD\r\n  Inputs: 168 21\r\n  Outputs: 169\r\nNode  73 Operator Builtin Code  49 SPLIT\r\n  Inputs: 39 169\r\n  Outputs: 170 171 172 173\r\nNode  74 Operator Builtin Code  14 LOGISTIC\r\n  Inputs: 170\r\n  Outputs: 174\r\nNode  75 Operator Builtin Code  14 LOGISTIC\r\n  Inputs: 171\r\n  Outputs: 175\r\nNode  76 Operator Builtin Code  14 LOGISTIC\r\n  Inputs: 173\r\n  Outputs: 176\r\nNode  77 Operator Builtin Code  28 TANH\r\n  Inputs: 172\r\n  Outputs: 177\r\nNode  78 Operator Builtin Code  18 MUL\r\n  Inputs: 174 177\r\n  Outputs: 178\r\nNode  79 Operator Builtin Code  18 MUL\r\n  Inputs: 175 149\r\n  Outputs: 179\r\nNode  80 Operator Builtin Code   0 ADD\r\n  Inputs: 179 178\r\n  Outputs: 180\r\nNode  81 Operator Builtin Code  28 TANH\r\n  Inputs: 180\r\n  Outputs: 181\r\nNode  82 Operator Builtin Code  18 MUL\r\n  Inputs: 176 181\r\n  Outputs: 182\r\nNode  83 Operator Builtin Code  93 ZEROS_LIKE\r\n  Inputs: 182\r\n  Outputs: 183\r\nNode  84 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 1 91\r\n  Outputs: 184\r\nNode  85 Operator Builtin Code  82 REDUCE_MAX\r\n  Inputs: 1 93\r\n  Outputs: 185\r\nNode  86 Operator Builtin Code  96 RANGE\r\n  Inputs: 49 185 39\r\n  Outputs: 186\r\nNode  87 Operator Builtin Code  58 LESS\r\n  Inputs: 186 184\r\n  Outputs: 187\r\nNode  88 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 187 32\r\n  Outputs: 188\r\nNode  89 Operator Builtin Code  39 TRANSPOSE\r\n  Inputs: 188 20\r\n  Outputs: 189\r\nNode  90 Operator Builtin Code 105 REVERSE_V2\r\n  Inputs: 189 93\r\n  Outputs: 190\r\nNode  91 Operator Builtin Code 119 WHILE\r\n  Inputs: 49 49 142 149 149 139 190 137 166\r\n  Outputs: 191 192 193 194 195 196 197 198 199\r\nNode  92 Operator Builtin Code  77 SHAPE\r\n  Inputs: 193\r\n  Outputs: 200\r\nNode  93 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 193 200\r\n  Outputs: 201\r\nNode  94 Operator Builtin Code  39 TRANSPOSE\r\n  Inputs: 201 20\r\n  Outputs: 202\r\nNode  95 Operator Builtin Code 105 REVERSE_V2\r\n  Inputs: 202 94\r\n  Outputs: 203\r\nNode  96 Operator Builtin Code 119 WHILE\r\n  Inputs: 49 49 142 149 149 139 189 136 183\r\n  Outputs: 204 205 206 207 208 209 210 211 212\r\nNode  97 Operator Builtin Code  77 SHAPE\r\n  Inputs: 206\r\n  Outputs: 213\r\nNode  98 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 206 213\r\n  Outputs: 214\r\nNode  99 Operator Builtin Code  39 TRANSPOSE\r\n  Inputs: 214 20\r\n  Outputs: 215\r\nNode 100 Operator Builtin Code   2 CONCATENATION\r\n  Inputs: 215 203\r\n  Outputs: 216\r\nNode 101 Operator Builtin Code  77 SHAPE\r\n  Inputs: 216\r\n  Outputs: 217\r\nNode 102 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 217 94 92 94\r\n  Outputs: 218\r\nNode 103 Operator Builtin Code  96 RANGE\r\n  Inputs: 49 218 39\r\n  Outputs: 219\r\nNode 104 Operator Builtin Code  58 LESS\r\n  Inputs: 219 184\r\n  Outputs: 220\r\nNode 105 Operator Builtin Code  53 CAST\r\n  Inputs: 220\r\n  Outputs: 221\r\nNode 106 Operator Builtin Code  77 SHAPE\r\n  Inputs: 221\r\n  Outputs: 222\r\nNode 107 Operator Builtin Code   2 CONCATENATION\r\n  Inputs: 222 94\r\n  Outputs: 223\r\nNode 108 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 221 223\r\n  Outputs: 224\r\nNode 109 Operator Builtin Code  18 MUL\r\n  Inputs: 216 224\r\n  Outputs: 225\r\nNode 110 Operator Builtin Code  77 SHAPE\r\n  Inputs: 225\r\n  Outputs: 226\r\nNode 111 Operator Builtin Code  36 GATHER\r\n  Inputs: 226 42\r\n  Outputs: 227\r\nNode 112 Operator Builtin Code  81 REDUCE_PROD\r\n  Inputs: 227 93\r\n  Outputs: 228\r\nNode 113 Operator Builtin Code   2 CONCATENATION\r\n  Inputs: 227 4\r\n  Outputs: 229\r\nNode 114 Operator Builtin Code  36 GATHER\r\n  Inputs: 226 92\r\n  Outputs: 230\r\nNode 115 Operator Builtin Code  81 REDUCE_PROD\r\n  Inputs: 230 93\r\n  Outputs: 231\r\nNode 116 Operator Builtin Code  83 PACK\r\n  Inputs: 228 231\r\n  Outputs: 232\r\nNode 117 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 225 232\r\n  Outputs: 233\r\nNode 118 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 233 51 -1\r\n  Outputs: 234\r\nNode 119 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 234 229\r\n  Outputs: 235\r\nNode 120 Operator Builtin Code  77 SHAPE\r\n  Inputs: 235\r\n  Outputs: 236\r\nNode 121 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 217 93 94 94\r\n  Outputs: 237\r\nNode 122 Operator Builtin Code  83 PACK\r\n  Inputs: 237 32 8\r\n  Outputs: 238\r\nNode 123 Operator Builtin Code  83 PACK\r\n  Inputs: 237 32\r\n  Outputs: 239\r\nNode 124 Operator Builtin Code  83 PACK\r\n  Inputs: 237\r\n  Outputs: 240\r\nNode 125 Operator Builtin Code  69 TILE\r\n  Inputs: 6 240\r\n  Outputs: 241\r\nNode 126 Operator Builtin Code  84 LOGICAL_OR\r\n  Inputs: 241 10\r\n  Outputs: 242\r\nNode 127 Operator Builtin Code  77 SHAPE\r\n  Inputs: 242\r\n  Outputs: 243\r\nNode 128 Operator Builtin Code  94 FILL\r\n  Inputs: 243 49\r\n  Outputs: 244\r\nNode 129 Operator Builtin Code  83 PACK\r\n  Inputs: 237 8\r\n  Outputs: 245\r\nNode 130 Operator Builtin Code  69 TILE\r\n  Inputs: 7 245\r\n  Outputs: 246\r\nNode 131 Operator Builtin Code  83 PACK\r\n  Inputs: 237 218\r\n  Outputs: 247\r\nNode 132 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 236 94 92 94\r\n  Outputs: 248\r\nNode 133 Operator Builtin Code  96 RANGE\r\n  Inputs: 49 248 39\r\n  Outputs: 249\r\nNode 134 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 249 49\r\n  Outputs: 250\r\nNode 135 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 236 93 94 94\r\n  Outputs: 251\r\nNode 136 Operator Builtin Code  83 PACK\r\n  Inputs: 251 39\r\n  Outputs: 252\r\nNode 137 Operator Builtin Code  69 TILE\r\n  Inputs: 250 252\r\n  Outputs: 253\r\nNode 138 Operator Builtin Code  83 PACK\r\n  Inputs: 237 43\r\n  Outputs: 254\r\nNode 139 Operator Builtin Code  94 FILL\r\n  Inputs: 254 50\r\n  Outputs: 255\r\nNode 140 Operator Builtin Code  83 PACK\r\n  Inputs: 237 44\r\n  Outputs: 256\r\nNode 141 Operator Builtin Code  94 FILL\r\n  Inputs: 256 50\r\n  Outputs: 257\r\nNode 142 Operator Builtin Code  94 FILL\r\n  Inputs: 247 50\r\n  Outputs: 258\r\nNode 143 Operator Builtin Code  94 FILL\r\n  Inputs: 240 49\r\n  Outputs: 259\r\nNode 144 Operator Builtin Code 119 WHILE\r\n  Inputs: 49 49 45 46 47 255 255 255 255 257 49 258 259 246 242 244 253 235 184 225 240\r\n  Outputs: 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\r\nNode 145 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 262 238\r\n  Outputs: 281\r\nNode 146 Operator Builtin Code 101 ABS\r\n  Inputs: 281\r\n  Outputs: 282\r\nNode 147 Operator Builtin Code  74 SUM\r\n  Inputs: 282 32\r\n  Outputs: 283\r\nNode 148 Operator Builtin Code  53 CAST\r\n  Inputs: 283\r\n  Outputs: 284\r\nNode 149 Operator Builtin Code  72 NOT_EQUAL\r\n  Inputs: 284 49\r\n  Outputs: 285\r\nNode 150 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 285 5\r\n  Outputs: 286\r\nNode 151 Operator Builtin Code 109 WHERE\r\n  Inputs: 286\r\n  Outputs: 287\r\nNode 152 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 287\r\n  Outputs: 288\r\nNode 153 Operator Builtin Code  77 SHAPE\r\n  Inputs: 281\r\n  Outputs: 289\r\nNode 154 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 289 93 92 94\r\n  Outputs: 290\r\nNode 155 Operator Builtin Code  81 REDUCE_PROD\r\n  Inputs: 290 93\r\n  Outputs: 291\r\nNode 156 Operator Builtin Code  83 PACK\r\n  Inputs: 291\r\n  Outputs: 292\r\nNode 157 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 289 92 93 94\r\n  Outputs: 293\r\nNode 158 Operator Builtin Code   2 CONCATENATION\r\n  Inputs: 292 293\r\n  Outputs: 294\r\nNode 159 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 262 294\r\n  Outputs: 295\r\nNode 160 Operator Builtin Code  36 GATHER\r\n  Inputs: 295 288\r\n  Outputs: 296\r\nNode 161 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 296 49\r\n  Outputs: 297\r\nNode 162 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 281 38\r\n  Outputs: 298\r\nNode 163 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 298 82 86\r\n  Outputs: 299\r\nNode 164 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 299\r\n  Outputs: 300\r\nNode 165 Operator Builtin Code   0 ADD\r\n  Inputs: 300 33\r\n  Outputs: 301\r\nNode 166 Operator Builtin Code  18 MUL\r\n  Inputs: 301 66\r\n  Outputs: 302\r\nNode 167 Operator Builtin Code   0 ADD\r\n  Inputs: 302 67\r\n  Outputs: 303\r\nNode 168 Operator Builtin Code  28 TANH\r\n  Inputs: 303\r\n  Outputs: 304\r\nNode 169 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 304 38\r\n  Outputs: 305\r\nNode 170 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 305 83 86\r\n  Outputs: 306\r\nNode 171 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 306\r\n  Outputs: 307\r\nNode 172 Operator Builtin Code   0 ADD\r\n  Inputs: 307 34\r\n  Outputs: 308\r\nNode 173 Operator Builtin Code  18 MUL\r\n  Inputs: 308 68\r\n  Outputs: 309\r\nNode 174 Operator Builtin Code   0 ADD\r\n  Inputs: 309 69\r\n  Outputs: 310\r\nNode 175 Operator Builtin Code  28 TANH\r\n  Inputs: 310\r\n  Outputs: 311\r\nNode 176 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 311 38\r\n  Outputs: 312\r\nNode 177 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 312 84 86\r\n  Outputs: 313\r\nNode 178 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 313\r\n  Outputs: 314\r\nNode 179 Operator Builtin Code   0 ADD\r\n  Inputs: 314 35\r\n  Outputs: 315\r\nNode 180 Operator Builtin Code  18 MUL\r\n  Inputs: 315 70\r\n  Outputs: 316\r\nNode 181 Operator Builtin Code   0 ADD\r\n  Inputs: 316 71\r\n  Outputs: 317\r\nNode 182 Operator Builtin Code  28 TANH\r\n  Inputs: 317\r\n  Outputs: 318\r\nNode 183 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 318 38\r\n  Outputs: 319\r\nNode 184 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 319 85 86\r\n  Outputs: 320\r\nNode 185 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 320\r\n  Outputs: 321\r\nNode 186 Operator Builtin Code   0 ADD\r\n  Inputs: 321 36\r\n  Outputs: 322\r\nNode 187 Operator Builtin Code  18 MUL\r\n  Inputs: 322 72\r\n  Outputs: 323\r\nNode 188 Operator Builtin Code   0 ADD\r\n  Inputs: 323 73\r\n  Outputs: 324\r\nNode 189 Operator Builtin Code  28 TANH\r\n  Inputs: 324\r\n  Outputs: 325\r\nNode 190 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 325 38\r\n  Outputs: 326\r\nNode 191 Operator Builtin Code   3 CONV_2D\r\n  Inputs: 326 87 86\r\n  Outputs: 327\r\nNode 192 Operator Builtin Code  43 SQUEEZE\r\n  Inputs: 327\r\n  Outputs: 328\r\nNode 193 Operator Builtin Code   0 ADD\r\n  Inputs: 328 37\r\n  Outputs: 329\r\nNode 194 Operator Builtin Code  18 MUL\r\n  Inputs: 329 74\r\n  Outputs: 330\r\nNode 195 Operator Builtin Code   0 ADD\r\n  Inputs: 330 75\r\n  Outputs: 331\r\nNode 196 Operator Builtin Code  77 SHAPE\r\n  Inputs: 331\r\n  Outputs: 332\r\nNode 197 Operator Builtin Code  36 GATHER\r\n  Inputs: 332 42\r\n  Outputs: 333\r\nNode 198 Operator Builtin Code  81 REDUCE_PROD\r\n  Inputs: 333 93\r\n  Outputs: 334\r\nNode 199 Operator Builtin Code   2 CONCATENATION\r\n  Inputs: 333 41\r\n  Outputs: 335\r\nNode 200 Operator Builtin Code  36 GATHER\r\n  Inputs: 332 92\r\n  Outputs: 336\r\nNode 201 Operator Builtin Code  81 REDUCE_PROD\r\n  Inputs: 336 93\r\n ERROR: tensorflow/lite/kernels/transpose.cc:55 op_context->perm->dims->data[0] != dims (3 != 2)\r\nERROR: Node number 89 (TRANSPOSE) failed to prepare.\r\n\r\nError at tensorflow/lite/examples/minimal/minimal.cc:505\r\n Outputs: 337\r\nNode 202 Operator Builtin Code  83 PACK\r\n  Inputs: 334 337\r\n  Outputs: 338\r\nNode 203 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 331 338\r\n  Outputs: 339\r\nNode 204 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 339 76 -1\r\n  Outputs: 340\r\nNode 205 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 340 335\r\n  Outputs: 341\r\nNode 206 Operator Builtin Code   0 ADD\r\n  Inputs: 341 40\r\n  Outputs: 342\r\nNode 207 Operator Builtin Code   0 ADD\r\n  Inputs: 281 342\r\n  Outputs: 343\r\nNode 208 Operator Builtin Code  77 SHAPE\r\n  Inputs: 343\r\n  Outputs: 344\r\nNode 209 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 344 93 92 94\r\n  Outputs: 345\r\nNode 210 Operator Builtin Code  81 REDUCE_PROD\r\n  Inputs: 345 93\r\n  Outputs: 346\r\nNode 211 Operator Builtin Code  83 PACK\r\n  Inputs: 346\r\n  Outputs: 347\r\nNode 212 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 344 92 93 94\r\n  Outputs: 348\r\nNode 213 Operator Builtin Code   2 CONCATENATION\r\n  Inputs: 347 348\r\n  Outputs: 349\r\nNode 214 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 343 349\r\n  Outputs: 350\r\nNode 215 Operator Builtin Code  36 GATHER\r\n  Inputs: 350 288\r\n  Outputs: 351\r\nNode 216 Operator Builtin Code  70 EXPAND_DIMS\r\n  Inputs: 351 49\r\n  Outputs: 352\r\nNode 217 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 263 239\r\n  Outputs: 353\r\n```\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@brendanxwhitaker,\r\nOn running the Colab notebook you have provided with the latest TF-nightly, I am facing an error stating `tensorflow/lite/kernels/reshape.cc:58 stretch_dim != -1 (0 != -1)Node number 5 (RESHAPE) failed to prepare.\r\nNode number 125 (WHILE) failed to prepare.` Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3191da10980ff8d1493e6ff76eaa394f/43624-tf-nightly.ipynb). \r\n\r\nAlso, the example given is fairly complex and it is difficult for us to pinpoint the issue. Can you please remove the dependencies and get the example down to the simplest possible repro? That will allow us to debug the error easily. Thanks!\r\n", "I saw that the transpose op says different dimensions. Could you check your input tensors have the right dimensions?\r\n\r\nTo check it, please use python interpreter first. It is easy to test the dimension.\r\n\r\nHere is the example colab notebook to resolve the issue.\r\n\r\nhttps://github.com/TensorSpeech/TensorFlowTTS/blob/master/notebooks/TensorFlowTTS_Tacotron2_with_TFLite.ipynb\r\n\r\nPlease see \"Inference from TFLite\"", "Hi @amahendrakar, what Colab notebook are you referring to? I will work on paring down the example. @jaeyoo, there is inference code in the python script attached above. It runs fine, the issue is in the translation to C++. I think you might be right, my suspicion is that my API usage in wrong in some way, the C++ API documentation is rather sparse. The you linked is what I've adapted the C++ inference program (minimal.cc) from. The notebook works fine.", "In particular, my guess is that the following snippet:\r\n```\r\n  // Define test input string.\r\n  std::string in = \"hello.\";\r\n\r\n  // Convert to input ids.\r\n  std::vector<int>* input_ids = text_to_sequence(in);\r\n  int n = input_ids->size();\r\n\r\n  // Fill input buffers\r\n  // TODO(user): Insert code to fill input tensors.\r\n  // Note: The buffer of the input tensor with index `i` of type T can\r\n  // be accessed with `T* input = interpreter->typed_input_tensor<T>(i);`\r\n  int* input = interpreter->typed_input_tensor<int>(0);\r\n\r\n  // Resize input tensors.\r\n  int t0 = interpreter->inputs()[0];\r\n  int t1 = interpreter->inputs()[1];\r\n  int t2 = interpreter->inputs()[2];\r\n  interpreter->ResizeInputTensor(t0, {1, n});\r\n  interpreter->ResizeInputTensor(t1, {1});\r\n  interpreter->ResizeInputTensor(t2, {1});\r\n\r\n  // Allocate tensors again?\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n\r\n  for (int i = 0; i < n; i++) {\r\n    input[i] = input_ids->data()[i];\r\n  }\r\n\r\n  // Run inference - uncomment invoke when input filled\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n```\r\nis not equivalent to the python version, which works correctly:\r\n```\r\ndef infer_tacotron2():\r\n    \"\"\"## Tacotron-2\"\"\"\r\n    # Load the TFLite model and allocate tensors.\r\n    interpreter = tf.lite.Interpreter(model_path=\"tacotron2.tflite\")\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Prepare input data.\r\n    def prepare_input(input_ids):\r\n        return (\r\n            tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),\r\n            tf.convert_to_tensor([len(input_ids)], tf.int32),\r\n            tf.convert_to_tensor([0], dtype=tf.int32),\r\n        )\r\n\r\n    # Test the model on random input data.\r\n    def infer(input_text):\r\n        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)\r\n        input_ids = processor.text_to_sequence(input_text.lower())\r\n        print(\"Input ids shape:\", input_ids.shape)\r\n        interpreter.resize_tensor_input(input_details[0][\"index\"], [1, len(input_ids)])\r\n        interpreter.allocate_tensors()\r\n        input_data = prepare_input(input_ids)\r\n        for i, detail in enumerate(input_details):\r\n            print(\"Input tensor %d detail:\" % i, detail)\r\n            input_shape = detail[\"shape\"]\r\n            print(\"Shape of tensor %d:\" % i, input_data[i].shape)\r\n            interpreter.set_tensor(detail[\"index\"], input_data[i])\r\n\r\n        interpreter.invoke()\r\n\r\n        # The function `get_tensor()` returns a copy of the tensor data.\r\n        # Use `tensor()` in order to get a pointer to the tensor.\r\n        return (\r\n            interpreter.get_tensor(output_details[0][\"index\"]),\r\n            interpreter.get_tensor(output_details[1][\"index\"]),\r\n        )\r\n```\r\n\r\nDoes the way I've filled the input tensor and resized the inputs make sense?", "@brendanxwhitaker,\r\n> Original file is located at\r\n>     https://colab.research.google.com/drive/1HudLLpT9CQdh2k04c06bHUwLubhGTWxA\r\n\r\nOn running the Colab notebook you have linked, I am facing an error stating `RuntimeError: tensorflow/lite/kernels/reshape.cc:58 stretch_dim != -1 (0 != -1)Node number 5 (RESHAPE) failed to prepare.\r\nNode number 125 (WHILE) failed to prepare.`.\r\n\r\nPlease find the gist of it below\r\n\r\nhttps://colab.research.google.com/gist/amahendrakar/3191da10980ff8d1493e6ff76eaa394f/43624-tf-nightly.ipynb#scrollTo=V5T91_v4Zp22&line=1&uniqifier=1\r\n\r\nThanks!", "I was able to solve this issue using the following syntax for loading the input tensor data:\r\n```\r\nmemcpy(input_mel_tensor->data.raw, &mel_output_ptr, float_size * n);\r\n```"]}, {"number": 43623, "title": "Tensorflow 2.3 CUDNN - Detected cudnn out-of-bounds write in convolution buffer!  + GPU Sync Failed", "body": "### System information\r\n\r\n-   I was trying to train standard CNN network\r\n-   OS Platform: Windows 10\r\n-   TF installed via Anaconda - standard release\r\n-   TF2.3.1 latest release\r\n-   Python 3.6.12\r\n-   Cuda  10.1.168 (via cudatoolkit) Cnn 7.6.5\r\n-   NVidia GTX 1080.\r\n\r\n### Describe the problem\r\nSimple CNN networks fails to train. At the beginning of each epoch the following message is displayed:\r\n\r\nThis message will be only logged once.\r\n2020-09-28 16:32:44.253794: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n\r\nAfter few epochs the script simply crash with\r\n\r\nTraceback (most recent call last):\r\n  File \"cnn.py\", line 44, in <module>\r\n    history = model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1103, in fit\r\n    callbacks.on_train_batch_end(end_step, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 440, in on_train_batch_end\r\n    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 289, in _call_batch_hook\r\n    self._call_batch_end_hook(mode, batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 309, in _call_batch_end_hook\r\n    self._call_batch_hook_helper(hook_name, batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 342, in _call_batch_hook_helper\r\n    hook(batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 961, in on_train_batch_end\r\n    self._batch_update_progbar(batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1016, in _batch_update_progbar\r\n    logs = tf_utils.to_numpy_or_python_type(logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 537, in to_numpy_or_python_type\r\n    return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 635, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 635, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 533, in _to_single_numpy_or_python_type\r\n    x = t.numpy()\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1063, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1031, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed\r\n\r\n\r\n### Source code / logs\r\nSource code attached to the issue. \r\n[cnn.zip](https://github.com/tensorflow/tensorflow/files/5293538/cnn.zip)\r\n\r\n\r\nExample logs are below:\r\n\r\n2020-09-28 16:32:34.965794: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-28 16:32:40.621408: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-28 16:32:40.639751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-09-28 16:32:40.639856: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-28 16:32:40.687455: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-28 16:32:40.723571: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-28 16:32:40.740330: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-28 16:32:40.779951: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-28 16:32:40.809284: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-28 16:32:40.882255: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-28 16:32:40.882629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-28 16:32:40.888697: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-28 16:32:40.914399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f8da8ad150 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-28 16:32:40.914510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-28 16:32:40.915321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-09-28 16:32:40.915404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-28 16:32:40.915461: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-28 16:32:40.915517: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-28 16:32:40.915592: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-28 16:32:40.915655: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-28 16:32:40.915711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-28 16:32:40.915766: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-28 16:32:40.915855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-28 16:32:41.807875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-28 16:32:41.807946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-09-28 16:32:41.808090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-09-28 16:32:41.809160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-09-28 16:32:41.813395: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f954359b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-28 16:32:41.813452: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\n1 Physical GPUs, 1 Logical GPUs\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nconv2d (Conv2D)              (None, 30, 30, 32)        896\r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0\r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        18496\r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0\r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 4, 4, 64)          36928\r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1024)              0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 64)                65600\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                650\r\n=================================================================\r\nTotal params: 122,570\r\nTrainable params: 122,570\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nEpoch 1/100\r\n2020-09-28 16:32:42.623247: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-28 16:32:43.041335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-28 16:32:44.216588: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-09-28 16:32:44.253794: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:44.253938: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 7993648; expected ffffffffffffffff but was ffffffff.\r\n2020-09-28 16:32:44.283822: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:44.283952: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 1530416; expected ffffffffffffffff but was 3c6ddc20ffffffff.\r\n2020-09-28 16:32:44.341115: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:44.341354: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 749232; expected ffffffffffffffff but was 3b28f5e2ffffffff.\r\n2020-09-28 16:32:44.362704: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:44.362874: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 6120752; expected ffffffffffffffff but was bb0a6dcdffffffff.\r\n2020-09-28 16:32:44.404275: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:44.404426: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 738712; expected ffffffffffffffff but was 3b0dd497ffffffff.\r\n2020-09-28 16:32:44.424932: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:44.425087: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x70295c600 at offset 482608; expected ffffffffffffffff but was b96f566dffffffff.\r\n1543/1563 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.44602020-09-28 16:32:48.198062: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:48.198215: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in LHS redzone of buffer 0x702400000 at offset 522160; expected ffffffffffffffff but was ffffffff.\r\n2020-09-28 16:32:48.228288: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:48.228471: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 350000; expected ffffffffffffffff but was 3e26dfe4ffffffff.\r\n2020-09-28 16:32:48.247663: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:48.247826: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 1634120; expected ffffffffffffffff but was b9b83185ffffffff.\r\n2020-09-28 16:32:48.263505: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:48.263808: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in LHS redzone of buffer 0x702400000 at offset 808496; expected ffffffffffffffff but was ffffffff.\r\n2020-09-28 16:32:48.298224: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:48.298378: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 354992; expected ffffffffffffffff but was b9847af9ffffffff.\r\n2020-09-28 16:32:48.323588: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:48.323747: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 5820336; expected ffffffffffffffff but was 3b0bd7deffffffff.\r\n2020-09-28 16:32:48.340817: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-28 16:32:48.340980: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x704400000 at offset 1602328; expected ffffffffffffffff but was bb654981ffffffff.\r\n1563/1563 [==============================] - 5s 3ms/step - loss: 1.5212 - accuracy: 0.4470 - val_loss: 1.2762 - val_accuracy: 0.5441\r\nEpoch 2/100\r\n 187/1563 [==>...........................] - ETA: 3s - loss: 1.2447 - accuracy: 0.55582020-09-28 16:32:49.840793: E tensorflow/stream_executor/cuda/cuda_driver.cc:951] could not synchronize on CUDA context: CUDA_ERROR_MISALIGNED_ADDRESS: misaligned address :: 0x00007FFE82613305       tensorflow::CurrentStackTrace\r\n0x00007FFE82361A5E      tensorflow::CostGraphDef_Node::set_is_final\r\n0x00007FFE82509F4E      stream_executor::StreamExecutor::SetDeviceSharedMemoryConfig\r\n0x00007FFE8009ACC6      tensorflow::StepStats::internal_default_instance\r\n0x00007FFE800AC4F4      google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add\r\n0x00007FFE67F60137      std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=\r\n0x00007FFE67F3B07B      absl::lts_2020_02_25::Span<tensorflow::Tensor const >::end\r\n0x00007FFE67EB3A8F      TFE_TensorHandleResolve\r\n0x00007FFE67E50E63      TFE_Py_TensorShapeSlice\r\n0x00007FFE67E4E6DA      std::_Tree<std::_Tmap_traits<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,0>,tensorflow::monitoring::CounterCell,std::less<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char>\r\n0x0000000070A8C25D      PyCFunction_FastCallDict\r\n0x0000000070A8C7B9      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8C980      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8C980      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A9B2A5      PyBytes_FromStringAndSize\r\n0x0000000070A84BA4      Py_BuildValue\r\n0x0000000070A8DFF7      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A8CC27      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A8CC27      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8C980      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A8CC27      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A8CC27      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8C980      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8C980      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A8CC27      PyObject_GetAttr\r\n0x0000000070A8DE48      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A8CC27      PyObject_GetAttr\r\n0x0000000070A8D1EE      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A9B2A5      PyBytes_FromStringAndSize\r\n0x0000000070A84BA4      Py_BuildValue\r\n0x0000000070A8DFF7      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070A8CC27      PyObject_GetAttr\r\n0x0000000070A8DE48      PyEval_EvalFrameDefault\r\n0x0000000070A8A488      PyObject_IsInstance\r\n0x0000000070AA69C7      PyEval_EvalCodeEx\r\n0x0000000070AA6925      PyEval_EvalCode\r\n0x0000000070AA68CF      PyArena_Free\r\n0x0000000070BFE011      PyRun_FileExFlags\r\n0x0000000070BFE83C      PyRun_SimpleFileExFlags\r\n0x0000000070BFDEDF      PyRun_AnyFileExFlags\r\n0x0000000070B4CCEB      Py_hashtable_size\r\n0x0000000070AD9DDD      PyThreadState_UncheckedGet\r\n0x000000001C7B1258      (unknown)\r\n0x00007FFF127F6FD4      BaseThreadInitThunk\r\n0x00007FFF13C1CEC1      RtlUserThreadStart\r\n\r\nTraceback (most recent call last):\r\n  File \"cnn.py\", line 44, in <module>\r\n    history = model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1103, in fit\r\n    callbacks.on_train_batch_end(end_step, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 440, in on_train_batch_end\r\n    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 289, in _call_batch_hook\r\n    self._call_batch_end_hook(mode, batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 309, in _call_batch_end_hook\r\n    self._call_batch_hook_helper(hook_name, batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 342, in _call_batch_hook_helper\r\n    hook(batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 961, in on_train_batch_end\r\n    self._batch_update_progbar(batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1016, in _batch_update_progbar\r\n    logs = tf_utils.to_numpy_or_python_type(logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 537, in to_numpy_or_python_type\r\n    return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 635, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 635, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 533, in _to_single_numpy_or_python_type\r\n    x = t.numpy()\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1063, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF2.3GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1031, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed", "comments": ["@Palkos83 \r\nCan you please refer to [this](https://github.com/zhihou7/VCL/issues/1) and let me know if it helps.", "There is only one GPU in the system and the model is so simple that it should not need more than one.\r\nSo sadly this is not helping me with the problem.\r\n\r\n\r\n\r\n________________________________\r\nFrom: Saduf2019 <notifications@github.com>\r\nSent: 28 September 2020 17:10\r\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\nCc: Palkos83 <palkos@hotmail.com>; Mention <mention@noreply.github.com>\r\nSubject: Re: [tensorflow/tensorflow] Tensorflow 2.3 CUDNN - Detected cudnn out-of-bounds write in convolution buffer! + GPU Sync Failed (#43623)\r\n\r\n\r\n@Palkos83<https://github.com/Palkos83>\r\nCan you please refer to this<https://github.com/zhihou7/VCL/issues/1> and let me know if it helps.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/43623#issuecomment-700133053>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE4LTGH3G7WMIS2O6CUH5BLSICYQJANCNFSM4R4VXU7A>.\r\n", "Can you check if Cuda/Cudnn/Driver versions are ok for your card https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html#cudnn-versions-764-765 ?", "I am currently running one of the latest NVidia Drivers (456.38).\r\nAccording to the link provided for cudnn 7.6.5 I am supposed to be running 418.39 version.\r\nI cannot find one officially available on NVidia website anymore but digging through I can access the 418.81.\r\nSadly when I try to install this driver it is incompatible with the later versions of Windows.\r\n\r\nAny suggestions?\r\n\r\nOn another note, are there any plans to update TF2.3 to newer versions of CUDA and CUDNN 8?\r\nThis would make the setup much simpler.", "> On another note, are there any plans to update TF2.3 to newer versions of CUDA and CUDNN 8?\r\nThis would make the setup much simpler.\r\n\r\nDo you mean cuda 11?", "Yes, CUDA 11.", "You can use `pip install tf-nightly` packages for CUDA 11 or you need to wait for 2.4 release.", "Well firstly I tried to force windows to install 418 driver again. \r\nI extracted the installer and used Device Manager to force old Nvidia driver.\r\nSadly when that completed the TF2.3 could no longer find a CUDA device and reverted back to CPU training.\r\n\r\n\r\nSo the next thing to try was to use the tf-nightly as suggested.\r\nDrivers checked (456 and 451)\r\nCuda 11\r\nCUDNN versions checked 8.3 and 8.2\r\n\r\nAll combinations end up with this same error (identical as the original issue)\r\n\r\nTraceback (most recent call last):\r\n  File \"cnn.py\", line 44, in <module>\r\n    history = model.fit(train_images, train_labels, epochs=100, validation_data=(test_images, test_labels))\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1081, in fit\r\n    callbacks.on_train_batch_end(end_step, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 454, in on_train_batch_end\r\n    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 296, in _call_batch_hook\r\n    self._call_batch_end_hook(mode, batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 316, in _call_batch_end_hook\r\n    self._call_batch_hook_helper(hook_name, batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 356, in _call_batch_hook_helper\r\n    hook(batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1020, in on_train_batch_end\r\n    self._batch_update_progbar(batch, logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1084, in _batch_update_progbar\r\n    logs = tf_utils.to_numpy_or_python_type(logs)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 509, in to_numpy_or_python_type\r\n    return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 644, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 644, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 505, in _to_single_numpy_or_python_type\r\n    x = t.numpy()\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1071, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Palkos\\.conda\\envs\\TF-Nightly\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1039, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed\r\n\r\n\r\nAttached is also full log from the run.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5295408/log.txt)\r\n\r\nAt the moment I cannot run TF1.14, TF1.15 TF2.3 or nightly. \r\nAll of them are having issues with CUDNN and allocating buffer for the conv net.\r\nEnding up with crashing on the GPU.\r\n\r\n\r\n", "Please note also the we don't officially support conda.\r\nFollow our official installation guide at https://www.tensorflow.org/install/pip\r\n\r\nAnaconda has only third_party support https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/", "Hi \r\nSo as suggested I tried to run it directly in Python using the clean install and excluding conda for environments management.\r\nNVidia Driver : 456.43\r\nCuda 11.1\r\nCUDNN 8.2\r\nTF Nightly:2.4.0-dev20200928 \r\n\r\nAnd sadly still this same issue...\r\n\r\n2020-09-29 12:09:29.665890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_110.dll\r\n###################\r\n2.4.0-dev20200928\r\n###################\r\n2020-09-29 12:09:32.233199: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-09-29 12:09:32.234239: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-29 12:09:32.253064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-09-29 12:09:32.253151: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_110.dll\r\n2020-09-29 12:09:32.283795: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_11.dll\r\n2020-09-29 12:09:32.299236: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-29 12:09:32.304090: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-29 12:09:32.320775: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-29 12:09:32.340452: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_11.dll\r\n2020-09-29 12:09:32.342338: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_8.dll\r\n2020-09-29 12:09:32.342439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-29 12:09:32.342945: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-29 12:09:32.343381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2020-09-29 12:09:32.343467: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_110.dll\r\n2020-09-29 12:09:32.343521: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_11.dll\r\n2020-09-29 12:09:32.343579: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-29 12:09:32.343626: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-29 12:09:32.343670: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-29 12:09:32.343717: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_11.dll\r\n2020-09-29 12:09:32.343760: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_8.dll\r\n2020-09-29 12:09:32.343845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-29 12:09:32.790614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-29 12:09:32.790713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-09-29 12:09:32.790874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-09-29 12:09:32.791096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6692 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-09-29 12:09:32.791519: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n1 Physical GPUs, 1 Logical GPUs\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nconv2d (Conv2D)              (None, 30, 30, 32)        896\r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0\r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        18496\r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0\r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 4, 4, 64)          36928\r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1024)              0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 64)                65600\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                650\r\n=================================================================\r\nTotal params: 122,570\r\nTrainable params: 122,570\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n2020-09-29 12:09:33.113108: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 1)\r\nEpoch 1/100\r\n2020-09-29 12:09:33.516424: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_11.dll\r\n2020-09-29 12:09:33.745613: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_8.dll\r\n2020-09-29 12:09:34.220255: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2020-09-29 12:09:34.250003: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2020-09-29 12:09:34.282370: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-29 12:09:34.282476: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0xa0c400000 at offset 613936; expected ffffffffffffffff but was ffffffff.\r\n2020-09-29 12:09:34.366209: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-29 12:09:34.366317: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0xa0a400000 at offset 744160; expected ffffffffffffffff but was ffffffff.\r\n2020-09-29 12:09:34.421464: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-29 12:09:34.421568: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0xa0a400000 at offset 1564136; expected ffffffffffffffff but was ffffffff00000000.\r\n1562/1563 [============================>.] - ETA: 0s - loss: 2.3028 - accuracy: 0.09892020-09-29 12:09:41.214508: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-29 12:09:41.214685: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0xa0a400000 at offset 1926064; expected ffffffffffffffff but was 7fffffffffffffff.\r\n2020-09-29 12:09:41.230850: E tensorflow/core/kernels/gpu_utils.cc:85] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n2020-09-29 12:09:41.231050: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0xa08400000 at offset 318128; expected ffffffffffffffff but was ffffffff.\r\n2020-09-29 12:09:41.255493: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-09-29 12:09:41.255602: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-09-29 12:09:41.255982: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: Error destroying CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-09-29 12:09:41.256365: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-09-29 12:09:41.256667: E tensorflow/stream_executor/stream.cc:4937] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-09-29 12:09:41.257006: W tensorflow/core/kernels/gpu_utils.cc:69] Failed to check cudnn convolutions for out-of-bounds reads and writes with an error message: 'Failed to load in-memory CUBIN: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered'; skipping this check. This only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\r\n2020-09-29 12:09:41.257360: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 8B (8 bytes) from device: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-09-29 12:09:41.257660: E tensorflow/stream_executor/stream.cc:4937] Internal: Failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2020-09-29 12:09:41.258028: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.", "What is the output of \n```\nimport\u00a0sys\nprint(sys.maxsize\u00a0>\u00a02**32)\n``` ", "Python 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import sys\r\n>>> print(sys.maxsize > 2**32)\r\n**True**\r\n>>>", "Can you check free GPU RAM with `nvidia-smi`?", "Tue Sep 29 13:32:24 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 456.43       Driver Version: 456.43       CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080   WDDM  | 00000000:01:00.0  On |                  N/A |\r\n| 26%   32C    P8     7W / 180W |    521MiB /  8192MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1472    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\r\n|    0   N/A  N/A      2328    C+G   ...me\\Application\\chrome.exe    N/A      |\r\n|    0   N/A  N/A      6492    C+G   ...b3d8bbwe\\WinStore.App.exe    N/A      |\r\n|    0   N/A  N/A     11032    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A     11080    C+G   ...nputApp\\TextInputHost.exe    N/A      |\r\n|    0   N/A  N/A     11692    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n|    0   N/A  N/A     11752    C+G   ...lPanel\\SystemSettings.exe    N/A      |\r\n|    0   N/A  N/A     12916    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\r\n|    0   N/A  N/A     14440    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A     15216    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A     16120    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\r\n|    0   N/A  N/A     17648    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A     17936    C+G   ...perience\\NVIDIA Share.exe    N/A      |\r\n|    0   N/A  N/A     19796    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\r\n+-----------------------------------------------------------------------------+", "/cc @cheshire", "Hi\r\nAny hints of what might be wrong here? Any new ideas to try?\r\n\r\nMany thanks", "In the meantime can you try to see It you have the same issue inside our Docker image https://www.tensorflow.org/install/docker", "Well, in order to run the GPU in docker I need NVidia containers for Docker.\r\nThese sadly do no support Windows :(\r\n\r\n", "Yes I know but I meant in the meantime, \"between the lines\", on WSL https://github.com/NVIDIA/nvidia-docker/issues/665#issuecomment-691242564", "So to give it a try I used brand new Ubuntu 20.04 installation.\r\nDocker + Nvidia containers + 2.3.1 image (tensorflow-gpu latest)\r\n\r\nSadly the result is exactly this same as on Windows. I still try to run basic vanilla CNN network to train.\r\n\r\nLogs attached for reference:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5305933/log.txt)\r\n", "Now I see again in this log:\r\n`` tensorflow/stream_executor/gpu/asm_compiler.cc:55] Couldn't invoke ptxas.exe --version``\r\n\r\nCan you try with `tensorflow/tensorflow:nightly-gpu`?\r\n", "I just tried that as suggested.\r\nSadly this same story.\r\n\r\nLog attached:\r\n[log2.txt](https://github.com/tensorflow/tensorflow/files/5306162/log2.txt)\r\n", "Thanks for the tests. I've already mentioned @cheshire\r\nWhat driver version are you using now?", "+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |\r\n| 26%   37C    P8     7W / 180W |    554MiB /  8116MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1093      G   /usr/lib/xorg/Xorg                159MiB |\r\n|    0   N/A  N/A      1670      G   /usr/bin/gnome-shell              227MiB |\r\n|    0   N/A  N/A      2379      G   ...AAAAAAAAA= --shared-files      164MiB |\r\n+-----------------------------------------------------------------------------+", "Can you try with (456.38) or (456.55)?", "I think 450 is the latest available for Ubuntu 20.04\r\n![image](https://user-images.githubusercontent.com/20494744/94704225-9999a500-0337-11eb-8439-1a0e3d40bc27.png)\r\n", "OK lets wait for a feeback on your error: `Detected cudnn out-of-bounds write in convolution buffer!`", "@Palkos83 \r\n\r\n`nvidia-smi` shows you have CUDA 11.0 but in the `log2.txt` code is looking for CUDA10 libraries such as  `libcudart.so.10.1`, `libcublas.so.10`. May be all the old drivers or path to old drivers were not deleted completely. Last time when I had similar issue with Anaconda, I had to manually remove all the TF files and paths, restart computer, install drivers freshly, install TF. Hope it helps. Thanks!\r\n\r\n> \r\n> +-----------------------------------------------------------------------------+\r\n> | NVIDIA-SMI 450.66 Driver Version: 450.66 CUDA Version: 11.0 |\r\n> |-------------------------------+----------------------+----------------------+\r\n> | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n> | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r\n> | | | MIG M. |\r\n> |===============================+======================+======================|\r\n> | 0 GeForce GTX 1080 Off | 00000000:01:00.0 On | N/A |\r\n> | 26% 37C P8 7W / 180W | 554MiB / 8116MiB | 0% Default |\r\n> | | | N/A |\r\n> +-------------------------------+----------------------+----------------------+\r\n> \r\n\r\n**from your log2.txt**\r\n```\r\n2020-09-30 13:41:07.059435: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 13:41:07.059531: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-30 13:41:07.059632: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-30 13:41:07.059732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-30 13:41:07.059819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-30 13:41:07.059904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n```\r\n", "Thanks for comments but sadly this is not going to be it.\r\nThe nvidia-smi command is running on the host OS ubuntu, the log files are from docker container.\r\nThat is why versions are slightly different. The output of the nvidia-smi command was to report the nvidia driver version used.\r\n\r\n\r\n\r\n", "Hi guys, \r\nAny progress on this?\r\n", "Is there anything else I can try?\r\nAny fixes I could test in the nightly builds?", "@jvishnuvardhan Can you check if you can re-route this?", "Tim, can you PTAL?", "@sanjoy, @jvishnuvardhan  - Any help, still struggling with running cudnn.", "Does the problem reproduce with tf-nightly?  If not, then this could be a cuDNN issue fixed in cuDNN 8.  If yes, then can you please attach the cuDNN [API log](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-logging) so that folks from NVIDIA can take a look?\r\n\r\nCC @kaixih @nluehr ", "Well I tried the latest nightly build and still fails but there is a bit more information this time:\r\n\r\n**2020-10-13 11:14:18.331536: E tensorflow/core/kernels/gpu_utils.cc:93] Redzone mismatch in RHS redzone of buffer 0x70ac00000 at offset 4835120; expected ffffffffffffffff but was 7fffffffffffffff.\r\n1563/1563 [==============================] - ETA: 0s - loss: 2.3029 - accuracy: 0.09612020-10-13 11:14:18.498882: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer**\r\n\r\n.....\r\n\r\n\r\n **476/1563 [========>.....................] - ETA: 11s - loss: 2.3026 - accuracy: 0.10442020-10-13 11:21:17.408864: F .\\tensorflow/core/kernels/conv_2d_gpu.h:1019] Non-OK-status: GpuLaunchKernel( SwapDimension1And2InTensor3UsingTiles<T, kNumThreads, kTileSize, kTileSize, conjugate>, total_tiles_count, kNumThreads, 0, d.stream(), input, input_dims, output) status: Internal: an illegal memory access was encountered**\r\n\r\nAttached is also the log from the entire run \r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5370725/log.txt)\r\n\r\nCUDNN Nvidia logs can be accessed here: (200MB compressed)\r\nhttps://drive.google.com/file/d/1jcB-VlBObleCoCz3WH2vj1PL5cpDkyZt/view?usp=sharing\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "I suffer from this error for several month.But today,I found that just let the size of every batch feed into the model being same,everything is going fine.\r\nMy nvidia-smi output\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.64       Driver Version: 430.64       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:81:00.0 Off |                  N/A |\r\n|  0%   56C    P0    66W / 300W |     10MiB / 11016MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n", "@ZoloftHuang, many thanks for your message.\r\nEarlier in the post I have attached the vanilla cifar10 CNN script I am using and you should see there that I am not playing with the batch size at all during training. So unless model.fit is doing this under the hood all batch sizes I use for training should be of this same size.\r\n\r\n", "> @ZoloftHuang, many thanks for your message.\r\n> Earlier in the post I have attached the vanilla cifar10 CNN script I am using and you should see there that I am not playing with the batch size at all during training. So unless model.fit is doing this under the hood all batch sizes I use for training should be of this same size.\r\n\r\nI run your code in my machine,same error occur.But when I add argument <batch_size=16> to model.fit and model.evalutate,everything is ok.The default vaule of <batch_size> is 32,and size of train dataset is 50000\uff0c50000/32=1562.5\uff01It looks like a same problem with me.", "Many thanks @ZoloftHuang,\r\n\r\nI tried your suggestion to reduce the batch size to 16 but on my setup this do not seem to work. \r\nI run this with the nightly build and standard 2.3.1 both of them failed.\r\n\r\nUpdated script:\r\n\r\n[cnn.zip](https://github.com/tensorflow/tensorflow/files/5383570/cnn.zip)\r\n\r\n\r\nLogs:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5383571/log.txt)\r\n\r\n\r\nWhat version of tensorflow, cuda, cudnn and nvidia driver are you using?\r\n", "Guys - any progress on this issue?\r\nAny news from Nvidia logs I provided?", "Anyone - anything? \r\nThis issue has been opened for nearly a month now and I am unable to train any CNN based networks at the moment which begun to have serious impact on my project.\r\n\r\nAny hints would be greatly appreciated.", "I could not reproduce the failure with Linux + CUDA 10.1 + cuDNN 7.6.5 + GTX 1080 + CUDA driver 455.23.04 + Python 3.8 + TF 2.3.1. I don't have access to exactly your setup.\r\n\r\nCan you run with the environment variable `TF_DISABLE_RZ_CHECK=1\" and attach the log?\r\n\r\nHow about other CUDA applications, other than Tensorflow? Do they run normally with the same Machine + GPU + OS + CUDA environment?", "I can run other cuda applications (like oceanFFT, randomFog, nbody from cuda samples with no issues, even in parallel).\r\n\r\nSadly even setting the TF_DISABLE_RZ_CHECK=1 did not help. It run for a bit longer but failed in the end again.\r\n\r\nI have updated to the latest TF-Night - and the issue with the layout optimiser is no longer displayed.\r\nHowever cuda/cudnn fails again.\r\n\r\nLog attached.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5422087/log.txt)\r\n", "Can you try turning on XLA? Namely:\r\n    tf.keras.backend.clear_session()\r\n    tf.config.optimizer.set_jit(True)\r\n\r\nAlso, with XLA, combine it with environment variable:\r\n  XLA_FLAGS=\"--xla_gpu_autotune_level=1\"\r\nand try all levels of 0, 1, 2, 3, 4.\r\n\r\nAlso, pass env var CUDA_LAUNCH_BLOCKING=1 for with/without XLA.", "@timshen91 \r\nI will try that later on today.\r\nIn the meantime once again I tried to reproduce working environment using ubuntu.\r\nFresh Ubuntu 20.04 installation.\r\nNVidia Driver 455.34.04.\r\nCuda 10.1 \r\nCUDNN 7.6.5\r\nTensorflow 2.3.1\r\nPython 3.8\r\n\r\nSo environment identical to yours.\r\n\r\nAgain failed with GPU sync. It is beyond my understanding why that is the case, even on clean systems!\r\n\r\nLogs attached:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5423939/log.txt)\r\n", "@timshen91 \r\nI tried your suggestion with XLA, sadly none of the suggested combinations made the system to work.\r\nLog attached:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5426207/log.txt)\r\n\r\n", "@timshen91 \r\nHi\r\n\r\nAny comments from XLA logs?", "XLA seems promising. Take a look:\r\n\r\n```\r\nCan't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\r\nSearched for CUDA in the following directories:\r\n  ./cuda_sdk_lib\r\n  C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0\r\n  .\r\nYou can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\r\n```\r\n\r\nTry CUDA 10.1. Also try passing `XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda` as env var. Also, try with `--xla_gpu_autotune_level=0`, namely:\r\n  XLA_FLAGS=\"--xla_gpu_cuda_data_dir=/path/to/cuda --xla_gpu_autotune_level=0\"\r\nas a single string env var.\r\n\r\n\r\nBesides all of these, from an ML user's perspective, seeing these errors, I think something is seriously off somewhere outside of the docker image (the docker image is tested throughout the community), which leaves us:\r\n* The program and data,\r\n* the kernel (WSL? and the Ubuntu you used),\r\n* and the hardware.\r\n\r\nI'd start bisect the problem ordered by level of difficulty. Maybe try a few different example models with different data, or use a different GPU/mobo/pcie slot, but nothing is for sure until tried, and it's out of the scope of this github issue.", "@timshen91 \r\nMany thanks for your response. I do not think this is OS/Kernel issue as I am trying Ubuntu 20.04 and Windows 10 Pro swapping between them. I am not sure if the hardware is an issue either as I have been training on that rig on TF 1.12, 1.13 and 2.2 in the past. Hardware was not touched since then. I cannot try another GPU as I have access to only one device.\r\n\r\nI managed to fix the issue with not finding the ${CUDA_DIR}/nvvm/libdevice by uninstalling and installing CUDA 11.0 again. That seems to have fixed the loading problem. \r\n\r\nNow I can run the training - sometimes. Some other time it fails with original SYNC issue or just breaking in the middle of the training. I would estimate that the chance of success is 50/50 at the moment.\r\n\r\n**I attached new XLA logs:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5448302/log.txt)**\r\n\r\nAnd would like to ask about what could be the reason of the following log entries:\r\n\r\n2020-10-27 21:46:07.453043: E tensorflow/core/kernels/gpu_utils.cc:86] Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n\r\n...\r\nE tensorflow/core/kernels/gpu_utils.cc:94] Redzone mismatch in RHS redzone of buffer 0x809c00000 at offset 816688; expected ffffffffffffffff but was ffffffff.\r\n\r\n**This looks like you are expecting 64bit data back from the card and you only get 32 bit answer. Any hints why?**\r\n\r\n....\r\n\r\n2020-10-27 21:46:07.665035: E tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:238] Detected cudnn out-of-bounds write in conv scratch buffer! This is likely a cudnn bug. We will skip this algorithm in the future, but your GPU state may already be corrupted, leading to incorrect results. Within Google, no action is needed on your part. Outside of Google, please ensure you're running the latest version of cudnn. If that doesn't fix the problem, please file a bug with this full error message and we'll contact nvidia.\r\n\r\n....\r\n\r\n2020-10-27 21:46:07.675856: E tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:675] Difference at 30012: 30.9046 vs 26.3584\r\n\r\nWhat does it mean? What is being compared here.\r\n\r\n\r\nSadly I have no confidence in the system at the moment. Even if it trains it feels like random number generator. Especially when some of the epochs are not finishing like this one:\r\n\r\n3122/3125 [============================>.] - ETA: 0s - loss: 2.3033 - accuracy: 0.09762020-10-27 21:46:18.972598: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\nAny ideas what to try next?\r\n\r\n\r\n\r\n\r\n\r\n", "@Palkos83 These logs usually mean that we've detected a cuDNN bug.  Can you try TF nightly with CUDA 11 and cuDNN 8.0.4?", "@sanjoy \r\nThe last log I provided was with latest TFnightly (2.4.0-dev20201023)  cudnn **8.0.2** (I can try with **8.0.4** if you think that will help) and Cuda 11.0", "@Palkos83 Yes try with 8.0.4", "@sanjoy \r\nPlease see attached log after updating to CuDNN 8.0.4.\r\nFirstly - the system failed again, this same errors are showing up as with 8.0.2.\r\nWhat is more if you look a the epoch timings it is now getting progressively slower (each epoch is slower than the previous one) and the GPU usage is getting lower and lower (Starting from 23% falling all the way to 1% before crash).\r\n\r\n**Logs attached:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5465893/log.txt)**\r\n\r\nCan you please explain what these errors mean:\r\nRedzone mismatch in RHS redzone of buffer 0x809c00000 at offset 450224; expected ffffffffffffffff but was be0c2ff6ffffffff.\r\n\r\nand \r\n\r\n2020-10-30 14:23:46.555920: E tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:675] Difference at 6383: 29.7886 vs 26.4336\r\n\r\nThanks", "@sanjoy, @timshen91 \r\n\r\nGuys, any feedback on the questions above? Any ideas what to try next?", "Guys - so there is no way to fix this? \r\nAny comments about the errors from the logs?\r\nAny feedback from Nvidia about CuDNN issues?", "Hi,\r\n\r\nThis issue is still open and there ha been not feedback on it in last two weeks.\r\nIs that a bug that is being worked one? When the fix can be expected?", "@Palkos83 Sorry none of us can reproduce your issue. The error message you are quoting (\"redzone mismatch\") indicates a cudnn bug.", "@Palkos83 It is quite possible that you have stumbled into a hardware issue, and your GPU card is broken, given that all configurations fail with the same error. Pointing out that other versions have worked is not really helpful since they might have been running different convolutions configurations. \r\n\r\nClosing as not reproducible.", "This is extremally disappointing response. Why would that be a hardware issue? The card is running games in full details in 4k, runs cuda examples with cudnn and fft with no issues. If that is an issue with cudnn why not raise that with NVidia? \r\n\r\n", "Palkos83@, see my previous comment per: `but nothing is for sure until tried, and it's out of the scope of this github issue.`\r\n\r\nTo me this issue was closed under \"not reproducible\". A TF issue is not a forum thread calling for arbitrary helps. We need reproducible examples to act on. It's your job to give us something actionable.", "Well, I thought I did. I run everything you asked me to. Provided detailed debug NVidia logs (no feedback on these yet), changed drivers, reinstalled the operating system swapped between windows and ubuntu, run direct installations and your docker image. \r\n\r\nNone of the suggestions you came up with worked so far.\r\n\r\nI provided very precise script and error log messages as well (See above) with questions about what is really the issue there but no precise answers so far. \r\n\r\nThe issue is reproducible on my rig and I can run whatever diagnostics and/or local debug build you want to find out what the issue is. If you have some diagnostic software to run that can prove this is hardware issue, just tell me what to run!\r\n\r\nBut sorry saying it is hardware or cuda without really getting to the bottom of the issue is a bit dismissive response, don't you think?\r\n", "@Palkos83 Can you, in parallel, open a ticket to Nvidia at https://developer.nvidia.com/developer-program mentioning this one?", "> If you have some diagnostic software to run that can prove this is\nhardware issue, just tell me what to run!\n\nThe redzone checker which prints the error message you are seeing is that\ndiagnostic software.\n\nOn Tue, Nov 17, 2020 at 12:55 PM bhack <notifications@github.com> wrote:\n\n> @Palkos83 <https://github.com/Palkos83> Can you, in parallel, open a\n> ticket to Nvidia at https://developer.nvidia.com/developer-program\n> mentioning this one?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43623#issuecomment-729208481>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGH7LAURTZVLF5JV5OIDSQLPNXANCNFSM4R4VXU7A>\n> .\n>\n", "@bhack  - I have raised the issue with Nvidia as suggested (https://developer.nvidia.com/nvidia_bug/3182499) \r\n\r\n@cheshire  - Many thanks for the answer. Just out of curiosity could you please explain what the redzone is and why the error reported with it is considered a hardware issue? In my understanding it could be other things as well like failing to synchronise after cuda kernel execution, cuda kernel compilation issue leading to wrong pointers or different data type (float32 vs float64 as one of the above errors seems to indicate). \r\n\r\nI really would like to be sure that this is a hardware problem before I invest in a new GPU.", "> Just out of curiosity could you please explain what the redzone is and why the error reported with it is considered a hardware issue? \r\n\r\nRedzone checker is essentially a \"poor man's address sanitizer\". After every convolution performed by cudnn, we check the \"redzone\" around the output buffer for changes. If the \"redzone violation\" occurs, it means that cudnn has written the code outside of the boundary, which usually indicates a cudnn bug. Since we can't repro it, it also might be a hardware bug.\r\n\r\nYou can also try running all these examples under `cuda-memcheck`  (https://docs.nvidia.com/cuda/cuda-memcheck/index.html) it might print some more useful information", "Disclaimer: the redzone checker error does not *necessarily imply* that the card has issues.\r\n\r\nAnd why do you need a new card to check? Would not it be easier to use one of the cloud providers and check the same program there first?", "@cheshire Many thanks for suggestions I will run the memcheck tool and send the logs when available.\r\nI am sure the script is fine as the test script I am running is very basic, standard CNN training of the CIFAR dataset. This is used in many tutorials and examples. \r\n\r\nI also thought that it was mentioned earlier that the issue was not reproducible so others must have run it successfully as well.\r\nThat would mean it is not the script that is the issue.\r\n\r\nOn top of that, I have attempted to run other scripts (GANs, LSTMs based networks etc).\r\nAll networks which are trying to use convolution fail.\r\n", "Apologies for late reply but last few days I was trying to get some sense out of the logs from the cuda memcheck tool.\r\nAttached is the log file from one of the runs which shows the issue.\r\nMaybe you guys will have more luck.\r\n\r\nPlease let me know if you found something interesting:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/5580276/log.txt)\r\n", "Here is another log:\r\n[log2.txt](https://github.com/tensorflow/tensorflow/files/5580279/log2.txt)\r\n\r\nStrange thing is that first one failed on queuing memset operation while the second one on initialising BLAS.", "```\r\n========= Invalid __shared__ read of size 0\r\n=========     at 0x000040a8 in void implicit_convolve_sgemm<float, float, int=128, int=5, int=5, int=3, int=3, int=3, int=0, bool=1, bool=1, bool=1>(int, int, int, float const *, int, float*, float const *, kernel_conv_params, __int64, int, float, float, int, float const *, float const *, bool, int, int)\r\n=========     by thread (5,1,0) in block (60,1,0)\r\n=========     Address 0x00000034 is out of bounds\r\n```\r\n\r\nThe error message is pretty self-descriptive, it's either a cudnn bug or (more likely, since no one can repro) a hardware issue.", "Well, to me that more looks like some code is trying to access 0 size array or trying to access not allocated GPU memory which could be down to an error which happened earlier:\r\n**Failed to enqueue async memset operation: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure**\r\n\r\nIt well may be an issue with the cudnn but I am not sure the issue indicates hardware failure.", "@Palkos83, i run into similar issue using CUDA 11.0, cuDnn 8.0.4 with tensorflow 2.4. Surprisingly, when i run \"nvidia-smi\" in cmd, the CUDA version reads 11.2. Probabily these new release ain't stable. I switched back to my old configurations (CUDA 10.1, cuDnn 7.6, tensorflow 2.2) and i was able to run your cnn.py.", "Hi @Isaac45 \r\n\r\nMany thanks for the message. I am glad I am not the only one with the problem.\r\nOut of curiosity, what version of the Nvidia driver you are using with your old configuration?\r\n\r\nAll the best", "@Palkos83 I'm on Turing RTX 2070 running NVIDIA Graphics Driver 460.89.\r\n\r\n![image](https://user-images.githubusercontent.com/22303833/103120004-db28ba80-46b0-11eb-8057-3bc0911d2f1c.png)\r\n", "Thanks for that @Isaac45, I will give it a try.", "Hi @Palkos83, I've stumbled upon the same error. I'm on Arch Linux and I've got the error either with **cuda 10.2.89-5 + cudnn 7.6.5.32-4 + tensorflow-cuda 2.2** or **cuda  11.2.0-3 + cudnn 8.0.5.39-1 + tensorflow-cuda 2.4.0-2**. \r\n\r\nNvidia Driver version is 455.45.01.\r\n\r\nHave you tried to @Isaac45 suggestion?", "Hi @iLeW, @Isaac45 \r\n\r\nApologies for late reply, I had only just been able to try it.\r\nSo I reverted back to Tensorflow 2.2 (I tried 2.2.0 and recently released 2.2.2)\r\n\r\nSadly both of them still fail. \r\n\r\nFunny enough once on the TF2.2.0 the script actually worked,  but on subsequent runs failed again.\r\nDid you manage to run it more than once?\r\n\r\nI am not sure why this issue is marked as closed as clearly we have no resolution and if anything more people are reporting the problem.\r\n", "@Palkos83 [this](https://gist.github.com/sanjoy/2b1fd1d644f8ff003a0c3405e6be4b10) is what I see in the log you attached in https://github.com/tensorflow/tensorflow/issues/43623#issuecomment-731841457.\r\n\r\nThe failing kernel is coming from a cuDNN call, so it is likely either a cuDNN bug or a HW issue, as @cheshire said.  Can you please follow up on the NVIDIA forums?  They're in a better position to help you.", "> Hi @iLeW, @Isaac45\r\n> \r\n> Apologies for late reply, I had only just been able to try it.\r\n> So I reverted back to Tensorflow 2.2 (I tried 2.2.0 and recently released 2.2.2)\r\n> \r\n> Sadly both of them still fail.\r\n> \r\n> Funny enough once on the TF2.2.0 the script actually worked, but on subsequent runs failed again.\r\n> Did you manage to run it more than once?\r\n> \r\n> I am not sure why this issue is marked as closed as clearly we have no resolution and if anything more people are reporting the problem.\r\n\r\nHi @Palkos83, thank you for your reply. I'm still having the issue. Maybe we should follow up on the NVIDIA forums as @sanjoy suggested and as you have already done.", "I already tried to contact them and rightly so as they do not see the code which is using their library, they cannot reproduce the issue. Personally I doubt this is hardware failure, especially now that multiple problem raised that and on different cards. My suspicion is that there is some misalignment in syncing between device and pc at some point or the memory allocation on the GPU fails from whatever reason and this is not being handled properly in TensorFlow. \r\n\r\nBut without debugging the source code myself I have no way of proving that. \r\n\r\nThere is always a possibility to use different framework (pytorch) but it would be a big shame to have to rewrite everything just to get this working, especially that Tensroflow was a great framework for me for years.\r\n\r\nAnyhow, I am stuck at the moment.\r\n", "Hi Palkos83,\n\nI've got a way around it at my end, it might be helpful to you. The error\nis related to excessive memory usage.\n\n1. Append this just after your imports\n\nimport tensorflow as tf\nconfig = tf.compat.v1.ConfigProto(allow_soft_placement=True)\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.3\ntf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n\n2. I also realized that the multiple tabs open in my browser were also\nrunning on my GPU, try closing that or restricting it from using the GPU.\n\n3. From what I experienced, I could train my model halfway through the\nspecified epochs before the error was thrown. So what I did was to train\nthe model in kinda batch using \"tf.keras.callbacks.ModelCheckpoint\"\n<https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint>\nwhich allows training to some point, saving and later picking up training\nright from the save point.  This \"\nhttps://www.tensorflow.org/tutorials/keras/save_and_load \" should guide you.\nBest of luck.\n\nOn Fri, Jan 8, 2021 at 12:51 PM Palkos83 <notifications@github.com> wrote:\n\n> I already tried to contact them and rightly so as they do not see the code\n> which is using their library, they cannot reproduce the issue. Personally I\n> doubt this is hardware failure, especially now that multiple problem raised\n> that and on different cards. My suspicion is that there is some\n> misalignment in syncing between device and pc at some point or the memory\n> allocation on the GPU fails from whatever reason and this is not being\n> handled properly in TensorFlow.\n>\n> But without debugging the source code myself I have no way of proving that.\n>\n> There is always a possibility to use different framework (pytorch) but it\n> would be a big shame to have to rewrite everything just to get this\n> working, especially that Tensroflow was a great framework for me for years.\n>\n> Anyhow, I am stuck at the moment.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43623#issuecomment-756934277>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFKFIWKPXXFDHGH3RMUJ23LSY5H23ANCNFSM4R4VXU7A>\n> .\n>\n", "@Isaac45 \r\nMany thanks for suggestions to walk around the problem. Nevertheless this feels now as if I had to drive a car with engine dying every 2 miles. This is not sustainable for my work.\r\n\r\nI just tried few simpler scripts with pytorch today and it is running fine. So I am further convinced the problem is not with hardware but as stated before could be some edge condition shown with the latest TF and Cudnn. \r\n\r\nI am getting nowhere with Nvidia, and I am not surprised. They have no idea how Cudnn is used and are not that keen on debugging someone else's python code. \r\n\r\nSo I do not really have a choice here....\r\n\r\nMany thanks for help and good luck with your projects.", "> @Isaac45\r\n> Many thanks for suggestions to walk around the problem. Nevertheless this feels now as if I had to drive a car with engine dying every 2 miles. This is not sustainable for my work.\r\n> \r\n> I just tried few simpler scripts with pytorch today and it is running fine. So I am further convinced the problem is not with hardware but as stated before could be some edge condition shown with the latest TF and Cudnn.\r\n> \r\n> I am getting nowhere with Nvidia, and I am not surprised. They have no idea how Cudnn is used and are not that keen on debugging someone else's python code.\r\n> \r\n> So I do not really have a choice here....\r\n> \r\n> Many thanks for help and good luck with your projects.\r\n\r\nAn update from here. It seemed to be a mixed problem of many conditions on specific hardware. Instead, I think that the `Detected cudnn out-of-bounds write in convolution buffer! This is likely a cudnn bug` message is not blocking. I have managed to work on the sizes of my data and now everything works. \r\n\r\nHowever, just to be noted, the message appears when I run the code on an Nvidia RTX 2080Ti and does not appear on an Nvidia 1060.", "@Palkos83 can you try running with the env var `TF_DISABLE_RZ_CHECK` set to `1`?  This is a \"break glass\" mechanism that disables the red zone checker which we generally don't recommend, but may unblock you.\r\n\r\nIn terms of helping NVIDIA triage this issue, you should be able to share the [cuDNN logs](https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-logging) with them.  With the logs they are typically able to \"replay\" the cuDNN library calls and hopefully reproduce the issue on their end.  When you reach out to NVIDIA please also mention the error you see with `cuda-memcheck`, that's a smoking gun IIUC."]}, {"number": 43622, "title": "divide_no_nan logic", "body": "The [divide_no_nan()](https://www.tensorflow.org/api_docs/python/tf/math/divide_no_nan) function by definition \"Computes a safe divide which returns 0 if the y is zero\".\r\n\r\nMy question is, wouldn't it make more sense to return the maximum value of dtype of the passed tensors? When dividing by very small positive numbers we get very large positive numbers, after all.\r\nIt is, of course, possible to apply `clip_by_value()` to the denominator first, and then simply use `divide()`, but that's probably less efficient and straightforward?", "comments": ["The answer you suggest to return is only valid if you divide a positive number to a positive number going towards 0 (or a negative number divided to a negative number that is close to 0).\r\n\r\nHowever, there is also the case where the operands have different signs. In this case, you have to return the minimum value of the type of the result (akin to -infinity).\r\n\r\nFinally, there is the case where both numbers are very small. Kind of 0/0. This is the only operation which would produce a nan, actually", "Fair! So maybe it makes sense to cover these cases - maybe not within this function actually (otherwise, its name would be misleading), but as a separate one - say, `safe_divide()`?", "We also don't want to blow up the API surface. It's better for the callers to just use `tf.where` to filter out these cases beforehand.", "Closing this issue for now Feel free to revisit if necessary. Thank you."]}, {"number": 43621, "title": "Tensorflow 2.3 doesn't log batch metrics and learning rate in Tensorboard", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14 or Ubuntu 18.04.3 (bug happens on both platforms)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nope\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.3 (bug) / 2.2 (good behavior)\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI used to make custom training loops to fit my models, with a tensorboard callback to log metrics, losses and learning rate using the following command:\r\n\r\n```python\r\n    callbacks.append(tf.keras.callbacks.TensorBoard(\r\n        tensorboard_dir,\r\n        profile_batch=0,\r\n        update_freq=2, # write in tensorboard metrics and losses every 2 batches\r\n        write_graph=False)\r\n    )\r\n```\r\n\r\nNow, after upgrading tensorflow from 2.2 to 2.3, the same code doesn't log in tensorboard the metrics and losses one every 2 batches, and also the learning rate for each epoch (that I manage either by using `tf.keras.callbacks.LearningRateScheduler`\r\n or `tf.keras.callbacks.ReduceLROnPlateau`).\r\n\r\nI shared a link below to a Colab with a minimal example, and the tensorboard for a model trained in 2.2 and in 2.3.\r\n\r\n**Describe the expected behavior**\r\n\r\nSame behavior in tf 2.3 as in tf 2.2.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/16G5b0wfcG0SuNWrbQbv1pQfPbk6Ix9qU#scrollTo=7tuJE8iW_-NY\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["It seems the behavior has been introduced by this commit: https://github.com/tensorflow/tensorflow/commit/69565ec4003902794bc94e10ba5fe9469a0b3ae4\r\n\r\nThe `self._log_metrics(logs, prefix='batch_', step=train_batches)` has been removed. For epoch logging this method `def _log_epoch_metrics(self, epoch, logs)` has been introduced.\r\n\r\n It seems that there should have an automatic update with:\r\n\r\n```python\r\nif self.update_freq == 'epoch':\r\n      should_record = False\r\n      writer = None\r\n    else:\r\n      should_record = lambda: math_ops.equal(step % self.update_freq, 0)\r\n\r\n    summary_state.is_recording = should_record\r\n    summary_state.writer = writer\r\n    # TODO(b/151339474): Fix deadlock when not using .value() here.\r\n    summary_ops_v2.set_step(step.value())\r\n```\r\nHowever, I really don't know how it works.\r\n\r\n@omalleyt12 do you have any information about this issue ?\r\nThanks", "Was able to reproduce the issue with TF v2.3 and TF-nightly, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5151b55dfb1341da1329f858c3c49a8d/43621-tf-nightly.ipynb). Thanks!", "> I used to make custom training loops to fit my models, with a tensorboard callback to log metrics, losses and learning rate using the following command:\r\n\r\nAh I understand now, the `TensorBoard` callback is designed for use with `Model.fit`. It still logs batch-level metrics when used in `Model.fit`. For a custom training loop, you should use lower-level `tf.summary` APIs to log batch-level summaries. The change from 2.2 to 2.3 made the batch-level summaries part of the `Model.train_function` rather than something that the `TensorBoard` callback creates itself. This resulted in a 2x improvement in speed for many small models in `Model.fit`, but it does have the side effect that calling `TensorBoard.on_train_batch_end(my_batch, my_metrics)` in a custom training loop will no longer log batch-level metrics ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43621\">No</a>\n"]}, {"number": 43620, "title": "Tensorflow session.run command 20% slower in tf2.3 compared to tf1.15 ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yea but an extremely simple one.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary):**  Binary\r\n- **TensorFlow version (use command below):** tf1.15.0 and tf2.3.0\r\n- **Python version:** python v3.6.12 and python 3.8\r\n- **CUDA/cuDNN version:** CUDA  v11.0/ cuDNN V9.1.85\r\n- **GPU model and memory:** GPU: NVIDIA Quadro P1000 / Intel UHD Graphics 630 - Memory: 2x Samsung M471A4G43MB1-CTD\r\n\r\n**tf_env_collect tool output:**\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5292801/tf_env.txt)\r\n\r\n### Describe the current behaviour\r\n\r\nI'm currently converting my tf1.15 scripts to tf2.3 eager execution scripts. I did this according to the [original migration documentation](https://www.tensorflow.org/guide/migrate). After running the automatic conversion script, however, I noticed that the converted `tf2.3` version is considerably slower than the `tf1.15` version. I used to profiler to find the cause of this increased execution time and found out that although the script is exactly the same the `sess.run` method takes longer to execute in `tf2.3`. As can be seen from the results below when `tf2.3` is used the script takes 20% longer to execute. For this simple script, the difference is not much but for my original script which runs for about 1 hour, this difference really starts to add up.\r\n\r\n#### Tf1.15 result\r\n\r\n**Script execution time:** 125.98906636238098\r\n\r\n**Cprofiler file:**\r\n\r\n[tf_115_cprof.zip](https://github.com/tensorflow/tensorflow/files/5293014/tf_115.zip)\r\n\r\n**Spyder profiler file:**\r\n\r\n[tf115.zip](https://github.com/tensorflow/tensorflow/files/5293107/tf115.zip)\r\n\r\n#### tf2.3 result\r\n\r\n**Script execution time:** 157.48224687576294\r\n\r\n**Cprofiler file:**\r\n\r\n[tf_23_cprof.zip](https://github.com/tensorflow/tensorflow/files/5293016/tf_23.zip)\r\n\r\n**Spyder profiler file**\r\n\r\n[tf23.zip](https://github.com/tensorflow/tensorflow/files/5293104/tf23.zip)\r\n\r\n#### Spyder profiler difference screenshot\r\n\r\nAs can be seen from the screenshot below. It looks like the biggest difference can be found in the [flatten_dict_items](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/util/nest.py#L414) and [FetchHandler.__init__](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/client/session.py#L474) methods (Left is `tf2.3` and right `tf1.15`).\r\n\r\n![image](https://user-images.githubusercontent.com/17570430/94447852-8e653e80-01aa-11eb-86f9-bc4d79d1aadf.png)\r\n\r\nWhen we dive deeper we can see these methods take longer since the [__hash__](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/framework/ops.py#L826) method takes longer. In the [__hash__](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/framework/ops.py#L826) method of `tf2.3` python spends a long time executing the [executing_eagerly_outside_functions](https://github.com/tensorflow/tensorflow/blob/02159bbe1f87638bb6cde6a6f4aaa2fc0362e53b/tensorflow/python/framework/ops.py#L5741) method.\r\n\r\n![image](https://user-images.githubusercontent.com/17570430/94450330-6e834a00-01ad-11eb-9b54-3c53b7f69db4.png)\r\n\r\nThese methods take longer since they are executed using the `executing_eagerly_outside_functions`.\r\n\r\n### Describe the expected behaviour\r\n\r\nI expected the execution time to be equal in both `tf1.15` and `tf2.3`.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nThe performance difference can be seen when running the following script in both tf1.15 and tf2.3:\r\n\r\n```python\r\n\"\"\"Script used to compare the execution time difference when running similar code in\r\ntensorflow 1.15 and tf2.3.\r\n\"\"\"\r\n\r\nimport time\r\nimport os\r\n\r\nfrom packaging import version\r\nimport tensorflow as tf\r\n\r\n# Script settings\r\nUSE_GPU = False\r\nN_STEPS = 1e5\r\n\r\n# Disable GPU if requested\r\nif not USE_GPU:  # NOTE: This works in both TF115 and tf2\r\n    # tf.config.set_visible_devices([], \"GPU\")\r\n    if version.parse(tf.__version__) > version.parse(\"1.15.4\"):\r\n        tf.config.set_visible_devices([], \"GPU\")\r\n    else:\r\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n    print(\"Tensorflow is using CPU\")\r\nelse:\r\n    print(\"Tensorflow is using GPU\")\r\n\r\n# Disable eager execution\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# Create session\r\nsess = tf.compat.v1.Session()\r\n\r\n# Setup placeholders\r\nt = 0  # Time\r\nx = tf.compat.v1.placeholder(tf.float32, [None, 1], \"x\")\r\ny = tf.math.sin(x, name=None)\r\n\r\n# Run session in loop\r\nprint(\"===TF115 speed test script===\")\r\nprint(f\"Running a tf session {int(N_STEPS)} times to test the execution speed.\")\r\nprint(\"Starting sess run loop.\")\r\nt1 = time.time()\r\nfor i in range(0, int(N_STEPS)):\r\n\r\n    # Run result in session\r\n    y_val = sess.run(y, feed_dict={x: [[t]]})\r\n\r\n    # Increment time\r\n    t += 1\r\n\r\n    # print current step\r\n    if i % 10000 == 0:\r\n        print(f\"Performed {i} steps.\")\r\n\r\n# Print end result\r\nprint(\"sess run loop stopped.\")\r\nprint(\"Running time: \", time.time() - t1)\r\n```\r\n\r\nI also created a simple [test repository](https://github.com/rickstaa/tf23-sess-run-performance-slowdown) that includes this script and a guide on how to run it.\r\n\r\n\r\n## Other info and logs\r\n\r\n**Is this fixed in tf_nightly:** No the script also takes 160 seconds in tf_nightly.", "comments": ["The performance characteristics of \"compat.v1\" APIs, especially those related to sessions, in 2.x releases is liable to change. We would recommend that you upgrade to the new 2.x APIs, including using eager execution and tf.function: https://www.tensorflow.org/guide/migrate#1_replace_v1sessionrun_calls", "@karmel Thanks for the clarification! I opened this issue as a performance bug report while I already translated my code to tf2 eager mode. I noticed that the new version with tf.function is faster than the original TF1 version. "]}, {"number": 43619, "title": "Error converting Tensorflow saved model to TFLite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow (command line)\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"tf_proximity_saved_model\")  # \"tf_proximity_saved_model\" is the name of my model\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\nerror: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\ntf.Cast {Truncate = false, device = \"\"}\r\nTraceback (most recent call last):\r\nFile \"/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 199, in toco_convert_protos\r\nenable_mlir_converter)\r\nFile \"/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\nenable_mlir_converter)\r\nException: <unknown>:0: error: loc(fused[\"sequential/Cast@__inference__wrapped_model_40145\", \"StatefulPartitionedCall/sequential/Cast\"]): 'tf.Cast' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"sequential/Cast@__inference__wrapped_model_40145\", \"StatefulPartitionedCall/sequential/Cast\"]): see current operation: %0 = \"tf.Cast\"(%arg0) {Truncate = false, device = \"\"} : (tensor<?x1xf64>) -> tensor<?x1xf32>\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\ntf.Cast {Truncate = false, device = \"\"}\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<?x1xf64>): // no predecessors\r\n%cst = \"std.constant\"() {value = dense<[0.136683047, 0.000000e+00, -1.8840152, 0.000000e+00, 0.000000e+00, 0.000000e+00, 3.257740e-01, 0.000000e+00, 0.000000e+00, 0.000000e+00]> : tensor<10xf32>} : () -> tensor<10xf32>\r\n%cst_0 = \"std.constant\"() {value = dense<[0.000000e+00, 1.69775057, 0.000000e+00, 0.000000e+00, -0.638654768, -1.93405676, -1.95157754, -0.0406359173, 1.72332501, -1.7537955]> : tensor<10xf32>} : () -> tensor<10xf32>\r\n%cst_1 = \"std.constant\"() {value = dense<1.7260325> : tensor<1xf32>} : () -> tensor<1xf32>\r\n%cst_2 = \"std.constant\"() {value = dense<[[0.00675311219], [0.341173232], [-0.481861204], [0.701621353], [0.0275682211], [0.0613476038], [-0.44652465], [0.166085422], [0.189539731], [0.020105958]]> : tensor<10x1xf32>} : () -> tensor<10x1xf32>\r\n%cst_3 = \"std.constant\"() {value = dense<[[0.29277724, 0.295953631, 0.205961823, 0.338899374, 0.49851656, -0.437764347, -0.505791903, 0.403120697, 0.0529763699, -0.417837113], [-0.445695817, -0.361287892, 0.101978905, -0.418562263, 0.0107798576, 0.0221389532, 0.220730424, 4.990560e-02, -0.208217949, -0.172240525], [0.0161076784, -0.396935463, -0.530536652, -0.379159153, -0.44315511, 0.213063776, -0.0682218968, -0.221574157, -0.0774391591, -0.472134769], [0.288429618, -0.0110605955, -0.490244329, 0.494588017, 0.330006599, 0.54180479, -0.499053359, 0.0791082978, -0.524112284, -0.0557569563], [-0.546745062, -4.394100e-01, 0.459474474, -0.248545527, 0.0341958404, -0.320446074, -0.368350893, 0.00832664966, -0.0379120708, -0.451237142], [-0.266778737, 0.49375844, -1.604300e-01, -0.315872908, 0.163485587, 0.116762996, 0.266193658, -0.30224967, 0.105117381, -0.534985602], [-0.263971537, -0.0919606983, -0.281746626, -0.310481608, -0.222500026, -0.0813589692, 0.382073581, 0.154834032, -0.285841495, -0.547358394], [0.459437639, 0.0883420109, -0.0378421694, 0.256017208, 0.206906617, -0.461118042, -0.0286277644, -0.377283901, 0.478136897, -0.519345164], [0.513453424, -0.12144509, -0.134186059, -0.39923197, -0.395939708, -0.34776777, 0.461619705, -0.185738802, -0.495007396, -0.359370708], [0.33326897, -0.100557774, 0.382778198, 0.0325809717, 0.231879592, -0.251019359, 0.165992916, -0.542559922, 0.0710098147, -0.00225681067]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>\r\n%cst_4 = \"std.constant\"() {value = dense<[[-6.715610e-01, 0.230374008, -0.0477389693, 0.28339678, -0.580778599, -1.10261536, -0.777723848, -0.451123297, 0.602046907, -0.688027441]]> : tensor<1x10xf32>} : () -> tensor<1x10xf32>\r\n%0 = \"tf.Cast\"(%arg0) {Truncate = false, device = \"\"} : (tensor<?x1xf64>) -> tensor<?x1xf32>\r\n%1 = \"tfl.fully_connected\"(%0, %cst_2, %cst) {fused_activation_function = \"RELU\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x1xf32>, tensor<10x1xf32>, tensor<10xf32>) -> tensor<?x10xf32>\r\n%2 = \"tfl.fully_connected\"(%1, %cst_3, %cst_0) {fused_activation_function = \"RELU\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x10xf32>, tensor<10x10xf32>, tensor<10xf32>) -> tensor<?x10xf32>\r\n%3 = \"tfl.fully_connected\"(%2, %cst_4, %cst_1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x10xf32>, tensor<1x10xf32>, tensor<1xf32>) -> tensor<?x1xf32>\r\n\"std.return\"(%3) : (tensor<?x1xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"dense_input\", outputs = \"Identity\"}, type = (tensor<?x1xf64>) -> tensor<?x1xf32>} : () -> ()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"prox_data_learn.py\", line 156, in <module>\r\ntflite_model = converter.convert()\r\nFile \"/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\nreturn super(TFLiteConverterV2, self).convert()\r\nFile \"/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 900, in convert\r\nself).convert(graph_def, input_tensors, output_tensors)\r\nFile \"/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 633, in convert\r\n**converter_kwargs)\r\nFile \"/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 574, in toco_convert_impl\r\nenable_mlir_converter=enable_mlir_converter)\r\nFile \"/usr/local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\nraise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(fused[\"sequential/Cast@__inference__wrapped_model_40145\", \"StatefulPartitionedCall/sequential/Cast\"]): 'tf.Cast' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"sequential/Cast@__inference__wrapped_model_40145\", \"StatefulPartitionedCall/sequential/Cast\"]): see current operation: %0 = \"tf.Cast\"(%arg0) {Truncate = false, device = \"\"} : (tensor<?x1xf64>) -> tensor<?x1xf32>\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\ntf.Cast {Truncate = false, device = \"\"}\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<?x1xf64>): // no predecessors\r\n%cst = \"std.constant\"() {value = dense<[0.136683047, 0.000000e+00, -1.8840152, 0.000000e+00, 0.000000e+00, 0.000000e+00, 3.257740e-01, 0.000000e+00, 0.000000e+00, 0.000000e+00]> : tensor<10xf32>} : () -> tensor<10xf32>\r\n%cst_0 = \"std.constant\"() {value = dense<[0.000000e+00, 1.69775057, 0.000000e+00, 0.000000e+00, -0.638654768, -1.93405676, -1.95157754, -0.0406359173, 1.72332501, -1.7537955]> : tensor<10xf32>} : () -> tensor<10xf32>\r\n%cst_1 = \"std.constant\"() {value = dense<1.7260325> : tensor<1xf32>} : () -> tensor<1xf32>\r\n%cst_2 = \"std.constant\"() {value = dense<[[0.00675311219], [0.341173232], [-0.481861204], [0.701621353], [0.0275682211], [0.0613476038], [-0.44652465], [0.166085422], [0.189539731], [0.020105958]]> : tensor<10x1xf32>} : () -> tensor<10x1xf32>\r\n%cst_3 = \"std.constant\"() {value = dense<[[0.29277724, 0.295953631, 0.205961823, 0.338899374, 0.49851656, -0.437764347, -0.505791903, 0.403120697, 0.0529763699, -0.417837113], [-0.445695817, -0.361287892, 0.101978905, -0.418562263, 0.0107798576, 0.0221389532, 0.220730424, 4.990560e-02, -0.208217949, -0.172240525], [0.0161076784, -0.396935463, -0.530536652, -0.379159153, -0.44315511, 0.213063776, -0.0682218968, -0.221574157, -0.0774391591, -0.472134769], [0.288429618, -0.0110605955, -0.490244329, 0.494588017, 0.330006599, 0.54180479, -0.499053359, 0.0791082978, -0.524112284, -0.0557569563], [-0.546745062, -4.394100e-01, 0.459474474, -0.248545527, 0.0341958404, -0.320446074, -0.368350893, 0.00832664966, -0.0379120708, -0.451237142], [-0.266778737, 0.49375844, -1.604300e-01, -0.315872908, 0.163485587, 0.116762996, 0.266193658, -0.30224967, 0.105117381, -0.534985602], [-0.263971537, -0.0919606983, -0.281746626, -0.310481608, -0.222500026, -0.0813589692, 0.382073581, 0.154834032, -0.285841495, -0.547358394], [0.459437639, 0.0883420109, -0.0378421694, 0.256017208, 0.206906617, -0.461118042, -0.0286277644, -0.377283901, 0.478136897, -0.519345164], [0.513453424, -0.12144509, -0.134186059, -0.39923197, -0.395939708, -0.34776777, 0.461619705, -0.185738802, -0.495007396, -0.359370708], [0.33326897, -0.100557774, 0.382778198, 0.0325809717, 0.231879592, -0.251019359, 0.165992916, -0.542559922, 0.0710098147, -0.00225681067]]> : tensor<10x10xf32>} : () -> tensor<10x10xf32>\r\n%cst_4 = \"std.constant\"() {value = dense<[[-6.715610e-01, 0.230374008, -0.0477389693, 0.28339678, -0.580778599, -1.10261536, -0.777723848, -0.451123297, 0.602046907, -0.688027441]]> : tensor<1x10xf32>} : () -> tensor<1x10xf32>\r\n%0 = \"tf.Cast\"(%arg0) {Truncate = false, device = \"\"} : (tensor<?x1xf64>) -> tensor<?x1xf32>\r\n%1 = \"tfl.fully_connected\"(%0, %cst_2, %cst) {fused_activation_function = \"RELU\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x1xf32>, tensor<10x1xf32>, tensor<10xf32>) -> tensor<?x10xf32>\r\n%2 = \"tfl.fully_connected\"(%1, %cst_3, %cst_0) {fused_activation_function = \"RELU\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x10xf32>, tensor<10x10xf32>, tensor<10xf32>) -> tensor<?x10xf32>\r\n%3 = \"tfl.fully_connected\"(%2, %cst_4, %cst_1) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<?x10xf32>, tensor<1x10xf32>, tensor<1xf32>) -> tensor<?x1xf32>\r\n\"std.return\"(%3) : (tensor<?x1xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"dense_input\", outputs = \"Identity\"}, type = (tensor<?x1xf64>) -> tensor<?x1xf32>} : () -> ()\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\nhttps://drive.google.com/file/d/1U59Z_Z4ZH4tGn2dTpvB5tm4t0NujUfTz/view?usp=sharing \r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@kmassa23 \r\n\r\nCan you please try adding the following code and and let us know if the issue still persists. \r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\ntflite_model = converter.convert()\r\n```\r\nPlease, try with nightly version as well. Thanks!"]}, {"number": 43618, "title": "[TFLite] Add int16x8 support for REDUCE_MIN and REDUCE_MAX operators", "body": "Hi,\r\n\r\nThis PR adds int16x8 support for the REDUCE_MIN  and REDUCE_MAX operators in TensorFlow Lite. \r\n\r\nIt also updates the reduce operators in the BuiltinRefOpResolver to point to the reference registers. The base reduce registers uses the reference registers so it doesn't change anything functionally , it's mainly for clarity.\r\n\r\nThibaut", "comments": ["@Tessil can you please resolve conflicts", "@rthadur I resolved the conflicts, thanks.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 43617, "title": "[TFLite] Add int16x8 support for non-fused RELU and RELU6 operators", "body": "Hi,\r\n\r\nThis PR adds int16x8 support for the non-fused RELU and RELU6 operators in TensorFlow Lite.\r\n\r\nThibaut", "comments": []}, {"number": 43615, "title": "Metal delegate Crash with C++ interface", "body": "Hello, I am trying to compare performance of TFLite delegates on iOS devices.\r\nThis issue is related to [comments](https://github.com/tensorflow/tensorflow/commit/60c4c3e2105afe28fb7a11cb8d440e0caabbf735#commitcomment-42709367) on 60c4c3e.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): iPhone 6, iPhone SE\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r2.3\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Apple Clang version 12.0.0\r\n\r\n**Describe the current behavior**\r\n\r\nI build an iOS static framework which includes below code and link it with camera demo application (iOS) to run image classification task.\r\n```cpp\r\n/* Same with tensorflowlite internal usage */\r\nusing TfLiteDelegatePtr = std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)>;\r\n\r\n/* for Metal delegate */\r\nTFLGpuDelegateOptions gpu_opts = {};\r\ngpu_opts.allow_precision_loss = true;\r\ngpu_opts.enable_quantization = true;\r\ngpu_delegate = TfLiteDelegatePtr(TFLGpuDelegateCreate(&gpu_opts), TFLGpuDelegateDelete);\r\n\r\n/* Error occur in `ModifyGraphWithDelegate()` */\r\nif (interpreter_->ModifyGraphWithDelegate(std::move(gpu_delegate)) != kTfLiteOk) {\r\n    LOGE(TAG, \"%s. failed to delegate to GPU, fallback to CPU\", __func__);\r\n    return;\r\n}\r\n```\r\n\r\nThen run tflite interpreter with full-integer quantized TFLite model.\r\n- On Xcode, `interpreter_->ModifyGraphWithDelegate()` issues `EXC_BAD_ACCESS (code=1, address=0x11261ca70)` (address varies after runs)\r\n- XNNPack delegate **does not break** on same quantized model.\r\n- GPU (Metal) delegate **does not break** on float16 model.\r\n\r\n**Describe the expected behavior**\r\n\r\n`interpreter_->ModifyGraphWithDelegate()` function (with Metal delegate on full-integer quantized model) does not crash and at least return an error code. (`!= kTfLiteOK`)\r\n\r\n**XCode Logs**\r\nps. `HyperFast` is the name of the iOS static framework.\r\nTo build the framework, I defined custom bazel rule to build static library (`.a`) (includes tflite runtime + xnnpack delegate + metal delegate + coreml delegate) with C++ interfaces.\r\nAnd then build framework with that static library. So xcode does not know about TFLite source code.\r\n\r\n```asm\r\nHyperFast`tflite::delegates::CreateNewTensorWithDifferentType:\r\n    0x1043ff408 <+0>:   stp    x24, x23, [sp, #-0x40]!\r\n    0x1043ff40c <+4>:   stp    x22, x21, [sp, #0x10]\r\n    0x1043ff410 <+8>:   stp    x20, x19, [sp, #0x20]\r\n    0x1043ff414 <+12>:  stp    x29, x30, [sp, #0x30]\r\n    0x1043ff418 <+16>:  add    x29, sp, #0x30            ; =0x30 \r\n    0x1043ff41c <+20>:  mov    x23, x4\r\n    0x1043ff420 <+24>:  mov    x20, x3\r\n    0x1043ff424 <+28>:  mov    x22, x2\r\n    0x1043ff428 <+32>:  mov    x21, x1\r\n    0x1043ff42c <+36>:  mov    x19, x0\r\n    0x1043ff430 <+40>:  ldr    x24, [x0, #0x10]\r\n    0x1043ff434 <+44>:  ldr    x8, [x0, #0x30]\r\n    0x1043ff438 <+48>:  mov    w1, #0x1\r\n    0x1043ff43c <+52>:  mov    x2, x4\r\n    0x1043ff440 <+56>:  blr    x8\r\n    0x1043ff444 <+60>:  cbnz   w0, 0x1043ff4dc           ; <+212>\r\n    0x1043ff448 <+64>:  ldr    x8, [x19, #0x10]\r\n    0x1043ff44c <+68>:  ldrsw  x9, [x23]\r\n    0x1043ff450 <+72>:  mov    w10, #0x70\r\n    0x1043ff454 <+76>:  madd   x8, x9, x10, x8\r\n    0x1043ff458 <+80>:  str    x8, [x20]\r\n    0x1043ff45c <+84>:  str    w22, [x8]\r\n    0x1043ff460 <+88>:  mov    w9, #0x2\r\n    0x1043ff464 <+92>:  str    w9, [x8, #0x20]\r\n    0x1043ff468 <+96>:  smaddl x8, w21, w10, x24\r\n->  0x1043ff46c <+100>: ldr    x21, [x8, #0x10]\r\n    0x1043ff470 <+104>: ldr    w0, [x21]\r\n    0x1043ff474 <+108>: bl     0x1044100fc               ; TfLiteIntArrayCreate\r\n    0x1043ff478 <+112>: mov    x2, x0\r\n    0x1043ff47c <+116>: ldr    w8, [x21]\r\n    0x1043ff480 <+120>: cmp    w8, #0x1                  ; =0x1 \r\n    0x1043ff484 <+124>: b.lt   0x1043ff4b0               ; <+168>\r\n    0x1043ff488 <+128>: mov    x8, #0x0\r\n    0x1043ff48c <+132>: add    x9, x2, #0x4              ; =0x4 \r\n    0x1043ff490 <+136>: add    x10, x21, #0x4            ; =0x4 \r\n    0x1043ff494 <+140>: lsl    x11, x8, #2\r\n    0x1043ff498 <+144>: ldr    w12, [x10, x11]\r\n    0x1043ff49c <+148>: str    w12, [x9, x11]\r\n    0x1043ff4a0 <+152>: add    x8, x8, #0x1              ; =0x1 \r\n    0x1043ff4a4 <+156>: ldrsw  x11, [x21]\r\n    0x1043ff4a8 <+160>: cmp    x8, x11\r\n    0x1043ff4ac <+164>: b.lt   0x1043ff494               ; <+140>\r\n    0x1043ff4b0 <+168>: ldr    x8, [x19, #0x20]\r\n    0x1043ff4b4 <+172>: ldr    x1, [x20]\r\n    0x1043ff4b8 <+176>: mov    x0, x19\r\n    0x1043ff4bc <+180>: blr    x8\r\n    0x1043ff4c0 <+184>: cbz    w0, 0x1043ff4dc           ; <+212>\r\n    0x1043ff4c4 <+188>: ldr    x8, [x19, #0x28]\r\n    0x1043ff4c8 <+192>: adr    x1, #0xa58e9              ; \"Could not resize new delegate tensor\"\r\n    0x1043ff4cc <+196>: nop    \r\n    0x1043ff4d0 <+200>: mov    x0, x19\r\n    0x1043ff4d4 <+204>: blr    x8\r\n    0x1043ff4d8 <+208>: mov    w0, #0x1\r\n    0x1043ff4dc <+212>: ldp    x29, x30, [sp, #0x30]\r\n    0x1043ff4e0 <+216>: ldp    x20, x19, [sp, #0x20]\r\n    0x1043ff4e4 <+220>: ldp    x22, x21, [sp, #0x10]\r\n    0x1043ff4e8 <+224>: ldp    x24, x23, [sp], #0x40\r\n    0x1043ff4ec <+228>: ret    \r\n```\r\n\r\n```asm\r\nHyperFast`tflite::impl::Interpreter::ModifyGraphWithDelegate:\r\n    0x1043f1a6c <+0>:   stp    x24, x23, [sp, #-0x40]!\r\n    0x1043f1a70 <+4>:   stp    x22, x21, [sp, #0x10]\r\n    0x1043f1a74 <+8>:   stp    x20, x19, [sp, #0x20]\r\n    0x1043f1a78 <+12>:  stp    x29, x30, [sp, #0x30]\r\n    0x1043f1a7c <+16>:  add    x29, sp, #0x30            ; =0x30 \r\n    0x1043f1a80 <+20>:  mov    x20, x1\r\n    0x1043f1a84 <+24>:  mov    x19, x0\r\n    0x1043f1a88 <+28>:  ldp    x8, x9, [x0, #0x18]\r\n    0x1043f1a8c <+32>:  cmp    x8, x9\r\n    0x1043f1a90 <+36>:  b.hs   0x1043f1ab4               ; <+72>\r\n    0x1043f1a94 <+40>:  ldr    x9, [x20]\r\n    0x1043f1a98 <+44>:  str    xzr, [x20]\r\n    0x1043f1a9c <+48>:  str    x9, [x8]\r\n    0x1043f1aa0 <+52>:  ldr    x9, [x20, #0x8]\r\n    0x1043f1aa4 <+56>:  str    x9, [x8, #0x8]\r\n    0x1043f1aa8 <+60>:  add    x8, x8, #0x10             ; =0x10 \r\n    0x1043f1aac <+64>:  str    x8, [x19, #0x18]\r\n    0x1043f1ab0 <+68>:  b      0x1043f1bb4               ; <+328>\r\n    0x1043f1ab4 <+72>:  add    x0, x19, #0x10            ; =0x10 \r\n    0x1043f1ab8 <+76>:  ldr    x10, [x0]\r\n    0x1043f1abc <+80>:  sub    x8, x8, x10\r\n    0x1043f1ac0 <+84>:  asr    x21, x8, #4\r\n    0x1043f1ac4 <+88>:  add    x8, x21, #0x1             ; =0x1 \r\n    0x1043f1ac8 <+92>:  lsr    x11, x8, #60\r\n    0x1043f1acc <+96>:  cbnz   x11, 0x1043f1c34          ; <+456>\r\n    0x1043f1ad0 <+100>: sub    x9, x9, x10\r\n    0x1043f1ad4 <+104>: asr    x10, x9, #3\r\n    0x1043f1ad8 <+108>: cmp    x10, x8\r\n    0x1043f1adc <+112>: csel   x8, x8, x10, lo\r\n    0x1043f1ae0 <+116>: mov    x10, #0x7ffffffffffffff\r\n    0x1043f1ae4 <+120>: cmp    x10, x9, asr #4\r\n    0x1043f1ae8 <+124>: mov    x9, #0xfffffffffffffff\r\n    0x1043f1aec <+128>: csel   x22, x8, x9, hi\r\n    0x1043f1af0 <+132>: cbz    x22, 0x1043f1b08          ; <+156>\r\n    0x1043f1af4 <+136>: lsr    x8, x22, #60\r\n    0x1043f1af8 <+140>: cbnz   x8, 0x1043f1c38           ; <+460>\r\n    0x1043f1afc <+144>: lsl    x0, x22, #4\r\n    0x1043f1b00 <+148>: bl     0x10444f960               ; symbol stub for: operator new(unsigned long)\r\n    0x1043f1b04 <+152>: b      0x1043f1b0c               ; <+160>\r\n    0x1043f1b08 <+156>: mov    x0, #0x0\r\n    0x1043f1b0c <+160>: add    x10, x0, x21, lsl #4\r\n    0x1043f1b10 <+164>: add    x9, x0, x22, lsl #4\r\n    0x1043f1b14 <+168>: ldr    q0, [x20]\r\n    0x1043f1b18 <+172>: str    xzr, [x20]\r\n    0x1043f1b1c <+176>: mov    x11, x10\r\n    0x1043f1b20 <+180>: str    q0, [x11], #0x10\r\n    0x1043f1b24 <+184>: ldp    x8, x12, [x19, #0x10]\r\n    0x1043f1b28 <+188>: cmp    x12, x8\r\n    0x1043f1b2c <+192>: b.eq   0x1043f1b68               ; <+252>\r\n    0x1043f1b30 <+196>: ldr    x13, [x12, #-0x10]!\r\n    0x1043f1b34 <+200>: str    xzr, [x12]\r\n    0x1043f1b38 <+204>: stur   x13, [x10, #-0x10]\r\n    0x1043f1b3c <+208>: ldr    x13, [x12, #0x8]\r\n    0x1043f1b40 <+212>: stur   x13, [x10, #-0x8]\r\n    0x1043f1b44 <+216>: sub    x10, x10, #0x10           ; =0x10 \r\n    0x1043f1b48 <+220>: cmp    x8, x12\r\n    0x1043f1b4c <+224>: b.ne   0x1043f1b30               ; <+196>\r\n    0x1043f1b50 <+228>: ldp    x20, x8, [x19, #0x10]\r\n    0x1043f1b54 <+232>: stp    x10, x11, [x19, #0x10]\r\n    0x1043f1b58 <+236>: str    x9, [x19, #0x20]\r\n    0x1043f1b5c <+240>: cmp    x8, x20\r\n    0x1043f1b60 <+244>: b.ne   0x1043f1b7c               ; <+272>\r\n    0x1043f1b64 <+248>: b      0x1043f1ba8               ; <+316>\r\n    0x1043f1b68 <+252>: mov    x20, x8\r\n    0x1043f1b6c <+256>: stp    x10, x11, [x19, #0x10]\r\n    0x1043f1b70 <+260>: str    x9, [x19, #0x20]\r\n    0x1043f1b74 <+264>: cmp    x8, x20\r\n    0x1043f1b78 <+268>: b.eq   0x1043f1ba8               ; <+316>\r\n    0x1043f1b7c <+272>: mov    x21, x8\r\n    0x1043f1b80 <+276>: b      0x1043f1b90               ; <+292>\r\n    0x1043f1b84 <+280>: mov    x8, x21\r\n    0x1043f1b88 <+284>: cmp    x20, x21\r\n    0x1043f1b8c <+288>: b.eq   0x1043f1ba8               ; <+316>\r\n    0x1043f1b90 <+292>: ldr    x0, [x21, #-0x10]!\r\n    0x1043f1b94 <+296>: str    xzr, [x21]\r\n    0x1043f1b98 <+300>: cbz    x0, 0x1043f1b84           ; <+280>\r\n    0x1043f1b9c <+304>: ldur   x8, [x8, #-0x8]\r\n    0x1043f1ba0 <+308>: blr    x8\r\n    0x1043f1ba4 <+312>: b      0x1043f1b84               ; <+280>\r\n    0x1043f1ba8 <+316>: cbz    x20, 0x1043f1bb4          ; <+328>\r\n    0x1043f1bac <+320>: mov    x0, x20\r\n    0x1043f1bb0 <+324>: bl     0x10444f948               ; symbol stub for: operator delete(void*)\r\n    0x1043f1bb4 <+328>: ldp    x21, x8, [x19, #0x68]\r\n    0x1043f1bb8 <+332>: cmp    x21, x8\r\n    0x1043f1bbc <+336>: b.eq   0x1043f1c1c               ; <+432>\r\n    0x1043f1bc0 <+340>: ldr    x9, [x19, #0x18]\r\n    0x1043f1bc4 <+344>: ldur   x20, [x9, #-0x10]\r\n    0x1043f1bc8 <+348>: sub    x22, x8, #0x8             ; =0x8 \r\n    0x1043f1bcc <+352>: mov    x23, x21\r\n    0x1043f1bd0 <+356>: ldr    x0, [x23], #0x8\r\n    0x1043f1bd4 <+360>: mov    x1, x20\r\n    0x1043f1bd8 <+364>: bl     0x1043ef2d0               ; tflite::impl::Subgraph::ModifyGraphWithDelegate(TfLiteDelegate*)\r\n->  0x1043f1bdc <+368>: cmp    x22, x21\r\n    0x1043f1be0 <+372>: b.eq   0x1043f1bec               ; <+384>\r\n    0x1043f1be4 <+376>: mov    x21, x23\r\n    0x1043f1be8 <+380>: cbz    w0, 0x1043f1bd0           ; <+356>\r\n    0x1043f1bec <+384>: cmp    w0, #0x2                  ; =0x2 \r\n    0x1043f1bf0 <+388>: b.ne   0x1043f1c20               ; <+436>\r\n    0x1043f1bf4 <+392>: ldp    x20, x19, [x19, #0x68]\r\n    0x1043f1bf8 <+396>: cmp    x20, x19\r\n    0x1043f1bfc <+400>: b.eq   0x1043f1c14               ; <+424>\r\n    0x1043f1c00 <+404>: ldr    x0, [x20], #0x8\r\n    0x1043f1c04 <+408>: bl     0x1043f01c8               ; tflite::impl::Subgraph::RemoveAllDelegates()\r\n    0x1043f1c08 <+412>: cbnz   w0, 0x1043f1c20           ; <+436>\r\n    0x1043f1c0c <+416>: cmp    x19, x20\r\n    0x1043f1c10 <+420>: b.ne   0x1043f1c00               ; <+404>\r\n    0x1043f1c14 <+424>: mov    w0, #0x2\r\n    0x1043f1c18 <+428>: b      0x1043f1c20               ; <+436>\r\n    0x1043f1c1c <+432>: mov    w0, #0x0\r\n    0x1043f1c20 <+436>: ldp    x29, x30, [sp, #0x30]\r\n    0x1043f1c24 <+440>: ldp    x20, x19, [sp, #0x20]\r\n    0x1043f1c28 <+444>: ldp    x22, x21, [sp, #0x10]\r\n    0x1043f1c2c <+448>: ldp    x24, x23, [sp], #0x40\r\n    0x1043f1c30 <+452>: ret    \r\n    0x1043f1c34 <+456>: bl     0x10444f498               ; symbol stub for: std::__1::__vector_base_common<true>::__throw_length_error() const\r\n    0x1043f1c38 <+460>: bl     0x1043f2a0c               ; std::__1::__throw_length_error(char const*)\r\n```\r\n", "comments": ["Hi @kalaluthien,\r\n\r\nFrom the stack trace, I guess it might be a similar problem to 60c4c3e. I'll take a look. Can you confirm that the model is a quantized model? Also, are you compiling the TFLite runtime from r2.3 branch or HEAD?\r\n\r\nMeanwhile, I found building the framework with `-c dbg` option is helpful in inspecting code sometimes. Can you try that? (note that the library size would be about 1GB)\r\n", "Hi @teijeong,\r\n\r\nThanks to your reply. I'll try `-c dbg` option and give you a feedback ASAP.\r\n1) The broken model is quantized MNasNet. I'm sorry for not attaching specific model architecture and parameters.\r\n2) I have tested it on `2.3.0`: b36436b and `2.3.1`: fcc4b96 and got same results.", "Hmm, `2.3.1` doesn't seem to have  60c4c3e, nor fc49cbb2adf0b69f843f7bf904978648bffbe268. Did you cherry-picked those changes as you said in the commit's comment?\r\n\r\n\r\nThe problem is a tensor pointing to an invalid location after `context->tensors` is updated This happens when the underlying container gets full and reallocated. If `TfLiteTensor*` is being referenced after calling `context->AddTensors` or `CreateNewTensorWithDifferentType`, the pointer should be updated.", "I tested (a) `2.3.0` + cherry-picked (60c4c3e), (b) `2.3.0`, (c) `2.3.1`.", "Thanks for your advise. `-c dbg` is amazing...\r\n```cpp\r\nTfLiteStatus CreateNewTensorWithDifferentType(TfLiteContext* context,\r\n                                              const int original_tensor_index,\r\n                                              TfLiteType new_type,\r\n                                              TfLiteTensor** new_tensor,\r\n                                              int* new_tensor_index) {\r\n  const TfLiteTensor& original_tensor = context->tensors[original_tensor_index];\r\n  TF_LITE_ENSURE_STATUS(context->AddTensors(context, 1, new_tensor_index));\r\n  *new_tensor = &context->tensors[*new_tensor_index];\r\n  (*new_tensor)->type = new_type;\r\n  (*new_tensor)->allocation_type = kTfLiteArenaRw;\r\n  const auto* original_dims = original_tensor.dims;           // EXC_BAD_ACCESS (code=1, address=0x115d74a70)\r\n  TfLiteIntArray* dims = TfLiteIntArrayCreate(original_dims->size);\r\n  for (int i = 0; i < original_dims->size; ++i) {\r\n    dims->data[i] = original_dims->data[i];\r\n  }\r\n  if (context->ResizeTensor(context, *new_tensor, dims) != kTfLiteOk) {\r\n    TF_LITE_KERNEL_LOG(context, \"Could not resize new delegate tensor\");\r\n    return kTfLiteError;\r\n  }\r\n  return kTfLiteOk;\r\n}\r\n```\r\n\r\nI'll cherry-pick them ( `2.3.1` + 60c4c3e and fc49cbb ) and give you the feedback soon.", "Same bug at `delegates/utils.cc`\r\n\r\n```cpp\r\nTfLiteStatus CreateNewTensorWithDifferentType(TfLiteContext* context,\r\n                                              const int original_tensor_index,\r\n                                              TfLiteType new_type,\r\n                                              TfLiteTensor** new_tensor,\r\n                                              int* new_tensor_index) {\r\n  TF_LITE_ENSURE_STATUS(context->AddTensors(context, 1, new_tensor_index));\r\n  const TfLiteTensor& original_tensor = context->tensors[original_tensor_index];\r\n  *new_tensor = &context->tensors[*new_tensor_index];\r\n  (*new_tensor)->type = new_type;\r\n  (*new_tensor)->allocation_type = kTfLiteArenaRw;\r\n  const auto* original_dims = original_tensor.dims;       // EXC_BAD_ACCESS (code=1, address=0x1163b0a70\r\n  TfLiteIntArray* dims = TfLiteIntArrayCreate(original_dims->size);\r\n  for (int i = 0; i < original_dims->size; ++i) {\r\n    dims->data[i] = original_dims->data[i];\r\n  }\r\n  if (context->ResizeTensor(context, *new_tensor, dims) != kTfLiteOk) {\r\n    TF_LITE_KERNEL_LOG(context, \"Could not resize new delegate tensor\");\r\n    return kTfLiteError;\r\n  }\r\n  return kTfLiteOk;\r\n}\r\n```\r\n\r\n`original_tensor` is corrupted.\r\n\r\n![\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2020-09-29 17 21 29](https://user-images.githubusercontent.com/6360552/94531996-43196180-0278-11eb-93f5-e529bd66fbae.png)\r\n", "Another try:\r\n![\u1109\u1173\u110f\u1173\u1105\u1175\u11ab\u1109\u1163\u11ba 2020-09-29 17 39 58](https://user-images.githubusercontent.com/6360552/94534619-8f19d580-027b-11eb-9fc4-467df872d427.png)", "Is there any progress on the issue? \ud83d\ude4f ", "Sorry for late reply,\r\n\r\nIt seems weird, and the framework might be an older one, not a new one with patched codes. Would you watch and compare `&context->tensors[original_tensor_index]` with the `original_tensor` for a sanity check?", "I found detailed `bazel` log includes some build errors in the `tensorflow/lite/delegates/gpu/common/object_reader.cc` maybe this issue results in code-binary mismatch.\r\n\r\n```cpp\r\ntensorflow/lite/delegates/gpu/common/object_reader.cc:63:23: error: no viable overloaded '='\r\n        tflite_tensor = &context->tensors[tensor_idx];\r\n        ~~~~~~~~~~~~~ ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/c/common.h:379:16: note: candidate function (the implicit copy assignment operator) not viable: no known conversion from 'TfLiteTensor *' to 'const TfLiteTensor' for 1st argument; remove &\r\ntypedef struct TfLiteTensor {\r\n               ^\r\n./tensorflow/lite/c/common.h:379:16: note: candidate function (the implicit move assignment operator) not viable: no known conversion from 'TfLiteTensor *' to 'TfLiteTensor' for 1st argument; remove &\r\ntypedef struct TfLiteTensor {\r\n               ^\r\n1 error generated.\r\n```\r\n\r\nI have **cherry-picked** 60c4c3e (and fc49cbb) but nothing else. I think I should fix something else to apply these patches.\r\n\r\n(e.g. `TfLiteTensor& tflite_tensor = context->tensors[tensor_idx];` -> `TfLiteTensor* tflite_tensor = &context->tensors[tensor_idx];` which is not included change in 60c4c3e)\r\n\r\nI'll do something and reply. thanks!", "Thanks @teijeong . It works after fix the type of `tflite_tensor` from `TfLiteTensor&` to `TfLiteTensor*`.\r\nI think it is okay to **close this issue** after you apply previous Metal delegate memory bug to r2.3 too.\r\n\r\nAnd then, I met strange error when I call `interpreter_->ModifyGraphWithDelegate(std::move(gpu_delegate)) // != kTfLiteOk` which prevent model from GPU delegate.\r\n\r\n```\r\n2020-10-14 11:05:35.093270+0900 WebRTCDemo[1027:707379] Created TensorFlow Lite delegate for Metal.\r\n2020-10-14 11:05:35.096629+0900 WebRTCDemo[1027:707379] Replacing 74 node(s) with delegate (TfLiteMetalDelegate) node, yielding 1 partitions.\r\n2020-10-14 11:05:35.274707+0900 WebRTCDemo[1027:707379] TfLiteMetalDelegate Prepare: Unsupported op type: convolution_2d; custom registry error: Unsupported op: convolution_2d; primary registry error: Only identical batch dimension is supported;\r\n2020-10-14 11:05:35.275255+0900 WebRTCDemo[1027:707379] Node number 74 (TfLiteMetalDelegate) failed to prepare.\r\n2020-10-14 11:05:35.276749+0900 WebRTCDemo[1027:707379] Restored original execution plan after delegate application failure.\r\n```\r\n\r\nI wonder why it yields **\"Only identical batch dimension is supported\"** message because the batch size if fixed to 1.\r\nThis tflite model is on production since TF1.13, so I think the model itself has no problem.\r\nAnd may be this is another issue.", "My bad! Thanks for catching. Will work on a fix. However, it's not likely for the fix to go into r2.3 as it's not a critical security fix, and quantization support in r2.3 is somewhat experimental.\r\n\r\nFor the new bug, the error is coming from batch size check in here https://github.com/tensorflow/tensorflow/blob/6bdae6145a521693aba42eff7f3c8b070429c05b/tensorflow/lite/delegates/gpu/common/model.cc#L503\r\n\r\nCan you check that the input/output tensors of the convolution has [1xNxMxK] shape?\r\n\r\n", "CORRECTION: seems \"fixing  type of `tflite_tensor` from `TfLiteTensor&` to `TfLiteTensor*`\" is already in the code, right? (correct me if I missed something)\r\n", "\"fixing type of `tflite_tensor` from `TfLiteTensor&` to `TfLiteTensor*`\" is already in the `master` but not `r2.3` + `60c4c3e` and `fc49cbb`. It is in `e25d3e0` which is too large commit to cherrypick.", "@kalaluthien It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.5 and let us know if the issue still persists? Thanks!", "> @kalaluthien It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest stable Version of TF 2.5 and let us know if the issue still persists? Thanks!\r\n\r\nThanks for your comment @sushreebarsa ! Nowadays, I am now working on updating backend TF version from `r2.3` to `r2.5` and I will test Metal delegate again. FYI. @teijeong ", "@kalaluthien Did you have any luck with the Metal delegate with TF 2.5?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43615\">No</a>\n"]}, {"number": 43614, "title": "Failed to load the native TensorFlow runtime.", "body": "System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): window 10\r\n![Capture1](https://user-images.githubusercontent.com/63404689/94395515-8642da00-017d-11eb-984a-0a23d1c0b19c.JPG)\r\n![Capture2](https://user-images.githubusercontent.com/63404689/94395523-880c9d80-017d-11eb-82e2-9dfbad52761c.JPG)\r\n\r\n- TensorFlow installed from (source or binary): jupyter notebook\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\nUnable to import keras and tensorflow in jupyter notebook\r\n\r\nInstalling tensorflow through pip install tensorflow but unable to install.\r\n\r\n\r\n", "comments": ["@akmal1994 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\n\r\nAlso, you can change the code to `from tensorflow import keras`\r\n\r\nThanks!", "Thanks for the support. The issue is resolved.\n\nOn Mon, Sep 28, 2020 at 3:43 PM ravikyram <notifications@github.com> wrote:\n\n> @akmal1994 <https://github.com/akmal1994>\n>\n> What is make/model of your cpu?\n> I suspect your cpu model does not support AVX instructions sets.See hardware\n> requirements\n> <https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements>\n> Make sure to download the latest microsoft visual c++ redistributable\n> from here.\n> <https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads>\n> .Also, please follow the instructions from to install from Tensorflow\n> website <https://www.tensorflow.org/install/source_windows>.\n>\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167\n> <https://github.com/tensorflow/tensorflow/issues/36167> and see if it\n> helps you.\n>\n> Also, you can change the code to from tensorflow import keras\n>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43614#issuecomment-699915776>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APDXVEMYWWSUJTBZONNC6M3SIBOWPANCNFSM4R4COYGQ>\n> .\n>\n", "@akmal1994 \r\n\r\nPlease, close this thread if your issue was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43614\">No</a>\n"]}, {"number": 43613, "title": "[Intel Mkl]  Fixed auto mixed precision bug ,Ops  have more than one attr_type(like FusedBatchNorm )", "body": " \r\n**this pr aim to fixed bf16 op have more than one attr_type problems, like FusedBatchNormV2 and FusedBatchNormV3, it has type attr \"U\", \"U\" only  support float32, but if we add this op to allow_list, it will add the idnex of attr_type \"U\"  to allow_set, this will encount an error.**\r\n\r\nUsing bfloat16 of FusedBatchNormV2 will encount below problems:\r\n- FusedBatchNorm's input with \"U\" type only support float32, but not support bf16.\r\n- After using bf16 for Resnet V2 101 and Resnet V2 50/101 , if add FusedBatchNormV2 into ALLOWLIST,  it will add more Cast op, this can greatly affect performance.\r\n\r\nthis pr is to fix FusedBatchNormV2/FusedBatchNormV3  with bf16 BUG,\r\n\r\nBelow table is model's **throughput** data(BatchSize = 128, Intel CPU\uff09\r\n\r\nthe throughput  compares with FP32 model.\r\n| model / fps |  Fp32 |  BF16(origin)| BF16 (fix FusedBatchNormV2 bug)|\r\n|---|---|---| --- |\r\n| Resnet v2 50 | 1| 0.77|  1.58|\r\n|Resnet v2 101 | 1 | 0.81 | 1.773|\r\n\r\nAfter fix this bug, the thoughput speed up **~1.6x** than float32.\r\n\r\nBelow is Resnet **acccuracy **in imagenet dataset:\r\n\r\n| model / (top1/top5)|  Fp32 |  BF16(origin)| BF16 (fix FusedBatchNormV2 bug)|\r\n|---|---|---| --- |\r\n| Resnet v2 50 | 0.6964/0.8948| 0.6988/0.8953|  0.6985/0.8953|\r\n|Resnet v2 101 | 0.7187/0.9059 | 0.7207/0.9068| 0.7205/0.9068|\r\n\r\n\r\nWhy need I add a new pass?:\r\n- if customer add FusedBatchNormV2/FusedBatchNormV3 in allow_list, attr_type 'U' only support float, it maybe add cast in their type U node(like mean, offset, variance and scale). \r\n\r\n\r\nI run a little case to show the result:  \r\n\r\n I run a test for conv_bn. used default set for ALLOWLIST. the below picture is bf16 pb\u3002\r\n![image](https://user-images.githubusercontent.com/35016185/94387011-7a561880-017b-11eb-8c5b-0ac31fadde52.png)\r\n\r\n\r\n\r\nafter I add FusedBatchNormV3 to ALLOWLIST, it will add more redundant Cast for FusedBatchnormV3(This cast will be added in FusedBatchNormV3  input of  type \"U\" ,  as same as FusedBatchNormV2)\r\n\r\n![image](https://user-images.githubusercontent.com/35016185/94387196-010af580-017c-11eb-90ce-653d35644f78.png)\r\n\r\n\r\nAfter I fixed this bug, and add FusedBatchNormV3 to ALLOWLIST, the result is;\r\n![Screenshot 2020-09-28 123759](https://user-images.githubusercontent.com/35016185/94391385-e2aaf700-0187-11eb-8e7d-781e44ae8ca9.png)\r\n\r\nThe difference with default set( not add FusedBatchNormV3 into ALLOWLIST, see the first picture) is **between Conv2D \r\n and FusedBatchNormV3 have no `cast`, but have `cast` in  FusedBatchNormV3  and Identity,  in first picture, it will add `cast`    between Conv2D  and FusedBatchNormV3** \r\n", "comments": []}, {"number": 43612, "title": "from tensorflow.contrib.seq2seq import Helper", "body": "HI.\r\n\r\nI am developing a system but in Tensorflow 2 don't exist this:\r\n\r\nfrom tensorflow.contrib.seq2seq import Helper\r\n\r\nHow can migrate to a similar solution in Tensorflow 2? Thank's a lot", "comments": ["Check https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/README.md", "Also make sure you are familiar with [python-op-compatibility-matrix](https://github.com/tensorflow/addons#). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43611, "title": "[PluggableDevice] Add device alias support", "body": "Adding a device alias support for resolving kernel registration\r\nconflict between third party devices and first party devices in TensorFlow.\r\nFor first party devices(GPU, CPU) in Tensorflow, alias is empty.\r\nFor third party devices, alias is registered from plugin", "comments": ["@annarev  @penpornk @yisitu This is the second PR which is for subdevice type support in TensorFlow, please help to review it. Thanks ", "@jzhoulon Can you please check @annarev's comments and keep us posted ? Thanks!", "@gbaned ,  sorry to late response, we are in a long public holiday last week, will address all comments soon. Thanks!", "@annarev @gbaned  all the comments are addressed, please help to review. Thanks.", "@annarev  It seems that device_factory moved back to core/common_runtime and cause the conflict, should we add the header dependency back? thanks ", "> @annarev It seems that device_factory moved back to core/common_runtime and cause the conflict, should we add the header dependency back? thanks\r\n\r\nYep, sorry for confusion. I had to roll it back due to a build failure. However, it has been fixed today and re-committed: https://github.com/tensorflow/tensorflow/commit/c60ced548cd52da5f078f7ed12a7b8f80148639d", "@jzhoulon Can you please check @annarev's comments and keep us posted ? Thanks!", "> @jzhoulon Can you please check @annarev's comments and keep us posted ? Thanks!\r\n\r\n@gbaned  thanks for the reminder, we have addressed all the comments internally and will push it when the internal validation passed as soon as possible.", "Thank you for updates! This change looks good to me now.\r\nI added API Review label now. Basically, designated API reviewers need to take a look at this change since it modifies TensorFlow API. You can read more about the process here: https://github.com/tensorflow/community/blob/master/governance/api-reviews.md\r\n\r\n@sanjoy, please take a look if you have any comments.", "@annarev I have fixed a pylint error. Can you help to review it again? thanks.", "@annarev , it seems that there are still two windows bazel test failed. however, I find that other PR [44102](https://github.com/tensorflow/tensorflow/pull/44102) seems also failed in the same case, so I am not sure whether this is a known issue, if there is any need to update in this PR, please let me know, thanks", "@penpornk @annarev @sanjoy @guptapriya seems all the comments are addressed, please help to review it, if there is any need to change, please let me know. Thanks", "> Are you sure \"subdevice\" is the right term to use here? To me this seems more like an \"alias\" -- a plugin can \"alias\" `GPU` to `XYZ_GPU`, and use that to register specialized kernels that will be preferred over the \"native\" kernels that are registered to \"GPU\". \"Subdevice\" sounds like the plugin can register various different kinds of GPUs, which isn't the case.\r\n> \r\n> CC @annarev @penpornk\r\n\r\nI guess we have been thinking of the names while keeping in mind that we might in the future support multiple devices of the same type but different subtype. Another note is that we actually refer to `subdevice_type` as `name` in C API: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/c/experimental/stream_executor/stream_executor.h;l=423\r\nIf we will always support just one subtype per type (which is likely), then we could rename it. I am guessing this would require changing some parts (for e.g. using `alias == \"\"` to check that device is not a pluggable device seems confusing since `alias` is generally something that doesn't carry any information).\r\n", "> If we will always support just one subtype per type (which is likely), then we could rename it.\r\n\r\nSorry, your response fell through the cracks.\r\n\r\nMy suggestion would be to rename this, unless we have scoped out and have commitment / staffing to actually make this a proper subdevice.", "> > If we will always support just one subtype per type (which is likely), then we could rename it.\r\n> \r\n> Sorry, your response fell through the cracks.\r\n> \r\n> My suggestion would be to rename this, unless we have scoped out and have commitment / staffing to actually make this a proper subdevice.\r\n\r\nWe haven't committed/scoped out a plan for multiple subdevices per device type. Therefore, I will defer to your judgement.", "> > If we will always support just one subtype per type (which is likely), then we could rename it.\r\n> \r\n> Sorry, your response fell through the cracks.\r\n> \r\n> My suggestion would be to rename this, unless we have scoped out and have commitment / staffing to actually make this a proper subdevice.\r\n\r\n@sanjoy @annarev  I have renamed the subdevice_type as \"alias\", please have a review. Thanks.", "> @sanjoy @annarev I have renamed the subdevice_type as \"alias\", please have a review. Thanks.\r\n\r\nPlease also change the PR description.", "> > @sanjoy @annarev I have renamed the subdevice_type as \"alias\", please have a review. Thanks.\r\n> \r\n> Please also change the PR description.\r\n\r\n@sanjoy  done, thanks for the reminder.", "One thing came up when I was discussing this PR with @penpornk: can we simplify the overall system by:\r\n\r\n - Not implementing device aliasing\r\n - Having plugins register their device at a higher priority than GPU (so that placer chooses INTEL_GPU over GPU when possible)\r\n - Implementing a grappler pass (using the integration you're already working on) that rewrites GPU to INTEL_GPU in `NodeDef::device` so that even for graphs explicitly placed on GPU work are redirected to work on INTEL_GPU", "> One thing came up when I was discussing this PR with @penpornk: can we simplify the overall system by:\r\n> \r\n> * Not implementing device aliasing\r\n> * Having plugins register their device at a higher priority than GPU (so that placer chooses INTEL_GPU over GPU when possible)\r\n> * Implementing a grappler pass (using the integration you're already working on) that rewrites GPU to INTEL_GPU in `NodeDef::device` so that even for graphs explicitly placed on GPU work are redirected to work on INTEL_GPU\r\n\r\nThe problem here is, this design will make all pluggable device depend on grappler pass to placer the correct device, this will bring problems for eager support.", "> > One thing came up when I was discussing this PR with @penpornk: can we simplify the overall system by:\r\n> > \r\n> > * Not implementing device aliasing\r\n> > * Having plugins register their device at a higher priority than GPU (so that placer chooses INTEL_GPU over GPU when possible)\r\n> > * Implementing a grappler pass (using the integration you're already working on) that rewrites GPU to INTEL_GPU in `NodeDef::device` so that even for graphs explicitly placed on GPU work are redirected to work on INTEL_GPU\r\n> \r\n> The problem here is, this design will make all pluggable device depend on grappler pass to placer the correct device, this will bring problems for eager support.\r\n\r\n@sanjoy @penpornk  Thanks for your comments! Just had a discussion with @yiqianglee. We think the proposal does simplify the TF device hierarchy, but we need to validate the details with POC.  The eager mode dispatch issue Yiqiang brought up might require a new interface so that pluggable device can intercept and redirect the dispatch to the new device. Any thoughts on this?  The other concern is that since the grappler pass for Modular TF Graph happens at the last step, prior graph optimization hard-wired to CUDA device may result in a graph which plugin grappler pass need to revert.  Are you aware of any prior graph optimization pass which might be a problem?  Appreciate your feedback. ", "> The other concern is that since the grappler pass for Modular TF Graph happens at the last step\r\n\r\nSeparately, have you considered putting the modular TF Graph pass at the first step of the pipeline?  That will probably be more future proof -- as we convert more things in the pipeline to use MLIR, it would be harder to have a `GraphDef` representation of a TF graph that has gone through some stages of optimization.  OTOH we will have to support `SavedModel` for a long time to come anyway, so having a `GraphDef` representation for in the first stage of the optimization pipeline is probably OK.\r\n\r\nCC @jpienaar ", "> The problem here is, this design will make all pluggable device depend on grappler pass to placer the correct device, this will bring problems for eager support.\r\n\r\nThat's a good point, I don't think we have a good solution that works well for eager mode at this time.\r\n\r\n@rjpower @agarwal-ashish Do you have any suggestions here?  We're trying to construct a pluggable way to let vendors alias a known device (say GPU) with a custom device (say INTEL_GPU) so that `OpKernel`s registered with INTEL_GPU are picked even if the user program has `with tf.device(\"gpu\"):`.\r\n\r\nMaybe this should be an explicit API?  Like`tf.redirect_device(\"gpu\", \"intel_gpu\")`?", "Thanks @sanjoy to bring the idea and discussion, seems alias device type is still the better way we can see currently, removing alias device type will bring some additional complex design, like eager support (need model change, not transparent to user, also front end \"GPU\" device redirect to pluggable device need to be added), graph optimization execution is also needed to change. Any idea?\r\n\r\n@annarev @penpornk Do you have any comments?", "> > The other concern is that since the grappler pass for Modular TF Graph happens at the last step\r\n> \r\n> Separately, have you considered putting the modular TF Graph pass at the first step of the pipeline? That will probably be more future proof -- as we convert more things in the pipeline to use MLIR, it would be harder to have a `GraphDef` representation of a TF graph that has gone through some stages of optimization. OTOH we will have to support `SavedModel` for a long time to come anyway, so having a `GraphDef` representation for in the first stage of the optimization pipeline is probably OK.\r\n> \r\n> CC @jpienaar\r\n\r\nYeah, so if this were to run as part of import passes then yes you'd still have GraphDef (well for SavedModel you would at loading, during execution you have the similar but different tensorflow::Graph). As GraphDef gets used less for internal graph representation and optimizations, we can avoid extra overhead of converting to GraphDef - IMHO we already convert too often between the two formats, and I'd rather iterate to where we only have GraphDef as resting input format.", "> > > The other concern is that since the grappler pass for Modular TF Graph happens at the last step\r\n> > \r\n> > \r\n> > Separately, have you considered putting the modular TF Graph pass at the first step of the pipeline? That will probably be more future proof -- as we convert more things in the pipeline to use MLIR, it would be harder to have a `GraphDef` representation of a TF graph that has gone through some stages of optimization. OTOH we will have to support `SavedModel` for a long time to come anyway, so having a `GraphDef` representation for in the first stage of the optimization pipeline is probably OK.\r\n> > CC @jpienaar\r\n> \r\n> Yeah, so if this were to run as part of import passes then yes you'd still have GraphDef (well for SavedModel you would at loading, during execution you have the similar but different tensorflow::Graph). As GraphDef gets used less for internal graph representation and optimizations, we can avoid extra overhead of converting to GraphDef - IMHO we already convert too often between the two formats, and I'd rather iterate to where we only have GraphDef as resting input format.\r\n\r\nPutting the modular TF Graph pass at the first step of the pipeline won't work for pluggable devices. The pluggable devices need to get a chance to conduct target-depedent optimization,  which may invalidate target-independent grappler passes.  ", "What's the underlying reason to alias over the existing name? I don't think we encode any particular meaning in the fact that something is a \"GPU\" vs \"TPU\" vs \"XPU\".", "> What's the underlying reason to alias over the existing name? I don't think we encode any particular meaning in the fact that something is a \"GPU\" vs \"TPU\" vs \"XPU\".\r\n\r\nOne reason that comes to mind is conditional checks. We might be checking if something is GPU to run general GPU logic, or we might be checking that something is GPU to run CUDA specific logic. We could probably differentiate whether something is a pluggable device or not instead (if pluggable, don't run CUDA-specific logic).\r\n\r\nAnother reason is supporting running code only on specific type of GPU. For e.g having `with tf.device('/INTEL_GPU:0')` (say, fail if INTEL_GPU is not available in this case). I don't know though if there is a known usecase for this though.", "> > What's the underlying reason to alias over the existing name? I don't think we encode any particular meaning in the fact that something is a \"GPU\" vs \"TPU\" vs \"XPU\".\r\n> \r\n> One reason that comes to mind is conditional checks. We might be checking if something is GPU to run general GPU logic, or we might be checking that something is GPU to run CUDA specific logic. We could probably differentiate whether something is a pluggable device or not instead (if pluggable, don't run CUDA-specific logic).\r\n> \r\n> Another reason is supporting running code only on specific type of GPU. For e.g having `with tf.device('/INTEL_GPU:0')` (say, fail if INTEL_GPU is not available in this case). I don't know though if there is a known usecase for this though.\r\n\r\nAnd also, alias over existing name will give pluggable device the opportunity to run legacy model (`with tf.device('/gpu:0')`) without changing any user code.", "I just want to clarify: is aliasing necessary for custom devices to work at all? Or is this a convenience to support existing models? (I don't think it's required, because things like TPUs work, but please correct me if I'm wrong). Existing code that checks for \"device:GPU\" is likely specialized for CUDA GPUs, so I don't think it's correct in general to assume we can alias over it.\r\n\r\nIf it's not a hard requirement, I'd recommend we move ahead with the other PRs related to pluggable devices until we understand this a bit better.", "> I just want to clarify: is aliasing necessary for custom devices to work at all? Or is this a convenience to support existing models? (I don't think it's required, because things like TPUs work, but please correct me if I'm wrong). Existing code that checks for \"device:GPU\" is likely specialized for CUDA GPUs, so I don't think it's correct in general to assume we can alias over it.\r\n> \r\n> If it's not a hard requirement, I'd recommend we move ahead with the other PRs related to pluggable devices until we understand this a bit better.\r\n\r\nI would say yes, we do see a need for us to run the existing TF application with \"device:GPU\"  as is.  GPU has a broad market so we expect more entry-level users and we want to enable them transparently.  If TF's \"device:GPU\" implies CUDA, it is against the spirit of modular TF, which intends to provide a vendor-neutral interface.  We are working on CUDA specific code to make it more general. ", "> > I just want to clarify: is aliasing necessary for custom devices to work at all? Or is this a convenience to support existing models? (I don't think it's required, because things like TPUs work, but please correct me if I'm wrong). Existing code that checks for \"device:GPU\" is likely specialized for CUDA GPUs, so I don't think it's correct in general to assume we can alias over it.\r\n> > If it's not a hard requirement, I'd recommend we move ahead with the other PRs related to pluggable devices until we understand this a bit better.\r\n> \r\n> I would say yes, we do see a need for us to run the existing TF application with \"device:GPU\" as is. GPU has a broad market so we expect more entry-level users and we want to enable them transparently. If TF's \"device:GPU\" implies CUDA, it is against the spirit of modular TF, which intends to provide a vendor-neutral interface. We are working on CUDA specific code to make it more general.\r\n\r\nI joined the discussion late, so please correct me if I missed some context here. As @Jianhui-Li mentioned it would be great if can enable existing TF applications on GPU seamlessly. As we are adding Metal backend support we noticed this dependency as well and as part of pluggable design it gives an opportunity to straighten it out. \r\n\r\n@rjpower Is the concern here  that the change has wider impact and will break existing code-paths in DEVICE_GPU? ", "@rjpower Any update on this PR? Please. Thanks!", "@kulinseth Sorry for the late reply! I believe the concern here is that whatever we choose here (having a `device_alias` or not), it might restrict how TensorFlow does device placement in the future. And there have been several existing issues with device placement to begin with. So some of us wanted to explore the impact on that side more before we proceed here. This PR is not blocking other PluggableDevice PRs and will likely be merged last. (We are still aiming for TF 2.5.)\r\n\r\n@gbaned This PR will be on hold for a while until we can make a decision on how we want to move forward.", "@penpornk Thanks for the reply. Makes sense and since this is not blocking other PRs, we can take a look later and not hold them up. \r\n", "(Removing the `API review` tag for now. Will add it back once the PR is ready for API review again.)", "@penpornk  Any update on this PR? Please. Thanks!", "@gbaned It's still on hold. Sorry! ", "@penpornk Any update on this PR? Please. Thanks!", "@gbaned We are exploring an alternative. I'll temporarily close this PR. We can reopen it if later on if we want to fall back to this one. Sorry for the inconveniences!"]}, {"number": 43610, "title": "[PluggableDevice] Add pluggable device load mechanism ", "body": "support for pluggable device plugin loading mechanism, this PR is following [Pluggable device for TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md), which provides the capability of loading a pluggable device plugin in the TensorFlow Proper.\r\n\r\nAdded by @penpornk on 11/18/20:\r\nThis PR is part of an effort to let new third-party devices connect to TensorFlow modularly. There are two RFCs:\r\n* [StreamExecutor C API](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md) gives the background in the [Motivation](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md#motivation) section. But this RFC is not really related to this implementation PR. \r\n    * [Design review notes.](https://github.com/tensorflow/community/pull/257#issuecomment-656818115)\r\n* [PluggableDevice](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md) is about the actual plug-in devices and has the designs for this PR. \r\n    * Design review notes: [1st meeting](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2nd meeting](https://github.com/tensorflow/community/pull/262#issuecomment-674199085).", "comments": ["@annarev @penpornk @yisitu  The first PR is for PluggableDevice load mechanism, Thanks", "@jzhoulon  Can you please check @annarev's comments and keep us posted ? Thanks!", "@gbaned ,  sorry to late response, we are in a long public holiday last week, will address all comments soon. Thanks!", "@annarev @gbaned  I have addressed all the comments, please help to review. thanks.", "@jzhoulon  Can you please check @annarev's comments and keep us posted ? Thanks!", "Looks good to me. I added API Owners and @sanjoy for any additional comments.", "@fchollet Thank you for the review! \r\nI'll cut in and answer. @jzhoulon, please correct me if I'm wrong.\r\n\r\n(Edited to add: Oops. I didn't see that @jzhoulon already posted his answer. Sorry!)\r\n\r\n> Is there a design doc or design review notes for this feature? Please link them in the PR description.\r\n\r\nYes, this is part of an effort to let new third-party devices connect to TensorFlow modularly. There are two RFCs:\r\n* [StreamExecutor C API](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md) gives the background in the [Motivation](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md#motivation) section. But this RFC is not really related to this implementation PR. \r\n    * [Design review notes.](https://github.com/tensorflow/community/pull/257#issuecomment-656818115)\r\n* [PluggableDevice](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md) is about the actual plug-in devices and has the designs for this PR. \r\n    * Design review notes: [1st meeting](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2nd meeting](https://github.com/tensorflow/community/pull/262#issuecomment-674199085).\r\n\r\nI have added this to the PR description.\r\n\r\n> Should the Python-side functions added be exported to the public API?\r\n\r\nWe don't expect the users to initiate the loading call (`load_pluggable_device_library`), i.e., the load should just happen automatically in the background. \r\n* User installs a third-party device plug-in to the default tensorflow-plugin directory.\r\n* TensorFlow automatically loads device plug-ins from this directory during initialization.\r\n* User can use these plug-in devices in the same way as pre-integrated devices in TensorFlow (\"CPU\", \"GPU\", \"TPU\").\r\n\r\nDo you think it should be exported?\r\n(Edited to add: What @jzhoulon said. Please ignore me. \ud83d\ude05)\r\n\r\n> Who are the potential users of this API?\r\n\r\nOn the TF user side: Anyone that would like to use third-party device plug-ins. \r\nOn the plug-in side:\r\n\r\n* @jzhoulon et al. are developing an Intel GPU device plug-in.\r\n* @wchao1115 et al. are considering developing a [DirectML](https://docs.microsoft.com/en-us/windows/win32/direct3d12/dml-intro) device plug-in. More details in the [TensorFlow on DirectML RFC](https://github.com/tensorflow/community/pull/243) and the PluggableDevice design review notes  [[1](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2](https://github.com/tensorflow/community/pull/262#issuecomment-674199085)].\r\n* @kulinseth et al. are considering developing a [Metal](https://developer.apple.com/metal/) device plug-in. More details in the PluggableDevice design review notes  [[1](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2](https://github.com/tensorflow/community/pull/262#issuecomment-674199085)]. \r\n\r\n> How are the potential users of this API supposed to figure out how to use it -- docs, examples? Maybe those should be linked from the docstring?\r\n\r\nRight now we don't expect anyone to call `load_pluggable_device_library` manually. But adding more info/examples in the docstring sounds good too. @jzhoulon, @annarev what do you think?\r\n\r\n(Edited to add: What @jzhoulon said. Please ignore me. \ud83d\ude05)\r\n\r\n> How are these custom packages meant to be distributed? (pip? direct downloads?) \r\n\r\nI'll leave this to @jzhoulon et al.\r\n\r\n> Are there version matching requirements?\r\n\r\nYes. Plugins and TensorFlow must at least use the same major version of StreamExecutor C API (as different major versions could have backward-incompatible changes). Please see the [StreamExecutor C API Versioning Strategy](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api/C_API_versioning_strategy.md) document for more details. Also, this PluggableDevice mechanism is only guaranteed to work with the current TensorFlow stack (i.e., not necessarily with the new TFRT + MLIR stack). ", "> @fchollet Thank you for the review!\r\n> I'll cut in and answer. @jzhoulon, please correct me if I'm wrong.\r\n> \r\n> (Edited to add: Oops. I didn't see that @jzhoulon already posted his answer. Sorry!)\r\n\r\nI saw most answers are similar, so I delete mine and  make some supplements. :bowtie:.\r\n> \r\n> > Is there a design doc or design review notes for this feature? Please link them in the PR description.\r\n> \r\n> Yes, this is part of an effort to let new third-party devices connect to TensorFlow modularly. There are two RFCs:\r\n> \r\n>     * [StreamExecutor C API](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md) gives the background in the [Motivation](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api.md#motivation) section. But this RFC is not really related to this implementation PR.\r\n>       \r\n>       * [Design review notes.](https://github.com/tensorflow/community/pull/257#issuecomment-656818115)\r\n> \r\n>     * [PluggableDevice](https://github.com/tensorflow/community/blob/master/rfcs/20200624-pluggable-device-for-tensorflow.md) is about the actual plug-in devices and has the designs for this PR.\r\n>       \r\n>       * Design review notes: [1st meeting](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2nd meeting](https://github.com/tensorflow/community/pull/262#issuecomment-674199085).\r\n> \r\n> \r\n> I have added this to the PR description.\r\n> \r\n> > Should the Python-side functions added be exported to the public API?\r\n> \r\n> We don't expect the users to initiate the loading call (`load_pluggable_device_library`), i.e., the load should just happen automatically in the background.\r\n> \r\n>     * User installs a third-party device plug-in to the default tensorflow-plugin directory.\r\n> \r\n>     * TensorFlow automatically loads device plug-ins from this directory during initialization.\r\n> \r\n>     * User can use these plug-in devices in the same way as pre-integrated devices in TensorFlow (\"CPU\", \"GPU\", \"TPU\").\r\n> \r\n> \r\n> Do you think it should be exported?\r\n> (Edited to add: What @jzhoulon said. Please ignore me. sweat_smile)\r\n\r\n This python side function has known deficiences: PluggableDevice implementation hasn't been added into the TensorFlow proper, as doc :https://github.com/tensorflow/community/blob/ddbdeac3f6df18045fc75c155a66d0b981d83fb8/governance/api-reviews.md#experimental-apis said, we will wait PluggableDevice implementation added, then to see whether we need to expose this API.(We currently prefer TensorFlow proper call this API, not the user.)\r\n\r\n> \r\n> > Who are the potential users of this API?\r\n> \r\n> On the TF user side: Anyone that would like to use third-party device plug-ins.\r\n> On the plug-in side:\r\n> \r\n>     * @jzhoulon et al. are developing an Intel GPU device plug-in.\r\n> \r\n>     * @wchao1115 et al. are considering developing a [DirectML](https://docs.microsoft.com/en-us/windows/win32/direct3d12/dml-intro) device plug-in. More details in the [TensorFlow on DirectML RFC](https://github.com/tensorflow/community/pull/243) and the PluggableDevice design review notes  [[1](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2](https://github.com/tensorflow/community/pull/262#issuecomment-674199085)].\r\n> \r\n>     * @kulinseth et al. are considering developing a [Metal](https://developer.apple.com/metal/) device plug-in. More details in the PluggableDevice design review notes  [[1](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2](https://github.com/tensorflow/community/pull/262#issuecomment-674199085)].\r\n> \r\n> \r\n> > How are the potential users of this API supposed to figure out how to use it -- docs, examples? Maybe those should be linked from the docstring?\r\n> \r\n> Right now we don't expect anyone to call `load_pluggable_device_library` manually. But adding more info/examples in the docstring sounds good too. @jzhoulon, @annarev what do you think?\r\n> \r\n> (Edited to add: What @jzhoulon said. Please ignore me. sweat_smile)\r\n\r\n  Currently our preference is user no need call `load_pluggable_device_library`manually, plugin authors need to make sure that the package will be installed to the specified dir(`\u2026python_dir.../site-packages/tensorflow-plugins`),  it's optional for application programmers call this api to load the library if it is not installed to the specified directory(I didn't think out a usecase for this yet, @annarev  do you have opinion on this?)\r\n  May be we can add this to [TensorFlow API doc](https://www.tensorflow.org/api_docs/python/tf/load_library)? However, we need to wait the pluggable device implementation  added and related python API exposed. \r\n> \r\n> > How are these custom packages meant to be distributed? (pip? direct downloads?)\r\n> \r\n> I'll leave this to @jzhoulon et al.\r\n   \r\n   It is decided by plugin publisher, plugin publisher can decide whether their package is distributed by pip or direct downloads, the only requirement is the custom packages is need to be installed to the specified directory(\u2026python_dir.../site-packages/tensorflow-plugins).\r\n> \r\n> > Are there version matching requirements?\r\n> \r\n> Yes. Plugins and TensorFlow must at least use the same major version of StreamExecutor C API (as different major versions could have backward-incompatible changes). Please see the [StreamExecutor C API Versioning Strategy](https://github.com/tensorflow/community/blob/master/rfcs/20200612-stream-executor-c-api/C_API_versioning_strategy.md) document for more details. Also, this PluggableDevice mechanism is only guaranteed to work with the current TensorFlow stack (i.e., not necessarily with the new TFRT + MLIR stack).\r\n\r\n", "> Currently our preference is user no need call load_pluggable_device_librarymanually, plugin authors need to make sure that the package will be installed to the specified dir(\u2026python_dir.../site-packages/tensorflow-plugins), it's optional for application programmers call this api to load the library if it is not installed to the specified directory(I didn't think out a usecase for this yet, @annarev do you have opinion on this?)\r\n\r\nI don't have a usecase in mind either right now. We can always expose it later if we find such case.", "Tagging `ready-to-pull` back so this PR can re-enter the API review tomorrow.", "> @fchollet Thank you for the review!\r\n> * @kulinseth et al. are considering developing a [Metal](https://developer.apple.com/metal/) device plug-in. More details in the PluggableDevice design review notes  [[1](https://github.com/tensorflow/community/pull/262#issuecomment-663774327), [2](https://github.com/tensorflow/community/pull/262#issuecomment-674199085)].\r\n\r\nThanks @penpornk and @fchollet, yes we are actively working on adding the Metal device plug-in using the modular design's pluggable interface."]}, {"number": 43609, "title": "The format of the table is wrong.", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/PPOAgent#\r\n\r\n## Description of issue (what needs changing):\r\nTable format is wrong. HTML code like table, tr, td etc is visible.\r\n\r\n### Clear description\r\nTable format is wrong. HTML code like table, tr, td etc is visible in the 2nd half of the page This makes it hard to read.\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\nNo", "comments": ["Looks like someone fixed this 1 month ago.\r\nhttps://github.com/tensorflow/agents/commit/bf10ec26f1552d7ebb7672cf7cc3fa85a05c933e\r\nBut, in the website, the bug is still visible.", "@gowthamnatarajan,\r\nCould you please submit a new issue in the tensorflow/agents repo from [this link](https://github.com/tensorflow/agents/issues/new) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43606, "title": "Beginner tutorial fails with internal error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nMy pc runs windows 10 pro\r\nInstalled anaconda 3 individual edition, Python 3.8\r\nInstalled tensorflow 2.3.1 using pip install tensorflow\r\n\r\nBug appears running the very first beginner tutorial  beginner.ipynb.\r\nAn exception is raised while trying to create the model object in the third tutorial's command. Below a copy of the console putput:\r\n\r\nPython 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]\r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 7.16.1 -- An enhanced Interactive Python.\r\n\r\nimport tensorflow as tf\r\n\r\n2020-09-27 11:38:37.267183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\n2020-09-27 11:38:37.267183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-27 11:39:35.773790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-9eaab81d20e2>\", line 1, in <module>\r\n    model = tf.keras.models.Sequential([\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 116, in __init__\r\n    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 308, in __init__\r\n    self._init_batch_counters()\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 317, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 244, in _variable_v2_call\r\n    return previous_getter(\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2633, in default_variable_creator_v2\r\n    return resource_variable_ops.ResourceVariable(\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1507, in __init__\r\n    self._init_from_args(\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1661, in _init_from_args\r\n    handle = eager_safe_variable_handle(\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 242, in eager_safe_variable_handle\r\n    return _variable_handle_from_shape_and_dtype(\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 174, in _variable_handle_from_shape_and_dtype\r\n    gen_logging_ops._assert(  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 49, in _assert\r\n    _ops.raise_from_not_ok_status(e, name)\r\n\r\n  File \"C:\\Users\\nmb31\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nInvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse\r\n\r\n\r\n2020-09-27 11:38:37.267183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-27 11:39:35.773790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-09-27 11:39:36.282700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-09-27 11:39:36.282745: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-27 11:39:36.288963: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-27 11:39:36.292338: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-27 11:39:36.293694: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-27 11:39:36.298451: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-27 11:39:36.301791: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-27 11:39:36.311463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-27 11:39:36.312165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-27 11:39:36.312794: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-27 11:39:36.320212: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ed9b2f1ee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-27 11:39:36.320264: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-27 11:39:36.321043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-09-27 11:39:36.321082: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-09-27 11:39:36.321094: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-09-27 11:39:36.321102: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-09-27 11:39:36.321110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-09-27 11:39:36.321117: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-09-27 11:39:36.321125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-09-27 11:39:36.321132: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-09-27 11:39:36.321849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-27 11:39:36.379494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-27 11:39:36.379521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-27 11:39:36.379530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-27 11:39:36.380229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3128 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-09-27 11:39:36.383452: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ed9e39a4d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-27 11:39:36.383481: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0\r\n\r\n", "comments": ["@kreegah,\r\nI was able to run the code without any issues on TF v2.3, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/9a9529f36a2977b1b72444a89ec5526e/43606.ipynb).\r\n\r\nAlso, please check [this comment](https://github.com/tensorflow/tensorflow/issues/38518#issuecomment-619538236) from a similar issue and let us know if it helps. Thanks!", "Hi Abhilash Mahendrakar,\nwhat python version did you run your notebook on?\nI suspect my problem might have to do with my current python installation.\nA wild guess.\n\nOn Sun, Sep 27, 2020 at 1:03 PM Abhilash Mahendrakar <\nnotifications@github.com> wrote:\n\n> @kreegah <https://github.com/kreegah>,\n> I was able to run the code without any issues on TF v2.3, please find the\n> gist of it here\n> <https://colab.research.google.com/gist/amahendrakar/9a9529f36a2977b1b72444a89ec5526e/43606.ipynb>\n> .\n>\n> Also, please check this comment\n> <https://github.com/tensorflow/tensorflow/issues/38518#issuecomment-619538236>\n> from a similar issue and let us know if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43606#issuecomment-699653749>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAM7YRKFF3E3KIAHCKKBBR3SH5O6PANCNFSM4R3S6WIA>\n> .\n>\n", "amahendrakar I can run your notebook on colab without problems but if I try to do it on my local pc the error appears. Being a newb I'm left clueless.", "Hi,\nall is well. I guess. I followed a hint from stackoverflow and inserted  a\ncommand so tensorflow ignores my GPU, like this (please refer to the sample\ncode you linked):\n\n> import tensorflow as tf\n> tf.config.set_visible_devices([], 'GPU')\n\n# remaining code follows here\n\nIt worked perfectly, just as in the tutorial. So I guess we'll have to work\nwithout GPUs.\n\nThanks!\n\nOn Sun, Sep 27, 2020 at 1:03 PM Abhilash Mahendrakar <\nnotifications@github.com> wrote:\n\n> @kreegah <https://github.com/kreegah>,\n> I was able to run the code without any issues on TF v2.3, please find the\n> gist of it here\n> <https://colab.research.google.com/gist/amahendrakar/9a9529f36a2977b1b72444a89ec5526e/43606.ipynb>\n> .\n>\n> Also, please check this comment\n> <https://github.com/tensorflow/tensorflow/issues/38518#issuecomment-619538236>\n> from a similar issue and let us know if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43606#issuecomment-699653749>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAM7YRKFF3E3KIAHCKKBBR3SH5O6PANCNFSM4R3S6WIA>\n> .\n>\n", "@kreegah,\r\nSorry for the delayed response. \r\n> what python version did you run your notebook on?\r\n\r\nThe Python version of the notebook is Python v3.6.9.\r\n> It worked perfectly, just as in the tutorial. \r\n\r\nMarking the issue as closed since it is resolved. Please feel free to re-open the issue if necessary. Thanks!"]}, {"number": 43605, "title": "TF2.3 load subclassing model within tf.feature_column layer get ValueError: Could not find matching function to call loaded from the SavedModel", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.7.4\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to create a custom classification model using Tensorflow2.3 through tf.keras.Model subclassing method, in the subclass model init function, i use tf.feature_column layer to precess features. going through all of above part, i can train, save and reload the Saved_model, but when i use the reload model to do inference, i get the following error:\r\n```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (3 total):\r\n    * {'age': [35], 'education': ['Bachelors']}\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 4 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (3 total):\r\n    * {'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='education'), 'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='age')}\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (3 total):\r\n    * {'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='age'), 'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='education')}\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 3:\r\n  Positional arguments (3 total):\r\n    * {'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='inputs/age'), 'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='inputs/education')}\r\n    * False\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 4:\r\n  Positional arguments (3 total):\r\n    * {'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='inputs/education'), 'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='inputs/age')}\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n```\r\nWhen I try to create model with tf.Keras.sequential class or without tf.feature_column layer, every thing works fine, so how can I use the reloaded tf.Keras.subclassing model within tf.feature_column layer to do inference\uff1f\r\nThis puzzled me for days.\r\n\r\n**Describe the expected behavior**\r\n\r\n tf.Keras.subclassing model within tf.feature_column layer can do inference like sequential model.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nJupyer minimum demo:\r\n**https://www.kaggle.com/hailinfufu/notebookadf7121b80**\r\n\r\nHere is a minimal demo to reproduce my problem:\r\n\r\n```py\r\nimport pathlib\r\nimport time\r\n\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n__SELECT_COLUMN_NAMES = ['age', 'education', 'income_bracket']\r\n\r\n\r\ndef get_train_test_pandas_data():\r\n    # data can be download from: https://www.kaggle.com/uciml/adult-census-income?select=adult.csv\r\n    census = pd.read_csv(\"adult_data.csv\")\r\n\r\n    census['income_bracket'] = census['income_bracket'].apply(lambda label: 0 if label == ' <=50K' else 1)\r\n    census = census[__SELECT_COLUMN_NAMES]\r\n\r\n    y_labels = census.pop('income_bracket')\r\n    x_data = census\r\n\r\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.3)\r\n\r\n    return x_train, x_test, y_train, y_test\r\n\r\n\r\ndef get_feature_columns():\r\n    age = tf.feature_column.numeric_column(\"age\", dtype=tf.int64)\r\n    education = tf.feature_column.embedding_column(\r\n        tf.feature_column.categorical_column_with_hash_bucket(\"education\", hash_bucket_size=1000),\r\n        dimension=100)\r\n\r\n    feat_cols = [age, education]\r\n\r\n    return feat_cols\r\n\r\n\r\nif (tf.__version__ < '2.0'):\r\n    tf.enable_eager_execution()\r\n\r\nx_train, _, y_train, _ = get_train_test_pandas_data()\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((dict(x_train), y_train))\r\n\r\ndataset = dataset.shuffle(len(x_train)).batch(4)\r\n\r\nfeat_cols = get_feature_columns()\r\n\r\n\r\nclass mymodel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(mymodel, self).__init__()\r\n        self.layer1 = tf.keras.layers.DenseFeatures(feature_columns=feat_cols)\r\n        self.layer2 = tf.keras.layers.Dense(10, activation='relu')\r\n        self.layer3 = tf.keras.layers.Dense(10, activation='relu')\r\n        self.layer4 = tf.keras.layers.Dense(1, activation='sigmoid')\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None, mask=None):\r\n        x = self.layer1(inputs)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n        return x\r\n\r\n\r\nmodel = mymodel()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(dataset, epochs=1)\r\n\r\n\r\n__SAVED_MODEL_DIR = './saved_models/census_keras/{}'.format(int(time.time()))\r\npathlib.Path(__SAVED_MODEL_DIR).mkdir(parents=True, exist_ok=True)\r\n\r\ntf.saved_model.save(model, export_dir=__SAVED_MODEL_DIR)\r\n```\r\n\r\nyou can replace model = mymodel() with\r\n\r\n```\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.DenseFeatures(feature_columns=feat_cols),\r\n    tf.keras.layers.Dense(10, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='relu'),\r\n    tf.keras.layers.Dense(1, activation='sigmoid')\r\n])\r\n```\r\nthat will work fine.\r\n\r\nAfter trained and saved the model, i try to load the SavedModel to do predict:\r\n\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\n# loaded_model = tf.keras.models.load_model(\"./saved_models/census_keras/1601196783\")  # tf.saved_model.load(\"saved/1\")\r\nloaded_model = tf.keras.models.load_model(\"./saved_models/census_keras/1601196783\")\r\n\r\ny_pred = loaded_model.call({\"age\": [35],\r\n                            \"education\": [\"Bachelors\"]})\r\nprint(y_pred)\r\n\r\n\r\ny_pred = loaded_model.call({\"age\": [40],\r\n                            \"education\": [\"Assoc-voc\"]})\r\nprint(y_pred)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@fuhailin \r\nPlease provide with all dependencies to run the code shared or if possible share a colab gist with issue reported.\r\nI ran the code shared and face [this error](https://colab.research.google.com/gist/Saduf2019/05a756219252f40dc5e4b24b39d3ec32/untitled418.ipynb)", "> @fuhailin\r\n> Please provide with all dependencies to run the code shared or if possible share a colab gist with issue reported.\r\n> I ran the code shared and face [this error](https://colab.research.google.com/gist/Saduf2019/05a756219252f40dc5e4b24b39d3ec32/untitled418.ipynb)\r\n\r\nUpdated, please check this colab : https://colab.research.google.com/gist/fuhailin/32e3b834cff5856c53d719fe877414cb/untitled418.ipynb", "@fuhailin \r\nI cannot download the csv, please share the dataset for us to replicate the issue.", "> @fuhailin\r\n> I cannot download the csv, please share the dataset for us to replicate the issue.\r\n\r\n@Saduf2019 \r\nI have created a [gist file](https://gist.githubusercontent.com/fuhailin/673b326fb4e7f653a35461326c700d2e/raw/341b36e21fb327af318842b4dad4a844cc5bfb60/adult_data.csv) to load, please check that [colab]( https://colab.research.google.com/gist/fuhailin/32e3b834cff5856c53d719fe877414cb/untitled418.ipynb) again.", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/77742b1c6572850ce87a6f8db0d1c604/untitled418.ipynb)", "Hi there--\r\n\r\nYou can do inference with your subclassed tf.keras.Model `loaded_model` by:\r\n```\r\ny_pred = loaded_model.call({\"age\": [[35]], \"education\": [[\"Bachelors\"]]})\r\ny_pred = loaded_model.call({\"age\": [[40]], \"education\": [[\"Assoc-voc\"]]})\r\n```\r\nor \r\n```\r\ny_pred = loaded_model.call({\"age\": [[35], [40]],  \"education\": [[\"Bachelors\"], [\"Assoc-voc\"]]})\r\n```\r\nThe error message you received stated that it was expecting arguments \"{'age': TensorSpec(shape=(None, 1), dtype=tf.int64, name='age'), 'education': TensorSpec(shape=(None, 1), dtype=tf.string, name='education')}\", so the shape of your inputs should be of shape (None, 1). \r\n\r\nYou can check the shape by:\r\n```\r\na = tf.constant([35])\r\nb = tf.constant([[35]])\r\nc = tf.constant([[35],[40]])\r\na.shape # TensorShape([1])\r\nb.shape # TensorShape([1, 1])\r\nc.shape  # TensorShape([2, 1])\r\n```\r\nThe `None` in the TensorSpec shape parameter means that the batch size is variable.\r\n\r\nClosing this issue, but please reopen if I overlooked something. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43605\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43605\">No</a>\n"]}, {"number": 43604, "title": "tfio.audio.AudioIOTensor doesn't prepare data in time in loop.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No mobile device used.\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CPU\r\n- GPU model and memory: CPU\r\n\r\n\r\n**Describe the current behavior**\r\nTraining loop crashes. It seems ```tfio.audio.AudioIOTensor``` doesn't prepare data in time when processing. It lazy loads and supposed to prepare data when called.\r\n\r\n**Describe the expected behavior**\r\n```tfio.audio.AudioIOTensor``` prepares audio data and process in time.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\ndataloader.py\r\n```py\r\nimport os\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_io as tfio\r\n\r\n\r\nduration = 5\r\nrate = 20000\r\nsamples = duration * rate\r\n\r\n\r\ndef process_paths(file_paths):\r\n  \"\"\"receive a batch of filepaths and return a batch of db mel-spectrogram and batch of labels\r\n  params\r\n  filepaths: string[]\r\n  \r\n  return\r\n  (db_mel_spectrograms: Tensor<tf.float32>, labels: Tensor<tf.string>)\r\n  \"\"\"\r\n  db_mel_spectrograms, labels = tf.map_fn(\r\n    fn=process_path,\r\n    elems=file_paths,\r\n    fn_output_signature=(tf.float32, tf.string)\r\n  )\r\n  return db_mel_spectrograms, labels\r\n\r\ndef process_path(file_path):\r\n  label = tf.strings.split(file_path, os.sep)[-2]\r\n  audio = tfio.audio.AudioIOTensor(file_path, dtype=tf.float32)\r\n  re_audio = tfio.audio.resample(\r\n    audio.to_tensor(),\r\n    rate_in=tf.cast(audio.rate, tf.int64),\r\n    rate_out=rate\r\n  )\r\n  re_audio_1c = tf.reduce_mean(re_audio, axis=-1)  # [samples, channels] => [samples]\r\n  zeros = tf.math.maximum(samples - tf.shape(re_audio_1c)[0], 0)\r\n  paddings = [[zeros // 2, zeros // 2 + zeros % 2]]\r\n  pad_audio = tf.pad(re_audio_1c, paddings=paddings, mode='CONSTANT')  # pad if audio is too short\r\n  cropped_audio = tf.image.random_crop(pad_audio, [samples])\r\n\r\n  return cropped_audio, label\r\n\r\ndef get_data(glob_path, batch_size):\r\n  \"\"\"\r\n  get optimized tf.data.Dataset. Use like\r\n  data = get_data(path, 128)\r\n  epochs = 10\r\n  for epoch in range(epochs):\r\n    for spectrograms, labels in data:\r\n      train_this_batch(spectrograms, labels)\r\n      \r\n  Args:\r\n    glob_path: string like /path/to/data/*.mp3\r\n    batch_size: int\r\n  Returns:\r\n    tf.data.Dataset with optimization\r\n  \"\"\"\r\n# The code in this comment is not optimized (no parallel, no prefetch), but even when I use this code, training loop fails miserably.\r\n#  data = (\r\n#    tf.data.Dataset.list_files(glob_path)\r\n#    .batch(\r\n#      batch_size,\r\n#      drop_remainder=True\r\n#    )\r\n#    .map(\r\n#      process_paths,\r\n#      num_parallel_calls=1\r\n#    )\r\n#  )\r\n#  \r\n\r\n# This is optimized data preparation\r\n  data = (\r\n    tf.data.Dataset.range(2)\r\n    .interleave(\r\n      lambda *args: tf.data.Dataset.list_files(glob_path),\r\n      num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n    )\r\n    .batch(\r\n      batch_size,\r\n      drop_remainder=True\r\n    )\r\n    .map(\r\n      process_paths,\r\n      num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n    )\r\n    .cache()\r\n    .prefetch(tf.data.experimental.AUTOTUNE)\r\n  )\r\n\r\n  return data\r\n```\r\n\r\n\r\nmain.py\r\n```py\r\nimport time\r\n\r\nimport dataloader\r\n\r\ndata = dataloader.get_data('path/to/audio/*/*.mp3', 64)  # Sorry I updated this line to call function.\r\n# path/to/audio/\u30e1\u30b8\u30ed/1.mp3\r\n# path/to/audio/\u30e1\u30b8\u30ed/2.mp3\r\n# ...\r\n# path/to/audio/\u30a6\u30b0\u30a4\u30b9/1.mp3\r\n# path/to/audio/\u30a6\u30b0\u30a4\u30b9/2.mp3\r\n# ...\r\n\r\nfor f, l in data:\r\n  # Training code is left out as it is irrelevant. This time.sleep is enough to reproduce.\r\n  time.sleep(0.01)  \r\n```\r\n\r\n\r\n**Other info / logs** \r\n\r\nWhen I replace ```tfio.audio.AudioIOTensor(path)``` with ```tf.zeros((6000,), dtype=tf.float32)``` , whole epoch run finely. I think tfio.audio.AudioIOTensor has no data when read, so below error happens as it describes. I have no idea how to data prepared when reading.\r\n\r\n```\r\n> python main.py2020-09-27 21:32:37.473956: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\r\n2020-09-27 21:32:37.582070: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-27 21:32:37.593090: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc74c15d830 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-27 21:32:37.593109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-27 21:32:47.870608: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 1100254 from 0 failed: 0\r\n2020-09-27 21:32:47.895352: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 1100254 from 0 failed: 0\r\n2020-09-27 21:32:48.574386: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 9796964 from 0 failed: 0\r\n2020-09-27 21:32:50.874190: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 307678 from 0 failed: 0\r\n2020-09-27 21:32:56.575449: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at audio_kernels.cc:212 : Invalid argument: read 3505630 from 0 failed: 0\r\nTraceback (most recent call last):\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 2102, in execution_mode\r\n    yield\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 758, in _next_internal\r\n    output_shapes=self._flat_output_shapes)\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 2610, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: read 1100254 from 0 failed: 0\r\n\t [[{{node map/while/body/_1/map/while/IO>AudioReadableRead}}]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 21, in <module>\r\n    for f, l in data:\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 736, in __next__\r\n    return self.next()\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 772, in next\r\n    return self._next_internal()\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 764, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 2105, in execution_mode\r\n    executor_new.wait()\r\n  File \"/Users/asterisk37n/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\", line 67, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: read 1100254 from 0 failed: 0\r\n\t [[{{node map/while/body/_1/map/while/IO>AudioReadableRead}}]]\r\n2020-09-27 21:33:04.393965: W tensorflow/core/kernels/data/cache_dataset_ops.cc:798] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.```", "comments": ["@asterisk37n \r\n\r\nCan you please share the dataset you are using in the code.Thanks!\r\n", "@ravikyram Sure. This is just 1GB data but I reproduced with it.\r\nhttps://drive.google.com/file/d/1WRywjR8pJaPkur3pPpd81mxMf1vRnZMv/view?usp=sharing", "@asterisk37n \r\n\r\nI have tried in colab with TF 2.3 and i am seeing the  error message(`TypeError: 'module' object is not callable`). Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/67d0a29ccb8d51a02e6bfe60d0df0228/untitled403.ipynb). Please, help me with reproducible code, to localize the issue faster.Thanks!", "@ravikyram \r\nSorry, the data loading line should be\r\n```py\r\ndata = dataloader.get_data('path/to/audio/*/*.mp3', 64)\r\n```\r\n, not ~~data = dataloader('path/to/audio/*/*.mp3', 64)~~ to call function, not module. I fixed and updated the code I shared in this issue (Scroll up to see).\r\n\r\nI reproduced in colab. I hope you find this notebook useful:\r\nhttps://colab.research.google.com/drive/1AGxb0cYWEHxpj-cWL_X6YT3lfW57H92Q?usp=sharing", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200929`) and was able to reproduce the issue.PLease, find the gist [here.](https://colab.research.google.com/gist/ravikyram/a6214cf24cad63043638daa58d38325c/untitled403.ipynb)Thanks!", "@asterisk37n  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you for fixing! Sorry I don't have time to review", "@asterisk37n I don't have the access to the files to reproduce the issue from my end.Could you please try to execute your code using the latest TF v2.6 and let us know if the issue still persists ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43604\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43604\">No</a>\n"]}, {"number": 43603, "title": "Batch  normalisation with shared vision model sometimes causes NaN.", "body": "Hello.\r\n\r\n**Intorduction:**\r\n\r\nI am writing a neural network that analyses video. For analyzing different frames I use shared vision model (the same set of layers and weights is applied over various images) and then I concatenate these columns into one model.\r\n\r\nWhy do I use Shared Vision Model?\r\nAs 3D convolutions and TimeDistributed models are not supported in conversion to TFLite that goes in pair with Tensorflow 1...\r\nI need to resort to using a **Shared Vision Model**.\r\n\r\n**When Problem does NOT occur:**\r\nTraining goes without any problems when I do not use Batch Normalization.\r\nTraining goes without problems if I use Batch Normalization in layers of neural network that are after the consatenation of frames.\r\nTraining also goes well if I train columns of neural network that analyze single images, with batch normalization, using TimeDistributed Embedding.\r\nTraining goes well if I use for the same purpose, 3D convolutions with depth (along time axis) set to 1.\r\n\r\n**When Problem occurs:**\r\nIf I train neural network with usage of Batch Normalization on layers that belong to Shared Vision Model... training sometimes ends with NaN loss.\r\n\r\nWhen in the same situation I switch batch normalization off, the problem never occurs.\r\n\r\nHere is an example of training with shared vision model and BatchNormalization layers activated:\r\n![obraz](https://user-images.githubusercontent.com/4332432/94397173-8fbf4880-0163-11eb-8bb5-8c24ef77b004.png)\r\n\r\n\r\nMy questions are:\r\n\r\n- What could have gone wrong?\r\n\r\n- Is it possible that batch normalization with usage of Shared Vision model has problem with accumulation of values from various instances?\r\n- Is it possible that Batch Normalization returns NaN when all samples in a batch returned the same value (so variance is 0 and difference between mean and max is 0)?\r\n\r\nCould NaN have came from a bug in implementation of Keras BatchNormalization() layer?\r\n(I have seen a suspicious line of code:) \r\n`# sample variance - unbiased estimator of population variance`\r\n`variance *= sample_size / (sample_size - (1.0 + self.epsilon))`\r\n\r\nFor me it seems that internal brackets cause that epsilon is subtracted instead of added, which for sample_size 1 will cause variance to be devided by a negative value.\r\nInside of K.normalize_batch_in_training() -> _broadcast_normalize_batch_in_training() -> tf.nn.batch_normalization(), there is line of code:\r\ninv = math_ops.rsqrt(variance + variance_epsilon)\r\n\r\nI guess that in some scenarios, negative value of variance could lead to rsqrt() from a negative value, which results in NaN...\r\n\r\n\r\nNaN appearance during training is so unpredictable, dependent on outputs of previous layers and on combination of random batch, that I am not able to exactly provide inputs that cause mentioned problem.\r\nHowever analysis what could have gone wrong (division by zero, root from minus value, or something else) could lead to finding a volatile spot.\r\n\r\nI do not knwo if the one I have fund is the one...\r\nI do nto know why this occurs for me only randomly and only in case of usage of Shard Vision Model.\r\n\r\nCould it be wrong aggregation of data, that occurs with Shared Vision Model, but not with TimeDistributed wrapper and not with 3DConvolutions which have more stable data aggregation?\r\n\r\nCould this error occur when all activations of layer predecessing to BatchNormalization() are the same across the batch?\r\n\r\nThat issue is important for me.", "comments": ["I see that there are people who have experienced the same issue for past 2 years.\r\nhttps://github.com/keras-team/keras/issues/11927\r\n\r\nTherefore I am not the only one - they have posted samples related maybe not to video analysis, but to Triplet and Siamese networks, which are the most common use cases of Shared Vision Model.\r\n\r\nI strongly suspect there is something wrong with moving mean not being aggregated well across the instances of shared layer.\r\nReal solution needed urgently.", "Can you try to debug your case with https://www.tensorflow.org/tensorboard/debugger_v2?hl=en#using_debugger_v2_gui_to_find_the_root_cause_of_nans ?", "Hello @bhack .\r\n\r\n**I will need help activating debugger correctly in TensorFlow 1, as I am encountering an error when I want to start it.**\r\n\r\n> Maybe it will be the easiest to reproduce the issue with simple code samples of siamese network provided in thread keras-team/keras#11927 , as according to thread participants problem still occurs in newest TensorFlow and their code sample reveals NaN quickly.\r\n\r\n\r\nI will try my best to activate debugger to help finding the source with my older version of tensorflow (as issue seems so still be the same).\r\n\r\nI have a following problem when I activate debugger under Kears 2.2.4 with tensorflow-gpu 1.12.3 :\r\n\r\nIn that version, there is no package \"experimental\" in tf.debugging.**experimental**.enable_dump_debug_info() introduced yet, so I tried the closest thing available for debugging, just like presented in that video: https://youtu.be/XcHWLsVmHvk?t=871\r\n\r\nFirst I activated my tensorboard:\r\n`tensorboard --logdir ./logs --debugger_port 999`\r\n\r\nMy TensorBoard advised me to use the following code:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\nimport keras\r\n\r\nkeras.backend.set_session(\r\n    tf_debug.TensorBoardDebugWrapperSession(tf.Session(), \"karol-Precision-7520:999\"))\r\n\r\n...........(and here my training pipeline, creating the Keras model and running the training).......................\r\n```\r\nI pasted it in the beginning of the code.\r\n\r\nWhen I tarted my training pipeline, during model building, the code in that debug mode stopped with the following call stack (surprisingly, on the BatchNormalization() layer in the call stack):\r\n\r\n>   File \"/home/karol/Repozytoria/xxx/common/models/merged_2_stages_saveable_splitable.py\", line 80, in _acquire_single_image_column\r\n>     if batch_normalisation: image_column = K.layers.BatchNormalization()(image_column)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 457, in __call__\r\n>     output = self.call(inputs, **kwargs)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/keras/layers/normalization.py\", line 185, in call\r\n>     epsilon=self.epsilon)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1858, in normalize_batch_in_training\r\n>     if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 292, in _has_nchw_support\r\n>     gpus_available = len(_get_available_gpus()) > 0\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 278, in _get_available_gpus\r\n>     _LOCAL_DEVICES = get_session().list_devices()\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 199, in get_session\r\n>     [tf.is_variable_initialized(v) for v in candidate_vars])\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/grpc_wrapper.py\", line 223, in run\r\n>     self._sent_graph_version)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/tensorflow/python/debug/wrappers/grpc_wrapper.py\", line 63, in publish_traceback\r\n>     send_source=True)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/tensorflow/python/debug/lib/source_remote.py\", line 209, in send_graph_tracebacks\r\n>     graph=graph, send_source=send_source)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/tensorflow/python/debug/lib/source_remote.py\", line 174, in _send_call_tracebacks\r\n>     stub.SendTracebacks(call_traceback)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/grpc/_channel.py\", line 826, in __call__\r\n>     return _end_unary_response_blocking(state, call, False, None)\r\n>   File \"/home/karol/.local/lib/python3.6/site-packages/grpc/_channel.py\", line 729, in _end_unary_response_blocking\r\n>     raise _InactiveRpcError(state)\r\n> grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\r\n> \tstatus = StatusCode.UNAVAILABLE\r\n> \tdetails = \"failed to connect to all addresses\"\r\n> \tdebug_error_string = \"{\"created\":\"@1601277231.862699005\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3981,\"referenced_errors\":[{\"created\":\"@1601277231.862696725\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\"\r\n> >\r\n> \r\n> Process finished with exit code 1\r\n\r\n**I am wondering what I can do to accelerate the investigation.\r\nCan I activate the debugger in TensorFlow 1 without the quoted bug?**\r\n\r\nI can present my part of neural network (not exactly the same, as I am bound by NDA, but idea stays the same):\r\n\r\nI am creating a shared vision model that takes 1 image on input.\r\nIt is a bunch of layers starting with:\r\n`input_tensor = K.layers.Input(shape=(self.img_rows,self.img_cols,self.channels))`\r\nThen there I have bunch of Conv2D, LeakyReLU and MaxPooling2D layers.\r\nand ending with:\r\n```\r\n        if batch_normalisation: image_column = K.layers.BatchNormalization()(image_column)\r\n        self.single_image_column = K.models.Model(input_tensor,image_column,name=\"single_image_column\")\r\n```\r\nLater I have a model that looks at pairs of images.\r\nIt has 2 inputs:\r\n\r\n```\r\nrecent_input_tensor = K.layers.Input(shape=self.single_image_column.output_shape[1:])\r\nlatter_input_tensor = K.layers.Input(shape=self.single_image_column.output_shape[1:])\r\nimages_pair_column = K.layers.Concatenate(axis=1 if self.channels_first else -1)([recent_input_tensor, latter_input_tensor])\r\n```\r\nAgain, story is the same, bunch of Conv2D, LeakyReLu and MaxPooling2D layers.\r\nModel ends with:\r\n```\r\nif batch_normalisation: images_pair_column = K.layers.BatchNormalization()(images_pair_column)\r\nself.images_pair_column = K.models.Model([recent_input_tensor,latter_input_tensor],images_pair_column,name=\"images_pair_column\")\r\n```\r\nAnd analogically, I have a third brick of the model, that concatenates 2D feature maps information from 2 images_pair columns (my total field of view is 3 video frames).\r\nThere is again bunch of Conv2D and LeakyReLu layers, there is also a global pooling and BatchNormalization.\r\nIn the end I have a few fully connected Dense layers and 1 neuron output of regression with mse loss function.\r\n\r\nI think that code sampels presented in keras-team/keras#11927 will be much simpler to debug, as they get the same kind of error, with a classical Siamese model (much less complex).\r\n\r\nWhat is common among all of the cases:\r\n- Problem happens when BatchNormalization() layer is in a shared model that is applied over 2 or more (in my case more) input images.\r\n![obraz](https://user-images.githubusercontent.com/4332432/94408064-97d3b400-0174-11eb-9cc8-1866bef87b2c.png)\r\n\r\n- Output layers from various images, processed by shared vision model are concatenated. When BatchNormalization() layer is in that concatenated part, it does not cause the problem. \r\n\r\nWhen I have BatchNormalization in the part of model that was concatenated (just before ending) - so there is just 1 instance...  it does not cause a problem.\r\n![obraz](https://user-images.githubusercontent.com/4332432/94408231-cb164300-0174-11eb-8e96-ba4fa1df5d1a.png)\r\n", "I don't think that `https://github.com/keras-team/keras/` ticket system is currently actively maintained at least until Keras will be standalone again (See https://github.com/keras-team/keras/releases/tag/2.4.0)\r\n\r\nWe need a very, very minimal but completely runnable standalone example (copy, paste and run) or Colab to reproduce your use case.\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43602, "title": "i can't  find tutorials. Examples. ", "body": "\r\n", "comments": ["@YahboomTechnology \r\nPlease specify what information are you looking for.\r\nPlease refer to below tensorflow tutorial links and let us know if it helps:\r\n[link](https://www.tensorflow.org/tutorials/quickstart/beginner), [link1](https://www.tutorialspoint.com/tensorflow/index.htm), [link2](https://www.datacamp.com/community/tutorials/tensorflow-tutorial), [link3](https://www.guru99.com/tensorflow-tutorial.html).", "I have the same problem. I found this commit \r\nhttps://github.com/tensorflow/tensorflow/commit/42e122e6a0acaa7bee32ce88e2423ed99b3a0d84\r\nwhere the examples were moved into a seperate repo 3 days ago. I havent found the new repo\r\n\r\nfor now, you can see the file contents in the commit diff", "i also can't find new repo. ", "@florianmarcher  @Dolphinzzx @YahboomTechnology \r\nCan you please be specific which tutorials are you talking about", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43601, "title": "Dynamic Library loading failed 'cudart64_101.dll' & AttributeError: module 'tensorflow' has no attribute 'Session'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, 64 config\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip3 install upgrade --tensorflow\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: Created a virtual environment. Used this command: conda create -n tfp3.8 python= 3.8\r\n- Bazel version (if compiling from source): No, not compliling from source.\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: NVIDIA GeForce MX250 \r\n\r\n**Problem Description:**\r\n\r\nFirstly it shows this after the installation of tensorflow:\r\n\r\n2020-09-27 13:11:39.558775:\r\nW tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-09-27 13:11:39.563129:\r\nI tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nAnd secondly, \r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-f75057d1d95f> in <module>\r\n----> 1 sess = tf.Session()\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'Session'\r\n\r\n**Steps Performed:**\r\n\r\nFor the first problem, I executed the following steps:\r\nStep 1: conda activate tfp3.8\r\nThis logs into the environment for the created tensorflow.\r\n\r\nStep 2: python\r\nLogs into the python terminal\r\n\r\n>>import tensorflow as tf\r\nThis resulted in the above error enlisted as the first.\r\n\r\nFor the second problem, I executed the following steps in the jupyter notebook:\r\nStep 1:  import tensorflow as tf //Success\r\nStep 2:  hello = tf.constant(\"Hello, Tensorflow!\") //Success\r\nStep 3: sess = tf.Session() //Oops! Attribute Error \r\n\r\n", "comments": ["@alinasahoo \r\n\r\nCan you please refer this [SO link](https://stackoverflow.com/questions/55142951/tensorflow-2-0-attributeerror-module-tensorflow-has-no-attribute-session/59179238#59179238) and see if it helps you. Thanks!", "This works fine for me:\r\n\r\n>> import tensorflow as tf  // Success\r\n>> msg = tf.constant('Hello, TensorFlow!') // Success along with some opened dynamic libraries CUDA and some failed cuDNN libraries\r\n>> tf.print(msg) // Hello, Tensorflow!\r\n\r\nAbsolutely fine. However another approach can work too.\r\n>> import tensorflow as tf // Success\r\n>> tf.compat.v1.disable_eager_execution() // Success\r\n>> hello = tf.constant('Hello, TensorFlow!') // Success\r\n>> sess = tf.compat.v1.Session() // Success along with some opened dynamic libraries CUDA and some failed cuDNN libraries\r\n>> print(sess.run(hello)) // b'Hello, TensorFlow!'\r\n\r\nThis too works absolutely fine. Thank you @ravikyram .\r\n\r\n", "@alinasahoo \r\n\r\nPlease, close this thread if your issue was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43601\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43601\">No</a>\n", "I have the same problem.\r\nI try to run my codes to GPU and I have this error.\r\n\r\ncould anyone help me?\r\nthank you"]}, {"number": 43600, "title": "Some times  ,interpreter_->Invoke() doesn't return", "body": "**System information**\r\nRK3399 \r\nLinux localhost 4.4.126 #1070 SMP PREEMPT Fri Sep 18 16:41:27 HKT 2020 aarch64\r\n\r\nIn mediapipe project, this function interpreter_->Invoke()  was called but don't return,so the program got stucked.", "comments": ["@lovehuanhuan,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43599, "title": "tensorflow returns error 132 on macbook pro 2009", "body": "Hello, i am trying to run tensorflow, with the following setup:\r\nmacbook mid 2009\r\nUbuntu 20.04\r\nPycharm latest version\r\npython 3.7.9, installed with pycharm\r\n\r\nnow if i use tensorflow 2.3.1 i get an error 132\r\nas suggested in others posts i down grade to tensorflow 1.5 but i get a different error; ModuleNotFoundError: No module named 'tensorflow.python.platform' but it is install on the virtual environment.\r\n\r\nis there a fix or a work around?\r\n", "comments": ["@fabiogeraci \r\nPlease refer to these issues and let us know if it helps: #42438 [link](https://stackoverflow.com/questions/33977273/no-module-named-tensorflow-python-platform), [link1](https://github.com/tensorflow/tensorflow/issues/374), [this comment](https://github.com/tensorflow/tensorflow/issues/40556#issuecomment-647536055).\r\nYou can try: conda install tensorflow\r\n\r\nCan you please share the error log of both the errors, as you have not filled the issue template which makes it difficult to analyse.", "i have attached the log file.\r\n\r\nwhen you say conda install tensorflow, do you mean on a terminal or inside pycharm?\r\n\r\n[idea.log](https://github.com/tensorflow/tensorflow/files/5288751/idea.log)\r\n\r\n", "@fabiogeraci \r\nWe have support for 2.x, please try on 2.x and let us know if you face any issues.", "> @fabiogeraci\r\n> We have support for 2.x, please try on 2.x and let us know if you face any issues.\r\n\r\nI have tried all the version from and i get the same result, also tried to run anaconda and tensorflow crashes that too.\r\n\r\ni added cpus info\r\n\r\n> processor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 23\r\nmodel name\t: Intel(R) Core(TM)2 Duo CPU     P7550  @ 2.26GHz\r\nstepping\t: 10\r\nmicrocode\t: 0xa0b\r\ncpu MHz\t\t: 2255.338\r\ncache size\t: 3072 KB\r\nphysical id\t: 0\r\nsiblings\t: 2\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit\r\nbogomips\t: 4510.67\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\n", "any news, please?", "Apologies for the delay in response. Were you able to install TF 2.3 in virtual env successfully?\r\nAlso what is your macOS version?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43599\">No</a>\n"]}, {"number": 43598, "title": "Remove error message if >1 DataAdapter can handle data ", "body": "This issue was touched upon in [1](https://github.com/tensorflow/tensorflow/issues/41328), [2](https://github.com/tensorflow/tensorflow/issues/33811) and [3](https://github.com/tensorflow/tensorflow/issues/33734). I believe that it should be handled differently:\r\n\r\nIf there is more than single DataAdapter which can handle certain data - great, just pick the best of them and return it from the function. It should not be the user problem if developers made adapters that can handle diverse types of data.\r\nIf some DataAdapters better fit than others. they can be prioritized by changing order in `ALL_ADAPTER_CLS`. Alternatively, adapters should be selected by type of data, with 1:1 clear match of data type and adapter.\r\n\r\nHere is an example code generating this error.\r\n```\r\nimport tensorflow as tf\r\n\r\nfeatures =  tf.random.normal(shape=(100, 1, 10))\r\nlabels = tf.random.normal((100,1,1))\r\ndataset = tf.data.Dataset.from_tensor_slices((features,labels))\r\nds_iter = iter(dataset) # tensorflow.python.data.ops.iterator_ops.OwnedIterator\r\nfeatures.shape, labels.shape\r\n\r\nx = tf.keras.layers.Input(shape=[10])\r\ny_pred = tf.keras.layers.Dense(1, activation='sigmoid', name=\"L0\")(x)\r\nmodel = tf.keras.Model(x, y_pred)\r\nmodel.compile(optimizer='sgd', loss='mse',)\r\nmodel.fit(ds_iter, epochs=1)\r\n```\r\n\r\nThe code above triggers error `RuntimeError: Data adapters should be mutually exclusive for handling inputs`, however this is a good kind of problem to have. There are 2 adapters that claim ability to  handle such iterator: `CompositeTensorDataAdapter` and `GeneratorDataAdapter`.\r\n\r\nI suggest not to throw `RuntimeError: Data adapters should be mutually exclusive for handling inputs ` but instead just return `adapter_cls[0]`. The change requires deleting [5 lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L958-L96) in `tf.python.keras.engine.data_adapter.select_data_adapter`\r\n\r\n\r\nLater I ran tests with `tensorflow.python.data.ops.iterator_ops.OwnedIterator` forcing it to choose one of the DataAdapters:\r\n1) `GeneratorDataAdapter` works fine with it,\r\n\r\n2) `CompositeTensorDataAdapter` fails with error `AttributeError: 'IteratorSpec' object has no attribute '_to_batched_tensor_list'` [error stack here](https://pastebin.com/g1RxEhdc) Apparently some change is needed either in `CompositeTensorDataAdapter.can_handle()` or in the handling process itself.", "comments": ["let me withdraw this PR and check the build errors. "]}, {"number": 43597, "title": "How add two pre defined keras models in keras tensorflow", "body": "I've wanted to define network that has two input and use EfficientNetB1 to extract features and fine-tune on new layers so I've tried below code:\r\n\r\n    def createNet(self,shape):\r\n        FE1 = K.applications.EfficientNetB1(include_top=False, input_shape=shape)\r\n        FE2 = K.applications.EfficientNetB1(include_top=False, input_shape=shape)\r\n        inp1 = FE1.input\r\n        out1 = FE1.layers[-1].output\r\n        inp2 = FE2.input\r\n        out2 = FE2.layers[-1].output\r\n\r\n        merged_out = K.layers.concatenate((out1, out2))\r\n        # .... other layers\r\n        self.model = K.models.Model(inputs=[inp1, inp2], outputs=[merged_out])\r\n        \r\n        self.model.summary()\r\n\r\nBut I got this error :\r\n\r\n    ValueError: The name \"stem_conv_pad\" is used 2 times in the model. All layer names should be unique.\r\n\r\nSo how can I define such a this network ?", "comments": ["@aligoglos \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, help us with the reproducible code to localize the issue faster.Thanks!", "You may want to rename all the layers of either FE1 or FE2 in your example.\r\nFor instance;\r\n```python\r\nfor layer in FE2.layers:\r\n    layer._name = 'new ' + str(layer._name)\r\n    print(layer.name)\r\n```", "Thanks it work.", "@ymodak thanks for the solution.\r\nOne thing to add, there shouldn't be a space in the new name, so rather:\r\n`layer._name = 'new_' + str(layer._name)`\r\n"]}, {"number": 43596, "title": "RuntimeError: Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 412 (FILL) failed to invoke.", "body": "Hi,\r\nI tried to run tflite model with INT8 post training. There is  no error converting .pb to .tflite. But error occurs when run inference on INT8 tflite model. BTW, there is no error when run inference on FP32 tflite model.\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from : bianry\r\n- TensorFlow version : tf-nightly 2.4.0-dev20200917\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n[{'name': 'input_x', 'index': 0, 'shape': array([ 1, 50], dtype=int32), 'shape_signature': array([-1, 50], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n[{'name': 'output/y_hat', 'index': 3339, 'shape': array([   1, 2363], dtype=int32), 'shape_signature': array([  -1, 2363], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nTraceback (most recent call last):\r\n  File \"run_tflite.py\", line 67, in <module>\r\n    interpreter.invoke()\r\n  File \"/home/ai/anaconda5/envs/tf-nightly2.4/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 539, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Fill only currently supports int32, int64, float32, bool, string for input 1, got 9.Node number 412 (FILL) failed to invoke.\r\n\r\n**Standalone code to reproduce the issue** \r\nConvert:\r\n\r\n##INT8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representive_dataset_gen():\r\n    num_calibration_steps = 10\r\n    for i in range(num_calibration_steps):\r\n        random_num = np.random.random_integers(0,10000, size=[50]).astype('int32')\r\n        random_input = np.expand_dims(random_num, 0)\r\n        yield [random_input]\r\n\r\nconverter.representative_dataset = representive_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n    \r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n\r\n\r\nInference:\r\nimport tensorflow as tf\r\n\r\nmodel_path = './model_int8.tflite'\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=model_path)\r\n\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\nprint(str(input_details))\r\n\r\noutput_details = interpreter.get_output_details()\r\nprint(str(output_details))\r\n\r\ninput_arr = np.random.random_integers(0,10000, size=[50])\r\ninput_arr_expanded = np.expand_dims(input_arr, 0)\r\n\r\ninput_arr_expanded = input_arr_expanded.astype('int32')\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], input_arr_expanded)\r\ninterpreter.invoke()\r\n\r\n", "comments": ["@le8888e,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to build the model, the SavedModel file itself and `model_int8.tflite` file as well. Thanks!\r\n", "I managed to exclude TFLite fill op from quantizable op set. Could you try the conversion with the tomorrow's TF nightly version?", "Hi @abattery ,\r\n\r\nThank you. But I tried both my model and ResNet50 with INT8 on x86 CPU, they both run much slower than FP32. Given the discuss in https://github.com/tensorflow/tensorflow/issues/40183, I think there is no speed up with INT8 on x86 CPUs.", "@le8888e It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.6.0 and let us know if the issue still persists? Please refer to the [link](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) and let us know if it helps ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43596\">No</a>\n"]}, {"number": 43595, "title": "post quant keras model with  UpSample2D \uff0cafter post-quantized  get a false output shape", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (or github SHA if from source):2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconvert=tf.lite.TFLiteConvert.from_keras_model(model)\r\nconvert.target_ops=[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconvert.optimizations=[tf.lite.Optimize.DEFAULT]\r\nconvert.representative\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\ngot a false tflite model,ps: the up  layer is 1x10x10x64 with an UpSample2D in the tflite i got a 1x1x1x64 output\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@sunzhe09 \r\nPlease share stand alone code to replicate issue faced along with error logs, or if possible share a colab gist with the issue reported.\r\n", "![image](https://user-images.githubusercontent.com/30410113/94382749-72dd4200-0170-11eb-9a6f-140c8302f291.png)\r\n@Saduf2019  I don't get an error,when I check my quantized tflite model,I found the UpSample2D resized make a false output;Here is my model code:\r\n![image](https://user-images.githubusercontent.com/30410113/94382848-d0718e80-0170-11eb-9d6b-1367d0bc727d.png)\r\n", "@sunzhe09 Can you please share a simple standalone code to reproduce the issue? Did you check whether the model (before conversion) upsampling as you are expecting? Thanks!", "![image](https://user-images.githubusercontent.com/30410113/94504138-ae444300-023a-11eb-815e-6ff926881cc1.png)\r\n@jvishnuvardhan I have check my training model summary,the Upsample2D is right shape;here is my  produce tflite code\uff1a\r\n    \r\n        f=open(\"./vallabels.txt\", 'r')\r\n        filenames=f.readlines()\r\n   \r\n        def representative_dataset_gen():\r\n            for line in filenames:\r\n                # Get sample input data as a numpy array in a method of your choosing.\r\n                line=line.strip()\r\n                if 'str' not in str(type(line)):\r\n                    line = line.decode()\r\n                image_path = line.split()[0]\r\n            \r\n                image=cv2.imread(image_path)\r\n                image=cv2.resize(image,(320,320))\r\n                image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\r\n                image=np.expand_dims(image,axis=-1)\r\n                image=np.expand_dims(image,axis=0).astype(np.float32)/255.\r\n                print(image_path)\r\n                yield [image]\r\n        \r\n        converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n        converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.representative_dataset = tf.lite.RepresentativeDataset(\r\n            representative_dataset_gen) \r\n        converter.inference_input_type = tf.int8\r\n        converter.inference_output_type = tf.int8\r\n        tflite_model_quant = converter.convert()\r\n        import pathlib\r\n        tflite_model_quant_file = pathlib.Path(\"./\")/\"test.tflite\"\r\n        tflite_model_quant_file.write_bytes(tflite_model_quant)", "@Saduf2019 @jvishnuvardhan  any update\uff1f", "within new model-optimizer,the bug has been fixed;", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43595\">No</a>\n"]}]