[{"number": 38422, "title": "Better documentation for the Embeddings Projector, especially graphs/edges?", "body": "Apparently it is possible to render edges between data points in the embeddings projector but I cannot find the documentation for this anywhere. It is also not clear what other things might be possible which are not obvious or documented.\r\nA column called `__next__` may have special meaning, judging from one of the examples, but it is not clear what exactly it can be used for, what the requirements on the input are or what other special meaning column names there may exist.\r\nOr am I missing some obvious documentation here?", "comments": ["@ravikyram  can i work on this issue ? Could you please point me where to start ?", "@gowthamkpr  can i work on this issue ? Could you please point me where to start ?", "Does anyone have some initial pointers to start figuring this out? Is the original source code of the JavaScript available? The code used in the tensorboard page is uglified and not very readable. ", "I'm also interested to know the answers to the questions that @johann-petrak asked originally.\r\n\r\nI did some reverse engineering and found `__next__` can be used to add links between nodes in the graph. There are some limitations though: \r\n- You can use line number (as in the embedding TSV file) to point to a node\r\n- It should be a 1-1 relationship so you cannot have duplicates in your `__next__` column\r\n- You cannot have a loop like 1 -> 2 -> 3 -> 1\r\n\r\nKeen to know if there is any other special column!"]}, {"number": 38409, "title": "Tensorflow Datasets with string inputs do not preserve data type", "body": "All **reproducible** code below is run at **Google Colab** with **TF 2.2.0-rc2**.\r\n\r\nAdapting the simple example from the [documentation][1] for creating a dataset from a simple Python list:\r\n\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    tf.__version__\r\n    # '2.2.0-rc2'\r\n    np.version.version\r\n    # '1.18.2'\r\n\r\n    dataset1 = tf.data.Dataset.from_tensor_slices([1, 2, 3]) \r\n    for element in dataset1: \r\n      print(element) \r\n      print(type(element.numpy()))\r\n\r\nwe get the result\r\n\r\n    tf.Tensor(1, shape=(), dtype=int32)\r\n    <class 'numpy.int32'>\r\n    tf.Tensor(2, shape=(), dtype=int32)\r\n    <class 'numpy.int32'>\r\n    tf.Tensor(3, shape=(), dtype=int32)\r\n    <class 'numpy.int32'>\r\n\r\nwhere all data types are `int32`, as expected.\r\n\r\nBut changing this simple example to feed a list of strings instead of integers:\r\n\r\n    dataset2 = tf.data.Dataset.from_tensor_slices(['1', '2', '3']) \r\n    for element in dataset2: \r\n      print(element) \r\n      print(type(element.numpy()))\r\n\r\ngives the result\r\n\r\n    tf.Tensor(b'1', shape=(), dtype=string)\r\n    <class 'bytes'>\r\n    tf.Tensor(b'2', shape=(), dtype=string)\r\n    <class 'bytes'>\r\n    tf.Tensor(b'3', shape=(), dtype=string)\r\n    <class 'bytes'>\r\n\r\nwhere, surprisingly, and despite the tensors themselves being of `dtype=string`, their evaluations are of type `bytes`.\r\n\r\nThis behavior is not confined to the `.from_tensor_slices` method; here is the situation with [`.list_files`][2] (the following snippet runs straightforward in a fresh Colab notebook):\r\n\r\n    disc_data = tf.data.Dataset.list_files('sample_data/*.csv') # 4 csv files\r\n    for element in disc_data: \r\n      print(element) \r\n      print(type(element.numpy()))\r\n\r\nthe result being:\r\n\r\n    tf.Tensor(b'sample_data/california_housing_test.csv', shape=(), dtype=string)\r\n    <class 'bytes'>\r\n    tf.Tensor(b'sample_data/mnist_train_small.csv', shape=(), dtype=string)\r\n    <class 'bytes'>\r\n    tf.Tensor(b'sample_data/california_housing_train.csv', shape=(), dtype=string)\r\n    <class 'bytes'>\r\n    tf.Tensor(b'sample_data/mnist_test.csv', shape=(), dtype=string)\r\n    <class 'bytes'>\r\n\r\nwhere again, the file names in the evaluated tensors are returned as `bytes`, instead of `string`, despite that the tensors themselves are of `dtype=string`.\r\n\r\nSimilar behavior is observed also with the `.from_generator` method (not shown here).\r\n\r\nA final demonstration: as shown in the `.as_numpy_iterator` method [documentation][3], the following equality condition is evaluated as `True`:\r\n\r\n    dataset3 = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]), \r\n                                                   'b': [5, 6]}) \r\n\r\n    list(dataset3.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5}, \r\n                                           {'a': (2, 4), 'b': 6}] \r\n    # True\r\n\r\nbut if we change the elements of `b` to be strings, the equality condition is now surprisingly evaluated as `False`!\r\n\r\n    dataset4 = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]), \r\n                                                   'b': ['5', '6']})   # change elements of b to strings\r\n    \r\n    list(dataset4.as_numpy_iterator()) == [{'a': (1, 3), 'b': '5'},   # here\r\n                                           {'a': (2, 4), 'b': '6'}]   # also\r\n    # False\r\n\r\nprobably due to the different data types, since the values themselves are evidently identical.\r\n\r\n---\r\n\r\nI didn't stumble upon this behavior by academic experimentation; I am trying to pass my data to TF Datasets using custom functions that read pairs of files from the disk of the form\r\n\r\n    f = ['filename1', 'filename2']\r\n\r\nwhich custom functions work perfectly well on their own, but mapped through TF Datasets give\r\n\r\n    RuntimeError: not a string\r\n\r\nwhich, after this digging, seems at least not unexplained, if the returned data types are indeed `bytes` and not `string`.\r\n\r\nSo, is this a bug (as it seems), or am I missing something here?\r\n\r\n\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/data/Dataset\r\n  [2]: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files\r\n  [3]: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#as_numpy_iterator", "comments": ["I was able to reproduce the issue with Tf2.2.rc2.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3168075f641b7f19800665001d58c01f/untitled499.ipynb). Thanks!", "This happens due to `numpy()` operations on string inputs. I made a toy example to show\r\n```python\r\nimport tensorflow as tf\r\nimport numpy\r\nprint(tf.__version__)\r\n\r\nstring = tf.constant('My string!')\r\nprint(string)\r\n\r\nnum_string = s.numpy()\r\nprint(type(num_string))\r\n```\r\noutput:\r\n```python\r\n2.2.0-rc2\r\ntf.Tensor(b'My string!', shape=(), dtype=string)\r\n<class 'bytes'>\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ymodak Is there something more expected from my side?", "I have the same issue. Tensorflow when dealing with texts generates string tensors that are stored as byte string:\r\n\r\n```python\r\n<tf.Tensor: shape=(2,), dtype=string, numpy=array(\r\n     [b'Th\u00ea first utf-8 string of the bat\u00e7h.',\r\n      b'Th\u00ea sec\u00f4nd utf-8 string of the bat\u00e7h.'], dtype=object)>\r\n```\r\n\r\nHowever, I didn't find an efficient way to decode this kind of tensor as a list of strings. It's even worse if the byte string containing a non-ascii character as point in the tensor above.\r\n\r\nWhat I really need is one of these two options:\r\n\r\n1. a tokenizer which is able to accept aforementioned byte string tensor as input to tokenize; or\r\n2. a vectorized approach to transforming a byte string tensor into a string list.\r\n", "As per @ymodak's response, this is not a specific to tf.data functionality but behavior of `numpy()` method:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.constant(\"hello\")\r\nprint(x)\r\nprint(type(x.numpy()))\r\n```\r\n\r\nPlease assign to someone on the TF core team for triage.", "I would like to help with this. I can pick up and triage the issue. \r\n(If there is someone who can help me if I have doubts, that'll be awesome)", "This is connected to the fact that the function unfortunately changes everything to `int32` (at least when it comes to int-based numpy arrays). See #49503.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210525, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/8b18fc13e3b76c8387fa9cfa7590b677/38409.ipynb). Thanks!", "Is there any progress on this problem? I have same problem with tf 2.4.1.", "Was able to reproduce your issue in Tf v2.7 and Nightly 2.9.0-dev20211223, please find the gist [here](https://colab.research.google.com/gist/tilakrayal/288353aace5456f285877bf18f175061/untitled158.ipynb). Thanks!"]}, {"number": 38390, "title": "tflm vc compilation error and how to fix", "body": "in file tensorflow\\lite\\micro\\kernels\\svdf.cc\r\nthere is __restrict__ keword, so it can't be compiled with VC.\r\nI added \r\n\r\n```\r\n#ifdef _MSC_VER\r\n\t#define RESTRICT __restrict\r\n#else\r\n\t#define RESTRICT __restrict__\r\n#endif\r\n```\r\n\r\nand replaced __restrict__  with RESTRICT  - now compiles without problems", "comments": ["@yadonskov \r\ncould you please let us know the tensorflor flow version for which you are facing this issue", "@Saduf2019 \r\nI used commit 920fefa2eeffddd491845d798836a6800668ded9.\r\nI found this when I was compiling my TFLM app with cl.exe compiler.\r\nIt's not tensorflow build problem - it's problem of TFLM usage.", "@yadonskov \r\ncan you please check [this link](https://github.com/tensorflow/tensorflow/pull/31847) and let us know if it would help", "@Saduf2019 \r\nit doesn't look like needed code. I just showed you compilation error with cl.exe. and a simple way to fix it. If you have better way to do it - just do it."]}, {"number": 38356, "title": "Add MPI cluster resolver", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThere should be a cluster resolver that works inside any MPI jobs by querying properties of MPI_COM_WORLD.\r\n\r\nThis allows running TF easily inside any HPC cluster environment independent of the Batch system\r\n\r\n**Will this change the current api? How?**\r\n\r\nNew class added.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nHPC users.\r\n\r\n**Any Other info.**\r\n\r\nExisting PR with implementation: https://github.com/tensorflow/tensorflow/pull/38112\r\n\r\nCCing  @jhseu @frankchn ", "comments": []}, {"number": 38352, "title": "The docker image provided to compile tensorflow custom ops is too big.", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: None\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): TF 2.1.0 from pypi\r\n- Python version: - Bazel \r\nversion (if compiling from source): None\r\n- GCC/Compiler version (if compiling from\r\nsource): None \r\n- CUDA/cuDNN version: - GPU model and memory: CUDA 10.1 CUDNN 7.6, no gpu, just compiling\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nFollow-up on the SIG-build meeting of 07/04/2020.\r\n\r\nIn TF Addons, we use the docker image provided by tensorflow to build our \"manylinux\" wheels:\r\n\r\n```\r\ndocker pull tensorflow/tensorflow:2.1.0-custom-op-gpu-ubuntu16\r\n```\r\n\r\nUncompressed, this image is around 9.1GB. This poses several issues:\r\n* Our disk quota of github-actions can't handle more than 14GB, we reached this limit multiple times and when we do, our CI build fails. We had to remove files from the docker image and squash it to make our CI work again.\r\n* Downloading and decompressing the docker image takes a significant amount of time in the CI 2-3m. As a reference point, running the full build takes 12m.\r\n* It makes it hard for developers to use the image locally because they need to download 4-5 GB of data (the docker image compressed is 4.24GB).\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nWe should have multiple images depending on what python versions we want to use to compile. Those docker images should be around 3-4GB uncompressed.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ndocker pull tensorflow/tensorflow:2.1.0-custom-op-gpu-ubuntu16  && docker image ls\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI digged into the docker image, see what was taking space, and I believe that I managed to find the biggest directories:\r\n\r\n```\r\nTotal: 9.1G\r\n\r\n1.7G    /usr/local/lib/python2.7\r\n417M    /usr/local/lib/python3.6\r\n157M    /usr/local/lib/python3.7\r\n2.0G    /usr/local/cuda-10.1\r\n2.0G    /usr/lib/x86_64-linux-gnu\r\n591M    /root/.cache/pip\r\n529M    /dt8  (devtoolset-8)\r\n469M    /dt7  (devtoolset-9)\r\n\r\nThose directories together take 7.8G, so 85% of the total size.\r\n```\r\n\r\nPossible actions:\r\n\r\n-> Do not use the pip cache\r\n-> Is there a need for the two devtoolset? Can one be enough?\r\n-> Many pip installs are done in the Dockerfile for py2.7 (tf and other). \r\n   Those do not seem necessary as in TF Addons, we can build \"manylinux\" wheels with the py3.7\r\n   which has nothing installed on it with pip (bare install).\r\n-> Make a docker image for each python version, should save ~200MB per python version. e.g. \r\n`docker pull tensorflow/tensorflow:2.1.0-py3.6-custom-op-gpu-ubuntu16`\r\n\r\nI would be nice to take care of this before the custom op docker image for tf 2.2.0 is released.\r\n\r\nI was asked during the sig-build meeting to tag @yifeif .\r\n\r\nTagging @seanpmorgan  because he manages TF Addons.", "comments": ["Thanks for the analysis @gabrieldemarmiesse! \r\nI'll look into if we can remove one of the devtoolset, and you are definitely right, we can remove python 2.7 related directory.", "I think that generally the custom-ops `Dockerfile` receipts could not diverge so much from devel receipts. \r\nSeems that instead the receipt start `FROM` a scratch base image.\r\nIt is strange that devel and custom-ops don't have a common baseline.", "/cc @angerson in the case the riunification with Devel images could be get in charge by the SIG-Build"]}, {"number": 38303, "title": "How to limit GPU memory usage in java?", "body": "Hello everyone,\r\n    In python, I can use bellow strategy to limit GPU memory usage.\r\n```\r\n tf.config.experimental.set_memory_growth(gpu, True)\r\n tf.config.experimental.per_process_gpu_memory_fraction = 0.5\r\n```\r\nBut how can I do this in java?\r\nI training my model with python, then convert model to pb format, last I load pb model in java.\r\nso I want limit GPU memory usage in java . I can't find any document about this.\r\nAny suggestions will help me, thanks.", "comments": ["@sanjoy  please help me ", "@payne4handsome sorry for the delay, was on leave.  I'm not familiar with the TF Java bindings but have you looked [here](https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/EagerSession.Options#config(byte[]))?", "Did you ever find an answer to this?", "You'll have more luck making Java issues on [tensorflow/java](https://github.com/tensorflow/java) directly, assuming you're using the new bindings.  If you are, what you're looking for are the [`Session`](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/Session#Session(org.tensorflow.Graph,%20org.tensorflow.proto.framework.ConfigProto)) and [`EagerSession`](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/EagerSession#options()) factories that let you pass in a `ConfigProto` object."]}, {"number": 38280, "title": "TF2.2.0rc2: exception if shape of input to tf.linag.set_diag not fully specified ", "body": "Hello, \r\n\r\nit's now possible to use tf.linalg.set_diag to set not only the diagonal of a matrix, but also bands (i.e a diagonal + sub- and superdiagonals) using the parameter \"k\". \r\n\r\nI would like to use this within a keras layer, yet when the model containing that layer is built, an exception (see below) is raised as long as the shape of the input parameter of tf.linalg.set_diag is not fully specified (i.e. the batch size should be dynamic, depending on the input at \"runtime\"). If I prespecify the axis size, everything seems to work fine\r\n \r\n**Standalone code to reproduce the issue** \r\n```python \r\nimport tensorflow as tf\r\nimport tensorflow.keras as k\r\n\r\ndef construct_band_mat(band):\r\n    # Dynamic batch size does not work\r\n    batch_size = tf.shape(band)[0]\r\n\r\n    # hard coded batch size works\r\n    #batch_size = 3\r\n\r\n    base = tf.zeros([batch_size, 5, 5])\r\n    return tf.linalg.set_diag(base, band, k=(-1, 1))\r\n\r\ninput = k.Input(shape=[3, 5])\r\noutput = k.layers.Lambda(construct_band_mat)(input)\r\n\r\nmodel = k.models.Model(inputs=[input], outputs=[output])\r\n\r\nband_in = tf.ones([3, 3, 5], dtype=tf.float32)\r\nprint(model(band_in))\r\n```\r\n\r\n**System information** \r\n- Have I written custom code:  yes \r\n- OS Platform and Distribution: Ubuntu 18.04 \r\n- TensorFlow version: 2.2.0rc2\r\n- TensorFlow installed from: pip\r\n- Python version:  3.6.9 \r\n- CUDA/cuDNN version: None, CPU only \r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nIf the shape of the \"input\" is fully specified when the model is build, the function acts as expected.\r\nIf the size of one axis (e.g. the batch axis) of \"input\" is unspecified during build, an exception is raised :\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 3 and 4 for '{{node lambda/set_diag}} = MatrixSetDiagV3[T=DT_FLOAT, align=\"RIGHT_LEFT\"](lambda/zeros, input_1, lambda/set_diag/k)' with input shapes: [?,5,5], [?,3,5], [2].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/philipp/projects/RKN_internal/banded_test.py\", line 16, in <module>\r\n    output = k.layers.Lambda(construct_band_mat)(input)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 922, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\", line 888, in call\r\n    result = self.function(inputs, **kwargs)\r\n  File \"/home/philipp/projects/RKN_internal/banded_test.py\", line 13, in construct_band_mat\r\n    return tf.linalg.set_diag(base, band, k=(-1, 1))\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2654, in matrix_set_diag\r\n    input=input, diagonal=diagonal, k=k, align=align, name=name)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5864, in matrix_set_diag_v3\r\n    name=name)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 744, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 595, in _create_op_internal\r\n    compute_device)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3327, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1817, in __init__\r\n    control_input_ops, op_def)\r\n  File \"/home/philipp/projects/rkn_venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1657, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Shapes must be equal rank, but are 3 and 4 for '{{node lambda/set_diag}} = MatrixSetDiagV3[T=DT_FLOAT, align=\"RIGHT_LEFT\"](lambda/zeros, input_1, lambda/set_diag/k)' with input shapes: [?,5,5], [?,3,5], [2].\r\n\r\nProcess finished with exit code 1\r\n\r\nNote that it says \"Shapes must be equal rank, but are 3 and 4 \" but then \" input shapes: [?,5,5], [?,3,5], [2]\" i.e. two shapes of rank 3.\r\n\r\nAlso note that the first dimension of the diagonal argument is also unspecified, which seems to work fine.\r\n\r\n**Describe the expected behavior**\r\ntf.linalg.set_diag should be able to handle inputs whose size is not fully specified while the graph is build up. \r\n(If there is some fundamental reason why this is not possible, which I am currently not seeing, the documentation should be adapted and the exception should be more expressive. ) \r\n\r\n\r\nBest wishes,\r\nPhilipp", "comments": ["I have tried on colab with TF version 2.2.0-rc2, 2.2.0-dev20200406 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0b93db5b2ca687a79f4e32c3c50493b2/untitled17.ipynb). Thanks!\r\n", "I have tried on colab with TF version 2.5 and was able to reproduce the issue.Please, find the [gist here](https://colab.research.google.com/gist/Saduf2019/76ca3e3ada81b1fcb79675a745d23f37/untitled589.ipynb). Thanks!\r\n\r\n", "Was able to reproduce the issue in tf v2.7.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/97873d4db143ac0cc3fe2d58a0f7d787/untitled159.ipynb)."]}, {"number": 38263, "title": "Add a Mutable Hash Table of Ragged Tensors ", "body": "**System information**\r\n- TensorFlow version 2.1:\r\n- Are you willing to contribute it Yes:\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf-nightly` has `MutableHashTableOfTensors` for fast lookups of key/tensor pairs. This feature request is to add similar functionality for `RaggedTensor`s, either by adding `RaggedTensor` support to `MutableHashTableOfTensors` or creating new ops for e.g. a `MutableHashTableOfRaggedTensors`. \r\n\r\n**Will this change the current api? How?** I don't think so. If extra `RaggedTensor` functionality is added `MutableHashTableOfTensors` I think the current `MutableHashTableOfTensors` API won't need to change. \r\n\r\n**Who will benefit with this feature?** Users who need to perform fast lookups on irregular data will benefit from this feature.\r\n\r\nPersonally I'd like to store a graph adjacency list as a `MutableHashTableOfRaggedTensors`, and then create a `tf.data.Dataset` pipeline to pre-process the graph data for machine learning. I'll implement this in [StellarGraph](https://github.com/stellargraph/stellargraph) to make our graph backend natively tensorflow! ", "comments": []}, {"number": 38245, "title": "register keras serializables is module_objects", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI've been using keras serialization and while it mostly works well, I have two issues:\r\n* there seems to be duplicated functionality in `get` and `deserialize` across `tf.keras` submodules, and inconsistent support (e.g. no `tf.keras.layers.get`).\r\n* use with custom implementations via  `tf.keras.utils.register_keras_serializable` gives odd and potentially surprising results.\r\n\r\nThese are illustrated in the below examples:\r\n```python\r\nimport tensorflow as tf\r\n\r\nregister = tf.keras.utils.register_keras_serializable('my_package.layers')\r\n\r\n@register\r\nclass MyLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\nlayer = MyLayer()\r\nconfig = tf.keras.utils.serialize_keras_object(layer)\r\n\r\nprint(tf.keras.layers.deserialize(config))      # works - but why no layers.get?\r\nprint(tf.keras.optimizers.get(config))          # works - but should it?\r\n```\r\n\r\nSomething like the following could reduce code duplication across different modules and make them extensible and more resilient to errors.\r\n\r\n```python\r\nclass Serializer(object):\r\n    def __init__(self, accepted_types=None, module_name=None):\r\n        self._module_objects = {}\r\n        self._module_name = module_name\r\n        self._accepted_types = accepted_types\r\n    \r\n    def register(self, fn_or_class: Callable):\r\n        assert callable(fn_or_class)\r\n        name = fn_or_class.__name__\r\n        if name in self._module_objects:\r\n            raise ValueError(\r\n                f'Could register {name} - already in {self._module_name} '\r\n                'module_objects')\r\n        self._module_objects[name] = fn_or_class\r\n        return fn_or_class\r\n    \r\n    def deserialize(self, config, custom_objects=None):\r\n        return tf.keras.utils.deserialize_keras_object(\r\n            config, module_objects=self._module_objects,\r\n            custom_objects=custom_objects,\r\n            printable_module_name=self._module_name)\r\n    \r\n    def _get_fallback(self, identifier):\r\n        raise ValueError(\r\n            f'Could not interpret optimizer identifier: {identifier}')\r\n    \r\n    def get(self, identifier):\r\n        if isinstance(identifier, self._accepted_types):\r\n            return identifier\r\n        elif isinstance(identifier, str):\r\n            identifier = dict(class_name=identifier, config={})\r\n        if isinstance(identifier, dict):\r\n            return self.deserialize(identifier)\r\n        else:\r\n            return self._get_fallback(identifier)\r\n\r\n# in layers.py\r\n_serializer = Serializer(Layer, 'layers')\r\nget = _serializer.get\r\ndeserialize = _serializer.deserialize\r\nregister = _serializer.register\r\n\r\n# in optimizers.py\r\nclass OptimizerSerializer(Serializer):\r\n\r\n    def __init__(self):\r\n        self.__init__(Optimizer, 'optimizers')\r\n\r\n    def _get_fallback(self, identifier):\r\n        if isinstance(identifier, tf_optimizer_module.Optimizer):\r\n            opt = TFOptimizer(identifier)\r\n            K.track_tf_optimizer(opt)\r\n            return opt\r\n        else:\r\n            return super()._get_fallback(identifier)\r\n\r\n_serializer = OptimizerSerializer()\r\nget = _serializer.get\r\ndeserialize = _serializer.deserialize\r\nregister = _serializer.register\r\n```\r\n\r\n**Will this change the current api? How?**\r\n* Add `register` function to each of `tf.keras.[layers,optimizers,...]`\r\n* Add `tf.keras.layers.get`\r\n\r\n**Who will benefit with this feature?**\r\nPeople who want to use keras serialization/deserialization with custom objects and retain error checking\r\n\r\n**Any Other info.**\r\nMore than happy to put the PR together myself if the idea is likely to be accepted. Also happy to put in `package` support similar to `tf.keras.utils.register_keras_serializable`. ", "comments": []}, {"number": 38228, "title": "Named Dimensions", "body": "**System information**\r\n\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, TensorFlow ops receive axes as integer indices, e.g. `tf.reduce_sum(tensor, axis=(2, 3))`. Keep track of axes after applying several operations on them and passing tensors around through multiple functions can be challenging.\r\n\r\nI've seen many researchers and developers grab a sheet of paper and manually work through how axes should change. We can avoid this slow and often frustrating process by supporting optional axes names for tensors and having the most common ops annotate their outputs:\r\n\r\n```python\r\nimage = tf.name_axes(image, ('B', 'H', 'W', 'C'))  # assert len(image.shape) == 4\r\nprint(image)  # <tf.Tensor dtype=uint8 shape={B: 500, H: 64, W: 64, C: 3}>\r\n\r\ngrid = tf.repeat(tf.repeat(image, 4, axis='H'), 4, axis='W')\r\nprint(grid)  # <tf.Tensor dtype=uint8 shape={B: 500, H: 256, W: 256, C: 3}>\r\n\r\ngray = tf.reduce_mean(grid, axis='C')\r\nprint(gray)  # <tf.Tensor dtype=uint8 shape={B: 500, H: 256, W: 256}>\r\n\r\nflipped = tf.transpose(gray[0], ('W', 'H'))\r\nprint(flipped)  # <tf.Tensor dtype=uint8 shape={W: 256, H: 256}>\r\n```\r\n\r\nA function starting with the line `tf.name_axes(...)` for its input tensors will immediately be easier to read. Moreover, developers could place `assert tensor.shape.names == ('B', 'F')` statements throughout their code so it becomes immediately clear from the written code how shapes are changing.\r\n\r\nAn advanced version of named axes would be to keep track of combined axes:\r\n\r\n<details>\r\n\r\n```python\r\nvideo = tf.name_axes(image, ('B', 'T', 'H', 'W', 'C'))\r\nprint(video)  # <tf.Tensor dtype=uint8 shape={B: 500, T: 100, H: 64, W: 64, C: 3}>\r\n\r\nframes = tf.reshape(frames, ('B*T', 'H', 'W', C'))\r\nprint(frames)  # <tf.Tensor dtype=uint8 shape={B*T: 50000, H: 64, W: 64, C: 3}>\r\n\r\nlogits = model(frames)\r\nprint(logits)  # <tf.Tensor dtype=float32 shape={B*T: 50000, 10}>\r\n\r\nlogits = tf.reshape(logits, ('B', 'T', 10))\r\nprint(logits)  # <tf.Tensor dtype=float32 shape={B: 500, T: 100, 10}>\r\n\r\nlogits = tf.name_axes(logits, ('B, 'T', 'K'))\r\nprint(logits)  # <tf.Tensor dtype=float32 shape={B: 500, T: 100, K: 10}>\r\n```\r\n\r\n</details>\r\n\r\n**Will this change the current api? How?**\r\n\r\nThis would extend the current API in a fully backward compatible way. Common low-level TensorFlow operations (e.g. reduce_sum, concat, slice, tile) have to check for input tensors with named axes to annotate their outputs correctly.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nNamed axes have the potential to increase productivity of both researchers and engineers. Especially when developing large TensorFlow applications, named axes can help keep the overview. For product development with multiple developers, it can increase readability.\r\n\r\n**Any Other info.**\r\n\r\nJAX supports axes names. PyTorch 1.3 added experimental support for [named dimensions](https://pytorch.org/docs/stable/named_tensor.html).", "comments": ["I would like to contribute on this feature, if any mentor could guide me.", "What would be the semantic of `x + y` (for example) ?\r\n\r\n```python\r\na = tf.name_axes(a, ('x', 'y'))\r\nb = tf.name_axes(b, ('y', 'x'))\r\na + b\r\n```\r\n\r\nAFAIU, in PyTorch, this currently yields a check that `x.names[i]` is consistent with `y.names[i]` for all `i`: it ensure that you don't mix up dimensions but you still have to keep track of them:\r\n\r\nI'd tend to think that automatically reordering dimensions would be more helpful (but this would probably preclude mixing named and unamed tensors and having partially names tensors).\r\n\r\nA workaround would be to define helper functions such as:\r\n\r\n```python\r\ndef plus_by_name(x, y):\r\n    return x + y.align_as(x)\r\n```\r\n", "[Here](http://nlp.seas.harvard.edu/NamedTensor)'s a relevant article with some ideas on how some higher-level interfaces could be improved using named dimensions. It's written using Torch as an example, but but many of the points apply to any tensor library. Really looking forward to this feature being implemented!", "Many good ideas in the article you linked!", "Relevant library that's compatible with TF and other frameworks: https://github.com/arogozhnikov/einops"]}, {"number": 38206, "title": "pixelwise-loss weight map part 2", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis a continuation of \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/38060\r\n\r\nwhich I think might have been prematurely closed.  It was said that tensorflow 2.x allows for pixelwise-loss map.  \r\n\r\nHowever, I looked at  https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit and pasted the relevant parts.  These options, I don't think, will be adequate for the weights needed for pixelwise-loss-weights.  For 2D images, weights will need to be shape [None, x_dim, y_dim, 1 or 3 or 4].  You see, the weights also need to be passed through  tf.keras.preprocessing.image. ImageDataGenerator along with X_train, Y_train.  So X_datagen, W_datagen (for weights), and Y_datagen all need all to be passed into Model.fit.  I don't see how to do that.\r\n\r\n   - *class_weight*: Optional dictionary mapping class indices (integers)\r\n   to a weight (float) value, used for weighting the loss function (during\r\n   training only). This can be useful to tell the model to \"pay more\r\n   attention\" to samples from an under-represented class.\r\n   - *sample_weight*: Optional Numpy array of weights for the training\r\n   samples, used for weighting the loss function (during training only). You\r\n   can either pass a flat (1D) Numpy array with the same length as the input\r\n   samples (1:1 mapping between weights and samples), or in the case of\r\n   temporal data, you can pass a 2D array with shape (samples,\r\n   sequence_length), to apply a different weight to every timestep of every\r\n   sample. In this case you should make sure to specify\r\n   sample_weight_mode=\"temporal\" in compile(). This argument is not\r\n   supported when x is a dataset, generator, or keras.utils.Sequence\r\n   <https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence> instance,\r\n   instead provide the sample_weights as the third element of x.\r\n\r\n**Will this change the current api? How?**\r\nIt will add more versatility to the api by incorporating an important aspect of computer vision.\r\n\r\n**Who will benefit with this feature?**\r\nThose using keras to implement computer vision programs.\r\n\r\n**Any Other info.**\r\nThank you kindly for your consideration.\r\n", "comments": ["I am thrilled this request got to Fran\u00e7ois Chollet.  I'm just wondering what the team's current thinking is with regard to incorporating pixelwise-loss weight maps.  It seems to me a veritable \"must-have\" for training deep learning algorithms for instance segmentation of objects that are often close together.  ", "Hello, @shinlin77 I have been following your questions about pixel-wise weight maps to be used in training and I too hope to follow Ronneberger, Fischer, and Brox's approach used in UNET.\r\n\r\nI saw the workaround you linked: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/keras-users/ue1S8uAPDKU/PzL7Vb0bBQAJ (edited)\r\n\r\nAnd I wanted to check in since then to hear if anyone had another solution to this challenge in the interim?", "I don't know what that link you provided points to.  In any case, I eventually found the solution in a book.  You have to write some code that is a bit lower level than the highest level tensorflow API commands to get it to work.  The code below should help.  Sorry about the formatting of the text.\r\n\r\n\r\n\r\nmodel = UNET()\r\nmodel.load_weights('/content/drive/My Drive/python/segmentation/UNET.h5')\r\n\r\n\r\n\r\n\r\nloss = tf.keras.losses.BinaryCrossentropy(from_logits = False)\r\nstep = tf.Variable(1, name=\"global_step\")\r\noptimizer = tf.optimizers.Adam(1e-3)\r\n\r\nckpt = tf.train.Checkpoint(step=step, optimizer=optimizer, model=model)\r\nmanager = tf.train.CheckpointManager(ckpt, '/content/drive/My Drive/python/segmentation/tf_unet1', max_to_keep=3)\r\nckpt.restore(manager.latest_checkpoint)\r\nif manager.latest_checkpoint:\r\n    print(f\"Restored from {manager.latest_checkpoint}\")\r\nelse:\r\n    print(\"Initializing from scratch.\")\r\n\r\naccuracy = tf.metrics.Accuracy()\r\nmean_loss = tf.metrics.Mean(name='loss')\r\n\r\n@tf.function\r\ndef train_step(inputs, labels, wts):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(inputs, training = True)\r\n        loss_value = loss(labels, logits, sample_weight = wts)\r\n\r\n    gradients = tape.gradient(loss_value, model.trainable_variables)\r\n    gradients = [(tf.clip_by_value(grad, -1, 1)) for grad in gradients]\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    step.assign_add(1)\r\n\r\n    accuracy.update_state(labels, tf.cast(logits > 0.5, dtype = tf.bool))#, sample_weights = wts) #tf.argmax(logits, -1))\r\n    return loss_value, accuracy.result()\r\n\r\nepochs = 5\r\nbatch_size = 20\r\nnr_batches_train = int(train_x.shape[0] / batch_size)\r\nprint(f\"Batch size: {batch_size}\")\r\nprint(f\"Number of batches per epoch: {nr_batches_train}\")\r\n\r\ntrain_summary_writer = tf.summary.create_file_writer('/content/drive/My Drive/python/segmentation/log/train')\r\n\r\nwith train_summary_writer.as_default():\r\n    for epoch in range(epochs):\r\n        for t in range(nr_batches_train):\r\n            start_from = t * batch_size\r\n            to = (t + 1) * batch_size\r\n\r\n            features, labels , wts= train_x[start_from:to], train_y[start_from:to], train_w[start_from:to]\r\n            #print(tf.shape(features))\r\n            loss_value, accuracy_value = train_step(features, labels, wts)\r\n            mean_loss.update_state(loss_value)\r\n\r\n            if t % 1000 == 0:\r\n                print(\r\n                    f\"{step.numpy()}: {loss_value} - accuracy: {accuracy_value}\"\r\n                )\r\n                save_path = manager.save()\r\n                print(f\"Checkpoint saved: {save_path}\")\r\n                tf.summary.image(\r\n                    'train_set', features, max_outputs=3, step=step.numpy())\r\n                tf.summary.scalar(\r\n                    'accuracy', accuracy_value, step=step.numpy())\r\n                tf.summary.scalar(\r\n                    'loss', mean_loss.result(), step=step.numpy())\r\n                accuracy.reset_states()\r\n                mean_loss.reset_states()\r\n        print(f\"Epoch {epoch} terminated\")\r\n        for t in range(nr_batches_train):\r\n            start_from = t * batch_size\r\n            to = (t + 1) * batch_size\r\n            features, labels, wts = train_x[start_from:to], train_y[start_from:to], train_w[start_from:to]\r\n            logits = model(features)\r\n            accuracy.update_state(labels, tf.cast(logits > 0.5, dtype = tf.bool))#, sample_weights = wts)#tf.argmax(logits, -1))\r\n        print(f\"Training accuracy: {accuracy.result()}\")\r\n        accuracy.reset_states()", "Thanks @shinlin77 for sharing. It is good to know that this is the approach that you landed on, I will be referring to the sample code.\r\n\r\nLink corrected, thanks for that note."]}, {"number": 38183, "title": "[PROPOSAL] Go back to cmake and drop support to bazel -- a real memory and time abusing toolkit for building medium size projects", "body": "## Motivation\r\n\r\nIf we build tensorflow using camke, I guess only 1~2 hours is enough for the first trial considering the current project size. However when we switch to bazel huge memory and time will be consumed.\r\n\r\nBazel by defaults will creates huge number of jobs (you can control by --jobs) and each job consumes nearly 10 GB memory. Besize, a lot of non-binary dependencies downloaded using \"git\", this breaks  philosophy of UNix design.  \r\n\r\nActually using \"git\" while building a large project is a not very good practice. Cmake by defaults does not support SSL and you have build it manually to support curl with ssl . Moreover,  instead of to clone a whole project, we only need to download released versions of that. Using `git clone` inside a complex installer will creates more problems that you can predict: proxy server, memory cache, fire wall, ...\r\n\r\nConsidering the fact that all-in-one installer should be friendly to a new machine, the bazel has been proved not good at that.\r\n\r\nI suggest to use traditional ways to maintain binary dependencies by tracing different binary managers for different OS distribution.\r\n\r\n## C++ dependencies\r\n\r\nA product manager or project manager can easily figure out that many projects integrated a compiled tensorflow shared library with cmake projects like HDMap localization module. There are little people who really need bazel because it breaks philosophy of Unix design and solve a problem which does not really exist.\r\n\r\n## Compiling details\r\n\r\n1) config\r\nBy default we change jvm memory flags using \"--host_jvm_args\" upon bazel server boot up and refine jobs to a specific number with flags \"--jobs\" following `bazel build`\r\n\r\nsee discussion https://stackoverflow.com/questions/34382360/decrease-bazel-memory-usage\r\n\r\n2) user specific installation may creates problems when `sudo` needed (checking cuda installation for example).\r\nSince default installation of  bazel is user specific , a script executed by root user will no longer be able to detect ${HOME}/bin/bazel automatically. But meanwhile, searching and modifying thirdparty libraries (cuda, cudnn for examples) requires root privileges. The creates a dilemma.\r\n\r\n3) The peak memory building tensorflow used by bazel as I observed  with htop:\r\n\r\nMax: 13 G\r\nAvg: 5 ~ 7 G\r\n\r\nwhile bazel boot use more than 10GB and using Malloc utilities, we observed that some of memory became free but was not retrieved by the system. \r\n\r\n", "comments": ["Hi, thanks for raising this issue.\r\nUnfortunately, internally google uses the BUILD files. Google's code base will not migrate off of these in any case for the foreseeable future. Therefore, TF has to support them.\r\nWhen we tried to support cmake and Makefile it created a lot of burden to edit multiple build systems for developers. So we decided to drop first party support for these.\r\n\r\nThat said, we would be happy if our community decided to maintain cmake files for TF. WE cannot have them in the main repository, but we would be able to point users who would like to use cmake towards them. We did try to have our community take over our cmake files, but those discussions did not pan our at the time.\r\n", "@gunan I'm going to have a look at creating a project to build the C++ API only using CMake and Conan. I'm not sure how sustainable it will be in the long run but I'm not convinced of Bazel's utility in the C++ sphere, and I think there are enough pain points from a C++ developers perspective (particularly on Windows) to make it worth the effort.\r\n\r\nI'll share with the community if I can get something workable.", "@planetmarshall Any progress on this? :)"]}, {"number": 38168, "title": "tf.gather Converting sparse IndexedSlices warning", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from: binary (Anaconda)\r\n- TensorFlow version: 2.1\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: GeForce 1080 Ti\r\n\r\n**Describe the current behavior**\r\nWhenever `tf.gather` (or `tf.boolean_mask`) is called with `params` (`tensor`) being the output of another function, this throws the warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.` This behavior seems to be [well known](https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation) but largely ignored/accepted. Calling `tf.gather` on the output of another function is perfectly reasonable and should in my opinion *not* throw a warning.\r\n\r\n**Describe the expected behavior**\r\nThe forward pass should infer the shape of the dense tensor required in the backward pass, if possible (issue with dynamic shapes?). No warning should be thrown or the user should be able to acknowledge/deactivate the warning (ideally a warning that is more specific than the current one).\r\n\r\n**Standalone code to reproduce the issue** \r\nSee e.g. https://github.com/klicperajo/dimenet/blob/master/train.ipynb\r\n\r\n**Other info / logs**\r\nhttps://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation\r\nRelated: https://github.com/tensorflow/tensorflow/issues/23566", "comments": ["@klicperajo,\r\nI tried to reproduce the issue, but I'm facing an error stating `KeyError: 'ceil'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/976ba78b3487a7f3e27dbe4f9876625d/38168.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code sample to reproduce the issue reported here. Thanks!", "Thank you for looking into this!\r\n\r\nThat's because the runtime uses a completely outdated version of sympy. Just add a `!pip install --upgrade sympy` at the beginning and you're good to go.\r\n\r\nThe notebook also throws a deprecation warning from within tensorflow (`calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated`), so that should be worthwile to investigate as well.", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/e05ae0768d95f2f9d34292f11ce3ec9d/38168.ipynb), however running the code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/37e1b2941bd4cd347a5677d87ebf8a84/38168-tf-nightly.ipynb) prints only a single warning. Please find the attached gist. Thanks!", "Great! Throwing a single warning is certainly better than throwing 12. However, it does still throw a warning, so my criticism and thoughts from above still applies.", "@klicperajo,\r\nCan you please try on the latest tf- version and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "As described earlier there is just 1 warning now. However, yes, in `tf-nightly-2.4.0.dev20201005` I still see the warning\r\n```\r\n.../lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/dimenet/interaction/Reshape_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/dimenet/interaction/Reshape_1:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/dimenet/interaction/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"shape. This may consume a large amount of memory.\" % value)\r\n```\r\n\r\nand in TensorFlow 2.3.1 it is\r\n```\r\n.../lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n```", "I am also getting this warning, but in a somewhat different context. I am training a language model in which the input embedding weights matrix and the final, pre-softmax weights matrix are shared (as described [here](https://arxiv.org/abs/1608.05859) and as used in the famous _Attention is All You Need_ paper). Invariably, whether I use the Keras Embedding layer, or if I call tf.gather() or tf.nn.embedding_lookup() directly, I get this warning. As soon as I allow the pre-softmax weights matrix to be distinct from the input embedding matrix, the error goes away. Presumably this has something to do with TF having to sum up the gradients with respect to the embedding matrix from the pre-softmax matrix multiply and the tf.gather() operation\u2014 but I'm still relatively new to TensorFlow and ML more generally, so I may be misunderstanding something.", "Was able to replicate the issue with TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/c6056e7ae6a66f4d93d91897a5b44c29/untitled261.ipynb) ..Thanks !"]}, {"number": 38099, "title": "Use only tensorflow-lite-gpu aar file", "body": "\r\nI am trying to build an image recognition app and have a ML model ready which I have converted to .tflite type for using. I have used tflite and tflite-gpu aar files in my project for performing inference with the model and deployed it on my devices and it works fine. But I am concerned about the apk size after including these two aar. Is it possible to use just tflite-gpu aar file for my app? Or any other work around ?", "comments": ["@t-niroc \r\ncould you please share the tensorflow version on which this error is faced.\r\n\r\nplease refer to this [comment](\r\nhttps://github.com/tensorflow/tensorflow/issues/25278#issuecomment-459609002), and let us know if its helps.", "@Saduf2019 I am on the latest master branch. \r\n> \r\n> please refer to this [comment](https://github.com/tensorflow/tensorflow/issues/25278#issuecomment-459609002), and let us know if its helps.\r\n\r\nThis comment specifies to use only necessary ops to decrease size.  My question is a little different from that.\r\n\r\nIf I use all the ops and also want to use GPU acceleration, is it possible to use just the tensorflowlite-gpu.aar and avoid tensorflowlite.aar as an alternate way of reducing size?\r\n\r\n", "You probably cannot only use the GPU AAR as there is a fallback-to-CPU mechanism.  However, there are a couple of places where you can trim for binary size:\r\n\r\n* Instead of using the builtin op resolver, you can create a custom op resolver with only the used ops and used that when initializing the Interpreter object.  I learned that the bare bone TFLite is 3-500kB, but when using the bulitin op resolver, binary size will increase to 1.5MB.\r\n* You can employ a similar trick to GPU as well.  You can trim bunch of code in `delegates/gpu/common/model_builder` to only accept the ops that you care about and limit dependencies in `delegates/gpu/gl/kernels` and/or `delegates/gpu/cl/kernels`.", "@impjdi Okay. Thanks a lot for the clarification.", "@impjdi How can you build these custom aar's/ op resolver? Is there any documentation of sorts?\r\n", "I'm not familiar with any TF documentation; maybe there is, maybe there isn't.  As an engineer, I look at existing code and try to mimic the closest example in the code base...", "Hi @t-niroc ! Does this [document](https://www.tensorflow.org/lite/guide/reduce_binary_size#build_with_custom_ops) answer above [ query](https://github.com/tensorflow/tensorflow/issues/38099#issuecomment-673240701)? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 38081, "title": "Allow users to specify method name when using tf.saved_model.save", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nToday, the `tf.saved_model.save` API does not allow users to specify a method name to use for a model signature. Instead, the method name is always `'tensorflow/serving/predict'`, because of the following line.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/saved_model/save.py#L462\r\n\r\nIn my case, this is particularly troublesome because I wish to use the TensorFlow Serving `classify` API which returns arrays of the form `(batch_size, classes, 2)` where the first entry is always a class name (string) and the second entry is a score (float). TensorFlow Serving seems to require that method name be `'tensorflow/serving/classify'`.\r\n\r\n**Will this change the current API? How?**\r\n\r\nYes, but I think it could be done without breaking existing functionality if we add another format option for the `signatures` argument to `tf.saved_model.save` that allows the user to supply a method name.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone wanting to use something other than the `predict` API in TensorFlow serving.\r\n\r\n**Any Other info.**\r\n\r\nFor now, I work around this issue with the following function, which implements a hack to get the desired method name saved.\r\n\r\n```python\r\nimport typing\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import python as tfp\r\n\r\ndef save_serving_model(export_dir: str, model: tf.keras.Model,\r\n                       classes: typing.List[str]):\r\n    \"\"\"Obtain a TensorFlow Serving function that can be used with\r\n    `tf.saved_models.save`\r\n\r\n    Args:\r\n        export_dir: The directory in which to export the saved model.\r\n        model: A classification model\r\n        classes: The list of classification labels\r\n    \"\"\"\r\n\r\n    @tf.function(\r\n        input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\r\n    def serve(inputs):\r\n        def map_fn(image):\r\n            image = tf.io.decode_image(\r\n                image['image'], channels=3, expand_animations=False)\r\n            image = tf.cast(image, dtype=tf.float32)\r\n            image = tf.compat.v1.image.resize_image_with_pad(\r\n                image=image,\r\n                target_height=model.input_shape[1],\r\n                target_width=model.input_shape[2],\r\n                align_corners=False,\r\n                method=tf.image.ResizeMethod.BILINEAR)\r\n            return image\r\n\r\n        images = tf.io.parse_example(\r\n            inputs,\r\n            features={\r\n                'image': tf.io.FixedLenFeature(shape=[], dtype=tf.string)\r\n            },\r\n            example_names=None,\r\n            name=None)\r\n        X = tf.map_fn(\r\n            fn=map_fn, elems=images, back_prop=False, dtype=tf.float32)\r\n        y = model.call(X)\r\n        labels = tf.constant([classes])\r\n        return {\r\n            'scores': y,\r\n            'classes': tf.repeat(\r\n                labels, repeats=tf.shape(y)[0], axis=0, name=None)\r\n        }\r\n\r\n    # This is a super-ugly hack, but we have to do it because for\r\n    # the API does not allow us to specify a method name. See:\r\n    # https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/saved_model/save.py#L462\r\n    try:\r\n        PREDICT_METHOD_NAME = tfp.saved_model.signature_constants.PREDICT_METHOD_NAME\r\n        tfp.saved_model.signature_constants.PREDICT_METHOD_NAME = tf.saved_model.CLASSIFY_METHOD_NAME  # pylint: disable=line-too-long\r\n        tf.saved_model.save(\r\n            model,\r\n            export_dir,\r\n            signatures={\r\n                tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\r\n                serve.get_concrete_function()\r\n            })\r\n    finally:\r\n        tfp.saved_model.signature_constants.PREDICT_METHOD_NAME = PREDICT_METHOD_NAME\r\n\r\n```\r\n\r\nOne can use this function with a Keras model in the following manner.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.layers.Input((100, 100, 3))\r\nx = tf.keras.layers.Conv2D(kernel_size=(2, 2), filters=3)(inputs)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\noutputs = tf.keras.layers.Activation('softmax')(x)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\nsave_serving_model(export_dir='export', model=model, classes=['class1', 'class2', 'class3'])\r\n```\r\nI can run the model with TensorFlow Serving using:\r\n\r\n```bash\r\ndocker run -it --rm -p 8501:8501 -v \"$PWD/export:/models/testmodel/1\" -e MODEL_NAME=testmodel tensorflow/serving\r\n```\r\n\r\nAnd I can make requests to the model using \r\n\r\n```python\r\nimport requests\r\nimport glob\r\n\r\nwith open('../tests/images/image1.jpg', 'rb') as f:\r\n    examples = [{\r\n        'image': {\r\n            'b64': base64.b64encode(f.read()).decode('utf-8')\r\n        }\r\n    }]\r\nresponse = requests.post(\r\n    'http://host.docker.internal:8501/v1/models/testmodel:classify',\r\n    json={\r\n        'examples': examples\r\n    }\r\n).json()\r\nprint(response['results'])\r\n```\r\n\r\nThis yields the following response:\r\n\r\n```\r\n[[['class1', 0.0], ['class2', 1.0], ['class3', 0.0]]]\r\n```\r\n\r\n*IMPORTANT:* If I'm mistaken on how this all works or is supposed to work, please do let me know. this is just what I was able to put together when transitioning to 2.x.", "comments": ["I can also work on this one with @shahshriya \r\n", "Thanks @faustomorales for opening the thread - it helped me a lot.\r\n\r\nFor others, that are using a higher version of TF (where tf.python is no longer exported) here is what I did to get around the limitation:\r\n\r\n```python\r\n    # TF version 2.4.1\r\n    import tensorflow as tf\r\n\r\n    # TFX Trainer or TF stuff to create model\r\n    signatures = { ... }\r\n    model.save(serving_model_dir, save_format=\"tf\", signatures=signatures)\r\n\r\n    # workaround\r\n    import os\r\n    import inspect\r\n    import imp\r\n    TF_PATH = inspect.getsourcefile(tf).replace(\"__init__.py\", \"\")\r\n    SAVE_PATH = os.path.join(TF_PATH, \"python/saved_model/method_name_updater.py\")\r\n    tfp = imp.load_module(\"tfp\", None, SAVE_PATH, (\"\", \"\", 5))\r\n    mnp = tfp.MethodNameUpdater(serving_model_dir)\r\n\r\n    # configure as many method names as you want\r\n    mnp.replace_method_name(\"serving_default\", tf.saved_model.CLASSIFY_METHOD_NAME)\r\n    mnp.save()\r\n```\r\n\r\nAfterwards was able to call the rest classify endpoint and get a result."]}, {"number": 38059, "title": "Also show \"Available Since\" for each API in tf docs", "body": "It would be useful to show the lifecycle of an API in the docs. That is, show when say, `tf.data.Dataset.take` was added or when a certain is removed / renamed. It could be extended to arguments of each API as well. Or to put it in one line,\r\n``` \r\nExpose version control information of APIs directly on tf docs\r\n```\r\nIt would really help developers keep up with rapid development of TF even better.", "comments": ["I'd love to work on this one. But where should we add these docs?", "Discussed a bit previously, but not sure it's worth it.\r\nGenerally, the API is cumulative to maintain backward compatibility (aside from the major version jumps). We publish multiple versions on the site (and archive the rest in GitHub) so if you're stuck on an old version, you can browse that.", "Yeah, looking at the older docs on Github, I realized it would be a useful\nextension to use git's power to extract when an API or its attributes were\nintroduced or modified automatically, without any additional work from TF\ndevelopers. As you might know many other popular libraries like pandas,\nsklearn and even Python itself use this approach. Many a times this\ninformation prompted me to upgrade my libraries after I learned about their\nrecent additions which could heavily simplify my code. Also in the current\ndocumentation I don't know of an easy way to search for 1.15 APIs, as the\ndefault search works for the latest stable release only. If we add this\nfeature to docs, users need not worry about which version of TF they are\nlooking at and can actively keep up with changes made by TF's vibrant\ncommunity every day. This is just my two cents. Feel free to ignore :)\n\n--\nNitin @ LinkedIn <http://bit.ly/nitinpasumarthy>\n\n\nOn Tue, Mar 31, 2020 at 12:38 PM Billy Lamberta <notifications@github.com>\nwrote:\n\n> Discussed a bit previously, but not sure it's worth it.\n> Generally, the API is cumulative to maintain backward compatibility (aside\n> from the major version jumps). We publish multiple versions on the site\n> (and archive the rest in GitHub) so if you're stuck on an old version, you\n> can browse that.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38059#issuecomment-606831806>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFDWXFYRKSXP44NRHXVFA3RKJBB7ANCNFSM4LW7I3CA>\n> .\n>\n", "The main issue is that the API surface of TensorFlow is extremely large at the moment, making documenting \"Available Since\" hard. While doable, we need to prioritize other work.\r\n\r\nOn the other work, modularization can help with this as every module could then start adding \"available since\" fields as needed."]}, {"number": 38039, "title": "flatbuffer.shape() might return nullptr", "body": "For certain tensors (e.g. multiply with a scalar (constant) tensor, such as 1/6 at the output of hard sigmoid), the shape attribute might be None/undefined and hence, flatbuffer.shape() returns a nullptr.\r\n\r\nIn this code\r\nhttps://github.com/tensorflow/tensorflow/blob/59c06b9016700dbf1ab0cefc062d247345cdd0f0/tensorflow/lite/micro/memory_helpers.cc#L82\r\nthis nullptr is dereferenced unconditionally.", "comments": ["@dauras , Please provide the standalone code to reproduce the issue. Thanks", "Sorry, I do not have an example at hand, but I can show you that TFL handles the shape==nullptr case differently than TFLM:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9fdaa925f4825b6c00da6f13fda7d75522fb82d6/tensorflow/lite/interpreter_builder.cc#L184-L188\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9fdaa925f4825b6c00da6f13fda7d75522fb82d6/tensorflow/lite/kernels/internal/tensor_ctypes.h#L34-L36\r\n\r\nSo, shape==nullptr is converted to an empty STL vector, or an empty RuntimeShape. Those are then similar/identical to [1,1,1,1], for where it matters (e.g. broadcast operators or buffer size calculations).", "I found another motivating example:\r\n\r\n- Download [MobileNet v3 small checkpoint](https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_1.0_uint8.tgz)\r\n- Extract the .tflite file\r\n- Decode with flatc\r\n      `flatc -t --defaults-json tensorflow/lite/schema/schema.fbs -- v3-small_224_1.0_uint8.tflite`\r\n- Inspect `v3-small_224_1.0_uint8.json`\r\n  Look for tensor named `MobilenetV3/expanded_conv_10/squeeze_excite/Conv_1/mul/y`\r\n  It has an empty shape:\r\n\r\n```\r\n        {       \r\n          shape: [\r\n\r\n          ],      \r\n          type: \"UINT8\",\r\n          buffer: 151,\r\n          name: \"MobilenetV3/expanded_conv_10/squeeze_excite/Conv_1/mul/y\",\r\n          quantization: {\r\n            min: [  \r\n              0.0     \r\n            ],      \r\n            max: [  \r\n              0.16667 \r\n            ],      \r\n            scale: [\r\n              0.000654\r\n            ],      \r\n            zero_point: [\r\n              0       \r\n            ],      \r\n            details_type: 0,\r\n            quantized_dimension: 0\r\n          },      \r\n          is_variable: false\r\n        },  \r\n```\r\n", "Ah that's an interesting point. If the shape is not known in advance (nullptr in the flatbuffer), TFLM interpreter won't be able to pre-calculate the memory usage for allocation. TFLite workaround this limitation by running prepare method to re-calculate the output shape for each layer, which is not support in TFLM.    I would suggest to investigate why the shape is undermined in the model.", "This tensor is one of the operands of a multiplication operator. This multiplication operator is part of the hard-sigmoid activation function ``tf.nn.relu6(x+3)*0.16667``, used in MobileNet v3. The tensor is a scalar, and it is constant. Its value is ``0.16667``.\r\nhttps://github.com/tensorflow/models/blob/295b3b4f2d304a6e90b518755fd6d5bf90005975/research/slim/nets/mobilenet/mobilenet_v3.py#L40", "I see. You meant that BytesRequiredForTensor should be able to handle scalar. I think that might be reasonable. We haven't had a case for passing scalar as a tensor. But feel free to submit a PR if you're blocked on this.", "Confirm the issue. It can be reproduced in the model with GRU when performing OPTIMIZE_FOR_SIZE quantization. In order to observe the issue one should also provide converter.representative_dataset, to enforce quantization of activations.\r\n\r\nDereferencing of the NULL pointer happens when processing tensors  **model/gru/gru_cell_3/split/split_dim** and  **model/gru/gru_cell_3/sub/x**.\r\n\r\nCan anyone suggest a workaround for this?"]}, {"number": 37991, "title": "[tf.estimator] - `tf.estimator.Head` do not work with TPUs", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Google Collab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: Nont issue\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): Google Collab\r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): None\r\n- CUDA/cuDNN version: - GPU model and memory: None, TPU\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.estimator.Heads` do not work with TPUs; they are scoped to the CPU, and are seen to belong to a different graph than the expected.\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.estimator.Heads` work across CPUs, GPUs, and TPUs.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nPlease run the below Collab notebook. You will need to create, provide, and authenticate to a GCS bucket.\r\n\r\nhttps://colab.research.google.com/gist/nmatare/3e08383751184bf10cc2a1ef0a922b97/untitled6.ipynb\r\n\r\n", "comments": ["@nmatare,\r\nI tried reproducing the issue but I'm facing an error stating `ValueError: Tensor(\"loss:0\", shape=(), dtype=string) must be from the same graph as Tensor(\"loss:0\", shape=(), dtype=string).`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/45c538419ae13eda80171f54d5a7d071/37991.ipynb).\r\n\r\nCould you please confirm if this is the error you are expecting? Thanks!", "Yup, I can confirm that this is the same underlying error. The operations have apparently been placed onto two distinct graphs.", "I tried to run the code on Colab  with TF v2.5 and faced `ValueError` ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/74cd2a89776183f0dafa768232b48664/37991.ipynb#scrollTo=x9CcijYYBjE5) ..Thanks!"]}, {"number": 37982, "title": "Large Model Support", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2 nightly\r\n- Are you willing to contribute it (Yes/No): Code exists\r\n\r\ncode is here:\r\nhttps://github.com/IBM/tensorflow-large-model-support\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorFlow Large Model Support (TFLMS) is a feature in the TensorFlow provided by IBM Watson Machine Learning Community Edition (WML CE) that allows the successful training of deep learning models that would otherwise exhaust GPU memory and abort with \"out-of-memory\" errors. LMS manages this oversubscription of GPU memory by temporarily swapping tensors to host memory when they are not needed.\r\n\r\n**Will this change the current api? How?**\r\nwill add:\r\n`tf.config.experimental.set_lms_enabled(True)`\r\n\r\n**Who will benefit with this feature?**\r\neveryone who trains models\r\n", "comments": ["Hi @bela127, are you proposing to implement this feature (i.e. will you send a PR at some point)?", "CC @reedwm ", "Hi,\r\nI definitely can help, but I'm not that deep into tensor flow backend, and more a python developer, than c++, so I guess I will need help.\r\n\r\nThe code is ready for tf 2.1 , so one could try to do the merging, but 2.2 changed a lot of API, at least at the python side, so I'm not shure how much has to be changed.", "I think having [an RFC](https://www.tensorflow.org/community/contribute/rfc_process) for this feature would be the best process to getting it into TensorFlow. The link RFC documentation describes you should first email [developers@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/developers) to discuss the proposal.", "@reedwm \r\nok, thanks.\r\nif i understand the RFC documentation correctly, i should email developers@tensorflow.org directly, without creating a RFC document or feature-request before?\r\n\r\nif this is correct, i will send a mail with a short description and a link to this issue.\r\n\r\nIt although seems to be a relatively long process, so i guess, i should wait a little before i start it, I'm just in the middle of a Thesis, and if i start a RFC i want to have the time to reply.", "@alextp can you confirm emailing develoeprs@tensorflow.org is the best process? And since @bela127 is fairly busy, is it better to wait to start the RFC process until they have more time?", "Email with a description of the plan to developers@tensorflow.org is the right way to start.\r\n\r\nI will say though that per the TF API guidelines in https://github.com/tensorflow/community/blob/master/governance/design-reviews.md#programmability--flexibility (specially the programmability section) I am not a fan of a one-push-button switch for this behavior. Can we build it instead out of other pieces in the TF API that would in principle be recombinable to achieve other results?", "Ok, Il send the mail.\r\n\r\n@alextp \r\nI just reviewed the TF API guidelines, and instead of the \"one button\" solution with `tf.config.experimental.set_lms_enabled(True)` (as it is implemented by IBM) one could implement a context manager.\r\nsomething like:\r\n`with auto_swap_tensors_to_dev(\"cpu\"):`\r\nevery tensor defined in the context gets swaped, but you can swap different tensors/model parts to other devs.\r\n\r\nThe only other solution i can think of, would be several evaluation hooks:\r\n`do_before_evaluation_step(func)` func gets called before every calculation\r\n`get_next_tensors_in_pipeline()` gets names/symbolic tensors that are needed for calculation\r\n`get_unused_tensors()` gets names/symbolic tensors that are currently not needed for calculation\r\n`swap_tensors_from_dev_to_dev(tensor, dev_source, dev_target)` dev copy already possible, but this should not create \"new\" symbolic tensor, instead it performs a dev swap.\r\n\r\nwith this one could write:\r\n```\r\ndef before_evaluation_callback():\r\n   unused = get_unused_tensors()\r\n   swap_tensors_from_dev_to_dev(unused, \"gpu\", \"cpu\")\r\n   next = get_next_tensors_in_pipeline()\r\n   swap_tensors_from_dev_to_dev(next, \"cpu\", \"gpu\")\r\n\r\ndo_before_evaluation_step(before_evaluation_callback)\r\n```\r\nor with context manager:\r\n`with do_before_evaluation_step(before_evaluation_callback):`\r\n\r\ninstead of:\r\n`swap_tensors_from_dev_to_dev(tensor, dev_source, dev_target)`\r\nit could be slitted into, smaler steps:\r\n`copy_tensor_to_dev(tensor, dev_target)` Is already possible\r\n`clear_tensor_from_dev(tensor, dev_source)`\r\n`load_tensor_from_dev(tensor, dev_source)`\r\n\r\nthis would give the possibility to implement various kind of memory management options:\r\n1. the proposed LMS (https://github.com/IBM/tensorflow-large-model-support) -> see above\r\n2. gradient checkpointing (https://github.com/cybertronai/gradient-checkpointing) -> just delete every n tenors, and they are recomputed if needed. -> how to select good tensors to delete?\r\n\r\ni hope i could provide a basis for discussion.", "My main concern is that multiple one-button solutions tend to not combine\nwell with each other and with unforeseen features of TF.\n\nOn Wed, Apr 1, 2020 at 12:13 PM Bela B\u00f6hnke <notifications@github.com>\nwrote:\n\n> Ok, Il send the mail.\n>\n> @alextp <https://github.com/alextp>\n> I just reviewed the TF API guidelines, and instead of the \"one button\"\n> solution with tf.config.experimental.set_lms_enabled(True) (as it is\n> implemented by IBM) one could implement a context manager.\n> something like:\n> with auto_swap_tensors_to_dev(\"cpu\"):\n> every tensor defined in the context gets swaped, but you can swap\n> different tensors/model parts to other devs.\n>\n> The only other solution i can think of, would be several evaluation hooks:\n> do_before_evaluation_step(func) func gets called before every calculation\n> get_next_tensors_in_pipeline() gets names/symbolic tensors that are\n> needed for calculation\n> get_unused_tensors() gets names/symbolic tensors that are currently not\n> needed for calculation\n> swap_tensors_from_dev_to_dev(tensor, dev_source, dev_target) dev copy\n> already possible, but this should not create \"new\" symbolic tensor, instead\n> it performs a dev swap.\n>\n> with this one could write:\n>\n> def before_evaluation_callback():\n>    unused = get_unused_tensors()\n>    swap_tensors_from_dev_to_dev(unused, \"gpu\", \"cpu\")\n>    next = get_next_tensors_in_pipeline()\n>    swap_tensors_from_dev_to_dev(next, \"cpu\", \"gpu\")\n>\n> do_before_evaluation_step(before_evaluation_callback)\n>\n> instead of:\n> swap_tensors_from_dev_to_dev(tensor, dev_source, dev_target)\n> it could be slitted into, smaler steps:\n> copy_tensor_to_dev(tensor, dev_target) Is already possible\n> clear_tensor_from_dev(tensor, dev_source)\n> load_tensor_from_dev(tensor, dev_source)\n>\n> this would give the possibility to implement various kind of memory\n> management options:\n>\n>    1. the proposed LMS (\n>    https://github.com/IBM/tensorflow-large-model-support) -> see above\n>    2. gradient checkpointing (\n>    https://github.com/cybertronai/gradient-checkpointing) -> just delete\n>    every n tenors, and they are recomputed if needed. -> how to select good\n>    tensors to delete?\n>\n> i hope i could provide a basis for discussion.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37982#issuecomment-607439505>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHROHHS4SXRV424ZUPPLRKOG63ANCNFSM4LVFXJMQ>\n> .\n>\n\n\n-- \n - Alex\n", "I fully understand, one button solutions are nice for high level work, but for low level work, multiple single functionality is much better composable.\r\n\r\nthat is why i tried to decompose the one button solution", "i just recognized that this feature request is basically, the request for a hook into tensorflows tensor memory management strategy.\r\n\r\nat the moment tensorflow only knows the, \"calculate and keep\" strategy, but there are a lot of possible [other strategies](https://github.com/cybertronai/gradient-checkpointing#how-it-works) and every thing between them, like moving tensors to other memory devs.\r\n\r\nHigh Level \"one button\" switches to choose between different strategies, would be nice, like the introduced [strategies for distribution](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly) but for memory management.\r\n\r\nbut a low level api for building ones one strategy, as suggested by @alextp would although be nice, likewise a low level api for building distribution strategies exists with explicit dev placement, also Im not aware of an api for building custom automated distribution strategies.", "the features could be implemented with a Api to, the [meta optimizer](https://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf)\r\nIn the slides, I found the memory optimizer, but i cant find it in the api docs, is this undocumented or not available?\r\nIn the slides It seems it already supports the features I asked for, there is also [this side](https://www.tensorflow.org/guide/graph_optimization?hl=sk), but i cant find anything in the API Docs how to configure it.\r\nWith a known and good API, it would be the perfect place for implementing all memory management strategies, and support for custom ones."]}, {"number": 37966, "title": "Guide for Distributed TF with c++", "body": "I see the core here https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/distributed_runtime\r\n\r\nBut there is no instruction about how to run TF with distributed system (C++ language). \r\nPlease share with us some documents about distributed TF c++.\r\n\r\nThanks,", "comments": ["Same. Tensorflow states [supports for many others](https://www.tensorflow.org/api_docs) but could not even find an example or guide for using to in C++. Even [installation is on Python only](https://www.tensorflow.org/install).", "I try to  run TF with distributed system,but program core dump .No session factory registered for the given session options: {target: \"grpc://127.0.0.1:2300\" config: placement_period: 1 graph_options { optimizer_options { opt_level: L0 } }} Registered factories are {DIRECT_SESSION}.", "Hi. Are there any updates about this?? Which is the best way to running inference with multiple GPUs on the same model with c++ API (Using SavedModel)", "Please refer to these links and let us know if it helps:[link](https://github.com/tensorflow/tensorflow/issues/18861),[link1](https://stackoverflow.com/questions/48422869/how-to-specify-which-device-to-use-for-saved-model)", "> Please refer to these links and let us know if it helps:[link](https://github.com/tensorflow/tensorflow/issues/18861),[link1](https://stackoverflow.com/questions/48422869/how-to-specify-which-device-to-use-for-saved-model)\r\n\r\nI reached the other issue and tried the solution it provides related to SavedModel. Here(https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-850057412)\r\nBut I couldn't make working -> you can see my errors here  https://github.com/tensorflow/tensorflow/issues/18861#issuecomment-920758912\r\n"]}, {"number": 37941, "title": "Update vocabulary and add a new embeddings for the new token as you see them in training time", "body": "I am aware i can use `x = tf.feature_column.categorical_column_with_vocabulary_list(cat_col_name,cat_col_unique_values)`. Where `cat_col_name` is the categorical column that i need to convert to consumable format by a dense layer `cat_col_unique_values` are the unique values present in this column. Then i use something like `feature_column.embedding_column(x, dimension=4)` to map each unique value in `cat_col_name` to an embedding of dimension 4. \r\n\r\nWhat if i have a really large dataset and it would not be feasable to know `cat_col_unique_values` beforehand. I mean to say, During training time:\r\n\r\n1. keep reading batches and mapping the unique values seen to their corresponding embedding\r\n2. In case a new value is seen in the columns cat_col_name, add another embedding to correspond to this newfound value\r\n\r\nI dont suppose there is a way to do this currently, or is there?\r\n", "comments": ["@nitinmnsn, Could you provide the standalone code and the Tensorflow version. Thanks", "tnesorflow version : 2.1.0\r\nstandalone code for what? This is a feature request", "any update on this?"]}, {"number": 37910, "title": "MicroInterpreter error: Seg Fault, *** stack smashing detected *** inside MicroInterpreter::Invoke() method", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 64-bit, \r\n- TensorFlow installed from (source or binary): Source \r\n- Tensorflow version (commit SHA if source): 0c487d64172c64d60a93bc98cf5ea07f1a8e95ba\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Ubuntu 18.04.4 64-bit, Linux raspberrypi 4.19.97-v7\r\n- Compiler: g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0 and arm-linux-gnueabihf-g++ 8.4.0\r\n- Python: Python 3.7.6\r\n\r\n**Describe the problem**\r\nI am trying to use the quantized model with Tensorflow Lite Micro, and got a segmentation error inside interpreter->Invoke() call.\r\n\r\nDebugger showed that segmentation error occurred on returning from Eval() in conv.cc on Node 28 of CONV_2D, and stack was corrupted. Error message is *** stack smashing detected ***: <unknown> terminated with compiler flags \"-fstack-protector-all -Wstack-protector\".\r\n\r\nMy test was simply from the [person detection example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection) with model replaced with Mobilenet_V1_0.25_224_quant at the [Tensorflow lite pre-trained models site](https://www.tensorflow.org/lite/guide/hosted_models) with increased enough kTensorArenaSize and model input/output size changed to 224x224x3 and 1x1001, amd pulled additional required operators.\r\n\r\nAlso tried a few different models, at another quantified mode Mobilenet_V1_0.25_192_quant is showing the same segfault problem, But the regular floating point modes Mobilenet_V1_0.25_192, and Mobilenet_V1_0.25_224 run OK with many loops.\r\n\r\nHave anyone seen similar problem ? Or is some limitations on Tensorflow Lite Micro that I should be aware of ?\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nThis problem can be reproduced at [this commit ](https://github.com/Jeff-Zhu/tensorflow/tree/bf2fef29281108c553c16e14dd4632b9a629de3c) of forked tensorflow repo.\r\n\r\nCommands to reproduce this problem:\r\nBuild command:\r\n```\r\n$ git clone https://github.com/Jeff-Zhu/tensorflow\r\n$ cd tensorflow\r\n$ git checkout bf2fef29281108c553c16e14dd4632b9a629de3c\r\n$ bazel build //tensorflow/lite/micro/examples/person_detection:person_detection       -c dbg --copt=-fstack-protector-all --copt=-Wstack-protector --copt=-fno-omit-frame-pointer\r\n$ ./bazel-bin/tensorflow/lite/micro/examples/person_detection/person_detection\r\n*** stack smashing detected ***: <unknown> terminated\r\nAborted (core dumped)\r\n```\r\ncoredump stack shows corrupted:\r\n#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:51\r\n#1  0x00007fd4e9c5b801 in __GI_abort () at abort.c:79\r\n#2  0x00007fd4e9ca4897 in __libc_message (action=action@entry=do_abort, \r\n    fmt=fmt@entry=0x7fd4e9dd1988 \"*** %s ***: %s terminated\\n\") at ../sysdeps/posix/libc_fatal.c:181\r\n#3  0x00007fd4e9d4fcd1 in __GI___fortify_fail_abort (need_backtrace=need_backtrace@entry=false, \r\n    msg=msg@entry=0x7fd4e9dd1966 \"stack smashing detected\") at fortify_fail.c:33\r\n#4  0x00007fd4e9d4fc92 in __stack_chk_fail () at stack_chk_fail.c:29\r\n#5  0x0000559e534cab55 in tflite::ops::micro::conv::Eval (context=0x559e536c9280 <setup::static_interpreter+32>, \r\n    node=0x559e536c5b18 <(anonymous namespace)::tensor_arena+1419832>) at tensorflow/lite/micro/kernels/conv.cc:269\r\n#6  0x42f6730442f67304 in ?? ()\r\n#7  0x42f6730442f67304 in ?? ()\r\n#8  0x42f6730442f67304 in ?? ()\r\n#9  0x42f6730442f67304 in ?? ()\r\n#10 0x42f6730442f67304 in ?? ()\r\n\r\n***Files changed from the original person_detection example***\r\n```\r\ntensorflow/lite/micro/examples/person_detection/main_functions.cc\r\ntensorflow/lite/micro/examples/person_detection/model_settings.h \r\ntensorflow/lite/micro/examples/person_detection/person_detect_model_data.cc\r\n```\r\n\r\nChanges in [main_functions.cc](https://github.com/Jeff-Zhu/tensorflow/blob/bf2fef29281108c553c16e14dd4632b9a629de3c/tensorflow/lite/micro/examples/person_detection/main_functions.cc):\r\n\r\n```\r\nconstexpr int kTensorArenaSize = 1400 * 1024;\r\nstatic tflite::MicroOpResolver<5> micro_op_resolver;\r\nmicro_op_resolver.AddBuiltin(tflite::BuiltinOperator_RESHAPE,\r\n                             tflite::ops::micro::Register_RESHAPE());\r\nmicro_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,\r\n                             tflite::ops::micro::Register_SOFTMAX(), 1, 2);\r\n```\r\n\r\nChanges in [model_settings.h](https://github.com/Jeff-Zhu/tensorflow/blob/bf2fef29281108c553c16e14dd4632b9a629de3c/tensorflow/lite/micro/examples/person_detection/model_settings.h)\r\n\r\n```\r\nconstexpr int kNumCols = 224;\r\nconstexpr int kNumRows = 224;\r\nconstexpr int kNumChannels = 3;\r\nconstexpr int kCategoryCount = 1001;\r\n```\r\n\r\nChanges in [person_detect_model_data.cc](https://github.com/Jeff-Zhu/tensorflow/blob/bf2fef29281108c553c16e14dd4632b9a629de3c/tensorflow/lite/micro/examples/person_detection/person_detect_model_data.cc):\r\n\r\nThis file is modified with data from Mobilenet_V1_0.25_224_quant.tflite from [this pre-trained quantized mobilenet v1 model](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_224_quant.tgz) on [Hosted models site](https://www.tensorflow.org/lite/guide/hosted_models).\r\n```\r\n$ xxd -i -c 12 mobilenet_v1_0.25_224_quantized.tflite  > model_data.cc\r\n```\r\nthen replacing model data in the new model_data.cc in the tensorflow/lite/micro/examples/person_detection/person_detect_model_data.cc, update the array name and size.\r\n\r\nThanks for your help.\r\n\r\n", "comments": ["The problem was first on Ubuntu 18.04 x86_64 platform.\r\nLater I set up a Raspberry Pi 3 with 32-bit Raspbian, tests show the same \"stack smashing\" problem.", "I found that the problem is caused by an array overrun of the layer operation data. Tensorflow microlite has a hidden limit (or I missed document, at least TF microlite runtime does not check) on output channels to maximum *256* in the OpData structure of [conv.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/conv.cc) for TF micro lite. \r\n```\r\nconstexpr int kMaxChannels = 256;\r\n....\r\nstruct OpData {\r\n...\r\n  // Per channel output multiplier and shift.\r\n  // TODO(b/141139247): Allocate these dynamically when possible.\r\n  int32_t per_channel_output_multiplier[kMaxChannels];\r\n  int32_t per_channel_output_shift[kMaxChannels];\r\n...\r\n}\r\n``` \r\nThe mobilenet model [Mobilenet_V1_0.25_224_quant.tflite](https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_224_quant.tgz) is with 1000 output classes, and total of 1001 channels internally. And it caused stack corruption in tflite::PopulateConvolutionQuantizationParams() of   tensorflow/lite/kernels/kernel_util.cc:90 for the last Conv2D with output size of 1001.\r\n\r\nNo problem for TF, and TF lite as they are believed not using this structure definition.\r\n\r\nConfirmed with increasing the channels to 1024 on loops of model evaluation calls.\r\n\r\nAlthough most of TF microlite cases are likely with small models, and probably won't run into this problem.\r\n\r\nShould this limit be documented and/or perform check at run-time ?"]}, {"number": 37904, "title": "**debug feature** for debugging numeric instability, like nan or infinity", "body": "**System information**\r\n- TensorFlow version (you are using): nightly\r\n- Are you willing to contribute it (Yes/No): cant do it alone, but willing to help\r\n\r\n\r\n\r\n**Describe the feature and the current behaviour/state.**\r\nI propose a **debug feature** for debugging numeric instability. The functionality should be similar to `tf.debugging.enable_check_numerics()` but more granular, but still more high level then `tf.debugging.check_numerics`.\r\n\r\nImplementation Idea:\r\nA `tf.debugging.check_func_numerics` annotation is added. It can be applied to ops warped in a python eager function and to tf.functons.\r\nIt will add numerics checks to all ops used in this function and, this is IMPORTANT, to all corresponding gradient ops.\r\n\r\nThe part of a model which courses numeric instability can be identified with this.\r\n\r\n**Will this change the current api? How?**\r\nit will add another debugging function/annotation, so the standard api is not changed.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone with problems with nan and infinity in there models.\r\nEspecially for big models where `tf.debugging.check_func_numerics` can not be applied to the hole model.", "comments": []}, {"number": 37808, "title": "Code improvements results from Cppcheck", "body": "I ran [cppcheck](https://github.com/danmar/cppcheck) on the Tensorflow codebase which resulted in the some code improvements. \r\n\r\nDownload the results here: [tensorflow_cppcheck.zip](https://github.com/tensorflow/tensorflow/files/4365647/tensorflow_cppcheck.zip)\r\n", "comments": ["I should handle these by the weekend", "@jensdenbraber could you please share the config files (suppress, includes, etc.) for cppcheck to run properly?\r\n\r\nThanks a lot"]}, {"number": 37806, "title": "tf.Module does not update state when a trackable attribute is reassigned a nontrackable", "body": "\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Sid and Linux Arch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): PyPi binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: - Bazel version (if compiling from source): 3.7.6\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen subclassing tf.Module and assigning a tf.Variable to an attribute, then later replacing it with a constant, tensorflow does not delete the Variable from its internal list of tracked objects. This leads to e.g. the Variable being saved and loaded with tf.saved_model, even though it's no longer referenced by any user-visible attribute.\r\n\r\n**Describe the expected behavior**\r\nTensorflow correctly deletes the tf.Variable from its internal state.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nclass test(tf.Module):\r\n    pass\r\n\r\na = test()\r\na.var = tf.Variable(1)\r\na.const = tf.constant(2)\r\na.var = tf.constant(a.var.numpy())\r\n\r\ntf.saved_model.save(a, \"a\")\r\nal = tf.saved_model.load(\"a\")\r\n```\r\nTrying to access `al.const` throws an exception:\r\n```python\r\nal.const\r\n```\r\n```python\r\nAttributeError: '_UserObject' object has no attribute 'const'\r\n```\r\nHowever, `al.var` still contains the tf.Variable:\r\n```python\r\nal.var\r\n```\r\n```python\r\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>\r\n```\r\n\r\n**Other info / logs**\r\n\r\nAs far as I can tell, the problem is that `AutoTrackable.__setattr__` does not delete its internal tracking state if a Trackable is replaced by something non-trackable, it only handles cases where a non-trackable is replaced by a Trackable or a Trackable is replaced by another Trackable.", "comments": ["i have replicated the code shared and issue persist, please find [gist here](https://colab.sandbox.google.com/gist/Saduf2019/50b876d78a699e5ac4feaef73b2dfd84/37806.ipynb)", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/aed14a440460e3cbf025d7932734bf8b/untitled7.ipynb). Thanks!", "I could reproduce the issue with TF 2.7 . Please, find the gist [here](https://colab.research.google.com/gist/kumariko/8a66abfc6f34386539a6e68d03b5b7f9/untitled7.ipynb#scrollTo=fPzJ0Pz_8T8A). Thanks!"]}, {"number": 37795, "title": "GradientTape.jacobian works for batch shape 0 when `experimental_use_pfor=True` but not when `experimental_use_pfor=False`", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  `macOS Catalina 10.15.2 (19C57)`\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): `v2.2.0-rc0-43-gacf4951a2f 2.2.0-rc1 0.10.0-dev20200321`\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: - GPU model and memory: n/a\r\n\r\n\r\n**Describe the current behavior**\r\nUsing `experimental_use_pfor=False` in `GradientTape.jacobian` behaves otherwise similarly as using `experimental_use_pfor=True`, but when running with tensors of batch shape 0, the former fails due to a type error, whereas the latter one works fine.\r\n\r\n**Describe the expected behavior**\r\nI would expect the behavior to be the same in both cases when `experimental_use_pfor=False` and `experimental_use_pfor=True`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    variable = tf.Variable(1.0)\r\n    inputs = (\r\n        tf.constant(tf.random.uniform((0, 4))),\r\n        tf.constant(tf.random.uniform((0, 3))),\r\n    )\r\n\r\n    with tf.GradientTape(persistent=True) as tape:\r\n        outputs = variable * tf.pow(tf.concat(inputs, axis=-1), 2.0)\r\n\r\n    jacobians_1 = tape.jacobian(\r\n        outputs,\r\n        variable,\r\n        experimental_use_pfor=True,\r\n    )\r\n    print(jacobians_1)\r\n    print(\"tape.jacobians(..., experimental_use_pfor=True) works!\")\r\n\r\n    try:\r\n        jacobians_2 = tape.jacobian(\r\n            outputs,\r\n            variable,\r\n            experimental_use_pfor=False,\r\n        )\r\n        print(jacobians_2)\r\n        print(\"tape.jacobians(..., experimental_use_pfor=False) works!\")\r\n    except TypeError:\r\n        print(\"tape.jacobians(..., experimental_use_pfor=False) doesn't work!\")\r\n        raise\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nOriginally posted here: https://github.com/tensorflow/tensorflow/issues/32460#issuecomment-572545864.\r\n\r\n```\r\ntape.jacobians(..., experimental_use_pfor=True) works!\r\ntape.jacobians(..., experimental_use_pfor=False) doesn't work!\r\nTraceback (most recent call last):\r\n  File \"./tests/test_tape_jacobian.py\", line 36, in <module>\r\n    main()\r\n  File \"./tests/test_tape_jacobian.py\", line 26, in main\r\n    experimental_use_pfor=False,\r\n  File \"/Users/hartikainen/conda/envs/softlearning-tf2/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\", line 1151, in jacobian\r\n    for i, out in enumerate(output):\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n", "comments": ["i have replicated this issue on nightly, please [find gist](https://colab.sandbox.google.com/gist/Saduf2019/a1e2219397c3ce701988c32552a3df72/untitled104.ipynb) here", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/d09e4026b226e4b8007f096810c9fb1b/untitled7.ipynb). Thanks!", "Thanks for reporting this!", "Was able to reproduce issue in Tf 2.7, please find the gist [here](https://colab.research.google.com/gist/kumariko/8a42c0c222e29898235da0b9a5ec87a1/untitled7.ipynb#scrollTo=fPzJ0Pz_8T8A). Thanks!"]}, {"number": 37768, "title": "min_epsilon of fused_batch_norm is incorrect", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): n/a\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): n/a\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source orbinary): n/a\r\n- TensorFlow version (use command below):  n/a\r\n- Python version: n/a\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\nThe source code in current master branch has this:\r\nhttps://github.com/tensorflow/tensorflow/blob/0c853d6cadf055be10562cfec92a93776fafd555/tensorflow/python/ops/nn_impl.py#L1516-L1519\r\n\r\nHowever, the statement in the comment is no longer correct, according to https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_750.html:\r\n> the value of epsilon is required to be greater or equal to CUDNN_BN_MIN_EPSILON which was defined in the cudnn.h file to the value 1e-5. This threshold value is now lowered to 0.0 to allow a wider range of epsilon value.\r\n\r\nIf compatibility with earlier version of cudnn is needed, the minimum should be obtained by using the `CUDNN_BN_MIN_EPSILON` constant defined by `cudnn.h`. \r\nOr, maybe it's better to just throw an error when a wrong epsilon is provided by the user.  Personally I don't like my parameter to be silently changed -- this can cause issues like `nn.fused_batch_norm` and `nn.batch_normalization` producing different results.", "comments": ["@ppwwyyxx \r\ncould you please share simple stand alone code along with tensor flow version for us to replicate the issue faced", "The issue is clearly written in TensorFlow source code which I pasted above.", "Hi @ppwwyyxx! Could you cross check the pull request #53065 ?", "Ok @ppwwyyxx! This issue will be closed one Above PR is merged. Thanks!", "Hi @ppwwyyxx ! Internal tests are failing for above changes. Could you let us know the epsilon value in Cuda 8.1 for 2.8 version? Thanks! "]}, {"number": 37751, "title": "corrupted tfrecord error should also have the info of file name", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\nCould TensorFlow 2.0 add the name of tfrecord file  when the file is corrupted in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/record_reader.cc#L105 ? It  is very helpful for investigating  tfrecord file issue or storage issue.\r\n\r\nwe tried to add  like https://github.com/cheyang/tensorflow/commit/54015629282b35d848d3c64550d15c3804e05c8d\r\n\r\nBut it has segment fault issue. \r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 37692, "title": "shape asserts to check internal code correctness", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n`tf.debugging.assert_shapes` throws `tf.errors.InvalidArgumentError` or `ValueError`. This is unsuitable for use as an assertion of internal code correctness.\r\n\r\n**Will this change the current api? How?**\r\n\r\nSome way of adding an assertion that mirrors the way `assert` works in python. In particular, it should\r\n  * throw an exception that shouldn't typically be caught (analogous to `AssertionError`)\r\n  * be possible to turn it off via the in built `__debug__` flag\r\n\r\nThis could be a flag in `tf.debugging.assert_shapes`, or a separate function\r\n\r\n**Who will benefit with this feature?**\r\n\r\nDevelopers that build on top of tensorflow and want to check the correctness of shapes within their code, as opposed to checking user input.", "comments": ["The change in the thrown type can't be done without a major version increase as it is backward-incompatible.\r\n\r\nWhat specifically is the `__debug__` flag in the context of tensorflow? I'm not familiar with it.", "ok, so perhaps a separate function would be better than changing the error type. `__debug__` is a python built-in that can be switched from `True` to `False`, see [the docs](https://docs.python.org/3/reference/simple_stmts.html#assert)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Why close the ticket because there's no activity? I don't believe it's been fixed yet", "@joelberkeley,\r\nSorry for the delayed response. As per the documentation of [**`tf.debugging.assert_shapes`**](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes#raises), it raises **`ValueError`** if:\r\n\r\n> If static checks determine any shape constraint is violated.\r\n\r\nAlso, there is no Error in the [First Example](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes#example). \r\n\r\nHope this helps. Thanks!", "@rmothukuru sorry it doesn't. I understand how `assert_shapes` works. I just don't think its functionality is suitable for internal code correctness checks. Bear in mind I'm talking about the correctness of API code, as opposed to the correctness of user code that calls that API. For example, in `add(x, y)`, I want to assert that I've implemented `add` correctly, not that the user has passed in the correct shape of `x` and `y`. It's unhelpful to raise the same kinds of error for both cases, cos then users can't effectively handle those errors"]}, {"number": 37681, "title": "Getting std::bad_alloc in Android - StyleTransfer", "body": "Hello, I've followed the examples/styletransfer repository. When I build&run the example everything worked with no errors. After that I changed the code a little bit to run my custom model. The model has these values;\r\n\r\ncustommodel.tflite {\r\ninput [1, 320, 320, 3] float32\r\noutput [1, 320, 320, 3] float32\r\n} \r\n\r\nWhen I run it everything works well, too. The result of trained model is showing up in a imageview in about 75~100 ms. But quality did not satisfied me. So I tried to run with model 640x640, here its values;\r\n\r\ncustommodel_two.tflite {\r\ninput [1, 640, 640, 3] float32\r\noutput [1, 640, 640, 3] float32\r\n}\r\n\r\nWhen I run the application, it crashed with errors;\r\n\r\nusing NNAPI:\r\n\t`Conv size is too large, not enough memory`\r\n\r\nusing CPU/GPU:\r\n\t`A/libc: /usr/local/google/buildbot/src/android/ndk-release-r17/external/libcxx/../../external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion \"terminating with uncaught exception of type std::bad_alloc: std::bad_alloc\" failed`\r\n\r\n`A/libc: Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 27666 (AsyncTask #1), pid 27614 (.astyletransfer)`\r\n\r\nAnd here is the call stack of methods that getting crash: \r\n`-NativeInterpreterWrapper.java`\r\n`--(line 156) run(this.interpreterHandle, this.errorHandle);`\r\n`---private static native void run(long var0, long var2);`\r\n\r\nI tried with LG-G6, Samsung S7, Nexus5, Pixel (Emulator) both x86&x64 architectures. Also changed thread numbers: 1 - 4 - 7 - 12 but no luck.\r\n\r\nUsing these versions:\r\n\r\n`implementation 'org.tensorflow:tensorflow-lite:2.1.0'`\r\n`implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'`\r\n`implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'`\r\n\r\nSo I wonder, what could be wrong with this ? \r\n\r\nThanks.", "comments": ["It seems that the built-in model works, while your custom model fails. From the context, it seems that your own model took up so much memory that it cannot allocate.\r\n\r\nPlease notice that for each layer inside a neural network, input and output tensors of that layer also require memory allocation. For example, for input image shaped [640, 640, 3], support a layer has output channel 128 with the same width and height, the output will be [640, 640, 128] (much larger than the input image). So It really depends on what kind of model is used.\r\n\r\nCan you provide more spec details of your custom model? Thanks!", "Thanks for your reply. Yes, one of my custom models fails but my other model works (320x320). I leave two links there which contains screenshots from netron output. By the way, what kind of data do you expect me to share ? Because I do not know how can I give you a convenient explanation about the model. \r\n\r\n1: https://icoupstudios.com/tflite/1.png\r\n2: https://icoupstudios.com/tflite/2.png\r\n\r\nAlso here you can see the profiler when I run the model.\r\nhttps://icoupstudios.com/tflite/3.png\r\n\r\nNote: The .tflite model has converted from .h5 file for Android, in IOS we are converting it to .mlmodel and everything is running perfect and smooth on the IOS side.  \r\n\r\nThanks!", "Hi Amit, can you try a smaller Conv2D kernel? (Maybe, from size 9x9 to 3x3, which is more commonly used)\r\n\r\nMaybe, because the size is too large, it runs out of GPU memory.", "Hello @lintian06 , thank you for your reply.\r\n\r\nThe Conv2d params are the same as IOS model. So I think Android should run it, because as you know; less conv2D value is less quality. We don't want to lose the quality.\r\n\r\nI share a repository below for unterstand that\r\nhow I run inference of the model.\r\n\r\nhttps://github.com/ikasapoglu/StyleTransferExample\r\n\r\nAlso, I upload a model file [here](https://drive.google.com/open?id=198K1WFEOvq0L9JRSW19_GBx01qBVBurL). The model size is 640x480. The strange case is this model that I can run it only once! Yes, the model running pretty well in first time, but when I run it again the result is the same as others; \r\n\r\n\"Fatal signal 6 (SIGABRT), code -6 (SI_TKILL) in tid 19685 (.astyletransfer), pid 19685 (.astyletransfer)\" \r\n\r\nIt is very weird\r\n\r\nI'm waiting your review and reply, thank you.\r\n\r\n\r\n**EDIT**: I changed Tflite version to nightly ('org.tensorflow:tensorflow-lite:0.0.0-nightly'). Right now, I can run the model 5 times but I got the same error in 6th run . Please look at the Memory Profiler in this [link](https://drive.google.com/open?id=1NsXEfw_6AQdG_1VwXSDlr490muKGKHM9). I think 70 MB native size should not be problem for a mobile device. You can see running points by looking  trash icons."]}]