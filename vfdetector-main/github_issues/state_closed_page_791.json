[{"number": 29813, "title": "Fix tflite_runtime pip package", "body": "Standalone pip package of tflite was broken at least since interpreter_wrapper refactoring.\r\nThis commit adds the missing file to the python extension sources.\r\nIt also fixes an invalid import in the `tflite_runtime/lite/__init__.py` file created by `build_pip_package.sh`, relates to #23082\r\n\r\nNot sure how to add tests for this...", "comments": ["@mayeut  Could you please resolve the conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29812, "title": "Revert forward horizon change, since Nov 1 was the tested horizon in RC1", "body": "https://github.com/tensorflow/tensorflow/pull/29712/commits/a08fad8977354f4efb040ad432fc10d823797ee5 is the correct fix for the issue described in b/129336724#151", "comments": ["Sorry, which changes exactly do we want to incorporate by moving this date?\r\n\r\nI know for example that @rmlarsen's  AddV2 change can go in but the gradients-of-log/exp/sqrt change should not go in, etc.", "I'm not sure what forward compat changes it would incorporate but as Nov 1 was the horizon tested in RC1 and there were no reported issues (other than fused_batch_norm_v3), it's safer to keep it consistent with RC1.\r\n\r\ncl/252672618 was the original reason the horizon was moved from Nov 1 to Sept 10 in https://github.com/tensorflow/tensorflow/pull/29764\r\n\r\nThis PR reverts that and https://github.com/tensorflow/tensorflow/pull/29888 moves the forward compat dates in cl/252672618 past Nov 1."]}, {"number": 29811, "title": "Update correct post_training_integer_quant colab and github url", "body": "Signed-off-by: jefby <jef199006@gmail.com>", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29811) for more info**.\n\n<!-- need_sender_cla -->", "Check out this pull request on ReviewNB: https://app.reviewnb.com/tensorflow/tensorflow/pull/29811 \n\n Visit www.reviewnb.com to know how we simplify your Jupyter Notebook workflows.", "I enable cla", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29811) for more info**.\n\n<!-- ok -->"]}, {"number": 29810, "title": "[ROCm] Adding + Removing  no_rocm tag from Python unit tests", "body": "This commit\r\n * adds the `no_rocm` tag to python unit tests that are currently failing with the `--config=rocm` build\r\n * removes the `no_rocm` tag from python unit tests that are now passing with the `--config=rocm` build\r\n\r\n-------------------------------------------------\r\n\r\n@tatianashp @whchung @parallelo ", "comments": ["@deven-amd could you please resolve conflicts.", "@rthadur \r\n\r\n> @deven-amd could you please resolve conflicts.\r\n\r\ndone.\r\n\r\n"]}, {"number": 29809, "title": "[ROCm] Adding a `test.is_built_with_rocm` python utility routine", "body": "This commit adds a `test.is_built_with_rocm` python utility routine, which will return true/false based on whether TF was built with ROCm support enables (i.e. with `--config=rocm` )\r\n\r\nThis routine will be used in subsequent PRs to make ROCm specific changes to certain unit tests. The nature of these changes will be to primarily disable certain subtests, which test features that are not yet supported on the ROCm platform (for e.g. complex datatypes, 3D pooling, etc)\r\n\r\nA `test.is_built_with_gpu_support` routine is also being added, which is essentially \"is_built_with_cuda() or is_built_with_rocm()\".\r\n\r\nThe other changes in this PR are ROCm implementation specific.\r\n\r\n-------------------------------------------\r\n\r\n@tatianashp @whchung @parallelo ", "comments": ["@chsigg gentle ping...this PR seems to have gotten stuck in merge pipe.", "how do I go about identifying and fixing the \"py_lint\" errors? \r\nIs there a `clang-format` equivalent I should be running on the python source code changes?\r\nthanks", "@deven-amd please fix the lint error below : \r\n`FAIL: Found 1 non-whitelited pylint errors:\r\ntensorflow/python/framework/test_util.py:1382: [C0301(line-too-long), ] Line too long (81/80)`", "> @deven-amd please fix the lint error below :\r\n> `FAIL: Found 1 non-whitelited pylint errors: tensorflow/python/framework/test_util.py:1382: [C0301(line-too-long), ] Line too long (81/80)`\r\n\r\n@rthadur done.\r\n\r\nI also fixed the failure in the `//tensorflow/python:build_info_test ` test. That required adding some more code, and re-arranging the PR into different commits.\r\n\r\n@chsigg , please re-review. thanks\r\n\r\n", "@rthadur @chsigg \r\n\r\npushed out two more changes\r\n1. updated the `gen_build_info.py` file to remove the lint errors (ran it through `yapf`, hopefully it fixed all lint issues)\r\n2. updated test goldens for the `//tensorflow/tools/api/tests:api_compatibility_test`. This is to account for the addition of the two new routines `is_built_with_rocm` and `is_built_with_gpu_support` to the API\r\n\r\nplease re-review and approve.\r\n\r\nthanks\r\n", "@rthadur \r\n\r\nrunning `yapf` seems to have made things worse from the linter's point of view.\r\n\r\nHow do I go about running the `do_pylint` check locally, so that I do not run into failures at CI check time? \r\n\r\nThis PR is a blocker for several others. Those other PRs also involve updating python files, and my updates to them will run the risk of failing the `do_pylint` step too. Knowing how to run that step locally will be huge help. \r\n\r\nThanks\r\n\r\ndeven\r\n\r\n\r\n", "@deven-amd you can check the sanity logs here https://source.cloud.google.com/results/invocations/29049114-e768-46e8-8026-ae3e554a516c/log \r\nand alternatively you can run this command  locally `tensorflow/tools/ci_build/ci_sanity.sh` to run all the checks in the build.", "@rthadur thank you for instructions on how to run the link check locally.\r\nI have pushed out another commit with a reformatted version of `gen_build_info.py`...it now passes the lint check for me locally...keeping my fingers crossed that the same holds true for the actual CI run as well.\r\n\r\n@chsigg , please re-re-review and re-re-re-bless it :)\r\n\r\nthanks again", "any update on the review status of this PR?  ", "This PR includes an API change, which has a separate process that I'm not familiar with. I've asked internally what the next steps are. Thank you for the patience.", "@chsigg thank you for the update. \r\n\r\nAs usual, we really appreciate all the work you have (and continue to) put in, in getting our PRs merged. \r\n\r\nthanks again\r\n\r\ndeven", "Removing the API review tag since it was reviewed internally and the API change was approved.", "@chsigg can you please approve the PR", "This PR has been merged in commit d0814e1."]}, {"number": 29808, "title": "FeatureColumn - implement many-to-one categorical column", "body": "- TensorFlow version (you are using): 2.0.0b0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, categorical column class numbers can be assigned sequentially based on a list or one-input-per-line file. It would be useful to support situations where groups of possible input values are known to be equivalent, and so should have the same class number. This feature could be implemented via a `categorical_column_with_vocabulary_dict` function that accepts a dictionary mapping input values to class numbers, rather than a list or file.\r\n\r\n**Will this change the current api? How?**\r\nYes; it would add a `categorical_column_with_vocabulary_dict` function.\r\n\r\n**Who will benefit with this feature?**\r\nPeople using non-normalized input data where multiple possible inputs can be equivalent will benefit.\r\n\r\n**Any Other info.**", "comments": ["@novog,\r\nSorry for the delayed response. The documentation of [Estimators](https://www.tensorflow.org/guide/estimator) states:\r\n\r\n> Warning: Estimators are not recommended for new code. Estimators run v1.Session-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities. See the migration guide for details.\r\n\r\nCan you please let us know if this **`Feature`** is still relevant? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29807, "title": "Hide hwloc symbols in libtensorflow_framework.so", "body": "Currently, `hwloc_*` symbols are being exported by libtensorflow_framework.so, which could conflict with `hwloc_*` symbols used by MPI.\r\n\r\nFixes horovod/horovod#1123\r\ncc @gunan @byronyi ", "comments": ["@gunan, can this be cherry-picked into r1.14? \ud83d\ude44", "Ping @bananabowl; hope it is not too late.", "This will get into 1.14.1, which will be released soon. We don't yet have a timeline, but I wanted to update the PR."]}, {"number": 29806, "title": "TensorFlow Lite: undefined reference to `flatbuffers::ClassicLocale::instance_'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.13\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI was following the [instructions](https://www.tensorflow.org/lite/guide/build_rpi) to crosscompile TensorFlow Lite for my Raspberry Pi. I got the error messages showing the undefined reference to flatbuffers. \r\nI believe all the dependencies for Lite are downloaded by the `download_dependencies.sh` script, but I do not have Bazel installed. Is Bazel necessary for compiling TensorFlow Lite?\r\n\r\n\r\n**Any other info / logs**\r\n```\r\n/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(while.o): In function `tflite::ops::custom::while_kernel::Init(TfLiteContext*, char const*, unsigned int)':\r\nwhile.cc:(.text+0x1648): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(audio_spectrogram.o): In function `tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned int)':\r\naudio_spectrogram.cc:(.text+0xe0c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(detection_postprocess.o): In function `tflite::ops::custom::detection_postprocess::Init(TfLiteContext*, char const*, unsigned int)':\r\ndetection_postprocess.cc:(.text+0x211c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(detection_postprocess.o): In function `flexbuffers::Reference::AsInt64() const':\r\ndetection_postprocess.cc:(.text._ZNK11flexbuffers9Reference7AsInt64Ev[_ZNK11flexbuffers9Reference7AsInt64Ev]+0x264): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(if.o): In function `tflite::ops::custom::if_kernel::Init(TfLiteContext*, char const*, unsigned int)':\r\nif.cc:(.text+0xf8c): undefined reference to `flatbuffers::ClassicLocale::instance_'\r\n/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/lib/libtensorflow-lite.a(mfcc.o):mfcc.cc:(.text+0x118c): more undefined references to `flatbuffers::ClassicLocale::instance_' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/lite/tools/make/Makefile:267: recipe for target '/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/minimal' failed\r\nmake: *** [/home/user/GitRepo/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/bin/minimal] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n```\r\n", "comments": ["Same issue here", "I realized that I made a stupid mistake: I forgot to change the current git branch to the correct version. Hope that could also solve your problem. @bhavints ", "Hi @YangJiao1996 ..\r\n\r\nI am trying to build tflite for aarch64 architecture. I faced some issues, and followed the thread [26731](https://github.com/tensorflow/tensorflow/issues/26731) to solve those one by one.\r\n\r\nBut still, I'm still facing the `undefined reference to 'flatbuffers::ClassicLocale::instance_'` issue in compiling minimal and benchmark binaries.\r\nSo, I am not sure whether the generated static library is functional or not.\r\n\r\nCould you please update on what branch you found the issue fixed ..?!\r\nMany Thanks in advance ..! :)", "Same problem here, but for armv7.\r\n\r\nSeems like new version of flatbuffers donesn't have pre-generated lib, and makefile doesn't build it.\r\nLet me check deeper.\r\n\r\nPS: library is broken too.", "@mohan-barathi \r\nI was using the r1.13 branch for cross-compiling.\r\n@T-Troll \r\nI also tried to compile locally on my armv7 device. After switching to r1.13 branch, I did not get the errors mentioned in this thread. (Other errors did occur, but I did not dive into it since I already have the library built.)", "I made a patch in Makefile for this -\r\n\r\n1. add `$(wildcard tensorflow/lite/tools/make/downloads/flatbuffers/src/util.cpp)` at ther end of CORE_CC_ALL_SRC\r\n2. change LIB_OBJS and BENCHMARK_OBJS to `$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(TF_LITE_CC_SRCS)))))` and `\r\n$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(BENCHMARK_SRCS)))))`\r\n3. add `$(OBJDIR)%.o: %.cpp\r\n        @mkdir -p $(dir $@)\r\n        $(CXX) $(CXXFLAGS) $(INCLUDES) -c $< -o $@` near other OBJDIR declarations.\r\n\r\nNow it builds and works.", "@T-Troll I am also trying to build TensorFlow Lite and am running into the same issue. I attempted to make the changed you suggested but am getting an error regarding `patsubst`. Could you share your edited Makefile?", "Hi @T-Troll ..!\r\n\r\nI am currently trying the change you suggested, for aarch64.\r\nI think there is an extra trailing ` )` at the end of 2 `patsubst` expressions you mentioned in change 2.\r\n\r\n@PeterVanNostrand ... Did you remove that trailing `)` and try ..?!", "@mohan-barathi Thanks for the reply. I thought that might have been the problem, but I found a different workaround. If you checkout branch [r1.13](https://github.com/tensorflow/tensorflow/tree/r1.13) from TensorFlow instead of the master branch then the build scripts work as is. I just successfully cross-compiled TFLite for the RPi3B", "@PeterVanNostrand ... Good that you can build it. But I am trying for aarch64 architecture, and there is no script for that in 1.13 branch. \ud83d\ude1e \r\n\r\n@T-Troll ... After making the changes you have suggested, i am facing the issue\r\n```\r\naarch64-linux-gnu-gcc: error: armv8-a: No such file or directory\r\n<builtin>: recipe for target 'tensorflow/lite/c/c_api_internal.o' failed\r\n```\r\n\r\nand when i checked with `./build_rpi_lib.sh`, a similar error occured:\r\n```\r\narm-linux-gnueabihf-gcc: error: armv7l: No such file or directory\r\narm-linux-gnueabihf-g++: error: armv7l: No such file or directory\r\n<builtin>: recipe for target 'tensorflow/lite/c/c_api_internal.o' failed\r\n```\r\nProbably i am doing something wrong when editing the `Makefile` as you suggested.\r\nCan you please provide the working Makefile via a gist or something .?!\r\n\r\nMany Thanks in advance ...! :)", "Similar libflatbuffers error trying cross compile in a HyperV Debian Linux for the RPI3B+. I already tried with r2.0 and r1.13 with same issue.\r\n\r\nIn my case, latest error message after several suggested fixes is:\r\n\r\n```\r\n//usr/local/lib/libflatbuffers.a: error adding symbols: File format not recognized\r\ncollect2: error: ld returned 1 exit status\r\n```\r\nI previously installed flatbuffers lib with:\r\n```\r\ngit clone https://github.com/google/flatbuffers.git\r\ncd flatbuffers\r\napt-get install cmake\r\ncmake -G \"Unix Makefiles\"\r\nmake\r\nmake install\r\n```\r\nand also updated LIBS at `tensorflow/lite/tools/make/Makefile`:\r\n```\r\nLIBS := \\\r\n+-libflatbuffers \\\r\n-lstdc++ \\\r\n-lpthread \\\r\n-lm \\\r\n-ldl \\\r\n-lrt\r\n```\r\nDid I miss anything? Looks like lib was not built for RPI3+", "Some points. I build TF lite for custom ARM board, so i use armel, not armhf.\r\nHere are my Makefile - https://yadi.sk/d/KDQHxzSb-y5u5Q\r\n\r\nPS: don't forget to run `make clean` after you change makefile!", "Looks like many people are still having problems in building the library, so I reopened this issue for further discussion. Please let me know when we have solutions for all platforms mentioned in this thread. Thank you! @T-Troll ", "> In my case, latest error message after several suggested fixes is:\r\n> //usr/local/lib/libflatbuffers.a: error adding symbols: File format not recognized\r\n> Did I miss anything? Looks like lib was not built for RPI3+\r\n\r\nIt's a bit heavy way, libflatbuffers not required by tflite, but seems like old hack (see in Makefile) didn't work yet. But it should be ok, if you build it for the same arch as well.\r\n\r\nYou error tells you didn't - i got it before in case of lib made for different architecture.", "I partially solved this issue by getting the build_aarch64.sh script from the master branch. I also had to change a few things in the makefile (like building zlib from source and also adding -ldl to my library opts).", "Hi @T-Troll, thanks for the hack. I checked that your makefile added the \"flatbuffers\" line below to CORE_CC_ALL_SRCS assignment. Then, I\u00b4ll keep trying.\r\n\r\n`$(wildcard tensorflow/lite/tools/make/downloads/flatbuffers/src/util.cpp)`\r\n\r\nBy the way, the regular Tensorflow is installing smoothly at RPI3B+:\r\n```\r\nsudo apt-get update\r\nsudo apt install libatlas-base-dev\r\npip3 install tensorflow\r\n```\r\n", "Same issue here, Debian 10 trying to build for a Pi. Thanks @T-Troll! The three edits below resolved the issue.\r\n\r\n> I made a patch in Makefile for this -\r\n> \r\n>     1. add `$(wildcard tensorflow/lite/tools/make/downloads/flatbuffers/src/util.cpp)` at ther end of CORE_CC_ALL_SRC\r\n> \r\n>     2. change LIB_OBJS and BENCHMARK_OBJS to `$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(TF_LITE_CC_SRCS)))))` and ` $(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(BENCHMARK_SRCS)))))`\r\n> \r\n>     3. add `$(OBJDIR)%.o: %.cpp @mkdir -p $(dir $@) $(CXX) $(CXXFLAGS) $(INCLUDES) -c $< -o $@` near other OBJDIR declarations.\r\n> \r\n> \r\n> Now it builds and works.", "> Same issue here, Debian 10 trying to build for a Pi. Thanks @T-Troll! The three edits below resolved the issue.\r\n> \r\n> > I made a patch in Makefile for this -\r\n> > ```\r\n> > 1. add `$(wildcard tensorflow/lite/tools/make/downloads/flatbuffers/src/util.cpp)` at ther end of CORE_CC_ALL_SRC\r\n> > \r\n> > 2. change LIB_OBJS and BENCHMARK_OBJS to `$(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(TF_LITE_CC_SRCS)))))` and ` $(patsubst %.cc,%.o,$(patsubst %.c,%.o,$(patsubst %.cpp,%.o,$(BENCHMARK_SRCS)))))`\r\n> > \r\n> > 3. add `$(OBJDIR)%.o: %.cpp @mkdir -p $(dir $@) $(CXX) $(CXXFLAGS) $(INCLUDES) -c $< -o $@` near other OBJDIR declarations.\r\n> > ```\r\n> > \r\n> > \r\n> > Now it builds and works.\r\n\r\nHi, I have the same use case but when adding those modifications, I ran into \r\n```\r\narm-linux-gnueabihf-g++: error: armv7l: No such file or directory\r\narm-linux-gnueabihf-gcc: error: armv7l: No such file or directory\r\n```\r\nlike @mohan-barathi.\r\nDid you faced something similar?\r\n\r\nEDIT: sorry I fucked up when modifying the makefile. It works now. Thank you very much for the help!", "> ```\r\n> arm-linux-gnueabihf-g++: error: armv7l: No such file or directory\r\n> arm-linux-gnueabihf-gcc: error: armv7l: No such file or directory\r\n> ```\r\n\r\nYes, it belongs to mess in makefile or in /targets/rpi_makefile.inc\r\n", "can someone attach the makefile please? I'm having trouble implementing the changes and the link for the makefile that has been posted does not work for me", "> can someone attach the makefile please? I'm having trouble implementing the changes and the link for the makefile that has been posted does not work for me\r\n\r\nOk, this one for you -https://mega.nz/#!eYVSzaII!cJbqTnJa2QYotKpJtnFnLl9gaIl4z9rnAizmOn_yQQ8 (but i don't know the country Yandex is blocked, you are the first). I will not use google drive, dropbox or any other global-spy services.", "Thank you! @T-Troll it finally works for me :)", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29806\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29806\">No</a>\n", "@jvishnuvardhan this issue is still in the codebase. I'm not knowledgeable enough about the TF codebase to know whether this solution is a hacky workaround or a clean solution \u2014 but I don't think the issue should be closed. Think we need a PR.", "@covertg I am opening it. Are you or anyone interested in creating a PR? Thanks!", "The only discussible hack here is adding utils.cpp, the rest is just support for .cpp file handling.\r\nI can't push it directly, let me check about request.\r\n\r\nUPD: i can't push into repo, so i can't create PR.", "> > can someone attach the makefile please? I'm having trouble implementing the changes and the link for the makefile that has been posted does not work for me\r\n> \r\n> Ok, this one for you -https://mega.nz/#!eYVSzaII!cJbqTnJa2QYotKpJtnFnLl9gaIl4z9rnAizmOn_yQQ8 (but i don't know the country Yandex is blocked, you are the first). I will not use google drive, dropbox or any other global-spy services.\r\n\r\nThanks, it works", "Hi @T-Troll ... \r\nThanks for the update. I have added the workarounds mentioned in issue [26731](https://github.com/tensorflow/tensorflow/issues/26731) along with your changes to the Makefile.\r\n\r\nNow I am successful in building the library for aarch64 architecture.\r\n[Gist of the new Makefile for aarch64](https://gist.github.com/mohan-barathi/538c45a77cd8531fb9f4367dd2e0cd1a)", "It looks like this is working now, so closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29806\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29806\">No</a>\n", "> @PeterVanNostrand ... Good that you can build it. But I am trying for aarch64 architecture, and there is no script for that in 1.13 branch. \r\n> \r\n> @T-Troll ... After making the changes you have suggested, i am facing the issue\r\n> \r\n> ```\r\n> aarch64-linux-gnu-gcc: error: armv8-a: No such file or directory\r\n> <builtin>: recipe for target 'tensorflow/lite/c/c_api_internal.o' failed\r\n> ```\r\n> \r\n> and when i checked with `./build_rpi_lib.sh`, a similar error occured:\r\n> \r\n> ```\r\n> arm-linux-gnueabihf-gcc: error: armv7l: No such file or directory\r\n> arm-linux-gnueabihf-g++: error: armv7l: No such file or directory\r\n> <builtin>: recipe for target 'tensorflow/lite/c/c_api_internal.o' failed\r\n> ```\r\n> \r\n> Probably i am doing something wrong when editing the `Makefile` as you suggested.\r\n> Can you please provide the working Makefile via a gist or something .?!\r\n> \r\n> Many Thanks in advance ...! :)\r\n\r\nI try build it in termux on Samsung S5e Devices\r\nfound this error too...\r\nso I do this:\r\n\r\nfirst, type this:\r\n`aarch64-linux- ` and press TAB button three times, to view your compiler command.\r\nmine is `aarch64-linux-android-g++-8`\r\n\r\nthen just edit the file build_aarch64_lib.sh\r\n`nano build_aarch64_lib.sh`\r\n\r\nyou will see this line\r\n\r\n`make -j4 VERBOSE=1 TARGET=aarch64 -C \"${TENSORFLOW_DIR}\" -f tensorflow/lite/tools/make/MakeFile`\r\n\r\nchange it with:\r\n\r\n`make -j4 VERBOSE=1 TARGET=aarch64-linux-android-g++-8 -C \"${TENSORFLOW_DIR}\" -f tensorflow/lite/tools/make/MakeFile`\r\n\r\nbut...\r\n\r\nhow to install it?\r\n\r\ni want to make it as python module"]}, {"number": 29805, "title": "Refactor {Skip, Take, & Tensor} DatasetOps", "body": "This PR refactors `SkipDatasetOp`, `TakeDatasetOp`, and `TensorDatasetOp`.\r\n\r\ncc: @jsimsa ", "comments": []}, {"number": 29804, "title": "Inconsistency between names in the conv 1d operations and no support for 'causal' padding", "body": "only tf.keras.layers.Conv1D supports padding 'causal', and it is very important for time series research, however in\r\n\r\ntf.python.ops.conv1d\r\ninput: (batch_size, dim1size, dim2size, ... , channels_in)\r\nfilters: (filter_size, channels_in, channels_out)\r\n\r\nAlso this function behaves as expected, setting a op node according to the math and predicting an output Tensor of predictable shape.\r\n\r\ntf.keras.layers.Conv1D\r\ninput: (batch_size, steps, input_dim)\r\nfilter: size of 1D filter\r\noutput: (batch_size, new_steps, filters) \r\n\r\nThese are non usual names for the mathematical definition of convolution. Is \"steps\" the size of the time series ? new_steps the relation of the filter size with the input_dim ? are filters the channel output size ? \r\n\r\nSeems incompatible with the former that follows closely the mathematical definition and also expands for multi-channel like the 2D case. Also, is desirable to use causal padding with tf.python.ops.conv1d, without having to add extraneous functions by hand to the code.\r\n\r\nthank you, pls have your appreciation.\r\n\r\n", "comments": ["Can you please help us to understand  tf.python.ops.conv1d  refers to which module or function ,since the mentioned function name looks related to tf.nn.conv1d & also please provide Tensorflow version.Thanks", "Exactly. However in tf.python.ops.nn_ops are where those convs are defined. It exports as nn.conv1d\r\nIt export 2 times the same thing, one with deprecation warning. \r\n\r\nNow i dont know where daheck* ive seem this:\r\n input: (batch_size, dim1size, dim2size, ... , channels_in)\r\n\r\nBut it does make sense for RMN/EEG and other biomedical datasets to do conv1d in data with more than one dimension.\r\n\r\nStill resides the difference between the names, which is confusing.\r\nTF version 1.13.01, conda-forge py27_0 build \r\n\r\n* sorry ive been sick and on fever, almost unable to code and read code.", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}, {"number": 29803, "title": "Follow up the while signature change.", "body": " - Assert that the input and output resources are in the same order to make the\r\n   expected signature of the xla computation more straightforward.", "comments": ["@sanjoy, this is the (minor) cleanup I promised you a while ago. Please help to take a look.\r\n\r\n@jlebar @hawkinsp ", "Understood. Let me see if I can place it in a better place.", "Done. Please help to take a look.\r\n\r\nAlthough I think it should not affect the change, I don't fully understand the purpose of the token inputs/outputs. How are they generated, stack access ops?\r\n\r\n", "> Done. Please help to take a look.\r\n> \r\n> Although I think it should not affect the change, I don't fully understand the purpose of the token inputs/outputs. How are they generated, stack access ops?\r\n\r\nThere are some internal users for this, but I'm not sure if there are any external uses.\r\n\r\nRoughly: `TOKEN` values let us sequence side effecting operations like `outfeed` and `infeed` in XLA, and we might need to thread these values through function calls, functional while loops etc.", "Revised accordingly.\r\n\r\nThanks for the explanations on `token`.\r\n", "This is merged by the commit: `5205ef9` , so closing this PR"]}, {"number": 29802, "title": "1.14.0 cherrypick request: Update tensorboard dependency to 1.14.x", "body": "This cherrypick is required to depend on a version of TensorBoard that\r\nis compatible with TensorFlow 1.14.x, such that `tensorboard`(1) starts\r\nsuccessfully.\r\n\r\nPiperOrigin-RevId: 253251054", "comments": []}, {"number": 29801, "title": "Keras Callbacks documentation for on_train_batch_end vs the actual code of ModelCheckpoint", "body": "## URL(s) with the issue:\r\n\r\n[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/Callback#on_train_batch_end](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/callbacks/Callback#on_train_batch_end)\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Parameters defined\r\n\r\nThe parameter section of `on_train_batch_end` reads, in part,\r\n\r\n> `logs`: dict. Metric results for this batch\r\n\r\nSo the `logs` argument is expected to only contain metrics when this method is called. This is what I assumed when I coded my own custom training function.\r\n\r\nThis is different than the `logs` argument which is expected by `on_train_batch_begin` for example, which is:\r\n\r\n> `logs`: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch.\r\n\r\nNow if we look at [the code](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/callbacks.py) of `ModelCheckpoint`, we see that is has an `on_batch_end` method (which is called by `on_train_batch_end`) which calls the `size` key of `logs`. This is line 950:\r\n`self._samples_seen_since_last_saving += logs.get('size', 1)`\r\n\r\nSo the documentation of `on_train_batch_end` is not correct as to what is the dict expected by the method. And this has an impact when we want to create custom training functions that work well with Keras Callbacks.", "comments": ["@durandg12 I think the document was updated since `TF2.0` and I cannot see that issue (self._samples_seen_since_last_saving += logs.get('size', 1)) anymore.  \r\n\r\nPlease check the recent code in `master` branch https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L572-L880\r\n\r\nCan you please verify once and close the issue if this was resolved for you. ", "`ModelCheckpoint` no longer has a `on_batch_end` method and its `on_train_batch_end` method don't use any key of `logs`. So it seems like the issue is indeed closed."]}, {"number": 29800, "title": "Keras Dropout layer behaviour in test mode seems to be \"do nothing\" instead of weight scaling inference rule", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip install\r\n- TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the expected behavior**\r\n\r\nThe weight scaling inference rule is a rule described in [the deep learning book by Goodfellow et al., section 7.12, page 260](https://www.deeplearningbook.org/contents/regularization.html) which consists in, in test or predict mode, multiplying the weights by 1 minus the dropout rate, \"to balance for the fact that more units are active than at training time\", to quote [the TF 2 tutorial about overfitting, section \"Add dropout\"](https://www.tensorflow.org/beta/tutorials/keras/overfit_and_underfit), implying that the rule is applied to the Keras `Dropout` layer.\r\n\r\n**Describe the current behavior**\r\n\r\nI have found no trace of this scaling down of the weights in the code of the `Dropout` Keras layer. In [the actual code](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/core.py), we can see for example, at the end of the `call` method,\r\n```\r\noutput = tf_utils.smart_cond(training,\r\n    dropped_inputs,\r\n    lambda: array_ops.identity(inputs))\r\n```\r\nso if `training == False`, the `Dropout` is just the identity.\r\n\r\nMy first thought was that I was missing something that was hidden elsewhere, maybe in the `fit`, `evaluate` and `predict` method of class `Model`. But then I read [the transformer tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer#evaluate) where custom `evaluate` and `translate` functions are used instead of the methods of class `Model`, along with some `Dropout` layers. And it seems to confirm that calling a `Dropout` layer with `training == False` does not implement the weight scaling inference rule.\r\n\r\n**Dropout as a Wrapper for other layers ?**\r\n\r\nOn the other hand, as a `Dropout` layer is not linked to any set of weights, it seems difficult to implement the weight scaling inference rule as is. Shouldn't `Dropout` be a subclass of `Wrapper` instead of `Layer`, allowing it to be feeded with a layer with some weights, and then applying the dropout to the outputs of this layer in training mode, while scaling down its weights in test and predict mode ?\r\n\r\n", "comments": ["After reading [the documentation](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/nn_ops.py) of `tf.python.ops.nn_ops.dropout_v2`, it seems that the weight scaling inference is indeed implemented, in the form of a scaling up of the non-zero inputs at training time.\r\n\r\nFor input x_i, what `dropout_v2` does is either set it to 0, with probability r (r is the dropout rate), either set it to x_i / (1 - r), with probability 1 - r. So the expected value of the output is x_i in training mode and so is the expected value of the `Dropout` layer in testing or predict mode, since it is the identity.\r\n\r\nAs presentend in Goodfellow et al., the rule consists in setting x_i either to 0, either to x_i in training mode, hence an expected value of (1 - r) * x_i. Thus the need to multiply x_i by (1 - r) in testing and predict mode, to keep the same expected value.\r\n\r\nSo the `Dropout` layer and the `dropout_v2` do indeed implement the weight scaling inference (up to a multiplying factor but that doesn't matter) and I am closing this issue."]}, {"number": 29799, "title": "Could we have F1 Score and F-Scores in TF 2.0?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.0 alpha\r\n- Are you willing to contribute it (Yes/No): Unlikely. I am going to try to build a workaround, but I've never contributed to TF and probably wouldn't be able to figure out the right way to put together code you'd want in master...I will post any useful code I come up with though if allowed.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to be able to do 'model.compile(..., metrics=['F1'])\r\nOr even 'model.compile(..., metrics=['FX']) for an arbitrary float X.\r\nI'm guessing we don't currently have it because it's not in Keras?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nF1 score and F-scores are relatively common in evaluating classification algorithms.\r\n\r\n**Any Other info.**\r\nI see a related request, seemingly for TF 1.x, in \r\nhttps://github.com/tensorflow/tensorflow/issues/28207\r\nbut I don't know if it went anywhere.\r\n\r\nhttps://en.wikipedia.org/wiki/F1_score\r\n", "comments": ["OK. I built a couple Metric classes on top of Precision and Recall. It should at least allow\r\nmetrics=[metrics.FScore(F=2)...].\r\n\r\nSince this issue arose from work I have to go through a paperwork chain before I'm allowed to post anything. Should take about a month. If this is not irrelevant by then I'll comment back to ask:\r\n\r\n1. What branch to do a pull request on (Just r2.0?)\r\n2. Whether this can go in metrics.py, or if it would have to live somewhere else by account of not being part of Keras.", "Could you share your implementation for the F1 score?", "As I said...it will be hopefully about a month.", "just a quick glimpse, since f1 == dice according to (https://en.wikipedia.org/wiki/F1_score)\r\nAnd the formula of dice is easier than the formula of  f1\r\nSo here is  a base sample code of f1-score for image segmentation:\r\n```python\r\n@tf.function\r\ndef f1(y_true, y_pred):\r\n    y_true = K.flatten(y_true)\r\n    y_pred = K.flatten(y_pred)\r\n    return 2 * (K.sum(y_true * y_pred)+ K.epsilon()) / (K.sum(y_true) + K.sum(y_pred) + K.epsilon())\r\n```", "Here is an implementation for tf.keras with tf 1.13 \r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.keras.utils.generic_utils import to_list\r\nfrom tensorflow.python.keras import metrics\r\n\r\nclass F1Score(metrics.Metric):\r\n  \"\"\"Computes the f1_score of the predictions with respect to the labels.\r\n\r\n  For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [1, 0, 1, 1]\r\n  then the f1_score value is 0.66. If the weights were specified as\r\n  [0, 0, 1, 0] then the precision value would be 1.\r\n\r\n  The metric creates three local variables, `true_positives`, `false_positives` and\r\n  `false_negatives` that are used to compute the f1 score.\r\n  This value is ultimately returned as `f1_score`, an operation that\r\n  simply computes 2* `precision`*`recall` /(`precision` + `recall`).\r\n  Where `precision` is ...\r\n  and `recall` is ...\r\n\r\n  If `sample_weight` is `None`, weights default to 1.\r\n  Use `sample_weight` of 0 to mask values.\r\n\r\n  Usage: ...\r\n  \"\"\"\r\n\r\n  def __init__(self, thresholds=None, name=None, dtype=None):\r\n    \"\"\"Creates a `F1Score` instance.\r\n\r\n    Args:\r\n      thresholds: (Optional) Defaults to 0.5. A float value or a python\r\n        list/tuple of float threshold values in [0, 1]. A threshold is compared\r\n        with prediction values to determine the truth value of predictions\r\n        (i.e., above the threshold is `true`, below is `false`). One metric\r\n        value is generated for each threshold value.\r\n      name: (Optional) string name of the metric instance.\r\n      dtype: (Optional) data type of the metric result.\r\n    \"\"\"\r\n    super(F1Score, self).__init__(name=name, dtype=dtype)\r\n    self.thresholds = metrics._parse_init_thresholds(\r\n        thresholds, default_threshold=0.5)\r\n    self.tp = self.add_weight(\r\n        'true_positives',\r\n        shape=(len(self.thresholds),),\r\n        initializer=metrics.init_ops.zeros_initializer)\r\n    self.fp = self.add_weight(\r\n        'false_positives',\r\n        shape=(len(self.thresholds),),\r\n        initializer=metrics.init_ops.zeros_initializer)\r\n    self.fn = self.add_weight(\r\n        'false_negatives',\r\n        shape=(len(self.thresholds),),\r\n        initializer=metrics.init_ops.zeros_initializer)\r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    \"\"\"Accumulates true positive, false positive and false negative statistics.\r\n\r\n    Args:\r\n      y_true: The ground truth values.\r\n      y_pred: The predicted values.\r\n      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\r\n        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\r\n        be broadcastable to `y_true`.\r\n\r\n    Returns:\r\n      Update op.\r\n    \"\"\"\r\n    return metrics._update_confusion_matrix_variables({\r\n        metrics._ConfusionMatrix.TRUE_POSITIVES: self.tp,\r\n        metrics._ConfusionMatrix.FALSE_POSITIVES: self.fp,\r\n        metrics._ConfusionMatrix.FALSE_NEGATIVES: self.fn\r\n    }, y_true, y_pred, self.thresholds, sample_weight)\r\n\r\n  def result(self):\r\n    precision = math_ops.div_no_nan(self.tp, self.tp + self.fp)\r\n    recall = math_ops.div_no_nan(self.tp, self.tp + self.fn)\r\n    numerator = math_ops.multiply(precision, recall)\r\n    denominator = math_ops.add(precision, recall)\r\n    frac = math_ops.div_no_nan(numerator, denominator)\r\n    result = math_ops.multiply(tf.constant(2.), frac)\r\n\r\n    return result[0] if len(self.thresholds) == 1 else result\r\n\r\n  def reset_states(self):\r\n    num_thresholds = len(to_list(self.thresholds))\r\n    for v in self.variables:\r\n      K.set_value(v, np.zeros((num_thresholds,)))\r\n```\r\n\r\nMetrics have changed a bit, there are now `top_k` and `class_id` parameters that would have to be implemented, and theres also the method `get_config` to implement. \r\n\r\nI can free some time and make a pull request if the approach in the code above is validated.  ", "This would be a good candidate for the AddOns repository (https://github.com/tensorflow/addons) and based on how widely it is being used we can move it to TensorFlow core.", "Please feel free to send a PR to the add ons repository.", "This \"kind of\" works for me (I wanted to have class_id in f1)\r\n\r\nBut it throws a warning that I don't know how to parse:\r\n\r\n> WARNING: Entity <bound method F1Score.update_state of <INS_model.F1Score object at 0x00000181D733C588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method F1Score.update_state of <INS_model.F1Score object at 0x00000181D733C588>>: AttributeError: module 'gast' has no attribute 'Str'\r\n\r\n```\r\nfrom tensorflow.python.ops import init_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.keras.metrics import Metric\r\nfrom tensorflow.python.keras.utils import metrics_utils\r\nfrom tensorflow.python.keras.utils.generic_utils import to_list\r\nfrom tensorflow.python.keras import backend as K\r\nclass F1Score(Metric):\r\n  \"\"\"Computes the F1 of the predictions with respect to the labels.\r\n  The metric creates two local variables, `true_positives` and `false_positives`\r\n  that are used to compute the precision. This value is ultimately returned as\r\n  `precision`, an idempotent operation that simply divides `true_positives`\r\n  by the sum of `true_positives` and `false_positives`.\r\n  If `sample_weight` is `None`, weights default to 1.\r\n  Use `sample_weight` of 0 to mask values.\r\n  If `top_k` is set, we'll calculate precision as how often on average a class\r\n  among the top-k classes with the highest predicted values of a batch entry is\r\n  correct and can be found in the label for that entry.\r\n  If `class_id` is specified, we calculate precision by considering only the\r\n  entries in the batch for which `class_id` is above the threshold and/or in the\r\n  top-k highest predictions, and computing the fraction of them for which\r\n  `class_id` is indeed a correct label.\r\n  Usage:\r\n  ```python\r\n  m = tf.keras.metrics.F1Score()\r\n  m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\r\n  print('Final result: ', m.result().numpy())\r\n  ```\r\n  Usage with tf.keras API:\r\n  ```python\r\n  model = tf.keras.Model(inputs, outputs)\r\n  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.F1Score()])\r\n  ```\r\n  \"\"\"\r\n\r\n  def __init__(self,\r\n               thresholds=None,\r\n               top_k=None,\r\n               class_id=None,\r\n               name=None,\r\n               dtype=None):\r\n    \"\"\"Creates a `F1Score` instance.\r\n    Args:\r\n      thresholds: (Optional) A float value or a python list/tuple of float\r\n        threshold values in [0, 1]. A threshold is compared with prediction\r\n        values to determine the truth value of predictions (i.e., above the\r\n        threshold is `true`, below is `false`). One metric value is generated\r\n        for each threshold value. If neither thresholds nor top_k are set, the\r\n        default is to calculate precision with `thresholds=0.5`.\r\n      top_k: (Optional) Unset by default. An int value specifying the top-k\r\n        predictions to consider when calculating precision.\r\n      class_id: (Optional) Integer class ID for which we want binary metrics.\r\n        This must be in the half-open interval `[0, num_classes)`, where\r\n        `num_classes` is the last dimension of predictions.\r\n      name: (Optional) string name of the metric instance.\r\n      dtype: (Optional) data type of the metric result.\r\n    \"\"\"\r\n    super(F1Score, self).__init__(name=name, dtype=dtype)\r\n    self.init_thresholds = thresholds\r\n    self.top_k = top_k\r\n    self.class_id = class_id\r\n\r\n    default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\r\n    self.thresholds = metrics_utils.parse_init_thresholds(\r\n        thresholds, default_threshold=default_threshold)\r\n    self.true_positives = self.add_weight(\r\n        'true_positives',\r\n        shape=(len(self.thresholds),),\r\n        initializer=init_ops.zeros_initializer)\r\n    self.false_positives = self.add_weight(\r\n        'false_positives',\r\n        shape=(len(self.thresholds),),\r\n        initializer=init_ops.zeros_initializer)\r\n    self.false_negatives = self.add_weight(\r\n        'false_negatives',\r\n        shape=(len(self.thresholds),),\r\n        initializer=init_ops.zeros_initializer)    \r\n\r\n  def update_state(self, y_true, y_pred, sample_weight=None):\r\n    \"\"\"Accumulates true positive and false positive statistics.\r\n    Args:\r\n      y_true: The ground truth values, with the same dimensions as `y_pred`.\r\n        Will be cast to `bool`.\r\n      y_pred: The predicted values. Each element must be in the range `[0, 1]`.\r\n      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\r\n        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\r\n        be broadcastable to `y_true`.\r\n    Returns:\r\n      Update op.\r\n    \"\"\"\r\n    metrics_utils.update_confusion_matrix_variables(\r\n        {\r\n            metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\r\n            metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\r\n            metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives\r\n        },\r\n        y_true,\r\n        y_pred,\r\n        thresholds=self.thresholds,\r\n        top_k=self.top_k,\r\n        class_id=self.class_id,\r\n        sample_weight=sample_weight)\r\n\r\n  def result(self):\r\n    precision = math_ops.div_no_nan(self.true_positives,\r\n                                 self.true_positives + self.false_positives)\r\n    recall = math_ops.div_no_nan(self.true_positives,\r\n                                 self.true_positives + self.false_negatives)\r\n    result = 2*math_ops.div_no_nan(math_ops.multiply(precision, recall),\r\n                                   math_ops.add(precision, recall))\r\n    return result[0] if len(self.thresholds) == 1 else result\r\n\r\n  def reset_states(self):\r\n    num_thresholds = len(to_list(self.thresholds))\r\n    K.batch_set_value(\r\n        [(v, np.zeros((num_thresholds,))) for v in self.variables])\r\n\r\n  def get_config(self):\r\n    config = {\r\n        'thresholds': self.init_thresholds,\r\n        'top_k': self.top_k,\r\n        'class_id': self.class_id\r\n    }\r\n    base_config = super(F1Score, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n```", "My solution is very outdated. \r\nF1 score has been implemented in the AddOns repository (https://github.com/tensorflow/addons).\r\n\r\nHave a look at `tensorflow_addons.metrics.F1Score`", "F1 in addons seems to have an ugly bug: https://github.com/tensorflow/addons/issues/825"]}, {"number": 29798, "title": "AttributeError: 'Sequential' object has no attribute 'target_tensors'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0-rc0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nExample works fine with tensorflow 1.13.1.\r\n\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\r\ntrain_images = train_images.reshape((60000, 28, 28, 1))\r\ntest_images = test_images.reshape((10000, 28, 28, 1))\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10, activation='softmax'))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(model,\r\n          strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n          tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n        ))\r\ntpu_model.fit(train_images, train_labels, epochs=5)\r\n\r\nWhen using tensorflow 1.14.0-rc1, I got following error:\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-73ec4b0d99a3> in <module>()\r\n      1 tpu_model = tf.contrib.tpu.keras_to_tpu_model(model,\r\n      2           strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n----> 3           tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n      4         ))\r\n      5 tpu_model.fit(train_images, train_labels, epochs=5)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py in __init__(self, cpu_model, strategy)\r\n   1426           self._cpu_model.sample_weight_mode,\r\n   1427           self._cpu_model._compile_weighted_metrics,\r\n-> 1428           self._cpu_model.target_tensors,\r\n   1429       )\r\n   1430 \r\n\r\nAttributeError: 'Sequential' object has no attribute 'target_tensors'", "comments": ["Facing the same error while running code on Colab. <br>\r\nAny solution anyone ? ", "I managed to get things running by moving to the new TPU API:\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\nwith strategy.scope():\r\n    model.compile(...)\r\n    model.fit(...)\r\n\r\nI also had to pass directly some tf.data tensor to the fit function rather than a generator function.\r\n\r\nHowever I think it is very unfortunate to break APIs during minor version updates. Also while training an epoch seems to run much faster than before, the accuracy stays much lower as before even after many epochs; so there seem to be some big change in the way it works compared to before.", "It is also needed to create the model with strategy.scope():\r\n```\r\nwith strategy.scope():\r\n  #Create model, eg.:\r\n  #model=tf.keras.models.Sequential([ .. \r\n  model.compile(...)\r\n  model.fit(...)\r\n```", "> It is also needed to create the model with strategy.scope():\r\n> \r\n> ```\r\n> with strategy.scope():\r\n>   #Create model, eg.:\r\n>   #model=tf.keras.models.Sequential([ .. \r\n>   model.compile(...)\r\n>   model.fit(...)\r\n> ```\r\n\r\nI tried this, but then it gives a new error saying ''Session e9459ae257feda74 is not found.''. \r\nHere is my code:\r\n```\r\nimport tensorflow as tf\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  base = InceptionV3(weights='imagenet', input_shape=(200,150,3), include_top=False)\r\n  model = Sequential()\r\n  model.add(base)\r\n  model.add(Flatten())\r\n  model.add(Dense(128))#, activation='relu'))\r\n  model.add(Activation('relu'))\r\n  model.add(Dropout(0.5))\r\n  model.add(Dense(128))#, activation='relu'))\r\n  model.add(Dropout(0.5))\r\n  model.add(Activation('relu'))\r\n  model.add(Dense(100))#, activation='relu'))\r\n  model.add(Activation('relu'))\r\n  model.add(Dense(12))#, activation='sigmoid'))\r\n  model.add(Activation('sigmoid'))\r\n\r\n  model.summary()\r\n  model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['acc'])\r\n  model.fit(trainX, trainy, epochs=5, batch_size=32, validation_split=0.08)\r\n```\r\nWhat should I do ? ", "I managed to run with TPUs and tf 1.14.0-rc1 but the accuracy was really poor compared to tensorflow 1.13.1. Even GPU did not give good results. So I switched back to using CPU for the moment and accuracy is much better (and performance not much degraded actually so I am a bit confused).", "Looks like the issue is been resolved , however you are facing low accuracy on 1.14-rc1 as compared to 1.13.1. Will it be possible to give us some information on the same or else can we close this one. Please let us know. Thanks!", "Thank you for your feedback.\r\nThe issue to me is that tensorflow API compatibility was broken between 1.13 and 1.14 concerning TPU usage. I understand that part is experimental so that may be acceptable to you and you can close the ticket.\r\n\r\nConcerning the performance and quality degradation when using TPU with tf 1.14, I will open a separate ticket with full info.\r\nThanks", "@ssable : Thanks for the response. Happy to help.", "@kaushil24, there are imports missing in your code\r\n", "> > It is also needed to create the model with strategy.scope():\r\n> > ```\r\n> > with strategy.scope():\r\n> >   #Create model, eg.:\r\n> >   #model=tf.keras.models.Sequential([ .. \r\n> >   model.compile(...)\r\n> >   model.fit(...)\r\n> > ```\r\n> \r\n> I tried this, but then it gives a new error saying ''Session e9459ae257feda74 is not found.''.\r\n> Here is my code:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n> tf.config.experimental_connect_to_host(resolver.master())\r\n> tf.tpu.experimental.initialize_tpu_system(resolver)\r\n> strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n> \r\n> with strategy.scope():\r\n>   base = InceptionV3(weights='imagenet', input_shape=(200,150,3), include_top=False)\r\n>   model = Sequential()\r\n>   model.add(base)\r\n>   model.add(Flatten())\r\n>   model.add(Dense(128))#, activation='relu'))\r\n>   model.add(Activation('relu'))\r\n>   model.add(Dropout(0.5))\r\n>   model.add(Dense(128))#, activation='relu'))\r\n>   model.add(Dropout(0.5))\r\n>   model.add(Activation('relu'))\r\n>   model.add(Dense(100))#, activation='relu'))\r\n>   model.add(Activation('relu'))\r\n>   model.add(Dense(12))#, activation='sigmoid'))\r\n>   model.add(Activation('sigmoid'))\r\n> \r\n>   model.summary()\r\n>   model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['acc'])\r\n>   model.fit(trainX, trainy, epochs=5, batch_size=32, validation_split=0.08)\r\n> ```\r\n> \r\n> What should I do ?\r\n\r\nI have the same problem. @kaushil24 How many samples do you have in `trainX`? I found out that if I increase too much the number of samples or the size of the images, `model.fit` gives this error. \r\n\r\nI tried this is colab (tf = 1.14.0-rc1).\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\ntpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\nbatchSize = 64\r\nnumImages = 1000 * batchSize\r\nwidth = 50\r\nheight = 50\r\n\r\nprint(\"Img\", str(numImages))\r\n\r\nrandomImages = np.random.rand(numImages,width,height,3).astype('float32') \r\nrandomLabels = np.random.randint(2, size=numImages).astype('int32') \r\n\r\n#model\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  inputs = tf.keras.layers.Input(shape=(width, height, 3))\r\n  outputLayer = inputs\r\n  for i in range(0,3):\r\n    outputLayer = tf.keras.layers.Conv2D(filters=64, kernel_size = 3, padding = 'valid')(outputLayer)\r\n    outputLayer = tf.keras.layers.BatchNormalization()(outputLayer)\r\n    outputLayer = tf.keras.layers.ReLU()(outputLayer)\r\n  outputLayer = tf.keras.layers.Flatten()(outputLayer)\r\n  outputLayer = tf.keras.layers.Dense(2)(outputLayer)\r\n  outputLayer = tf.keras.layers.BatchNormalization()(outputLayer)\r\n  outputLayer = tf.keras.layers.Softmax()(outputLayer)\r\n\r\n  model = tf.keras.Model(inputs = inputs, outputs = outputLayer)\r\n  \r\n  model.compile(\r\n      optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),\r\n      loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n      metrics=['sparse_categorical_accuracy']\r\n    )\r\n```\r\n\r\nWith `numImages = 64000`, `width = 50`, `height = 50` the code works fine. If I set `numImages = 80000` or `width = 55` and `height = 55`, I get this error:\r\n\r\n<details>\r\n   <summary> Error </summary>\r\n\r\n   ```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1340       return self._call_tf_sessionrun(\r\n-> 1341           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1342 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1428         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1429         run_metadata)\r\n   1430 \r\n\r\nAbortedError: Session 290fcb1d6cd9689a is not found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAbortedError                              Traceback (most recent call last)\r\n<ipython-input-3-b914521409eb> in <module>()\r\n     67                 y = randomLabels,\r\n     68                 batch_size = batchSize,\r\n---> 69                 epochs = 10,\r\n     70               )\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    647             steps_per_epoch=steps_per_epoch,\r\n    648             validation_steps=validation_steps,\r\n--> 649             validation_freq=validation_freq)\r\n    650 \r\n    651     batch_size = self._validate_or_infer_batch_size(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit_distributed(model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\r\n     82       validation_split=validation_split,\r\n     83       shuffle=shuffle,\r\n---> 84       epochs=epochs)\r\n     85   if not distributed_training_utils.is_distributing_by_cloning(model):\r\n     86     with model._distribution_strategy.scope():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2380 \r\n   2381         ds = strategy.extended.experimental_make_numpy_dataset(in_tuple,\r\n-> 2382                                                                session=session)\r\n   2383         if shuffle:\r\n   2384           # We want a buffer size that is larger than the batch size provided by\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py in experimental_make_numpy_dataset(self, numpy_input, session)\r\n   1431     \"\"\"\r\n   1432     _require_cross_replica_or_default_context_extended(self)\r\n-> 1433     return self._experimental_make_numpy_dataset(numpy_input, session=session)\r\n   1434 \r\n   1435   def _experimental_make_numpy_dataset(self, numpy_input, session):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in _experimental_make_numpy_dataset(self, numpy_input, session)\r\n    345     return numpy_dataset.one_host_numpy_dataset(\r\n    346         numpy_input, numpy_dataset.SingleDevice(self._host_device),\r\n--> 347         session)\r\n    348 \r\n    349   def _experimental_distribute_dataset(self, dataset):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/numpy_dataset.py in one_host_numpy_dataset(numpy_input, colocate_with, session)\r\n     86                       for i in numpy_flat)\r\n     87   for v, i in zip(vars_flat, numpy_flat):\r\n---> 88     init_var_from_numpy(v, i, session)\r\n     89   vars_nested = nest.pack_sequence_as(numpy_input, vars_flat)\r\n     90   return dataset_ops.Dataset.from_tensor_slices(vars_nested)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/numpy_dataset.py in init_var_from_numpy(input_var, numpy_input, session)\r\n     70           start_placeholder: start,\r\n     71           end_placeholder: end,\r\n---> 72           slice_placeholder: numpy_input[start:end]})\r\n     73       start = end\r\n     74 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    948     try:\r\n    949       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 950                          run_metadata_ptr)\r\n    951       if run_metadata:\r\n    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1172       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1173                              feed_dict_tensor, options, run_metadata)\r\n   1174     else:\r\n   1175       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1348     if handle is None:\r\n   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1350                            run_metadata)\r\n   1351     else:\r\n   1352       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nAbortedError: Session 290fcb1d6cd9689a is not found.\r\n```\r\n</details>", "model.fit (at least when used with the above distribution strategy) adds nodes to the graph every time it is called, causing about a 100x slowdown. So, is there a workaround?", "> fit_generator is not supported for models compiled with tf.distribute.Strategy.\r\n\r\nAny ideas how to get around it?", "@hadaev8 I got around it by using tf estimators.\r\n I made a feature request a couple of weeks back,let us see if it gets implemented", "@capilano \r\nBut need callbacks.\r\nI cant pass dataset object, right?\r\nI get assert without error message.", "@hadaev8 you can pass dataset object. I am actually passing tfrecords dataset into the estimator. \r\nKeras callbacks, doubtful but maybe it's possible (not sure) . Do you need callbacks for early stopping and learning rate schedule or for something else?", "Okay get this error, maybe because I have multi output model\r\n\r\nEdit: this is about test data, it work if i remove.\r\n\r\n```\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\r\ntf.config.experimental_connect_to_host(resolver.master())\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\nwith strategy.scope():\r\n    model = get_model(le.classes_, 4)\r\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(clipnorm=1.), metrics=['accuracy'])\r\nbatch_size=128\r\nh = model.fit(X_train, [y_train[:,0], y_train[:,1], y_train[:,2], y_train[:,3]],\r\n                validation_data=(X_test, [y_test[:,0], y_test[:,1], y_test[:,2], y_test[:,3]]),\r\n                steps_per_epoch=len(y_train)//batch_size-1,\r\n                validation_steps=len(y_test)//batch_size-1,\r\n                verbose=1,\r\n                epochs=100,\r\n                batch_size=batch_size,\r\n                callbacks=[\r\n                    keras.callbacks.TerminateOnNaN(),\r\n                    keras.callbacks.EarlyStopping(patience=5, verbose=1),\r\n                    keras.callbacks.ReduceLROnPlateau(factor=0.9, patience=0, verbose=1, min_lr=1e-6),\r\n                    keras.callbacks.ModelCheckpoint('model', period=0),\r\n                ])\r\n```\r\n\r\n```\r\nInternalError                             Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1355     try:\r\n-> 1356       return fn(*args)\r\n   1357     except errors.OpError as e:\r\n\r\n11 frames\r\n\r\nInternalError: From /job:worker/replica:0/task:0:\r\nCompilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_parameter_binding.cc:130) dynamic_dimension.dimension < ShapeUtil::GetSubshape( entry->parameter_instruction(dynamic_dimension.parameter_num) ->shape(), dynamic_dimension.parameter_index) .rank() \r\n\tTPU compilation failed\r\n\t [[{{node TPUReplicateMetadata_1}}]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nInternalError: From /job:worker/replica:0/task:0:\r\nCompilation failure: RET_CHECK failure (third_party/tensorflow/compiler/xla/service/dynamic_parameter_binding.cc:130) dynamic_dimension.dimension < ShapeUtil::GetSubshape( entry->parameter_instruction(dynamic_dimension.parameter_num) ->shape(), dynamic_dimension.parameter_index) .rank() \r\n\tTPU compilation failed\r\n\t [[node TPUReplicateMetadata_1 (defined at <ipython-input-8-7930ebedf468>:20) ]]\r\n\r\nOriginal stack trace for 'TPUReplicateMetadata_1':\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\r\n    self._run_callback(self._callbacks.popleft())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\r\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-8-7930ebedf468>\", line 20, in <module>\r\n    keras.callbacks.ModelCheckpoint('model', period=0),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 649, in fit\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 128, in fit_distributed\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 446, in experimental_tpu_fit_loop\r\n    callbacks=callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 520, in experimental_tpu_test_loop\r\n    _test_step_fn, args=(test_input_data,))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 249, in experimental_run_v2\r\n    return _tpu_run(self, fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py\", line 196, in _tpu_run\r\n    maximum_shapes=maximum_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py\", line 592, in replicate\r\n    maximum_shapes=maximum_shapes)[1]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py\", line 854, in split_compile_and_replicate\r\n    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_tpu_ops.py\", line 6039, in tpu_replicate_metadata\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```", "@hadaev8 \r\n1. Use tensorflow.keras layers and tensorflow.keras.callbacks,do not use any keras layers directly or functions. Even during the import replace all the keras imports with tensorflow .keras. might need to slightly change your code, because there are some subtle differences between tf.keras and keras , but for most cases they have exact same functionality.\r\n2. You cannot pass a dataset object to fit function. I used tensorflow estimators to train my model as a workaround. as far as I can see , you should be able to train yr model using tensorflow estimators,but I do not know if that's what u are doing \r\n3. If your dataset fits in RAM I.e if X_train is already available to you,and if you are not doing data augmentation, then you can directly pass the data to model .fit. the problem is you cannot call model.fit in a for loop because that causes a 50x slowdown probably dye to distribute strategy, I created a bug issue 2 weeks ago, and I think it's still not resolved.", "@capilano yes, i have everything from tf.keras. I tested it in 2 cases: with data augmentation and not. Second break on test phase. Sad to see keras and tpu api is broken. And why this issue is closed?", "@hadaev8 the issue that I opened is still open. https://github.com/tensorflow/tensorflow/issues/30162\r\nP.S: Use tf.keras for optimizers and callbacks also I think tf estimators are the only way to train right now. The only problem is callbacks are not directly supported there, so I guess you will have to find a way to  do LR decay and early stopping. You can save checkpoints though, that is supported", "AttributeError: 'Sequential' object has no attribute 'version'\r\n\r\ni have this type of error ...please anyone tell me the solution\r\n"]}, {"number": 29797, "title": "No library found under: /usr/lib64/stubs/libcuda.so", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 30\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master \r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia Quadro 2200M 4G\r\n\r\nAfter problems with manually installed nvidia drivers, I've switched to using the nvidia packages from negativo17 (https://github.com/negativo17/nvidia-driver), and I get the below error when building TF. As per this issue\r\n\r\nhttps://github.com/negativo17/cuda/issues/18\r\n\r\nthere are no stubs in the packages. Does the build absolutely need the stubs, or is there a workaround (e.g., some local modification I could make to the `bazel` config)?\r\n\r\nMany thanks in advance!!\r\n\r\n```\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\r\nINFO: Reading rc options for 'build' from /home/key/code/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from /home/key/code/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/key/anaconda3/envs/0613/bin/python --action_env PYTHON_LIB_PATH=/home/key/anaconda3/envs/0613/lib/python3.7/site-packages --python_path=/home/key/anaconda3/envs/0613/bin/python --config=xla --action_env CUDA_TOOLKIT_PATH=/usr --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env GCC_HOST_COMPILER_PATH=/home/key/MYBUILDS/GCC7/bin/gcc --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:xla in file /home/key/code/tensorflow/.tf_configure.bazelrc: --define with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /home/key/code/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/key/code/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file /home/key/code/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file /home/key/code/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/key/code/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Call stack for the definition of repository 'local_config_cuda' which is a cuda_configure (rule definition at /home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl:1268:18):\r\n - /home/key/code/tensorflow/tensorflow/workspace.bzl:63:5\r\n - /home/key/code/tensorflow/WORKSPACE:94:1\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1033, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 608, in _find_libs\r\n\t\t_find_cuda_lib(\"cuda\", repository_ctx, cpu_value, (cu...), ...)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 589, in _find_cuda_lib\r\n\t\tfind_lib(repository_ctx, [(\"%s/%s\" % (based...))], ...)))\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 566, in find_lib\r\n\t\tauto_configure_fail((\"No library found under: \" + \",...)))\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 325, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: No library found under: /usr/lib64/stubs/libcuda.so\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1033, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 608, in _find_libs\r\n\t\t_find_cuda_lib(\"cuda\", repository_ctx, cpu_value, (cu...), ...)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 589, in _find_cuda_lib\r\n\t\tfind_lib(repository_ctx, [(\"%s/%s\" % (based...))], ...)))\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 566, in find_lib\r\n\t\tauto_configure_fail((\"No library found under: \" + \",...)))\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 325, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: No library found under: /usr/lib64/stubs/libcuda.so\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1266\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1033, in _create_local_cuda_repository\r\n\t\t_find_libs(repository_ctx, cuda_config)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 608, in _find_libs\r\n\t\t_find_cuda_lib(\"cuda\", repository_ctx, cpu_value, (cu...), ...)\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 589, in _find_cuda_lib\r\n\t\tfind_lib(repository_ctx, [(\"%s/%s\" % (based...))], ...)))\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 566, in find_lib\r\n\t\tauto_configure_fail((\"No library found under: \" + \",...)))\r\n\tFile \"/home/key/code/tensorflow/third_party/gpus/cuda_configure.bzl\", line 325, in auto_configure_fail\r\n\t\tfail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: No library found under: /usr/lib64/stubs/libcuda.so\r\n\r\n```\r\n\r\n", "comments": ["Hi, just a quick update.\r\n\r\nI've tried setting up a `/usr/lib64/stubs` directory and symlinking `libcuda.so` there, but I can't say if that's sufficient as now the build fails with \r\n\r\n```\r\nERROR: /home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/external/local_config_cuda/cuda/BUILD:1399:1: Executing genrule @local_config_cuda//cuda:cuda-bin failed (Exit 1)\r\ncp: cannot open '/usr/bin/./sudoedit' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./locate' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./sudo' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./sudoreplay' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./chfn' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./chsh' for reading: Permission denied\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\nIt's true that these do not have read permissions, as assumed by the `cuda-bin` rule.\r\nI'll try changing permissions (at least temporarily) to see how far that gets me in the build.\r\n\r\nIn the meantime, I'd be happy about any advice :-)\r\nThanks!\r\n", "Hi,\r\n\r\nit turned out that my build worked with just the 2 aforementioned changes:\r\n\r\n1. symlinking `libcuda.so` (only this single file) into a newly-created `stubs` dir\r\n2. making the above files other-readable\r\n\r\nPlease feel free to close the issue! I'd be very happy though to learn why the build does not use `libcuda.so` directly, but that `stub`?\r\nWould it make sense for the build config to check for existence of that directory (not necessarily the case on linux)?\r\n\r\nMany thanks!", "it is probably due to negativo drivers, but I am not 100% sure.\r\nIt may also be fedora, as we have never experimented with fedora before."]}, {"number": 29796, "title": "Reuse tensorflow::MakeUnique", "body": "for this\r\n```\r\nnamespace {\r\n// TODO(suharshs): Move this to a common location to allow other part of the\r\n// repo to use it.\r\ntemplate <typename T, typename... Args>\r\nstd::unique_ptr<T> MakeUnique(Args&&... args) {\r\n  return std::unique_ptr<T>(new T(std::forward<Args>(args)...));\r\n}\r\n}  // namespace\r\n```", "comments": []}, {"number": 29795, "title": "Protobuf converter for backwards compatibility", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.3.0 / 1.13.1\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSituation: I need to develop a CNN for a system which only has tensorflow 1.3.0. Since that version doesn't come with keras, I simply create a model using tensorflow 1.13.1 and save it as a protobuf file. Then, I import the protobuf file in tensorflow 1.3.0\r\n\r\nExpected result: I can import the protobuf file also in tensorflow 1.3.0, since I didn't use any new features\r\n\r\nActual result: I get an error similar to https://github.com/tensorflow/tensorflow/issues/17628\r\n\r\n**Proposed solution: Can we write a converter, which tries to translate protobufs file generated from newer tensorflow versions to ones which can be read in older versions?**\r\n\r\n**Will this change the current api? How?**\r\nThis could be an additional script, but would not change existing code\r\n\r\n**Who will benefit with this feature?**\r\nAnybody who has to develop models for systems running old tensorflow versions\r\n\r\n**Any Other info.**\r\nIn order to start one would need an overview of how the protobuf file generation (from graph_defs) has developed in each tensorflow version. Is there any such overview available?\r\n", "comments": ["@cgebbe Sorry for the delay in my response. There were huge improvements in TF. Why do you want to use TF1.3? Do you have any use case that requires TF1.3 which was not associated with keras? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "The issue came up because Udacity only features tensorflow 1.3.0 in their final capstone project of their self-driving car nanodegree, see https://github.com/udacity/CarND-Capstone/issues/276\r\n\r\nI thought that such situations may occur quite often. However, I don't have the issue anymore, so it can be closed."]}, {"number": 29794, "title": "Convert Keras to tflite", "body": "I tried converting my keras file to tflite file .But facing below issue \r\n\r\njava.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 30000 bytes and a ByteBuffer with 602112 bytes.\r\n\r\n**used below code in colab**\r\n\r\nfrom tensorflow.contrib import lite\r\nconverter = lite.TFLiteConverter.from_keras_model_file( 'plano.h5')\r\ntfmodel = converter.convert()\r\nopen (\"plano_tensor_colab.tflite\" , \"wb\") .write(tfmodel)\r\n\r\nNew to tensor flow .Please help how to convert keras to tensorflowlite.", "comments": ["This Code seems to work in colabs:\r\n```python\r\nimport tensorflow as tf\r\n# create model\r\nbase_model = tf.keras.applications.MobileNetV2(input_shape=(160, 160, 3),\r\n                                               include_top=False, \r\n                                               weights='imagenet')\r\n# compile model\r\nbase_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001), \r\n              loss='binary_crossentropy', \r\n              metrics=['accuracy'])\r\n# save keras model\r\nbase_model.save('plano.h5')\r\n\r\n# convert model\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file( 'plano.h5')\r\nconverter.allow_custom_ops=True\r\ntfmodel = converter.convert()\r\nopen (\"plano_tensor_colab.tflite\" , \"wb\") .write(tfmodel)\r\n```", "@KabirKwatra 's answers seems right. \r\n\r\nAlso the error seems to not be coming from the tflite converter?  If this is still an issue, could you provide more info on how to reproduce the error please? Thanks!", "@DurgaArunkumarSmitiv I think the issue is with input/output shape. Input layer's shape of model is not equal to the input shape of the input your the passing to the model", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I want to convert my keras model into a tf lite file.\r\nthis is my code\r\n\r\nimport tensorflow as tf\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file( r'/content/drive/My Drive/inceptionv3-transfer-learning__fine_tune.h5') # Your model's name\r\nmodel = converter.convert()\r\nfile = open( 'model.tflite' , 'wb' )\r\nfile.write( model )\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n\r\nwhat can I do?\r\n", "You have saved your model with input shape (None, None,3),but tflite wants some input shape. You have to change your Input layer of your model, say (299,299,3), then save as .h5 file and then convert that file to tflite.", "I already specified the image height and width to be 299. so I expected (299,299,3) as input \r\nhow  will I do that in the code?\r\n\r\n\r\ndef create_img_generator():\r\n    return ImageDataGenerator(\r\n    preprocessing_function = preprocess_input,\r\n    rotation_range=30,\r\n    width_shift_range = 0.2,\r\n    height_shift_range=0.2,\r\n    shear_range = 0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True)\r\n\r\nImage_width,Image_height = 299,299\r\nTraining_Epochs = 5\r\nBatch_Size = 32\r\nNumber_FC_Neurons = 1024\r\n\r\ntrain_dir = r'/content/tomato/train'\r\nvalidate_dir = r'/content/tomato/validate' \r\n\r\nnum_train_samples = get_num_files(train_dir)\r\nnum_classes = get_num_subfolders(train_dir)\r\nnum_validate_samples = get_num_files(validate_dir)\r\n\r\nnum_epoch = Training_Epochs\r\nbatch_size = Batch_Size\r\n\r\ntrain_image_gen = create_img_generator()\r\ntest_image_gen = create_img_generator()\r\n\r\ntrain_generator = train_image_gen.flow_from_directory(\r\ntrain_dir,\r\ntarget_size=(Image_width,Image_height),\r\nbatch_size=batch_size,\r\nseed=42\r\n)\r\n\r\nvalidation_generator = train_image_gen.flow_from_directory(\r\nvalidate_dir,\r\ntarget_size=(Image_width,Image_height),\r\nbatch_size=batch_size,\r\nseed=42\r\n)\r\nx=InceptionV3_base_model.output\r\nx= GlobalAveragePooling2D()(x)\r\nx=Dense(Number_FC_Neurons,activation='relu')(x)\r\npredictions = Dense(num_classes,activation='softmax')(x)\r\n\r\nmodel=Model(inputs=InceptionV3_base_model.input,outputs=predictions)\r\n\r\nhistory_transfer_learning = model.fit_generator(\r\n train_generator,\r\n epochs=num_epoch,\r\n steps_per_epoch = num_train_samples // batch_size,\r\n validation_data = validation_generator,\r\n validation_steps = num_validate_samples // batch_size,\r\n class_weight ='auto'\r\n )", "InceptionV3_base_model = keras.applications.inception_v3.InceptionV3(include_top=False,weights='imagenet', input_shape=(299,299,3))\r\n\r\nx=InceptionV3_base_model.output\r\nx= GlobalAveragePooling2D()(x)\r\nx=Dense(Number_FC_Neurons,activation='relu')(x)\r\npredictions = Dense(num_classes,activation='softmax')(x)\r\n\r\nmodel=Model(inputs=InceptionV3_base_model.input,outputs=predictions)\r\n"]}, {"number": 29793, "title": "latest stable tensorflow which is compatible with cuda 9", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.2\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to know which is the last known stable version which is compatible with cuda 9. Iwant to use lateset tensorflow code which has a bug fix but i am not able to do coz that is not compatible with cuda 9. I cant upgrade to cuda 10, it will break others old model architecture. \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["1.12.2 is the latest version", "Can we close the issue since the query is been answered. Let us know. Thanks!"]}, {"number": 29792, "title": "Tensorflow lite: micro_vision is missing person_detect_model_data.cc", "body": "Micro vision demo of the Tensorflow Lite is missing model file `person_detect.tflite` as well as `person_detect_model_data.cc` due to which it can't be compiled.\r\n\r\nTensorflow repo info:\r\n```\r\ncommit b5102aa41000068787f56a25c3fd48f7d9a80f42 (HEAD -> master, origin/master, origin/HEAD)\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Fri Jun 14 02:39:21 2019 -0700\r\n\r\n    Remove absl::bit_casts of non-trivially-copyable Eigen types, and replace them with a new, extensible xla::BitCast call. The latter does everything absl::bit_cast does, but additionally can be specialized to support non-trivial types.\r\n    \r\n    PiperOrigin-RevId: 253194104\r\n```\r\n", "comments": ["This should be fixed now, so closing. Could you let us know if not and we can reopen.", "Hi,\r\n\r\nI'm looking at the master branch (856af529) and can not find the .tflite model or .cc data file. Is there a different commit or branch that has these files?\r\n\r\nI'm looking at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_vision", "@nealjack You need to run makefile as it will be downloaded during build process.", "Where can I find the model files (.pb or .tflite etc) which have been used to generate person_detect_model_data.cc? @petewarden ", "I have the same question.", "They are currently only available as c++ source files.  I will have a look at adding the source model to the download.  In the meantime, a simple c++ method which dumps the array to file would re-create the tflite model.  You can find the grayscale c++ file [here](https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale.zip):  and the RGB model c++ file [here](https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data.tgz).", "@njeffrie, Thanks - actually for my workspace I have already reverse engineered the .tflite file as well as input data images (I couldn't clearly the face of the person in the picture. Is it you? :) ). IMO uploading .tflite makes sense for the use-case where one doesn't want to compiled alongwith the executable as my application already loading some resources from the storage device so placing model file(s) there makes sense.\r\n\r\nI will give shot to the RGB model too.", "@njeffrie Could you upload the .tflite model in the mean time?\r\nIt would be very beneficial for me to have the original (unquantized) model as well.\r\nIs there a possibility to get these files?\r\n\r\nThank you in advance :)", "@aakbar5 How did you reversely get the proper .tflite model?\r\nI found some scripts, but they didn't reproduce the model correctly since the activation functions were missing."]}, {"number": 29791, "title": "AttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0-rc1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nCode is running in Colab and was working fine with tensorflow 1.13.1.\r\nColab was updated to tensorflow 1.14.0-rc1 today apparently which is causing the regression.\r\nRunning tpu_model.fit for a TPU model that was generated with tf.contrib.tpu.keras_to_tpu_model, I get:\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-bbab49a84214> in <module>()\r\n     14 \r\n     15 print(datetime.now())\r\n---> 16 history = tpu_model.fit(train_dataset_fn, epochs=nb_epochs, steps_per_epoch=total_samples // batch_size, validation_data=val_dataset_fn, validation_steps=batch_size) # , verbose=2)\r\n     17 print(datetime.now())\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\r\n    118   callback_list.model.stop_training = False\r\n    119   # pylint: disable=protected-access\r\n--> 120   if callback_list.model._ckpt_saved_epoch is not None:\r\n    121     # The attribute `_ckpt_saved_epoch` is supposed to be None at the start of\r\n    122     # training (it should be made None at the end of successful multi-worker\r\n\r\nAttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'", "comments": ["@ssable In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "I tried to reproduce the issue with a minimal code snippet, but while doing that I found another bug #29798.\r\nOnce that other bug is resolved, the code provided there should raise this issue or I should be able to come up with a simple example how to reproduce.", "I have the same issue. This code should raise the error. Instead, in version 1.13.1 should work fine. I tried it in colab (1.14.0-rc1).\r\n\r\n```\r\n#!pip install -q tensorflow==1.13.1\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\nprint(tf.__version__)\r\n\r\ntpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\n\r\nrandomImages = np.random.rand(128,50,50,3)\r\nrandomLabels = np.random.randint(2, size=128)\r\n\r\n#model\r\ninputs = tf.keras.layers.Input(shape=(50, 50, 3))\r\noutputLayer = inputs\r\nfor i in range(0,3):\r\n  outputLayer = tf.keras.layers.Conv2D(filters=64, kernel_size = 3, padding = 'valid')(outputLayer)\r\n  outputLayer = tf.keras.layers.BatchNormalization()(outputLayer)\r\n  outputLayer = tf.keras.layers.ReLU()(outputLayer)\r\noutputLayer = tf.keras.layers.Flatten()(outputLayer)\r\noutputLayer = tf.keras.layers.Dense(2)(outputLayer)\r\n\r\nmodel = tf.keras.Model(inputs = inputs, outputs = outputLayer)\r\n\r\n#tpu_model \r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(\r\n    model,\r\n    strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n    tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address))\r\n  )\r\n\r\ntpu_model.compile(\r\n    optimizer=tf.train.AdamOptimizer(learning_rate = 0.0001),\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    metrics=['sparse_categorical_accuracy']\r\n  )\r\n\r\n\r\ntpu_model.fit(\r\n              x = randomImages,\r\n              y = randomLabels,\r\n              batch_size = 64,\r\n              epochs = 10\r\n            )\r\n```", "@giovannibaratta I tried reproducing the issue on colab with Tf 1.14.1 but I am getting different error, ValueError: Variable tpu_140309960546512//kernel/0 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?. \r\nIs this same error what you are getting?. Thanks!", "> @giovannibaratta I tried reproducing the issue on colab with Tf 1.14.1 but I am getting different error, ValueError: Variable tpu_140309960546512//kernel/0 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?.\r\n> Is this same error what you are getting?. Thanks!\r\n\r\nNo, I get this error : \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-54cc81aef34f> in <module>()\r\n     40               y = randomLabels,\r\n     41               batch_size = 64,\r\n---> 42               epochs = 10\r\n     43             )\r\n\r\n\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\r\n    118   callback_list.model.stop_training = False\r\n    119   # pylint: disable=protected-access\r\n--> 120   if callback_list.model._ckpt_saved_epoch is not None:\r\n    121     # The attribute `_ckpt_saved_epoch` is supposed to be None at the start of\r\n    122     # training (it should be made None at the end of successful multi-worker\r\n\r\nAttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'\r\n```\r\nThere are also some warnings (\"Keras support is now deprecated in support of TPU Strategy\") but it's not clear to me how I should migrate to the new distribution strategy.\r\n", "I get exactly the same error\r\n`%%time \r\nhistory=tpu_model.fit_generator(\r\n                           train_generator(),\r\n                           steps_per_epoch = 150,\r\n                           validation_data = test_generator(),\r\n                           validation_steps = 30,\r\n                           epochs=15,\r\n                           callbacks=[cp_callback]\r\n)`\r\n`---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-45-89e3eb7c573a> in <module>()\r\n----> 1 get_ipython().run_cell_magic('time', '', 'history=tpu_model.fit_generator(\\n                           train_generator(),\\n                           steps_per_epoch = 150,\\n                           validation_data = test_generator(),\\n                           validation_steps = 30,\\n                           epochs=15,\\n                           callbacks=[cp_callback]\\n)')\r\n\r\n5 frames\r\n</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60> in time(self, line, cell, local_ns)\r\n\r\n<timed exec> in <module>()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in configure_callbacks(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\r\n    118   callback_list.model.stop_training = False\r\n    119   # pylint: disable=protected-access\r\n--> 120   if callback_list.model._ckpt_saved_epoch is not None:\r\n    121     # The attribute `_ckpt_saved_epoch` is supposed to be None at the start of\r\n    122     # training (it should be made None at the end of successful multi-worker\r\n\r\nAttributeError: 'KerasTPUModel' object has no attribute '_ckpt_saved_epoch'`", "I could able to reproduce the issue on Colab with tensorflow 1.14.0-rc1. Thanks!", "Following the MNIST example (https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb#scrollTo=xLeZATVaNAnE) I was able to run the code in the new version.\r\n\r\n```\r\nresolver = tf.contrib.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.contrib.distribute.initialize_tpu_system(resolver)\r\nstrategy = tf.contrib.distribute.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n  #define model\r\n  #compile model\r\n  #fit model\r\n```\r\n\r\nNow, in my code (not the code posted above), I have an other issue related to `tf.keras.layers.concatenate` that seems to be not implemented, or at least this is what the error says.", "If I rollback to tensorflow 1.13.1 (!pip install tensorflow==1.13.1) my code (above) works in colaboratory, however it becomes very slow. In fact it is quicker to run on GPU", "@javicivit I am not completely sure since I never run code on TPU, but perhaps you should install tensorflow-gpu  (!pip install tensorflow-gpu==1.13.1)?", "> @javicivit I am not completely sure since I never run code on TPU, but perhaps you should install tensorflow-gpu (!pip install tensorflow-gpu==1.13.1)?\r\n\r\nNot if we are running on TPU, which is when this error arise. TPU uses `tensorflow `and not `tensorflow-gpu`", "By default the training loop for distribution strategy is running on the Colab host, not the TPU host. For larger models this isn't a problem, but for small models like MNIST, the latency between the 2 hosts can dominate the model execution time.\r\n\r\nI believe DistributionStrategy performance has improved in the upcoming TF 2.1 release as well. I'm closing this for now as I believe the original question is answered, feel free to re-open if I missed something.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29791\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29791\">No</a>\n"]}, {"number": 29790, "title": "Do you have a 3Ddata of crop_and_resize function implementation?", "body": "Do you have a 3Ddata of crop_and_resize function implementation? DATA of shape `[batch, depth, image_height, image_width, channel]`.  If not, are the crop_and_resize implementations of 2D detailed? As long as I know the implementation details of 2D, I can realize the application of 3D.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n"]}, {"number": 29789, "title": "Runtime parameters passed to input_fn for tf.estimator ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.10+ < 2\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, to my knowledge, if one uses `tf.estimator.TrainSpec` / `tf.estimator.EvalSpec` the argument \"`input_fn`\" takes a function for producing either an iterator or something that returns a tuple of `(features, labels)`. However there does not seem to be a way to add epoch based variation to the dataset. \r\n\r\nIt would be nice if TF would  automatically pass runtime parameters to this function e.g. via\r\n\r\n```python\r\ndef input_fn(files:list, params:dict, _runtime_params:dict=None):\r\n    # access to epoch via _runtime_params[\"epoch\"]\r\n``` \r\n\r\nWhile the use of epoch specific inputs during training will introduce a bottle neck it can be sometimes necessary.\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will not break existing code. It will simply allow the user to have epoch / step specific control over the input function (e.g. if they want to upsample an underrepresented class from the last 200 epochs)\r\n\r\n**Who will benefit with this feature?**\r\neveryone who uses estimators\r\n\r\n**Any Other info.**\r\n", "comments": ["Estimator does not make it easy to have different input fns based on context. @guptapriya -- is there a way to achieve this using the new 2.0 APIs?", "@karmel  my current solution isn't so much of a super solution... but maybe could be wrangled into one.... \r\n\r\n# context\r\nThe goal of the estimator API was to clearly separate the pre-processing pipeline, the data input pipeline, the training, and then inference.\r\n\r\nThe current estimator class however really only works as a scheduler for input, training/eval and inference. This forces the user to ensure the data is prepossessed thoroughly prior to the input function being called, or at the very least not require it to be called more than once. \r\n\r\nYet there are many use cases where were batch / epoch specific modifications could be beneficial (weighting the next epoch based off of sampling bias from previous epoch, batch specific softening of labels, etc). \r\n\r\n\r\n\r\n# putative solutions\r\n\r\n## hot fix and maybe easiest\r\nA \"hot-fix\" would be changing the default of behavior. Currently, if steps (or an early stopping hook) is not provided, the model will run-indefinitely. An addition `RunConfig` parameter `loop_on_end_of_iteration` (or a snazzier variable name)  could be used to tell the Estimator to essentially recall `self.train` (and hence the `input_fn`) when terminating after hitting the end of the original `input_fn`. Of course this would have to auto-add `warm_start_from` to the call, so that the estimator picks up where it left off.\r\n\r\n## maybe more in line with the api style\r\nRight now `input_fn` is expected to be a  function that return `(feature, label)` or a dataset thereof.  It could however also expect a function that returns the `input_fn`.\r\n\r\nThis would, however, still require an overhaul to the estimator source code, for as far as I can tell `input_fn` is only called once.\r\n\r\n\r\n## notes\r\nin either case above, auto-passing some basic info to the `input_fn` (e.g. global step, batch number, epoch, etc) would allow for some nice dynamic datasets.\r\n\r\nnotable cons of both of these include:\r\n- bottleneck now part of training\r\n- all data must be in memory at once\r\n\r\n\r\n# my meh workaround\r\nAt the moment, provided one has the luxury of disk space, is simply to precompute _n_  datasets and then have the `input_fn` uses those files appropriately (basically stack and treat as one large dataset)\r\n\r\n## notes\r\n\r\na pro to this is that if one is using tfrecords all the benefits of tf records are still there (albeit with the aforementioned inconsistencies in [making and parsing them](https://github.com/tensorflow/tensorflow/issues/23802))\r\n\r\na con is that if one is not using tfrecords to stream from the files then your efforts go to waste unless you have an equally abundant amount of ram\r\n\r\n\r\n# adjacent\r\nto my knowledge in tf 2.0 the keras API also adopts the same standard for the `input_fn` and also shares this pitfall. So maybe this feature helps bolster estimator apis for the many with  custom estimators that will be around for years to come as well as move tf.keras forward? \r\n", "@SumNeuron,\r\nSorry for the delayed response. In the **`Tensorflow Version 2.x`**, since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras/) and [tf.data](https://www.tensorflow.org/guide/data) predominantly and don't use [Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this Feature is still relevant? \r\n\r\nThanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29788, "title": "Install from pip, run test and results in \"illegal instruction\" for tensorflow-gpu", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOs 7.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip per https://www.tensorflow.org/install/pip\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:  cuda9.0 and cudnn7.0\r\n- GPU model and memory: Tesla P100\r\n\r\n**Describe the problem**\r\nwhen i  import tensorflow\uff0cthe error as follow\uff1a\r\nillegal instruction\r\ni don't know why\uff0cplease help me.\r\n", "comments": ["Which version of TF are you trying to use? \r\n\r\nSee #17411 , #19809 and https://stackoverflow.com/questions/49094597/illegal-instruction-core-dumped-after-running-import-tensorflow\r\n\r\nAlso maybe try running it in a `virtualenv` with Python 3.6 then `pip3 install tensorflow-gpu` or even 2.0 `tensorflow-gpu==2.0.0-beta1`. Check the full instructions on how to install with pip or docker [here](https://www.tensorflow.org/install/pip).\r\n\r\n", "@8bitmp3  thank you for your reply. emm, i us tf1.11. In fact,  i just want do some try on bert and i must upgrade tf. I use pip but the result is too bad. errors alway go on. \r\nso, i use conda, but another error happen. that's very terribel!! please save me~~", "> @8bitmp3 thank you for your reply. emm, i us tf1.11. In fact, i just want do some try on bert and i must upgrade tf. I use pip but the result is too bad. errors alway go on.\r\n> so, i use conda, but another error happen. that's very terribel!! please save me~~\r\n\r\nCan you include error message that you are getting. Thanks! ", "@gadagashwini  here is the detail\uff1a\r\ntensorflow.python.framework.errors_impl.InternalError: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_NOT_SUPPORTED: operation not supported", "@AlexYoung757 Just to verify, did you add the lib and bin to path? Thanks!", "@gadagashwini   yep! i am sure that i add the lib and bin to path. i can use the command: nvcc -v\r\nto verify \r\n", "TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets. This means on any CPU that do not have these instruction sets either CPU or GPU version of TF will fail to load. I suspect that your CPU model does not support AVX instruction set. Can you please confirm?. Thanks! ", "@ymodak thank you for your reply.  en i find that my cpu model does not support AVX instruction set. it only supports sse \u3001sse2 . \r\nso if  i want to install tensorflow 1.11 , what should i do. Maybe i must install from source code?", "(1) The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed stable TF version on it (currently running TF 1.14). Also you can use ```pip install``` to install any other preferred TF version if required.\r\nIt has added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. All you need is good internet connection and you are all set.\r\n(2) You have build TF from sources by changing CPU optimization flags.\r\n(3) Another option will be to install TF version 1.5 on your local machine.\r\n", "@ymodak . thanks , i have solved this problem. After modify the AVX instruction sets ,all the question has done.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 29787, "title": "fix bug of unable to link rdft2d", "body": "the implementation of function `rdft2d` in `tensorflow/lite/kernels/rfft2d.cc`  is in fftsg2d.cc, the Makefile here not add this c file", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29787) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29787) for more info**.\n\n<!-- ok -->"]}, {"number": 29786, "title": "Why does tensorflow occupy different gpu memory for the same inference process on different gpu card?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes, my code is similar to examples/label_image.cc in c++ with *.pb(frozen model)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):centos7.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: sorry, I dont't know\r\n- TensorFlow installed from (source or binary): source, just compile the c++ api library\r\n- TensorFlow version (use command below):v1.13.1\r\n- Python version:python2.7\r\n- Bazel version (if compiling from source):0.21.0\r\n- GCC/Compiler version (if compiling from source):4.8.5\r\n- CUDA/cuDNN version:10.0/7.5\r\n- GPU model and memory: p40(24G * 4), v100(24G*8)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n\r\nwhen I run the inference process (video action recognition with I3D, the input shape (32 * 224 * 224 * 3)) on different card, I got following infomation:\r\n\r\n(1) on p40, on device:0, it occupy 4235MiB memory(the available free memory is still about 6G), some other process have been existing on the same card;\r\n\r\n(2) on p40, on device:1, it occupy 8300MiB, only the I3D inference model is running on the card\r\n\r\n(3) on v100, on device:0, it only occupy about 1G memory,  only the I3D inference model is running on the card\r\n\r\n\r\nAnd where is the relative code with allocator in tensorflow ? \r\n\r\nAnd I want to know clearly the gpu memory allocator mechanism, than you very much!\r\n\r\nAnd I set allow_growth = true in my c++ inference program, and I set CUDA_VISIBLE_DEVICE to the current device id at each running!\r\n\r\nThank you!\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@idealboy the allocation logic is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/bfc_allocator.cc#L49). When we set allow_growth to True, it'll start by allocating 1MB and double the allocation size when running out of memory. One possibility is that for different GPU cards it picks different algorithms for the gpu ops which will require different amount of memory. I'm closing this one, feel free to reopen and/or file a new issue if you encounter further problems.\r\n\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29786\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29786\">No</a>\n"]}, {"number": 29785, "title": "Cannot build the code including stream_executor/rng.h on Windows", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 x64\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.14-rc0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):0.26.0\r\n\r\n**Describe the current behavior**\r\nI cannot build the code including stream_executor/rng.h on Visual Studio 2015 as following.\r\n```\r\ntensorflow/compiler/jit/graphcycles/graphcycles.cc(62): note: see reference to class template instantiation 'tensorflow::OrderedSet<tensorflow::int32>' being compiled\r\nERROR: D:/a/1/s/tensorflow/tensorflow/stream_executor/BUILD:102:1: C++ compilation of rule '//tensorflow/stream_executor:kernel' failed (Exit 2): cl.exe failed: error executing command \r\n  cd D:/a/1/b/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\VSSADM~1\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\VSSADM~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/stream_executor/_objs/kernel/kernel.obj /c tensorflow/stream_executor/kernel.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n.\\tensorflow/stream_executor/rng.h(66): error C2589: 'constant': illegal token on right side of '::'\r\n.\\tensorflow/stream_executor/rng.h(66): error C2059: syntax error: '::'\r\n.\\tensorflow/stream_executor/rng.h(72): error C2589: 'constant': illegal token on right side of '::'\r\n.\\tensorflow/stream_executor/rng.h(72): error C2059: syntax error: '::'\r\nTarget //tensorflow/compiler/aot:tfcompile failed to build\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nbazel build -c opt --color=yes --config=monolithic --verbose_failures //tensorflow/compiler/aot:tfcompile\r\n```\r\n\r\nIt seems that two odd newline character was mixed into [rng.h](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/stream_executor/rng.h#L64) as follows.\r\n```\r\n  virtual bool DoPopulateRandGaussian(Stream *stream, float mean, float stddev,\r\n                                      DeviceMemory<float> *v) {\r\n    LOG(ERROR)\r\n        << \"platform's random number generator does not support gaussian\";\r\n    return false;\r\n  }\r\n  virtual bool DoPopulateRandGaussian(Stream *stream, double mean,\r\n                                      double stddev, DeviceMemory<double> *v) {\r\n    LOG(ERROR)\r\n        << \"platform's random number generator does not support gaussian\";\r\n    return false;\r\n  }\r\n```\r\n\r\nThis issue is critical for us.\r\nWould you like to modify the code?", "comments": ["I have a theory. In `LOG(ERROR)`, ERROR is replaced by some windows-defined macro that's called ERROR. An easy fix would be moving these function implementations into the strean.cc file.\r\n\r\nBut I don't know how to build TF on windows to verify it. Passing onto @gunan ", "From what I can see, our continuous builds for windows are green with msvc 2015.\r\nAre you building TF together with something else?", "I prepared [CI environment with Azure DevOps](https://dev.azure.com/mlops/build-tfcompile/_build/results?buildId=181&view=logs&j=37ac93e3-95ad-598b-a5c9-c557853b15ab&t=eafe61f6-d619-5846-d967-e9cd72823209&l=764).\r\nAs for r1.14 branch, it seems to cause different compile error as follows.\r\n\r\n```\r\nERROR: D:/a/1/s/tensorflow/tensorflow/compiler/xla/service/BUILD:2276:1: C++ compilation of rule '//tensorflow/compiler/xla/service:hlo_execution_profile' failed (Exit 2): cl.exe failed: error executing command | \u00a0\r\n-- | --\r\n\u00a0 | .\\tensorflow/compiler/xla/service/hlo_cost_analysis.h(42): error C2131: expression did not evaluate to a constant | \u00a0\r\n\u00a0 | cd D:/a/1/b/execroot/org_tensorflow | \u00a0\r\n\u00a0 | SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt; | \u00a0\r\n\u00a0 | SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\windows\\system32\r\n```", "a-ha, I think the issue is you are trying to build XLA on windows.\r\nThis is still known to be broken. There may be some progress, but I cannot commit to any dates to get this working.\r\nHere is another relevant issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/30415", "I build it with msvc 2019 and get the same error. Should I uninstall the 2019 and reinstall the 2015?", "OK I can reproduce this on MSVC 2017", "With XLA turned OFF.", "If you are building `tensorflow/compiler/aot:tfcompile` you have to build XLA.\r\n\r\nThis actually should be resolved in master. XLA now can build on windows. I am trying it again, and will close the issue if the command reported on the original issue works. I am building on master right now\r\nBranches cut before february will now work. but I think 2.2 has all the fixes needed to build this target.", "Just successfully built this target:\r\n```\r\nTarget //tensorflow/compiler/aot:tfcompile up-to-date:\r\n  bazel-bin/tensorflow/compiler/aot/tfcompile.exe\r\nINFO: Elapsed time: 1660.510s, Critical Path: 276.99s\r\nINFO: 7933 processes: 7933 local.\r\nINFO: Build completed successfully, 8200 total actions\r\n```\r\nAs I mentioned, this will definitely not work with older branches. But branches after 2.2, and master branch, using visual studio 2019 you should be able to build this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29785\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29785\">No</a>\n"]}, {"number": 29784, "title": "Cannot use object loaded by tf.saved_model.load() to create Keras model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION='2.0.0-dev20190613'\r\nGIT_VERSION='v1.12.1-4034-gb81b902c37'\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nThe Keras code example in the documentation for the `tf.saved_model.load()` function raises an exception:\r\n\r\n```python\r\nmodel = tf.keras.Model(...)\r\ntf.saved_model.save(model, path)\r\nimported = tf.saved_model.load(path)\r\noutputs = imported(inputs)\r\n```\r\n\r\n**Describe the expected behavior**\r\nI expect it to work as advertised. ;-)\r\n\r\n**Code to reproduce the issue**\r\nHere's the full code (I tried to add as little as I could):\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\npath = \"my_saved_model\"\r\nX_train = np.random.rand(100, 5)\r\ny_train = np.random.rand(100, 1)\r\n\r\nmodel = keras.models.Sequential([keras.layers.Dense(1, input_shape=[5])])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nmodel.fit(X_train, y_train)\r\n\r\ntf.saved_model.save(model, path)\r\n\r\nimported = tf.saved_model.load(path)\r\n\r\ninputs = keras.layers.Input(shape=[5])\r\noutputs = imported(inputs) # Raises _SymbolicException (see stacktrace below) <<<!!!\r\nmodel = keras.Model(inputs=[inputs], outputs=[outputs])\r\n```\r\n\r\n**Stacktrace**\r\nHere's the full stacktrace:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     60                                                op_name, inputs, attrs,\r\n---> 61                                                num_outputs)\r\n     62   except core._NotOkStatusException as e:\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: input_1:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n_SymbolicException                        Traceback (most recent call last)\r\n<ipython-input-1-304bb0c5e2fc> in <module>\r\n     16\r\n     17 inputs = keras.layers.Input(shape=[5])\r\n---> 18 outputs = imported(inputs) # Raises _SymbolicException <<<<<<<<<<<<<<!!!\r\n     19 model = keras.Model(inputs=[inputs], outputs=[outputs])\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)\r\n    399\r\n    400 def _call_attribute(instance, *args, **kwargs):\r\n--> 401   return instance.__call__(*args, **kwargs)\r\n    402\r\n    403\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    431               *args, **kwds)\r\n    432       # If we did not create any variables the trace we have is good enough.\r\n--> 433       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    434\r\n    435     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    600          if isinstance(t, (ops.Tensor,\r\n    601                            resource_variable_ops.ResourceVariable))),\r\n--> 602         self.captured_inputs)\r\n    603\r\n    604   def _call_flat(self, args, captured_inputs):\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs)\r\n    682     # Only need to override the gradient in graph mode and when we have outputs.\r\n    683     if context.executing_eagerly() or not self.outputs:\r\n--> 684       outputs = self._inference_function.call(ctx, args)\r\n    685     else:\r\n    686       self._register_gradient()\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args)\r\n    451             attrs=(\"executor_type\", executor_type,\r\n    452                    \"config_proto\", config),\r\n--> 453             ctx=ctx)\r\n    454       # Replace empty list with None\r\n    455       outputs = outputs or None\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     68   except TypeError as e:\r\n     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n---> 70       raise core._SymbolicException\r\n     71     raise e\r\n     72   # pylint: enable=protected-access\r\n\r\n_SymbolicException:\r\n```\r\n\r\n**Other info**\r\nI can actually call the `imported` object with tensors, as long as I pass the `training` argument:\r\n\r\n```python\r\n>>> imported(tf.random.uniform([10, 5]), training=False)\r\n<tf.Tensor: id=780, shape=(10, 1), dtype=float32, numpy=\r\narray([[-0.7474083 ],\r\n       [-0.25788566],\r\n       [-0.48218375],\r\n       [-0.60376763],\r\n       [-1.4071536 ],\r\n       [-0.45902687],\r\n       [-0.07063864],\r\n       [-0.9497912 ],\r\n       [ 0.03344992],\r\n       [-0.33686087]], dtype=float32)>\r\n```\r\n\r\nHowever, I cannot pass it the `inputs` tensor:\r\n\r\n```\r\n>>> imported(inputs, training=False)\r\n...\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     68   except TypeError as e:\r\n     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n---> 70       raise core._SymbolicException\r\n     71     raise e\r\n     72   # pylint: enable=protected-access\r\n\r\n_SymbolicException:\r\n```", "comments": ["@ageron The [documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/load) in the TF website mentions that \r\n\r\n`The object returned by tf.saved_model.load is not a Keras object (i.e. doesn't have .fit, .predict, etc. methods). `\r\n\r\nAlso it was mentioned that\r\n\r\n`Use tf.keras.models.load_model to restore the Keras model.`\r\n\r\nSo, if you replace one line in your code, it works without any issues. \r\n\r\n```\r\n#imported = tf.saved_model.load(path)\r\nimported = tf.keras.models.load_model(path)\r\n```\r\n\r\nAs @k-w-w [mentioned](https://github.com/tensorflow/tensorflow/issues/28923#issuecomment-521027273)\r\n\r\n\r\n\r\n> The best practices can be summed up to:\r\n> If you are planning to load the model back into Keras, use the Keras APIs for saving and loading. If you're not planning on using the Keras APIs to further train the model, then tf.saved_model.save and tf.saved_model.load is sufficient.\r\n\r\n\r\n\r\nMay be we need to update docs to represent things clearly. Please let us know what you think? Please check [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d64dd0103fbf6a2cbb8ab4f2cf48cf9c/tf_29784.ipynb). Thanks!", "Indeed, everything works fine with `tf.keras.models.load_model(path)`, thanks @jvishnuvardhan.\r\n\r\nHowever, it's a bit odd that the function returned by `tf.saved_model.load(path)` cannot be used with Keras `Input` objects, since it can be used inside TF Functions:\r\n\r\n```python\r\n[...]\r\n>>> imported = tf.saved_model.load(path)\r\n>>> imported(tf.constant([[1., 2., 3., 4., 5.]]))\r\n<tf.Tensor: [...], numpy=array([[2.1844666]], dtype=float32)>\r\n>>> @tf.function\r\n... def my_tf_func(x):\r\n...     y = x + 1.0\r\n...     return 2.0 * imported(x)\r\n...\r\n>>> my_tf_func(tf.constant([[1., 2., 3., 4., 5.]]))\r\n<tf.Tensor: [...], numpy=array([[4.368933]], dtype=float32)>\r\n```\r\n\r\nHow can I use any arbitrary `SavedModel` in a Keras model?", "I am closing this issue as it was resolved by replacing `tf.saved_model.load(path)` with `tf.keras.models.load_model(path)`. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29784\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29784\">No</a>\n"]}]