[{"number": 31280, "title": "Change i++ to be ++i", "body": "This PR fixes a small issue by changing `i++` to `++i`.", "comments": ["This doesn't make too much difference. Will keep the original version."]}, {"number": 31279, "title": "[TF 2.0 API Docs] tf.tuple", "body": "\r\n## Documentation contributor guide: \r\nhttps://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tuple\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n### Correct links\r\nThe GitHub link to `tf.tuple` leads to `tf.tuple_v2`.\r\n### Usage example\r\nThere is no usage example.\r\n", "comments": ["Hi @jvishnuvardhan this is not a bug, the link is correct because `tuple_v2` has different param order than `tuple``", "@johnkegz,\r\nSorry for the delayed response. This is expected because the implementation of [tf.tuple](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tuple) correspond to the **`Tensorflow Version 2.x`** and hence it is pointing to [tuple_v2](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/ops/control_flow_ops.py#L2911-L2944).\r\n\r\nIf we observe the [Tensorflow 1.x version of tf.Tuple](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/tuple), it points to [tuple in Github](https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/ops/control_flow_ops.py#L2947-L3014).\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31278, "title": "[TF.2.0 API Docs] tf.train.list_variables", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/train/list_variables\r\n\r\n## Description of the issue (what needs changing):\r\nRaises for exceptions to deal with any errors and a few drawings to be added to make it easier to understand.\r\n\r\n### Raises listed and defined\r\nNo, the errors are not listed and returned.\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\nNo, there are not any current visuals.\r\n\r\n### Submit a pull request?\r\n\r\nNo, I am not willing to submit a pull request.\r\n", "comments": ["@KabohaJeanMark \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "The docs are updated with TF 2.4.1 api docs.\r\nSee https://www.tensorflow.org/api_docs/python/tf/train/list_variables \r\nThanks!"]}, {"number": 31277, "title": "[TF 2.0 API Docs] tf.keras.backend.random_uniform", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/backend/random_uniform\r\n\r\n## Description of issue (what needs changing):\r\nThe function has no usage example and there are no errors raised.\r\n\r\n### Raises listed and defined\r\nNo raises listed or defined.\r\n\r\n### Usage example\r\nNo usage example\r\n\r\n### Submit a pull request?\r\nNo.\r\n", "comments": ["@mwinel , Hello, why do you think that these random function need examples?"]}, {"number": 31276, "title": "[TF 2.0 API Docs] tf.nn.log_softmax", "body": "##  URL(s) with the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/log_softmax\r\n\r\n## Description of the issue (what needs changing):\r\n- No errors defined\r\n- No visuals\r\n\r\n### Are the errors defined?\r\n- There are no errors defined\r\n\r\n### Visuals, if applicable\r\n- There are no visuals available. I think that visuals are needed to clarify the content.\r\n\r\n\r\n", "comments": ["@MuhanguziDavid, Errors are defined [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/log_softmax#raises). Please take look and let us know. Thanks!", "@gadagashwini , seen the errors. They are well defined. Thanks."]}, {"number": 31274, "title": "Large Batch training throws memory allocation errors in tf.keras and Eager Mode but works fine when using keras imports", "body": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Relevant.\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below): tf-nightly-gpu 1.15.0.dev20190728\r\nPython version: Python 3.7.4\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: release 10.0, V10.0.130\r\nNVIDIA-SMI 418.43 Driver Version: 418.43 CUDA Version: 10.1\r\nGPU model and memory:\r\nGeForce RTX 2080Ti 11GB\r\n\r\n**Describe the current behavior**\r\ntf.keras libraries and eager mode seems to use more GPU memory than keras libraries and limits batch size during training. We train on large batch sizes (ex. 2048) for faster convergence, which works fine when we use keras libraries but when we ported over to tf.keras and use it in eager mode, we see memory allocation errors. We also see this warning just before the memory error. Strangely, the training seem to continue after throwing the error in this reproducible code but our training stops after throwing this error.  See full trace at the end. \r\n\r\n019-08-02 09:35:48.951204: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\r\n\r\n**Describe the expected behavior**\r\nNot use higher memory than directly using keras. \r\n\r\n**Code to reproduce the issue**\r\nChange mode to tf.keras, which throws memory error but when we change it to direct keras, it works fine.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\nfrom timeit import default_timer as timer\r\n\r\nmode = \"tf.keras\"\r\n#mode = \"keras\"\r\n\r\nif(mode == \"tf.keras\"):\r\n    print(\"Importing tf.keras\")\r\n    tf.enable_eager_execution()\r\n    from tensorflow.keras.models import Sequential\r\n    from tensorflow.keras.layers import LSTM, Activation, CuDNNLSTM, Dense\r\n    from tensorflow.keras.optimizers import Adadelta\r\nelse:\r\n    from keras.engine.sequential import Sequential\r\n    from keras.layers import LSTM, Activation, CuDNNLSTM, Dense\r\n    from keras.optimizers import Adadelta\r\n\r\nfeature_count = 300\r\nbatch_size = 2048\r\nlook_back = 100\r\ntarget_groups = 10\r\n\r\n\r\ndef random_data_generator( ):\r\n\r\n    x_data_size =(batch_size, look_back, feature_count) # batches, lookback, features\r\n    x_data = np.random.uniform(low=-1.0, high=5, size=x_data_size)\r\n \r\n    y_data_size = (batch_size, target_groups)\r\n    Y_data = np.random.randint(low=1, high=21, size=y_data_size)\r\n    \r\n    return x_data, Y_data\r\n \r\ndef get_simple_Dataset_generator():        \r\n    while True:\r\n        yield random_data_generator()\r\n\r\ndef build_model():\r\n    model = Sequential()     \r\n    model.add(CuDNNLSTM(feature_count,\r\n                        batch_input_shape=(batch_size,look_back, feature_count),\r\n                        stateful=False))\r\n\r\n    model.add(Dense(target_groups, activation='softmax'))\r\n    optimizer = Adadelta()        \r\n\r\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer,\r\n                      metrics = ['accuracy'])\r\n    return model\r\n\r\n \r\ndef run_training():\r\n   \r\n    model = build_model()\r\n    train_generator = get_simple_Dataset_generator()\r\n    validation_generator = get_simple_Dataset_generator()\r\n    class_weights = {0:2, 1:8, 2:1, 3:4, 4:8, 5:35, 6:30, 7:4, 8:5, 9:3}\r\n    model.fit_generator(generator = train_generator,\r\n            steps_per_epoch=1,\r\n            epochs=1000,            \r\n            verbose=2,\r\n            validation_data=validation_generator,\r\n            validation_steps=20,\r\n            max_queue_size = 10,\r\n            workers = 0, \r\n            use_multiprocessing = False,\r\n            class_weight = class_weights\r\n            )\r\n\r\nif __name__ == '__main__': \r\n    run_training()\r\n \r\n```\r\n\r\n**Other info / logs**\r\n 2019-08-02 09:35:48.951204: W tensorflow/core/common_runtime/bfc_allocator.cc:305] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\r\n2019-08-02 09:35:59.359646: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.06GiB (rounded to 5429285376).  Current allocation summary follows.\r\n2019-08-02 09:35:59.359694: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): \tTotal Chunks: 19, Chunks in use: 19. 4.8KiB allocated for chunks. 4.8KiB in use in bin. 177B client-requested in use in bin.\r\n2019-08-02 09:35:59.359706: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359717: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359727: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359737: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 5.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359752: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): \tTotal Chunks: 9, Chunks in use: 8. 90.0KiB allocated for chunks. 82.0KiB in use in bin. 79.3KiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359762: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359774: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359788: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): \tTotal Chunks: 3, Chunks in use: 3. 240.0KiB allocated for chunks. 240.0KiB in use in bin. 240.0KiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359797: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359808: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359818: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 666.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359829: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): \tTotal Chunks: 4, Chunks in use: 4. 5.49MiB allocated for chunks. 5.49MiB in use in bin. 5.49MiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359838: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): \tTotal Chunks: 12, Chunks in use: 12. 31.54MiB allocated for chunks. 31.54MiB in use in bin. 27.01MiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359848: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359857: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359867: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359877: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359887: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:35:59.359902: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 234.38MiB allocated for chunks. 234.38MiB in use in bin. 234.38MiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359914: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): \tTotal Chunks: 7, Chunks in use: 5. 9.32GiB allocated for chunks. 6.97GiB in use in bin. 6.89GiB client-requested in use in bin.\r\n2019-08-02 09:35:59.359924: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 5.06GiB was 256.00MiB, Chunk State: \r\n2019-08-02 09:35:59.359937: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1016.97MiB | Requested Size: 996.52MiB | in_use: 0 | bin_num: 20, prev:   Size: 2.34MiB | Requested Size: 2.34MiB | in_use: 1 | bin_num: -1\r\n2019-08-02 09:35:59.359949: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1.35GiB | Requested Size: 351.6KiB | in_use: 0 | bin_num: 20, prev:   Size: 1.14GiB | Requested Size: 1.14GiB | in_use: 1 | bin_num: -1\r\n2019-08-02 09:35:59.359958: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8108529920\r\n2019-08-02 09:35:59.359971: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd418000000 next 48 of size 5429285376\r\n2019-08-02 09:35:59.359980: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd55b9c5200 next 52 of size 1228800000\r\n2019-08-02 09:35:59.359989: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5a4da5200 next 18446744073709551615 of size 1450444544\r\n2019-08-02 09:35:59.359997: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1073741824\r\n2019-08-02 09:35:59.360005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc000000 next 50 of size 2457600\r\n2019-08-02 09:35:59.360014: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc258000 next 47 of size 2457600\r\n2019-08-02 09:35:59.360022: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc4b0000 next 54 of size 2457600\r\n2019-08-02 09:35:59.360030: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5fc708000 next 18446744073709551615 of size 1066369024\r\n2019-08-02 09:35:59.360041: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 536870912\r\n2019-08-02 09:35:59.360048: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd63c000000 next 57 of size 245760000\r\n2019-08-02 09:35:59.360057: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd64aa60000 next 18446744073709551615 of size 291110912\r\n2019-08-02 09:35:59.360064: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456\r\n2019-08-02 09:35:59.360074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6b0000000 next 18446744073709551615 of size 268435456\r\n2019-08-02 09:35:59.360081: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456\r\n2019-08-02 09:35:59.360090: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6c0000000 next 18446744073709551615 of size 268435456\r\n2019-08-02 09:35:59.360097: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608\r\n2019-08-02 09:35:59.360106: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e000000 next 18 of size 1440000\r\n2019-08-02 09:35:59.360114: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e15f900 next 21 of size 1440000\r\n2019-08-02 09:35:59.360122: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e2bf200 next 22 of size 1440000\r\n2019-08-02 09:35:59.360130: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e41eb00 next 18446744073709551615 of size 4068608\r\n2019-08-02 09:35:59.360139: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 16777216\r\n2019-08-02 09:35:59.360147: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71ee00000 next 37 of size 2457600\r\n2019-08-02 09:35:59.360155: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f058000 next 55 of size 2521344\r\n2019-08-02 09:35:59.360163: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f2bf900 next 60 of size 2889728\r\n2019-08-02 09:35:59.360171: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f581100 next 58 of size 2457600\r\n2019-08-02 09:35:59.360179: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f7d9100 next 56 of size 2457600\r\n2019-08-02 09:35:59.360188: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71fa31100 next 18446744073709551615 of size 3993344\r\n2019-08-02 09:35:59.360196: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 4194304\r\n2019-08-02 09:35:59.360204: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd721600000 next 6 of size 1440000\r\n2019-08-02 09:35:59.360213: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd72175f900 next 18446744073709551615 of size 2754304\r\n2019-08-02 09:35:59.360223: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1048576\r\n2019-08-02 09:35:59.360231: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00000 next 1 of size 1280\r\n2019-08-02 09:35:59.360241: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00500 next 4 of size 256\r\n2019-08-02 09:35:59.360249: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00600 next 7 of size 256\r\n2019-08-02 09:35:59.360258: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00700 next 12 of size 256\r\n2019-08-02 09:35:59.360265: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00800 next 13 of size 256\r\n2019-08-02 09:35:59.360274: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00900 next 14 of size 256\r\n2019-08-02 09:35:59.360281: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00a00 next 15 of size 256\r\n2019-08-02 09:35:59.360290: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00b00 next 16 of size 256\r\n2019-08-02 09:35:59.360298: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00c00 next 17 of size 256\r\n2019-08-02 09:35:59.360307: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00d00 next 20 of size 256\r\n2019-08-02 09:35:59.360314: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00e00 next 25 of size 256\r\n2019-08-02 09:35:59.360322: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00f00 next 26 of size 256\r\n2019-08-02 09:35:59.360330: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01000 next 27 of size 256\r\n2019-08-02 09:35:59.360338: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01100 next 28 of size 256\r\n2019-08-02 09:35:59.360346: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01200 next 29 of size 256\r\n2019-08-02 09:35:59.360354: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01300 next 32 of size 256\r\n2019-08-02 09:35:59.360361: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01400 next 33 of size 256\r\n2019-08-02 09:35:59.360370: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01500 next 34 of size 256\r\n2019-08-02 09:35:59.360378: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01600 next 35 of size 256\r\n2019-08-02 09:35:59.360387: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01700 next 38 of size 256\r\n2019-08-02 09:35:59.360394: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae01800 next 9 of size 5376\r\n2019-08-02 09:35:59.360403: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae02d00 next 10 of size 9728\r\n2019-08-02 09:35:59.360411: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae05300 next 19 of size 9728\r\n2019-08-02 09:35:59.360420: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae07900 next 8 of size 14336\r\n2019-08-02 09:35:59.360428: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0b100 next 11 of size 12032\r\n2019-08-02 09:35:59.360437: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0e000 next 23 of size 9728\r\n2019-08-02 09:35:59.360445: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae10600 next 24 of size 12032\r\n2019-08-02 09:35:59.360455: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae13500 next 30 of size 8192\r\n2019-08-02 09:35:59.360462: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae15500 next 31 of size 81920\r\n2019-08-02 09:35:59.360471: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae29500 next 40 of size 81920\r\n2019-08-02 09:35:59.360478: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3d500 next 41 of size 8192\r\n2019-08-02 09:35:59.360486: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3f500 next 42 of size 16384\r\n2019-08-02 09:35:59.360494: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae43500 next 44 of size 8192\r\n2019-08-02 09:35:59.360502: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae45500 next 45 of size 81920\r\n2019-08-02 09:35:59.360510: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae59500 next 18446744073709551615 of size 682752\r\n2019-08-02 09:35:59.360518: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 2097152\r\n2019-08-02 09:35:59.360525: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79b000000 next 18446744073709551615 of size 2097152\r\n2019-08-02 09:35:59.360534: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: \r\n2019-08-02 09:35:59.360544: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 19 Chunks of size 256 totalling 4.8KiB\r\n2019-08-02 09:35:59.360553: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB\r\n2019-08-02 09:35:59.360561: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 8192 totalling 16.0KiB\r\n2019-08-02 09:35:59.360569: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 9728 totalling 28.5KiB\r\n2019-08-02 09:35:59.360578: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 12032 totalling 23.5KiB\r\n2019-08-02 09:35:59.360586: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 14336 totalling 14.0KiB\r\n2019-08-02 09:35:59.360596: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 16384 totalling 16.0KiB\r\n2019-08-02 09:35:59.360604: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 81920 totalling 240.0KiB\r\n2019-08-02 09:35:59.360612: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 4 Chunks of size 1440000 totalling 5.49MiB\r\n2019-08-02 09:35:59.360621: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2097152 totalling 2.00MiB\r\n2019-08-02 09:35:59.360629: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 6 Chunks of size 2457600 totalling 14.06MiB\r\n2019-08-02 09:35:59.360637: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2521344 totalling 2.40MiB\r\n2019-08-02 09:35:59.360645: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2754304 totalling 2.63MiB\r\n2019-08-02 09:35:59.360653: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2889728 totalling 2.76MiB\r\n2019-08-02 09:35:59.360662: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 3993344 totalling 3.81MiB\r\n2019-08-02 09:35:59.360670: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4068608 totalling 3.88MiB\r\n2019-08-02 09:35:59.360680: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 245760000 totalling 234.38MiB\r\n2019-08-02 09:35:59.360688: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 268435456 totalling 512.00MiB\r\n2019-08-02 09:35:59.360696: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 291110912 totalling 277.62MiB\r\n2019-08-02 09:35:59.360704: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1228800000 totalling 1.14GiB\r\n2019-08-02 09:35:59.360715: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5429285376 totalling 5.06GiB\r\n2019-08-02 09:35:59.360723: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 7.24GiB\r\n2019-08-02 09:35:59.360731: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 10288519424 memory_limit_: 10288519578 available bytes: 154 curr_region_allocation_bytes_: 17179869184\r\n2019-08-02 09:35:59.360743: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: \r\nLimit:                 10288519578\r\nInUse:                  7771009536\r\nMaxInUse:               7771009536\r\nNumAllocs:                     155\r\nMaxAllocSize:           5429285376\r\n\r\n2019-08-02 09:35:59.360758: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *****************************************************************_____________*__________***********\r\n2019-08-02 09:36:09.532360: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.06GiB (rounded to 5429285376).  Current allocation summary follows.\r\n2019-08-02 09:36:09.532426: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (256): \tTotal Chunks: 19, Chunks in use: 19. 4.8KiB allocated for chunks. 4.8KiB in use in bin. 177B client-requested in use in bin.\r\n2019-08-02 09:36:09.532438: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532446: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532453: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532461: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 5.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532470: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8192): \tTotal Chunks: 9, Chunks in use: 8. 90.0KiB allocated for chunks. 82.0KiB in use in bin. 79.3KiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532478: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 16.0KiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532486: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532495: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (65536): \tTotal Chunks: 3, Chunks in use: 3. 240.0KiB allocated for chunks. 240.0KiB in use in bin. 240.0KiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532501: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532510: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532520: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 666.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532528: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (1048576): \tTotal Chunks: 4, Chunks in use: 4. 5.49MiB allocated for chunks. 5.49MiB in use in bin. 5.49MiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532538: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (2097152): \tTotal Chunks: 12, Chunks in use: 12. 31.54MiB allocated for chunks. 31.54MiB in use in bin. 27.01MiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532545: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532552: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532558: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532567: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532575: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2019-08-02 09:36:09.532589: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 234.38MiB allocated for chunks. 234.38MiB in use in bin. 234.38MiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532600: I tensorflow/core/common_runtime/bfc_allocator.cc:869] Bin (268435456): \tTotal Chunks: 7, Chunks in use: 5. 9.32GiB allocated for chunks. 6.97GiB in use in bin. 6.89GiB client-requested in use in bin.\r\n2019-08-02 09:36:09.532609: I tensorflow/core/common_runtime/bfc_allocator.cc:885] Bin for 5.06GiB was 256.00MiB, Chunk State: \r\n2019-08-02 09:36:09.532624: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1016.97MiB | Requested Size: 996.52MiB | in_use: 0 | bin_num: 20, prev:   Size: 2.34MiB | Requested Size: 2.34MiB | in_use: 1 | bin_num: -1\r\n2019-08-02 09:36:09.532635: I tensorflow/core/common_runtime/bfc_allocator.cc:891]   Size: 1.35GiB | Requested Size: 351.6KiB | in_use: 0 | bin_num: 20, prev:   Size: 1.14GiB | Requested Size: 1.14GiB | in_use: 1 | bin_num: -1\r\n2019-08-02 09:36:09.532642: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8108529920\r\n2019-08-02 09:36:09.532657: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd418000000 next 48 of size 5429285376\r\n2019-08-02 09:36:09.532664: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd55b9c5200 next 52 of size 1228800000\r\n2019-08-02 09:36:09.532673: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5a4da5200 next 18446744073709551615 of size 1450444544\r\n2019-08-02 09:36:09.532679: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1073741824\r\n2019-08-02 09:36:09.532687: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc000000 next 50 of size 2457600\r\n2019-08-02 09:36:09.532693: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc258000 next 47 of size 2457600\r\n2019-08-02 09:36:09.532700: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd5fc4b0000 next 54 of size 2457600\r\n2019-08-02 09:36:09.532706: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd5fc708000 next 18446744073709551615 of size 1066369024\r\n2019-08-02 09:36:09.532713: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 536870912\r\n2019-08-02 09:36:09.532719: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd63c000000 next 57 of size 245760000\r\n2019-08-02 09:36:09.532726: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd64aa60000 next 18446744073709551615 of size 291110912\r\n2019-08-02 09:36:09.532731: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456\r\n2019-08-02 09:36:09.532738: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6b0000000 next 18446744073709551615 of size 268435456\r\n2019-08-02 09:36:09.532747: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 268435456\r\n2019-08-02 09:36:09.532753: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd6c0000000 next 18446744073709551615 of size 268435456\r\n2019-08-02 09:36:09.532761: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 8388608\r\n2019-08-02 09:36:09.532767: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e000000 next 18 of size 1440000\r\n2019-08-02 09:36:09.532773: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e15f900 next 21 of size 1440000\r\n2019-08-02 09:36:09.532780: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e2bf200 next 22 of size 1440000\r\n2019-08-02 09:36:09.532787: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71e41eb00 next 18446744073709551615 of size 4068608\r\n2019-08-02 09:36:09.532795: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 16777216\r\n2019-08-02 09:36:09.532803: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71ee00000 next 37 of size 2457600\r\n2019-08-02 09:36:09.532810: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f058000 next 55 of size 2521344\r\n2019-08-02 09:36:09.532818: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f2bf900 next 60 of size 2889728\r\n2019-08-02 09:36:09.532826: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f581100 next 58 of size 2457600\r\n2019-08-02 09:36:09.532833: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71f7d9100 next 56 of size 2457600\r\n2019-08-02 09:36:09.532841: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd71fa31100 next 18446744073709551615 of size 3993344\r\n2019-08-02 09:36:09.532847: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 4194304\r\n2019-08-02 09:36:09.532853: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd721600000 next 6 of size 1440000\r\n2019-08-02 09:36:09.532861: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd72175f900 next 18446744073709551615 of size 2754304\r\n2019-08-02 09:36:09.532866: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 1048576\r\n2019-08-02 09:36:09.532876: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00000 next 1 of size 1280\r\n2019-08-02 09:36:09.532884: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00500 next 4 of size 256\r\n2019-08-02 09:36:09.532891: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00600 next 7 of size 256\r\n2019-08-02 09:36:09.532898: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00700 next 12 of size 256\r\n2019-08-02 09:36:09.532904: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00800 next 13 of size 256\r\n2019-08-02 09:36:09.532912: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00900 next 14 of size 256\r\n2019-08-02 09:36:09.532917: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00a00 next 15 of size 256\r\n2019-08-02 09:36:09.532926: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00b00 next 16 of size 256\r\n2019-08-02 09:36:09.532933: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00c00 next 17 of size 256\r\n2019-08-02 09:36:09.532939: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00d00 next 20 of size 256\r\n2019-08-02 09:36:09.532946: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00e00 next 25 of size 256\r\n2019-08-02 09:36:09.532953: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae00f00 next 26 of size 256\r\n2019-08-02 09:36:09.532959: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01000 next 27 of size 256\r\n2019-08-02 09:36:09.532966: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01100 next 28 of size 256\r\n2019-08-02 09:36:09.532973: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01200 next 29 of size 256\r\n2019-08-02 09:36:09.532979: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01300 next 32 of size 256\r\n2019-08-02 09:36:09.532985: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01400 next 33 of size 256\r\n2019-08-02 09:36:09.532991: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01500 next 34 of size 256\r\n2019-08-02 09:36:09.532997: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01600 next 35 of size 256\r\n2019-08-02 09:36:09.533005: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae01700 next 38 of size 256\r\n2019-08-02 09:36:09.533010: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae01800 next 9 of size 5376\r\n2019-08-02 09:36:09.533018: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae02d00 next 10 of size 9728\r\n2019-08-02 09:36:09.533025: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae05300 next 19 of size 9728\r\n2019-08-02 09:36:09.533031: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae07900 next 8 of size 14336\r\n2019-08-02 09:36:09.533038: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0b100 next 11 of size 12032\r\n2019-08-02 09:36:09.533045: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae0e000 next 23 of size 9728\r\n2019-08-02 09:36:09.533050: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae10600 next 24 of size 12032\r\n2019-08-02 09:36:09.533061: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae13500 next 30 of size 8192\r\n2019-08-02 09:36:09.533068: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae15500 next 31 of size 81920\r\n2019-08-02 09:36:09.533074: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae29500 next 40 of size 81920\r\n2019-08-02 09:36:09.533081: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3d500 next 41 of size 8192\r\n2019-08-02 09:36:09.533087: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae3f500 next 42 of size 16384\r\n2019-08-02 09:36:09.533094: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae43500 next 44 of size 8192\r\n2019-08-02 09:36:09.533102: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79ae45500 next 45 of size 81920\r\n2019-08-02 09:36:09.533109: I tensorflow/core/common_runtime/bfc_allocator.cc:905] Free  at 0x7fd79ae59500 next 18446744073709551615 of size 682752\r\n2019-08-02 09:36:09.533115: I tensorflow/core/common_runtime/bfc_allocator.cc:898] Next region of size 2097152\r\n2019-08-02 09:36:09.533123: I tensorflow/core/common_runtime/bfc_allocator.cc:905] InUse at 0x7fd79b000000 next 18446744073709551615 of size 2097152\r\n2019-08-02 09:36:09.533128: I tensorflow/core/common_runtime/bfc_allocator.cc:914]      Summary of in-use Chunks by size: \r\n2019-08-02 09:36:09.533137: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 19 Chunks of size 256 totalling 4.8KiB\r\n2019-08-02 09:36:09.533143: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1280 totalling 1.2KiB\r\n2019-08-02 09:36:09.533150: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 8192 totalling 16.0KiB\r\n2019-08-02 09:36:09.533157: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 9728 totalling 28.5KiB\r\n2019-08-02 09:36:09.533163: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 12032 totalling 23.5KiB\r\n2019-08-02 09:36:09.533172: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 14336 totalling 14.0KiB\r\n2019-08-02 09:36:09.533180: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 16384 totalling 16.0KiB\r\n2019-08-02 09:36:09.533188: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 3 Chunks of size 81920 totalling 240.0KiB\r\n2019-08-02 09:36:09.533195: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 4 Chunks of size 1440000 totalling 5.49MiB\r\n2019-08-02 09:36:09.533203: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2097152 totalling 2.00MiB\r\n2019-08-02 09:36:09.533210: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 6 Chunks of size 2457600 totalling 14.06MiB\r\n2019-08-02 09:36:09.533218: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2521344 totalling 2.40MiB\r\n2019-08-02 09:36:09.533226: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2754304 totalling 2.63MiB\r\n2019-08-02 09:36:09.533234: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 2889728 totalling 2.76MiB\r\n2019-08-02 09:36:09.533242: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 3993344 totalling 3.81MiB\r\n2019-08-02 09:36:09.533248: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 4068608 totalling 3.88MiB\r\n2019-08-02 09:36:09.533255: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 245760000 totalling 234.38MiB\r\n2019-08-02 09:36:09.533262: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 2 Chunks of size 268435456 totalling 512.00MiB\r\n2019-08-02 09:36:09.533268: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 291110912 totalling 277.62MiB\r\n2019-08-02 09:36:09.533275: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 1228800000 totalling 1.14GiB\r\n2019-08-02 09:36:09.533283: I tensorflow/core/common_runtime/bfc_allocator.cc:917] 1 Chunks of size 5429285376 totalling 5.06GiB\r\n2019-08-02 09:36:09.533290: I tensorflow/core/common_runtime/bfc_allocator.cc:921] Sum Total of in-use chunks: 7.24GiB\r\n2019-08-02 09:36:09.533296: I tensorflow/core/common_runtime/bfc_allocator.cc:923] total_region_allocated_bytes_: 10288519424 memory_limit_: 10288519578 available bytes: 154 curr_region_allocation_bytes_: 17179869184\r\n2019-08-02 09:36:09.533307: I tensorflow/core/common_runtime/bfc_allocator.cc:929] Stats: \r\nLimit:                 10288519578\r\nInUse:                  7771009536\r\nMaxInUse:               7771009536\r\nNumAllocs:                     157\r\nMaxAllocSize:           5429285376\r\n\r\n2019-08-02 09:36:09.533324: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *****************************************************************_____________*__________***********\r\n", "comments": ["@talipini \r\nI have tried on colab with tf-nightly-gpu 1.15.0.dev20190728 and i am not getting any issue with tf.keras and keras.Please, find the attachment for your reference\r\n[Untitled95.ipynb.tar.gz](https://github.com/tensorflow/tensorflow/files/3467657/Untitled95.ipynb.tar.gz)\r\n.Please, let me know if the issue still persists.Thanks!", "Thanks for looking into this. I will try to rerun again and see what I am doing different. This error has been consistent and we had to decrease the batch size when tf.keras was used to make it work. I will try again and update on this but it would be couple of days.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31273, "title": "[TF2.0] Use Adam and GradientTape() to optimize own function wrt to input (no keras model)", "body": "**Optimization**\r\nI want to optimize my own loss function with adam optimizer of keras, with respect to the input. Unfortunately this doesn't work with GradientTape(). \r\n\r\n**Executed Code with TF2.0**\r\n```\r\n@tf.function\r\ndef train(opt, input):\r\n   with tf.GradientTape() as tape:\r\n        tape.watch(input)\r\n        loss = tf.reduce_mean(input)\r\n    gradients = tape.gradient(loss, input)\r\n    opt.apply_gradients(zip(gradients, input))\r\n\r\nopt = tf.keras.optimizers.Adam(learning_rate=1)\r\ninput = tf.random.normal((1, 1))\r\n\r\ntrain(opt, input)\r\n```\r\n\r\n**Current behavior**\r\nCode gives error if `opt.apply_gradients(zip(gradients, input))` is called.\r\nTypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\r\n\r\n\r\n\r\n\r\n", "comments": ["@de-core Hi, I think the example has some errors within it:\r\n\r\n1. `apply_gradient` takes **list of (gradient, variable) pairs** as input (that's why you use **zip**). However, since `input` is a **tensor** (not a list of tensors), `tape.gradient(loss, input)` thus returns a gradient tensor w.r.t. input. You can find that both `gradients` and `input` are a tensor instead of a list of tensors, implying they are just not iterable while calling `zip`. To fix this, simply modify this line\r\n```python\r\nopt.apply_gradients(zip(gradients, input))\r\n```\r\nwith\r\n```python\r\nopt.apply_gradients(zip([gradients], [input]))\r\n```\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam#apply_gradients\r\n\r\n2. Again, because apply_gradient takes **list of (gradient, variable) pairs**, `input` must be a instance of `tf.Variable` rather than `tf.Tensor`. However, the line `input = tf.random.normal((1, 1))` creating a **tensor** with normal distribution. To make it a `tf.Variable`, wrap it with `tf.Variable` like\r\n```python\r\ninput = tf.Variable(tf.random.normal((1, 1)))\r\n```\r\nwhich means you create a `tf.Variable` with initial value `tf.random.normal(1, 1)`\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random/normal#returns\r\n\r\nThanks!", "Thank you very much, that solved it.", "Closing the issue since its resolved. Feel free to reopen if the problem still persists. Thanks!"]}, {"number": 31272, "title": "[TF2] UnicodeDecodeError when using tf.saved_model.save", "body": "**System information**\r\n- Have I written custom code: Yes.\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0.0-dev20190731\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\nCode crashes with following exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"mwe.py\", line 10, in <module>\r\n    tf.saved_model.save(model, '/tmp/model')\r\n  File \"***/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 869, in save\r\n    saveable_view, asset_info.asset_index)\r\n  File \"***/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 624, in _serialize_object_graph\r\n    _write_object_proto(obj, obj_proto, asset_file_def_index)\r\n  File \"***/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py\", line 663, in _write_object_proto\r\n    metadata=obj._tracking_metadata)\r\n  File \"***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2816, in _tracking_metadata\r\n    metadata = json.loads(super(Model, self)._tracking_metadata)\r\n  File \"***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2198, in _tracking_metadata\r\n    metadata['config'] = self.get_config()\r\n  File \"***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 878, in get_config\r\n    layer_config = layer.get_config()\r\n  File \"***/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2378, in get_config\r\n    'node_def': self.node_def.SerializeToString().decode('utf-8'),\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 68: invalid start byte\r\n```\r\n\r\nThis seems to be a regression; I was running code I had working ago some weeks/months with earlier TF2 nightlies, and this appears to me as unexpected behavior.\r\n\r\nMight be introduced by https://github.com/tensorflow/tensorflow/commit/7cc180f107f142432358ac33787466de90afd776\r\n\r\nTo me, `<protobuf object>` `.SerializeToString().decode('utf-8')` seems incorrect, since `SerializeToString()` can return arbitrary binary data (https://developers.google.com/protocol-buffers/docs/pythontutorial#parsing-and-serialization \"Note that the bytes are binary, not text; we only use the str type as a convenient container.\"), decodability into UTF-8 cannot be guaranteed. This bug might have gone unnoticed, as many other serializations just happen to be be UTF-8 decodable \u2026\r\n\r\n**Describe the expected behavior**\r\n\r\nA properly saved model.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\ninput_ = keras.layers.Input((None, None, None), dtype='float32')\r\n\r\noutput = tf.image.extract_patches(input_, sizes=(1,128,128,1), strides=(1,128,128,1), rates=(1,1,1,1), padding='VALID')\r\n\r\nmodel = keras.Model(inputs=[input_], outputs=[output])\r\n\r\ntf.saved_model.save(model, '/tmp/model')\r\n```", "comments": ["I was able to reproduce the error with TF 2.0 beta1 and nightly. ", "This seems to have been fixed in 39bc7bcf94983261a3ee8a72802f5de056728a9c .", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31272\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31272\">No</a>\n"]}, {"number": 31271, "title": "[tflite][micro] [patch] Incorrect arguments to ComputePaddingHeightWidth, causing wrong offsets in maxpool implementation", "body": "With a 1x4 kernel on a one dimensional tensor the max pool result was always minimum (0 in my case), because no input data was selected.\r\n\r\nSee padding.h: TfLitePaddingValues ComputePaddingHeightWidth(int stride_height, int stride_width, int dilation_rate_height, int **dilation_rate_width**, int in_height, int in_width, int filter_height, int filter_width, TfLitePadding **padding**, int* out_height, int* out_width)\r\n\r\n```\r\n---\r\n tensorflow/lite/experimental/micro/kernels/pooling.cc | 4 ++--\r\n 1 file changed, 2 insertions(+), 2 deletions(-)\r\n\r\ndiff --git a/tensorflow/lite/experimental/micro/kernels/pooling.cc b/tensorflow/lite/experimental/micro/kernels/pooling.cc\r\nindex 385143050f..4acda7ef68 100644\r\n--- a/tensorflow/lite/experimental/micro/kernels/pooling.cc\r\n+++ b/tensorflow/lite/experimental/micro/kernels/pooling.cc\r\n@@ -44,9 +44,9 @@ TfLiteStatus CalculateOpData(const TfLiteContext* context,\r\n   int out_height, out_width;\r\n \r\n   data->padding = ComputePaddingHeightWidth(\r\n-      params->stride_height, params->stride_width, /* dilation_rate= */ 1,\r\n+      params->stride_height, params->stride_width, /* dilation_rate= */ 1,1,\r\n       height, width, params->filter_height, params->filter_width,\r\n-      params->padding, params->padding, &out_height, &out_width);\r\n+      params->padding, &out_height, &out_width);\r\n \r\n   return kTfLiteOk;\r\n }\r\n-- \r\n```", "comments": ["It looks like this is fixed in https://github.com/tensorflow/tensorflow/commit/7645ab725f5b8ce92031e761a6fe0bbcf596b2ce.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31271\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31271\">No</a>\n", "I agree that it was solved by that commit."]}, {"number": 31270, "title": "tf.data.Iterator complain \"Function tf_data_structured_function_wrapper_xxxx is not defined.\"", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **YES**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  CentOS release 6.3 (Final)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.10.0\r\n- Python version: 2.7.14\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: - (CPU)\r\n- GPU model and memory: - (CPU)\r\n\r\n('v1.10.0-0-g656e7a2b34', '1.10.0')\r\n\r\n**Describe the current behavior**\r\n\r\nAs the title said, it complains \r\n\r\n```\r\nNotFoundError: Function tf_data_structured_function_wrapper_qCoCAdpVJm4 is not defined.\r\n\t [[Node: IteratorGetNext_8 = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator_9)]]\r\n```\r\n\r\nI follow the sample code of [api doc](https://www.tensorflow.org/versions/r1.10/api_docs/python/tf/data/Iterator#from_structure) , **Except that I change the `tf.data.Dataset` creating place.**\r\n\r\nIn original sample code, it creates 2 datasets before run `sess.run`, while I create 1 dataset  after the other dataset has run out, and it complains.\r\n\r\nIf it is hard to understand, I'll try to describe it after the following code section.\r\n\r\n**Describe the expected behavior**\r\n\r\nAll is ok.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```Python\r\nfrom tensorflow.data import Dataset \r\nfrom tensorflow.data import Iterator\r\nimport tensorflow as tf\r\n\r\niterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))\r\n\r\ndef gen():\r\n    for i in range(30):\r\n        yield i\r\n\r\ndef model_fn(ite):\r\n  return (ite + 1, ite + 23)\r\n\r\nprediction, loss = model_fn(iterator.get_next())\r\n\r\nwith tf.Session() as sess:\r\n  dataset_range = Dataset.from_generator(gen, tf.int64)\r\n  range_initializer = iterator.make_initializer(dataset_range)\r\n\r\n  sess.run(range_initializer)\r\n  while True:\r\n    try:\r\n      pred, loss_val = sess.run([prediction, loss])\r\n      print pred\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n\r\n  dataset_evens = Dataset.from_generator(gen, tf.int64)\r\n  evens_initializer = iterator.make_initializer(dataset_evens)\r\n\r\n  sess.run(evens_initializer)\r\n  while True:\r\n    try:\r\n      ## <------ HERE!! OH, it complaint!\r\n      pred, loss_val = sess.run([prediction, loss])\r\n      print pred\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n```\r\n\r\nif I change the `dataset_evens = Dataset.from_generator(gen, tf.int64)` to the place after `dataset_range = Dataset.from_generator(gen, tf.int64)`, it also will be OK.\r\n\r\n**Other info / logs**\r\n\r\nExplain Why I need create after `sess.run`:\r\nbecause our data is generated from a generator, and I want to shuffle it at every epoch end.\r\nThe easiest way and most clear way is change the generator (let it generates data in shuffle order) at every epoch end.\r\n\r\nWhy not `repeat()`?  => Oh, it can't report the `epoch end` signal.\r\n\r\nHas other way? => YES, I can keep only 1 generator and change the generator's inner state to shuffle, BUT it may be tricky and cause confusion?\r\n\r\n\r\n", "comments": ["@memeda This is not an issue in Tensorflow 1.14.0. Would you like give try. Thanks!", "Oh~ I'll try it~ sorry I am too lazy\r\n\u2014\u2014\u2014\u2014\u2014\u2014\r\nOK\uff0ctf 1.14.0 really fix it, with some warnings \ud83d\ude02 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31270\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31270\">No</a>\n"]}, {"number": 31269, "title": "no such file or directory -Vs Build tools 2017 no longer available", "body": "**System information**\r\n- OS Platform and Distribution: Windows Server 2016\r\n- TensorFlow installed from: trying to install from source\r\n- TensorFlow version: Current/Master\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.26.1\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the problem**\r\nWhen trying to build tensorflow with bazel, the Visual Studio 2017 Build Tools cannot be located:\r\n    C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)  \r\n\r\nUnfortunately the Build tools for VS 2017 could no longer be located on the Microsoft Website...\r\nVS Build tools for VS 2019 have been installed but are not supported.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nRather a deprecation than a bug. Is there any source for 2017 Build tools available? or possibility to build with 2019 build tools?\r\n", "comments": ["@forReason Please take a look at the [Microsoft website](https://visualstudio.microsoft.com/vs/older-downloads/) for VS 2017. Let us know how it progresses. Thanks!", "https://my.visualstudio.com/Downloads?q=visual%20studio%202017&wt.mc_id=o~msft~vscom~older-downloads\r\n\r\nthank you, this link helped.\r\n", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31269)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31269)\r\n"]}, {"number": 31268, "title": "Raise error on to_code when entity didn't contain __code__ attr", "body": "This PR address #30895 issue.", "comments": ["Can one of the admins verify this patch?", "@ilhamfp Could you look into the failed test?  You can click \"Details\" next to each test, e.g., \"Windows Bezel\" and see the details, e.g., https://source.cloud.google.com/results/invocations/38d5d46d-ad98-4052-b26c-557478091394/targets/%2F%2Ftensorflow%2Fpython%2Fautograph%2Fimpl:api_test/tests\r\n\r\nWe have to make sure that all the tests are passing before we land.  I didn't take a look those failing tests closely yet, but please let me know in case you need any help!\r\n\r\n", "@kkimdev Ah, I think I've found the error. I should not check for ValueError since any error is converted into ConversionError [here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/autograph/impl/api.py#L607-L618). I've changed it to `self.assertRaisesRegex(Exception, 'try passing.*python_function')`.", "Changes have been merged into master by commit 975b093. So closing the PR."]}, {"number": 31267, "title": "How to configure Tensorflow to use a specific GPU?", "body": "These are the activated devices that I have:\r\n\r\n    [name: \"/device:CPU:0\"\r\n    device_type: \"CPU\"\r\n    memory_limit: 268435456\r\n    locality {\r\n    }\r\n    incarnation: 5415837867258701517\r\n    , name: \"/device:GPU:0\"\r\n    device_type: \"GPU\"\r\n    memory_limit: 3198956339\r\n    locality {\r\n      bus_id: 1\r\n      links {\r\n      }\r\n    }\r\n    incarnation: 12462133041849407996\r\n    physical_device_desc: \"device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\r\n    ]\r\n\r\nWhat I want to do is to configure my program to use `GeForce GTX 960M` and also make this configuration permanent for all my previous/future programs if is it possible?", "comments": ["Hi,\r\nTensorFlow automatically places operations on a GPU, if possible. As you only have one GPU in your system, it is used by default.\r\nFind more information here: https://www.tensorflow.org/guide/using_gpu\r\nMake sure you have TensorFlow GPU installed:\r\nhttps://www.tensorflow.org/install/gpu", "@pzobel : Hi and thank you,\r\nBut I have two graphic cards. The first and default card is `intel hd 540` and the second card that I want to use is `Geforce GTX 960M`.", "Your Intel HD 540 cannot be utilized by TensorFlow. TensorFlow requires NVIDIA GPUs with CUDA Compute Capability of at least 3.5 (https://www.tensorflow.org/install/gpu). This requirement is only met by your GTX 960M.\r\n\r\n", "@pzobel , Thanks again!\r\nSo, how can I find if I am using GPU or CPU? Also how can I switch between them?\r\nI feel using GPU reduced my speed instead of increasing it!", "- To find out which devices your operations and tensors are assigned to\r\n```python    \r\nimport tensorflow as tf\r\ntf.debugging.set_log_device_placement(True)\r\n```\r\n\r\n- To switch between gpu and cpu\r\nYou can use ```tf.device```\r\nSee https://www.tensorflow.org/guide/using_gpu#manual_device_placement"]}, {"number": 31266, "title": "undeclared inclusion(s) in rule '//tensorflow/lite/toco/python:toco_python_api'", "body": "\r\n**System information**\r\n- OS Platform and Distribution: 19.04\r\n- TensorFlow installed from (source or binary): 2.0beta1\r\n- TensorFlow version: 2.0beta1\r\n- Python version: 3.7.3\r\n- Bazel version: 0.26.0\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1/7.6.2\r\n- GPU architecture: 5.2 \r\n\r\n**Describe the problem**\r\n```console\r\nERROR: ....../tensorflow/tensorflow/lite/toco/python/BUILD:22:1: undeclared inclusion(s) in rule '//tensorflow/lite/toco/python:toco_python_api':\r\n```\r\n\r\nAny further suggestions?", "comments": ["@jiapei100 ,\r\nCan you please provide the exact sequence of commands / steps that you executed before running into the problem?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31265, "title": "No gradients provided for any variable ", "body": "tensorflow : 2.0.0-beta1\r\npython : 3.5.2\r\n\r\nraise error \r\n```\r\nValueError: No gradients provided for any variable: ['dense/kernel:0', \r\n```\r\nmy code blew \r\n\r\n```\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport os\r\n\r\n\r\nclass Generator(keras.Model):\r\n\r\n    def __init__(self, n_f=512, n_k=4):\r\n        super(Generator, self).__init__()\r\n        self.model = keras.models.Sequential([\r\n            keras.layers.Input((100)),\r\n            keras.layers.Dense(3 * 3 * n_f, activation=tf.nn.leaky_relu),\r\n            keras.layers.Reshape((3, 3, n_f)),\r\n            keras.layers.Conv2DTranspose(filters=n_f // 2, kernel_size=3, strides=2, padding='valid'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.LeakyReLU(),\r\n            keras.layers.Conv2DTranspose(filters=n_f // 4, kernel_size=n_k, strides=2, padding='same'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.LeakyReLU(),\r\n            keras.layers.Conv2DTranspose(filters=1, kernel_size=n_k, strides=2, padding='same'),\r\n\r\n        ])\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        return self.model(inputs, training=training, mask=mask)\r\n\r\n\r\nclass Discriminator(keras.Model):\r\n\r\n    def __init__(self, n_f=512, n_k=4):\r\n        super(Discriminator, self).__init__()\r\n\r\n        self.model = keras.models.Sequential([\r\n            keras.layers.Input((28, 28, 1)),\r\n            keras.layers.Conv2D(filters=n_f, kernel_size=n_k, strides=2, padding='same', activation=tf.nn.leaky_relu),\r\n            keras.layers.Conv2D(filters=n_f * 2, kernel_size=n_k, strides=2, padding='same'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.Conv2D(filters=n_f * 4, kernel_size=n_k, strides=2, padding='same'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.Flatten(),\r\n            keras.layers.Dense(1),\r\n        ])\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        return self.model(inputs, training=training, mask=mask)\r\n\r\n\r\nclass GAN():\r\n\r\n    def __init__(self, generator, discriminator, lr=1e-3):\r\n        self.generator = generator\r\n        self.discriminator = discriminator\r\n\r\n        self.d_optimizer = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5)\r\n        self.g_optimizer = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5)\r\n\r\n    def _get_loss(self, logits, label):\r\n        return tf.losses.mean_squared_error(logits, label)\r\n\r\n    def _gan_loss_fn(self, generator, discriminator, input_noise, real_image):\r\n        fake_image = generator(input_noise)\r\n        real_logits = discriminator(fake_image)\r\n        fake_logits = discriminator(real_image)\r\n\r\n        g_loss = self._get_loss(fake_logits, tf.ones_like(fake_logits, dtype=tf.int32))\r\n        d_loss = self._get_loss(real_logits, tf.ones_like(real_logits, dtype=tf.int32)) + \\\r\n                 self._get_loss(fake_logits, tf.zeros_like(fake_logits, dtype=tf.int32))\r\n\r\n        return g_loss, d_loss\r\n\r\n    def train(self, real_image, input_noise, verbose=False):\r\n        g_loss, d_loss = self._gan_loss_fn(self.generator, self.discriminator, input_noise, real_image)\r\n        with tf.GradientTape() as tape:\r\n            grads = tape.gradient(g_loss, self.generator.trainable_variables)\r\n            self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\r\n\r\n        with tf.GradientTape() as tape:\r\n            grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\r\n            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_variables))\r\n\r\n        if verbose:\r\n            print(\"d_loss : %.5f , g_loss : %.5f\" % (d_loss, g_loss))\r\n\r\n    def save_weights(self, path):\r\n        os.makedirs(path, exist_ok=True)\r\n        self.generator.save_weights(os.path.join(path, 'generator'))\r\n        self.discriminator.save_weights(os.path.join(path, 'discriminator'))\r\n\r\n    def restore_weights(self, path):\r\n        self.generator.load_weights(os.path.join(path, 'generator'))\r\n        self.discriminator.load_weights(os.path.join(path, 'discriminator'))\r\n\r\n\r\nif __name__ == '__main__':\r\n    import numpy as np\r\n    import sys\r\n    print(sys.version)\r\n    print(tf.__version__)\r\n\r\n    gan = GAN(Generator(), Discriminator())\r\n    batch_size = 64\r\n    gan.train(np.random.rand(batch_size, 28, 28, 1).astype(np.float32),\r\n              np.random.rand(batch_size, 100).astype(np.float32))\r\n\r\n```", "comments": ["Hi,\r\n\r\n_Fixing the GradientTape_\r\n\r\nThe main issue is that you are using the `GradientTape` wrong. The schematic correct way is the following:\r\n```python\r\n# Set up the tape.\r\nwith tf.GradientTape() as tape:\r\n    # Transform the data and compute the loss with the tape recording gradients.\r\n    prediction = model(data)\r\n    loss = loss(prediction)\r\n# Now, gather the gradients and apply them.\r\ngrads = tape.gradient(loss, model.trainable_variables)\r\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n```\r\n\r\nAdditionally, in your case, you compute two intertwined losses and thus need to compute two sets of gradients (as you correctly sketched it). By default, `GradientTape` is non-persistent, i.e. it can only compute gradients once, after which it releases its resource variables. Thus, you need to explicitly make the tape consistent (`tf.GradientTape(persistent=True)`), and will probably want to explicitly `del tape` once gradients are computed to ensure the now-persistent variables are released upon the tape's garbage collection (avoiding a potential memory leak). See the docstring of `tf.GradientTape` for details (s/o to the developers; tensorflow is not always well-documented, but this class's docstring is really great!).\r\n\r\nSo, in your case:\r\n```python\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    # Compute the losses (note: I changed the method's signature; you\r\n    # can use self.generator and self.discriminator in _gan_loss_fn)\r\n    g_loss, d_loss = self._gan_loss_fn(input_noise, real_image)\r\n# Compute and apply the first set of gradients.\r\ng_grads = tape.gradient(g_loss, self.generator.trainable_variables)\r\nself.g_optimizer.apply_gradients(zip(g_grads, self.generator.trainable_variables))\r\n# Compute and apply the second set of gradients.\r\nd_grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\r\nself.d_optimizer.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\r\n# Delete the tape to ensure proper garbage collection and variables release.\r\ndel tape\r\n```\r\n\r\n_Additional Issues_\r\n\r\nNow, if you do this, it still will not work, because there is another issue in your code: you compute losses with `tf.int32` reference labels, which is incorrect if you really want to use an MSE loss. Change the dtype to `tf.float32` and things will run. That being said, I am pretty sure this is not the correct loss function, and you actually want to use a binary cross-entropy loss (in which case the true labels should be of dtype `tf.int32`). I also think you interchanged the `true_logits` and `fake_logits` tensors...\r\n\r\nI leave it to you to see which loss you need to set up, etc. At any rate, the losses tensors should be of dtype float for the tape to compute gradients.\r\n\r\nFinally, this is a detail, but your losses print-out will not work either, as the losses are arrays of values; but you can simply `tf.reduce_mean` them upon printing (and preferably use `tf.print` rather than `print`) and everything should be fine :)", "\r\nI'm very much obliged to you. @pandrey-fr \uff0cbut still the same problem.\r\n```\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport os\r\n\r\n\r\nclass Generator(keras.Model):\r\n\r\n    def __init__(self, n_f=512, n_k=4):\r\n        super(Generator, self).__init__()\r\n        self.model = keras.models.Sequential([\r\n            keras.layers.Input((100)),\r\n            keras.layers.Dense(3 * 3 * n_f, activation=tf.nn.leaky_relu),\r\n            keras.layers.Reshape((3, 3, n_f)),\r\n            keras.layers.Conv2DTranspose(filters=n_f // 2, kernel_size=3, strides=2, padding='valid'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.LeakyReLU(),\r\n            keras.layers.Conv2DTranspose(filters=n_f // 4, kernel_size=n_k, strides=2, padding='same'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.LeakyReLU(),\r\n            keras.layers.Conv2DTranspose(filters=1, kernel_size=n_k, strides=2, padding='same'),\r\n\r\n        ])\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        return self.model(inputs, training=training, mask=mask)\r\n\r\n\r\nclass Discriminator(keras.Model):\r\n\r\n    def __init__(self, n_f=512, n_k=4):\r\n        super(Discriminator, self).__init__()\r\n\r\n        self.model = keras.models.Sequential([\r\n            keras.layers.Input((28, 28, 1)),\r\n            keras.layers.Conv2D(filters=n_f, kernel_size=n_k, strides=2, padding='same', activation=tf.nn.leaky_relu),\r\n            keras.layers.Conv2D(filters=n_f * 2, kernel_size=n_k, strides=2, padding='same'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.Conv2D(filters=n_f * 4, kernel_size=n_k, strides=2, padding='same'),\r\n            keras.layers.BatchNormalization(),\r\n            keras.layers.Flatten(),\r\n            keras.layers.Dense(1),\r\n        ])\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        return self.model(inputs, training=training, mask=mask)\r\n\r\n\r\nclass GAN():\r\n\r\n    def __init__(self, generator, discriminator, lr=1e-3):\r\n        self.generator = generator\r\n        self.discriminator = discriminator\r\n\r\n        self.d_optimizer = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5)\r\n        self.g_optimizer = keras.optimizers.Adam(learning_rate=lr, beta_1=0.5)\r\n\r\n    def _get_loss(self, logits, label):\r\n        return tf.losses.mean_squared_error(logits, label)\r\n\r\n    def _gan_loss_fn(self, generator, discriminator, input_noise, real_image):\r\n        fake_image = generator(input_noise)\r\n        real_logits = discriminator(fake_image)\r\n        fake_logits = discriminator(real_image)\r\n\r\n        g_loss = self._get_loss(fake_logits, tf.ones_like(fake_logits, dtype=tf.float32))\r\n        d_loss = self._get_loss(real_logits, tf.ones_like(real_logits, dtype=tf.float32)) + \\\r\n                 self._get_loss(fake_logits, tf.zeros_like(fake_logits, dtype=tf.float32))\r\n\r\n        return g_loss, d_loss\r\n\r\n    def train(self, real_image, input_noise, verbose=False):\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            g_loss, d_loss = self._gan_loss_fn(self.generator, self.discriminator, input_noise, real_image)\r\n\r\n        grads = tape.gradient(g_loss, self.generator.trainable_variables)\r\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\r\n\r\n        grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\r\n        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_variables))\r\n\r\n        if verbose:\r\n            tf.print(\"d_loss : %.5f , g_loss : %.5f\" % (d_loss, g_loss))\r\n\r\n    def save_weights(self, path):\r\n        os.makedirs(path, exist_ok=True)\r\n        self.generator.save_weights(os.path.join(path, 'generator'))\r\n        self.discriminator.save_weights(os.path.join(path, 'discriminator'))\r\n\r\n    def restore_weights(self, path):\r\n        self.generator.load_weights(os.path.join(path, 'generator'))\r\n        self.discriminator.load_weights(os.path.join(path, 'discriminator'))\r\n\r\n\r\nif __name__ == '__main__':\r\n    import numpy as np\r\n    import sys\r\n\r\n    print(sys.version)\r\n    print(tf.__version__)\r\n\r\n    gan = GAN(Generator(), Discriminator())\r\n    batch_size = 64\r\n    gan.train(np.random.rand(batch_size, 28, 28, 1).astype(np.float32),\r\n              np.random.rand(batch_size, 100).astype(np.float32))\r\n\r\n```\r\n\r\n\r\n", "That is due to the aforementioned inversion between the `fake_logits` and `real_logits`.\r\nIt should be:\r\n\r\n```python\r\nfake_logits = discriminator(fake_image)\r\nreal_logits = discriminator(real_image)\r\n```\r\n\r\nOtherwise the `g_loss` is computed on the real image's processing by the discriminator ; since the generator has, in this erroneous case, nothing to do with the loss, it is just logical that no gradients could be computed.", "@LieJiang Please make changes as suggested by  @pandrey-fr as I did the same and it works. Closing this issue as it has been resolved."]}, {"number": 31264, "title": "GradientTape() unable to compute the gradient wrt Keras model inputs", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0beta\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIt is easy to use keras.backend.gradients to compute the gradients of loss wrt model input in a Keras model. However this function has been banned in TF 2.0 and GradientTape now cannot provide this usage, as model input cannot be watched before model.fit() and will be useless to be watched after that! Here is an example:\r\n\r\n```\r\ndef train_step(images, labels):\r\n  with tf.GradientTape() as tape:\r\n    predictions = model(images)\r\n    tape.watch(model.inputs)\r\n    loss = loss_object(labels, predictions)\r\n  print(tape.gradient(loss, model.inputs))\r\n```\r\nIt will result in [None] no matter where I put the tape.watch sentense.\r\n\r\n**Will this change the current api? How?**\r\nI think no.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who need to calculate the gradients for further usage.\r\n\r\n", "comments": ["You need to call tape.watch(images) if you want the gradient wrt images.", "(i.e. don't use model.inputs, use images, as that's a robust thing while model.inputs is not)", "@fchollet do we have a plan for how to discourage people from making this user error (the error that model.inputs doesn't mean what you want it to mean in eager mode)?", "> (i.e. don't use model.inputs, use images, as that's a robust thing while model.inputs is not)\r\n\r\nWatching images will take into an error, as in common we directly feed a numpy.ndarray object into the model, images is not a tensor object and this happens:\r\n`AttributeError                            Traceback (most recent call last)\r\n<ipython-input-21-01d8068e12ab> in <module>()\r\n      1 with tf.GradientTape() as tape:\r\n----> 2     tape.watch(train_images)\r\n      3     model.evaluate(x=train_images,y=train_labels)\r\n      4 tape.gradient(model.total_loss,train_images)\r\n\r\nc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py in watch(self, tensor)\r\n    815     \"\"\"\r\n    816     for t in nest.flatten(tensor):\r\n--> 817       if not t.dtype.is_floating:\r\n    818         logging.log_first_n(\r\n    819             logging.WARN, \"The dtype of the watched tensor must be \"\r\n\r\nAttributeError: 'numpy.dtype' object has no attribute 'is_floating'`", "So use tf.convert_to_tensor on images before applying the model.\n\nOn Mon, Aug 5, 2019, 18:34 lujq96 <notifications@github.com> wrote:\n\n> (i.e. don't use model.inputs, use images, as that's a robust thing while\n> model.inputs is not)\n>\n> Watching images will take into an error, as in common we directly feed a\n> numpy.ndarray object into the model, images is not a tensor object and this\n> happens:\n> `AttributeError Traceback (most recent call last)\n> in ()\n> 1 with tf.GradientTape() as tape:\n> ----> 2 tape.watch(train_images)\n> 3 model.evaluate(x=train_images,y=train_labels)\n> 4 tape.gradient(model.total_loss,train_images)\n>\n> c:\\Program Files (x86)\\Microsoft Visual\n> Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\n> in watch(self, tensor)\n> 815 \"\"\"\n> 816 for t in nest.flatten(tensor):\n> --> 817 if not t.dtype.is_floating:\n> 818 logging.log_first_n(\n> 819 logging.WARN, \"The dtype of the watched tensor must be \"\n>\n> AttributeError: 'numpy.dtype' object has no attribute 'is_floating'`\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31264?email_source=notifications&email_token=AAABHRIMSVMCT2J2ZB7R55TQDDIIVA5CNFSM4II3PUX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3TRBDI#issuecomment-518459533>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRM5NGUCZARDHFVJE53QDDIIVANCNFSM4II3PUXQ>\n> .\n>\n", "@lujq96, \r\nDid you try the @alextp's solution. Thanks", "> @lujq96,\r\n> Did you try the @alextp's solution. Thanks\r\n\r\n@gadagashwini @alextp \r\nI tried using the following code:\r\n```\r\ntensor_train_images = tf.convert_to_tensor(train_images, dtype=tf.float32)\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(tensor_train_images)\r\n    model.evaluate(x=tensor_train_images,y=train_labels, steps=10)\r\nprint(tape.gradient(model.total_loss,tensor_train_images))\r\n```\r\nAnd the return was\r\n```\r\n10/10 [==============================] - 8s 753ms/step - loss: 0.4758 - acc: 0.8343\r\nNone\r\n```\r\nI'm not sure if my usage was wrong. However as the result of a lack of time, I went back to use the r1.14 API for gradients.", "Oh, I see now. model.total_loss in keras is also not a tensor, so you can't differentiate wrt it.\r\n\r\nInstead do something like\r\n\r\n```\r\ntensor_train_images = tf.convert_to_tensor(train_images, dtype=tf.float32)\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(tensor_train_images)\r\n    output = model(tensor_train_images)\r\n    loss = my_loss_function(train_lavels, output)\r\nprint(tape.gradient(model.total_loss,tensor_train_images))\r\n```\r\n", "@fchollet how do you feel about trying to keep this intermediate keras information (like model.total_loss) as tensors, now that we have tf2?", "def adversarial_pattern(image, label):\r\n---->    images = tf.convert_to_tensor(image, dtype=tf.float32)\r\n---->    with tf.GradientTape() as tape:\r\n---->---->        tape.watch(images)\r\n---->---->        prediction = model(images) \r\n---->---->        loss = tf.keras.losses.MSE(label, prediction)\r\n---->    print(\"Gradient \",tape.gradient(model.total_loss,images)) \r\nIf i run this code i get : Gradient None\r\n\r\nHowever instead of total_loss if i use loss like as  \r\n---->---->tape.gradient(loss,images) \r\nit gives error : \r\n'Tensor' object has no attribute 'numpy' .\r\n\r\nI don't understand how to solve it . I need to find the gradient with its sign of an image \r\nThanks\r\n@alex-petrenko . @fchollet ", "Despite all above comments, none appear to actually resolve the issue in the latest version of tf2.0.\r\n\r\ntape.watch on a numpy input converted to a tensor still causes tape.gradient(model_output, model_input) to return None.\r\n\r\nThis behaviour works as expected using tf.gradients without eager execution enabled.", "ckyleda: please file a separate issue with instructions to reproduce\nagainst nightly.\n\nOn Tue, Apr 21, 2020 at 8:12 AM ckyleda <notifications@github.com> wrote:\n\n> Despite all above comments, none appear to actually resolve the issue in\n> the latest version of tf2.0.\n>\n> tape.watch on a numpy input converted to a tensor still causes\n> tape.gradient(model_output, model_input) to return None.\n>\n> This behaviour works as expected using tf.gradients without eager\n> execution enabled.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31264#issuecomment-617242529>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKC7MM7LM76H2CX3DDRNWZWDANCNFSM4II3PUXQ>\n> .\n>\n\n\n-- \n - Alex\n", "> ckyleda: please file a separate issue with instructions to reproduce against nightly.\r\n> [\u2026](#)\r\n> On Tue, Apr 21, 2020 at 8:12 AM ckyleda ***@***.***> wrote: Despite all above comments, none appear to actually resolve the issue in the latest version of tf2.0. tape.watch on a numpy input converted to a tensor still causes tape.gradient(model_output, model_input) to return None. This behaviour works as expected using tf.gradients without eager execution enabled. \u2014 You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <[#31264 (comment)](https://github.com/tensorflow/tensorflow/issues/31264#issuecomment-617242529)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAABHRKC7MM7LM76H2CX3DDRNWZWDANCNFSM4II3PUXQ> .\r\n> -- - Alex\r\n\r\nThat seems unnecessary as this issue is erroneously closed as it has not actually been fixed.", "I can't reproduce it, so please add instructions to reproduce.\n\nOn Tue, Apr 21, 2020 at 8:18 AM ckyleda <notifications@github.com> wrote:\n\n> ckyleda: please file a separate issue with instructions to reproduce\n> against nightly.\n> \u2026 <#m_6829052761456696308_>\n> On Tue, Apr 21, 2020 at 8:12 AM ckyleda *@*.***> wrote: Despite all above\n> comments, none appear to actually resolve the issue in the latest version\n> of tf2.0. tape.watch on a numpy input converted to a tensor still causes\n> tape.gradient(model_output, model_input) to return None. This behaviour\n> works as expected using tf.gradients without eager execution enabled. \u2014 You\n> are receiving this because you modified the open/close state. Reply to this\n> email directly, view it on GitHub <#31264 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/31264#issuecomment-617242529>>,\n> or unsubscribe\n> https://github.com/notifications/unsubscribe-auth/AAABHRKC7MM7LM76H2CX3DDRNWZWDANCNFSM4II3PUXQ\n> .\n> -- - Alex\n>\n> That seems unnecessary as this issue is erroneously closed as it has not\n> actually been fixed.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31264#issuecomment-617245926>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHROGSLSEIUIOY2TE3SDRNW2LZANCNFSM4II3PUXQ>\n> .\n>\n\n\n-- \n - Alex\n", "> I can't reproduce it, so please add instructions to reproduce.\r\n> [\u2026](#)\r\n> On Tue, Apr 21, 2020 at 8:18 AM ckyleda ***@***.***> wrote: ckyleda: please file a separate issue with instructions to reproduce against nightly. \u2026 <#m_6829052761456696308_> On Tue, Apr 21, 2020 at 8:12 AM ckyleda *@*.***> wrote: Despite all above comments, none appear to actually resolve the issue in the latest version of tf2.0. tape.watch on a numpy input converted to a tensor still causes tape.gradient(model_output, model_input) to return None. This behaviour works as expected using tf.gradients without eager execution enabled. \u2014 You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <#31264 (comment) <[#31264 (comment)](https://github.com/tensorflow/tensorflow/issues/31264#issuecomment-617242529)>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AAABHRKC7MM7LM76H2CX3DDRNWZWDANCNFSM4II3PUXQ . -- - Alex That seems unnecessary as this issue is erroneously closed as it has not actually been fixed. \u2014 You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <[#31264 (comment)](https://github.com/tensorflow/tensorflow/issues/31264#issuecomment-617245926)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAABHROGSLSEIUIOY2TE3SDRNW2LZANCNFSM4II3PUXQ> .\r\n> -- - Alex\r\n\r\nI have worked around this issue.\r\n\r\nThere seem to be two workarounds:\r\nInstantiate a tf.variable from the input (not recommended as this breaks distribution strategies), or create two tapes, with one who watches the input converted to a tensor.\r\n\r\nThe latter seems to work better.\r\n\r\n"]}, {"number": 31263, "title": "Fixed cuda runtime deprecation warning", "body": "In cuda 10.0 `memoryType` attribute is deprecated in favour of `type`. \r\n\r\nSee Runtime Documentation [here](https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaPointerAttributes.html#structcudaPointerAttributes_191ef95dac597cc710558dabbc8b9ae38).", "comments": ["@kashif Could you please resolve the conflicts? Thanks!", "Can one of the admins verify this patch?", "@kashif Could you please resolve the conflicts? Thanks!"]}, {"number": 31262, "title": "I have installed \"tensorflos-gpu\" but it doesn't work(Asus N552VW with 2 graphic cards)", "body": "I just installed `tensorflow-gpu` successfully. I also installed `tensorflow` after that successfully. I tried the following code on `spyder3`:\r\n\r\n    from tensorflow.python.client import device_lib\r\n    print(device_lib.list_local_devices())\r\n\r\nBut the result is:\r\n\r\n    [name: \"/device:CPU:0\"\r\n    device_type: \"CPU\"\r\n    memory_limit: 268435456\r\n    locality {\r\n    }\r\n    incarnation: 18318063967424845606\r\n    ]\r\n\r\nIt seems the GPU version of tensorflow doesn't work! How can I fix it?\r\n\r\n**System information**\r\n- OS Platform and Distribution (Windows 10):\r\n- TensorFlow installed from (pip install tensorflow/tensorflow-gpu):\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip\r\n- CUDA/cuDNN version: 10.1 /  7.6.2\r\n- GPU model and memory: Geforce GTX 960M\r\n\r\nI also must mention that my laptop has 2 graphic cards that the second card that I installed CUDA/cuDNN for that is `Geforce GTX 960M`.\r\n\r\nI also tried the following code:\r\n\r\n    import tensorflow as tf\r\n    \r\n    # Creates a graph.\r\n    with tf.device('/device:GPU:2'):\r\n      a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n      b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n      c = tf.matmul(a, b)\r\n    # Creates a session with allow_soft_placement and log_device_placement set\r\n    # to True.\r\n    sess = tf.Session(config=tf.ConfigProto(\r\n          allow_soft_placement=True, log_device_placement=True))\r\n    # Runs the op.\r\n    print(sess.run(c))\r\n\r\nAnd got this result:\r\n\r\n    [[22. 28.]\r\n     [49. 64.]]", "comments": ["- TF fails to recognize your GPU instance.\r\n  The reason for that is, You are using cuda 10.1 along with pre-built TF-GPU binary.\r\n  The TF-GPU 1.14 prebuilt binary supports cuda 10.0\r\n  See https://www.tensorflow.org/install/gpu#software_requirements\r\n- Therefore, you need to uninstall cuda 10.1 and install cuda 10.0\r\n- Further make sure you update environment path for cuda.\r\n  See https://www.tensorflow.org/install/gpu#windows_setup", "I have the same setup. \r\nInput:\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\nOutput:\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 13583293957510262229\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 3181130547\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 2244988091806553314\r\nphysical_device_desc: \"device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\r\n]\r\n\r\n\r\nBUT I try to use Pytorch and when I try:\r\nInput:\r\nimport torch\r\ntorch.cuda.is_available()\r\nOutput:\r\n False"]}, {"number": 31261, "title": "Build TFLite Model Benchmark Tool with GPU delegate for iOS?", "body": "**System information**\r\n- TensorFlow version (you are using):master repogitory\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm trying to build TFLite Model Benchmark Tool with GPU delegate on iOS devices.\r\n@freedomtan showed us the great patches for Android and some samples on #27402. I deeply appreciate his commit.\r\nHowever it is still difficult for me to build the latest TFLite model benchmark tool with the latest GPU delegate. Unfortunately, building TFLite core library `libtensorflow-lite.a` and benchmark module `benchmark-lib.a` depends on the shell script `/tensorflow/lite/tools/make/build_ios_universal_lib.sh` using CMake. On the other hand, building GPU delegate module `libmetal_delegate.a` depends on bazel `//tensorflow/lite/delegates/gpu:metal_delegate`. Hence I'm puzzled. \r\nWould you like to provide bazel script to build TFLite Model Benchmark Tool with GPU delegate for iOS?\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone using the TFLite.\r\n\r\n**Any Other info.**\r\n", "comments": ["@miaout17 \r\n\r\nCould you help @stakemura set up the TFLite model benchmark for iOS?  It's a domain I'm not familiar with...", "Just made a commit (f560b2745e59ac663bb554fc75304d8d3fa9d34e) for adding GPU delegate option to the benchmark tool.\r\n\r\nCould you follow the instructions here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios\r\n\r\nand let me know if it works for you?", "@yyoon \r\n\r\nI succeeded to build TensorFlowLiteBenchmarkC.framework on my environment.\r\nI thanks a lot for the great work.\r\nIn my opinion, I wish we could build TFLiteBenchmark .ipa file with the [bazel](https://docs.bazel.build/versions/master/tutorial/ios-app.html#add-an-ios_application-rule).", "Great. Will consider adding a bazel rule for building the app in the future."]}, {"number": 31260, "title": "fix comparison of integers of different signs", "body": "fix comparison of integers of different signs in bitmap", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31260) for more info**.\n\n<!-- need_author_cla -->", "@yanchong Please sign CLA in order to proceed with next steps. Thank you!", "> @yanchong Please sign CLA in order to proceed with next steps. Thank you!\r\n\r\nOK, I am working on it now.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31260) for more info**.\n\n<!-- ok -->", "> > @yanchong Please sign CLA in order to proceed with next steps. Thank you!\r\n> \r\n> OK, I am working on it now.\r\n\r\n@gbaned  Now It's fine."]}, {"number": 31259, "title": "Xla modify column reduction threshold", "body": "@thomasjoerg this is the change you recommended to allow more column reduction nodes to be fused as KInput for more threading.  It worked for the graph I showed you (where reduce node's kept dimension is on the minor side, making it column reduction and tile_size_x is only 2) as well as the original network.  Since it involves both reduction and fusion, it may have broad impact.  Please let me know whether this change is good for merge.", "comments": []}, {"number": 31258, "title": "tensorboard error:'tensorflow.python.estimator.api.estimator' has no attribute 'SessionRunHook'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win 10\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:1.14.0\r\n- Python version:3.6.4\r\n- Installed using virtualenv? pip? conda?:pip\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n  I input 'tensorboard' in cmd,and get an error:'tensorflow.python.estimator.api.estimator' has no attribute 'SessionRunHook'.\r\n  My tensorflow,tensorboard,tensorflow-estimator all share the same version,1.14.0.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ntensorboard\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@GjyJason DO you have multiple versions of Tensorflow on your system? Also please refer to this following [link](https://blog.csdn.net/qq_18649781/article/details/89842430) where the cause of this problem is clearly explained. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been inactive for more than 14 days. Please add additional comments and we can reopen the issue again. Thanks!"]}, {"number": 31257, "title": "[Intel MKL] Bugfix: MklSlice op crash on 3d shapes", "body": "We found 3d shapes (NCDHW / NDHWC) are not well supported in MklSlice op, and causes crashes on 3d-CNN models such as https://github.com/Youngseok0001/Intel_benchmark/tree/master/3D_CNN.\r\n\r\nThis PR is to enable 3d support in MklSlice op.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31257) for more info**.\n\n<!-- need_author_consent -->", "@wenxizhu Please sign CLA in order to proceed with next steps. Thank you!", "Recall this PR for CLA not approved. We will fire a new PR once CLA is ready."]}, {"number": 31256, "title": "can contrib.model_pruning speed up old model? ", "body": "i wonder if when i prune my old model, can it speed up the old large dense model?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31255, "title": "how to do prediction after quantizing model", "body": "while i transformer ckpt to pb model by freeze and quantize model to compression_pb_model,  and then serve the compression_pb_model occuring the errors like this:\r\n\r\nValueError: NodeDef mentions attr 'batch_dims' not in Op<name=GatherV2; signature=params:Tparams, indices:Tindices, axis:Taxis -> output:Tparams; attr=Tparams:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; attr=Taxis:type,allowed=[DT_INT32, DT_INT64]>; NodeDef: {{node bert/embeddings/GatherV2}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, batch_dims=0](bert/embeddings/word_embeddings/read, bert/embeddings/Reshape, zeros/Const). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\nhowever, i run the code in the same meachine,,,thx,,,", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "> Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\ni find why the errors, sorry for reply delaying.\r\ni compile tensorflow from the source code, however, i compress model on the server which tensorflow version is 1.14 while i serve compressed_model on 1.13...."]}, {"number": 31254, "title": "[Intel MKL] Enable context-based graph rewrite", "body": "This PR enables context-based graph rewrite to make better decisions about\r\nrewriting an op to MKL op. This PR also cleans up rewrite logic used for\r\nelement-wise ops and unifies it with context-based graph rewrite. Unit tests\r\nare also added.\r\n\r\nAs a side note, a few changes are suggested by Clang format check, and are\r\nnot related to this PR.", "comments": []}, {"number": 31253, "title": "Using class_weights in fit_generator causes continuous increase in CPU memory usage until depletion. OOM", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not Relevant.\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): tf-nightly-gpu           1.15.0.dev20190728\r\n- Python version: Python 3.7.4 \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: release 10.0, V10.0.130\r\nNVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1  \r\n- GPU model and memory:\r\nGeForce RTX 2080Ti 11GB\r\n\r\n**Describe the current behavior**\r\nWhen using class_weights in fit_generator causes the training process to continuously consume more and more CPU RAM until depletion. There is a stepped increased in memory usage after each epoch. See below for the reproducible example.  To keep the reproducible example small, I decreased the size of the dataset and batch size, which shows the trend of increasing memory. While training with my actual data, it depletes the full 128GB RAM by 70 EPOCS.  \r\n\r\nIn the code below, if you comment out the class weights, the program trains without depleting memory. \r\n\r\n**Describe the expected behavior**\r\nNot leak memory. \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nimport numpy as np\r\n \r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import CuDNNLSTM, Dense\r\nfrom tensorflow.keras.optimizers import Adadelta\r\n\r\n\r\nfeature_count = 25\r\nbatch_size = 16\r\nlook_back = 5\r\ntarget_groups = 10\r\n\r\ndef random_data_generator( ):\r\n    x_data_size =(batch_size, look_back, feature_count) # batches, lookback, features\r\n    x_data = np.random.uniform(low=-1.0, high=5, size=x_data_size)\r\n \r\n    y_data_size = (batch_size, target_groups)\r\n    Y_data = np.random.randint(low=1, high=21, size=y_data_size)\r\n    \r\n    return x_data, Y_data\r\n \r\ndef get_simple_Dataset_generator():        \r\n    while True:\r\n        yield random_data_generator()\r\n\r\ndef build_model():\r\n    model = Sequential()\r\n    model.add(CuDNNLSTM(feature_count,\r\n                    batch_input_shape=(batch_size,look_back, feature_count),\r\n                    stateful=False))  \r\n    model.add(Dense(target_groups, activation='softmax'))\r\n    optimizer = Adadelta(learning_rate=1.0, epsilon=None) \r\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer) \r\n    return model\r\n\r\n\r\ndef run_training():\r\n   \r\n    model = build_model()\r\n    train_generator = get_simple_Dataset_generator()\r\n    validation_generator = get_simple_Dataset_generator()\r\n    class_weights = {0:2, 1:8, 2:1, 3:4, 4:8, 5:35, 6:30, 7:4, 8:5, 9:3}\r\n\r\n    model.fit_generator(generator = train_generator,\r\n            steps_per_epoch=1,\r\n            epochs=1000,            \r\n            verbose=2,\r\n            validation_data=validation_generator,\r\n            validation_steps=20,\r\n            max_queue_size = 10,\r\n            workers = 0, \r\n            use_multiprocessing = False,\r\n            class_weight = class_weights\r\n            )\r\n\r\nif __name__ == '__main__': \r\n    run_training()\r\n \r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n# Memory Usage When class_weights are used\r\n![Mem_with_class_weights](https://user-images.githubusercontent.com/46456896/62335750-aea84900-b492-11e9-9b23-72b29c6f2499.png)\r\n\r\n# Memory Without class_weights\r\n![Mem_wo_class_weights](https://user-images.githubusercontent.com/46456896/62335751-aea84900-b492-11e9-9f2a-ef9c296850ee.png)\r\n", "comments": ["Was able to reproduce the issue on Colab with Tf-nightly-gpu version. Please have a look at Colab [link](https://colab.research.google.com/drive/1udDjMNZ0AkbApQLy43fvQpXTvIih4Qlf). Thanks!", "Any workarounds or updates this? Thanks ", "I'm not able to reproduce this - can you update to the latest tf-nightly-gpu package and try again? Thanks!", "@talipini This was resolved in the `!pip install tf-nightly-gpu`\r\nPlease check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/20aeb317e65ecb8a0ec3e15812c41b10/tf31253_keras_memory_issue.ipynb). \r\n\r\nI am closing the issue here. Please feel free to open the issue if it persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31253\">No</a>\n", "Looks like new nightly build has fixed this issue. But training time is significantly longer (about 30%) in my case between 1.15.0.dev20190728 and 1.15.0.dev20190813. Same codebase takes 30% longer to train in the newer nightly build but it definitely seems to solve the memory issue though. I will try to create a reproducible example and open an new issue. Thanks for checking into this. \r\n\r\n", "@talipini Sure. Thanks for your support.", "I'm still having the same memory issue when using class_weights with Keras model.fit_generator. Running the stable version of TF 2.0.", "@TKassis Please open a new issue with details about your issue, platform details and a standalone code to reproduce the issue. Thanks!", "> @TKassis Please open a new issue with details about your issue, platform details and a standalone code to reproduce the issue. Thanks!\r\n\r\nThanks, it seems multiple people have reported the issue already.", "@TKassis do you have a link to the other issues? I am not fining them. Thank you!", "@talipini Thanks for the issue!\r\n\r\nCan you please try this again with the latest nightly (`pip install -U tf-nightly`) an report back? I believe this issue should be fixed in the latest nightly"]}, {"number": 31252, "title": "Model can't be checkpointed with Keras+MultiworkerMirroredStrategy", "body": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-18.0.0-x86_64-i386-64bit\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 2.0.0-beta1\r\nPython version:3.6.8\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nYou can collect some of this information using our environment capture\r\nscript\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" 2. TF 2.0: python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n\r\nDescribe the current behavior\r\nThis is a followup for https://github.com/tensorflow/tensorflow/issues/31070\r\n\r\nI tried following 2 solutions\r\n1.I applied 6345ad5\r\nto my tensorflow installed code\r\n2. I install latest nightly dev build\r\n\r\nboth gave me following error, seems though previous commit change data type to int64, somewhere else still expects int32\r\n\r\n```\r\n2019-08-01 22:41:51.971726: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at collective_ops.cc:354 : Internal: RecvBufResponse returned 8 bytes where to_tensor expected 4\r\nTraceback (most recent call last):\r\n  File \"example_tf2.py\", line 124, in <module>\r\n    steps_per_epoch = parallel_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n    return fn(instance, model, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 681, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 294, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py\", line 813, in execution_function\r\n    return [out.numpy() for out in distributed_function(input_fn)]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py\", line 416, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py\", line 359, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py\", line 1360, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py\", line 1648, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py\", line 1541, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 716, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py\", line 309, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 706, in wrapper\r\n    raise e.ag_error_metadata.to_exception(type(e))\r\ntensorflow.python.autograph.impl.api.StagingError: in converted code:\r\n\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py:804 distributed_function  *\r\n        outputs = strategy.experimental_run_v2(\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:708 experimental_run_v2\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1710 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:708 _call_for_each_replica\r\n        fn, args, kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:195 _call_for_each_replica\r\n        coord.join(threads)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py:389 join\r\n        six.reraise(*self._exc_info_to_raise)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\r\n        yield\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:926 run\r\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py:908 train_on_batch\r\n        output_loss_metrics=self._output_loss_metrics)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.py:307 train_on_batch\r\n        output_loss_metrics=output_loss_metrics))\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.py:260 _process_single_batch\r\n        model.trainable_weights))\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:434 apply_gradients\r\n        self._create_hypers()\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:608 _create_hypers\r\n        aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:770 add_weight\r\n        aggregation=aggregation)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/base.py:713 _add_variable_with_custom_getter\r\n        **kwargs_for_getter)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:154 make_variable\r\n        shape=variable_shape if variable_shape else None)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:260 __call__\r\n        return cls._variable_v1_call(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\r\n        shape=shape)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/shared_variable_creator.py:69 create_new_variable\r\n        v = next_creator(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1250 creator_with_resource_vars\r\n        return self._create_variable(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:368 _create_variable\r\n        _real_mirrored_creator, *args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:251 _create_mirrored_variable\r\n        value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:355 _real_mirrored_creator\r\n        v = next_creator(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope\r\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:264 __call__\r\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py:139 __init__\r\n        initial_value() if init_from_fn else initial_value,\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:330 _overridden_initial_value_fn\r\n        group_key, collective_instance_key)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/collective_ops.py:161 broadcast_recv\r\n        instance_key=instance_key)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_collective_ops.py:66 collective_bcast_recv\r\n        _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    /root/.local/lib/python2.7/site-packages/six.py:737 raise_from\r\n        raise value\r\n\r\n    InternalError: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]\r\n\r\n\r\n```\r\n\r\nDescribe the expected behavior\r\nKeras model could be checkpoint-ed under multi worker training\r\n\r\nCode to reproduce the issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport datetime\r\nimport json\r\nimport os\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport subprocess\r\nimport shlex\r\nimport sys\r\n\r\ntfds.disable_progress_bar()\r\n\r\nBUFFER_SIZE = 60000\r\nBATCH_SIZE = 64\r\n\r\nNUM_WORKERS = 2\r\nGLOBAL_BATCH_SIZE = NUM_WORKERS * BATCH_SIZE\r\n\r\nif __name__ == \"__main__\":\r\n  worker_addrs = ['localhost:9999', 'localhost:9998']\r\n  os.environ['TF_CONFIG'] = json.dumps({\r\n      'cluster': {\r\n          'worker': worker_addrs,\r\n      },\r\n      'task': {'type': 'worker', 'index': int(sys.argv[1])}\r\n  })\r\n\r\n  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  def build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=['accuracy'])\r\n    return model\r\n\r\n  datasets, info = tfds.load(name='mnist',\r\n                             with_info=True,\r\n                             as_supervised=True)\r\n\r\n  train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)\r\n\r\n  train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)\r\n\r\n  with strategy.scope():\r\n    multi_worker_model = build_and_compile_cnn_model()\r\n\r\n   \r\n  checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n        filepath='/tmp/chk.hdf5',\r\n        monitor='val_loss',\r\n        save_best_only=True,\r\n        load_weights_on_restart=True)\r\n\r\n  multi_worker_model.fit(x=train_datasets, epochs=100, callbacks = [checkpoint_callback])\r\n\r\n```\r\nOther info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@rchao ", "@jtang7, I don't seem to be able to repro this. Can you try tf nightly and see if it fixes the problem?", "Are we trying to fix this problem described in tf 2.0 doc?\r\n_At the time of writing, fault tolerance is supported when eager execution is disabled (see following code snippet for how this can be done). TensorFlow team is actively investing in making fault tolerance available in eager execution as well._\r\n\r\nhttps://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras#fault_tolerance\r\n", "@PigApple eager execution used to only affect worker failure-recovery, not regular training with `model.fit()`, plus now this is resolved (tutorial is now being updated). Would like to learn if this still happens with tf-nightly.", "@rchao Thanks! Will double check with nightly build and get back here", "@rchao, thanks for the reply. Besides tf-nightly, will there be a tf 2.0 new beta version soon or we will directly jump to GA?", "We're making progress for 2.0 release and feel free to join [TensorFlow announcement](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce) for the latest updates. Thanks!", "Using the MultiworkerMirrored example I\r\n\r\n**TF2.1.0**\r\n\r\nprint(\"Found Cluster spec \", cluster)\r\nprint(\"Found TFConfig\", tf_config)\r\nos.environ['TF_CONFIG'] = json.dumps(tf_config)\r\n\r\nBUFFER_SIZE = 1000\r\nBATCH_SIZE = 8\r\n\r\ndef make_datasets_unbatched():\r\n  #Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float16)\r\n    image /= 255\r\n    return image, label\r\n\r\n  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  return datasets['train'].map(scale, num_parallel_calls=tf.data.experimental.AUTOTUNE).cache().repeat()\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n\r\nprint(\"Created strategy \" , strategy)\r\nNUM_WORKERS = strategy.num_replicas_in_sync\r\nprint(\"Number of workers \" ,NUM_WORKERS )\r\nGLOBAL_BATCH_SIZE = 1 * NUM_WORKERS\r\n\r\nprint(\"Creating datasets inside scope...\")\r\ntrain_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\nprint(\"Created datasets \" , train_datasets)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA  # AutoShardPolicy.OFF can work too.\r\ntrain_datasets_no_auto_shard = train_datasets.with_options(options)\r\n\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(28,28,1)),\r\n      tf.keras.layers.Dense(256, activation='relu'),\r\n      tf.keras.layers.Dropout(0.5),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.Adam(),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\nif my_task_index == 0:\r\n  callbacks = [tf.keras.callbacks.TensorBoard(log_dir='logs'), tf.keras.callbacks.ModelCheckpoint(filepath=\"models\", save_best_only=False)]\r\nelse:\r\n  callbacks = None\r\n\r\nwith strategy.scope():\r\n  print(\"Creating model inside scope...\")\r\n  model = build_and_compile_cnn_model()\r\n  print(model.summary())\r\n\r\n\r\n\r\nprint(\"Starting to fit model....\")\r\nverbose = 1 if my_task_index == 0 else 0\r\nmodel.fit(x=train_datasets_no_auto_shard, epochs=100, steps_per_epoch=150, callbacks = callbacks , verbose=verbose)\r\nprint(\"Model fit completed\")\r\n\r\nYields .....which goes away if your disable Modelcheckpoint.\r\nStarting to fit model....\r\nTraceback (most recent call last):\r\n  File \"main_2.py\", line 72, in <module>\r\n    model.fit(x=train_datasets_no_auto_shard, epochs=100, steps_per_epoch=150, callbacks = callbacks , verbose=verbose)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 694, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 201, in _call_for_each_replica\r\n    coord.join(threads)\r\nTraceback (most recent call last):\r\n  File \"main_2.py\", line 72, in <module>\r\n    model.fit(x=train_datasets_no_auto_shard, epochs=100, steps_per_epoch=150, callbacks = callbacks , verbose=verbose)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 273, in _process_single_batch\r\n    model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 433, in apply_gradients\r\n    self._create_hypers()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 655, in _create_hypers\r\n    aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 817, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/shared_variable_creator.py\", line 69, in create_new_variable\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 694, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1330, in creator_with_resource_vars\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 548, in _create_variable\r\n    values.SyncOnReadVariable, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py\", line 1034, in create_mirrored_variable\r\n    value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 540, in _real_mirrored_creator\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 201, in _call_for_each_replica\r\n    coord.join(threads)\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 485, in variable_capturing_scope\r\n    lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 273, in _process_single_batch\r\n    model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 433, in apply_gradients\r\n    self._create_hypers()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 655, in _create_hypers\r\n    aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 817, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/shared_variable_creator.py\", line 69, in create_new_variable\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1330, in creator_with_resource_vars\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 548, in _create_variable\r\n    values.SyncOnReadVariable, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py\", line 1034, in create_mirrored_variable\r\n    value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 540, in _real_mirrored_creator\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 485, in variable_capturing_scope\r\n    lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 178, in __init__\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 383, in initial_value_fn\r\n    collective_instance_key)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/collective_ops.py\", line 176, in broadcast_recv\r\n    communication_hint=communication_hint.lower())\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_collective_ops.py\", line 57, in collective_bcast_recv\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 178, in __init__\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 383, in initial_value_fn\r\n    collective_instance_key)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/collective_ops.py\", line 176, in broadcast_recv\r\n    communication_hint=communication_hint.lower())\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_collective_ops.py\", line 57, in collective_bcast_recv\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\n**tensorflow.python.framework.errors_impl.InternalError: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]**\r\n", "@ianferreira is it possible for you to test this again on TF nightly? (or 2.2rc) We think there was a fix that was made after 2.1 that might have fixed this issue. ", "This is still an issue in 2.2.0-rc4. I ran into the same error:\r\n```\r\n2020-05-16 21:41:22.789209: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at collective_ops.cc:401 : Internal: RecvBufResponse returned 8 bytes where to_tensor expected 4\r\nINFO:tensorflow:Error reported to Coordinator: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]\r\n```\r\nTried to enable checkpoints through `tf.keras.callbacks.ModelCheckpoint` while running with `MultiWorkerMirroredStrategy`. This fails deterministically at the end of the first epoch. The error went away when I remove the callback.", "@guptapriya could you point me to the commit that you think might have fixed this? I want to see if I can cherry-pick it.", "Hi @andrewor14, if you are still facing this issue can you please provide reproducible code + stack trace so we can try to debug? Thanks.", "```\r\nfrom datetime import datetime\r\nfrom packaging import version\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport json\r\n# Create a TensorBoard callback\r\nlogs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\r\n                                                 histogram_freq = 1,\r\n                                                 profile_batch = '2048')\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"node67:12345\", \"node68:23456\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})\r\ndef mnist_dataset(batch_size):\r\n    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n    # The `x` arrays are in uint8 and have values in the range [0, 255].\r\n    # We need to convert them to float32 with values in the range [0, 1]\r\n    x_train = x_train / np.float32(255)\r\n    y_train = y_train.astype(np.int64)\r\n    train_dataset = tf.data.Dataset.from_tensor_slices(\r\n      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size).prefetch(100)\r\n    return train_dataset\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n      tf.keras.Input(shape=(28, 28)),\r\n      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n      tf.keras.layers.Conv2D(256, 2, activation='relu'),\r\n      tf.keras.layers.Conv2D(128, 2, activation='relu'),\r\n      tf.keras.layers.Conv2D(32, 1, activation='relu'),  \r\n      tf.keras.layers.Conv2D(32, 2, activation='relu'),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(2048, activation='relu'),        \r\n      tf.keras.layers.Dense(1024, activation='relu'),        \r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n    ])\r\n    model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n      metrics=['accuracy'])\r\n    return model\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\nnum_workers = 2\r\nper_worker_batch_size = 2048\r\n# Here the batch size scales up by number of workers since \r\n# `tf.data.Dataset.batch` expects the global batch size. Previously we used 64, \r\n# and now this becomes 128.\r\nglobal_batch_size = per_worker_batch_size * num_workers\r\nmulti_worker_dataset = mnist_dataset(global_batch_size)\r\n\r\nwith strategy.scope():\r\n  multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(multi_worker_dataset, epochs=103, steps_per_epoch=70,callbacks = [tboard_callback])\r\n```\r\n\r\n\r\n```\r\n@ @---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-7-402c6ad248bd> in <module>\r\n     14 # number of steps per epoch. Note that the numbers here are for demonstration\r\n     15 # purposes only and may not sufficiently produce a model with good quality.\r\n---> 16 multi_worker_model.fit(multi_worker_dataset, epochs=103, steps_per_epoch=70,callbacks = [tboard_callback])\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     73         lambda _: method(self, *args, **kwargs),\r\n     74         self.distribute_strategy,\r\n---> 75         mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n     76 \r\n     77   return tf_decorator.make_decorator(\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py in run_distribute_coordinator(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)\r\n    851         # All jobs run `worker_fn` if between-graph.\r\n    852         return _run_single_worker(worker_fn, strategy, cluster_spec, task_type,\r\n--> 853                                   task_id, session_config, rpc_layer)\r\n    854       else:\r\n    855         # Only one node runs `worker_fn` if in-graph.\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py in _run_single_worker(worker_fn, strategy, cluster_spec, task_type, task_id, session_config, rpc_layer, worker_barrier, coord)\r\n    358         return worker_fn(strategy)\r\n    359     else:\r\n--> 360       return worker_fn(strategy)\r\n    361 \r\n    362 \r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in <lambda>(_)\r\n     71 \r\n     72     return dc.run_distribute_coordinator(\r\n---> 73         lambda _: method(self, *args, **kwargs),\r\n     74         self.distribute_strategy,\r\n     75         mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    828       self.stop_training = False\r\n    829       train_function = self.make_train_function()\r\n--> 830       callbacks.on_train_begin()\r\n    831       # Handle fault-tolerance for multi-worker.\r\n    832       # TODO(omalleyt): Fix the ordering issues that mean this has to\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in on_train_begin(self, logs)\r\n    445     logs = self._process_logs(logs)\r\n    446     for callback in self.callbacks:\r\n--> 447       callback.on_train_begin(logs)\r\n    448 \r\n    449   def on_train_end(self, logs=None):\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in on_train_begin(self, logs)\r\n   1948 \r\n   1949   def on_train_begin(self, logs=None):\r\n-> 1950     self._init_batch_steps()\r\n   1951     if self._start_batch == 1:\r\n   1952       self._enable_trace()\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py in _init_batch_steps(self)\r\n   1893       self._total_batches_seen = {\r\n   1894           self._train_run_name: variables.Variable(0, dtype='int64'),\r\n-> 1895           self._validation_run_name: variables.Variable(0, dtype='int64')\r\n   1896       }\r\n   1897     else:\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    259       return cls._variable_v1_call(*args, **kwargs)\r\n    260     elif cls is Variable:\r\n--> 261       return cls._variable_v2_call(*args, **kwargs)\r\n    262     else:\r\n    263       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n    253         synchronization=synchronization,\r\n    254         aggregation=aggregation,\r\n--> 255         shape=shape)\r\n    256 \r\n    257   def __call__(cls, *args, **kwargs):\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in getter(**kwargs)\r\n     64 \r\n     65   def getter(**kwargs):\r\n---> 66     return captured_getter(captured_previous, **kwargs)\r\n     67 \r\n     68   return getter\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in creator_with_resource_vars(next_creator, **kwargs)\r\n   1765         kwargs[\"initial_value\"] = kwargs[\"initial_value\"].wrapped_value\r\n   1766 \r\n-> 1767       return self._create_variable(next_creator, **kwargs)\r\n   1768 \r\n   1769     def distributed_getter(getter, *args, **kwargs):\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _create_variable(self, next_creator, **kwargs)\r\n    608                                            _real_mirrored_creator,\r\n    609                                            values.MirroredVariable,\r\n--> 610                                            values.SyncOnReadVariable, **kwargs)\r\n    611 \r\n    612   def _validate_colocate_with_variable(self, colocate_with_variable):\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/values.py in create_mirrored_variable(strategy, real_mirrored_creator, mirrored_cls, sync_on_read_cls, **kwargs)\r\n    692   # here.\r\n    693   with tape.stop_recording():\r\n--> 694     value_list = real_mirrored_creator(**kwargs)\r\n    695     var_cls = sync_on_read_cls if is_sync_on_read else mirrored_cls\r\n    696     result = var_cls(strategy, value_list, aggregation)\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py in _real_mirrored_creator(**kwargs)\r\n    600             # variable creation.\r\n    601             with tape.stop_recording():\r\n--> 602               v = next_creator(**kwargs)\r\n    603           assert not isinstance(v, values.DistributedVariable)\r\n    604           value_list.append(v)\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)\r\n    234                         shape=None):\r\n    235     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 236     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n    237     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n    238       previous_getter = _make_getter(getter, previous_getter)\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n   2645       synchronization=synchronization,\r\n   2646       aggregation=aggregation,\r\n-> 2647       shape=shape)\r\n   2648 \r\n   2649 \r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    261       return cls._variable_v2_call(*args, **kwargs)\r\n    262     else:\r\n--> 263       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    264 \r\n    265 \r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n   1432           aggregation=aggregation,\r\n   1433           shape=shape,\r\n-> 1434           distribute_strategy=distribute_strategy)\r\n   1435 \r\n   1436   def _init_from_args(self,\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n   1565           with ops.name_scope(\"Initializer\"), device_context_manager(None):\r\n   1566             initial_value = ops.convert_to_tensor(\r\n-> 1567                 initial_value() if init_from_fn else initial_value,\r\n   1568                 name=\"initial_value\", dtype=dtype)\r\n   1569           if shape is not None:\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py in initial_value_fn()\r\n    384                                                    initial_value.dtype,\r\n    385                                                    group_size, group_key,\r\n--> 386                                                    collective_instance_key)\r\n    387           return initial_value\r\n    388 \r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/collective_ops.py in broadcast_recv(shape, dtype, group_size, group_key, instance_key, communication_hint)\r\n    174       group_key=group_key,\r\n    175       instance_key=instance_key,\r\n--> 176       communication_hint=communication_hint.lower())\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_collective_ops.py in collective_bcast_recv(T, group_size, group_key, instance_key, shape, communication_hint, name)\r\n     55         pass  # Add nodes to the TensorFlow graph.\r\n     56     except _core._NotOkStatusException as e:\r\n---> 57       _ops.raise_from_not_ok_status(e, name)\r\n     58   # Add nodes to the TensorFlow graph.\r\n     59   T = _execute.make_type(T, \"T\")\r\n\r\n~/miniconda2/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6651   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6652   # pylint: disable=protected-access\r\n-> 6653   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6654   # pylint: enable=protected-access\r\n   6655 \r\n\r\n~/.local/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: RecvBufResponse returned 4 bytes where to_tensor expected 8 [Op:CollectiveBcastRecv]\r\n\r\n```\r\n\u200b", "Thanks for providing this example @nimaaghli. I'm running this script in 2.2 but it seems to be training without problems, though I haven't made it to the last epoch yet. Does this fail right away for you, or after a certain number of epochs? What TF version are you using?", "I had the Tensorboard configured only on one worker and thought I can show results from only one worker. After replicating  Tensorboard callback code on other workers as well, the error goes away.", "Great! I'll close this issue now since checkpointing with Keras and MWMS is now working. Feel free to reopen if anything feels unresolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31252\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31252\">No</a>\n", "@nikitamaia could you point me to the commit(s) that may have fixed this? I'd like to see if I can cherry-pick them into my branch if necessary.", "I am facing the same issue with 2.3 tf\r\n", "For anyone who is struggling with `RecvBufResponse returned 4 bytes where to_tensor expected 8`.\r\n\r\nTry disable all your callbacks first. \r\n\r\n(If it works for you, pls add a `+1` reaction ;)", "For those who are still stuck on this error, I was able to get rid of it by making sure every worker has the same set of callbacks. In my case, previously I only added the `ModelCheckpoint` callback on the first worker, e.g.\r\n```\r\nif is_master:\r\n  callbacks.add(tf.keras.callbacks.ModelCheckpoint(...))\r\n```\r\nThis caused the `RecvBufResponse` error in TF 2.2.0 and caused hanging in TF 2.4.0. Removing the `is_master` check solved the problem for me. IMO this requirement should be better documented, or at least the error message should be more descriptive."]}, {"number": 31251, "title": "Add _FusedBatchNormEx to auto_mixed_precision list", "body": "This op is added by the remapper grappler pass, which runs before auto_mixed_precision.\r\n\r\nattn. @reedwm \r\ncc. @nluehr ", "comments": []}, {"number": 31250, "title": "google.protobuf.json_format.MessageToDict incorrectly decodes bytes_list", "body": "**System information**\r\n\r\n> - Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYES, custom code.\r\n> - OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\nCentOS 7-1810 in VirtualBox VM\r\n```\r\nuname -a\r\nLinux centos7-1810 3.10.0-957.10.1.el7.x86_64 #1 SMP Mon Mar 18 15:06:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n> - TensorFlow installed from (source or binary):\r\n\r\nBinary.  Unchanged from tf-nightly.\r\n> - TensorFlow version (use command below):\r\n\r\nNote that a warning is printed if I use tf.GIT_VERSION.\r\n```\r\nprint(tf.version.GIT_VERSION, tf.VERSION) \r\nv1.12.1-7396-g12481e7e74 1.15.0-dev20190730\r\n```\r\n> - Python version:\r\n\r\nPython 3.6.8 :: Anaconda, Inc.\r\n\r\n**Describe the current behavior**\r\ngoogle.protobuf.json_format.MessageToDict, I believe \"decodes\" bytes_list incorrectly.  What I get is:\r\n`['VGF4aSBBZmZpbGlhdGlvbiBTZXJ2aWNlcw==']`\r\n\r\n**Describe the expected behavior**\r\ngoogle.protobuf.json_format.MessageToDict should decode my byte_list to:\r\n`[b'Taxi Affiliation Services']`\r\n, which is what I get when looking directly at the tf.train.Example.\r\n\r\nData is from the chicago_taxi_pipeline demo at:\r\nhttps://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline\r\n\r\n\"data_tfrecord-00000-of-00001\" is the \"eval\" output of:\r\ntfx.components.example_gen.csv_example_gen.component.CsvExampleGen\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport google.protobuf.json_format as jfmt\r\n\r\nprint(tf.__version__)\r\nexample = next(tf.python_io.tf_record_iterator(\"data_tfrecord-00000-of-00001\"))\r\nresult = tf.train.Example.FromString(example)\r\n\r\ncompany_name_direct = result.features.feature['company'].bytes_list.value\r\nprint(company_name_direct)\r\n\r\nresult_as_dict = jfmt.MessageToDict(result)\r\ncompany_name_indirect = result_as_dict['features']['feature']['company']['bytesList']['value'] \r\nprint(company_name_indirect)\r\n```\r\n```\r\n# output:\r\n1.15.0-dev20190730\r\n[b'Taxi Affiliation Services']\r\n['VGF4aSBBZmZpbGlhdGlvbiBTZXJ2aWNlcw==']\r\n```\r\n\r\n**Other info / logs**\r\nI have not tested this on any other build except 1.15 nightly.\r\n", "comments": ["[data_tfrecord-00000-of-00001.gz](https://github.com/tensorflow/tensorflow/files/3458925/data_tfrecord-00000-of-00001.gz)\r\n", "I have tried on Jupyter notebook with TF version 1.15.0-dev20190802 and 1.14 was able to reproduce the issue. Please, find the zip file. Thanks!\r\n[protobuf.tar.gz](https://github.com/tensorflow/tensorflow/files/3461295/protobuf.tar.gz)\r\n\r\n", "Thanks for taking this on.  I suspect it should actually be in the https://github.com/protocolbuffers/protobuf project, but it would have been hard to create a test case which didn't use `tf.train.Example`", "@robertlugg We see you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]