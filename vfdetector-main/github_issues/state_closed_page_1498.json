[{"number": 7998, "title": "Strange messages shown up when define a tensorflow variable", "body": "When I try to define a variable, the following messages show. \r\nHowever, these messages will only show when I define the first variable.  \r\n\r\n\r\nPython 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> bias = tf.Variable(tf.random_normal([1]))\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n", "comments": ["I'm getting the same problem with pretty much the exact same error message.  This issue was covered here in issue #7500 with the advice being to upgrade to the nightly build.  I've done that but it hasn't made any difference.", "Upgrading to the most recent [nightly build](https://ci.tensorflow.org/view/Nightly/job/nightly-win/DEVICE=cpu,OS=windows/) just fixed it for me.  Now I'm just getting the SSE warnings...", "@xarxziux Thanks for the redirect!  And note that the SSE warnings are innocuous.\r\n\r\nI'm closing this out, since this is a duplicate of #7500 .\r\n\r\n"]}, {"number": 7997, "title": "I can successfully open cupti64_80.dll library. But It seems the program run all the time and will never stop.", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System: Windows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n`C:\\Users\\Jun Xiao>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2016 NVIDIA Corporation\r\nBuilt on Sat_Sep__3_19:05:48_CDT_2016\r\nCuda compilation tools, release 8.0, V8.0.44`\r\n\r\n1. A link to the pip package you installed:\r\n`C:\\Users\\Jun Xiao>pip3.5 install --upgrade tensorflow-gpu\r\nRequirement already up-to-date: tensorflow-gpu in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\r\nRequirement already up-to-date: protobuf>=3.1.0 in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: six>=1.10.0 in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: numpy>=1.11.0 in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: wheel>=0.26 in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from tensorflow-gpu)\r\nRequirement already up-to-date: setuptools in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from protobuf>=3.1.0->tensorflow-gpu)\r\nRequirement already up-to-date: appdirs>=1.4.0 in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu)\r\nRequirement already up-to-date: packaging>=16.8 in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu)\r\nRequirement already up-to-date: pyparsing in c:\\users\\jun xiao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu)`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n`>>> import tensorflow\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:135] successfully opened CUDA library curand64_80.dll locally\r\n>>> print(tensorflow.__version__)\r\n1.0.0`\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n`I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:841] OptimizeLoss/gradients/generator/fully_connected/BatchNorm/moments/normalize/Square_grad/mul/x: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nOptimizeLoss/gradients/generator/fully_connected/BatchNorm/moments/sufficient_statistics/SquaredDifference_grad/scalar:\r\n(Const): /job:localhost/replica:0/task:0/gpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:841] OptimizeLoss/gradients/generator/fully_connected/BatchNorm/moments/sufficient_statistics/SquaredDifference_grad/scalar: (Const)/job:localhost/replica:0/task:0/gpu:0\r\nOptimizeLoss/train/value: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\simple_placer.cc:841] OptimizeLoss/train/value: (Const)/job:localhost/replica:0/task:0/cpu:0\r\n[I 13:13:00.712 NotebookApp] Saving file at /WGAN-tensorflow-master/WGAN.ipynb\r\nsuccessfully opened CUDA library cupti64_80.dll locally`\r\n\r\n`KeyboardInterrupt                         Traceback (most recent call last)\r\n<ipython-input-10-58ca95c5b364> in <module>()\r\n----> 1 main()\r\n\r\n<ipython-input-9-221574c19227> in main()\r\n     32                 citers = Citers\r\n     33             for j in range(citers):\r\n---> 34                 feed_dict = next_feed_dict()\r\n     35                 if i % 100 == 99 and j == 0:\r\n     36                     run_options = tf.RunOptions(\r\n\r\n<ipython-input-9-221574c19227> in next_feed_dict()\r\n     20                                mode='constant', constant_values=-1)\r\n     21             train_img = np.expand_dims(train_img, -1)\r\n---> 22         batch_z = np.random.normal(0, 1, [batch_size, z_dim])             .astype(np.float32)\r\n     23         feed_dict = {real_data: train_img, z: batch_z}\r\n     24         return feed_dic`\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n I google for the problem. At first, I can't open \"not find cuptiActivityRegisterCallbacksin libcupti DSO\", and I adopt the idea from the [\"menggangmark\".](https://github.com/tensorflow/tensorflow/issues/6235)\r\n and it can successfully open the cupti64_80 library. \r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n", "comments": ["cupti64_80 library is just an information message. As far as I can tell, it has no relation whatsoever to the issue you are reporting.  Also I do not really see clear instructions for us to reproduce the issue you are facing. You seem to have just copy pasted your error message there.\r\n\r\nIt looks to me like your training code may not be converging. I recommend adding print statements to see what your code is doing.", "Looks like you also filed the issue #7890\r\nAre these different issues or the same issues?", "Closing due to lack of activity."]}, {"number": 7996, "title": "Cant  implement Miking Human", "body": "I want to implement a neural architecture from  A Neural Architecture Mimicking Humans\r\nEnd-to-End for Natural Language Inference . In keras , I can get accuracy  86% . But the same  architecture in tensorflow , just get 76% . Here is the main code \uff1a\r\n`class miki(BaseModel):\r\n\r\n    def build(self):\r\n        params = self.params\r\n        batch_size = params.batch_size\r\n        max_length = params.max_length\r\n        nr_hidden = params.nr_hidden\r\n        keep_dr=params.dr\r\n\r\n\r\n        ids1 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='premise')  # none l1\r\n        ids2 = tf.placeholder(tf.int32, shape=[batch_size, max_length], name='hypoyhesis')\r\n        lable = tf.placeholder(tf.float32, shape=[batch_size, 3], name='lable')\r\n        train_dr = tf.placeholder(tf.bool)\r\n\r\n\r\n        oov_W = np.load(open(os.path.join('glove/', 'oov_W.weights'), 'rb'))\r\n        oov_W = oov_W.astype('float32')\r\n        unchanged_W = np.load(open(os.path.join('glove/', 'unchanged_W.weights'), 'rb'))\r\n        unchanged_W = unchanged_W.astype('float32')\r\n        embedding = np.concatenate((oov_W, unchanged_W), axis=0)\r\n        W = tf.get_variable(name=\"W\", shape=embedding.shape, initializer=tf.constant_initializer(embedding),\r\n                            trainable=False)\r\n\r\n        def he_nomal(fan_in):\r\n            s = np.sqrt(2. / fan_in)\r\n            seed = np.random.randint(10e8)\r\n            return tf.random_normal_initializer(0.0, s, dtype=tf.float32, seed=seed)\r\n\r\n        with tf.variable_scope('embedding1'):\r\n            embed1 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\r\n            em_reshape1=tf.reshape(embed1,[-1,nr_hidden])\r\n            network_en1 = tl.layers.InputLayer(inputs=em_reshape1, name='em_layer1-1')\r\n            network_en1 = tl.layers.DropoutLayer(network_en1, is_fix=True, keep=keep_dr, name='emdrop1-1',is_train=train_dr)\r\n            network_en1 = tl.layers.DenseLayer(network_en1, n_units=nr_hidden,\r\n                                           act=tf.nn.relu, name='emrelu2-1')\r\n            embed1_out=network_en1.outputs\r\n            embed1_re=tf.reshape(embed1_out,[-1,max_length,nr_hidden])\r\n\r\n\r\n        with tf.variable_scope('embedding2'):\r\n            embed2 = tf.nn.embedding_lookup(W, ids1)  # none l1  h\r\n            em_reshape2=tf.reshape(embed2,[-1,nr_hidden])\r\n            network_en2 = tl.layers.InputLayer(inputs=em_reshape2, name='em_layer2-1')\r\n            network_en2 = tl.layers.DropoutLayer(network_en2, is_fix=True, keep=keep_dr, name='emdrop2-1',is_train=train_dr)\r\n            network_en2 = tl.layers.DenseLayer(network_en2, n_units=nr_hidden,\r\n                                           act=tf.nn.relu, name='emrelu1-1')\r\n            embed2_out=network_en2.outputs\r\n            embed2_re=tf.reshape(embed2_out,[-1,max_length,nr_hidden])\r\n\r\n\r\n        _seq_len = tf.fill(tf.expand_dims(batch_size, 0),\r\n                           tf.constant(max_length, dtype=tf.int32))\r\n\r\n        with tf.variable_scope('ecode1'):\r\n            fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            h, _ = tf.nn.bidirectional_dynamic_rnn(\r\n                cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed1_re, sequence_length=(_seq_len), dtype=tf.float32)\r\n            encode1 = tf.concat(2, h)  # none l 2h\r\n\r\n        with tf.variable_scope('ecode2'):\r\n            fwd_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            back_cell = tf.nn.rnn_cell.BasicLSTMCell(nr_hidden)\r\n            h, _ = tf.nn.bidirectional_dynamic_rnn(\r\n                cell_fw=fwd_cell, cell_bw=back_cell, inputs=embed2_re, sequence_length=(_seq_len), dtype=tf.float32)\r\n            encode2 = tf.concat(2, h)  # none l 2h\r\n\r\n        with tf.variable_scope('atte_layer'):\r\n\r\n            encode2_tr = tf.transpose(encode2, perm=[0, 2, 1])  # none 2h l2\r\n            attention = tf.batch_matmul(encode1, encode2_tr)  # none l1 l2\r\n            e = tf.exp(attention - tf.reduce_max(attention, 2, keep_dims=True))\r\n            s = tf.reduce_sum(e, 2, keep_dims=True)  #none l1 1\r\n            am_att = e / s  #none l1 l2\r\n            aligh_attention = tf.batch_matmul(am_att, encode2)  # none l1 2h\r\n            concat = tf.concat(2, [aligh_attention, encode1])  # none l1 4h\r\n            concat_reshape=tf.reshape(concat,[-1,4*nr_hidden])    #none*l1 4h\r\n\r\n\r\n        with tf.variable_scope('task1_operator'):\r\n            network_task1=tl.layers.InputLayer(concat_reshape, name='task_layer1-1')\r\n            network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-1',is_train=train_dr,is_fix=True)\r\n            network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-1')\r\n            network_task1 = tl.layers.DropoutLayer(network_task1, keep=keep_dr, name='drop1-2',is_train=train_dr,is_fix=True)\r\n            network_task1 = tl.layers.DenseLayer(network_task1, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu1-2')\r\n            task1 = network_task1.outputs  # none*l 2h\r\n            task1_re = tf.reshape(task1, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n        with tf.variable_scope('task2_operator'):\r\n            network_task2=tl.layers.InputLayer(concat_reshape, name='task_layer2-1')\r\n            network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-1',is_train=train_dr,is_fix=True)\r\n            network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-1')\r\n            network_task2 = tl.layers.DropoutLayer(network_task2, keep=keep_dr, name='drop2-2',is_train=train_dr,is_fix=True)\r\n            network_task2 = tl.layers.DenseLayer(network_task2, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu2-2')\r\n            task2 = network_task2.outputs  # none*l 2h\r\n            task2_re = tf.reshape(task2, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n        with tf.variable_scope('task3_operator'):\r\n            network_task3=tl.layers.InputLayer(concat_reshape, name='task_layer3-1')\r\n            network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-1',is_train=train_dr,is_fix=True)\r\n            network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-1')\r\n            network_task3 = tl.layers.DropoutLayer(network_task3, keep=keep_dr, name='drop3-2',is_train=train_dr,is_fix=True)\r\n            network_task3 = tl.layers.DenseLayer(network_task3, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu3-2')\r\n            task3 = network_task3.outputs  # none*l 2h\r\n            task3_re = tf.reshape(task3, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n        with tf.variable_scope('task4_operator'):\r\n            network_task4=tl.layers.InputLayer(concat_reshape, name='task_layer4-1')\r\n            network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-1',is_train=train_dr,is_fix=True)\r\n            network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-1')\r\n            network_task4 = tl.layers.DropoutLayer(network_task4, keep=keep_dr, name='drop4-2',is_train=train_dr,is_fix=True)\r\n            network_task4 = tl.layers.DenseLayer(network_task4, n_units=2 * nr_hidden, act=tf.nn.relu, name='relu4-2')\r\n            task4 = network_task4.outputs  # none*l 2h\r\n            task4_re = tf.reshape(task4, [-1, max_length, 2 * nr_hidden])  #none l h\r\n\r\n\r\n        with tf.variable_scope('gate_operator'):\r\n            network_gate= tl.layers.InputLayer(concat_reshape, name='gate_layer')\r\n            network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop1',is_train=train_dr)\r\n            network_gate = tl.layers.DenseLayer(network_gate, n_units=2 * nr_hidden,\r\n                                                act=tf.nn.relu, name='relu_gate1')\r\n            network_gate = tl.layers.DropoutLayer(network_gate, is_fix=True, keep=keep_dr, name='gate_drop2',is_train=train_dr)\r\n            network_gate = tl.layers.DenseLayer(network_gate, n_units=4,\r\n                                                act=tf.nn.softmax, name='relu_gate2')\r\n            gate = network_gate.outputs\r\n            gate_re = tf.reshape(gate, [-1, max_length, 4])  # none l 2\r\n\r\n        def repeat(gate_i):\r\n\r\n            gate_a = gate_i  # none l\r\n            gate_b = tf.tile(gate_a, [1, 2 * nr_hidden])\r\n            gate_c = tf.reshape(gate_b, [-1, 2 * nr_hidden, max_length])\r\n            gate_d = tf.transpose(gate_c, perm=[0, 2, 1])\r\n            return gate_d  # none l 2h\r\n\r\n        def Out(t1,t2,t3,t4,gate):\r\n            gate_shuffle = tf.transpose(gate, perm=[0, 2, 1])  # none 4 l\r\n\r\n            g1 = repeat(gate_shuffle[:, 0, :])  # none l\r\n            g2 = repeat(gate_shuffle[:, 1, :])\r\n            g3 = repeat(gate_shuffle[:, 2, :])  # none l\r\n            g4 = repeat(gate_shuffle[:, 3, :])\r\n\r\n            O1=g1*t1\r\n            O2 = g2 * t2\r\n            O3=g3*t3\r\n            O4 = g4 * t4\r\n\r\n            out = O1 + O2 +O3+O4\r\n            return out\r\n\r\n        task_output = Out(task1_re,task2_re,task3_re,task4_re,gate_re)  # none l1 2h\r\n\r\n\r\n        with tf.variable_scope('aggregate'):\r\n            fwd_lstm = tf.nn.rnn_cell.BasicLSTMCell(2 * nr_hidden)\r\n            x_output, x_state = tf.nn.dynamic_rnn(cell=fwd_lstm, inputs=task_output, dtype=tf.float32,sequence_length=(_seq_len))\r\n\r\n            composable = x_output[:,-1, :]  # none  2h\r\n\r\n        with tf.variable_scope('Entaiment'):\r\n            network3 = tl.layers.InputLayer(composable, name='layer_entalment1')\r\n            network3 = tl.layers.DropoutLayer(network3, keep=keep_dr, name='entaidrop3',is_train=train_dr,is_fix=True)\r\n            network3 = tl.layers.DenseLayer(network3, n_units=2 * nr_hidden,\r\n                                            act=tf.nn.tanh, name='layer_entaiment2')\r\n            network3 = tl.layers.DenseLayer(network3, n_units=3,\r\n                                            act=tf.nn.softmax, name='entai_relu3.2')\r\n            entaiment = network3.outputs\r\nwith tf.name_scope('Loss'):\r\n            cross_entropy = tf.reduce_mean(-tf.reduce_sum(lable * tf.log(entaiment), reduction_indices=[1]))\r\n\r\n            loss = cross_entropy\r\n\r\n        with tf.variable_scope('Accuracy'):\r\n            predicts = tf.cast(tf.argmax(entaiment, 1), 'int32')   #entaiment none 3\r\n            lable_one = tf.cast(tf.argmax(lable, 1), 'int32')\r\n            corrects = tf.equal(predicts, lable_one)\r\n            num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\r\n            accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\r\n\r\n        optimizer = tf.train.AdamOptimizer(params.learning_rate)\r\n        opt_op = optimizer.minimize(loss, global_step=self.global_step)\r\n`", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7995, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "Traceback (most recent call last):\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"d:\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"d:\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"d:\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"d:\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"d:\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n====\r\nFinally,I find the **solution**:\r\nInstall [Microsoft Visual C++ 2015 Redistributable Update 3 x64.](https://www.microsoft.com/en-us/download/details.aspx?id=53587)\r\n", "comments": ["@flyi Thanks for posting your problems and the fix!\r\n\r\nI'm closing this out.  If you have any other problems, feel free to open a new issue.  Thanks!", "that link returned an no webpage found error.", "@specman \r\nYes,that link returned 404. So I post my solution.", "I have similar problem. After installation of MS VC++ 2015 Redistributable Update 3 x64, the problem still exists. I also double checked the CUDA and CUDNN path issue mentioned in [7705](https://github.com/tensorflow/tensorflow/issues/7705). However, it still does not work.\r\nCan anyone help? Thank you. \r\n\r\nMy environment:\r\nWindows 7 64 bit\r\nPython 3.5.3\r\nTensorflow GPU 1.10rc1\r\nCuda 8.0\r\nCudnn 6.0\r\n\r\n---------------Update--------------------\r\nIt's CuDNN 6's problem, which does NOT work on my environment! When I switch to Cudnn5.1, everything works fine! Thanks to @ZacDiggum who found the problem at [9066](https://github.com/tensorflow/tensorflow/issues/9066)\r\n\r\nHere are the error message when I \"import tensorflow\":\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Programs\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_mod\r\nule\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Programs\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_mod\r\nule\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24,\r\n in <module>\r\n    from tensorflow.python import *\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", l\r\nine 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Programs\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_mod\r\nule\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\now_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Programs\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_mod\r\nule\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "Same here, didn't work for me either. Can we please re-open this? @tatatodd ", "I just downloaded Anaconda and got it through there. fwiw.", "@hughlomas Hi, have you tried to use cuDNN 5.1 instead of the newest 6.x? Also, please try to use python 3.5, but not 3.6. These work for me.", "Hello,\n\n        I'm not using GPU instead I'm using CPU installation so there is no need of any cuDNN in it. Please visit the code again and have a review over it.\n\nThanks & Regards,\n\n\n\nRAJAT KUMAR\n\nTechnical Trainee\n\nFUJITSU CONSULTING INDIA\n Talwade, Pune\nOffice: +91-020-2769001\n\nMobile: + 91 9411292603\n\nEmail: Rajat.Kumar@in.fujitsu.com<mailto:Rajat.Kumar@in.fujitsu.com>\n\nShaping Tomorrow with You\n\n________________________________\nFrom: ybsave <notifications@github.com>\nSent: Wednesday, April 26, 2017 8:48 PM\nTo: tensorflow/tensorflow\nCc: Kumar, Rajat; Comment\nSubject: Re: [tensorflow/tensorflow] ImportError: DLL load failed: The specified module could not be found. (#7995)\n\n\n@hughlomas<https://github.com/hughlomas> Hi, have you tried to use cuDNN 5.1 instead of the newest 6.x? Also, please try to use python 3.5, but not 3.6. These work for me.\n\n-\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/7995#issuecomment-297442684>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AavOApl-ZGgDhXxDP2rd25ArE0kKf52eks5rz2AogaJpZM4MQjwB>.\n", "I use the latest CUDA 8.0 and cuDNN6.0 for Windows 10, and it works well. ", "Hello,\n\n        Thanks for your response but I want it for windows 7 and I'm not using the GPU , I'm using  CPU which does not include CUDA and cuDNN.\n\nThanks & Regards,\n\n\n\nRAJAT KUMAR\n\nTechnical Trainee\n\nFUJITSU CONSULTING INDIA\n Talwade, Pune\nOffice: +91-020-2769001\n\nMobile: + 91 9411292603\n\nEmail: Rajat.Kumar@in.fujitsu.com<mailto:Rajat.Kumar@in.fujitsu.com>\n\nShaping Tomorrow with You\n\n________________________________\nFrom: Shiyong Ma <notifications@github.com>\nSent: Wednesday, May 3, 2017 1:32 AM\nTo: tensorflow/tensorflow\nCc: Kumar, Rajat; Comment\nSubject: Re: [tensorflow/tensorflow] ImportError: DLL load failed: The specified module could not be found. (#7995)\n\n\nI use the latest CUDA 8.0 and cuDNN6.0 for Windows 10, and it works well.\n\n-\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/7995#issuecomment-298744540>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AavOAkrJns4-V9oR8QDpRwuzMJ_-n5iaks5r14vggaJpZM4MQjwB>.\n", "Same problem here, worked after changing to cuDNN5.1 instead of 6.0.\r\nwin10 x64\r\nPython 3.5\r\nTensorflow-gpu 1.1.0\r\nCUDA8.0", "Hello,\n\nok thanks for the reply and let me try it with your solution on my system.\n\nThanks & Regards,\n\n\n\nRAJAT KUMAR\n\nTechnical Trainee\n\nFUJITSU CONSULTING INDIA\n Talwade, Pune\nOffice: +91-020-2769001\n\nMobile: + 91 9411292603\n\nEmail: Rajat.Kumar@in.fujitsu.com<mailto:Rajat.Kumar@in.fujitsu.com>\n\nShaping Tomorrow with You\n\n________________________________\nFrom: jingyibo123 <notifications@github.com>\nSent: Friday, June 9, 2017 2:43 AM\nTo: tensorflow/tensorflow\nCc: Kumar, Rajat; Comment\nSubject: Re: [tensorflow/tensorflow] ImportError: DLL load failed: The specified module could not be found. (#7995)\n\n\nSame problem here, worked after changing to cuDNN5.1 instead of 6.0.\nwin10 x64\nPython 3.5\nTensorflow-gpu 1.1.0\n\n-\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/7995#issuecomment-307228716>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AavOAn9UcGggDMy7f_Ig7KJ3zrjlWB7Gks5sCGPvgaJpZM4MQjwB>.\n", "Not sure why this error is coming for CPU systems. Facing the same. Can't find resolution anywhere", "have a look at this link for workaround https://github.com/tensorflow/tensorflow/issues/17386"]}, {"number": 7994, "title": "Loading and re-training the model", "body": "Hi, I am confused, if it is possible to load the last checkpoint for the model and now use another data to train the same model, so widen it's knowledge ?\r\n\r\nUsecase: chatbot\r\ndata1: cornell-movie data\r\ndata2: stack exchange dataset\r\n\r\nFirstlt, I had trained a seq2seq model on the cornell data, with some hyperparameters and other important variables, now i want my same model to train on other datasets also. Is this possible ?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hi @prakhar21. Did you happen to find a way around on this? I am specifically interested in how could we perform the re-training part using the new [Save and Load](https://www.tensorflow.org/tutorials/keras/save_and_load) methodologies with Keras models. "]}, {"number": 7993, "title": "tf.contrib.slim Out of Date API", "body": "I am using tf.contrib.slim API to build nets, it's simple and convenient, however, I think it's some kind of confusion me, I occurs a lot of out-of-date api which I think should be update or fix (except I got wrong idea to using it).\r\nBasiclly, I got images and labels from tfrecord file, run this:\r\n\r\n```\r\ndef run_training():\r\n\r\n    train_log_dir = './train_log'\r\n    if not tf.gfile.Exists(train_log_dir):\r\n        tf.gfile.MakeDirs(train_log_dir)\r\n\r\n    images, labels = inputs(train=True, batch_size=FLAGS.batch_size,\r\n                            num_epochs=FLAGS.num_epochs, one_hot_labels=True)\r\n    predictions = vgg.vgg_16(images, is_training=True)\r\n\r\n    slim.losses.softmax_cross_entropy(predictions, labels)\r\n    total_loss = slim.losses.get_total_loss()\r\n    tf.summary.scalar('loss', total_loss)\r\n\r\n    optimizer = tf.train.RMSPropOptimizer(0.001, 0.9)\r\n    train_op = slim.learning.create_train_op(total_loss, optimizer, summarize_gradients=True)\r\n\r\n    slim.learning.train(train_op, train_log_dir, save_summaries_secs=20)\r\n```\r\nall imports from `tf.contirb.slim`. \r\nAnd I got these error.\r\n```\r\nFile \"train_slim_vgg16.py\", line 157, in run_training2\r\n    slim.losses.softmax_cross_entropy(predictions, labels)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 117, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py\", line 380, in softmax_cross_entropy\r\n    logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())\r\nAttributeError: 'tuple' object has no attribute 'get_shape'\r\n```\r\nI read and load images labels like this:\r\n```\r\ndef read_and_decode(filename_queue):\r\n    reader = tf.TFRecordReader()\r\n    _, serialized_example = reader.read(filename_queue)\r\n    features = tf.parse_single_example(\r\n        serialized=serialized_example,\r\n        features={\r\n            'image/height': tf.FixedLenFeature([], tf.int64),\r\n            'image/width': tf.FixedLenFeature([], tf.int64),\r\n            'image/channels': tf.FixedLenFeature([], tf.int64),\r\n            'image/encoded': tf.FixedLenFeature([], tf.string),\r\n            'image/class/label': tf.FixedLenFeature([], tf.int64),\r\n        })\r\n    height = tf.cast(features['image/height'], dtype=tf.int32)\r\n    width = tf.cast(features['image/width'], dtype=tf.int32)\r\n    channels = tf.cast(features['image/channels'], dtype=tf.int32)\r\n    label = tf.cast(features['image/class/label'], dtype=tf.int32)\r\n\r\n    image = tf.image.decode_jpeg(features['image/encoded'], channels=3)\r\n    image = tf.image.resize_image_with_crop_or_pad(\r\n        image=image,\r\n        target_height=FLAGS.target_image_height,\r\n        target_width=FLAGS.target_image_width,\r\n    )\r\n    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\r\n    return image, label\r\n\r\n\r\ndef inputs(train, batch_size, num_epochs, one_hot_labels):\r\n    if not num_epochs:\r\n        num_epochs = None\r\n    with tf.name_scope('input'):\r\n        filename_queue = tf.train.string_input_producer(\r\n            tf_records_walker(tf_records_dir=FLAGS.tf_record_dir),\r\n            num_epochs=num_epochs,\r\n            shuffle=True)\r\n        image, label = read_and_decode(filename_queue)\r\n\r\n        if one_hot_labels:\r\n            label = tf.one_hot(indices=label, depth=FLAGS.num_classes+1, dtype=tf.int32)\r\n        images, sparse_labels = tf.train.shuffle_batch(\r\n            [image, label],\r\n            batch_size=batch_size,\r\n            num_threads=2,\r\n            capacity=10 + 3 * batch_size,\r\n            min_after_dequeue=10)\r\n\r\n        return images, sparse_labels\r\n```\r\nObviously, the way used to write slim nets and train graph not effect anymore, I suggest develope team  update this contrib docs or update some APIs. (If I did something wrong, sincerely help you guys help me out, I just can't get my network run.)", "comments": ["@sguada @nathansilberman might have some ideas here.", "Apologies if this does not helpful but I have an idea.  I am pretty new but I used Slim recently and while there are a number of warnings, I think your issues is here:\r\n\r\n`predictions = vgg.vgg_16(images, is_training=True)`\r\n\r\nshould be something like\r\n\r\n`logits, end_points = vgg.vgg_16(images, is_training=True)`\r\n\r\nor\r\n\r\n`predictions, end_points = vgg.vgg_16(images, is_training=True)`\r\n\r\nIf you prefer predictions as the name.  \r\n\r\nThe documents might be poor in areas, especially contrib, but I find even though I do not understand Python that if I follow the stack trace I often solve most problems even if the documents are not complete.  I hope this helps and good luck in the future.  I am closing the issue because I am pretty confident this will fix your problem, but feel free to reopen if my guess was wrong.  \r\n\r\nSee [source code](https://github.com/tensorflow/tensorflow/blob/433e706bfe0145845911bce0d26e44c9a821b4d4/tensorflow/contrib/slim/python/slim/nets/vgg.py#L198) for where the functions returns a tuple not one value.\r\n", "Et voila! Thanks man! You saved the world!", "Many thanks!", "@Mickeypeng   I do not know if that was for me but if so you made my morning better.  :-)  "]}, {"number": 7992, "title": "Update MNIST source url, original url is dead", "body": "http://yann.lecun.com/exdb/mnist/ is not accessible", "comments": ["Can one of the admins verify this patch?", "@ilblackdragon @martinwicke \r\nI don't think I can accept this PR.\r\nI am not sure we would like to have a reference to a personal repository. We can think about alternate storage options for this dataset. Ilia, Martin, should we host this file instead? Are there any license implications we need to consider?", "@gunan That would be the ideal solution. I think you should also consider hosting the files in https://github.com/tensorflow/tensorflow/commit/d2a5b7c8f9bd789423566c56c01d1f92cc94d74d in some storage you can manage if license allows", "Yeah, apparently there's been a change and it now requires a signup?\r\n\r\nAnyway, we have asked before and we've always been turned down for re-hosting. Possibly we'll have to remove mnist altogether. Let me ask again.", "Looks like this http://yann.lecun.com/exdb/mnist/ is working fine now!"]}, {"number": 7991, "title": "{Base,LocalCLI}DebugWrapperSession and SessionInterface don't have 'as_default'.", "body": "I tried to use tfdbg to hunt those Infs, and it complained `LocalCLIDebugWrapperSession`  was lacking `as_default`, which I use to set the default session for each thread. Neither its parent class `BaseDebugWrapperSession` or the interface `SessionInterface` it implements seem to have it. I thought it was supposed to be a drop-in replacement, is this part of the design?", "comments": ["@lemonzi Thanks for filing the issue, and using tfdbg!\r\n\r\nWhat version of TensorFlow are you using, and on which OS / architecture?\r\n\r\nAssigning this to @caisq to follow up.", "It's great to have so much tooling! :)\r\n\r\nI'm on OSX 10.10, TensorFlow 1.0.0 without GPU from PyPi. I checked the relevant code on `master` when I submitted the issue and it appeared to be the same.", "This should have already been fixed in master branch HEAD. Please see:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/wrappers/framework.py#L378\r\n\r\nThe fix should become usable to external users on the next successful nightly build and/or the next release (r1.1)."]}, {"number": 7990, "title": "Fixed broken link to installation instructions", "body": "This link was broken when the docs were moved (and apparently restructured a bit).\r\n\r\nIt looks like the original file, os_setup.md doesn't exist anymore, but it looks like install.md is the most appropriate replacement.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@oldgeeksguide Thank you for sending this fix. Could you please sign the CLA?", ">Could you please sign the CLA?\n\nI'm happy to sign it, but I'll need to check with my manager first. I work\nfor Intel, and I don't expect any problem with signing the CLA, but I have\nto check first, not sure how long that will take.  Do you need it before\nthis fix is merged?\n\nThanks!\nDale\n\n\nOn Wed, Mar 1, 2017 at 6:49 PM, Shanqing Cai <notifications@github.com>\nwrote:\n\n> @oldgeeksguide <https://github.com/oldgeeksguide> Thank you for sending\n> this fix. Could you please sign the CLA?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7990#issuecomment-283540296>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACe5OAEz_fioXB2ITKzLkMVv0oW1gD4fks5rhi5AgaJpZM4MQado>\n> .\n>\n", "@oldgeeksguide Acknowledged. FWIW, we have had contributors from Intel before, e.g., https://github.com/tensorflow/tensorflow/pull/7113", "Thanks for the PR, but someone else got to it first :) Closing as duplicate of https://github.com/tensorflow/tensorflow/pull/8169.", "Great, Thanks!"]}, {"number": 7989, "title": "\"Download and Setup\" link broken", "body": "In README file, the link of \"Download and Setup\" is pointing to _https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md_\r\nHowever, it looks like that is broken now, please help fix, thanks~\r\n\r\n\"\r\nInstallation\r\nSee Download and Setup for instructions on how to install our release binaries or how to build from source.\r\n\"", "comments": ["I found that they moved the old folder to the new place.\r\nThe folder of g3doc is moved to docs_src, but I cannot find the os_setup.md in the folder...\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/docs_src/get_started", "@calpa thanks for the tips~\r\nI think I have found the page.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/index.md\r\n", "#7990 fixes this issue.", "Closing this out.  Thanks for filing the bug @purlvin , and for the tips @calpa !"]}, {"number": 7988, "title": "Scope issue with Adam after upgrade from 0.12 to 1.0.0.", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone directly related, but indirectly this one is related: https://github.com/tensorflow/tensorflow/issues/7462 (but seems to be code error).\r\n\r\nTHE ISSUE:\r\n\r\nMy code works with r0.12:\r\ngit@github.com:pseudotensor/temporal_autoencoder.git (branch timeconv, commit 3cfc56566931ad592eef7a9b7ddbb32f3346819c, i.e. just below HEAD)\r\n\r\nBit Fails with r1.0.0 (after using upgrade script):\r\ngit@github.com:pseudotensor/temporal_autoencoder.git (branch timeconv, commit f7ee1e5325797173cc26e132f152f969839f9975, i.e. HEAD)\r\n\r\nError:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 476, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 473, in main\r\n    autoencode(continuetrain=continuetrain,modeltype=modeltype,num_balls=num_balls)\r\n  File \"main.py\", line 273, in autoencode\r\n    train_operation = tf.train.AdamOptimizer(FLAGS.adamvar).minimize(loss)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 289, in minimize\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 403, in apply_gradients\r\n    self._create_slots(var_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adam.py\", line 117, in _create_slots\r\n    self._zeros_slot(v, \"m\", self._name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 647, in _zeros_slot\r\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py\", line 123, in create_zeros_slot\r\n    colocate_with_primary=colocate_with_primary)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot\r\n    return _create_slot_var(primary, val, '')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var\r\n    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 988, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 890, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 348, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 333, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 657, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable cnn_1_cnn/weights/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\nHypothesis1:  Something changed in how tensorflow1.0.0 handles scope so that Adam for some reason thinks it should be under cnn_1_cnn_weights scope, when should be in base scope\r\nHypothesis2: Code was \"wrong\" to begin with, even though worked perfectly with r0.12, but now stricter tensorflow 1.0.0 checks led to this message.\r\nIn either case, I'm not sure what to do, but I'll continue checking that what I'm doing is correct as per examples at: https://www.tensorflow.org/programmers_guide/variable_scope\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nUbuntu 16.04.2 LTS (latest updates/upgrades)\r\n\r\nInstalled version of CUDA and cuDNN: \r\n\r\nls -l $CUDA_HOME/lib64/libcud*\r\n-rwxr-xr-x 1 root root    558720 Jan 29 16:23 /usr/local/cuda/lib64/libcudadevrt.a*\r\nlrwxrwxrwx 1 root root        16 Jan 29 16:23 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0*\r\nlrwxrwxrwx 1 root root        19 Jan 29 16:23 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44*\r\n-rwxr-xr-x 1 root root    415432 Jan 29 16:23 /usr/local/cuda/lib64/libcudart.so.8.0.44*\r\n-rwxr-xr-x 1 root root    775162 Jan 29 16:23 /usr/local/cuda/lib64/libcudart_static.a*\r\nlrwxrwxrwx 1 jon  users       13 Jul 26  2016 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5*\r\nlrwxrwxrwx 1 jon  users       17 Jul 26  2016 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5*\r\n-rwxrwxr-x 1 jon  users 79337624 Jul 26  2016 /usr/local/cuda/lib64/libcudnn.so.5.1.5*\r\n-rwxrwxr-x 1 jon  users 69756172 Jul 26  2016 /usr/local/cuda/lib64/libcudnn_static.a*\r\n\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\njon@pseudotensor:~/tensorflow$ cd ..\r\njon@pseudotensor:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n\r\njon@pseudotensor:~/tensorflow$ git rev-parse HEAD\r\n29a6b4661258ef99842904d7c54993c963a8c2c0\r\n\r\n2. The output of `bazel version`\r\n\r\njon@pseudotensor:~/tensorflow$ bazel version\r\n.....................\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n\r\n\r\nI also tried the latest nightly build: https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl  and this fails in the same way as my compiled-from-source version.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nStep 1) git@github.com:pseudotensor/temporal_autoencoder.git\r\nStep 2) cd temporal_autoencoder\r\nStep 3) git checkout timecov\r\nStep 4) python main.py\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nChecking scope makes sense, googling, looking at existing issues.  No positive result.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["you can try to define tf.train.AdamOptimizer(0.01).minimize(cost) out of  tf.variable_scope() like this:\r\n```python\r\nwith tf.variable_scope('graph'):\r\n    W= tf.get_variable('W',[16,16],initializer=tf.truncated_normal_initializer(stddev=0.01))\r\n    ...#your graph\r\noptimizer = tf.train.AdamOptimizer.AdadeltaOptimizer(0.01).minimize(cost)\r\n```\r\n    ", "Hi, that actually worked, thanks!  Do you have a clear idea why the scope is messed-up when the graph and optimizer are in the same scope?  That cnn_1_cnn is a very deep scope and the optimizer shouldn't have been anywhere near such a scope.  Thanks!", "Thanks for the suggestion @Leishuyu-bupt , and thanks for filing the issue @pseudotensor .  I'm glad the fix works!\r\n\r\nTo follow-up on variable scopes, please use [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow), since there is a larger community that reads questions there. Thanks!", "@pseudotensor Hi, I don't know the reason why the scope is messed-up clearly when the graph and optimizer are in the same scope. I guess that some variables like \"cnn_1_cnn/weights/Adam/\" can not create in the scope, so i initialize the tf.train.Adam out of the scope.", "@pseudotensor @Leishuyu-bupt \r\nIn my case, Adam parameter can't be created if we have used \"tf.get_variable_scope().reuse_variables()\" in the scope which contain Adam.\r\n\r\nSo, we can create a scope if we need to reuse variable and reuse variables under that scope.\r\n"]}, {"number": 7987, "title": "Branch 148925596", "body": "Pushing internal changes.", "comments": []}, {"number": 7986, "title": "tf.contrib.metrics.accuracy does not accept name", "body": "The `tf.contrib.metrics.accuracy` operation does not take in a name and instead is hardcoded to be `accuracy`:\r\nhttps://github.com/tensorflow/tensorflow/blob/a230417c58c258b2417225c739a1e5f0890491e6/tensorflow/contrib/metrics/python/metrics/classification.py#L55\r\n\r\nsignature should be:\r\n```\r\ndef accuracy(predictions, labels, weights=None, name=None):\r\n```\r\nand then:\r\n```\r\nwith ops.name_scope(name, 'accuracy', values=[predictions, labels]):\r\n```", "comments": ["Would you like to submit a PR to fix this?  We have limited manpower for tf.contrib, but definitely welcome contributions!  :)"]}, {"number": 7985, "title": "Fix typos and minor issues in Docker README.md", "body": "Fix typos, company/product name capitalization, and other minor issues.\r\n\r\n### Typos:\r\nrecomended &#x27F6; recommended\r\nnighlty &#x27F6; nightly\r\n\r\n### Capitalization (changed to capitalization that's most consistent with rest of TensorFlow repo):\r\nubuntu &#x27F6; Ubuntu\r\nNvidia &#x27F6; NVidia\r\nCuda &#x27F6; CUDA\r\n\r\n### Note:\r\nThe change from three Docker container images to only two being listed happened in [this commit](https://github.com/tensorflow/tensorflow/commit/60796d7c0d401e5e7b7a139f165e78ce778583be).", "comments": ["Can one of the admins verify this patch?", "Thank you for the contribution, @ClarkZinzow "]}, {"number": 7984, "title": "Fix TF issue that prevented compiling it as a submodule.", "body": "", "comments": []}, {"number": 7983, "title": "failed initializing StreamExecutor for CUDA device ordinal 0", "body": "I encountered a strange issue that tensorflow asks for extreme amount of memory and report an error, which I cannot find any solution over the internet. I have two Titan X, the gpu0 is running traning process which occupies the whole memory of gpu0 and 100m of gpu1. I was trying to run inference python program on gpu1 and got that error. \r\n\r\nSystem settings: Ubuntu 16.04, Cuda 8.0, CUDNN 5.1 for 8.0, Nvidia 367.57 driver, tensorflow 1.0.0. The rest you can see in the log. \r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nE tensorflow/core/common_runtime/direct_session.cc:137] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073709551615\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 143, in <module>\r\n    main()\r\n  File \"inference.py\", line 102, in main\r\n    sess = tf.Session(config=config)\r\n  File \"/home/zhouzh/virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1187, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/zhouzh/virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 552, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/zhouzh/virtualenvs/tensorflow/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n```\r\n\r\n", "comments": ["Tensorflow tries to preallocate memory on the GPUs. I think the default is 70%  of the GPU memory.\r\n\r\nThe easiest solution would be to have the two different processes sequentially.\r\nOr you can use the `CUDA_VISIBLE_DEVICES` environment variables to give exclusive access to each process. In your programs, you would then start your session to use `gpu:0`", "Closing due to lack of activity.", "@gunan Thanks for your reply. Yeah I knew this environment variables. But this problem actually randomly happened if I already occupied most of the memory in gpu0. Even When I create a session with single tf.constant. Do you have any ideas about this?", "No matter how small a variable you create TF preallocates GPU memory.\r\nThe randomness may come from the concurrency, the exact timing where you start both programs.\r\n\r\nIn any case, this is the expected behaviour from TF.\r\nYou can search for `CUDA_ERROR_OUT_OF_MEMORY` to read more about this, there are quite a few github issues filed about this problem.", "@gunan Okay got you. Thanks!", "I had experienced the same issue. What exactly worked for me that I just closed all opened terminals and opened again, then it worked! . Because I was working on multiple virtual environments at the same time and most probably this caused GPU's memory overloaded. \r\nHope works for all having this issue.", "thank you @mytrjp  \r\nI checked first GPU load by nvidia-smi command and found some apps are running on top of that. after closing them, and closing current terminal, and open terminal again it starts working.", "I also have this problem. Finally, I found a tensorflow task was killed and it disappears in \"nvidia-smi\". But there are still some threads remained which hold the GPU memory.  After killing it again, GPU's memory is released. ", "I had experienced the same issue. I had 8 GPUs and run two python scripts on two of them. What worked for me that I just closed one of the python programs.", "I have experienced same issue when I ran codes on 4GPUs, when I open jupyter notebook using GPU, and then another python script in terminal. Check 'nvidia-smi' and figured out the first GPU(0) memory is almost fully utilized, and I specify the other 3 GPU using os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"1,2,3\"  then both of them can work together and didn't need to close one to support the other one.", "A couple of things that helped me:\r\n\r\n1. Close all terminal windows and start fresh or kill background processes that are holding onto GPU resources by running the following command:\r\n```\r\nnvidia-smi | grep 'python' | awk '{ print $3 }' | xargs -n1 kill -9\r\n```\r\nReference: https://stackoverflow.com/questions/50193538/how-to-kill-process-on-gpus-with-pid-in-nvidia-smi-using-keyword\r\n\r\n2. I still got an error that the GPU does not have sufficient memory\r\n\r\n\r\n```\r\n2019-09-18 15:39:14.544076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created\r\nTensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2491 MB memory) -> \r\nphysical GPU (device: 0, name: Quadro P1000, pci bus id: 0000:65:00.0, compute capability: 6.1)\r\n.\r\n.\r\n\r\n2019-09-18 15:37:02.323090: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator\r\n(GPU_0_bfc) ran out of memory trying to allocate 2.25GiB with freed_by_count=0. The caller \r\nindicates that this is not a failure, but may mean that there could be performance gains if more \r\nmemory were available.\r\n```\r\nYou'll notice in the above logs that the GPU has ~2.5 GB but the memory needed was 2.25GB. Technically, it should work, but maybe it needs a bit more memory (maybe 2.25 + extra > ~2.5 ) and it failed. \r\n\r\nI just tried training a smaller model (4X smaller) and the issue went away! ( 0.55 + extra < ~2.5 ) Basically my GPU isn't great :laughing: Got all my models to train successfully by upgrading my GPU!\r\n\r\n", "I currently faced such error:\r\nInternal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 6442450944\r\n\r\nI have only one 6GB GPU, there is only one terminal to run the program (Anaconda+PyCharm) on Windows 7 (x64). Guidance in step-by-step fashion is highly appreciated.  The CUDA version is 9 and CuDNN is 7, and the Tensorflow and its gpu version are 1.10."]}, {"number": 7982, "title": "Branch 148894335", "body": "Push internal changes.", "comments": ["@tensorflow-jenkins Test this, please."]}, {"number": 7981, "title": "Fix Inconsistency in the seq2seq documentation.", "body": "With TF 1.0, the parameters `softmax_loss_function` has changed from `Function (inputs-batch, labels-batch)` to `Function (labels-batch, inputs-batch)`.\r\n\r\nThe documentation has been updated for `sequence_loss_by_example`:\r\nhttps://github.com/tensorflow/tensorflow/blob/a83290927bc2c7d64560d442d8aaec2d1cf155ab/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L1063\r\n\r\nBut not for `sequence_loss` and `model_with_buckets` even if those are calling `sequence_loss_by_example` internally:\r\nhttps://github.com/tensorflow/tensorflow/blob/a83290927bc2c7d64560d442d8aaec2d1cf155ab/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L1114\r\n\r\nEdit: Here is the commit that made the change but forgot to update the documentation in some places: https://github.com/tensorflow/tensorflow/commit/24246a1097048883bc83951cda347c447c562bcc", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please\r\n", "@Conchylicultor PR Merged. Thank you for the contribution!", "I don't know if this is voluntary but the new `seq2seq` module still use the old parameters order.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L51", "Good question for lukasz.\n\nOn Sat, Mar 4, 2017 at 12:06 PM, Conchylicultor <notifications@github.com>\nwrote:\n\n> I don't know if this is voluntary but the new seq2seq module still use\n> the old parameters order.\n>\n> https://github.com/tensorflow/tensorflow/blob/master/\n> tensorflow/contrib/seq2seq/python/ops/loss.py#L51\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7981#issuecomment-284177217>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-FTigBweRRGMtWU_dak4nXZtn-Oks5ricQogaJpZM4MQEv3>\n> .\n>\n", "I think Alex wrote the new functions before the change happend. Probably we want to update them?"]}, {"number": 7980, "title": "TensorFlow upgrade to 1.0.0 breaking import", "body": "On the Mac OS X El Capitan V 10.11.6 using python 2.7.11 with anaconda\r\n\r\nwhen I run this code:\r\n\r\npip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.9.0-py2-none-any.whl\r\n\r\nI get a working version of tensorflow 0.9.0. import tensorflow in python throws no errors.\r\n\r\nI type in the command:\r\n\r\npip install --upgrade tensorflow\r\n\r\nI get a successful install \r\n\r\n```\r\nCollecting tensorflow\r\n  Using cached tensorflow-1.0.0-cp27-cp27m-macosx_10_11_x86_64.whl\r\nRequirement already up-to-date: mock>=2.0.0 in ./anaconda/lib/python2.7/site-packages (from tensorflow)\r\nRequirement already up-to-date: six>=1.10.0 in ./anaconda/lib/python2.7/site-packages (from tensorflow)\r\nRequirement already up-to-date: numpy>=1.11.0 in ./anaconda/lib/python2.7/site-packages (from tensorflow)\r\nCollecting protobuf>=3.1.0 (from tensorflow)\r\n  Using cached protobuf-3.2.0-py2.py3-none-any.whl\r\nRequirement already up-to-date: wheel in ./anaconda/lib/python2.7/site-packages (from tensorflow)\r\nRequirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in ./anaconda/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\r\nRequirement already up-to-date: pbr>=0.11 in ./anaconda/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\r\nRequirement already up-to-date: setuptools in ./anaconda/lib/python2.7/site-packages (from protobuf>=3.1.0->tensorflow)\r\nRequirement already up-to-date: appdirs>=1.4.0 in ./anaconda/lib/python2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorflow)\r\nRequirement already up-to-date: packaging>=16.8 in ./anaconda/lib/python2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorflow)\r\nRequirement already up-to-date: pyparsing in ./anaconda/lib/python2.7/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow)\r\nInstalling collected packages: protobuf, tensorflow\r\n  Found existing installation: protobuf 3.0.0b2\r\n    Uninstalling protobuf-3.0.0b2:\r\n      Successfully uninstalled protobuf-3.0.0b2\r\n  Found existing installation: tensorflow 0.9.0\r\n    Uninstalling tensorflow-0.9.0:\r\n      Successfully uninstalled tensorflow-0.9.0\r\nSuccessfully installed protobuf-3.2.0 tensorflow-1.0.0\r\n\r\n```\r\n\r\nand I get a broken import statement in python (.. modify dir names)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"../anaconda/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"../anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 124, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"../anaconda/lib/python2.7/site-packages/tensorflow/python/platform/test.py\", line 83, in <module>\r\n    import mock                # pylint: disable=g-import-not-at-top,unused-import\r\n  File \"../anaconda/lib/python2.7/site-packages/mock/__init__.py\", line 2, in <module>\r\n    import mock.mock as _mock\r\n  File \"../anaconda/lib/python2.7/site-packages/mock/mock.py\", line 71, in <module>\r\n    _v = VersionInfo('mock').semantic_version()\r\n  File \"../anaconda/lib/python2.7/site-packages/pbr/version.py\", line 460, in semantic_version\r\n    self._semantic = self._get_version_from_pkg_resources()\r\n  File \"../anaconda/lib/python2.7/site-packages/pbr/version.py\", line 447, in _get_version_from_pkg_resources\r\n    result_string = packaging.get_version(self.package)\r\n  File \"../anaconda/lib/python2.7/site-packages/pbr/packaging.py\", line 750, in get_version\r\n    name=package_name))\r\nException: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name mock was given, but was not able to be found.\r\n```\r\n\r\nwhen I import again in the same interface, I get a different error which repeats if I do it anymore\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"../anaconda/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"../anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"../anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name pywrap_tensorflow\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["Same issue here.\r\nTried to reinstall several times using virtualenv with different settings (--no-site-packages, --system-site-packages, update packages, etc.), no difference made.\r\n\r\nList of package versions:\r\nadium-theme-ubuntu==0.3.4\r\nappdirs==1.4.2\r\nattrs==15.2.0\r\nbackports-abc==0.4\r\nbeautifulsoup4==4.4.1\r\nblinker==1.3\r\ncertifi==2016.8.31\r\ncharacteristic==14.3.0\r\nchardet==2.3.0\r\ncommand-not-found==0.3\r\ncryptography==1.2.3\r\ncycler==0.10.0\r\nCython==0.23.4\r\ndebtags==2.0\r\ndecorator==4.0.6\r\ndefer==1.0.6\r\ndirspec==13.10\r\nduplicity==0.7.6\r\nenum34==1.1.2\r\nfuncsigs==1.0.2\r\nfunctools32==3.2.3.post2\r\ngevent==1.1.0\r\ngreenlet==0.4.9\r\nh5py==2.5.0\r\nhtml5lib==0.999\r\nhttplib2==0.9.1\r\nidna==2.0\r\nipaddress==1.0.16\r\nipdb==0.10.1\r\nipykernel==4.2.2\r\nipython==4.0.3\r\nipython-genutils==0.1.0\r\nipywidgets==5.2.2\r\nJinja2==2.8\r\njsonschema==2.5.1\r\njupyter==1.0.0\r\njupyter-client==4.1.1\r\njupyter-console==5.0.0\r\njupyter-core==4.0.6\r\nleveldb==0.193\r\nlockfile==0.12.2\r\nlxml==3.7.3\r\nM2Crypto==0.22.6rc4\r\nMarkupSafe==0.23\r\nmatplotlib==2.0.0\r\nmistune==0.7.1\r\nmock==2.0.0\r\nnbconvert==4.1.0\r\nnbformat==4.0.1\r\nndg-httpsclient==0.4.0\r\nnetworkx==1.11\r\nnose==1.3.7\r\nnotebook==4.1.0\r\nnumpy==1.12.0\r\noauthlib==1.0.3\r\nolefile==0.44\r\noneconf==0.3.9\r\npackaging==16.8\r\nPAM==0.4.2\r\npandas==0.17.1\r\npath.py==8.1.2\r\npbr==2.0.0\r\npexpect==4.2.1\r\npickleshare==0.6\r\nPillow==4.0.0\r\npiston-mini-client==0.7.5\r\npkg-resources==0.0.0\r\nprompt-toolkit==1.0.7\r\nprotobuf==3.2.0\r\npsutil==3.4.2\r\nptyprocess==0.5\r\npyasn1==0.1.9\r\npyasn1-modules==0.0.7\r\npycrypto==2.6.1\r\npycups==1.9.73\r\nPygments==2.1\r\npygobject==3.20.0\r\nPyJWT==1.3.0\r\npyOpenSSL==0.15.1\r\npyparsing==2.1.10\r\npyserial==3.0.1\r\npysqlite==2.7.0\r\nPyste==0.9.10\r\npython-apt==1.1.0b1\r\npython-dateutil==1.5\r\npython-debian==0.1.27\r\npython-gflags==2.0\r\npytz==2014.10\r\npyxdg==0.25\r\nPyYAML==3.11\r\npyzmq==15.4.0\r\nqtconsole==4.2.1\r\nscikit-image==0.11.3\r\nscikit-learn==0.18.1\r\nscipy==0.18.1\r\nscour==0.32\r\nservice-identity==16.0.0\r\nsimplegeneric==0.8.1\r\nsingledispatch==3.4.0.3\r\nsix==1.10.0\r\nsoftware-center-aptd-plugins==0.0.0\r\nsubprocess32==3.2.7\r\ntensorflow==1.0.0\r\nterminado==0.6\r\ntornado==4.4.1\r\ntraitlets==4.1.0\r\nTwisted==16.0.0\r\nubuntuone-client-data==14.4\r\nunity-lens-photos==1.0\r\nupdate==0.4.4\r\nurllib3==1.13.1\r\nvirtualenv==15.0.1\r\nwcwidth==0.1.7\r\nwidgetsnbextension==1.2.6\r\nwxPython==3.0.2.0\r\nwxPython-common==3.0.2.0\r\nxlrd==1.0.0\r\nzope.interface==4.1.3\r\n", "Yeah, I wouldn't expect it to break like that if 0.9.0 works fine. Any necessary updates of packages should be handled at installation. ", "We test our upgrade script from 0.12 to 1.0\r\n0.9 to 1.0 is a substantial upgrade, I don't think we did test for that.\r\n@aselle any ideas?", "Gunan,\r\n\r\nI tried all the upgrades in between 0.10.0, 0.11.0, and 0.12.0 and they all have the same problem. 0.9.0 still works when I pip install it though.", "Ah, wait now I see.\r\nThe issue is you are missing the `mock` library. Sorry for the confusion.\r\n\r\nCould you try running `pip install mock` and see if that resolves the issue for you?", "I'm in macOS 10.12.3 and I have the same error. When I try to install mock it says it is already installed.\r\n\r\nForgot to mention that I'm installing it directly from PIP and not using Anaconda.", "Sergio could you copy paste your full error message?\nThe last error can look similar, but the real error may be hidden in the\nbig stack trace preceeding the last import error.\n\nOn Mar 3, 2017 12:38 AM, \"Sergio D\u00edaz\" <notifications@github.com> wrote:\n\n> I'm in macOS 10.12.3 and I have the same error. When I try to install mock\n> it says it is already installed.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7980#issuecomment-283897212>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOWQP1VOrSCHHBsuyAl_CsXe9Zrogks5rh9GhgaJpZM4MQC1z>\n> .\n>\n", "No gunan, I have mock. I have used mock before in my own projects. That is not the issue. This appears to be a tensorflow issue and it seems to have appeared when tensorflow went from one version to two versions - one for the CPU and one for the GPU.", "The first stack trace you have in the report is tied to mock.\r\nThe 2nd one however, is not.\r\n\r\nTensorflow also had one CPU and one GPU version in 0.9.0, too.\r\nWe just renamed the GPU package after 0.9 to help distinguish the pip packages for users.\r\nAnd the two version thing has been working for thousands of people for months.\r\n\r\nI am suspecting pip tool itself may be struggling to update everything correctly. Could you try first uninstalling then reinstalling tensorflow?\r\nAlso, with anaconda we see various issues. Can you try the following?\r\n\r\n```\r\npip uninstall tensorflow\r\npip install --upgrade --ignore_installed tensorflow\r\n```\r\n\r\n", "pip install --upgrade --ignore_installed tensorflow\r\n\r\n\"--ignore_installed\" solved the issue in my case. Otherwise some of the packages are using system installed ones instead of in virtual environment.\r\n\r\nThanks!", "pip install --upgrade --ignore_installed tensorflow\r\n\r\nshould be:\r\n\r\npip install --upgrade --ignore-installed tensorflow\r\n\r\nand it did not solve my issue... same error as before. But I am not using a virtual environment.", "@xiaoliangbai I'm glad the issue is resolved for you.\r\n\r\n@MikeTam1021 This is interesting. What is your Anaconda installation located?\r\nIs it possible we need to add \"sudo\" to all pip commands we have?\r\n", "My anaconda installation is in my home directory.. I use pip without sudo all the time, so I would imagine we do not need to use sudo. I think I am going to try to start developing in a container. I would really like to get this working though as it would speed my development time.", "Sorry for all the inconvenience @MikeTam1021 \r\nWe are seeing similar issues on Anaconda, I will try to find a macos machine and reproduce your setup.\r\n\r\nHave you been using the GPU version, or the CPU version with 0.9?", "I believe it is the CPU version", "Got it!!! \r\n\r\nSo, I tried the virtual environment conda install. It worked (but was buggy, as noted). Then I thought \"maybe this will work on my native system\". It did. Now I can access tensorflow 1.0.0 from my system. Many thanks!\r\n\r\nconda install -c conda-forge tensorflow", "I encountered the same problem. I tried all the method mentioned but they did not work. How about you? @MikeTam1021 ", "I did a conda install ningyuma. That solved it for me.", "@MikeTam1021 the problem is that such solution don't install the GPU version, as it is documented here: https://www.tensorflow.org/install/install_linux", "I was not installing the GPU version hfranco. Conda installed worked for me. pip install was breaking on the import statement in the REPL.", "@MikeTam1021 Good approach if you are not using CUDA. The other side has the nightmare of installing tensorflow and anaconda alongside", "## **Still facing the same problem**..\r\ni did try the all the above mentioned solution. **Nothing worked for me**..\r\ntried all in **\"MacOS 10.12.1\"** and using virtual environment ...\r\ni am installing **CPU version of tensorflow**.", "I am also facing the issue. Tensorflow was working fine in my system, but suddenly this problem appears. I am using ubuntu 16.04 and installation is running on virtualenv. My tensorflow verson is tensorflow==1.1.0\r\n\r\n` from pbr.version import VersionInfo\r\n   ...: _v = VersionInfo('mock').semantic_version()\r\n`\r\n The problem is with mock, and my mock version is mock==2.0.0", "try this \r\npip install -U protobuf==3.0.0b2\r\nand in my case it solved the problem", "@imanabbasnejad Thanks, it solved my problem.", "export PBR_VERSION='your pbr version', it's will be worked, but i don't know why VersionInfo('mock') is wrong, maybe setup.cfg in mock has some problem."]}, {"number": 7979, "title": ".configure error Unrecognized option: --action_env=PATH", "body": "OS: Ubuntu 16.04\r\ngcc 5.4.0\r\nbazel info >> bazel is already the newest version (0.4.4)\r\nCuda compilation tools, release 8.0, V8.0.61\r\n\r\nTried with tensorflow source https://github.com/tensorflow/tensorflow, commit 27a9808\r\n\r\ngopi@gp:~/tensorflow$ ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] Y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use. Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]:\r\nPlease specify the location where CUDA toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.10\r\nPlease specify the location where cuDNN 5.1.10 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n.\r\nINFO: Options provided by the client:\r\nInherited 'common' options: --isatty=1 --terminal_columns=97\r\nINFO: Reading options for 'clean' from /home/gopi/tensorflow/tools/bazel.rc:\r\nInherited 'build' options: --force_python=py2 --host_force_python=py2 --python2_path=/usr/bin/python --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define PYTHON_BIN_PATH=/usr/bin/python --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading options for 'clean' from /etc/bazel.bazelrc:\r\nInherited 'build' options: --action_env=PATH --action_env=LD_LIBRARY_PATH --action_env=TMPDIR --test_env=PATH --test_env=LD_LIBRARY_PATH\r\nUnrecognized option: --action_env=PATH", "comments": ["I do not think our configure script or our bazelrc template has the `--action_env` flag set. It looks like you have these options in your `/etc/bazel.bazelrc` file. Could you share its contents?", "build --action_env=PATH\r\nbuild --action_env=LD_LIBRARY_PATH\r\nbuild --action_env=TMPDIR\r\nbuild --test_env=PATH\r\nbuild --test_env=LD_LIBRARY_PATH", "My theory seems to be correct.\r\nDid you manually edit this file?\r\n\r\nIf you set these manually, I am pretty certain you want to set them up this way:\r\n```\r\nbuild --action_env=$PATH\r\nbuild --action_env=$LD_LIBRARY_PATH\r\nbuild --action_env=$TMPDIR\r\nbuild --test_env=$PATH\r\nbuild --test_env=$LD_LIBRARY_PATH\r\n```\r\n\r\nPlease note the added `$` signs.\r\nIf the above also does not work, if you remove the file `/etc/bazel.bazelrc`, the build should work.\r\nThe issue is related to your setup, so I will close this issue. ", "Hi\r\n\r\nI didn't edit anything manually till now.\r\n\r\nAfter your suggestion i edited like below but still error.\r\nbuild --action_env=$PATH\r\nbuild --action_env=$LD_LIBRARY_PATH\r\nbuild --action_env=$TMPDIR\r\nbuild --test_env=$PATH\r\nbuild --test_env=$LD_LIBRARY_PATH\r\n\r\n>>>>>\r\n\r\ngopi@gp:~/tensorflow$ ./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] Y\r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.1.10\r\nPlease specify the location where cuDNN 5.1.10 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=163\r\nINFO: Reading options for 'clean' from /home/gopi/tensorflow/tools/bazel.rc:\r\n  Inherited 'build' options: --force_python=py2 --host_force_python=py2 --python2_path=/usr/bin/python --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define PYTHON_BIN_PATH=/usr/bin/python --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading options for 'clean' from /etc/bazel.bazelrc:\r\n  Inherited 'build' options: --action_env=$PATH --action_env=$LD_LIBRARY_PATH --action_env=$TMPDIR --test_env=$PATH --test_env=$LD_LIBRARY_PATH\r\nUnrecognized option: --action_env=$PATH\r\n\r\n\r\n***********************\r\n\r\nand some environment info of my pc\r\n\r\ngopi@gp:~/tensorflow$ echo $CUDA_HOME\r\n/usr/local/cuda-8.0\r\ngopi@gp:~/tensorflow$ echo $PATH\r\n/home/gopi/aom_scripts/:/home/gopi/Android/Sdk/platform-tools/:/usr/local/android-ndk-r12b/:/usr/local/android-studio/bin/:/usr/local/cuda-8.0/bin:/home/gopi/bin:/home/gopi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin\r\ngopi@gp:~/tensorflow$ echo $LD_LIBRARY_PATH\r\n/usr/local/cuda-8.0/lib64\r\n", "you mentioned \"My theory seems to be correct.\" & closed the bug, sad.", "The issue is caused by the contents of your file `/etc/bazel.bazelrc`.\r\nThat file is clearly not a part of the tensorflow repository.\r\nAnd as I also suggested above, if you remove that file, your build should work.\r\nThe creation of that file however, if you did not create yourself, is a bazel issue, not a TensorFlow one as we do not install bazelrc files to your system.", "The command sudo apt-get upgrade bazel had issues, so did below steps.\r\n\r\n1. cleaned-up all the bazel related files in my machine as per https://github.com/bazelbuild/bazel/issues/838\r\n2. did fresh installation of bazel v4.4 with \"Install with Installer\" option as per https://bazel.build/versions/master/docs/install.html.\r\n\r\nNow the ./configure runs fine, thanks"]}, {"number": 7978, "title": "Ignore generated bazel_config while configure", "body": "`./configure` with default setting generates untracked files of bazel configs.\r\n\r\n```\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n\ttensorflow/core/platform/default/build_config.bzl-e\r\n\ttensorflow/core/platform/default/build_config_root.bzl-e\r\n```", "comments": ["Can one of the admins verify this patch?", "Do you know what exactly is generating those files? I haven't seen them before.", "I see what the problem is. It's a cross platform sed issue. I think the solution is to not have sed generate those files on Mac. Will send out PR soon."]}, {"number": 7977, "title": "add function get_collection for scope", "body": "#7719\r\n\r\nadd get_collection(key) to class VariableScope.\r\n\r\nexample:\r\n```\r\nwith tf.variable_scope(\"foo\") as scope:\r\n  print scope.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n```", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "Thanks for the PR, this is a method we should have. But yes, it will need a test. Also, instead of the generic \"get_collection\" I'd suggest 2 methods: scope.trainable_variables and scope.global_variables -- to mimic the TF methods.\r\n\r\nIs this PR still in active development?", "Or we could have all 3: get_collection, trainable_variables, and global_variables.", "@lukaszkaiser I'll add trainable_variables and global_variables.", "The test file is in ../kernel_tests/variable_scope_test (not in ops).", "@tensorflow-jenkins test this please.", "@martinwicke please let us know whether it is okay to merge this PR, which involves additive API changes.", "the failed test target is passed on my computer.", "@suiyuan2009 The failure you saw on our Jenkins is from a flaky test. You can ignore that.", "@tensorflow-jenkins test this please (trying to deflake)\r\ngentle ping @martinwicke ", "Ok, LGTM. Go ahead and merge."]}, {"number": 7976, "title": "distributed tensorflow data Data in parallel\uff0cps server can only use cpu? use gpu is faster?", "body": "\r\nWhen I use kears and tensorflow to do the parallel,use sgd sync optimizer I have 10 machines,\r\n each machine has a gpu, so I do\r\n\r\n\r\n\r\n1.Each gpu corresponds to a worker hosts\uff0cuse cuda_visible_devices \r\n2.every ps server only use cpu\r\n3.use Gigabit Ethernet \r\nwhen i run it,i find it is so lower,,\r\n\r\nwho can tell me ?what is wrong??\r\n\r\n\r\n\r\n\r\n\r\n\r\nflags.DEFINE_string(\"train_desc_file\",'train.json','train config')\r\nflags.DEFINE_string(\"val_desc_file\",'test.json','dev config')\r\nflags.DEFINE_string(\"save_dir\",'model','save model dir')\r\nflags.DEFINE_integer('epochs',1000,'epoch number')\r\nflags.DEFINE_integer('batch_size',64,'batch_size')\r\nflags.DEFINE_string(\"ps_hosts\",\"172.16.186.86:2221\",\r\n                    \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"worker_hosts\", \"172.16.186.86:2222,172.16.186.86:2223\",\"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"job_name\", None,\"job name: worker or ps\")\r\nflags.DEFINE_integer(\"task_index\", None,\r\n                     \"Worker task index, should be >= 0. task_index=0 is \"\r\n                     \"the master worker task the performs the variable \"\r\n                     \"initialization \")\r\n\r\n\r\nif FLAGS.job_name is None or FLAGS.job_name == \"\":\r\n        raise ValueError(\"Must specify an explicit `job_name`\")\r\n    if FLAGS.task_index is None or FLAGS.task_index == \"\":\r\n        raise ValueError(\"Must specify an explicit `task_index`\")\r\n    print(\"job name = %s\" % FLAGS.job_name)\r\n    print(\"task index = %d\" % FLAGS.task_index)\r\n    ps_spec = FLAGS.ps_hosts.split(\",\")\r\n    worker_spec = FLAGS.worker_hosts.split(\",\")\r\n    num_workers = len(worker_spec)\r\n\r\n    cluster = tf.train.ClusterSpec({\r\n        \"ps\": ps_spec,\r\n        \"worker\": worker_spec})\r\n    server = tf.train.Server(\r\n        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n            cluster=cluster)):\r\n        is_chief = (FLAGS.task_index == 0)\r\n        model=....\r\n        acoustic_input = model.inputs[0]\r\n        network_output = model.outputs[0]\r\n        network_output = tf.transpose(network_output, [1, 0, 2])\r\n        ctc_cost = warpctc_tensorflow.ctc(network_output, sess_label, sess_label_lens, sess_output_lens)\r\n        ctc_cost = tf.reduce_mean(ctc_cost)\r\n        trainable_vars = model.trainable_weights\r\n        learning_rate = 2e-4\r\n\r\n        global_step = tf.contrib.framework.get_or_create_global_step()\r\n        # optimizer = SGD(lr=learning_rate, momentum=0.9, nesterov=True,clipnorm=100)\r\n        optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.99,use_nesterov=True)\r\n        optimizer = tf.train.SyncReplicasOptimizer(\r\n            optimizer,\r\n            replicas_to_aggregate=num_workers,\r\n            total_num_replicas=num_workers,\r\n            use_locking=True,\r\n            name=\"sync_replicas\")\r\n\r\n        grads = optimizer.compute_gradients(ctc_cost, trainable_vars)\r\n        grads, _ = tf.clip_by_global_norm(tf.gradients(ctc_cost, trainable_vars), 100)\r\n        train_op = optimizer.apply_gradients(zip(grads, trainable_vars), global_step=global_step)\r\n        time_begin = time.time()\r\n\r\n        chief_queue_runner = optimizer.get_chief_queue_runner()\r\n        sync_init_op = optimizer.get_init_tokens_op()\r\n        ready_for_local_init_op = optimizer.ready_for_local_init_op\r\n        K.manual_variable_initialization(True)\r\n        local_init_op = optimizer.local_step_init_op\r\n        if is_chief:\r\n            local_init_op = optimizer.chief_init_op\r\n        sv = tf.train.Supervisor(\r\n            is_chief=is_chief,\r\n            logdir='train_log',\r\n            local_init_op=local_init_op,\r\n            ready_for_local_init_op=ready_for_local_init_op,\r\n            recovery_wait_secs=1,\r\n            global_step=global_step)\r\n\r\n        sess = sv.prepare_or_wait_for_session(server.target)\r\n        K.set_session(sess)\r\n        K.get_session().run(sync_init_op)\r\n        sv.start_queue_runners(sess, [chief_queue_runner])\r\n        print(\"Training begins @ %f\" % time_begin)\r\n        main(FLAGS.train_desc_file, FLAGS.val_desc_file, FLAGS.epochs, FLAGS.save_dir,\r\n             FLAGS.sortgrad, model=model, run_require_op=[ctc_cost, train_op], val_fn=val_fn)\r\n\r\n\r\n\r\n\r\nbut when i run it,", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 7975, "title": "Uable to load cuDNN DSO.", "body": "My server is CentOS 6.7\r\nwhen i do \"import tensorflow as tf\"\r\nshow   I tensorflow/stream_executor/dso_loader.cc:77] LD_LIBRARY_PATH: :/usr/local/cuda/lib64\r\n           I tensorflow/stream_executor/cuda/cuda_dnn.cc:1062]unable to load cuDNN DSO.\r\n\r\nHow can i solve it? Thans!", "comments": ["Same issue: but Ubuntu 16.04\r\nPlease see this SO issue repeated here\r\nThis question has nothing to do with the warnings SSE AVX etc.. I've included the output for completeness. The issue is the fail on some cuda libs, I think, at the end, the machine has a NVIDA 1070 card and has the Cuda libs that are used earlier in the process but something is missing at the end?\r\nI pip installed release 1.0 of TensorFlow\r\nI also downloaded the repo separately to get the most up to date tutorials.\r\nThis tutorial specifically to get instances of all of Tensorboard capabilities..\r\nAttempting to run the Minst_with_summaries.py from the tensorFlow tutorials in the repo (I copied the file out of the repo into a working directory) and I'm using Anaconda and Python 3.6 I get the following:\r\n\r\n    (py36) tom@tomServal:~/Documents/LearningRepos/Working$ python Minst_with_summaries.py\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\n    I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n    Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\n    Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\n    Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\n    Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n    W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n    I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\n    name: GeForce GTX 1070\r\n    major: 6 minor: 1 memoryClockRate (GHz) 1.645\r\n    pciBusID 0000:01:00.0\r\n    Total memory: 7.92GiB\r\n    Free memory: 7.48GiB\r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\n    I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\n    Accuracy at step 0: 0.1213\r\n    Accuracy at step 10: 0.6962\r\n    Accuracy at step 20: 0.8054\r\n    Accuracy at step 30: 0.8447\r\n    Accuracy at step 40: 0.8718\r\n    Accuracy at step 50: 0.8779\r\n    Accuracy at step 60: 0.8846\r\n    Accuracy at step 70: 0.8783\r\n    Accuracy at step 80: 0.8853\r\n    Accuracy at step 90: 0.8989\r\n    I tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: :/usr/local/cuda/lib64\r\n    F tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: /home/tom/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks)could not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n    Aborted\r\nLooks to me that the installation of TensorFlow may be missing some stuff See that last several lines above?\r\nHow to fix? ", "@Marryli You'll need to install cuDNN in addition to CUDA. \r\n@dartdog You'll need to add /usr/local/cuda/extras/CUPTI/lib64 to your LD_LIBRARY_PATH", "@dartdog or (probably preferably) `apt-get install libcupti-dev`", "I did the (sudo) apt-get install libcupti-dev which did install 3 packages I re-ran and got substantially the same result \r\nI tensorflow/stream_executor/dso_loader.cc:126] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: :/usr/local/cuda/lib64\r\nF tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: /home/tom/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks)could not find cuptiActivityRegisterCallbacksin libcupti DSO\r\nAborted\r\n", "Interesting description of the library as well.. https://launchpad.net/ubuntu/xenial/+package/libcupti-dev\r\nIncidentally wondering why it is being called at all?", "Note that you want the version from NVIDIA's apt repos rather than the (older) version in Ubuntu. Alternatively just set the LD_LIBRARY_PATH I mentioned above, because the library comes with CUDA.", "Thank you, that did it I'd suggest that the issue should be open until a standard install actually works, as someone needs to change something?"]}, {"number": 7974, "title": "InvalidArgumentError for tf.gather_nd() function", "body": "When I use `tf.gather_nd(im, idx)`, I encountered the following error:\r\n```\r\n  File \"model.py\", line 86, in <module>\r\n    num=len(up_outputs) - 1, pixels_count=pixels_count)(up_outputs)\r\n  File \"/home/dl/gds/dl_modules/keras/keras/engine/topology.py\", line 572, in __call__\r\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n  File \"/home/dl/gds/dl_modules/keras/keras/engine/topology.py\", line 635, in add_inbound_node\r\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n  File \"/home/dl/gds/dl_modules/keras/keras/engine/topology.py\", line 172, in create_node\r\n    output_tensors = to_list(outbound_layer.call(input_tensors, mask=input_masks))\r\n  File \"/home/dl/gds/important/key_frame/frame_propagation/lib/layer_utils_tf.py\", line 101, in call\r\n    results.append(_interpolate(vects[i], ori_loc, stride=self.strides[i], ori_shp=self.ori_shp))\r\n  File \"/home/dl/gds/important/key_frame/frame_propagation/lib/layer_utils_tf.py\", line 65, in _interpolate\r\n    Id = tf.gather_nd(im, idx_d)\r\n  File \"/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1473, in gather_nd\r\n    name=name)\r\n  File \"/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/dl/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Must have updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:], got updates.shape [1,200,256], indices.shape [1,200,3], params_shape [1,128,128,256]\r\n\t [[Node: gradients/GatherNd_3_grad/ScatterNd = ScatterNd[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@GatherNd_3\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](stack_3/_1441, gradients/mul_10_grad/Reshape_1/_1525, gradients/GatherNd_3_grad/Shape)]]\r\n```\r\nThe shape of tensor `im` is `(batch, row, col, channel) (1, 128, 128, 256)`, and the shape of indices tensor `ind` is `(batch, 200, 3)`, of which the 3 values in the third dimension represent `(batch, row, col)`.\r\nThus we get a tensor of shape `(1, 200, 256)`, however, why it said that:\r\n```\r\nMust have updates.shape = indices.shape[:IXDIM] + params_shape[IXDIM:], got updates.shape [1,200,256], indices.shape [1,200,3], params_shape [1,128,128,256]\r\n```\r\nIt's weird, what does `IXDIM` represents here ? Anyone knows ?", "comments": ["@happygds \r\n\r\nIf you believe this is a bug please boil this down to a small reproducible example, and include the version of TensorFlow you're using.\r\n\r\nIf you're looking for debugging help, this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there.\r\n\r\nClosing this out for now.  Thanks!", "@happygds  Did you found solution? I have similar problem - [stackoverflow](http://stackoverflow.com/questions/43007410/tensorflow-unexpected-tf-gather-nd-behaviour-bug)\r\n\r\nIt seems there really is a bug in TF v1.0 with scater_nd op.\r\nYour report is consistent with : #7585 ", "@Arsakes I use tf.reshape and tf.gather instead.", "@happygds Could you elaborate? Possibly on stackoverflow."]}, {"number": 7973, "title": "im2txt: W tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for", "body": "When I run training script provided by im2txt show and tell model, when I run this script --input_file_pattern=\"${MSCOCO_DIR}/train-?????-of-00256\"   --inception_checkpoint_file=\"${INCEPTION_CHECKPOINT}\"   --train_dir=\"${MODEL_DIR}/train\"   --train_inception=false   --number_of_steps=1000000\r\n\r\n\r\nI get following error\r\ncan anyone help me with this please.....\r\n\r\n\r\nINFO:tensorflow:Prefetching values from 256 files matching im2txt/data/new/train-?????-of-00256\r\nWARNING:tensorflow:From /home/hajira/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py:344 in __init__.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\r\nINFO:tensorflow:Restoring Inception variables from checkpoint file \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for \r\n\r\n\r\n", "comments": ["At a guess, does it work if you change the flag\r\n\r\n```\r\n--inception_checkpoint_file=\"${INCEPTION_CHECKPOINT}\"\r\n```\r\n\r\n...to add `./` at the beginning of the filename?\r\n\r\n```\r\n--inception_checkpoint_file=\"./${INCEPTION_CHECKPOINT}\"\r\n```\r\n\r\nI think there's a fix for this pending, but /cc @rohan100jain to be sure!", "still not solved....\r\n", "Actually, perhaps I'm overthinking this. The error message suggests that an empty filename is being passed to the `Saver.restore()` method. Perhaps there's a typo in `\"${INCEPTION_CHECKPOINT}\"`? Or a bug in the script that's taking the filename from a different flag? (I don't know what code you're running, so it's hard to guess at what the bug could be.)", "This is most likely a tensorflow/models problem - seems to be running this code: https://github.com/tensorflow/models/tree/master/im2txt\r\n\r\n@cshallue Could you please take a look at this?", "@hajrakomal I just ran the code and this did not happen. It looks like you're using an older version of TensorFlow and/or this repository. Can you please upgrade to TF1.0 and use the latest version of this repository and see if this issue still occurs?", "Closing this for now, because it seems to be fixed by upgrading to TF1.0. Please reopen if you continue to have issues."]}, {"number": 7972, "title": "batch_matmul", "body": "How can I use batch_matmul in tensorflow 1.0", "comments": ["The functionality has been rolled into `tf.matmul`."]}, {"number": 7971, "title": "During initial training when I write this script ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": []}, {"number": 7970, "title": "SyncReplicasOptimizer race condition strange behavior?", "body": "It seems there is a strange race condition in SyncReplicasOptimizer leading to strange behaviour. I include below an example code to reproduce what seems to be a bug (hopefully in my code) as well as the commands to reproduce it (pretty much the same code as in mnist_replica.py).\r\n\r\n\r\nI am trying to implement  synchronized SGD using SyncReplicasOptimizer, I also used the queue trick to make the parameter server stop gracefully when all workers are done. I have 4 workers and 1 parameter server. Worker 0 is the chief worker.\r\n\r\nPlease bear with me for the long explanation of the different issues (they depend on the order in which processes are launched)\r\n\r\n**** First kind of issue ****\r\n\r\nlaunch the processes in this order \r\n    \r\n    python test.py --job_name ps\r\n    python test.py --job_name worker --taks_index 0\r\n    python test.py --job_name worker --taks_index 1\r\n    python test.py --job_name worker --taks_index 2\r\n    python test.py --job_name worker --taks_index 3\r\n\r\nThe last worker throws the following error :\r\n\r\n    I tensorflow/core/distributed_runtime/master_session.cc:909] DeregisterGraph error: Unavailable: {\"created\":\"@1488366991.043859719\",\"description\":\"OS Error\",\"errno\":104,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_posix.c\",\"file_line\":229,\"grpc_status\":14,\"os_error\":\"Connection reset by peer\",\"syscall\":\"recvmsg\"}\r\n\r\nand quits, and it happens also that it hangs (not realising that the variable epoch is greater than 4, triggering the break from the training loop, and the enqueue operation to let the ps stop gracefully).\r\n\r\nIt also happen that all is fine, and the execution terminates without any errors.\r\n\r\n\r\n**** Second kind of issue ****\r\n\r\nlaunch the processes in this order \r\n    \r\n    python test.py --job_name ps\r\n    python test.py --job_name worker --taks_index 3\r\n    python test.py --job_name worker --taks_index 2\r\n    python test.py --job_name worker --taks_index 1\r\n    python test.py --job_name worker --taks_index 0\r\n\r\n\r\nThe chief here being launched at last.\r\n\r\nStrangely, the chief completes the loop and quits ( I thought with SyncReplicasOptimizer it had to wait for the other workers to complete each step).\r\n\r\nAs for the other workers, I had all sort of results when doing the same experiment many times \r\n\r\n1) Some workers simply hang and do not execute a single step in the `while true` training loop\r\n\r\n2) Some execute some steps, then simply hang, apparently they lose contact with the chief, and do not realise that the variable `epoch` is greater than 4, triggering the `break from the training loop.\r\n\r\nThank you for help with this issue.\r\n\r\nBelow is the code of test.py\r\n\r\n    import os\r\n    import shutil\r\n    import tempfile\r\n    import numpy as np\r\n    import pandas as pd\r\n    import argparse\r\n    \r\n    from keras.models import Sequential\r\n    from keras.layers.core import Dense\r\n    from keras.regularizers import l2\r\n    import tensorflow as tf\r\n    import keras\r\n    \r\n    nb_samples = 50\r\n    nb_features = 5\r\n    X_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\r\n    Y_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\r\n    \r\n    def build_keras_model(input_dim):\r\n      hidden_dim = 10\r\n    \r\n      model = Sequential()\r\n      model.add(Dense(input_dim = input_dim,\r\n                      output_dim=hidden_dim,\r\n                      activation='tanh'\r\n                      ))\r\n    \r\n      model.add(Dense(output_dim=1, activation='linear'))\r\n    \r\n      model.compile(loss='mse', optimizer='adam')\r\n      \r\n      return model\r\n    \r\n    \r\n    \r\n    \r\n    ################################################\r\n    # DISTRIBUTE\r\n    ################################################\r\n    \r\n    parser = argparse.ArgumentParser(description='tensorflow')\r\n    parser.add_argument('--job_name', dest='job_name')\r\n    parser.add_argument('--task_index', dest='task_index', default=0)\r\n    args = parser.parse_args()\r\n    \r\n    \r\n    ps_hosts = ['localhost:2222']\r\n    worker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\r\n    job_name = args.job_name\r\n    task_index = int(args.task_index)\r\n    \r\n    # Create a cluster from the parameter server and worker hosts.\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n      \r\n    server = tf.train.Server(cluster,\r\n                             job_name=job_name,\r\n                             task_index=task_index,\r\n                             config=tf.ConfigProto(log_device_placement=True,\r\n                                                   inter_op_parallelism_threads=1,\r\n                                                   intra_op_parallelism_threads=1))\r\n    \r\n    \r\n    if job_name =='ps':\r\n      with tf.device(\"/job:ps/task:0\"):\r\n        queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n      sess = tf.Session(server.target)\r\n      # wait until all workers are done\r\n      for i in range(len(worker_hosts)):\r\n        sess.run(queue.dequeue())\r\n    else:\r\n      with tf.device(tf.train.replica_device_setter(\r\n                                  worker_device=\"/job:worker/task:%d\" % task_index,\r\n                                  cluster=cluster)):\r\n    \r\n        keras.backend.set_learning_phase(1)\r\n        keras.backend.manual_variable_initialization(True)\r\n    \r\n        model = build_keras_model(nb_features)\r\n        preds = model.output\r\n        targets = tf.placeholder(tf.float32, [None, 1])\r\n        total_loss = tf.reduce_mean(\r\n                            keras.objectives.mean_squared_error(targets, preds))\r\n    \r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n        # For early stopping management\r\n        epoch = tf.Variable(0, name=\"epoch\", trainable=False)\r\n        inc_epoch_op = tf.assign_add(epoch, 1)\r\n    \r\n        is_chief=(task_index == 0)\r\n    \r\n        opt = tf.train.AdamOptimizer()\r\n        num_workers = len(worker_hosts)\r\n        replicas_to_aggregate = num_workers\r\n        opt = tf.train.SyncReplicasOptimizer(\r\n                                             opt,\r\n                                             replicas_to_aggregate=replicas_to_aggregate,\r\n                                             total_num_replicas=num_workers,\r\n                                             name=\"sync_replicas\")\r\n    \r\n        train_op = opt.minimize(total_loss, global_step=global_step)\r\n        local_init_op = opt.local_step_init_op\r\n        if is_chief:\r\n          local_init_op = opt.chief_init_op\r\n        ready_for_local_init_op = opt.ready_for_local_init_op\r\n    \r\n        # Initial token and chief queue runners required by the sync_replicas mode\r\n        chief_queue_runner = opt.get_chief_queue_runner()\r\n        sync_init_op = opt.get_init_tokens_op()\r\n    \r\n        init_op = tf.global_variables_initializer()\r\n        with tf.device(\"/job:ps/task:0\"):\r\n          queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n          enqueue_op = queue.enqueue(1)\r\n     \r\n        train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\r\n        sv = tf.train.Supervisor(\r\n                                 is_chief=is_chief,\r\n                                 logdir=train_dir,\r\n                                 init_op=init_op,\r\n                                 local_init_op=local_init_op,\r\n                                 ready_for_local_init_op=ready_for_local_init_op,\r\n                                 recovery_wait_secs=1,\r\n                                 global_step=global_step)\r\n        \r\n        print '######################################### ALL CREATED'\r\n        sess = sv.prepare_or_wait_for_session(server.target)\r\n        keras.backend.set_session(sess)\r\n        print '#######  SESSION OK ********'\r\n        if is_chief:\r\n          sess.run(sync_init_op)\r\n          sv.start_queue_runners(sess, [chief_queue_runner])\r\n        local_step = 0\r\n        while True:\r\n          train_feed = {model.input: X_train, targets: Y_train}\r\n    \r\n          _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n          loss = sess.run(total_loss, feed_dict = train_feed)\r\n          if is_chief:\r\n            sess.run(inc_epoch_op)\r\n          local_step += 1\r\n          print '## epoch ', epoch.eval(sess)\r\n          if epoch.eval(sess) > 4:\r\n            print '######################  TRYING TO LEAVE'\r\n            break\r\n    \r\n        shutil.rmtree(train_dir)\r\n        print '######################  WHILE LOOP LEFT'\r\n        sess.run(enqueue_op)\r\n        print '## ENQUEUE OP DONE'\r\n    \r\n", "comments": ["@ispirmustafa might have some ideas.", "No charitable tensorflow soul to help with this?", "cc @jmchen-g ", "Could you please try with MonitoredSession. worker code should be as follows:\r\n```\r\n  sync_replicas_hook = opt.make_session_run_hook(is_chief)\r\n  with tf.train.MonitoredTrainingSession(\r\n      master=server.target, is_chief=is_chief, checkpoint_dir=train_dir,\r\n      hooks=[sync_replicas_hook]) as mon_sess:\r\n    while not mon_sess.should_stop():\r\n      mon_sess.run(train_op)\r\n```", "Thanks @ispirmustafa \r\nI replaced my \r\n\r\n    sess = sv.prepare_or_wait_for_session(server.target)\r\n\r\nBy the code you suggested. It resulted in the following error:\r\n\r\n    Traceback (most recent call last):\r\n    File \"test.py\", line 127, in <module>\r\n      sync_replicas_hook = opt.make_session_run_hook(is_chief)\r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/training/sync_replicas_optimizer.py\", line 431, in make_session_run_hook\r\n      self.get_init_tokens_op(num_tokens))\r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/training/sync_replicas_optimizer.py\", line 418, in get_init_tokens_op\r\n      tokens = array_ops.fill([num_tokens], self._global_step)\r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/ops/gen_array_ops.py\", line 1318, in fill                                                                                                                                                                                                       \r\n      result = _op_def_lib.apply_op(\"Fill\", dims=dims, value=value, name=name)                                                                                                                                                                                                                                                 \r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/framework/op_def_library.py\", line 491, in apply_op                                                                                                                                                                                             \r\n      preferred_dtype=default_dtype)                                                                                                                                                                                                                                                                                           \r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor                                                                                                                                                                                      \r\n      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                                                                                                                                                      \r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function                                                                                                                                                                    \r\n      return constant(v, dtype=dtype, name=name)                                                                                                                                                                                                                                                                               \r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/framework/constant_op.py\", line 169, in constant                                                                                                                                                                                                \r\n      attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]                                                                                                                                                                                                                                               \r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/framework/ops.py\", line 2354, in create_op                                                                                                                                                                                                      \r\n      self._check_not_finalized()                                                                                                                                                                                                                                                                                              \r\n    File \"../tensorflow-1.0.0/lib/tensorflow/python/framework/ops.py\", line 2077, in _check_not_finalized                                                                                                                                                                                           \r\n      raise RuntimeError(\"Graph is finalized and cannot be modified.\")                                                                                                                                                                                                                                                         \r\n    RuntimeError: Graph is finalized and cannot be modified.                                                      ", "This should not happen. Could you please show all the code after your change?", "This is it.. the change is at the end. Thanks for your help. I am sure I am making a silly mistake\r\n\r\n    import os\r\n    import shutil\r\n    import tempfile\r\n    import numpy as np\r\n    import pandas as pd\r\n    import argparse\r\n    \r\n    from keras.models import Sequential\r\n    from keras.layers.core import Dense\r\n    from keras.regularizers import l2\r\n    import tensorflow as tf\r\n    import keras\r\n    \r\n    nb_samples = 50\r\n    nb_features = 5\r\n    X_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\r\n    Y_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\r\n    \r\n    def build_keras_model(input_dim):\r\n      hidden_dim = 10\r\n    \r\n      model = Sequential()\r\n      model.add(Dense(input_dim = input_dim,\r\n                      output_dim=hidden_dim,\r\n                      activation='tanh'\r\n                      ))\r\n    \r\n      model.add(Dense(output_dim=1, activation='linear'))\r\n    \r\n      model.compile(loss='mse', optimizer='adam')\r\n      \r\n      return model\r\n    \r\n    \r\n    \r\n    \r\n    ################################################\r\n    # DISTRIBUTE\r\n    ################################################\r\n    \r\n    parser = argparse.ArgumentParser(description='tensorflow')\r\n    parser.add_argument('--job_name', dest='job_name')\r\n    parser.add_argument('--task_index', dest='task_index', default=0)\r\n    args = parser.parse_args()\r\n    \r\n    \r\n    ps_hosts = ['localhost:2222']\r\n    worker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\r\n    job_name = args.job_name\r\n    task_index = int(args.task_index)\r\n    \r\n    # Create a cluster from the parameter server and worker hosts.\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n      \r\n    server = tf.train.Server(cluster,\r\n                             job_name=job_name,\r\n                             task_index=task_index,\r\n                             config=tf.ConfigProto(log_device_placement=True,\r\n                                                   inter_op_parallelism_threads=1,\r\n                                                   intra_op_parallelism_threads=1))\r\n    \r\n    \r\n    if job_name =='ps':\r\n      with tf.device(\"/job:ps/task:0\"):\r\n        queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n      sess = tf.Session(server.target)\r\n      # wait until all workers are done\r\n      for i in range(len(worker_hosts)):\r\n        sess.run(queue.dequeue())\r\n    else:\r\n      with tf.device(tf.train.replica_device_setter(\r\n                                  worker_device=\"/job:worker/task:%d\" % task_index,\r\n                                  cluster=cluster)):\r\n    \r\n        keras.backend.set_learning_phase(1)\r\n        keras.backend.manual_variable_initialization(True)\r\n    \r\n        model = build_keras_model(nb_features)\r\n        preds = model.output\r\n        targets = tf.placeholder(tf.float32, [None, 1])\r\n        total_loss = tf.reduce_mean(\r\n                            keras.objectives.mean_squared_error(targets, preds))\r\n    \r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n        # For stopping management\r\n        epoch = tf.Variable(0, name=\"epoch\", trainable=False)\r\n        inc_epoch_op = tf.assign_add(epoch, 1)\r\n    \r\n        is_chief=(task_index == 0)\r\n    \r\n        opt = tf.train.AdamOptimizer()\r\n        num_workers = len(worker_hosts)\r\n        replicas_to_aggregate = num_workers\r\n        opt = tf.train.SyncReplicasOptimizer(\r\n                                             opt,\r\n                                             replicas_to_aggregate=replicas_to_aggregate,\r\n                                             total_num_replicas=num_workers,\r\n                                             name=\"sync_replicas\")\r\n    \r\n        train_op = opt.minimize(total_loss, global_step=global_step)\r\n        local_init_op = opt.local_step_init_op\r\n        if is_chief:\r\n          local_init_op = opt.chief_init_op\r\n        ready_for_local_init_op = opt.ready_for_local_init_op\r\n    \r\n        # Initial token and chief queue runners required by the sync_replicas mode\r\n        chief_queue_runner = opt.get_chief_queue_runner()\r\n        sync_init_op = opt.get_init_tokens_op()\r\n    \r\n        init_op = tf.global_variables_initializer()\r\n        with tf.device(\"/job:ps/task:0\"):\r\n          queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n          enqueue_op = queue.enqueue(1)\r\n     \r\n        train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\r\n        sv = tf.train.Supervisor(\r\n                                 is_chief=is_chief,\r\n                                 logdir=train_dir,\r\n                                 init_op=init_op,\r\n                                 local_init_op=local_init_op,\r\n                                 ready_for_local_init_op=ready_for_local_init_op,\r\n                                 recovery_wait_secs=1,\r\n                                 global_step=global_step)\r\n        \r\n        print '######################################### ALL CREATED'\r\n        #sess = sv.prepare_or_wait_for_session(server.target)\r\n        sync_replicas_hook = opt.make_session_run_hook(is_chief)\r\n        with tf.train.MonitoredTrainingSession(\r\n                   master=server.target, is_chief=is_chief, checkpoint_dir=train_dir,\r\n                         hooks=[sync_replicas_hook]) as sess:\r\n          keras.backend.set_session(sess)\r\n          print '#######  SESSION OK ********'\r\n          if is_chief:\r\n            sess.run(sync_init_op)\r\n            sv.start_queue_runners(sess, [chief_queue_runner])\r\n          local_step = 0\r\n          while True:\r\n            train_feed = {model.input: X_train, targets: Y_train}\r\n    \r\n            _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n            loss = sess.run(total_loss, feed_dict = train_feed)\r\n            if is_chief:\r\n              sess.run(inc_epoch_op)\r\n            local_step += 1\r\n            print '## epoch ', epoch.eval(sess)\r\n            if epoch.eval(sess) > 4:\r\n              print '######################  TRYING TO LEAVE'\r\n              break\r\n    \r\n        shutil.rmtree(train_dir)\r\n        print '######################  WHILE LOOP LEFT'\r\n        sess.run(enqueue_op)\r\n        print '## ENQUEUE OP DONE'\r\n    \r\n", "Please check the documentation of MonitoredSession. \r\nyou need to remove following lines:\r\n```\r\nlocal_init_op = opt.local_step_init_op\r\n    if is_chief:\r\n      local_init_op = opt.chief_init_op\r\n    ready_for_local_init_op = opt.ready_for_local_init_op\r\n\r\n    # Initial token and chief queue runners required by the sync_replicas mode\r\n    chief_queue_runner = opt.get_chief_queue_runner()\r\n    sync_init_op = opt.get_init_tokens_op()\r\n\r\n    init_op = tf.global_variables_initializer()\r\n```\r\n\r\nAlso followings:\r\n```\r\n   sv = tf.train.Supervisor(\r\n                             is_chief=is_chief,\r\n                             logdir=train_dir,\r\n                             init_op=init_op,\r\n                             local_init_op=local_init_op,\r\n                             ready_for_local_init_op=ready_for_local_init_op,\r\n                             recovery_wait_secs=1,\r\n                             global_step=global_step)\r\n    \r\n    print '######################################### ALL CREATED'\r\n    #sess = sv.prepare_or_wait_for_session(server.target)\r\n```\r\nAlso followings:\r\n      if is_chief:\r\n        sess.run(sync_init_op)\r\n        sv.start_queue_runners(sess, [chief_queue_runner])\r\n\r\nalso can you delete train_dir after running enqueue_op.\r\n\r\n", "Thanks. It still does not work. Only the chief executes the train loop (even if in the code replicas_to_aggregate=nb_workers), and it ends with an exception> I put below the new code as well as the output of the chief worker \r\n\r\n    import os\r\n    import shutil\r\n    import tempfile\r\n    import numpy as np\r\n    import pandas as pd\r\n    import argparse\r\n    \r\n    from keras.models import Sequential\r\n    from keras.layers.core import Dense\r\n    from keras.regularizers import l2\r\n    import tensorflow as tf\r\n    import keras\r\n    \r\n    nb_samples = 50\r\n    nb_features = 5\r\n    X_train = np.random.randn(nb_samples * nb_features).reshape((nb_samples, nb_features))\r\n    Y_train = np.random.randn(nb_samples).reshape((nb_samples, 1))\r\n    \r\n    def build_keras_model(input_dim):\r\n      hidden_dim = 10\r\n    \r\n      model = Sequential()\r\n      model.add(Dense(input_dim = input_dim,\r\n                      output_dim=hidden_dim,\r\n                      activation='tanh'\r\n                      ))\r\n    \r\n      model.add(Dense(output_dim=1, activation='linear'))\r\n    \r\n      model.compile(loss='mse', optimizer='adam')\r\n      \r\n      return model\r\n    \r\n    \r\n    \r\n    \r\n    ################################################\r\n    # DISTRIBUTE\r\n    ################################################\r\n    \r\n    parser = argparse.ArgumentParser(description='tensorflow')\r\n    parser.add_argument('--job_name', dest='job_name')\r\n    parser.add_argument('--task_index', dest='task_index', default=0)\r\n    args = parser.parse_args()\r\n    \r\n    \r\n    ps_hosts = ['localhost:2222']\r\n    worker_hosts = ['localhost:2223', 'localhost:2224', 'localhost:2225', 'localhost:2226']\r\n    job_name = args.job_name\r\n    task_index = int(args.task_index)\r\n    \r\n    # Create a cluster from the parameter server and worker hosts.\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n      \r\n    server = tf.train.Server(cluster,\r\n                             job_name=job_name,\r\n                             task_index=task_index,\r\n                             config=tf.ConfigProto(log_device_placement=True,\r\n                                                   inter_op_parallelism_threads=1,\r\n                                                   intra_op_parallelism_threads=1))\r\n    \r\n    \r\n    if job_name =='ps':\r\n      with tf.device(\"/job:ps/task:0\"):\r\n        queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n      sess = tf.Session(server.target)\r\n      # wait until all workers are done\r\n      for i in range(len(worker_hosts)):\r\n        sess.run(queue.dequeue())\r\n    else:\r\n      with tf.device(tf.train.replica_device_setter(\r\n                                  worker_device=\"/job:worker/task:%d\" % task_index,\r\n                                  cluster=cluster)):\r\n    \r\n        keras.backend.set_learning_phase(1)\r\n        keras.backend.manual_variable_initialization(True)\r\n    \r\n        model = build_keras_model(nb_features)\r\n        preds = model.output\r\n        targets = tf.placeholder(tf.float32, [None, 1])\r\n        total_loss = tf.reduce_mean(\r\n                            keras.objectives.mean_squared_error(targets, preds))\r\n    \r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n        # For stopping management\r\n        epoch = tf.Variable(0, name=\"epoch\", trainable=False)\r\n        inc_epoch_op = tf.assign_add(epoch, 1)\r\n    \r\n        is_chief=(task_index == 0)\r\n    \r\n        opt = tf.train.AdamOptimizer()\r\n        num_workers = len(worker_hosts)\r\n        replicas_to_aggregate = num_workers\r\n        opt = tf.train.SyncReplicasOptimizer(\r\n                                             opt,\r\n                                             replicas_to_aggregate=replicas_to_aggregate,\r\n                                             total_num_replicas=num_workers,\r\n                                             name=\"sync_replicas\")\r\n    \r\n        train_op = opt.minimize(total_loss, global_step=global_step)\r\n        with tf.device(\"/job:ps/task:0\"):\r\n          queue = tf.FIFOQueue(len(worker_hosts), tf.int32, shared_name=\"done_queue\")\r\n          enqueue_op = queue.enqueue(1)\r\n     \r\n        train_dir = tempfile.mkdtemp(prefix = 'worker_%d' % task_index)\r\n        \r\n        print '######################################### ALL CREATED'\r\n        sync_replicas_hook = opt.make_session_run_hook(is_chief)\r\n        with tf.train.MonitoredTrainingSession(\r\n                   master=server.target, is_chief=is_chief, checkpoint_dir=train_dir,\r\n                         hooks=[sync_replicas_hook]) as sess:\r\n          keras.backend.set_session(sess)\r\n          print '#######  SESSION OK ********'\r\n          local_step = 0\r\n          while True:\r\n            train_feed = {model.input: X_train, targets: Y_train}\r\n    \r\n            _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n            loss = sess.run(total_loss, feed_dict = train_feed)\r\n            if is_chief:\r\n              sess.run(inc_epoch_op)\r\n            local_step += 1\r\n            print '## epoch ', epoch.eval(sess)\r\n            if epoch.eval(sess) > 4:\r\n              print '######################  TRYING TO LEAVE'\r\n              break\r\n    \r\n        print '######################  WHILE LOOP LEFT'\r\n        sess.run(enqueue_op)\r\n        print '## ENQUEUE OP DONE'\r\n        shutil.rmtree(train_dir)\r\n    \r\n\r\nand this is the output of the chief worker \r\n\r\n    #######  SESSION OK ********\r\n    ## epoch  1\r\n    ## epoch  2\r\n    ## epoch  3\r\n    ## epoch  4\r\n    ## epoch  5\r\n    ######################  TRYING TO LEAVE\r\n    Exception in thread Thread-2:\r\n    Traceback (most recent call last):\r\n      File \"/usr/lib64/python2.7/threading.py\", line 811, in __bootstrap_inner\r\n        self.run()\r\n      File \"/usr/lib64/python2.7/threading.py\", line 764, in run\r\n        self.__target(*self.__args, **self.__kwargs)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 250, in _run\r\n        coord.request_stop(e)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 211, in request_stop\r\n        six.reraise(*sys.exc_info())\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n        sess.run(enqueue_op)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 767, in run\r\n        run_metadata_ptr)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 965, in _run\r\n        feed_dict_string, options, run_metadata)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n        target_list, options, run_metadata)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    CancelledError: RunManyGraphs\r\n    \r\n    Traceback (most recent call last):\r\n      File \"test.py\", line 148, in <module>\r\n        break\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 478, in __exit__\r\n        self._close_internal(exception_type)\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 511, in _close_internal\r\n        self._sess.close()\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 739, in close\r\n        self._sess.close()\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/monitored_session.py\", line 827, in close\r\n        self._coord.join()\r\n      File \"../tensorflow-1.0.0/lib/tensorflow/python/training/coordinator.py\", line 390, in join\r\n        \" \".join(stragglers))\r\n    RuntimeError: Coordinator stopped with threads still running: Thread-2", "@jmchen-g could you please review the code?", "Looks like that the session trains well but couldn't go outside the loop\nbecause some threads are still running (the queue runner thread in this\ncase)...\n\nOn Fri, Mar 10, 2017 at 2:29 PM, ispirmustafa <notifications@github.com>\nwrote:\n\n> @jmchen-g <https://github.com/jmchen-g> could you please review the code?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7970#issuecomment-285802619>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APD49jfjsRwXleF4iVdQfEeMNio2OIS0ks5rkc7egaJpZM4MPifr>\n> .\n>\n", "Please confirm that the training went normal during these epocs. Thanks.", "Indeed training goes well, and I see the loss decreasing when I print it in the chief worker. However, the workers could not go outside the loop as you mentioned.\r\nAnother issue is that it seems only the chief is doing the training, even if in the constructor of `SyncReplicasOptimizer` I have the parameter `replicas_to_aggregate=num_workers`", "No good samaritan to help with this issue?", "What do you mean by only the chief is doing the training? Only the chief will update the variables through the chief queue runner but it should use the grads from all available workers...\r\n\r\nFor the issue that when training finished successfully, the other workers should at least exit via time out unless the time out is set to infinity?", "I mean in the training loop, the print statement \r\n\r\n     print '## epoch ', epoch.eval(sess)\r\n\r\ngot executed only by the chief.\r\nIf I repeat my experiment several times, it may get executed by the other workers from time to time.\r\nI thought in SyncReplicasOptimizer the chief was supposed to wait until all the workers are live before beginning the optimization (indeed, `replicas_to_aggregate=num_workers` in my code)", "The chief is supposed to wait until enough grads are collected so not necessarily from all workers.\r\n\r\nHmm... You guys are using the tf.train.replica_device_setter(..) which doesn't tell global from local variables (We only notice this recently!)... This is not right and we are trying to fix it. Please wait until that is fixed. Thanks.", "Thank you", "When I use TF 1.1.0,The chief worker throws the following error :\r\nException in thread Thread-7:\r\nTraceback (most recent call last):\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 250, in _run\r\n    coord.request_stop(e)\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 211, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/export/App/anaconda3/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nCancelledError: Step was cancelled by an explicit call to `Session::Close()`.\r\n", "Hi @jmchen-g,\r\n\r\nI'm facing the same kind of issues using tensorflow 1.0.0. In a distributed run, for most of the runs the non-chief workers get stuck and log from time to time:\r\n\r\n> INFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized:\r\n\r\nIn some runs, they start working and print some logs as expected:\r\n\r\n> INFO:tensorflow:For task_index 1, at step 88714, cost is 0.00616120453924\r\n\r\nIt might be related to the issue you mentionned as I'm also using\r\n`tf.train.replica_device_setter(\"/job:worker/task:%d\" % FLAGS.task_index)`\r\n\r\nI wonder if you could give any insights and/or workarounds on the issue?\r\n\r\nMany thanks,\r\n\r\n", "@ispirmustafa, any further thoughts about this?", "The device setter issue should have already been fixed in the latest branch. When did you pull?", "Can you let me know the fix commit? What was the problem with replica_device_setter. I experienced same problem, but I just want to modify this solution instead of pulling latest branch.", "The setter issue should have already been fixed in the latest head. Do you\nstill have this after syncing?\n\nOn Thu, Jul 13, 2017 at 8:01 AM, sj6077 <notifications@github.com> wrote:\n\n> Can you let me know the fix commit?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7970#issuecomment-315104227>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APD49sZI-jz2GZmFhMepvXG0FCM4P319ks5sNjFBgaJpZM4MPifr>\n> .\n>\n", "I need to keep the version, so I just want to modify that parts. When was it fixed? The last commit in my repo is May 4. I also want to catch up the reason of the issue.", "I also tested with the latest head, but I still got same error with @fanlu ", "@npfp Hi, I have encountered the same problem. Have you solved that? \r\n\r\n> \"INFO:tensorflow:Waiting for model to be ready. Ready_for_local_init_op: None, ready: Variables not initialized:\"", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "is this still an issue with tensorflow? otherwise, please close the issue", "I think it's okay, now. Can you close it?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@sj6077 \r\nI am also facing the problem @fanlu had.  Could you share how you fix that? Thanks. ", "Have the similar issue with @fanlu 's exception after training in `TensorFlow 1.8.0`."]}, {"number": 7969, "title": "Bottom-right precedence of padding?", "body": "Why has Tensorflow chosen to prefer padding on the bottom right?\r\nWith 'SAME' padding, to me it would feel logical to start the kernel's center anchor at the first real pixel. Due to the use of asymmetric padding, this results in a discrepancy with some other frameworks. I do understand that asymmetric padding in principle is good because otherwise one would be left with an unused padding row/column.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/7721540/23457266/e6b57d8e-fe7f-11e6-9b88-dc793ce10c73.png)\r\n", "comments": ["@TimZaman Thanks for the comment.  I assume you've seen the following documentation, since you understand how SAME padding works:\r\nhttps://www.tensorflow.org/api_guides/python/nn#Convolution\r\n\r\nBut this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Because Tensorflow does this differently than other frameworks, it breaks weight compatibility with other frameworks. If the padding precedence was given to the left and top rows, it would align with other frameworks (and cudnn). Therefore I asked the question here, and ask the developers directly why this was chosen in this way."]}]