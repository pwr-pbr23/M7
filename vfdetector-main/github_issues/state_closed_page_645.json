[{"number": 34268, "title": "tf.image.non_max_suppression bug when iou_threshold=0.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): both Mac and Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\nI'm running following code:\r\n```\r\nboxes = [[ 90., 480., 106., 496.],\r\n         [269.33333333, 536.33333333, 282.66666667, 549.66666667],\r\n         [382.33333333, 556.33333333, 395.66666667, 569.66666667]]\r\nresult = tf.image.non_max_suppression(boxes, [1, 1, 1], 3, iou_threshold=0.0)\r\n    with tf.Session() as sess:\r\n        out = sess.run([result])\r\n        indexes = out[0]\r\nprint(len(indexes))\r\n```\r\nWhich prints out: 1 in `tensorflow==1.15.0`.\r\n\r\n**Describe the expected behavior**\r\nExpected behaviour is printing 3 as in tensorflow==1.13.1.\r\nI've found a workaround for tensorflow==1.15.0 as setting io_threshold to some very small number, eg. `iou_threshold=0.001`, but the behaviour, probably a bug when `iou_threshold=0.0` is very very unexpected.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nboxes = [[ 90., 480., 106., 496.],\r\n         [269.33333333, 536.33333333, 282.66666667, 549.66666667],\r\n         [382.33333333, 556.33333333, 395.66666667, 569.66666667]]\r\nresult = tf.image.non_max_suppression(boxes, [1, 1, 1], 3, iou_threshold=0.0)\r\n    with tf.Session() as sess:\r\n        out = sess.run([result])\r\n        indexes = out[0]\r\nprint(len(indexes))\r\n```\r\n\r\nThis seems to me probably as a bug in case when iou_threshold=0.0. The bug was not present at tensorflow==1.13.1, but it's in tensorflow==1.15.0.", "comments": ["I see that actually this \"corner case\" is probably not tested in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_test.py it looks to me that the functionality is only tested for `iou_threshold=0.5`.", "I tried in colab and was able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/79a3a8395a6edb27e870bac86cd9035f/untitled369.ipynb). Thanks!", "@ravikyram @rmothukuru btw. the very same bug seems to be also in tensorflow==2.0.0, so maybe you can add also label `TF 2.0`?\r\n\r\nYou can reproduce it by:\r\n```\r\nboxes = [[ 90., 480., 106., 496.],\r\n         [269.33333333, 536.33333333, 282.66666667, 549.66666667],\r\n         [382.33333333, 556.33333333, 395.66666667, 569.66666667]]\r\nindexes = tf.image.non_max_suppression(boxes, [1, 1, 1], 3, iou_threshold=0.0)\r\nprint(len(indexes))\r\n```\r\nwhich prints 1\r\n\r\nand\r\n```\r\nboxes = [[ 90., 480., 106., 496.],\r\n         [269.33333333, 536.33333333, 282.66666667, 549.66666667],\r\n         [382.33333333, 556.33333333, 395.66666667, 569.66666667]]\r\nindexes = tf.image.non_max_suppression(boxes, [1, 1, 1], 3, iou_threshold=0.001)\r\nprint(len(indexes))\r\n```\r\nwhich prints 3\r\n\r\nIn both cases expected result is 3", "Could reproduce the issue with 2.0 as well. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/6e27abeb2ef09c6384fa64e038371a47/untitled369.ipynb#scrollTo=sPsaV2E7ua0S). ", "@jch1 I think you wrote this code, can you take a look?", "@alextp @jch1 any progress looking into this issue?", "Was able to reproduce the issue with Tf-nightly.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/1de3e0bedb94476be842fcc8923f2e8a/untitled470.ipynb). Thanks", "@danielcdh can you take over this issue? (you should be in the TF github org)", "@jinliangwei will work on this issue. Both of us are in the process to be added to TF github org.", "The real question here is what is the correct definition for \"iou_threshold\"?\r\n\r\nAccording to Bodla et al. (http://www.cs.umd.edu/~bharat/snms.pdf), NMS suppresses box_a or box_b depending on their score, if iou(box_a, box_b) >= iou_threshold, which is what the current implementation does, i.e., select only one box when iou_threshold = 0.0 in the above example.\r\n\r\nIn previous implementations, NMS suppresses box_a or box_b, if iou(box_a, box_b) > iou_threshold, and thus select three boxes when iou_threshold = 0.0 in the above example.\r\n\r\nWhat definition do we really want? @jch1 Would you please comment?", "Issue persists in [TF v2.2](https://colab.research.google.com/gist/amahendrakar/d5e885eb1cef9d2a157c5ca9c9e3c004/34268.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/22694d3212c28afe0a8f2208bf0b8c4b/34268-tf-nightly.ipynb) i.e. v2.3.0-dev20200527. Please find the attached gist. Thanks!", "It is fixed in nightly.", "@ziky90,\r\nAs per [bhack's comment](https://github.com/tensorflow/tensorflow/issues/34268#issuecomment-840717628) this bug is fixed in **`Nightly Version of Tensorflow (2.6.0-dev20210516)`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/60ff1d38ba9af0d7ffffbd452f07cbdf/34268-tf-nightly.ipynb#scrollTo=JrpVby8YUwdF) of the working code. \r\n\r\nCan you please let us know if we can close this issue? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34268\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34268\">No</a>\n"]}, {"number": 34267, "title": "Invoking Interpreter from C API fails with exit code 1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 and Android 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10\r\n- TensorFlow installed from (source or binary): Little bit of both\r\n- TensorFlow version: 1.14, 1.15, 2.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Pip and source\r\n- Bazel version (if compiling from source): 26.1, 29.1\r\n- GCC/Compiler version (if compiling from source): 7.4\r\n- CUDA/cuDNN version: 10.1, 7\r\n- GPU model and memory: 1050ti and whatever is in my phone\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen I build the tflite C api (currently on 1.14 after testing tf-nightly 1.15 and 2.0 (which I found out is not ready yet?)), I get a variety of weird errors. I have a simple tflite SSD model provided from the tf model zoo, which takes a 300x300x3 float32 input and outputs the usual 4 arrays of the detection_postprocess node provided by the tflite team. \r\n\r\nWhen I feed it a float array like in the Unity example, it crashes. I can either feed it a byte array or trick it into accepting the actual float array by dividing the input_data_size argument by 4, since it believes the TFL_TensorByteSize is 270000 when it should be 1080000.\r\n\r\nEither way, when I call TFL_InterpreterInvoke, it crashes, and I haven't been able to follow the C api code to figure out why. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFirst:\r\n```\r\nGCHandle tensorDataHandle = GCHandle.Alloc(inputTensorData, GCHandleType.Pinned);\r\n        IntPtr tensorDataPtr = tensorDataHandle.AddrOfPinnedObject();\r\n        IntPtr tensor = TFL_InterpreterGetInputTensor(interpreter, inputTensorIndex);\r\n\r\n        int tensorByteSize = TFL_TensorByteSize(tensor);\r\n        int inputDimsSize = Buffer.ByteLength(inputTensorData);\r\n\r\n        Debug.Log(tensorByteSize + \" | \" + inputDimsSize);\r\n        \r\n        ThrowIfError(TFL_TensorCopyFromBuffer(\r\n            tensor, tensorDataPtr, Buffer.ByteLength(inputTensorData)));\r\n```\r\nThis works with a byte array or a float array with the third argument like `Buffer.ByteLength(inputTensorData) / 4`\r\n\r\nThen\r\n```\r\nThrowIfError(TFL_InterpreterInvoke(interpreter));\r\n```\r\nBut since both of the input methods are actually wrong, it of course crashes with error code 1. \r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI am able to load the model using the python tf.lite.Interpreter like so:\r\n```\r\nmodel = tf.compat.v2.lite.Interpreter(\"/path/detecter.tflite\")\r\n```\r\nAll of the following calls, until model.invoke, work fine too. However, I can only model.invoke if the set_tensor call before uses a copy of x, otherwise it complains about references to internal data:\r\n```\r\nRuntimeError: There is at least 1 reference to internal data\r\n      in the interpreter in the form of a numpy array or slice. Be sure to\r\n      only hold the function returned from tensor() if you are using raw\r\n      data access.\r\n```\r\n\r\nSo to summarize; this works:\r\n```\r\nx = np.random.normal(size=(1,300,300,3))\r\nprint(x)\r\nmodel.set_tensor(tensor_index=175,value=np.copy(x))\r\nmodel.invoke()\r\n```\r\nBut this does not:\r\n```\r\nx = np.random.normal(size=(1,300,300,3))\r\nprint(x)\r\nmodel.set_tensor(tensor_index=175,value=x)\r\nmodel.invoke()\r\n```\r\n\r\nDoes TFL_TensorCopyFromBuffer not take ownership of the copy and fear breaking it?\r\n\r\nRegardless, I can't seem to find any information about this and can't find the documentation to follow it myself, so any help is appreciated.\r\n\r\nPS. When is the 2.0 C API ready to roll out? The new converter and tf2.0 control flow ops are very appealing.", "comments": ["A few quick updates: \r\nI tried a similar trick with cloning the input array in my script, but to no avail. Using a cloned Array for input did not help.\r\n\r\nI also tested a similar network that I've called detect_new. This network shows the correct input dimensions (byte size 1080000) and will run, but crashes out of Unity entirely when invoked without any error codes.", "Just to be clear, is this crashing both on device (Android) and in the Unity Editor (on Linux)? Or with just one?\r\n\r\nWould you mind sharing the exact model that fails? Also, are there any other failure logs in the editor log or device log that are relevant?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34267\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34267\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 34266, "title": "Cannot convert models to the *.tflite format with GRU/GRUCells", "body": "Despite the declared support (https://www.tensorflow.org/lite/convert/rnn), it seems, it is not possible to convert the existing TF 2.0 model with GRU/GRUCells to the *.tflite format.\r\nWhere is the problem?\r\n\r\nHere is the sample:\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras import Sequential\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = Sequential()\r\nmodel.add(Input((10, 1)))\r\n#model.add(GRU(1))\r\n#model.add(RNN(tf.compat.v1.nn.rnn_cell.GRUCell(1)))\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n\r\nX = np.random.uniform(size=(10, 10, 1))\r\nY = np.random.uniform(size=(10, 1))\r\nmodel.fit(X, Y)\r\n\r\nmodel.save('model.h5', include_optimizer=False)\r\n\r\nAnd the logs:\r\n\r\n1. For the GRU:\r\n2019-11-13 18:31:17.808698: E tensorflow/lite/toco/toco_tooling.cc:498] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/harry/.local/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: RESHAPE, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n2. For the GRUCells:\r\nTraceback (most recent call last):\r\n  File \"/home/harry/.local/bin/toco\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 594, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 577, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/lite/python/tflite_convert.py\", line 228, in _convert_tf2_model\r\n    model = keras.models.load_model(flags.keras_model_file)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 168, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 303, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 377, in from_config\r\n    custom_objects=custom_objects)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 303, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 958, in from_config\r\n    cell = deserialize_layer(config.pop('cell'), custom_objects=custom_objects)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 305, in deserialize_keras_object\r\n    return cls.from_config(cls_config)\r\n  File \"/home/harry/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 519, in from_config\r\n    return cls(**config)\r\nTypeError: __init__() missing 1 required positional argument: 'units'\r\n", "comments": ["pls use LSTM \r\n", "Hi,\r\n\r\nWe have released the new MLIR-based TF Lite converter. Can you please use the latest tf-nightly pip package, and then set converter.experimental_new_converter = True. Try convert it and let me know if you have any issues.", "Thanks for the suggestions above. \r\n1. Unfortunately, we do not want to switch the current architecture to LSTM.\r\n2. We tried the nightly build with converter.experimental_new_converter = True included and have successfully converted the model into a .tflite format. \r\nBut we can't run it under the device either. The log is the following:\r\n```\r\n2019-10-18 17:21:08.904 2379-2379/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x24\r\n2019-10-18 17:21:08.904 2379-2379/? A/DEBUG: Cause: null pointer dereference\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG: backtrace:\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG:       #00 pc 0014f83c  /data/app/com.example.testtflite-ZYXaKdbbZTvM7reT0E3AKw==/lib/arm/libtensorflowlite.so (tflite::Interpreter::AllocateTensors())\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG:       #01 pc 000025ad  /data/app/com.example.testtflite-ZYXaKdbbZTvM7reT0E3AKw==/lib/arm/libtest-tflite.so (Java_com_example_testtflite_MainActivity_testTFliteJNI+300) (BuildId: 83603e68ca620851b360d9cec44918f3e29c10d0)\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG:       #02 pc 000dc519  /apex/com.android.runtime/lib/libart.so (art_quick_generic_jni_trampoline+40) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG:       #03 pc 000d7bc5  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub_internal+68) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG:       #04 pc 0042ec67  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub+250) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG:       #05 pc 000dffb7  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+174) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.127 2379-2379/? A/DEBUG:       #06 pc 00210ca1  /apex/com.android.runtime/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+280) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.128 2379-2379/? A/DEBUG:       #07 pc 0020c5e3  /apex/com.android.runtime/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+774) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.128 2379-2379/? A/DEBUG:       #08 pc 00423f39  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+556) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.128 2379-2379/? A/DEBUG:       #09 pc 000d2814  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+20) (BuildId: 1293e54ab8aab4c227f7f4fe128764a5)\r\n2019-10-18 17:21:09.136 2379-2379/? A/DEBUG:       #79 pc 000a3351  /system/lib/libandroid_runtime.so (_JNIEnv::CallStaticVoidMethod(_jclass*, _jmethodID*, ...)+28) (BuildId: 6e5f5bb42191731a49879c6d8896c22e)\r\n2019-10-18 17:21:09.136 2379-2379/? A/DEBUG:       #80 pc 000a58d9  /system/lib/libandroid_runtime.so (android::AndroidRuntime::start(char const*, android::Vector<android::String8> const&, bool)+508) (BuildId: 6e5f5bb42191731a49879c6d8896c22e)\r\n2019-10-18 17:21:09.136 2379-2379/? A/DEBUG:       #81 pc 000022ff  /system/bin/app_process32 (main+698) (BuildId: deaf4765ec05ab374756aaef2c373c0a)\r\n2019-10-18 17:21:09.136 2379-2379/? A/DEBUG:       #82 pc 0005a8f9  /apex/com.android.runtime/lib/bionic/libc.so (__libc_init+68) (BuildId: 97e84ef7f774f86432848e5f1ffb1ecd)\r\n2019-10-18 17:21:09.136 2379-2379/? A/DEBUG:       #83 pc 0000202f  /system/bin/app_process32 (_start_main+38) (BuildId: deaf4765ec05ab374756aaef2c373c0a)\r\n```\r\nWe suppose, such a conversion requires the 'nightly' version of tensorflowLite.so that is not acceptable for the moment.\r\n3.  We changed the code to:  \r\n`model.add(GRU(1), unroll = True)`\r\nand/or\r\n`model.add(RNN(GRUCell(1), unroll = True))`\r\nand have successfully converted and run this model under the device. \r\nIt seems this might be a solution for a while.\r\n\r\nBest,\r\nIgor\r\n", "With unroll = True, the control flow op is eliminated, so you can run it with an old tensorflowLite.so. But if you want to run it on device without the change of your code, you need to download the tf-nightly tensorflowLite.so file.", "@i-tolmachev \r\nIs this still an issue.", "This is fixed with latest TF versions (tested with TF 2.4). See [gist](https://colab.research.google.com/gist/ymodak/1774a32e85ec0adff2db3136662b8d3b/untitled14.ipynb) for your reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34266\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34266\">No</a>\n", "We have successfully converted the Keras model to the tflite format. Unfortunately, when building the interpreter with the converted tflite model, we have got the following error message:\r\n\"Didn't find op for builtin opcode '\u2592\\\u2592\u2592' version '-258516128'\r\nRegistration failed.\"\r\nWe appreciate any suggestions on this issue.", "It seems that opcode and version is messed up.. could you try to run it in tf-nightly?"]}, {"number": 34265, "title": "Updating metrics_export_meta_graph.pb for fixing //tensorflow/python:framework_meta_graph_test", "body": "The testdata in [metrics_export_meta_graph.pb](https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/framework/testdata/metrics_export_meta_graph.pb#L506) is consisting of tensors serialized in little endian format. This causes //tensorflow/python:framework_meta_graph_test to fail on big endian. By replacing the tensor_content bytes as float_val/int_val the test passes on x86_64 and s390x architectures.\r\n", "comments": ["@sherrym @jaingaurav @rmlarsen could you please check?", "Any updates on this?"]}, {"number": 34264, "title": "API function changed from Keras 2.3.1 to Keras 2.4.2?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version : `TF 2.0.0 stable + Keras 2.4.2 `(A )and `1.13.1 +Keras 2.3.1` (B)\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI want to do a transfer learning on MobileNet and change the input size ,i copied the code [colab](https://colab.research.google.com/drive/1s1lkOiqKgl9_MsXJtPF--oh6uXHYkNYm#scrollTo=dV7a_Jca4cgC) according to [Changing input size of pre-trained models in Keras](https://medium.com/@ckyrkou/changing-input-size-of-pre-trained-models-in-keras-3dfbe3ca3091) .Everythin goes fine on my machine **B**(TF 1.13.1 + Keras 2.3.1) ,i could got exact the same output of author namely the input size got changed,while on my another machine **A**(TF 2..0.0+ Keras 2.4.2) i have to do some change and the result of input shape stays 224x224x3 but not as excepted 130x130x3.\r\n\r\n+ Tensorflow 1.13.1+ Keras 2.3.1 works fine\r\n\r\n```\r\n# work on tensorflow 1.13.1 \r\nimport keras\r\nimport numpy as np\r\nkeras.backend.clear_session()\r\n\r\ndef change_model(model, new_input_shape=(None, 40, 40, 3)):\r\n    # replace input shape of first layer\r\n    model._layers[0].batch_input_shape = new_input_shape\r\n\r\n    # rebuild model architecture by exporting and importing via json\r\n    new_model = keras.models.model_from_json(model.to_json())\r\n\r\n    # copy weights from old model to new one\r\n    for layer in new_model.layers:\r\n        try:\r\n            layer.set_weights(model.get_layer(name=layer.name).get_weights())\r\n            print(\"Loaded layer {}\".format(layer.name))\r\n        except:\r\n            print(\"Could not transfer weights for layer {}\".format(layer.name))\r\n    return new_model\r\n\r\nfrom keras.applications.mobilenet import MobileNet\r\nfrom keras.preprocessing import image\r\nfrom keras.applications.mobilenet import preprocess_input,decode_predictions\r\nimport numpy as np\r\nmodel = MobileNet(weights='imagenet',include_top=True,input_shape=(224, 224,3))\r\nnew_model = change_model(model,new_input_shape=(None, 130, 130, 3))\r\nnew_model.summary()\r\n```\r\nouput is 130x130x3\r\n```\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         (None, 130, 130, 3)       0\r\n_________________________________________________________________\r\nconv1_pad (ZeroPadding2D)    (None, 131, 131, 3)       0\r\n_________________________________________________________________\r\nconv1 (Conv2D)               (None, 65, 65, 32)        864\r\n_________________________________________________________________\r\nconv1_bn (BatchNormalization (None, 65, 65, 32)        128\r\n...\r\n```\r\n+ Tensorflow 2.0.0 + Keras 2.4.2 \r\n\r\n```\r\n# work on Tensorflow 2.0.0 \r\nimport keras\r\nimport tensorflow as tf\r\nfrom keras_applications.mobilenet import MobileNet\r\n\r\ndef change_model(model, new_input_shape=(None,40, 40, 3)):\r\n    # replace input shape of first layer\r\n    model._layers[0].batch_input_shape = new_input_shape\r\n    # rebuild model architecture by exporting and importing via json\r\n    new_model = tf.keras.models.model_from_json(model.to_json())\r\n    # copy weights from old model to new one\r\n    for layer in new_model.layers:\r\n        try:\r\n            layer.set_weights(model.get_layer(name=layer.name).get_weights())\r\n            print(\"Loaded layer {}\".format(layer.name))\r\n        except:\r\n            print(\"Could not transfer weights for layer {}\".format(layer.name))\r\n    return new_model\r\n\r\nmodel = MobileNet(include_top=True,weights=\"imagenet\",input_shape=(224,224,3),backend = tf.keras.backend, layers = tf.keras.layers, models = tf.keras.models, utils = tf.keras.utils)\r\nnew_model = change_model(model,new_input_shape=(None,130,130,3))\r\nprint(new_model.summary())\r\n```\r\noutput stays 224x224x3\r\n\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \r\n_________________________________________________________________\r\nconv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \r\n_________________________________________________________________\r\nconv1 (Conv2D)               (None, 112, 112, 32)      864       \r\n_________________________________________________________________\r\nconv1_bn (BatchNormalization (None, 112, 112, 32)      128       \r\n_________________________________________________________________\r\nconv1_relu (ReLU)            (None, 112, 112, 32)      0         \r\n_________________________________________________________________\r\n....\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nnot sure.\r\n\r\n", "comments": ["I noticed that different versions of the keras can return different layer shapes.\r\nThe output shape of the layer is different in the keras and tf.keras: https://github.com/tensorflow/tensorflow/issues/33785", "Could reproduce the issue by changing the TF Version from 1.13 to 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/oanush/47bf6c47011dacd95beca106b25d654e/34264.ipynb). Thanks!", "Hi @shartoo , with the current implementation, you may use the [`_batch_input_shape`](https://github.com/tensorflow/tensorflow/blob/a36fbd6a82d91f47cda9cc8da26d6a83d551b42a/tensorflow/python/keras/engine/base_layer.py#L374) for the attribute assignment to Keras Layer or Model. Can you try it to see if it works? Thanks.", "@shartoo  Closing this issue as it is the intended behaviour.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34264\">No</a>\n"]}, {"number": 34263, "title": "lite: enable (u)int8 quantization and int32 for ABS", "body": "Remake of PR #31914 (the branch is recreated, leading to impossible to reopen). The change in this PR removes the *key reordering* of the last patch, nothing bit change.\r\n\r\nPlease have look into this @wangtz @jianlijianli @suharshs .", "comments": ["@liyunlu0618 Would you like to take a look at this?", "@jackwish \r\n\r\nCan you please resolve conflicts? Thanks!", "> @jackwish\r\n> Can you please resolve conflicts? Thanks!\r\n\r\nResolved. Would you mind to push this PR to be reviewed? I have been getting tired of the conflict resolving process. @ravikyram ", "Would you mind to push the *internal safe review approval* a bit? I am afraid that conflicts may come as operators are changing frequently, which takes time from us all...", "Hmm, strange I have approved, but am not seeing the internal review request.", "Trying re-approving", "Thanks for your kind effort @suharshs ", "I have checked the failed list of ***Ubuntu CPU \u2014 Internal CI build failed***, seems not related to this patch set.", "@jackwish Could you please check other failed build errors? Thanks!\r\n", "> @jackwish Could you please check other failed build errors? Thanks!\r\n\r\nTried locally, no failures found. Would you please re-trigger the test or build?\r\n\r\nPS. We don't know how the patch is tested? Is it a *checkout and test* or *cherry pick and test*?", "@jackwish  can you please check below error : \r\n`ERROR: /tmpfs/src/github/tensorflow/tensorflow/lite/kernels/BUILD:414:1: Couldn't build file tensorflow/lite/kernels/_objs/builtin_op_kernels/abs.pic.o: C++ compilation of rule '//tensorflow/lite/kernels:builtin_op_kernels' failed (Exit 1)\r\n[8,502 / 11,019] Compiling tensorflow/lite/kernels/reduce.cc; 17s remote ... (200 actions, 199 running)\r\n[9,964 / 11,023] Compiling tensorflow/lite/toco/tflite/operator.cc; 27s remote ... (193 actions, 186 running)\r\n[13,224 / 14,698] Compiling tensorflow/compiler/mlir/lite/flatbuffer_translate.cc; 24s remote ... (198 actions, 197 running)\r\ntensorflow/lite/kernels/abs.cc:18:10: fatal error: tensorflow/lite/c/c_api_internal.h: No such file or directory\r\n #include \"tensorflow/lite/c/c_api_internal.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.`\r\n\r\nlink for more detailed logs : https://source.cloud.google.com/results/invocations/f20488f8-b408-4b40-a556-246e08af48c4/log\r\n", "Too long merging progress."]}, {"number": 34262, "title": "train with muliple gpu get error: Out of range:  End of sequence", "body": "I am training with tensorflow2.0 with multiple GPU. It got the following errors. But if I use only one GPU it ran without any error. My tensorflow version is `tensorflow-gpu-2.0.0`:\r\n```\r\ntensorflow.python.framework.errors_impl.CancelledError: 4 root error(s) found.\r\n  (0) Cancelled:  Operation was cancelled\r\n     [[{{node cond_6/else/_59/IteratorGetNext}}]]\r\n  (1) Out of range:  End of sequence\r\n     [[{{node cond_4/else/_37/IteratorGetNext}}]]\r\n  (2) Out of range:  End of sequence\r\n     [[{{node cond_7/else/_70/IteratorGetNext}}]]\r\n     [[metrics/accuracy/div_no_nan/ReadVariableOp_6/_154]]\r\n  (3) Out of range:  End of sequence\r\n     [[{{node cond_7/else/_70/IteratorGetNext}}]]\r\n0 successful operations.\r\n1 derived errors ignored. [Op:__inference_distributed_function_83325]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function -> distributed_function\r\n````\r\n\r\n**and this is my code:**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\ndata_name = 'uc_merced'\r\ndataset = tfds.load(data_name)\r\ntrain_data, test_data = dataset['train'], dataset['train']\r\n\r\ndef parse(img_dict):\r\n    img = tf.image.resize_with_pad(img_dict['image'], 256, 256)\r\n    label = img_dict['label']\r\n    return img, label\r\n\r\ntrain_data = train_data.map(parse)\r\ntrain_data = train_data.batch(96)\r\n\r\ntest_data = test_data.map(parse)\r\ntest_data = test_data.batch(96)\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n    model = tf.keras.applications.ResNet50(weights=None, classes=21, input_shape=(256, 256, 3))\r\n    model.compile(optimizer='adam',\r\n            loss='sparse_categorical_crossentropy',\r\n            metrics=['accuracy'])\r\n\r\n\r\nmodel.fit(train_data, epochs=50, verbose=2, validation_data=test_data)\r\nmodel.save('model/resnet_{}.h5'.format(data_name))\r\n```", "comments": ["Can you please provide the full stack trace so we can debug this further? How many GPUs did you run this with? \r\nAlso can you test with the latest TF 2 nightly? \r\n\r\n(For reference, I tried with the latest TF and not able to repro)", "I ran with 4 GPUs. This thefull stack trace:\r\n```\r\nEpoch 1/50\r\n22/22 - 78s - loss: 3.4754 - accuracy: 0.2224 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\r\nEpoch 2/50\r\n22/22 - 9s - loss: 2.1286 - accuracy: 0.3386\r\nTraceback (most recent call last):\r\n  File \"simple.py\", line 76, in <module>\r\n    model.fit(train_data, epochs=50, verbose=2, validation_data=test_data)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 370, in fit\r\n    total_epochs=1)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 494, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/soft/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.CancelledError: 5 root error(s) found.\r\n  (0) Cancelled:  Operation was cancelled\r\n\t [[{{node cond_7/else/_70/IteratorGetNext}}]]\r\n  (1) Out of range:  End of sequence\r\n\t [[{{node cond_6/else/_59/IteratorGetNext}}]]\r\n\t [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_103]]\r\n  (2) Out of range:  End of sequence\r\n\t [[{{node cond_5/else/_48/IteratorGetNext}}]]\r\n  (3) Out of range:  End of sequence\r\n\t [[{{node cond_6/else/_59/IteratorGetNext}}]]\r\n\t [[metrics/accuracy/div_no_nan/ReadVariableOp_4/_150]]\r\n  (4) Out of range:  End of sequence\r\n\t [[{{node cond_6/else/_59/IteratorGetNext}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_83325]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function -> distributed_function -> distributed_function\r\n```\r\nI will try tf-nightly later.\r\n", "@guptapriya I can not repro for tf-nightly-gpu. But for tensorflow-gpu-2.0 I reproduce the error every time. Is this a bug of tensorflow2.0? ", "@honeytidy that's possible, we've fixed a number of issues since TF 2.0 branch was cut. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34262\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34262\">No</a>\n"]}, {"number": 34261, "title": "[Intel MKL] Fix memory leak", "body": "", "comments": []}, {"number": 34260, "title": "ValueError when computing Jacobian in eager mode", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\nI'm trying to add jacobian regularization to our loss function for reduce the intputs of sensitivity.\r\nbut in tensorflow(version 1.14.0), it's hard to build the dynamic graph of jacobian regularization.\r\nI try to use \"GradientTape()\" for get the jacobian matrix. but there is problem to get the weight parameters's gradient.  \"raise ValueError(\"Tape is already recording.\")\", which is means we cannot use GradientTape() to get the jacobian regularization function. I guess there is another way to get it done. That's is build the dynamic graph of jacobian regularization function expliciltly. but this could be very hard for me. Is there any way to do this convenient\uff1f\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\ntensorflow version 1.14.0\r\n- Are you willing to contribute it (Yes/No):\r\nNo, Cause i'm not very famililar with tensorflow's framework.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI want to use the jacobian regurlarization to my model in tensorflow. \r\n**Will this change the current api? How?**\r\nyes,\r\n**Who will benefit with this feature?**\r\nexpert, researcher.\r\n**Any Other info.**\r\n", "comments": ["Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "@TakeByEarn \r\n\r\nWill it be possible to share minimal stand alone code to reproduce the issue reported here. Thanks! ", "@TakeByEarn \r\n\r\nAny update on this issue please. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34260\">No</a>\n"]}, {"number": 34259, "title": "fix debug_events_writer: avoid possible NULL pointer dereference", "body": "```cpp\r\nvoid DebugEventsWriter::WriteSerializedExecutionDebugEvent(\r\n    const string& debug_event_str, DebugEventFileType type) {\r\n  const std::unique_ptr<SingleDebugEventFileWriter>* writer = nullptr;\r\n  std::deque<string>* buffer = nullptr;\r\n  mutex* mu = nullptr;\r\n  switch (type) {\r\n    case EXECUTION:\r\n      writer = &execution_writer_;\r\n      buffer = &execution_buffer_;\r\n      mu = &execution_buffer_mu_;\r\n      break;\r\n    case GRAPH_EXECUTION_TRACES:\r\n      writer = &graph_execution_traces_writer_;\r\n      buffer = &graph_execution_trace_buffer_;\r\n      mu = &graph_execution_trace_buffer_mu_;\r\n      break;\r\n// 1. suppose default case was chosen\r\n    default:\r\n      break;\r\n  }\r\n\r\n  if (circular_buffer_size_ <= 0) {\r\n    // No cyclic-buffer behavior.\r\n\r\n// 2. null is passed as the this pointer to function operator->\r\n    (*writer)->WriteSerializedDebugEvent(debug_event_str);\r\n  } else {\r\n    // Circular buffer behavior.\r\n    mutex_lock l(*mu);\r\n\r\n// 3. null is passed as the this pointer to function push_back\r\n    buffer->push_back(debug_event_str);\r\n    if (buffer->size() > circular_buffer_size_) {\r\n      buffer->pop_front();\r\n    }\r\n  }\r\n}\r\n```", "comments": []}, {"number": 34258, "title": "fix debug_events_writer: avoid possible NULL pointer dereference", "body": "```cpp\r\nvoid DebugEventsWriter::WriteSerializedExecutionDebugEvent(\r\n    const string& debug_event_str, DebugEventFileType type) {\r\n  const std::unique_ptr<SingleDebugEventFileWriter>* writer = nullptr;\r\n  std::deque<string>* buffer = nullptr;\r\n  mutex* mu = nullptr;\r\n  switch (type) {\r\n    case EXECUTION:\r\n      writer = &execution_writer_;\r\n      buffer = &execution_buffer_;\r\n      mu = &execution_buffer_mu_;\r\n      break;\r\n    case GRAPH_EXECUTION_TRACES:\r\n      writer = &graph_execution_traces_writer_;\r\n      buffer = &graph_execution_trace_buffer_;\r\n      mu = &graph_execution_trace_buffer_mu_;\r\n      break;\r\n    default:\r\n      break;\r\n  }\r\n\r\n// 1. null assigned to buffer reaches here\r\n  if (circular_buffer_size_ <= 0) {\r\n    // No cyclic-buffer behavior.\r\n\r\n// 2. null is passed as the this pointer to function operator->\r\n    (*writer)->WriteSerializedDebugEvent(debug_event_str);\r\n  } else {\r\n    // Circular buffer behavior.\r\n    mutex_lock l(*mu);\r\n\r\n// 3. null is passed as the this pointer to function push_back\r\n    buffer->push_back(debug_event_str);\r\n    if (buffer->size() > circular_buffer_size_) {\r\n      buffer->pop_front();\r\n    }\r\n  }\r\n}\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34258) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34258) for more info**.\n\n<!-- need_author_cla -->", "I'll fix it in another pull request.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/34259 new PR here."]}, {"number": 34257, "title": "Check failed: dim_size >= 1 (0 vs. 1)", "body": "**System information** \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 12.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nbazel run --config=opt tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/install_bar/frozen_inference_graph_quantization.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=./frozen_inference_graph_quantization.tflite --inference_type=FLOAT \\\r\n  --input_data_types =FLOAT --input_arrays=[\"image_tensor\"] \\\r\n  --output_arrays=\"detection_boxes\",\"detection_scores\",\"detection_classes\",\"num_detections\",\"raw_detection_boxes\",\"raw_detection_scores\" \\\r\n  --input_shapes=1,299,299,3\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n2019-11-14 02:55:54.079426: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: SecondStagePostprocessor/map/TensorArray_2\r\n2019-11-14 02:55:54.079449: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.079540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.079595: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.079641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.079742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LoopCond\r\n2019-11-14 02:55:54.079824: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: SecondStagePostprocessor/map/while/LoopCond\r\n2019-11-14 02:55:54.079927: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayReadV3\r\n2019-11-14 02:55:54.080043: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.080130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-11-14 02:55:54.080154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.080274: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayReadV3\r\n2019-11-14 02:55:54.080337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.080365: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-11-14 02:55:54.080381: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.080610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayWriteV3\r\n2019-11-14 02:55:54.080645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: SecondStagePostprocessor/map/while/TensorArrayWrite/TensorArrayWriteV3\r\n2019-11-14 02:55:54.080667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-11-14 02:55:54.080696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-11-14 02:55:54.080727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\r\n2019-11-14 02:55:54.080834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArraySizeV3\r\n2019-11-14 02:55:54.080855: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: SecondStagePostprocessor/map/TensorArrayStack/TensorArraySizeV3\r\n2019-11-14 02:55:54.080950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayGatherV3\r\n2019-11-14 02:55:54.138743: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2132 operators, 3712 arrays (0 quantized)\r\n2019-11-14 02:55:54.229959: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1999 operators, 3467 arrays (0 quantized)\r\n2019-11-14 02:55:54.334917: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1999 operators, 3467 arrays (0 quantized)\r\n2019-11-14 02:55:54.385257: F tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)\r\nAborted (core dumped)\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Any solution to this issue, please?\r\n", "@Julius-ZCJ We see that you are using older version of tensorflow .Many bug have been fixed in latest version.  We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34257\">No</a>\n"]}, {"number": 34256, "title": "tensorflow java 1.15.0 failed to load native lib", "body": "**Describe the current behavior**\r\n```\r\norg.tensorflow.NativeLibrary: tryLoadLibraryFailed: no tensorflow_jni in java.library.path\r\norg.tensorflow.NativeLibrary: jniResourceName: org/tensorflow/native/linux-x86_64/libtensorflow_jni.so\r\norg.tensorflow.NativeLibrary: frameworkResourceName: org/tensorflow/native/linux-x86_64/libtensorflow_framework.so\r\norg.tensorflow.NativeLibrary: org/tensorflow/native/linux-x86_64/libtensorflow_framework.so not found. This is fine assuming org/tensorflow/native/linux-x86_64/libtensorflow_jni.so is not built to depend on it.\r\norg.tensorflow.NativeLibrary: extracting native library to: /xxx/tmp/tensorflow_native_libraries-1573652659702-0/libtensorflow_jni.so\r\norg.tensorflow.NativeLibrary: copied 154073736 bytes to /xxx/tmp/tensorflow_native_libraries-1573652659702-0/libtensorflow_jni.so\r\njava.lang.UnsatisfiedLinkError:/xxx/tmp/tensorflow_native_libraries-1573652659702-0/libtensorflow_jni.so: libtensorflow_framework.so.1: \u65e0\u6cd5\u6253\u5f00\u5171\u4eab\u5bf9\u8c61\u6587\u4ef6: \u6ca1\u6709\u90a3\u4e2a\u6587\u4ef6\u6216\u76ee\u5f55\r\n\tat java.lang.ClassLoader$NativeLibrary.load(Native Method)\r\n\tat java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\r\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)\r\n\tat java.lang.Runtime.load0(Runtime.java:809)\r\n\tat java.lang.System.load(System.java:1086)\r\n\tat org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)\r\n\tat org.tensorflow.TensorFlow.init(TensorFlow.java:67)\r\n\tat org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:82)\r\n\tat org.tensorflow.SavedModelBundle.<clinit>(SavedModelBundle.java:170)\r\n```\r\n\r\n**Describe the expected behavior**\r\nno exceptions\r\n", "comments": ["I found the tensorflow jar does not include `org/tensorflow/native/linux-x86_64/libtensorflow_framework.so`, but include `org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1`, so it failed to copy `libtensorflow_framework.so` to tmp directory when loading the native library.", "`NativeLibrary.getMajorVersionNumber` returns `null` when libtensorflow and libtensorflow_jni jar were shaded to my jar", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34256\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34256\">No</a>\n", "I had the same problem (using a shaded jar, `NativeLibrary.getMajorVersionNumber` returned null). But since your fix wasn't yet available in the 1.15 release, I worked around it by adding an `Implementation-Version` that starts with `1.` to the shaded jar's manifest using Maven's `ManifestResourceTransformer`.", "@mttsndrs Could you share what you did with the manifest resource transformer? I'm having a similar problem.", "@geometrikal I added this to the pom.xml:\r\n```\r\n    <properties>\r\n        <tensorflow.version>1.15.0</tensorflow.version>\r\n    </properties>\r\n\r\n    <build>\r\n        <plugins>\r\n            <plugin>\r\n                <groupId>org.apache.maven.plugins</groupId>\r\n                <artifactId>maven-shade-plugin</artifactId>\r\n                <configuration>\r\n                    <transformers combine.self=\"override\">\r\n                        <transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\r\n                            <manifestEntries>\r\n                                <!--\r\n                                Note: When loaded from a jar, tensorflow will use the\r\n                                \"Implementation-Version\" tag in the jar's manifest to identify the\r\n                                tensorflow_framework lib to extract. When packaged in a shaded jar,\r\n                                this can cause problems locating the correct version of the\r\n                                tensorflow library.\r\n                                By overriding the \"Implementation-Version\" tag here, we ensure that\r\n                                the correct tensorflow_framework lib gets extracted at runtime.\r\n                                -->\r\n                                <Implementation-Version>${tensorflow.version}</Implementation-Version>\r\n                            </manifestEntries>\r\n                        </transformer>\r\n                    </transformers>\r\n                </configuration>\r\n            </plugin>\r\n        </plugins>\r\n    </build>\r\n```"]}, {"number": 34255, "title": "Soft Placement Fails with Estimator Multi-worker CollectiveAllReduceStrategy ", "body": "The cpu device appears to be ignored when `SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert/data_0` is found to be incompatible with the GPU \r\n\r\n- Container: tensorflow/tensorflow:2.0.0-gpu-py3\r\n- Cuda: 10\r\n- Env: Kubeflow (tfjob)\r\n\r\n```python\r\n    session_config = tf.compat.v1.ConfigProto(\r\n        allow_soft_placement=True,\r\n    )\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    run_config = tf.estimator.RunConfig(\r\n        model_dir=flags_obj.model_dir,\r\n        save_checkpoints_steps=flags_obj.save_checkpoint_steps,\r\n        log_step_count_steps=flags_obj.log_step_count_steps,\r\n        save_summary_steps=flags_obj.save_summary_steps,\r\n        train_distribute=strategy,\r\n        eval_distribute=strategy,\r\n        session_config=session_config\r\n    )\r\n```\r\n\r\nLogs:\r\n```I1114 00:37:58.694102 140070930646848 main.py:253] {'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': 0, 'verbosity': 0, 'stderrthreshold': 'fatal', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'op_conversion_fallback_to_while_loop': False, 'test_random_seed': 301, 'test_srcdir': '', 'test_tmpdir': '/tmp/absl_testing', 'test_randomize_ordering_seed': None, 'xml_output_file': '', 'model_dir': 's3://**censor**/**censor**', 'train_batch_size': 3, 'eval_batch_size': 4, 'log_step_count_steps': 64, 'save_summary_steps': 100, 'save_checkpoint_steps': 1000, 'train_steps': 1000000, 'eval_steps': 10, 'learning_rate': 1e-05, 'train_dir': '**censor**train/', 'eval_dir': '**censor**eval/', 'epochs': 5000, 'clean': False, 'ds': 'default', 'distribution_strategy': 'default', 'ara': None, 'all_reduce_alg': None, 'num_packs': 1, 'crop_dim': [512, 512], 'learning_momentum': 0.9, 'gradient_clipping_norm': 5.0, 'gradient_summaries': False, 'weight_decay': 0.0, 'rpn_class_loss_weight': 10.0, 'rpn_bbox_loss_weight': 1.0, 'detection_class_loss_weight': 1.0, 'detection_bbox_loss_weight': 1.0, 'detection_mask_loss_weight': 1.0, 'num_classes': 7, 'backbone': 'efficientnet-b3', 'mask_shape': [28, 28], 'roi_proposals': 200, 'rpn_train_anchors_per_image': 256, 'roi_positive_ratio': 0.33, 'mini_mask_shape': [56, 56], 'backbone_strides': [4, 8, 16, 32, 64], 'fpn_classifier_fc_layer_size': 1024, 'fpn_size': 256, 'classify_pool_size': 7, 'mask_pool_size': 14, 'max_instances': 50, 'min_confidence': 0.7, 'detection_nms_threshold': 0.3, 'post_nms_rois': 2000, 'pre_nms_limit': 6000, 'rpn_nms_threshold': 0.7, 'bbox_stdev': [0.1, 0.1, 0.2, 0.2], 'anchor_scales': ['18', '36', '72', '180', '460'], 'anchor_ratios': [0.5, 1, 2], 'anchor_stride': 1, '?': False, 'help': False, 'helpshort': False, 'helpfull': False, 'helpxml': False}\r\nI1114 00:37:58.694322 140070930646848 main.py:254] {\"cluster\":{\"chief\":[\"**censor**-chief-0.kubeflow-jobs.svc:2222\"],\"worker\":[\"**censor**-worker-0.kubeflow-jobs.svc:2222\",\"**censor**-worker-1.kubeflow-jobs.svc:2222\"]},\"task\":{\"type\":\"chief\",\"index\":0},\"environment\":\"cloud\"}\r\nINFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:XLA_CPU:0', '/job:chief/replica:0/task:0/device:XLA_GPU:0', '/job:chief/replica:0/task:0/device:GPU:0']\r\nI1114 00:37:58.906222 140070930646848 collective_all_reduce_strategy.py:269] Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:XLA_CPU:0', '/job:chief/replica:0/task:0/device:XLA_GPU:0', '/job:chief/replica:0/task:0/device:GPU:0']\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nI1114 00:37:58.908060 140070930646848 collective_all_reduce_strategy.py:310] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nINFO:tensorflow:TF_CONFIG environment variable: {'cluster': {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, 'task': {'type': 'chief', 'index': 0}, 'environment': 'cloud'}\r\nI1114 00:37:58.909245 140070930646848 run_config.py:535] TF_CONFIG environment variable: {'cluster': {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, 'task': {'type': 'chief', 'index': 0}, 'environment': 'cloud'}\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nI1114 00:37:58.909479 140070930646848 run_config.py:566] Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with INDEPENDENT_WORKER mode\r\nI1114 00:37:58.909686 140070930646848 estimator_training.py:177] RunConfig initialized for Distribute Coordinator with INDEPENDENT_WORKER mode\r\nINFO:tensorflow:Using config: {'_model_dir': 's3://**censor**/**censor**', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 64, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f649946f390>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f649946f390>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f649946fe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': 'independent_worker'}\r\nI1114 00:37:58.909949 140070930646848 estimator.py:212] Using config: {'_model_dir': 's3://**censor**/**censor**', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 64, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f649946f390>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f649946f390>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f649946fe80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': 'independent_worker'}\r\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\r\nI1114 00:37:58.912131 140070930646848 training.py:462] Running `train_and_evaluate` with Distribute Coordinator.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, environment = 'cloud', rpc_layer = 'grpc'\r\nI1114 00:37:58.912826 140070930646848 distribute_coordinator.py:776] Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, environment = 'cloud', rpc_layer = 'grpc'\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nI1114 00:37:58.913650 140070930646848 collective_all_reduce_strategy.py:310] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nI1114 00:37:58.914542 140070930646848 collective_all_reduce_strategy.py:310] Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['**censor**-chief-0.kubeflow-jobs.svc:2222'], 'worker': ['**censor**-worker-0.kubeflow-jobs.svc:2222', '**censor**-worker-1.kubeflow-jobs.svc:2222']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO\r\nINFO:tensorflow:Updated config: {'_model_dir': 's3://**censor**/**censor**', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 64, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f6499492908>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f64994943c8>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f64994947f0>, '_task_type': 'chief', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://**censor**-chief-0.kubeflow-jobs.svc:2222', '_evaluation_master': 'grpc://**censor**-chief-0.kubeflow-jobs.svc:2222', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 3, '_distribute_coordinator_mode': 'independent_worker'}\r\nI1114 00:37:58.915921 140070930646848 estimator_training.py:228] Updated config: {'_model_dir': 's3://**censor**/**censor**', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 64, '_train_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f6499492908>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7f64994943c8>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f64994947f0>, '_task_type': 'chief', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://**censor**-chief-0.kubeflow-jobs.svc:2222', '_evaluation_master': 'grpc://**censor**-chief-0.kubeflow-jobs.svc:2222', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 3, '_distribute_coordinator_mode': 'independent_worker'}\r\nINFO:tensorflow:The `input_fn` accepts an `input_context` which will be given by DistributionStrategy\r\nI1114 00:37:59.079214 140070930646848 estimator.py:1111] The `input_fn` accepts an `input_context` which will be given by DistributionStrategy\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nW1114 00:38:02.574301 140070930646848 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nINFO:tensorflow:Calling model_fn.\r\nI1114 00:38:02.581103 140054211045120 estimator.py:1147] Calling model_fn.\r\nI1114 00:38:02.781386 140054211045120 model.py:348] tracing train forward: (3, 512, 512, 3) | (3, 65472, 4) | (3, None) | (3, None, 56, 56)\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nW1114 00:38:02.793168 140054211045120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\nINFO:tensorflow:Collective batch_all_reduce: 389 all-reduces, num_workers = 3\r\nI1114 00:38:21.225312 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 389 all-reduces, num_workers = 3\r\nINFO:tensorflow:Done calling model_fn.\r\nI1114 00:38:26.023765 140054211045120 estimator.py:1149] Done calling model_fn.\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:26.024793 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nI1114 00:38:27.469296 140070930646848 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\r\nINFO:tensorflow:all_hooks [<tensorflow_estimator.python.estimator.util.DistributedIteratorInitializerHook object at 0x7f6496df1358>, <tensorflow.python.training.basic_session_run_hooks.StopAtStepHook object at 0x7f6499494be0>, <tensorflow.python.training.basic_session_run_hooks.NanTensorHook object at 0x7f62f8076c88>, <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7f62d4072828>, <tensorflow.python.training.basic_session_run_hooks.StepCounterHook object at 0x7f62d424f0f0>, <tensorflow.python.training.basic_session_run_hooks.SummarySaverHook object at 0x7f62d424fcc0>, <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f62b46132b0>]\r\nI1114 00:38:27.469497 140070930646848 monitored_session.py:408] all_hooks [<tensorflow_estimator.python.estimator.util.DistributedIteratorInitializerHook object at 0x7f6496df1358>, <tensorflow.python.training.basic_session_run_hooks.StopAtStepHook object at 0x7f6499494be0>, <tensorflow.python.training.basic_session_run_hooks.NanTensorHook object at 0x7f62f8076c88>, <tensorflow.python.training.basic_session_run_hooks.LoggingTensorHook object at 0x7f62d4072828>, <tensorflow.python.training.basic_session_run_hooks.StepCounterHook object at 0x7f62d424f0f0>, <tensorflow.python.training.basic_session_run_hooks.SummarySaverHook object at 0x7f62d424fcc0>, <tensorflow.python.training.basic_session_run_hooks.CheckpointSaverHook object at 0x7f62b46132b0>]\r\nINFO:tensorflow:Creating chief session creator with config: device_filters: \"/job:chief/task:0\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:chief/replica:0/task:0\"\r\n}\r\n\r\nI1114 00:38:27.469661 140070930646848 distribute_coordinator.py:251] Creating chief session creator with config: device_filters: \"/job:chief/task:0\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:chief/replica:0/task:0\"\r\n}\r\n\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:33.972837 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:33.976987 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:33.988512 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:33.992657 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:34.086326 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:34.090441 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:34.102018 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:34.106123 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nI1114 00:38:34.117756 140070930646848 cross_device_ops.py:1107] Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\nINFO:tensorflow:Graph was finalized.\r\nI1114 00:38:40.463714 140070930646848 monitored_session.py:240] Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nI1114 00:38:55.357866 140070930646848 session_manager.py:500] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI1114 00:38:56.174259 140070930646848 session_manager.py:502] Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into s3://**censor**/**censor**/model.ckpt.\r\nI1114 00:39:17.262061 140070930646848 basic_session_run_hooks.py:606] Saving checkpoints for 0 into s3://**censor**/**censor**/model.ckpt.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:chief/replica:0/task:0:\r\nCannot assign a device for operation detection_class_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert/data_0: Could not satisfy explicit device specification '/job:chief/replica:0/task:0/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:chief/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]\r\nConst: CPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  detection_class_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert/data_0 (Const) /job:chief/replica:0/task:0/device:GPU:0\r\n\r\nOp: Const\r\nNode attrs: dtype=DT_STRING, value=Tensor<type: string shape: [] values: >\r\nRegistered kernels:\r\n  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='GPU'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n\r\n\t [[{{node detection_class_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert/data_0}}]]\r\n\t [[loss_ops/detection_loss/StatefulPartitionedCall]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/opt/model/main.py\", line 307, in <module>\r\n    absl_app.run(main)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/opt/model/main.py\", line 299, in main\r\n    run_train_and_eval(flags.FLAGS, flags.FLAGS.flag_values_dict())\r\n  File \"/opt/model/main.py\", line 287, in run_train_and_eval\r\n    tf.estimator.train_and_evaluate(est, train_spec, eval_spec)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 464, in train_and_evaluate\r\n    estimator, train_spec, eval_spec, _TrainingExecutor)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/estimator_training.py\", line 290, in train_and_evaluate\r\n    session_config=run_config.session_config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/estimator_training.py\", line 252, in _worker_fn\r\n    hooks=hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1221, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1332, in _actual_train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1410, in _train_with_estimator_spec\r\n    estimator_spec, worker_hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1356, in _train_with_estimator_spec_distributed\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1360, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/lib/python3/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1418, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1176, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:chief/replica:0/task:0:\r\nCannot assign a device for operation detection_class_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert/data_0: Could not satisfy explicit device specification '/job:chief/replica:0/task:0/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:chief/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='' supported_device_types_=[CPU] possible_devices_=[]\r\nConst: CPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  detection_class_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert/data_0 (Const) /job:chief/replica:0/task:0/device:GPU:0\r\n\r\nOp: Const\r\nNode attrs: dtype=DT_STRING, value=Tensor<type: string shape: [] values: >\r\nRegistered kernels:\r\n  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='GPU'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n\r\n\t [[{{node detection_class_loss/SparseSoftmaxCrossEntropyWithLogits/assert_equal/Assert/Assert/data_0}}]]\r\n\t [[loss_ops/detection_loss/StatefulPartitionedCall]]\r\n```\r\n", "comments": ["@gdj0nes \r\nWill it be possible to share related complete code snippet to reproduce the issue in our environment,it helps us localizing the issue faster. Thanks!", "@gdj0nes \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34254, "title": "Allow `pathlib.Path` paths for loading models via Keras API", "body": "This change allows the path for `tf.keras.models.load_model` to be specified as a `pathlib.Path` object. Previously, the provided path was explicitly checked to see if it was a string, triggering an `IOError` for `pathlib.Path` objects.", "comments": ["@mpetroff thank you , is it possible to add a test case around the new changes ?", "@rthadur I added simple test cases to make sure there are no exceptions raised when loading models specified by either string-based paths or `pathlib`-based paths.", "thank you", "Hi, thanks for this change! Is here a reason why the user would prefer passing a `pathlib.Path` object to `load_model`, rather than calling `load_model(str(obj))` themselves?", "With Python 3.6 / [PEP 519](https://www.python.org/dev/peps/pep-0519/) `pathlib.Path` objects work seamlessly in the standard library where paths are used and are supported by other common libraries such as [NumPy](https://github.com/numpy/numpy/pull/6660) (and at least partly by [multi-backend Keras](https://github.com/keras-team/keras/pull/11466)). The expectation is that `pathlib.Path` objects will just work without an explicit cast wherever paths are used. Other parts of TensorFlow should probably be checked for `pathlib` support as well; `tf.keras.models.load_model` is just where I noticed the issue.\r\n\r\nAs noted by PEP 519, a `str` cast should generally be avoided for paths; since many object types have `str` representations that are valid paths, adding the cast can cause undesired behavior instead of raising an exception if the wrong object is accidentally passed to the code. While this isn't the case here, the cast is still best avoided.", "Thanks for the clear explanation, that makes sense.", "While it was working for me locally, the change failed CI, with an issue with the `Path` object deeper in TensorFlow. I've now changed it to cast the `Path` object to a string instead, which should be more robust. I made the same change for `save_model` as well, so it will also support `Path` objects.", "Changes have been merged internally , waiting for auto-merge to happen."]}, {"number": 34253, "title": "When the result of tf.saved_model.load goes out of scope, it invalidates models that are still in-scope", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, a trivial change to a stock example\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- TensorFlow installed from (source or binary): binary via pip\r\n- TensorFlow version (use command below): tensorflow-gpu 2.0.0\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 10.0 / 10.0-windows10-x64-v7.6.0.64\r\n- GPU model and memory: GeForce GTX 1050 Ti (laptop)\r\n\r\n**Describe the current behavior**\r\n\r\nLoading a TF2 model built with the Keras API, and then predicting with random data follows this minimal recipe:\r\n```\r\nimport tensorflow as tf\r\nloaded = tf.saved_model.load('some/model')\r\ninfer = loaded.signatures['serving_default']\r\ndata = tf.constant(np.asarray(np.random.randn(my_shape), dtype=np.float32))\r\npreds = infer(data)\r\n```\r\n\r\nHowever, if we don't keep a reference to `loaded` around or lose it some time later...\r\n```\r\nimport tensorflow as tf\r\ninfer = tf.saved_model.load('some/model').signatures['serving_default']\r\ndata = tf.constant(np.asarray(np.random.randn(my_shape), dtype=np.float32))\r\npreds = infer(data)\r\n```\r\n\r\n... it fails with this utterly confusing error about variables being potentially uninitialized (for which google searches lead down various unrelated tf 1.x related rabbit holes):\r\n```\r\n2019-11-13 15:37:09.423218: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Failed precondition: Error while reading resource variable batch_normalization_150/moving_mean_12513 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/batch_normalization_150/moving_mean_12513/class tensorflow::Var does not exist.\r\n\t [[{{node StatefulPartitionedCall/model_65/model_60/batch_normalization_150/FusedBatchNormV3/ReadVariableOp}}]]\r\nTraceback (most recent call last):\r\n  File \"D:/DEVEL/experiments/test_load.py\", line 17, in <module>\r\n    preds = infer(data)\r\n  File \"C:\\Users\\chris\\Miniconda3\\envs\\np2019-11-06e2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1081, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"C:\\Users\\chris\\Miniconda3\\envs\\np2019-11-06e2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1121, in _call_impl\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"C:\\Users\\chris\\Miniconda3\\envs\\np2019-11-06e2\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\load.py\", line 99, in _call_flat\r\n    cancellation_manager)\r\n  File \"C:\\Users\\chris\\Miniconda3\\envs\\np2019-11-06e2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"C:\\Users\\chris\\Miniconda3\\envs\\np2019-11-06e2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\chris\\Miniconda3\\envs\\np2019-11-06e2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable batch_normalization_150/moving_mean_12513 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/batch_normalization_150/moving_mean_12513/class tensorflow::Var does not exist.\r\n\t [[{{node StatefulPartitionedCall/model_65/model_60/batch_normalization_150/FusedBatchNormV3/ReadVariableOp}}]] [Op:__inference_signature_wrapper_5266]\r\n\r\nFunction call stack:\r\nsignature_wrapper\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `infer` object holds a reference to the data that it depends on so that it doesn't get garbage-collected.\r\n\r\n**Code to reproduce the issue**\r\nIf you run the TF2.0 Saved Model tutorial colab notebook (https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb) up to and including the cell\r\n\r\n```\r\n!saved_model_cli show --dir /tmp/mobilenet/1 --tag_set serve --signature_def serving_default\r\n```\r\n\r\nbut then you execute the following cell instead of the one that would create the variable `loaded`:\r\n\r\n```\r\ninfer = tf.saved_model.load(\"/tmp/mobilenet/1/\").signatures[\"serving_default\"]\r\nprint(infer.structured_outputs)\r\nlabeling = infer(tf.constant(x))[pretrained_model.output_names[0]]\r\ndecoded = imagenet_labels[np.argsort(labeling)[0,::-1][:5]+1]\r\nprint(\"Result after saving and loading:\\n\", decoded)\r\n```\r\n\r\n... then it fails with:\r\n```\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-8-f42985f0118d> in <module>()\r\n----> 1 labeling = infer(tf.constant(x))[pretrained_model.output_names[0]]\r\n      2 \r\n      3 decoded = imagenet_labels[np.argsort(labeling)[0,::-1][:5]+1]\r\n      4 \r\n      5 print(\"Result after saving and loading:\\n\", decoded)\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nFailedPreconditionError:  Error while reading resource variable conv_pw_7_bn/moving_variance_33153 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/conv_pw_7_bn/moving_variance_33153/N10tensorflow3VarE does not exist.\r\n\t [[{{node StatefulPartitionedCall/mobilenet_1.00_224/conv_pw_7_bn/FusedBatchNormV3/ReadVariableOp_1}}]] [Op:__inference_signature_wrapper_30328]\r\n\r\nFunction call stack:\r\nsignature_wrapper\r\n```\r\n", "comments": ["Was able to reproduce this issue. Please find the gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/c38e0f0526c3cae25572f9ddb158ce90/copy-of-copy-of-saved_model.ipynb). ", "I'm seeing a same issue when I load and use my custom saved_model.", "Does anyone have a work around for this?\r\n\r\nEdit: I refactored my code so that I am not overwriting models with load as I train. This worked for me.", "Functions only keep weak references to variables, so if the loaded object is garbage collected, the variables will be deleted as well. Make sure the loaded object remains in memory (using @jrbuhl93's approach as one such workaround)", "Can this behavior be documented? This is not obvious for many people.", "I made it a global variable and it worked. Something like this:\r\n\r\n```\r\nloaded_model = None\r\n\r\ndef load_model():\r\n    global loaded_model\r\n    # load_model\r\n```\r\n", "> I made it a global variable and it worked. Something like this:\r\n> \r\n> ```\r\n> loaded_model = None\r\n> \r\n> def load_model():\r\n>     global loaded_model\r\n>     # load_model\r\n> ```\r\nI'm also having a same issue.\r\nYou mean you assigned your loaded model(tf.saved_model.load('some/model')) to variable \"loaded_model\"?", "> > I made it a global variable and it worked. Something like this:\r\n> > ```\r\n> > loaded_model = None\r\n> > \r\n> > def load_model():\r\n> >     global loaded_model\r\n> >     # load_model\r\n> > ```\r\n> \r\n> I'm also having a same issue.\r\n> You mean you assigned your loaded model(tf.saved_model.load('some/model')) to variable \"loaded_model\"?\r\n\r\nJust write it as Class and make it an attribute. It works for me."]}, {"number": 34252, "title": "Cherrypicks r63 jv", "body": "TF2.1-rc0 cherry-pick request: Cherrypick for release branch to fix libtensorflowlite.so symbols", "comments": []}, {"number": 34251, "title": "Update collective op to enable polymorphic output shape", "body": "To enable the polymorphic output shape support for collective ops, this change disables caching `output_shape` for collective ops. Every time an `allGather` collective op is called, a new `output_shape` will be inferred and assigned to the latest instance.shape. \r\n\r\nA new unit test case is added correspondingly.\r\n\r\nFix #34250. For more information, please refer to the issue description.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34251) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34251) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34251) for more info**.\n\n<!-- ok -->", "@pw2393 \r\nPlease submit a PR against the master and if it is approved and critical for the release, it can then be cherrypicked.", "@goldiegadde cool. created here: #34295 "]}, {"number": 34250, "title": "Collective AllGather Fails on Polymorphic Shapes", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary whl\r\n- TensorFlow version (use command below): `tensorflow-gpu==2.0.0`\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.4\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n\r\n**Describe the current behavior**\r\n```python\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.ops import collective_ops\r\n\r\nt0 = [0, 1, 2, 3, 4, 5, 6, 7]\r\nt1 = [10, 11, 12, 13, 14, 15, 16, 17]\r\nexpected = [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17]\r\n\r\ngroup_size = 2\r\ngroup_key = 1\r\ninstance_key = 123\r\n\r\nwith tf.compat.v1.Session(\r\n        config=config_pb2.ConfigProto(device_count={'CPU': group_size})) as sess:\r\n\r\n    with tf.device('/CPU:0'):\r\n        in0 = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None])\r\n        c0 = collective_ops.all_gather(in0, group_size=group_size, group_key=group_key, instance_key=instance_key)\r\n    with tf.device('/CPU:1'):\r\n        in1 = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None])\r\n        c1 = collective_ops.all_gather(in1, group_size=group_size, group_key=group_key, instance_key=instance_key)\r\n\r\n    # SUCCESS:\r\n    results = sess.run([c0, c1], feed_dict={in0: t0, in1: t1})\r\n    assert np.allclose(results[0], expected)\r\n    assert np.allclose(results[1], expected)\r\n\r\n    # FAIL:\r\n    results_ = sess.run([c0, c1], feed_dict={in0: t0[1:], in1: t1[1:]})\r\n    # > 2019-11-13 17:45:50.521948: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: Inconsistent output shapes, got [14], but expected is [16].\r\n```\r\n\r\nIn one session, if one runs the above graph second time with the feed in a size different from the first time, it will raise an error: `Inconsistent output shapes, got [14], but expected is [16].`\r\n\r\n**Describe the expected behavior**\r\n\r\n* The graph construction above sets the expected shapes of the placeholders as polymorphic `[None]`. However, after the first `session.run`, the collective op caches its output shape (which is `16` in our case) in \"tensorflow/tensorflow/core/kernels/collective_ops.cc\". But should it be expected that the collective op keeps its graph-defined polymorphic behavior? Specifically, in our case, should it allow a user to all gather two size `7` tensors into a size `14`? \r\n\r\n**Code to reproduce the issue**\r\nSee above\r\n", "comments": ["Hi @tensorflower, I opened a pull request there #34295  for this issue. If someone would like to review it, I'd very appreciate it  \ud83d\ude04 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34250\">No</a>\n"]}, {"number": 34249, "title": "Update version numbers for TensorFlow 2.1.0-rc0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 0 -> 1\nPatch: 0 -> 0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:34:2.0\n.0\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:39:2.0\n.0\ntensorflow/tools/pip_package/setup.py:64:2.0.0\ntensorflow/tools/pip_package/setup.py:65:2.0.0\ntensorflow/tools/pip_package/setup.py:72:2.0.0\ntensorflow/tools/ci_build/windows/integration/gpu_pip_on_cpu/build_tf_windows.sh\n:114:2.0.0\ntensorflow/tools/ci_build/windows/gpu/pip/build_tf_windows.sh:114:2.0.0\ntensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh:114:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:670:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:670:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:671:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:672:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:46:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:47:2.0.0\ntensorflow/tools/ci_build/install/install_auditwheel.sh:19:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:54:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:55:2.0.0\ntensorflow/lite/toco/tflite/op_version.cc:84:2.0.0\ntensorflow/lite/tools/pip_package/debian/changelog:1:2.0.0\ntensorflow/lite/tools/pip_package/debian/changelog:3:2.0.0\ntensorflow/lite/g3doc/guide/ios.md:61:2.0.0\ntensorflow/lite/g3doc/guide/ios.md:64:2.0.0\ntensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec:3:2.0.0\ntensorflow/lite/experimental/swift/TensorFlowLiteSwift.podspec:3:2.0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.mbed.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.make.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/third_party_downloads.inc:26:2.0.0\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh:52:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:8:\n2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:15\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:20\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:208:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:209:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:215:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:228:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:264:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:166:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:167:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:173:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:186:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:222:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/create_sine_model.ipynb:\n96:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:124:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:125:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:131:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:144:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:180:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:176:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:177:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:183:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:196:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:234:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:18:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:19:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:20:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:21:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:22:2.0.0\ntensorflow/lite/experimental/ios/TensorFlowLiteC.podspec:3:2.0.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:34:2.0\n.0\ntensorflow/tools/tensorflow_builder/compat_checker/compat_checker_test.py:39:2.0\n.0\ntensorflow/tools/pip_package/setup.py:64:2.0.0\ntensorflow/tools/pip_package/setup.py:65:2.0.0\ntensorflow/tools/pip_package/setup.py:72:2.0.0\ntensorflow/tools/ci_build/windows/integration/gpu_pip_on_cpu/build_tf_windows.sh\n:114:2.0.0\ntensorflow/tools/ci_build/windows/gpu/pip/build_tf_windows.sh:114:2.0.0\ntensorflow/tools/ci_build/windows/cpu/pip/build_tf_windows.sh:114:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:670:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:670:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:671:2.0.0\ntensorflow/tools/ci_build/builds/pip_new.sh:672:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:46:2.0.0\ntensorflow/tools/ci_build/install/install_centos_pip_packages.sh:47:2.0.0\ntensorflow/tools/ci_build/install/install_auditwheel.sh:19:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:54:2.0.0\ntensorflow/tools/ci_build/install/install_pip_packages.sh:55:2.0.0\ntensorflow/lite/toco/tflite/op_version.cc:84:2.0.0\ntensorflow/lite/tools/pip_package/debian/changelog:1:2.0.0\ntensorflow/lite/tools/pip_package/debian/changelog:3:2.0.0\ntensorflow/lite/g3doc/guide/ios.md:61:2.0.0\ntensorflow/lite/g3doc/guide/ios.md:64:2.0.0\ntensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec:3:2.0.0\ntensorflow/lite/experimental/swift/TensorFlowLiteSwift.podspec:3:2.0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.mbed.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/templates/tasks.json.make.tpl:4:2.\n0.0\ntensorflow/lite/experimental/micro/tools/make/third_party_downloads.inc:26:2.0.0\ntensorflow/lite/experimental/micro/tools/make/download_and_extract.sh:52:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:8:\n2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:15\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:20\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/tools/make/targets/apollo3evb_makefile.inc:22\n:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:208:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:209:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:215:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:228:2.0.0\ntensorflow/lite/experimental/micro/examples/person_detection/README.md:264:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:166:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:167:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:173:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:186:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/README.md:222:2.0.0\ntensorflow/lite/experimental/micro/examples/hello_world/create_sine_model.ipynb:\n96:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:124:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:125:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:131:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:144:2.0.0\ntensorflow/lite/experimental/micro/examples/micro_speech/README.md:180:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:176:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:177:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:183:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:196:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/README.md:234:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:18:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:19:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:20:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:21:2.0.0\ntensorflow/lite/experimental/micro/examples/magic_wand/sparkfun_edge/output_hand\nler.cc:22:2.0.0\ntensorflow/lite/experimental/ios/TensorFlowLiteC.podspec:3:2.0.0\n\nWARNING: Below are potentially instances of lingering old version string \"r2.0\" \nin source directory \"tensorflow/\" that are not updated by this script. Please \ncheck them manually!\ntensorflow/python/keras/activations.py:111:r2.0\ntensorflow/python/ops/math_ops.py:3556:r2.0\ntensorflow/python/ops/math_ops.py:3602:r2.0\ntensorflow/python/ops/math_ops.py:3652:r2.0\ntensorflow/python/ops/math_ops.py:3728:r2.0\ntensorflow/python/ops/math_ops.py:3797:r2.0\ntensorflow/python/ops/math_ops.py:3843:r2.0\ntensorflow/python/ops/math_ops.py:3919:r2.0\n```", "comments": ["Verified the warnings about the lingering old version string \"r2.0\". Those are docstrings and can be ignored"]}, {"number": 34248, "title": "Fix to the build override input-shape issue", "body": "https://github.com/tensorflow/tensorflow/issues/34141\r\n\r\nWas passing input_shape (which was null). Please review and comment.", "comments": ["@rthadur I have made a minor change. The test cases are already present.", "Please give a better title and description. Please add tests.", "@mihaimaruseac Please review and comment", "@mihaimaruseac Is there anything that is blocking the merge? \r\nI am new to the community hence I don't know much.", "You have the check statuses below the PR. For example, Ubuntu Sanity failed (ignore the Android one)\r\n\r\n```\r\n5. do_buildifier: buildifier check\r\n  FAIL\r\n```\r\n\r\n```\r\n=== Sanity check step 5 of 15: do_buildifier (buildifier check) ===\r\n\r\nRunning do_buildifier on 373 files\r\n\r\ntensorflow/compiler/jit/BUILD # reformat\r\ntensorflow/compiler/xla/BUILD # reformat\r\n\r\nbuildifier took 1 s\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/compiler/jit/BUILD:\r\n1c1\r\n< load(\"//tensorflow:tensorflow.bzl\", \"if_mlir\", \"tf_cc_test\", \"cc_header_only_library\")\r\n---\r\n> load(\"//tensorflow:tensorflow.bzl\", \"cc_header_only_library\", \"if_mlir\", \"tf_cc_test\")\r\ntensorflow/compiler/xla/BUILD:\r\n1c1\r\n< load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_test\", \"cc_header_only_library\")\r\n---\r\n> load(\"//tensorflow:tensorflow.bzl\", \"cc_header_only_library\", \"tf_cc_test\")\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```\r\n\r\nAs you didn't touch C++/build rules, you can ignore that.\r\n\r\nHowever, Ubuntu CPU/Linux GPU fail with Python errors on the files you touched\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1245, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/tpu/feature_column_test.py\", line 156, in test_get_dense_tensor\r\n    'aaa': sparse_input\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/tpu/feature_column.py\", line 428, in _get_dense_tensor\r\n    self, inputs, weight_collections, trainable)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py\", line 2516, in _get_dense_tensor\r\n    trainable=trainable)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/feature_column/feature_column.py\", line 2482, in _get_dense_tensor_internal\r\n    scope=variable_scope.get_variable_scope())\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/tpu/feature_column.py\", line 128, in _creator\r\n    return embedding_column_layer(None, scope=scope)  # pylint: disable=not-callable\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/layers/base.py\", line 547, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/keras/engine/base_layer.py\", line 818, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/tpu/feature_column_test.runfiles/org_tensorflow/tensorflow/python/keras/engine/base_layer.py\", line 2119, in _maybe_build\r\n    self.build(tuple(input_shapes))\r\nTypeError: 'NoneType' object is not iterable\r\n```", "@mihaimaruseac \r\nIs Docker the only way of conducting the sanity checks?\r\n", "@rthadur Do I need to create a new PR or will it take time for the reviewers to review the change?", "Sorry about this. The PR slipped through the cracks.\r\n\r\nUsually we try to respond to the PRs fast. If there's no review after fixing the previous review comments, feel free to ping reviewers.", "@mihaimaruseac Hi, there were a few build failures in the previous PR. I have made the changes again, please review and approve. And how can I conduct the tests (Dense Test to be precise) which are failing during the build, locally on my PC?\r\nThank you", "bot:mlx:test", "<details><summary>Mellanox CI status: <b>UNKNOWN</b> (click for details)</summary>\n\n**Note:** the artefacts will be deleted after 28-Nov-2019\n\nArtefact file | Status\n-- | --\n[build_tf.log](https://gist.githubusercontent.com/mellanox-github/844a65c4b7e3512eb43e92f39b13cd20/raw/3d9ee8ab00ca7bf985aeefebdb2117e6da189bc4/build_tf.log) | :heavy_check_mark: PASS\n[configure_sharp_startup.log](https://gist.githubusercontent.com/mellanox-github/844a65c4b7e3512eb43e92f39b13cd20/raw/139685e783e1a92183d626417e1fbd49b96afeb7/configure_sharp_startup.log) | :heavy_check_mark: PASS\n[run_tf_cnn_benchmarks.log](https://gist.githubusercontent.com/mellanox-github/844a65c4b7e3512eb43e92f39b13cd20/raw/e5b8e501f31726509861e3e98e4f48f4ad38074e/run_tf_cnn_benchmarks.log) | :heavy_check_mark: PASS\n[run_tf_models_imagenet_host1.log](https://gist.githubusercontent.com/mellanox-github/844a65c4b7e3512eb43e92f39b13cd20/raw/c57e8c7a8067d50149946d407a2338e5a81425ae/run_tf_models_imagenet_host1.log) | :question: UNKNOWN\n[run_tf_models_imagenet_host2.log](https://gist.githubusercontent.com/mellanox-github/844a65c4b7e3512eb43e92f39b13cd20/raw/2eff4ac8045d09a2523a637dc4a37be5fdbb733a/run_tf_models_imagenet_host2.log) | :question: UNKNOWN\n[run_tf_models_imagenet_launcher.log](https://gist.githubusercontent.com/mellanox-github/844a65c4b7e3512eb43e92f39b13cd20/raw/9af7fa08d92f402502dc0d0cd29c8b571c059069/run_tf_models_imagenet_launcher.log) | :question: UNKNOWN\n[run_tf_tests.log](https://gist.githubusercontent.com/mellanox-github/844a65c4b7e3512eb43e92f39b13cd20/raw/7bd981773f1bcc0cc046b96aa3514dcf31a9df87/run_tf_tests.log) | :question: UNKNOWN\n", "Mellanox CI FAILed.\n", "bot:mlx:test", "<details><summary>Mellanox CI status: <b>UNKNOWN</b> (click for details)</summary>\n\n**Note:** the artefacts will be deleted after 28-Nov-2019\n\nArtefact file | Status\n-- | --\n[build_tf.log](https://gist.githubusercontent.com/mellanox-github/33be6b2a6d6ff16e5d87f2090f1fc293/raw/83ade73bde98320014c1b9665ce95ab025b38c0e/build_tf.log) | :question: UNKNOWN\n", "Mellanox CI FAILed.\n", "@bono1567 can you please check the build failures ?", "> @bono1567 can you please check the build failures ?\r\n\r\n@rthadur I wanted to know how can I run these build tests prior to pushing the next change. Can I do it locally?", "@bono1567 please follow the steps here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/README.md", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I am encountering build issues. \r\nI am not working on it anymore"]}, {"number": 34247, "title": "Feature/issue fix", "body": "https://github.com/tensorflow/tensorflow/issues/34141 \r\n\r\nPlease review the solution and comment.\r\nThe commit on April 9th needs to be rejected\r\n", "comments": ["@bono1567 can you please resolve conflicts and add some tests cases if possible ?"]}, {"number": 34246, "title": "Update release notes for TensorFlow 2.1.0", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.1.0\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 34245, "title": "2.1-rc0 cherry-pick request: \"Rollback ruy avx2/avx512 changes to fix accuracy regression.\"", "body": "This resolves a potentially severe accuracy regression on x86 platforms when using post-training quantization. ", "comments": ["The Android Demo App build failure looks unrelated, and is tracked in b/144367297."]}, {"number": 34244, "title": "Update release notes for TensorFlow 2.1.0", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.1.0\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 34243, "title": "[ROCm] Fix for the broken ROCm CSB.", "body": "The following commit breaks the --config=rocm build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/921003e1c4ff0421b34a3db7ddd338eb376d213f\r\n\r\nThe above commit adds `//tensorflow/core/profiler/lib:traceme` as a build dependency on the CUDA side but not on the ROCm side, leading to bazel errors during the ROCm build.\r\nThe \"fix\" is the to add the dependency on the ROCm side as well.\r\n\r\n**update:**\r\n\r\nAlso added a commit to revert the following commit https://github.com/tensorflow/tensorflow/commit/22a97f1ef5a8b10c72ba1ec8b463f063c6e5f22d#diff-2455058fada3b63ffc514e7038321516\r\n\r\nThe reverted commit also attempts to fix the same error (being fixed by this PR), but fails to do so correctly. The reverted commit introduces compile errors of the form\r\n\r\n```\r\n    tensorflow/core/nccl/nccl_manager.cc: In member function 'void tensorflow::NcclManager::LoopKernelLaunches(tensorflow::NcclManager::NcclStream*)':\r\n    tensorflow/core/nccl/nccl_manager.cc:689:9: error: 'profiler' has not been declared\r\n             profiler::TraceMe trace_me(\"ncclAllReduce\");\r\n             ^\r\n    tensorflow/core/nccl/nccl_manager.cc:718:9: error: 'profiler' has not been declared\r\n             profiler::TraceMe trace_me(\"ncclBroadcast\");\r\n             ^\r\n    tensorflow/core/nccl/nccl_manager.cc:729:9: error: 'profiler' has not been declared\r\n             profiler::TraceMe trace_me(\"ncclReduce\");\r\n             ^\r\n    ...\r\n```\r\n\r\n\r\n---------------------------------\r\n\r\n/cc @chsigg @whchung ", "comments": ["@chsigg  gentle ping", "@rthadur , anything we can do to help get this PR merged?", "@deven-amd let me try to pull this again "]}, {"number": 34242, "title": "Eager context seems contradicting the distributed TensorFlow concept", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary whl\r\n- TensorFlow version (use command below): `tensorflow-gpu==2.0.0`\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.4\r\n- GPU model and memory: GeForce GTX 1080 Ti\r\n\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport argparse\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf.tensorflow_server_pb2 import ServerDef\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.python.training.server_lib import ClusterSpec\r\n\r\n\r\nCLUSTER_SPEC = {\r\n    \"worker\": [\r\n        \"localhost:14286\",\r\n        \"localhost:14287\"\r\n    ]\r\n}\r\n\r\n\r\ndef test():\r\n    c = context.context()\r\n    c.set_server_def(\r\n        ServerDef(\r\n            cluster=ClusterSpec(CLUSTER_SPEC).as_cluster_def(),\r\n            job_name=FLAGS.job_name,\r\n            task_index=FLAGS.task_index,\r\n            protocol='grpc'\r\n        )\r\n    )\r\n    print(tf.constant(3))\r\n    print('lol')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        \"--job_name\",\r\n        type=str,\r\n    )\r\n    parser.add_argument(\r\n        \"--task_index\",\r\n        type=int,\r\n    )\r\n    FLAGS, _ = parser.parse_known_args()\r\n    test()\r\n```\r\n\r\nWe refer the above code as `a.py`.\r\nSTEP 1: we launch it by\r\n```bash\r\npython a.py --job_name=worker --task_index=0\r\n```\r\nwith the log\r\n```\r\n...\r\n2019-11-13 13:53:38.096726: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:14286, 1 -> localhost:14287}\r\n2019-11-13 13:53:38.099209: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:14286\r\n```\r\nand it is waiting for another server as described in the `CLUSTER_SPEC`. Note that no `print` above has been executed yet.\r\n\r\nSTEP 2: we launch it as another process by\r\n```bash\r\npython a.py --job_name=worker --task_index=1\r\n```\r\nwith the log\r\n```\r\n...\r\n2019-11-13 14:00:01.431825: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:14286, 1 -> localhost:14287}\r\n2019-11-13 14:00:01.433641: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:14287\r\n2019-11-13 14:00:01.471394: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> localhost:14286, 1 -> localhost:14287}\r\ntf.Tensor(3, shape=(), dtype=int32)\r\nlol\r\n2019-11-13 14:00:01.585211: E tensorflow/core/distributed_runtime/rpc/eager/grpc_eager_client.cc:72] Remote EagerContext with id 2379290034739913425 does not seem to exist.\r\n```\r\nand the first processor with the `task_index=0` hangs forever.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nIn TensorFlow graph mode, we know when multiple sessions talk to the same cluster (which is defined by `ClusterSpec`), everything works pretty smooth and natural. \r\n\r\nShould eager context behave similarly?  That is, both the above processes are expected to run naturally: print the contant and string and exited.\r\n\r\nIn other words, when multiple contexts talking to the same cluster (which is defined by `ClusterSpec`), should they know how to coordinate with each other, just as the TensorFlow core is architectured and designed before?\r\n\r\n(Or is there any other API that could help resolve this issue?)\r\n\r\n\r\n**Code to reproduce the issue**\r\nSee above\r\n", "comments": ["Was able to replicate the issue with TF v2.5 ,after removing \"`print(tf.constant(3)`)\" for sake of simplicity, was able to run both session without hanging first session,please find the [gist ](https://colab.research.google.com/gist/mohantym/9c1a3d1f4f078bf0efbd4e6d8a8db27a/34242.ipynb#scrollTo=RVf58TTSs_UQ)here ..Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34242\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34242\">No</a>\n"]}, {"number": 34241, "title": "2.1-RC0 cherry-pick request: [tf.data] Improving detection of infinitely repeated datasets in the presence of errors.", "body": "PiperOrigin-RevId: 280200721\r\nChange-Id: Icfaffb567b970da140e9b0d3a6c2093452893f01", "comments": []}, {"number": 34240, "title": "Updated documentation in image_ops_impl.py ", "body": "Fixed grammatical errors in documentation  image_ops_impl.py", "comments": ["@srihari-humbarwadi can you please fix the sanity errors ?", "```\r\n5. do_buildifier: buildifier check\r\n  FAIL\r\n```\r\n@rthadur Looks like`buildifier` failed. But I did not change any build rules, don't think so the files i made changes in are failing the check, can you please confirm?", "np, let me try to pull this in.", "Thank You!", "it got merged internally , waiting for auto-merge to happen."]}, {"number": 34239, "title": "Duplicating tf.keras.layers.Layer inputs to prevent \"Graph disconnected\" errors", "body": "To prevent disconnected graph errors, which can occur if the list input to (concatenate) layers is modified after calling them, as it a common approach to build certain types of network graphs, e.g. DenseNets.\r\n\r\nFixes #30355 and #32023. Similar to https://github.com/keras-team/keras/pull/6035 of standalone Keras.", "comments": ["The issue seems to have been fixed by @omalleyt12 's commit (816ec796ea6a96940188356628566ed11a11c186).", "@csachs thanks for confirming , will be closing the PR."]}]