[{"number": 43500, "title": "Official Tensorflow build for s390x CPU", "body": "There is currently a Tensorflow on Linux s390x CPU Stable Release as well as a Tensorflow on Linux s390x Nightly build. Both under the category of community builds. \r\n\r\nUser story:\r\nAs a developer Tensorflow user, I want to be able to work with an official Tensorflow build for s390x so I can have certainty on the lifecycle and I can communicate to my end users (enterprises) that I work with an official Tensorflow version.\r\n\r\nThere is a growing demand for ML applications in the s390x arch platform, recent feedback from large enterprise customers like banks are working on ML models in the same platforms where their mission-critical apps are, s390x.\r\n\r\n", "comments": ["i don't understand the ask in the issue? What should we do?", "@mihaimaruseac thanks for getting back to me.   The ask is to have an official Tensorflow build for s390x architecture. Today is only a community build.\r\nIf there's a process to upgrade a build from \"community build\" to \"official build\" I'm requesting to go through that process.\r\n", "Unfortunately we don't have enough CI infrastructure and developers to maintain this build internally.", "@mihaimaruseac if you guys are open to get help, I can talk to my IBM colleagues. We can certainly help with infrastructure and we have engineers already contributing upstream.", "But then that's equivalent to the other community builds."]}, {"number": 43497, "title": "einsum for SparseTensors", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n**Provide the text output from tflite_convert**\r\nI do not find an 'optimized' way to compute a multiplication between an sparse and a dense tensor that are not matrices.\r\nFor example, I would like to perform the following multiplication between a 3D sparse tensor A and a 2D dense tensor B:\r\nC[i,j] = \\sum_k B[i,k] A[i,k,j]\r\n\r\nIMPORTANT! A needs to maintain its sparsity. Functions like tf.sparse.to_dense() must be avoided!\r\n```\r\n# Example\r\n# inputs\r\nA = tf.sparse.SparseTensor(values=tf.constant([1,1,1,1,2,1,1,1,1,1,-1], tf.float32),indices=[[0,0,1],[0,1,2],[0,2,0],[1,0,0],[1,1,1],[1,1,2],[1,2,2],[2,0,2],[2,1,1],[2,2,1],[2,2,2]], dense_shape=[3,3,3])\r\nB = tf.constant([[1, 2, 3],\r\n                 [4, 5, 6],\r\n                 [7, 8, 9]], tf.float32)\r\n\r\n# output\r\nC = tf.constant([[3, 1, 2],\r\n                 [4, 10, 11],\r\n                 [9, 8, -1]], tf.float32)\r\n```\r\nA 'tf.sparse.einsum(B,A, 'ik,ikj->ij')'-like function would be nice to reproduce the behaviour of einsum for dense tensors in sparse tensors.\r\n", "comments": ["Sorry for encountering this issue. I tried to help you as a TFLite team member. Are you willing to run this case in TF or TFLite?", "In TF. I prefer to avoid TFLite. Maybe I am in the wrong issue branch. Do you know where should I forward this issue?\n\nRegards,\n> Sorry for encountering this issue. I tried to help you as a TFLite team member. Are you willing to execute this case in TF or TFLite?\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@curiarteb This is the right place for the tensorflow issues like this one. The \"comp:lite\" tag took my attention, which is used for TFLite issues.", "Ok. Sorry for the tag. Any ideas on how can I employ einsum with sparse Tensors? Is it possible in the current TF version?\n> @curiarteb This is the right place for the tensorflow issue like this one. The \"comp:lite\" tag took my attention, which is used for TFLite issues.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "Any news?", "Have you tried with [tf.sparse.sparse_dense_matmul](https://www.tensorflow.org/api_docs/python/tf/sparse/sparse_dense_matmul) which will allow multiplication between sparse and dense matrix.", "Yes! Of course! The problem is that I need to compute a \"multiplication\" between a 3D sparse tensor (batch of sparse matrices) and a 2D dense tensor (batch of vectors). The tf.sparse.sparse_dense_matmul function only permits multiplications between 2D sparse/dense tensors. Currently, I reshape the 3D sparse tensor to a 2D huge non-overlapping block matrix, and the 2D tensor to a large 1D tensor (that thereafter I reshape it again to a 2D tensor). The problem on doing this is that I need to access the shape of the 3D sparse tensor. For the fitting function in Keras this is conflicting because a priori the first axis is assumed to be 'None'. This leads us to miss the Keras batch flow behaviour during the training of the network that deals with the aforementioned kind of tensors, forcing us to design a customized and significantly slower low-level training.\n\nI guess a kind of a tf.einsum function dedicated for the sparse library, which currently does not exist, would resolve this issue."]}, {"number": 43490, "title": "Feature Request: Support sparse to dense with max_length input", "body": "Using tensorflow 2.3, some times I found I need tfrecords with tf.io.VarlenFeature, but when using tpu or for some other reasons you might need to pad it to fixed max_length like 50, 200(fixed) not just using padded_batch or tf.sparse.to_dense to padd it to the max length in the batch(dynamic).\r\n\r\nIt is ok if you pre padding to max_length when generating tfrecords but that is wasting space and also reduce flexbilty.\r\n\r\nI'd like padded_batch(x, max_length), tf.sparse.to_dense(x, max_length) or just set tf.io.VarlenFeature(max_length).\r\n\r\nWill this feature be possible ? Thanks!\r\n\r\n", "comments": []}, {"number": 43487, "title": "Add support for dataset to pandas dataframe", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThere is a feature for numpy conversion, but none for a conversion to a pandas dataframe. \r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will add a function to do this, could be called as_dataframe()\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who uses pandas over numpy or prefers pandas dataframes over numpy arrays. \r\n\r\n**Any Other info.**\r\n\r\nInspired by the feature in the datasets package:\r\nhttps://www.tensorflow.org/datasets/api_docs/python/tfds/as_dataframe\r\n", "comments": ["Does this need to be in core tf.data? It seems simple enough to call `tfds.as_dataframe(dataset)` instead of `dataset.as_dataframe()`.", "@aaudiber Wouldn't it make it more accessible as the numpy function is if a user is unaware of the tfds package? ", "Fair enough, it will be much easier to find if it's in the Dataset API. I think we could add it, but we need to be very clear that it won't work for large datasets, and that it needs to wait to load the entire dataset, which could take some time. I see the tfds implementation relies on their `dataset_utils.as_numpy(tf.data.Datasaet)` implementation. We would probably want a similar `tf.data.Dataset.as_numpy()`. @jsimsa what are your thoughts?", "I would be open to adding support for `tf.data.experimental.as_numpy(dataset)` and `tf.data.experimental.as_dataframe(dataset)`.", "Can I help with coding this new feature? ", "Is this open to being coded still?", "Yes, contributions are still welcome here", "@nelsonlin2708968 I was hoping to work on this soon, if you want to tackle it together. \ud83d\udc4d ", "Can i join to working on this feature? I just starting to get my feet wet on tensorflow dev stuff.", "@habernal Sure! Just message me for details. \ud83d\udc4d ", "Is this issue still open? Would like to work on this one.", "@AdityaKane2001 Sure, you can join! Feel free to send me a message. ", "It seems like the PR already closed. Do you have any plan to reopen it? @rd16395p ", "@hfahrudin Yes I do, commented on the pr. "]}, {"number": 43483, "title": "Reusing Keras.Model errors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n**Describe the current behavior**\r\nThe code below throws an exception on training.\r\n\r\n**Describe the expected behavior**\r\nNo exception expected.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1KYTl8T_8_dZgqFcdSd6QIn9lEB9d7CPU?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nSee in the colab notebook. The exception thrown is:\r\nInvalidArgumentError: You must feed a value for placeholder tensor 'ftrs' with dtype float and shape [?,10]\r\n\t [[{{node ftrs}}]]\r\n\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200924`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5e084d1356f05a99e253a4016262261d/untitled393.ipynb).Thanks!", "Hi @tzachar, because you have provided custom code, can you give some more context/information on your use case? This helps expedite the debugging process. Additionally, is there a specific reason you are disabling eager execution?", "Hi @nikitamaia . \r\n\r\nI am disabling eager execution as it is causing errors in some of my code.\r\nWhich other context do you require?\r\n", "Disabling eager execution in TF2 is not recommended. It's best to try and fix the errors without resorting to`disable_eager_execution()`, as this will likely just cause more errors and problems in your program.", "I used to had various problems with Keras when eager execution was enabled.\r\nI'll file specific bugs if I can recreate them.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210527, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/96c271742f85e07aa33b511e00169b87/43483.ipynb). Thanks!"]}, {"number": 43476, "title": "Significant accuracy drop during conversion of DistillBert.", "body": "**System information**\r\n- OS: Ubuntu 18.04.5 LTS (x86_64)\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\r\n# For normal conversion:\r\n#converter.experimental_new_converter = True\r\n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6cc9b669e8>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c426cba58>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c4fd080>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c50d668>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c519c50>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dropout object at 0x7f6c3c4b5278>, because it is not built.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: /tmp/tmpsca4zbmo/assets\r\nINFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://huggingface.co/distilbert-base-multilingual-cased#.\r\n```\r\n\r\n**Failure details**\r\n- Producing wrong results and/or decrease in accuracy\r\nThe original model produces output:\r\n```\r\nTFBaseModelOutput([('last_hidden_state',\r\n                    <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\r\n                    array([[[ 0.07324436, -0.13601898,  0.24253392, ...,  0.2757071 ,\r\n                              0.05075948, -0.067513  ],\r\n                            [-0.00969718, -0.6601796 ,  0.07741702, ...,  0.20618156,\r\n                             -0.3217669 ,  0.25172737],\r\n                            [ 0.28169543, -0.36096776, -0.07832485, ...,  0.00399976,\r\n                              0.07738406, -0.06696145],\r\n                            ...,\r\n                            [ 0.04910686, -0.37189117,  0.33814144, ...,  0.03981559,\r\n                              0.14327443, -0.20109934],\r\n                            [ 0.07308616, -0.35554424,  0.33334315, ...,  0.08324002,\r\n                              0.1068868 , -0.22119546],\r\n                            [ 0.06794482, -0.37025544,  0.3678054 , ...,  0.0837122 ,\r\n                              0.11539332, -0.2085252 ]]], dtype=float32)>)])\r\n```\r\nThe converted model gives:\r\n```\r\narray([[[ 0.19460055, -0.07871183,  0.348869  , ...,  0.56901807,\r\n          0.40855035, -0.36336952],\r\n        [ 0.111944  , -0.10565656,  0.47841206, ...,  0.7020322 ,\r\n          0.1973463 , -0.56403255],\r\n        [ 0.2462692 , -0.04639575,  0.47352695, ...,  0.6418205 ,\r\n          0.26107264, -0.72952   ],\r\n        ...,\r\n        [ 0.2746247 , -0.06043699,  0.4579026 , ...,  0.7356582 ,\r\n          0.4970783 , -0.6069904 ],\r\n        [ 0.2640199 , -0.05569951,  0.4638841 , ...,  0.734777  ,\r\n          0.48725146, -0.60665435],\r\n        [ 0.25305653, -0.04610543,  0.44723004, ...,  0.7529757 ,\r\n          0.47617406, -0.6056732 ]]], dtype=float32)\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nSource code to reproduce:\r\n\r\n```\r\nfrom transformers import TFDistilBertModel, DistilBertTokenizer, DistilBertConfig\r\nimport tensorflow as tf\r\nfrom typing import Dict\r\nimport numpy as np\r\n\r\nclass myTFDistilBertModel(TFDistilBertModel):\r\n    # DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]\r\n    # tf.constant(np.zeros(shape=(1, 100), dtype=int).tolist())\r\n    # For details refer https://github.com/huggingface/transformers/blob/7cbf0f722d23440f3342aafc27697b50ead5996b/src/transformers/modeling_tf_utils.py#L218\r\n    def __init__(self, config, *inputs, **kwargs):\r\n        super().__init__(config, *inputs, **kwargs)\r\n    \r\n    @property\r\n    def dummy_inputs(self) -> Dict[str, tf.Tensor]:\r\n        \"\"\"\r\n        Dummy inputs to build the network.\r\n        Returns:\r\n            :obj:`Dict[str, tf.Tensor]`: The dummy inputs.\r\n        \"\"\"\r\n        return {\"input_ids\": tf.constant(np.zeros(shape=(1, 128), dtype=int).tolist())}\r\n\r\ntf_model = myTFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', return_dict=True)\r\ntf_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased', return_tensors='tf')\r\n\r\ninputs = tf_tokenizer(\"le droit d'acc\u00e8s\", padding='max_length', max_length=128, return_tensors='tf')\r\noutputs  = tf_model(inputs)\r\n\r\ninput_spec = tf.TensorSpec([1, 128], tf.int32)\r\ntf_model._set_inputs(input_spec, training=False)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\r\n# For normal conversion:\r\n#converter.experimental_new_converter = True\r\n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\n# Run the model with TensorFlow Lite\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n# Allocate tensors.\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput = tf_tokenizer(\"le droit d'acc\u00e8s\", padding='max_length', max_length=128, return_tensors='tf')\r\ninterpreter.set_tensor(0, input['input_ids'])\r\ninterpreter.invoke()\r\ninterpreter_output = interpreter.get_tensor(output_details[0][\"index\"])\r\n```\r\n", "comments": ["@ruichernob \r\nI ran the code shared and face a different error, please find [gist here](https://colab.research.google.com/gist/Saduf2019/1a333e92acaf40bc14fccf6c966750fb/untitled419.ipynb), could you provide us with a colab gist replicating the issue faced.", "@Saduf2019 As far as I understand you just need to '!pip install transformers'. Ther rest of the code works well.\r\nI've changed shared notebook a little bit in order to let you observe the reported [issue](https://colab.research.google.com/gist/Saduf2019/1a333e92acaf40bc14fccf6c966750fb/untitled419.ipynb#scrollTo=BlFb98ps1y_8)\r\n\r\nJust in case you're unable to fetch the changes:\r\n\r\nAdded transformers:\r\n```\r\n!pip install tf-nightly\r\n!pip install transformers\r\n```\r\n\r\nThe next cell didn't changed:\r\n```\r\nfrom transformers import TFDistilBertModel, DistilBertTokenizer, DistilBertConfig\r\nimport tensorflow as tf\r\nfrom typing import Dict\r\nimport numpy as np\r\n\r\nclass myTFDistilBertModel(TFDistilBertModel):\r\n    # DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]\r\n    # tf.constant(np.zeros(shape=(1, 100), dtype=int).tolist())\r\n    # For details refer https://github.com/huggingface/transformers/blob/7cbf0f722d23440f3342aafc27697b50ead5996b/src/transformers/modeling_tf_utils.py#L218\r\n    def __init__(self, config, *inputs, **kwargs):\r\n        super().__init__(config, *inputs, **kwargs)\r\n    \r\n    @property\r\n    def dummy_inputs(self) -> Dict[str, tf.Tensor]:\r\n        \"\"\"\r\n        Dummy inputs to build the network.\r\n        Returns:\r\n            :obj:`Dict[str, tf.Tensor]`: The dummy inputs.\r\n        \"\"\"\r\n        return {\"input_ids\": tf.constant(np.zeros(shape=(1, 128), dtype=int).tolist())}\r\n\r\ntf_model = myTFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased', return_dict=True)\r\ntf_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased', return_tensors='tf')\r\n\r\ninputs = tf_tokenizer(\"le droit d'acc\u00e8s\", padding='max_length', max_length=128, return_tensors='tf')\r\noutputs  = tf_model(inputs)\r\n\r\ninput_spec = tf.TensorSpec([1, 128], tf.int32)\r\ntf_model._set_inputs(input_spec, training=False)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\r\n# For normal conversion:\r\n#converter.experimental_new_converter = True\r\n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n\r\n# Run the model with TensorFlow Lite\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n# Allocate tensors.\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput = tf_tokenizer(\"le droit d'acc\u00e8s\", padding='max_length', max_length=128, return_tensors='tf')\r\ninterpreter.set_tensor(0, input['input_ids'])\r\ninterpreter.invoke()\r\ninterpreter_output = interpreter.get_tensor(output_details[0][\"index\"])\r\n```\r\n\r\nTo observe interpreter output:\r\n```\r\ninterpreter_output\r\n```\r\nThat gives:\r\n```\r\narray([[[ 0.1946005 , -0.07871191,  0.34886912, ...,  0.56901795,\r\n          0.4085501 , -0.36336952],\r\n        [ 0.11194398, -0.1056565 ,  0.47841233, ...,  0.70203197,\r\n          0.19734594, -0.56403244],\r\n        [ 0.24626903, -0.0463957 ,  0.4735275 , ...,  0.64182055,\r\n          0.2610727 , -0.72952026],\r\n        ...,\r\n        [ 0.2746247 , -0.06043694,  0.45790255, ...,  0.73565805,\r\n          0.4970783 , -0.6069902 ],\r\n        [ 0.26401982, -0.05569961,  0.4638846 , ...,  0.7347771 ,\r\n          0.48725143, -0.60665435],\r\n        [ 0.25305668, -0.0461056 ,  0.44722998, ...,  0.7529754 ,\r\n          0.47617412, -0.6056729 ]]], dtype=float32)\r\n```\r\nTo observe the model output:\r\n```\r\noutputs\r\n```\r\n\r\nThat gives:\r\n```\r\nTFBaseModelOutput([('last_hidden_state',\r\n                    <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\r\n                    array([[[ 0.07324453, -0.13601895,  0.24253356, ...,  0.27570707,\r\n                              0.05075945, -0.06751309],\r\n                            [-0.00969684, -0.6601795 ,  0.07741669, ...,  0.20618166,\r\n                             -0.32176685,  0.25172746],\r\n                            [ 0.28169546, -0.36096776, -0.07832497, ...,  0.00399961,\r\n                              0.07738377, -0.06696163],\r\n                            ...,\r\n                            [ 0.04910718, -0.37189072,  0.33814105, ...,  0.03981549,\r\n                              0.14327444, -0.20109943],\r\n                            [ 0.07308632, -0.35554397,  0.3333426 , ...,  0.08323996,\r\n                              0.10688686, -0.22119588],\r\n                            [ 0.06794469, -0.37025517,  0.36780512, ...,  0.08371204,\r\n                              0.11539327, -0.20852552]]], dtype=float32)>)])\r\n```\r\n\r\nAs we can observe, there is a significant mismatch.", "@ruichernob \r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d4b211faf3dc3e81c1caddba3952f413/untitled418.ipynb). Can you please share a colab gist with the error reported.", "@Saduf2019 \r\nI've been able to run your code without any errors. If you still faces the issue you can attempt comment\r\n```\r\n#input_spec = tf.TensorSpec([1, 128], tf.int32)\r\n#tf_model._set_inputs(input_spec, training=False)\r\n```\r\n\r\nAlso I've created the copy of your notebook. Please find it [here](https://colab.research.google.com/gist/ruichernob/db5857e36c952d13ee2bdf16318f20e1/untitled418.ipynb)", "Renjie, can you please have a look\r\n\r\nThanks"]}, {"number": 43473, "title": "TFLiteCCoreML Delegate Runtime Crash (Espresso::ANERuntimeEngine overflow error)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\niOS 13.5.1\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\niPhone 11 Max Pro\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\nBinary\r\n\r\n- TensorFlow version (use command below):\r\n\r\nTensorFlowLiteCCoreML v0.0.1 (via CocoaPods)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating a CoreML delegate using the TensorFlowLiteC API, creating an interpreter using the delegate, then invoking the interpreter, iOS crashes because of a memory error upon invocation of the interpreter.\r\n\r\n**Describe the expected behavior**\r\n\r\nFor the app not to crash when invoking the interpreter.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe code used to create the CoreML delegate is as follows (this all runs fine without error)\r\n\r\n```\r\nTfLiteCoreMlDelegateOptions coremldelegate_opts;                            \r\ncoremldelegate_opts.enabled_devices =  TfLiteCoreMlDelegateDevicesWithNeuralEngine;                            \r\ncoremldelegate_opts.coreml_version = 3; // have tried switching this to 2 but doesn't fix the crash                                 \r\ncoremldelegate_opts.max_delegated_partitions = 200;                           \r\n                                                                              \r\nTfLiteDelegate *delegate = TfLiteCoreMlDelegateCreate(&coremldelegate_opts);\r\n\r\nTfLiteInterpreterOptions *options = TfLiteInterpreterOptionsCreate();       \r\n                                                                             \r\nif (delegate)                                                               \r\n    TfLiteInterpreterOptionsAddDelegate(options, delegate);                 \r\n                                                                               \r\nTfLiteInterpreterOptionsSetNumThreads(options, 2);                          \r\ninterpreter = TfLiteInterpreterCreate(model, options); \r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\nWithout the CoreML delegate the app doesn't crash when invoking the interpreter. The app also doesn't crash when creating and using an XNNPackDelegate with the interpreter.\r\n\r\nThe model being used is a non-quantized yolov4 tflite model. The interpreter runs the model fine (although with large latency) without the CoreML delegate. Here is a link to the model: https://www.dropbox.com/s/zqq1mooq3icepvk/yolov4-coco.tflite?dl=0\r\n\r\nXCode console log upon crash is below\r\n\r\n2020-09-23 01:51:38.095470-0400 testapp[5212:1077669] [espresso] [Espresso::ANERuntimeEngine::__forward_segment 0] evaluate[RealTime]WithModel returned 0; code=5 err=Error Domain=com.apple.appleneuralengine Code=5 \"processRequest:qos:qIndex:error:: 0x1: Program Inference overflow\" UserInfo={NSLocalizedDescription=processRequest:qos:qIndex:error:: 0x1: Program Inference overflow}\r\n\r\n2020-09-23 01:51:38.095507-0400 testapp[5212:1077669] [espresso] [Espresso::overflow_error] /private/var/mobile/Containers/Data/Application/93F932CB-0799-4691-874F-B8DF78E67F5B/tmp/(A Document Being Saved By testapp 180)/A8A4CB7A-68E3-4352-B9C8-840119ACDA73-5212-0000030A34678314.mlmodelc/model.espresso.net:0\r\n\r\n2020-09-23 01:51:38.126818-0400 testapp[5212:1077669] [espresso] [Espresso::ANERuntimeEngine::__forward_segment 0] evaluate[RealTime]WithModel returned 0; code=5 err=Error Domain=com.apple.appleneuralengine Code=5 \"processRequest:qos:qIndex:error:: 0x1: Program Inference overflow\" UserInfo={NSLocalizedDescription=processRequest:qos:qIndex:error:: 0x1: Program Inference overflow}\r\n\r\n2020-09-23 01:51:38.126847-0400 testapp[5212:1077669] [espresso] [Espresso::overflow_error] /private/var/mobile/Containers/Data/Application/93F932CB-0799-4691-874F-B8DF78E67F5B/tmp/(A Document Being Saved By testapp 181)/2B38A561-4343-4868-8BF1-7AE106179D5D-5212-0000030A347917A7.mlmodelc/model.espresso.net:0\r\nMessage from debugger: Terminated due to memory issue\r\n\r\n\r\n", "comments": ["Hi, is there any update on this?\n\nThanks!", "Hi TF team, following up on the above. Thanks!", "Hi @raryanpur,\r\n\r\nSorry for late reply. I was on leave :( will take a look!\r\n\r\nFrom my experience, I saw program inference overflow message when the graph gets fragmented. If this is true, lowering down `max_delegate_partitions` might make the error go.\r\n\r\nWhen looking at the model, there are some not supported ops, such as `EXP`, `LOG`, and `LEAKY_RELU`. https://www.tensorflow.org/lite/performance/coreml_delegate#supported_ops\r\n\r\nSo even if the graph runs fine with the Core ML delegate, the performance might be much slower than expected. I'll add support for them in the foreseeable future.\r\n", "Thanks @teijeong!", "@raryanpur Could you please let us know if the issue still persists ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sushreebarsa I don't currently have the bandwidth to test this for you. If it's helpful for you and the team to test, please see my notes in the original ticket that include standalone code to reproduce the issue and a link to the model being used.", "Seeing the same problem with TFLite, and model where only some layers support CoreML delegate. What's interesting, the model works if used in 32-bit float mode; but exhibits the behavior above when quantized to 16-bit floats.\r\n\r\nThis is with TFLite 2.5.0 on iOS 14.8 and iPad Pro M1.", "im facing the same problem, \r\nthe models are mediapipe pose and hands models tflite.\r\ntflite version is 2.6.0 C api.\r\ncode:\r\n\r\n`\r\n  m_model = TfLiteModelCreateFromFile(modelFile.c_str());\r\n  if (m_model == nullptr)\r\n  {\r\n    LOG_ERROR(\"Failed to build tflite model. Got null object from BuildFromFile() for model: {}\", modelFile);\r\n    return false;\r\n  }\r\n  TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n  TfLiteCoreMlDelegateOptions optionsCoreML;\r\n  optionsCoreML.enabled_devices = TfLiteCoreMlDelegateAllDevices;\r\n  m_delegateCoreML = TfLiteCoreMlDelegateCreate(&optionsCoreML);\r\n  TfLiteInterpreterOptionsAddDelegate(options, m_delegateCoreML);\r\n  if(!m_delegateCoreML)\r\n  {\r\n    TfLiteXNNPackDelegateOptions optionsXNNPACK = TfLiteXNNPackDelegateOptionsDefault();\r\n    m_delegateXNNPACK = TfLiteXNNPackDelegateCreate(&optionsXNNPACK);\r\n    TfLiteInterpreterOptionsAddDelegate(options, m_delegateXNNPACK);\r\n  }\r\n  \r\n  m_interpreter = (TfLiteInterpreter*)TfLiteInterpreterCreate(m_model, options);\r\n  if (m_interpreter == nullptr)\r\n  {\r\n    LOG_ERROR(\"Failed to build tflite interpreter. Got null object from InterpreterBuilder() for model: {}\", modelFile);\r\n    return false;\r\n  }\r\n  \r\n  // Allocate tensor buffers\r\n  if (TfLiteInterpreterAllocateTensors(m_interpreter) != kTfLiteOk)\r\n  {\r\n    LOG_ERROR(\"Failed to allocate. interpreter TfLiteInterpreterAllocateTensors != kTfLiteOk for model: {}\", modelFile);\r\n    return false;\r\n  }\r\n`"]}, {"number": 43471, "title": "Additive Attention Layer is not really Additive Attention", "body": "In Bahdanau's paper, after applying the non-linearity (tanh), there is a dot product between a vector va and the result of the non-linearity (page 12 of the paper). Indeed the result of the tanh(...) operation is a n x 1 matrix. va is a n x 1 matrix, which is transposed into a 1 x n. So the final result is a single score for each key. After this, the softmax function is applied to the set of scores.\r\nInstead, the current AdditiveAttention layer does a simple (optional) scaling along the feature dimension.\r\n\r\nMoreover, the score is computed incorrectly, as reported by #39332\r\n\r\n\r\n", "comments": ["@AGalassi \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease, share colab link or simple standalone code to eproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "/cc @roumposg", "> @AGalassi\r\n> \r\n> Request you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> Please, share colab link or simple standalone code to eproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n\r\nThanks for the fast answer. It's not about the execution, it's the implementation itself (and the documentation).\r\nThe current implementation is some sort of different co-attention that does not correspond to Additive Attention nor to Bahdanau's paper.\r\n\r\nThe file is tensorflow/python/keras/layers/dense_attention.py, the class is AdditiveAttention. If you need more information of any sort just tell me.", "Any update to this issue? \r\nI was reviewing the source code of `keras.layers.AdditiveAttention()`, and found it was not implemented as the original paper intended.", "Upon reading #39332, it seems like I should aware of the base implementation of this source code and call additional this and that(matrices W1, W2 and so on) by my own when I use this..", "As far as I know, no updates.\r\nAlso, I am not sure why @jvishnuvardhan has labeled this as a feature and not a bug. Maybe because is more of a content matter than a technical matter."]}, {"number": 43468, "title": "[ROCm] Incorrect path to clang. ", "body": "Linux 5.7.6\r\n\r\nPython 3.8\r\n\r\nROCm 3.8\r\n\r\ngcc 10.2.0-9 (Debian testing).\r\n\r\nrocminfo, hipconfig works.\r\n\r\npip3 install tensorflow-rocm  works.\r\n\r\nVerified tensorflow-rocm mostly works by checking few things manually.\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n>>> tf.version.GIT_VERSION\r\n'v2.3.0-rc1-2358-gc0826c7973'\r\n>>> tf.version.VERSION\r\n'2.3.0'\r\n>>> tf.test.is_built_with_rocm()\r\nTrue\r\n>>> tf.config.list_physical_devices('GPU')\r\n2020-09-22 21:25:14.201524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libamdhip64.so\r\n2020-09-22 21:25:14.255178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:43:00.0 name: Fiji [Radeon R9 FURY / NANO Series]     ROCm AMD GPU ISA: gfx803\r\ncoreClock: 1.05GHz coreCount: 64 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 59.60GiB/s\r\n2020-09-22 21:25:14.258181: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so\r\n2020-09-22 21:25:14.258947: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so\r\n2020-09-22 21:25:14.264645: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so\r\n2020-09-22 21:25:14.264860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so\r\n2020-09-22 21:25:14.264958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n>>> \r\n```\r\n\r\nDoing more complex things, like mnist training sanity check fails:\r\n\r\n```\r\nroot@debian:~# LD_LIBRARY_PATH=/opt/rocm-3.8.0/lib ROCM_PATH=/opt/rocm-3.8.0 python3 /home/user/tf_train_test.py \r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2020-09-22 21:20:23.251851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libamdhip64.so\r\n2020-09-22 21:20:23.304418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:43:00.0 name: Fiji [Radeon R9 FURY / NANO Series]     ROCm AMD GPU ISA: gfx803\r\ncoreClock: 1.05GHz coreCount: 64 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 59.60GiB/s\r\n2020-09-22 21:20:23.307404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so\r\n2020-09-22 21:20:23.308140: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so\r\n2020-09-22 21:20:23.313617: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so\r\n2020-09-22 21:20:23.313820: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so\r\n2020-09-22 21:20:23.313904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-22 21:20:23.314125: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-22 21:20:23.320609: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3499500000 Hz\r\n2020-09-22 21:20:23.321952: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28c5f10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-22 21:20:23.322004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-22 21:20:23.323548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2815ff0 initialized for platform ROCM (this does not guarantee that XLA will be used). Devices:\r\n2020-09-22 21:20:23.323562: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Fiji [Radeon R9 FURY / NANO Series], AMDGPU ISA version: gfx803\r\n2020-09-22 21:20:23.615134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:43:00.0 name: Fiji [Radeon R9 FURY / NANO Series]     ROCm AMD GPU ISA: gfx803\r\ncoreClock: 1.05GHz coreCount: 64 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 59.60GiB/s\r\n2020-09-22 21:20:23.615223: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so\r\n2020-09-22 21:20:23.615241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so\r\n2020-09-22 21:20:23.615256: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocfft.so\r\n2020-09-22 21:20:23.615269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocrand.so\r\n2020-09-22 21:20:23.615352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-22 21:20:23.615382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-22 21:20:23.615393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-22 21:20:23.615400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-22 21:20:23.615531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3796 MB memory) -> physical GPU (device: 0, name: Fiji [Radeon R9 FURY / NANO Series], pci bus id: 0000:43:00.0)\r\n2020-09-22 21:20:23.840997: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.\r\n2020-09-22 21:20:23.844554: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.\r\n2020-09-22 21:20:23.846662: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.\r\nEpoch 1/12\r\n2020-09-22 21:20:24.065540: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:505] ROCm Fusion is enabled.\r\n2020-09-22 21:20:24.071930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library librocblas.so\r\n2020-09-22 21:20:24.116035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libMIOpen.so\r\nMIOpen(HIP): Error [ValidateGcnAssemblerImpl] Wrong path to assembler: '/opt/rocm/llvm/bin/clang'. Expect performance degradation.\r\nclang-11: error: cannot find ROCm installation.  Provide its path via --rocm-path, or pass -nogpulib.\r\nMIOpen Error: /root/driver/MLOpen/src/tmp_dir.cpp:47: Can't execute cd /tmp/miopen-gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp-c34b-acdd-79a3-774e;  /opt/rocm-3.8.0/llvm/bin/clang++  -std=c++14  -DCK_PARAM_PROBLEM_N=128 -DCK_PARAM_PROBLEM_K=64 -DCK_PARAM_PROBLEM_C=32 -DCK_PARAM_PROBLEM_HI=26 -DCK_PARAM_PROBLEM_WI=26 -DCK_PARAM_PROBLEM_HO=24 -DCK_PARAM_PROBLEM_WO=24 -DCK_PARAM_PROBLEM_Y=3 -DCK_PARAM_PROBLEM_X=3 -DCK_PARAM_PROBLEM_CONV_STRIDE_H=1 -DCK_PARAM_PROBLEM_CONV_STRIDE_W=1 -DCK_PARAM_PROBLEM_CONV_DILATION_H=1 -DCK_PARAM_PROBLEM_CONV_DILATION_W=1 -DCK_PARAM_PROBLEM_IN_LEFT_PAD_H=0 -DCK_PARAM_PROBLEM_IN_LEFT_PAD_W=0 -DCK_PARAM_PROBLEM_IN_RIGHT_PAD_H=0 -DCK_PARAM_PROBLEM_IN_RIGHT_PAD_W=0 -DCK_PARAM_TUNABLE_BLOCK_SIZE=64 -DCK_PARAM_TUNABLE_GEMM_M_PER_BLOCK=32 -DCK_PARAM_TUNABLE_GEMM_N_PER_BLOCK=64 -DCK_PARAM_TUNABLE_GEMM_K_PER_BLOCK=16 -DCK_PARAM_TUNABLE_GEMM_M_PER_THREAD=2 -DCK_PARAM_TUNABLE_GEMM_N_PER_THREAD=4 -DCK_PARAM_TUNABLE_GEMM_M_LEVEL0_CLUSTER=4 -DCK_PARAM_TUNABLE_GEMM_N_LEVEL0_CLUSTER=4 -DCK_PARAM_TUNABLE_GEMM_M_LEVEL1_CLUSTER=2 -DCK_PARAM_TUNABLE_GEMM_N_LEVEL1_CLUSTER=2 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_K=8 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_M=8 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_SRC_DATA_PER_READ_GEMM_M=4 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_K=4 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_CLUSTER_LENGTHS_GEMM_N=16 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_SRC_DATA_PER_READ_GEMM_N=4 -DCK_PARAM_TUNABLE_GEMM_C_THREAD_COPY_DST_DATA_PER_WRITE_GEMM_N1=1 -DCK_PARAM_DEPENDENT_GRID_SIZE=10368 -DCK_THREADWISE_GEMM_USE_AMD_INLINE_ASM=0 -DCK_USE_AMD_INLINE_ASM=0 -DCK_USE_AMD_BUFFER_ATOMIC_ADD=0 -DMIOPEN_USE_FP16=0 -DMIOPEN_USE_FP32=1 -DMIOPEN_USE_INT8=0 -DMIOPEN_USE_INT8x4=0 -DMIOPEN_USE_BFP16=0 -DMIOPEN_USE_INT32=0 -DMIOPEN_USE_RNE_BFLOAT16=1 -DCK_PARAM_TUNABLE_GEMM_A_BLOCK_COPY_DST_DATA_PER_WRITE_GEMM_M=4 -DCK_PARAM_TUNABLE_GEMM_B_BLOCK_COPY_DST_DATA_PER_WRITE_GEMM_N=4 -mcpu=gfx803 -Wno-everything --cuda-gpu-arch=gfx803 --cuda-device-only -c -O3  -Wno-unused-command-line-argument -I. -mllvm -amdgpu-early-inline-all=true -mllvm -amdgpu-function-calls=false -xhip --hip-device-lib-path=/opt/rocm/lib -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 -D__HIP_ROCclr__=1 -isystem /opt/rocm-3.8.0/hip/../include -isystem /opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/.. -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 -D__HIP_PLATFORM_HCC__=1 -D__HIP_ROCclr__=1 -isystem /opt/rocm-3.8.0/hip/include -isystem /opt/rocm/include -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 --hip-link -L/opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/../lib/linux -lclang_rt.builtins-x86_64 -mllvm -amdgpu-enable-global-sgpr-addr -mllvm --amdgpu-spill-vgpr-to-agpr=0 gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp -o /tmp/miopen-gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp-c34b-acdd-79a3-774e/gridwise_convolution_backward_data_implicit_gemm_v1r1_nchw_kcyx_nkhw.cpp.o\r\n2020-09-22 21:20:28.516500: F tensorflow/stream_executor/rocm/rocm_dnn.cc:3572] call to miopenFindConvolutionBackwardDataAlgorithm failed: miopenStatusUnknownError\r\nAborted\r\n```\r\n\r\n\r\n\r\n\r\nhipconfig:\r\n\r\n```\r\nroot@debian:~# ROCM_PATH=/opt/rocm-3.8.0 /opt/rocm-3.8.0/bin/hipconfig \r\nHIP version  : 3.8.20371-d1886b0b\r\n\r\n== hipconfig\r\nHIP_PATH     : /opt/rocm-3.8.0/hip\r\nROCM_PATH    : /opt/rocm-3.8.0\r\nHIP_COMPILER : clang\r\nHIP_PLATFORM : hcc\r\nHIP_RUNTIME  : ROCclr\r\nCPP_CONFIG   :  -D__HIP_PLATFORM_HCC__=  -I/opt/rocm-3.8.0/hip/include -I/opt/rocm-3.8.0/llvm/bin/../lib/clang/11.0.0 -I/opt/rocm-3.8.0/hsa/include -D__HIP_ROCclr__\r\n\r\n== hip-clang\r\nHSA_PATH         : /opt/rocm-3.8.0/hsa\r\nHIP_CLANG_PATH   : /opt/rocm-3.8.0/llvm/bin\r\nclang version 11.0.0 (/src/external/llvm-project/clang b98349b12ffa706d0e863a3f1176b20d2a6c438b)\r\nTarget: x86_64-unknown-linux-gnu\r\nThread model: posix\r\nInstalledDir: /opt/rocm-3.8.0/llvm/bin\r\nLLVM (http://llvm.org/):\r\n  LLVM version 11.0.0git\r\n  Optimized build.\r\n  Default target: x86_64-unknown-linux-gnu\r\n  Host CPU: znver1\r\n\r\n  Registered Targets:\r\n    amdgcn - AMD GCN GPUs\r\n    r600   - AMD GPUs HD2XXX-HD6XXX\r\n    x86    - 32-bit X86: Pentium-Pro and above\r\n    x86-64 - 64-bit X86: EM64T and AMD64\r\nhip-clang-cxxflags : -D__HIP_ROCclr__ -std=c++11 -isystem /opt/rocm-3.8.0/llvm/lib/clang/11.0.0/include/.. -isystem /opt/rocm-3.8.0/hsa/include -D__HIP_ROCclr__ -isystem /opt/rocm-3.8.0/hip/include -D__HIP_ARCH_GFX803__=1  -O3\r\nhip-clang-ldflags  :  -L/opt/rocm-3.8.0/hip/lib -O3 -lgcc_s -lgcc -lpthread -lm\r\n\r\n=== Environment Variables\r\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\n\r\n== Linux Kernel\r\nHostname     : debian\r\nLinux debian 5.7.0-1-amd64 #1 SMP Debian 5.7.6-1 (2020-06-24) x86_64 GNU/Linux\r\nNo LSB modules are available.\r\nDistributor ID:\tDebian\r\nDescription:\tDebian GNU/Linux bullseye/sid\r\nRelease:\tunstable\r\nCodename:\tsid\r\n```\r\n\r\nrocminfo:\r\n```\r\n# /opt/rocm-3.8.0/bin/rocminfo\r\nROCk module is loaded\r\nAble to open /dev/kfd read-write\r\n=====================    \r\nHSA System Attributes    \r\n=====================    \r\nRuntime Version:         1.1\r\nSystem Timestamp Freq.:  1000.000000MHz\r\nSig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count)\r\nMachine Model:           LARGE                              \r\nSystem Endianness:       LITTLE                             \r\n\r\n==========               \r\nHSA Agents               \r\n==========               \r\n*******                  \r\nAgent 1                  \r\n*******                  \r\n  Name:                    AMD Ryzen Threadripper 2950X 16-Core Processor\r\n  Uuid:                    CPU-XX                             \r\n  Marketing Name:          AMD Ryzen Threadripper 2950X 16-Core Processor\r\n  Vendor Name:             CPU                                \r\n  Feature:                 None specified                     \r\n  Profile:                 FULL_PROFILE                       \r\n  Float Round Mode:        NEAR                               \r\n  Max Queue Number:        0(0x0)                             \r\n  Queue Min Size:          0(0x0)                             \r\n  Queue Max Size:          0(0x0)                             \r\n  Queue Type:              MULTI                              \r\n  Node:                    0                                  \r\n  Device Type:             CPU                                \r\n  Cache Info:              \r\n    L1:                      32768(0x8000) KB                   \r\n  Chip ID:                 0(0x0)                             \r\n  Cacheline Size:          64(0x40)                           \r\n  Max Clock Freq. (MHz):   3500                               \r\n  BDFID:                   0                                  \r\n  Internal Node ID:        0                                  \r\n  Compute Unit:            16                                 \r\n  SIMDs per CU:            0                                  \r\n  Shader Engines:          0                                  \r\n  Shader Arrs. per Eng.:   0                                  \r\n  WatchPts on Addr. Ranges:1                                  \r\n  Features:                None\r\n  Pool Info:               \r\n    Pool 1                   \r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    65850456(0x3eccc58) KB             \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       TRUE                               \r\n    Pool 2                   \r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      \r\n      Size:                    65850456(0x3eccc58) KB             \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       TRUE                               \r\n  ISA Info:                \r\n    N/A                      \r\n*******                  \r\nAgent 2                  \r\n*******                  \r\n  Name:                    AMD Ryzen Threadripper 2950X 16-Core Processor\r\n  Uuid:                    CPU-XX                             \r\n  Marketing Name:          AMD Ryzen Threadripper 2950X 16-Core Processor\r\n  Vendor Name:             CPU                                \r\n  Feature:                 None specified                     \r\n  Profile:                 FULL_PROFILE                       \r\n  Float Round Mode:        NEAR                               \r\n  Max Queue Number:        0(0x0)                             \r\n  Queue Min Size:          0(0x0)                             \r\n  Queue Max Size:          0(0x0)                             \r\n  Queue Type:              MULTI                              \r\n  Node:                    1                                  \r\n  Device Type:             CPU                                \r\n  Cache Info:              \r\n    L1:                      32768(0x8000) KB                   \r\n  Chip ID:                 0(0x0)                             \r\n  Cacheline Size:          64(0x40)                           \r\n  Max Clock Freq. (MHz):   3500                               \r\n  BDFID:                   0                                  \r\n  Internal Node ID:        1                                  \r\n  Compute Unit:            16                                 \r\n  SIMDs per CU:            0                                  \r\n  Shader Engines:          0                                  \r\n  Shader Arrs. per Eng.:   0                                  \r\n  WatchPts on Addr. Ranges:1                                  \r\n  Features:                None\r\n  Pool Info:               \r\n    Pool 1                   \r\n      Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED\r\n      Size:                    66044092(0x3efc0bc) KB             \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       TRUE                               \r\n    Pool 2                   \r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      \r\n      Size:                    66044092(0x3efc0bc) KB             \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       TRUE                               \r\n  ISA Info:                \r\n    N/A                      \r\n*******                  \r\nAgent 3                  \r\n*******                  \r\n  Name:                    gfx803                             \r\n  Uuid:                    GPU-XX                             \r\n  Marketing Name:          Fiji [Radeon R9 FURY / NANO Series]\r\n  Vendor Name:             AMD                                \r\n  Feature:                 KERNEL_DISPATCH                    \r\n  Profile:                 BASE_PROFILE                       \r\n  Float Round Mode:        NEAR                               \r\n  Max Queue Number:        128(0x80)                          \r\n  Queue Min Size:          4096(0x1000)                       \r\n  Queue Max Size:          131072(0x20000)                    \r\n  Queue Type:              MULTI                              \r\n  Node:                    2                                  \r\n  Device Type:             GPU                                \r\n  Cache Info:              \r\n    L1:                      16(0x10) KB                        \r\n  Chip ID:                 29440(0x7300)                      \r\n  Cacheline Size:          64(0x40)                           \r\n  Max Clock Freq. (MHz):   1050                               \r\n  BDFID:                   17152                              \r\n  Internal Node ID:        2                                  \r\n  Compute Unit:            64                                 \r\n  SIMDs per CU:            4                                  \r\n  Shader Engines:          4                                  \r\n  Shader Arrs. per Eng.:   1                                  \r\n  WatchPts on Addr. Ranges:4                                  \r\n  Features:                KERNEL_DISPATCH \r\n  Fast F16 Operation:      FALSE                              \r\n  Wavefront Size:          64(0x40)                           \r\n  Workgroup Max Size:      1024(0x400)                        \r\n  Workgroup Max Size per Dimension:\r\n    x                        1024(0x400)                        \r\n    y                        1024(0x400)                        \r\n    z                        1024(0x400)                        \r\n  Max Waves Per CU:        40(0x28)                           \r\n  Max Work-item Per CU:    2560(0xa00)                        \r\n  Grid Max Size:           4294967295(0xffffffff)             \r\n  Grid Max Size per Dimension:\r\n    x                        4294967295(0xffffffff)             \r\n    y                        4294967295(0xffffffff)             \r\n    z                        4294967295(0xffffffff)             \r\n  Max fbarriers/Workgrp:   32                                 \r\n  Pool Info:               \r\n    Pool 1                   \r\n      Segment:                 GLOBAL; FLAGS: COARSE GRAINED      \r\n      Size:                    4194304(0x400000) KB               \r\n      Allocatable:             TRUE                               \r\n      Alloc Granule:           4KB                                \r\n      Alloc Alignment:         4KB                                \r\n      Accessible by all:       FALSE                              \r\n    Pool 2                   \r\n      Segment:                 GROUP                              \r\n      Size:                    64(0x40) KB                        \r\n      Allocatable:             FALSE                              \r\n      Alloc Granule:           0KB                                \r\n      Alloc Alignment:         0KB                                \r\n      Accessible by all:       FALSE                              \r\n  ISA Info:                \r\n    ISA 1                    \r\n      Name:                    amdgcn-amd-amdhsa--gfx803          \r\n      Machine Models:          HSA_MACHINE_MODEL_LARGE            \r\n      Profiles:                HSA_PROFILE_BASE                   \r\n      Default Rounding Mode:   NEAR                               \r\n      Default Rounding Mode:   NEAR                               \r\n      Fast f16:                TRUE                               \r\n      Workgroup Max Size:      1024(0x400)                        \r\n      Workgroup Max Size per Dimension:\r\n        x                        1024(0x400)                        \r\n        y                        1024(0x400)                        \r\n        z                        1024(0x400)                        \r\n      Grid Max Size:           4294967295(0xffffffff)             \r\n      Grid Max Size per Dimension:\r\n        x                        4294967295(0xffffffff)             \r\n        y                        4294967295(0xffffffff)             \r\n        z                        4294967295(0xffffffff)             \r\n      FBarrier Max Size:       32                                 \r\n*** Done ***             \r\n```\r\n\r\n\r\n\r\nTraining code:\r\n```python3\r\n#!/usr/bin/env python3\r\n\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\nfrom tensorflow.keras import backend as K\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n  input_shape = (1, img_rows, img_cols)\r\nelse:\r\n  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n  input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n```", "comments": ["FYI. Creating `/opt/rocm` symlink of course solves the problem, and the whole program works (trains, uses GPU, etc). But that is not the point. It shouldn't be looking at hard coded paths like that. Using PATH instead for searching for `clang` maybe? Allow `ROCM_PATH` to override `/opt/rocm`? I expect some distro packagers might even install ROCm in standard locations (/usr/bin), so maybe `rocm-clang` / `hip-clang` ? (not sure what is more fitting, because I am not familiar with structure of compilation pipeline).\r\n\r\n\r\n\r\n", "@baryluk for the time being the presence of the `/opt/rocm` symlink is a requirement that cannot be bypassed :(\r\n\r\nWe are aware of this problem, and are working towards removing this requirement in a future ROCm release.", "Thank you, I would really appreciate ability to not depend on `/opt/rocm`  , for 3 reasons:\r\n\r\n1) Ability to install multiple versions of rocm  (some versions sometimes do have regressions in some applications, etc, so it is useful to be able to keep multiple versions installed)\r\n\r\n2) Ability to compile / install it as a normal user without root permissions, i.e. somewhere under `$HOME`.\r\n\r\n3) Ability to integrate into distros packages, which usually will install rocm somewhere under `/usr`.\r\n\r\nThanks for understanding.\r\n\r\nIt is not super high priority, but definitively something people do find confusing from time to time, so fixing this would be really nice.\r\n", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/ac17d922cda8e1c57d767390d6588f7e/untitled218.ipynb)..Thanks !"]}, {"number": 43467, "title": "\"Cadence processor cores only\" license restriction", "body": "There are 29 files in the repo which look like they have an MIT license header, but upon closer inspection it actually contains a restricted license. For example, from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/testing/test_xtensa_hifi_binary.sh:\r\n\r\n> Permission is hereby granted, free of charge, to any person obtaining\r\n> a copy of this software and associated documentation files (the\r\n> \"Software\"), **to use this Software with Cadence processor cores only and\r\n> not with any other processors and platforms**, subject to\r\n> the following conditions:\r\n\r\nThe files with this license notice can be seen at https://github.com/tensorflow/tensorflow/search?q=cadence+processor+cores+only\r\n\r\nThe project may want to consider removing these files, or perhaps moving them to a different repository, as content with a use restriction like this might not be seen by the community as compatible with Apache-2.0. (I gather this code will be received by anyone who checks out the source code repo, whether or not they're using this xtensa section of it.)", "comments": ["@petewarden or @advaitjain can you comment?"]}, {"number": 43460, "title": "Feature Request: Support for ragged argmin/argmax", "body": "**System information**\r\n- TensorFlow version: 2.3.0\r\n- Are you willing to contribute it: Possibly\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.math.argmax/argmin does not support ragged tensors.\r\n**Will this change the current api? How?**\r\nNo breaking changes, simply additive in nature.\r\n**Who will benefit with this feature?**\r\nIn my opinion this is particularly important, as it can condense a ragged dimension down to a 1-dimension. In my use case, I have predictions that can be of variable length. I want to take the minimum, which would effectively eliminate the ragged dimension.\r\n**Any Other info.**\r\nFor anyone else looking for a workaround, currently my approach is to convert the ragged tensor to a regular tensor. The default value should be max positive number if you are performing min or max negative number if you are performing max.", "comments": ["Interested !!", "Here is a simple workaround.\r\n```\r\nindices = tf.map_fn(tf.math.argmax, ragged_tensor, fn_output_signature=tf.int64)\r\n```"]}, {"number": 43448, "title": "Complete functionality of SparseTensor.__mul__", "body": "**System information**\r\n- TensorFlow version (you are using): 2,3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently SparseTensor.__mul__ is limited to multiplication between a sparse tensor and a dense tensor. Broadcasting is only supported from the dense side to the sparse side, as per the documentation: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor#__mul__\r\n\r\nMy request is to expand this behaviour to also include multiplications between two sparse tensors, and to allow broadcasting in both directions. This will complete the functionality and make the use of sparse tensors similar to that of dense tensors.\r\n\r\n**Will this change the current api? How?**\r\nThe current options should of course still be supported, but more tensor combinations should be supported. This is purely the removal of a limitation that is already stated in the API.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who uses SparseTensors will want to manipulate them using the basic functions like multiplication and division (which can probably be added at the same time). This is certainly true for my own use in Graph Neural networks, but sparse tensor calculations are needed in many other applications as well.\r\n\r\n**Any Other info.**\r\nI have written a pure python implementation of broadcast multiply between two sparse tensors, which is shared in this stackoverflow question: https://stackoverflow.com/questions/63023958/how-to-efficiently-broadcast-multiply-two-sparse-tensors-in-tensorflow. However, as noted there, it is not memory efficient enough to be really useful. I expect that a C++ implementation is needed, and unfortunately I do not know how to write that. If I can contribute any more code, I would be happy to, but my knowledge of C++ and CUDA is limited.", "comments": ["We have also https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseMatrixSparseMatMul for SparseTensor (CSR).", "> We have also https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseMatrixSparseMatMul for SparseTensor (CSR).\r\n\r\nThank you, Yes, I have looked at that as well and it is very different from what I reference here. Note that matrix multiplication is a different operation than regular tensor multiplication with broadcasting. The fact that both are called multiplication is actually a little misleading, but that's the fault of mathematics terminology I guess.", "You are right. I have re-read your ticket description and yes you was specifically requesting broadcasting in both directions."]}, {"number": 43425, "title": "Negate Kernel for int8 is yet to be implemented", "body": "There's a TODO to implement the negate kernel for the int8 data type of TFLite Micro: https://github.com/tensorflow/tensorflow/blob/b253e82d5360c83db0d2c4e958f7822b38a06660/tensorflow/lite/micro/kernels/neg.cc#L36\r\nI already have the code written in a PR earlier, however, I have to restart due to the present guidelines, should I proceed or leave this as it seems it's not a high-priority concern at the moment.", "comments": ["/cc @wangtz", "@wangtz ", "@bhack @ymodak ", "Hi Basit,\r\n\r\nIf you already have the PR plz feel free to send a PR. I would be good to have quantized neg support for some model.\r\n\r\nI think Advait just meant to prefer small, well scoped PR, instead of large PR. \r\n\r\nAlso cc Advait.\r\n\r\nThanks,\r\nTiezhen", "Let's hold off on this for now. I'm in the process of putting together a porting guide and would like to have PRs follow that -- ETA is a week or two.\r\n\r\nOnce we have the guide, and if the missing functionality is blocking @lamarrr then a PR would be very welcome. @lamarrr, please add some context on what you're trying to do that needs an int8 negate support in TFLM.", "> Let's hold off on this for now. I'm in the process of putting together a porting guide and would like to have PRs follow that -- ETA is a week or two.\r\n> \r\n> Once we have the guide, and if the missing functionality is blocking @lamarrr then a PR would be very welcome. @lamarrr, please add some context on what you're trying to do that needs an int8 negate support in TFLM.\r\n\r\nNo, It\u2019s not blocking me. It\u2019s marked as TODO and I wanted to help solve the technical debt.", "This filed to moved under `tensorflow/tflite-micro` repo, below is the updated path for the file.\r\nhttps://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/kernels/neg.cc"]}, {"number": 43380, "title": "Using keras GRUCell layer in TFLM", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- TensorFlow installed from (source or binary): TFLM source, tf-nightly from installer\r\n- Tensorflow version (commit SHA if source):  tf-nightly 2.4.0.dev20200918\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nMy goal is to use a network using a keras.GRUCell layer in TFLM.\r\n\r\nThis issue is somwhat connected to:\r\nhttps://github.com/tensorflow/tensorflow/issues/42582\r\nand\r\nhttps://github.com/tensorflow/tensorflow/issues/41690\r\nAs they both arose from the goal of using a GRU layer. \r\n\r\n\r\n\r\nThis is a sample model:\r\nstate0 = tf.keras.Input(shape=( 1,128), dtype='float32', name='state0')\r\nmain_input = tf.keras.Input(shape=(1, 200), dtype='float32', name='main_input')\r\ngru0, state0_out = tf.keras.layers.GRUCell(128,reset_after=True,recurrent_activation='sigmoid',use_bias = True,name=\"grucell0\")(main_input,state0)\r\n\r\nmodel3 = tf.keras.models.Model(inputs=[main_input,state0],outputs = [gru0,state0_out])\r\n\r\n\r\nafter adding in SPLIT_V, I ran into two problems:\r\nA. Missing SHAPE operator - I will open a PR shortly.\r\n\r\nB. A problem with tensor allocation which I will describe and suggest a workaround for TFLM and open a PR if you think the solution is acceptable.\r\n\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nI converted the above model to TFLite and then imported into TFLM.\r\nThe error I got was from AllocationInfoBuilder::AddTensors() in micro_allocator.cc\r\n\r\n\"Logic error in memory planner, tensor 14 has an invalid lifetime: \"\r\n          \"first_created: 1, last_used: -1\",\r\n\r\nI examined the ops the converter generated:\r\n{'index': 0, 'op_name': 'SHAPE', 'inputs': array([0], dtype=int32), 'outputs': array([12], dtype=int32)\r\n **{'index': 1, 'op_name': 'UNPACK', 'inputs': array([12], dtype=int32), 'outputs': array([13, 14, 15], dtype=int32)**\r\n {'index': 2, 'op_name': 'RESHAPE', 'inputs': array([0, 3], dtype=int32), 'outputs': array([16], dtype=int32)\r\n {'index': 3, 'op_name': 'PACK', 'inputs': array([13,  5,  6], dtype=int32), 'outputs': array([17], dtype=int32)\r\n {'index': 4, 'op_name': 'FULLY_CONNECTED', 'inputs': array([16, 10, -1], dtype=int32), 'outputs': array([18], dtype=int32)\r\n {'index': 5, 'op_name': 'RESHAPE', 'inputs': array([18, 17], dtype=int32), 'outputs': array([19], dtype=int32)\r\n {'index': 6, 'op_name': 'ADD', 'inputs': array([19,  9], dtype=int32), 'outputs': array([20], dtype=int32)\r\n {'index': 7, 'op_name': 'SPLIT', 'inputs': array([ 7, 20], dtype=int32), 'outputs': array([21, 22, 23], dtype=int32)\r\n {'index': 8, 'op_name': 'RESHAPE', 'inputs': array([1, 4], dtype=int32), 'outputs': array([24], dtype=int32)\r\n {'index': 9, 'op_name': 'FULLY_CONNECTED', 'inputs': array([24, 11, -1], dtype=int32), 'outputs': array([25], dtype=int32)\r\n {'index': 10, 'op_name': 'SHAPE', 'inputs': array([1], dtype=int32), 'outputs': array([26], dtype=int32)\r\n {'index': 11, 'op_name': 'UNPACK', 'inputs': array([26], dtype=int32), 'outputs': array([27, 28, 29], dtype=int32)\r\n {'index': 12, 'op_name': 'PACK', 'inputs': array([27,  5,  6], dtype=int32), 'outputs': array([30], dtype=int32)\r\n {'index': 13, 'op_name': 'RESHAPE', 'inputs': array([25, 30], dtype=int32), 'outputs': array([31], dtype=int32)\r\n {'index': 14, 'op_name': 'ADD', 'inputs': array([31,  9], dtype=int32), 'outputs': array([32], dtype=int32)\r\n {'index': 15, 'op_name': 'SPLIT_V', 'inputs': array([32,  2,  7], dtype=int32), 'outputs': array([33, 34, 35], dtype=int32)\r\n {'index': 16, 'op_name': 'ADD', 'inputs': array([21, 33], dtype=int32), 'outputs': array([36], dtype=int32)\r\n {'index': 17, 'op_name': 'LOGISTIC', 'inputs': array([36], dtype=int32), 'outputs': array([37], dtype=int32)\r\n {'index': 18, 'op_name': 'SUB', 'inputs': array([ 8, 37], dtype=int32), 'outputs': array([38], dtype=int32)\r\n {'index': 19, 'op_name': 'ADD', 'inputs': array([22, 34], dtype=int32), 'outputs': array([39], dtype=int32)\r\n {'index': 20, 'op_name': 'LOGISTIC', 'inputs': array([39], dtype=int32), 'outputs': array([40], dtype=int32)\r\n {'index': 21, 'op_name': 'MUL', 'inputs': array([40, 35], dtype=int32), 'outputs': array([41], dtype=int32)\r\n {'index': 22, 'op_name': 'ADD', 'inputs': array([23, 41], dtype=int32), 'outputs': array([42], dtype=int32)\r\n {'index': 23, 'op_name': 'TANH', 'inputs': array([42], dtype=int32), 'outputs': array([43], dtype=int32)\r\n {'index': 24, 'op_name': 'MUL', 'inputs': array([38, 43], dtype=int32), 'outputs': array([44], dtype=int32)\r\n {'index': 25, 'op_name': 'MUL', 'inputs': array([37,  1], dtype=int32), 'outputs': array([45], dtype=int32)\r\n {'index': 26, 'op_name': 'ADD', 'inputs': array([45, 44], dtype=int32), 'outputs': array([46], dtype=int32)}\r\n\r\nNotice that UNPACK uses tensors 14 and 15 as outputs but they aren't used later on in the model. This is also true for the next SHAPE/UNPACK pattern and tensors 28 and 29.\r\n\r\nI assume this is caused for reasons unknown by the TFLite conveter? The model works fine in python so I assume the tensor allocator in TFLite doesn't check for this.\r\n\r\nWhat happens in TFLM is that those tensors aren't \"caught\" in the flow marking last usage.\r\n\r\nThe solution/workaround I implemented and works for me was to initialize all tensors as persistent  (in the same way the current code does for the graph's output tensors):\r\n\r\nfor (int i = (subgraph->operators()->size() - 1); i >= 0; --i) {\r\n    const auto* op = subgraph->operators()->Get(i);\r\n    for (size_t n = 0; n < op->outputs()->size(); ++n) {\r\n      const int tensor_index = op->outputs()->Get(n);\r\n      AllocationInfo* current = &info_[tensor_index];\r\n      current->last_used = subgraph->operators()->size() - 1;\r\n    }\r\n  }\r\n\r\nAfterwards, the current implementation will handle all the tensors that serve as input to ops correctly and leave these problematic ones as persistent to the end of the invocation.\r\n\r\nAnother option would be to search specifically for this case and handle it separately and\r\nYET another option would be to see what in the TFLite converter is doing this...\r\n\r\n@advaitjain Please let me know what you think, I'd really like to get this sorted out :)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@advaitjain \r\n\r\nOpened a PR with what I described above as a possible fix:\r\nhttps://github.com/tensorflow/tensorflow/pull/43382\r\n\r\nLet me know what you think. I'd be glad to implement changes if you think this might break something or if there's a method you'd prefer.\r\n", "Thanks, will take a proper look on Monday.\n\nFor the SHAPE pull request, can you open a separate one for any changes to\nflatbuffer_conversions? That can be approved right away but sometimes take\nlonger to merge.\n\n\nOn Sun, Sep 20, 2020, 8:21 AM yair_ehrenwald <notifications@github.com>\nwrote:\n\n> @advaitjain <https://github.com/advaitjain>\n>\n> Opened a PR with what I described above as a possible fix:\n> #43382 <https://github.com/tensorflow/tensorflow/pull/43382>\n>\n> Let me know what you think. I'd be glad to implement changes if you think\n> this might break something or if there's a method you'd prefer.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43380#issuecomment-695799160>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAVJERR7FWZ4DY5KSSNQNOTSGYMV7ANCNFSM4RTVRA6A>\n> .\n>\n", "Thanks, will do.\r\n\r\nRegarding the memory_allocator, I think from a memory footprint POV it would be better to search explicitly for this case and set the life time to the minimum required (creation and end of life at the same index) but thought such a test might \"legalize\" some cases that should genuinely raise a flag..", "> Thanks, will do.\r\n> \r\n> Regarding the memory_allocator, I think from a memory footprint POV it would be better to search explicitly for this case and set the life time to the minimum required (creation and end of life at the same index) but thought such a test might \"legalize\" some cases that should genuinely raise a flag..\r\n\r\nOur suggestion would be to do some flatbuffer manipulations from python and leave the TFLM code unchanged. I'll send you some pointers later today.", "> > Thanks, will do.\r\n> > Regarding the memory_allocator, I think from a memory footprint POV it would be better to search explicitly for this case and set the life time to the minimum required (creation and end of life at the same index) but thought such a test might \"legalize\" some cases that should genuinely raise a flag..\r\n> \r\n> Our suggestion would be to do some flatbuffer manipulations from python and leave the TFLM code unchanged. I'll send you some pointers later today.\r\n\r\nSounds interesting, thanks!", "Update here is that our suggestion would involve using the python flatbuffer API to make some changes.\r\n\r\nI'm in the process of writing some documentation that would help but it appears that we are going to hit the same underlying issue as #41846 -- i.e. if schema_py_generated.py is empty with the bazel builds, you will not be able to directly do what I have in mind.\r\n\r\nI'm going to do some more investigation to see if there is some workaround here."]}, {"number": 43379, "title": "tf.keras.Model with multiple outputs, using fit() with a generator dataset passes y to loss with wrong shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (Docker on WSL)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9:\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: RTX 2070 Super 8GB\r\n\r\n**Describe the current behavior**\r\nWhen using a tf.keras.Model with multiple outputs, then using fit() with a generator dataset (created with tf.data.Dataset.from_generator), the loss function is passed a wrong shape (looks to be the shape of a flattened array of the y's for all toutputs).\r\n\r\n\r\n**Describe the expected behavior**\r\nThe loss function should be passed the correct shape from the generator.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere I am creating a model with one input of shape (1), and 2 outputs of shape (32) and a SparseCategoricalCrossentropy for the loss. Then I am creating a generator dataset with batch size 2 which yields 2 inputs, and 2 arrays with value for each output.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\ninp = layers.Input((1))\r\nout1 = layers.Dense(32)(inp)\r\nout2 = layers.Dense(32)(inp)\r\n\r\nmodel = models.Model(inputs=[inp], outputs=[out1, out2])\r\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy())\r\n\r\ndef gen():\r\n    for i in itertools.count(1):\r\n        yield [1, 2], [[11, 12], [21, 22]]\r\n\r\ngenerator_dataset = tf.data.Dataset.from_generator(\r\n    gen,\r\n    (tf.uint8, tf.uint8),\r\n    output_shapes=(\r\n        tf.TensorShape((2, 1)),\r\n        tf.TensorShape((2, 2))\r\n    )\r\n)\r\n\r\nmodel.fit(generator_dataset)\r\n```\r\nCollab notebook: https://colab.research.google.com/drive/1vEoWRzSKyeJCNkucqmkGQrVQp0mrJE4u\r\n\r\n**Other info / logs**\r\nBelow I post the error I get from the example above. The loss is being passed a tensor of shape (4,), which is the flattened shape of the y's instead of the correct shape, (2,1).\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-4b723daf89ac> in <module>()\r\n     22 )\r\n     23 \r\n---> 24 model.fit(generator_dataset)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1096                 batch_size=batch_size):\r\n   1097               callbacks.on_train_batch_begin(step)\r\n-> 1098               tmp_logs = train_function(iterator)\r\n   1099               if data_handler.should_sync:\r\n   1100                 context.async_wait()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781 \r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    821       # This is the first call of __call__, so we have to initialize.\r\n    822       initializers = []\r\n--> 823       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    824     finally:\r\n    825       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    695     self._concrete_stateful_fn = (\r\n    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 697             *args, **kwds))\r\n    698 \r\n    699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211 \r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    971           except Exception as e:  # pylint:disable=broad-except\r\n    972             if hasattr(e, \"ag_error_metadata\"):\r\n--> 973               raise e.ag_error_metadata.to_exception(e)\r\n    974             else:\r\n    975               raise\r\n\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step\r\n        y, y_pred, sample_weight, regularization_losses=self.losses)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\r\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:149 __call__\r\n        losses = ag_call(y_true, y_pred)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:253 call  **\r\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1567 sparse_categorical_crossentropy\r\n        y_true, y_pred, from_logits=from_logits, axis=axis)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4783 sparse_categorical_crossentropy\r\n        labels=target, logits=output)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:4176 sparse_softmax_cross_entropy_with_logits_v2\r\n        labels=labels, logits=logits, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:4091 sparse_softmax_cross_entropy_with_logits\r\n        logits.get_shape()))\r\n\r\n    ValueError: Shape mismatch: The shape of labels (received (4,)) should equal the shape of logits except for the last dimension (received (2, 32)).\r\n```\r\n", "comments": ["@mdatsev \r\nCould you try adding a Flatten Layer before the Dense Layers?\r\nPlease refer to: [link](https://stackoverflow.com/questions/50559654/shape-of-logits-and-labels-mismatch), [link1](https://blog.csdn.net/qq_23869697/article/details/104980566),#33855.", "@Saduf2019 \r\nThank you for the suggestions. I think a Flatten layer doesn't change things because the problem is with the final outputs of the dense layers and the given y data, which are passed to the SparseCategoricalCrossentropy.\r\nJust in case here is a collab notebook with the Flatten layer added, the behavior is the same as above: https://colab.research.google.com/drive/1YCvnikagqb8FI_fUbgbUYMS-lEqLjwds", "I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/90a741baeedd822314748f64f888f305/untitled415.ipynb).", "I could reproduce the issue with TF 2.5 .Please, find the gist [here](https://colab.research.google.com/gist/Saduf2019/64487dc440dd3a2cfb1c5cc9ec891a75/untitled589.ipynb).Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210527, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/848d32ffa184216705e458cbde915365/43379.ipynb). Thanks!"]}, {"number": 43374, "title": "XLA future", "body": "As declared the XLA shows promising results for certain use cases, its specialization for executable size reduction, and TensorFlow could use `tfcompile` tool that leverages XLA for ahead-of-time compilation (AOT). I would wonder if there is the implementation supporting other alternative backends and devices (e.g. ARM64)?\r\nWhat is the interconnection between XLA and  TensorFlow Lite?\r\n\r\n", "comments": ["I don't know If @asaadaldien can comment here expecially on AArch64 experiments.", "@peter197321 In the meantime I suggest you to take a look at:\r\nhttps://github.com/tensorflow/mlir-hlo#overview\r\nhttps://llvm.discourse.group/t/case-study-docs-on-vector-dialect-cpu-codegen/1674/15\r\n", "I am not familiar with `tf_compile`, but quickly looking at the AOT library build rules seems like `arm64`is already supported : \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl#L430\r\n", "@asaadaldien As the title Is also about the \"XLA future\" I think that you are involved in some  interesting codegen experimental activities on this arch right?"]}, {"number": 43369, "title": "Cant save keras RNN model with custom cell whose call function accepts constants", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.3.0\r\n- Python version:3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nWhen creating an RNN model with custom cells that accept constants. saving the model with the default SavedModel format will raise a ValueError \"RNN cell does not support constants\". Using the h5 format does work\r\n\r\n**Describe the expected behavior**\r\n\r\nI dont expect the ValueError, since the call function does support the constants argument\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI've re-used the custom cell sample from https://keras.io/guides/working_with_rnns/#rnns-with-listdict-inputs-or-nested-inputs\r\nbut just added the constants argument:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\r\nclass NestedCell(keras.layers.Layer):\r\n    def __init__(self, unit_1, unit_2, unit_3, **kwargs):\r\n        self.unit_1 = unit_1\r\n        self.unit_2 = unit_2\r\n        self.unit_3 = unit_3\r\n        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\r\n        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\r\n        super(NestedCell, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shapes):\r\n        # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]\r\n        i1 = input_shapes[0][1]\r\n        i2 = input_shapes[1][1]\r\n        i3 = input_shapes[1][2]\r\n\r\n        self.kernel_1 = self.add_weight(\r\n            shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"\r\n        )\r\n        self.kernel_2_3 = self.add_weight(\r\n            shape=(i2, i3, self.unit_2, self.unit_3),\r\n            initializer=\"uniform\",\r\n            name=\"kernel_2_3\",\r\n        )\r\n\r\n    def call(self, inputs, states, constants):\r\n        # inputs should be in [(batch, input_1), (batch, input_2, input_3)]\r\n        # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]\r\n        input_1, input_2 = tf.nest.flatten(inputs)\r\n        s1, s2 = states\r\n\r\n        output_1 = tf.matmul(input_1, self.kernel_1)\r\n        output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)\r\n        state_1 = s1 + output_1\r\n        state_2_3 = s2 + output_2_3\r\n\r\n        output = (output_1, output_2_3)\r\n        new_states = (state_1, state_2_3)\r\n\r\n        return output, new_states\r\n\r\n    def get_config(self):\r\n        return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}\r\n\r\nunit_1 = 10\r\nunit_2 = 20\r\nunit_3 = 30\r\n\r\ni1 = 32\r\ni2 = 64\r\ni3 = 32\r\nbatch_size = 64\r\nnum_batches = 10\r\ntimestep = 50\r\n\r\n\r\ninput_1 = keras.Input((None, i1))\r\ninput_2 = keras.Input((None, i2, i3))\r\ninput_const = keras.Input((None, i1))\r\ncell = NestedCell(unit_1, unit_2, unit_3)\r\noutputs = keras.layers.RNN(cell = cell)(inputs = (input_1, input_2), constants=input_const)\r\n\r\nmodel = keras.models.Model([input_1, input_2, input_const], outputs)\r\n\r\nmodel.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\r\nmodel.save(\"test\")\r\n```", "comments": ["@juulie \r\n\r\nPlease, save the model using `model.save(\"test.h5\")` .I am not seeing any issue in saving the model. please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6f763516bffa0cd53d6120bf6cdda1f2/untitled381.ipynb).Please refer this [tutorial ](https://www.tensorflow.org/tutorials/keras/save_and_load?hl=en) which will help you. Thanks!", "I know saving using the h5 format will work, it is stated in the original bug description. But saving using the SavedModel has one big advantage, from your [link](https://www.tensorflow.org/tutorials/keras/save_and_load):\r\n\r\n> The key difference between HDF5 and SavedModel is that HDF5 uses object configs to save the model architecture, while SavedModel saves the execution graph. Thus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the orginal code.\r\n\r\nIn the mean time, what would you suggest be the best way of saving my custom layer?", "I am experiencing the same problem. A CustomCell i created, takes constants as an argument to cal(), which results in an error when trying to save (in the standard tf format). Conversely, it is possible to save the CustomCell if I remove the constants-argument. Similar to @juulie, I want to save the execution graph in addition the the weights. \r\n\r\nUsing eager execution and the debugger, it somehow loops through the CustomCell and at a certain iteration, the state-tuple is empty, and the constants-tuple contains a copy of the input-tuple...?\r\n\r\nAny way to save the computation graph, other then using the standard tf format?\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):2.3.0\r\n- Python version:3.7\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.3\r\n- CUDA/cuDNN version: 10.1, 7.6\r\n- GPU model and memory: GeForce RTX 2080 with Max-Q Design\r\n", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210527, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/acdd3473fac9f65e57290d07b5b899e9/43369.ipynb). Thanks!"]}, {"number": 43362, "title": "distributed_dataset tensorflow.python.framework.errors_impl.UnavailableError: Socket closed", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): bin\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: TPU v3-8\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nurllib.error.URLError: <urlopen error [Errno 110] Connection timed out> in Colab,\r\ntensorflow.python.framework.errors_impl.UnavailableError: Socket closed in GCE VM\r\n\r\n**Describe the expected behavior**\r\nNot having these errors(code ran just fine a few hours ago)\r\n\r\n**Standalone code to reproduce the issue**\r\n[COLAB](https://colab.research.google.com/drive/173UJ_icUBvl-a6-ujSZHoHHLL2qj9B-3?usp=sharing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nin Colab:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/urllib/request.py\", line 1325, in do_open\r\n    encode_chunked=req.has_header('Transfer-encoding'))\r\n  File \"/usr/lib/python3.6/http/client.py\", line 1264, in request\r\n    self._send_request(method, url, body, headers, encode_chunked)\r\n  File \"/usr/lib/python3.6/http/client.py\", line 1310, in _send_request\r\n    self.endheaders(body, encode_chunked=encode_chunked)\r\n  File \"/usr/lib/python3.6/http/client.py\", line 1259, in endheaders\r\n    self._send_output(message_body, encode_chunked=encode_chunked)\r\n  File \"/usr/lib/python3.6/http/client.py\", line 1038, in _send_output\r\n    self.send(msg)\r\n  File \"/usr/lib/python3.6/http/client.py\", line 976, in send\r\n    self.connect()\r\n  File \"/usr/lib/python3.6/http/client.py\", line 948, in connect\r\n    (self.host,self.port), self.timeout, self.source_address)\r\n  File \"/usr/lib/python3.6/socket.py\", line 724, in create_connection\r\n    raise err\r\n  File \"/usr/lib/python3.6/socket.py\", line 713, in create_connection\r\n    sock.connect(sa)\r\nTimeoutError: [Errno 110] Connection timed out\r\n```\r\n\r\nin VM:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_gpt2.py\", line 77, in <module>\r\n    train()\r\n  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 829, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/usr/local/lib/python3.7/dist-packages/click/core.py\", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"train_gpt2.py\", line 64, in train\r\n    model.fit(iter(dist_dataset), graph_mode)\r\n  File \"/home/ksjcom0705_gmail_com/gpt-2-tensorflow2.0/gpt2_model.py\", line 343, in fit\r\n    (inputs, targets) = next(train_dataset)\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py\", line 649, in __next__\r\n    return self.get_next()\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py\", line 706, in get_next\r\n    global_has_value, replicas = _get_next_as_optional(self, self._strategy)\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py\", line 548, in _get_next_as_optional\r\n    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py\", line 1512, in get_next_as_list\r\n    strict=True,\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n2020-09-19 08:05:53.047063: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running\r\n a tf.function, it usually indicates some op in the graph gets an error: Socket closed\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1600502753.043380028\",\"description\":\"Error received from peer ipv4:10.82.138.50:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.\r\ncc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\r\n    return func(*args, **kwargs)\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1207, in cond\r\n    if pred:\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 984, in __bool__\r\n    return bool(self._numpy())\r\n  File \"/home/ksjcom0705_gmail_com/.local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1031, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access   \r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: Socket closed\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:```...", "comments": ["@ksjae \r\nPlease refer to this issue and let us know: #42228 #41817", "@ksjae This example is too much articulated. We need something that has a smaller surface to investigate reproduce it (copy, paste and run it). \r\nCan you share a very minimal example that we could run on colab without too much gcs or too much filesystem dependencies (e.g. dummy inputs). \r\nThanks", "Done. Example is now minimal enough for testing (I hope).\r\nAlso, I found out that on GPUs, the code gives 'Segmentation Fault' on the very same line.", "@ksjae \r\nI ran the code shared and face a different error please find the [gist here](https://colab.research.google.com/gist/Saduf2019/42d69bf831c8d69d2e3ae01d9b47e4ee/untitled.ipynb).\r\nPlease share a colab gist with the error reported in the gist.", "@Saduf2019 this Is a specific nightly issue like https://github.com/tensorflow/tensorflow/issues/43254#issuecomment-693461228", "@ksjae I ran the colab notebook in the original post with tf 2.3 and was able to reproduce the error. \r\n```\r\nUnavailableError: Socket closed\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n```\r\nWhen I set `tpu = False` on a GPU runtime the session crashes. Do you see that as well?\r\n\r\nYou can see in this similar issue here  #36136 that this error message is sometimes caused from `trying to cache a large dataset into memory` so I'm wondering if the root cause here is an OOM issue.\r\n\r\nWhen running this on GCE are you trying to use TPUs or GPUs? Also when you say \"code ran just fine a few hours ago\" does that mean you are not able to reproduce this error consistently on GCE?", "I've been consistently able to reproduce this(haven't ran this for a few days though, just used another training code).\r\n\r\nSlightly doubtful about OOMs, since I tried setting batch size from 8 and it still crashed.", "I experienced this issue today with tf 2.3.\r\n\r\n```\r\nUnavailableError: Socket closed\r\nAdditional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n:{\"created\":\"@1607389193.224325108\",\"description\":\"Error received from peer ipv4:10.77.25.2:8470\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\r\n```\r\n\r\nThere was no error with either CPU or GPU.\r\n\r\nHere is a colab that reproduces the error:\r\n\r\nhttps://colab.research.google.com/drive/1C09OUXP-7Es4KIthVA6daRcGq_bJKGe8?usp=sharing", "@CalebEverett : Above error seems to be from user code. \r\n\r\nNormally \"UnavailableError: Socket closed\" is an aftermath of an underlying error\r\n\r\n` TypeError: tensordot() got an unexpected keyword argument 'dtype' ` \r\n\r\nCould you try updating `call` function to use correct semantics for tensordot()? \r\n\r\n\r\n", "@hongjunChoi Hi, thanks for looking into this. I've taken out that dtype argument and it is still producing the same error. I must have put that in as I was trying to get it work.\r\n\r\nThe colab is updated. https://colab.research.google.com/drive/1C09OUXP-7Es4KIthVA6daRcGq_bJKGe8?usp=sharing\r\n\r\nI was trying to reproduce this example:\r\nhttps://keras.io/examples/structured_data/collaborative_filtering_movielens/\r\n\r\nI think the result of  `dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)` is a scalar and it should be a vector with a batch size dimension. I'm not sure if that was causing the socket closed error.\r\n\r\nI changed it to `tf.reduce_sum(user_vector * question_vector, axis=1, keepdims=True)` to get it working.", "Was able to replicate the issue with TF v2.5,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/7c419e9f182d90587aed40f755b0df9c/untitled266.ipynb)..Thanks !"]}, {"number": 43324, "title": "Standard Deviation calculation Pytorch vs TF, how to set unbiased", "body": "std calculation in Pytorch (with `unbiased=True`):\r\n```\r\nimport torch\r\na = torch.tensor([[3.4, 5.6, 7.8], [2, 3, 4], [0.1, 0.2, 0.3]])\r\na.std(-1, unbiased=True)\r\n\r\n>>> tensor([2.2000, 1.0000, 0.1000])\r\n```\r\nwhich is the **results I want in Tensorflow**. \r\nHowever, in TF I get:\r\n\r\n```\r\nimport tensorflow as tf\r\na = tf.constant([[3.4, 5.6, 7.8], [2, 3, 4], [0.1, 0.2, 0.3]])\r\ntf.math.reduce_std(a, axis=-1)\r\n\r\n>>> <tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.7962925 , 0.8164966 , 0.08164966], dtype=float32)>\r\n```\r\nThis is similar to when `unbiased=False` in Pytorch:\r\n\r\n```\r\nimport torch\r\na = torch.tensor([[3.4, 5.6, 7.8], [2, 3, 4], [0.1, 0.2, 0.3]])\r\na.std(-1, unbiased=False)\r\n\r\n>>> tensor([1.7963, 0.8165, 0.0816])\r\n```\r\n\r\nQuestion: is there a way to have `unbiased=True` in TF to get this `tensor([2.2000, 1.0000, 0.1000])` std result in TF?\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200917`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/bf7cbdc87aecd93814a48d6a445f1eb5/untitled377.ipynb).Thanks!", "@ravikyram It is not a bug it is at least a feature request label.\r\n\r\n@turmeric-blend Currently we don't directly support Bessel's correction in `tf.math.reduce_std`.\r\n\r\nWith `pip tf-nighlty` you could workaround the correction with this:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\na = np.array([[3.4, 5.6, 7.8], [2, 3, 4], [0.1, 0.2, 0.3]],dtype=np.float32)\r\nvar = tf.experimental.numpy.var(a,axis=-1, dtype=tf.float32, ddof=1)\r\nstd = tf.experimental.numpy.sqrt(var) # or tf.sqrt(var) \r\nprint(std)\r\n```\r\nYou can see in detail why we have this workaround on this line https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/numpy_ops/np_array_ops.py#L648\r\n\r\n/cc @akshaym ", "thanks @bhack. I'll leave this open as feature request for future implementation then."]}, {"number": 43288, "title": "edgeTPU compiler error on TF2.3 quantized model", "body": "Hi,\r\nI trained a custom ssd model base on 'ssd_mobilenet_v2_320x320_coco17_tpu-8' in tensorflow 2.3 environment.\r\n\r\nIf I converted it to .tflite quantized model by TF2.3 / TF2.4 nightly environment, and edgeTPU compiler will show \r\n\"Edge TPU Compiler version 14.1.317412892 \r\nInvalid model: /content/ssd_mobilenet_v2_320x320_coco17_tpu-8_model_quant.tflite \r\nModel could not be parsed ERROR: Didn't find op for builtin opcode 'CONV_2D' version '5' ERROR: Registration failed.\"     \r\n\r\nIf I converted it to .tflite quantized model by TF2.1 environment, and edgeTPU compiler will show \r\n\"Edge TPU Compiler version 14.1.317412892\r\nInvalid model: /content/ssd_mobilenet_v2_320x320_coco17_tpu-8_model_quant_new.tflite\r\nModel could not be parsed\r\nERROR: Did not get operators, tensors, or buffers in subgraph 0.\"     \r\n\r\n#### System information\r\nLinux kernel version:4.19.112+\r\nDistributor ID:Ubuntu\r\nDescription:\tUbuntu 18.04.5 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\nTensorflow python module version 2.3.0\r\n\r\nEdge TPU python module version:2.14.1\r\nEdge TPU Compiler version 14.1.317412892\r\nEdge TPU runtime file:\r\nii  libedgetpu1-std:amd64                  14.1                                              amd64        Support library for Edge TPU\r\nEdge TPU runtime version:\r\nBuildLabel(COMPILER=5.4.0 20160609,DATE=redacted,TIME=redacted,CL_NUMBER=317268237), RuntimeVersion(13)\r\n\r\nWhat is wrong with it actually? Please help me out~ Thank you!\r\n", "comments": []}, {"number": 43263, "title": "TF 2.3 - loading of saved_model from disk with ragged=True input is slower than ragged=False", "body": "**System information**\r\nGoogle Colab Notebook with TF 2.3\r\n\r\n**Describe the current behavior**\r\nLoading saved model with input `tf.keras.layers.Input(shape=[None], dtype=tf.int64, ragged=True)` is 5-10x slower than `tf.keras.layers.Input(shape=[None], dtype=tf.int64, ragged=False)`\r\n\r\n**Describe the expected behavior**\r\nThe above should not impact performance\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/10ZXvdbp1lf8X2LLtemcHr2rX692n2jv9?usp=sharing\r\n\r\n**Other info / logs**\r\nThe above is achieved with a custom model implementation. However, when I used the model from one of the Tensorflow tutorials I could not replicate the issue.\r\n\r\nOn top of that, we were able to replicate this issue with tensorflow serving where loading of the saved_model + first prediction was 5-10x slower than model without ragged tensors.", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/f9e28ebe24fb701b7859b6c3d56b9bdb/43263.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/89196ab681b8f0f3fe32419338e8328d/43263-tf-nightly.ipynb). \r\n\r\nNo time difference when running TensorFlow example, slows down when running a custom model implementation. Please find the attached gist. Thanks!", "@michalderdak did you have any luck on this ? we're still struggling with it in our side... thanks ", "@michalderdak, Sorry for the late response. Is this still an issue for you?\r\n\r\nCan you please try recent `TF2.6` or `tf-nightly` and let us know whether it is persisting? Thanks!", "@chunduriv Thanks for the reply! We have gone with another alternative that does not use ragged tensors, however, I will test with `TF2.6` and let you know if I can replicate the issue", "@chunduriv I've replicated the issue with `TF2.6` and `tf-nightly`. Furthermore, I have added examples without our custom loss function to have as little custom code as possible.\r\n\r\nsee the notebooks here:\r\nhttps://colab.research.google.com/drive/1FW2cVAGSL5aWuFBbDcLNbwvtseOaOYO2?usp=sharing\r\nhttps://colab.research.google.com/drive/1YNKSG21CQf3Q6mQFzrLKNO6FzB7Z6j2T?usp=sharing"]}, {"number": 43232, "title": "Didn't find op for builtin opcode 'CONV_2D' version '5'", "body": "Hi, I'm trying to run inference of a custom trained mobilenet_v2_coco17_320x320_tpu-8 model on a Raspberry pi. I have succesfully trained, saved and converted the model to tf.lite. However, when trying to run the inttepreter on a Raspberry pi 4, I'm getting the following error: ValueError: Didn't find op for builtin opcode 'CONV_2D' version '5' Registration failed.\r\nBut when running the interpreter on google colab notebook it works just fine.    \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab\r\n- TensorFlow installed from (source or binary): tf-nightly on colab. tf-2.2 on Rapsberry pi\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/My Drive/Deteccion_escaleras/models/research/Mobilenet_entrenada/saved_model',signature_keys=['serving_default'])\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('/content/drive/My Drive/training_mobilenet/model_3.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n  print(\"Model saved\")\r\n```\r\nCode use for training,evaluation,conversion:\r\nhttps://colab.research.google.com/drive/1M7SBWcA_gRe4xEww3q8QYdyblV8Ic2Te?usp=sharing\r\n\r\n\r\n**Any other info / logs**\r\n\r\nGenerated model files (saved_model.pb & model.tflite): https://drive.google.com/drive/folders/1-JHyeg268jQWyXF_AO348SNN8fgU1TCT?usp=sharing\r\n", "comments": ["The saved model is incomplete. Can you share a saved model dir that we can load directly?", "@bhack Sorry what do you mean when you say the model is incomplete? That folder attached contains a saved_model.pb file that I've loaded directly using the tf.saved_model.load() command. Did you get a problem when loading the model?\r\nHowever here is the complete directory containning the saved model folder and  the checkpoints folder:\r\nhttps://drive.google.com/drive/folders/1-JHyeg268jQWyXF_AO348SNN8fgU1TCT?usp=sharing", "Ok.. what TF version are you using on Raspberry?", "I'm running tf 2.2", "Can you try to align the exporting version and the one running on Raspberry?", "When I try to do that I run in to a problem with the exporter beacuse of the dynamic input tensor in model. ValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[1, None, None, 3]'.\r\n\r\nAnd when I try to export using concretefunc I run into another error: Exception: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_1\"): requires all operands to be either same as or ref type of results\r\nHere(https://colab.research.google.com/drive/1Uwj0Boede08W0e7gpb3evQp3RSQysqtr?usp=sharing) I attach a notebook where I downgraded TF to 2.2 and try to run an export job by the 2 different means mentioned.", "I think It Is hard to get a precomp nightly for Raspberry to quickly test the exported model cause we don't have a new link for nightly: https://github.com/tensorflow/tensorflow/issues/21469\n\nCan you try to build https://www.tensorflow.org/install/source_rpi", "@juanpbotero98 It looks like TF 2.2 version does not have the CONV_2D version '5'. You need to use a recent version of TF runtime or at least the same TF version, used for conversion in Raspberry.", "@bhack I managed to upgrade tf on my Raspberry pi to the latest available version (2.3) however i couldn't find any data for installing tf-nightly and when running the interpreter I still get the same error. ", "@abattery Thanks I will now try to convert using tf 2.3 since now i have that version running in my raspberry pi.", "If 2.3 will not convert your model (if it requires ft-nightly) you will need to build the PI with the same instructions but from the master branch.", "When trying to convert using tf 2.3 I ran into the following error:\r\n<unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n<unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {_class = [\"loc:@Func/StatefulPartitionedCall/StatefulPartitionedCall/input/_335\"], device = \"\"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>", "@bhack Could you please elaborate? what do you mean by building the Pi from the master branch?\r\n", "Yes, same instruction but from the master branch:\r\nhttps://www.tensorflow.org/install/source_rpi", "@bhack I tried to run the instruction for the master branch but after all the compiling was done I wasn't able to find the .whl file. I don't really know what went wrong. \r\nthe only thing is that when I run the commad: git checkout branch_name I get an error saying: git checkout branch name did not match any file known to git.", "<branch_name> is e.g. `master`\n\nThe wheel is in the outputdir defined at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh#L128", "@bhack I'm running the commands listed in https://www.tensorflow.org/install/source_rpi, and it seems to run without any problem however the out-artifacts directory doesn't get created. Hence no .whl file gets created either. I don't know what might be the problem.\r\n", "I found the error i was running out of memory before the .whl file could be generated. I'm going trough the buil once again using a digital ocean virtual machine.\r\n", "@juanpbotero98 You can also add some option to limit the RAM https://www.tensorflow.org/install/source#bazel_build_options", "@bhack Now I'm getting a really strange error. \r\nExecution platform: @local_execution_config_platform//:platform arm-rpi-linux-gnueabihf-gcc: internal compiler error: Killed (program cc1plus)\r\nand after that the console prints: FAILED: Build did NOT complete successfully", "@juanpbotero98 Sorry I don't have currently enought free resources to rebuild It locally.\r\nIn the meantime can I suggest you to open a ticket mentioning this one in https://github.com/tensorflow/build? That one is to request if they can publish again regularly Raspberry tf-nightly wheels.\r\nCause at some point we lost It as you can see from #21469\r\n\r\nIt think that could useful for other devs that need to tf-nightly to convert their model to be aligned at inference time on the device. \r\n\r\n", "@bhack Thanks for all the help, I really appriciate it! . I will definitely post a ticket requesting the tf-nigthly wheels for Raspberry pi. However I was wondering if in the meantime there is anything I can do, like  train the model from scratch using a certain TF version or something like that.\r\n", "I think in the meantime you have also two other actions:\r\n\r\n- open a new ticket here about the error you got building Rasberry wheel on master\r\n- try to find a TF version for the model training and conversion that works with your model but that is also compatible with an already available precompiled Raspberry wheel (<= 2.3)\r\n\r\n", "I have the same problem with tflite_runtime 2.5.0 on Raspberry pi 4. Is there any new solution?"]}, {"number": 43202, "title": "running TF operations on mobile GPU - error conversion to dynamic graph yields issues while running on mobile GPU", "body": "this happens to me as well.\r\n\r\nany progress with it?\r\n\r\n@pranshugo Your model has TF ops (converted with SELECT_TF) currently this introduces dynamic shape in the runtime. It's on our radar to fix SELECT option to avoid dynamic shape during inference.\r\n\r\n_Originally posted by @karimnosseir in https://github.com/tensorflow/tensorflow/issues/38036#issuecomment-685021311_", "comments": ["ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n", "@shlomi-amitai \r\nPlease share minimal indented code such that we could replicate your issue, and the tf version on which error is faced.", "TF version 2.2 & 2.3.\r\n\r\nCannot send the code, but can elaborate.. it happens when converting unsupported operators such as conv3d from TF to tflite.\r\nFor doing that we use :\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nthen during runtime on GPU (TF 2.3) I get the above error. \r\n(ERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.)", "@shlomi-amitai \r\nPlease refer to this issue and let us know if this helps: #42863", "Hi @shlomi-amitai\r\n\r\nThis is an expected behavior. Select TF ops option is not working with NNAPI or GPU delegate either. Team is working on fixing this issue. I will update this thread when the feature is landed.\r\n\r\nfyi, @terryheo ", "another thing is that when running this operation on the device, output tensor is wrongly extracted (index is confused). this happens only with NN contatning TF-Ops.\r\nattached dummy NNs tflite \r\n[dummyNNs.zip](https://github.com/tensorflow/tensorflow/files/5237033/dummyNNs.zip)\r\n\r\nshowing that issue \r\n@abattery ", "FYI, Flex + Hardware acceleration combination (e.g., NNAPI or GPU) is now available at tf-nightly version if the given graph has only static shapes.", "@shlomi-amitai regarding https://github.com/tensorflow/tensorflow/issues/43202#issuecomment-694026155 , I think it is a different issue so could you file another issue for that with the detailed information? I need more information anyway to understand the problem.", "> FYI, Flex + Hardware acceleration combination (e.g., NNAPI or GPU) is now available at tf-nightly version if the given graph has only static shapes.\r\n\r\nThanks! I will try", "> @shlomi-amitai regarding [#43202 (comment)](https://github.com/tensorflow/tensorflow/issues/43202#issuecomment-694026155) , I think it is a different issue so could you file another issue for that with the detailed information? I need more information anyway to understand the problem.\r\n\r\nMy Bad. issue started earlier during the conversion from Onnx"]}, {"number": 43201, "title": "micro speech example not working with STM32F769", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): f2c9a930a3f9f8dc0b7904f1d490b2665979d768\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Mbed OS (STM32F769)\r\n\r\n**Describe the problem**\r\nI'm trying to build the micro_speech application for STM32F769, following the readme file (which refers instead to STM32F746), but each iteration of the program has the same final scores for each word possible: `[64 64 64 64]`, so the `is_new_command` variable remains always false and there's no prediction.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI followed the same identical steps listed on the readme, but changing the target:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=\"CMSIS disco_f769ni\" generate_micro_speech_mbed_project`\r\n\r\n`tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed`\r\n\r\n`mbed config root .`\r\n\r\n`mbed deploy`\r\n\r\n`python -c 'import fileinput, glob;\r\nfor filename in glob.glob(\"mbed-os/tools/profiles/*.json\"):\r\n  for line in fileinput.input(filename, inplace=True):\r\n    print line.replace(\"\\\"-std=gnu++98\\\"\",\"\\\"-std=c++11\\\", \\\"-fpermissive\\\"\")'`\r\n\r\n`mbed compile -m DISCO_F769NI -t GCC_ARM`\r\n\r\nIn order to compile fine I also had to:\r\n- Modify `tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc` as explained [here](https://github.com/tensorflow/tensorflow/pull/36444/files)\r\n- Manually copy `arm_math_f16.h` inside `tensorflow\\lite\\micro\\tools\\make\\gen\\mbed_cortex-m4\\prj\\micro_speech\\mbed\\third_party\\cmsis\\CMSIS\\DSP\\Include` since it wasn't there \r\n- Add `BSP_DISCO_F769NI` and `LCD_DISCO_F769NI`\r\n\r\nThe last thing I tried was following the steps explained [here](https://github.com/tensorflow/tensorflow/issues/42918), then use the cmsis-nn flag, but with my board the proposed solution seems not to work.\r\n\r\nThanks in advance for your help and suggestions.\r\n\r\n", "comments": ["I think I just found a possible problem: there is no dedicated `audio_provider.cc` file for the STM32F769 board, and the standard one doesn't work. Can anyone confirm me that this is the cause of the the micro speech example malfunction? Also, if there were solutions to this class' implementation for my board they would be really appreciated.", "Could you try running the micro_speech_test in the same way and see if that passes or fails, with the cmsis-nn tag and the fix in #43110?\r\n\r\nSimply swap out **generate_micro_speech_mbed_project** for **generate_micro_speech_test_mbed_project** in the make-command. That should rule out if it's due to the audio_provider.cc or not.", "@jenselofsson  I tried your suggestion and the test passes, here is the result:\r\n\r\n```\r\n!b^W\u00cd \u00eb\u00cdt\"\u00d5e\u00a2\u00bd X+\u009d\u00b9\u00b5\u2022\u00b9\u00d1\u00b9 To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.\r\nRan successfully\r\n\r\n1/1 tests passed\r\n~~~ALL TESTS PASSED~~~\r\n\r\n```", "I think you're correct regarding the audio_provider.cc.\r\n\r\nI'm using the DISCO F746NG board, and I'm seeing the same result (same score in every category) when I swap out the disco_f746ng/audio_provider.cc for the default one.\r\n\r\nWhile if I use the audio provider for the disco_f746ng the scores are different for the different categories (ie. as expected).\r\n\r\nI also got the same result when using the reference kernels.", "Ok, thank you so much for the check!\r\nIt seems that the only option I have is to create my own audio provider for the DISCO F769NI.", "I tried to recreate the `audio_provider.cc` for DISCO_F769NI following the [demo ](https://os.mbed.com/teams/ST/code/DISCO-F769NI_AUDIO_demo//file/db1aee306f50/main.cpp/)on the mbed site. Here is the code I wrote:\r\n```\r\n#include \"tensorflow/lite/micro/examples/micro_speech/audio_provider.h\"\r\n#include \"mbed.h\"\r\n#include \"stm32f769i_discovery_audio.h\"\r\n#include \"stm32f769i_discovery_sdram.h\"\r\n#include \"tensorflow/lite/micro/examples/micro_speech/micro_features/micro_model_settings.h\"\r\n\r\nnamespace {\r\n\r\n  bool g_is_audio_initialized = false;\r\n  constexpr int kAudioCaptureBufferSize = kAudioSampleFrequency * 0.5;\r\n  int16_t g_audio_capture_buffer[kAudioCaptureBufferSize];\r\n  int16_t g_audio_output_buffer[kMaxAudioSampleSize];\r\n  int32_t g_latest_audio_timestamp = 0;\r\n\r\n  uint8_t audioRecorder = BSP_AUDIO_IN_Init(BSP_AUDIO_FREQUENCY_44K, DEFAULT_AUDIO_IN_BIT_RESOLUTION, 2 * DEFAULT_AUDIO_IN_CHANNEL_NBR);\r\n  uint8_t sdramState = BSP_SDRAM_Init();\r\n\r\n  typedef enum {\r\n    BUFFER_OFFSET_NONE = 0,\r\n    BUFFER_OFFSET_HALF = 1,\r\n    BUFFER_OFFSET_FULL = 2,\r\n  } BUFFER_StateTypeDef;\r\n\r\n  #define SCRATCH_BUFF_SIZE  1024\r\n  #define RECORD_BUFFER_SIZE  4096\r\n  #define AUDIO_BLOCK_SIZE ((uint32_t)2048)\r\n  #define AUDIO_BUFFER_IN SDRAM_DEVICE_ADDR /* In SDRAM */\r\n  #define AUDIO_BUFFER_OUT \\\r\n  (SDRAM_DEVICE_ADDR + (AUDIO_BLOCK_SIZE * 2)) /* In SDRAM */ \r\n\r\n  volatile uint32_t audio_rec_buffer_state = BUFFER_OFFSET_NONE;\r\n\r\n  int32_t Scratch[SCRATCH_BUFF_SIZE];\r\n \r\n  /* Buffer containing the PCM samples coming from the microphone */\r\n  int16_t RecordBuffer[RECORD_BUFFER_SIZE];\r\n  \r\n  /* Buffer used to stream the recorded PCM samples towards the audio codec. */\r\n  int16_t PlaybackBuffer[RECORD_BUFFER_SIZE];\r\n\r\n  uint8_t SetSysClock_PLL_HSE_200MHz() {\r\n  RCC_ClkInitTypeDef RCC_ClkInitStruct;\r\n  RCC_OscInitTypeDef RCC_OscInitStruct;\r\n\r\n  // Enable power clock\r\n  __PWR_CLK_ENABLE();\r\n\r\n  // Enable HSE oscillator and activate PLL with HSE as source\r\n  RCC_OscInitStruct.OscillatorType = RCC_OSCILLATORTYPE_HSE;\r\n  RCC_OscInitStruct.HSEState = RCC_HSE_ON; /* External xtal on OSC_IN/OSC_OUT */\r\n\r\n  // Warning: this configuration is for a 25 MHz xtal clock only\r\n  RCC_OscInitStruct.PLL.PLLState = RCC_PLL_ON;\r\n  RCC_OscInitStruct.PLL.PLLSource = RCC_PLLSOURCE_HSE;\r\n  RCC_OscInitStruct.PLL.PLLM = 25;   // VCO input clock = 1 MHz (25 MHz / 25)\r\n  RCC_OscInitStruct.PLL.PLLN = 400;  // VCO output clock = 400 MHz (1 MHz * 400)\r\n  RCC_OscInitStruct.PLL.PLLP = RCC_PLLP_DIV2;  // PLLCLK = 200 MHz (400 MHz / 2)\r\n  RCC_OscInitStruct.PLL.PLLQ = 8;  // USB clock = 50 MHz (400 MHz / 8)\r\n\r\n  if (HAL_RCC_OscConfig(&RCC_OscInitStruct) != HAL_OK) {\r\n    return 0;  // FAIL\r\n  }\r\n\r\n  // Activate the OverDrive to reach the 216 MHz Frequency\r\n  if (HAL_PWREx_EnableOverDrive() != HAL_OK) {\r\n    return 0;  // FAIL\r\n  }\r\n\r\n  // Select PLL as system clock source and configure the HCLK, PCLK1 and PCLK2\r\n  // clocks dividers\r\n  RCC_ClkInitStruct.ClockType = (RCC_CLOCKTYPE_SYSCLK | RCC_CLOCKTYPE_HCLK |\r\n                                 RCC_CLOCKTYPE_PCLK1 | RCC_CLOCKTYPE_PCLK2);\r\n  RCC_ClkInitStruct.SYSCLKSource = RCC_SYSCLKSOURCE_PLLCLK;  // 200 MHz\r\n  RCC_ClkInitStruct.AHBCLKDivider = RCC_SYSCLK_DIV1;         // 200 MHz\r\n  RCC_ClkInitStruct.APB1CLKDivider = RCC_HCLK_DIV4;          //  50 MHz\r\n  RCC_ClkInitStruct.APB2CLKDivider = RCC_HCLK_DIV2;          // 100 MHz\r\n\r\n  if (HAL_RCC_ClockConfig(&RCC_ClkInitStruct, FLASH_LATENCY_7) != HAL_OK) {\r\n    return 0;  // FAIL\r\n  }\r\n  HAL_RCC_MCOConfig(RCC_MCO1, RCC_MCO1SOURCE_HSE, RCC_MCODIV_4);\r\n  return 1;  // OK\r\n}\r\n\r\n  TfLiteStatus InitAudioRecording(tflite::ErrorReporter* error_reporter) {\r\n    SetSysClock_PLL_HSE_200MHz();\r\n\r\n    /* Initialize Audio Recorder with 4 channels to be used */\r\n    if (audioRecorder == AUDIO_ERROR) {\r\n        printf(\"BSP_AUDIO_IN_Init error\\n\");\r\n    }\r\n \r\n    /* Allocate scratch buffers */\r\n    if (BSP_AUDIO_IN_AllocScratch (Scratch, SCRATCH_BUFF_SIZE) == AUDIO_ERROR) {\r\n        printf(\"BSP_AUDIO_IN_AllocScratch error\\n\");\r\n    }\r\n\r\n    // // Initialize SDRAM buffers.\r\n    // memset((uint16_t*)AUDIO_BUFFER_IN, 0, AUDIO_BLOCK_SIZE * 2);\r\n    // memset((uint16_t*)AUDIO_BUFFER_OUT, 0, AUDIO_BLOCK_SIZE * 2);\r\n    // audio_rec_buffer_state = BUFFER_OFFSET_NONE;\r\n\r\n    /* Start Recording */\r\n    if (BSP_AUDIO_IN_Record((uint16_t*)&RecordBuffer[0], RECORD_BUFFER_SIZE) == AUDIO_ERROR) {\r\n        printf(\"BSP_AUDIO_IN_Record error\\n\");\r\n    }\r\n    uint8_t ChannelNumber = BSP_AUDIO_IN_GetChannelNumber();\r\n \r\n    audio_rec_buffer_state = BUFFER_OFFSET_NONE;\r\n\r\n    return kTfLiteOk;\r\n  }\r\n\r\n  void CaptureSamples(const int16_t* sample_data) {\r\n    const int sample_size = AUDIO_BLOCK_SIZE / (sizeof(int16_t) * 2);\r\n    const int32_t time_in_ms =\r\n      g_latest_audio_timestamp + (sample_size / (kAudioSampleFrequency / 1000));\r\n    const int32_t start_sample_offset =\r\n      g_latest_audio_timestamp * (kAudioSampleFrequency / 1000);\r\n\r\n    for (int i = 0; i < sample_size; ++i) {\r\n    const int capture_index =\r\n        (start_sample_offset + i) % kAudioCaptureBufferSize;\r\n    g_audio_capture_buffer[capture_index] =\r\n        (sample_data[(i * 2) + 0] / 2) + (sample_data[(i * 2) + 1] / 2);\r\n    }\r\n    // This is how we let the outside world know that new audio data has arrived.\r\n    g_latest_audio_timestamp = time_in_ms;\r\n  }\r\n} // namespace\r\n\r\n// These callbacks need to be linkable symbols, because they override weak\r\n// default versions.\r\nvoid BSP_AUDIO_IN_TransferComplete_CallBack(void) {\r\n  audio_rec_buffer_state = BUFFER_OFFSET_FULL;\r\n  /* Copy recorded 1st half block */\r\n  memcpy((uint16_t*)(AUDIO_BUFFER_OUT), (uint16_t*)(AUDIO_BUFFER_IN),\r\n         AUDIO_BLOCK_SIZE);\r\n  CaptureSamples(reinterpret_cast<int16_t*>(AUDIO_BUFFER_IN));\r\n  return;\r\n}\r\n\r\n// Another weak symbol override.\r\nvoid BSP_AUDIO_IN_HalfTransfer_CallBack(void) {\r\n  audio_rec_buffer_state = BUFFER_OFFSET_HALF;\r\n  /* Copy recorded 2nd half block */\r\n  memcpy((uint16_t*)(AUDIO_BUFFER_OUT + (AUDIO_BLOCK_SIZE)),\r\n         (uint16_t*)(AUDIO_BUFFER_IN + (AUDIO_BLOCK_SIZE)), AUDIO_BLOCK_SIZE);\r\n  CaptureSamples(\r\n      reinterpret_cast<int16_t*>(AUDIO_BUFFER_IN + AUDIO_BLOCK_SIZE));\r\n  return;\r\n}\r\n\r\n// Main entry point for getting audio data.\r\nTfLiteStatus GetAudioSamples(tflite::ErrorReporter* error_reporter,\r\n                             int start_ms, int duration_ms,\r\n                             int* audio_samples_size, int16_t** audio_samples) {\r\n  \r\n  if (!g_is_audio_initialized) {\r\n    TfLiteStatus init_status = InitAudioRecording(error_reporter);\r\n    if (init_status != kTfLiteOk) {\r\n      return init_status;\r\n    }\r\n    g_is_audio_initialized = true;\r\n  }\r\n  // This should only be called when the main thread notices that the latest\r\n  // audio sample data timestamp has changed, so that there's new data in the\r\n  // capture ring buffer. The ring buffer will eventually wrap around and\r\n  // overwrite the data, but the assumption is that the main thread is checking\r\n  // often enough and the buffer is large enough that this call will be made\r\n  // before that happens.\r\n  const int start_offset = start_ms * (kAudioSampleFrequency / 1000);\r\n  const int duration_sample_count =\r\n      duration_ms * (kAudioSampleFrequency / 1000);\r\n  for (int i = 0; i < duration_sample_count; ++i) {\r\n    const int capture_index = (start_offset + i) % kAudioCaptureBufferSize;\r\n    g_audio_output_buffer[i] = g_audio_capture_buffer[capture_index];\r\n  }\r\n\r\n  *audio_samples_size = kMaxAudioSampleSize;\r\n  *audio_samples = g_audio_output_buffer;\r\n  return kTfLiteOk;\r\n}\r\n\r\nint32_t LatestAudioTimestamp() { return g_latest_audio_timestamp; }\r\n```\r\nRegarding this, the SDRAM buffers initialization and the use of `SetSysClock_PLL_HSE_200MHz()` inside of the \r\n `InitAudioRecording` function, seem not to alterate the final result (I copied them from the DISCO F746NG's `audio_provider.cc`).\r\nI tried to develope the project both with cmsis and cmsis-nn tags, but apparently the issue is not solved. The strange thing is that if I build in release mode, the final scores for each word are `[64, 64, 64, 64]`, but, if I choose the normal mode the command recognizer returns at line 88 of `recognize_commands.cc` (ie. \"If there are too few results, assume the result will be unreliable and bail.\") because the if statement at line 83 is satisfied. Obviously, in both cases, `is_new_command` stays false, so there's no prediction.\r\nIf anyone could help me solving this issue it would be really appreciated. Thanks in advance."]}, {"number": 43146, "title": "[EfficientNet] Understanding architectural implementation decision for the Squeeze and Excitation phase", "body": "The following question refers to \"Squeeze and Excitation\" block from the EfficientNet implementation (https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/applications/efficientnet.py#L517-L540), lines 478 - 498.\r\n\r\nConcretely, \"Squeeze and Excite\" current implementation makes use of two Conv2D blocks: one for filters reduction at line 483 and the other one for filters expanding at line 491.\r\n\r\nGiving the fact that the input in this two Conv2D layers is just an array of features reshaped into a 3D tensor (of shape (1, 1, nr_features) according to line 482), the operations that this layers are doing is just a standard one-shot matrix multiplication followed by and activation (as in exactly what a Dense layer does by default).\r\n\r\nTherefore, my question is why the implementation uses a Conv2D instead of a regular Dense layer? Is it because of easier code understanding/maintenance? I guess the reason could not be speed execution since both Dense and Conv2D make use of optimized matrix multiplication.\r\n\r\n_Thanks, would really appreciate if this question can be answered as I can't find any docs that address this question._", "comments": []}, {"number": 43124, "title": "Generating ICU normalization_data.c and icu_conversion_data.c on s390x architecture", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): bazel 2.0.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nICU data provided [here ](https://github.com/tensorflow/tensorflow/tree/master/third_party/icu/data) appears to be generated on x86 platform.  I am trying to generate this data on s390x but running into some issues.\r\n\r\nFirstly, filters.json as per [instructions ](https://github.com/tensorflow/tensorflow/blob/master/third_party/icu/data/BUILD.bazel) is not compatible with `icu` `release-64-2`. It seems that TF 2.2.0 expects `release-64-2`.  I can make it work with `release-65-1` but unsure of generating `icu_conversion_data.c.gz.*`   I could see that `conversion_data.c` is generated by concatenating `*.gz` unzipped contents.  Can someone please provide some pointers here on how to generate the `*.gz` files?\r\n\r\nSecondly, [instructions ](https://github.com/tensorflow/text/blob/master/third_party/icu/data/BUILD) on generating `normalization_data.c`  are almost identical to the `icu_conversion_data.c`.  Are these two same files or am I missing something?\r\n\r\nThanks again for your time.", "comments": ["Please redirect to owners of the ICU code in TF.", "@hamatake Please take a look at this issue. Thanks!", "@rposts,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar I checked TF 2.4.1 code and the problem persists on s390x.\r\n\r\n[Bazel](https://github.com/tensorflow/tensorflow/blob/v2.4.1/third_party/icu/data/BUILD.bazel) build file seem to indicate that data is pre-processed for little-endian platforms and I have little reason to think it is fixed in latest TF release on s390x (BE) arch. \r\n\r\nLet me know whether this is a limitation on big-endian platforms or we could publish BE `icu_conversion_data` as well.  If latter then I still need clarifications on the earlier two questions raised on filters.json and generating `normalization_data.c`.\r\n\r\nThanks.", "@rposts Could you please try on the latest stable TF v2.6.0 and let us know if the issue still persists ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sushreebarsa - just checked - data is still x86 specific."]}, {"number": 43121, "title": "[TFLu] int8 ops slower than f32", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- Tensorflow version (commit SHA if source): 2.4.0-dev20200908\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Cortex M4f\r\n\r\n**Describe the problem**\r\nI compared the time spent by `MicroInterpreter::Invoke()` to perform different ops on the same model with with int8 quantization and without. I also tried the CMSIS-NN kernels for some of the ops. The problem is that besides the fully connected op, every other op is the same or slower with the int8 ops. \r\n\r\nHere is a table showing the average time in ticks spent by each op's `Eval()`. The first column shows model with int8 quantization using cmsis-nn kernels for mul, add, fullyconnected. The second column uses the reference kernels and third is floating point.\r\n\r\n\u00a0 | q7cmsis | q7 ref | f32\r\n-- | -- | -- | --\r\nfullyconnected | 5990 | 8611 | 6639\r\ntanh | 15114 | 15122 | 1101\r\nadd | 2686 | 2887 | 971\r\nmul | 2202 | 1834 | 1261\r\nsub | 3301 | 3299 | 1380\r\nsplit_v | 898 | 915 | 836\r\nsplit | 794 | 817 | 814\r\nreshape | 441 | 443 | 918\r\n\r\nThe tanh kernel performs the worst with 13x slower than the floating point equivalent. Is this expected or known behavior?\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI have attached the models that I used for profiling. \r\n[profiling_models.zip](https://github.com/tensorflow/tensorflow/files/5204489/profiling_models.zip)\r\n\r\n\r\n", "comments": ["We have optimized for neon on arm, but for micro, unfortunately those simd instructions are not available.\r\n\r\nHi Pete, do you have any suggestions?\r\n\r\nThanks", "@renjie-liu What's about X86 case? Did these int8 ops optimized?", "The Cortex M4f has an FPU so this shouldn't be too surprising.\r\nOn a platform without an FPU the quantized ops should be considerably faster.\r\n\r\n The split operations and reshape are basically memcpy ops so they probably take similar amounts of cycles, but for the mathematical ops consider that while 'sub' for example might take one cycle for fp32 (assuming you have an FPU) when using int8 you have scaling, zero point etc to handle.", "@yair-ehrenwald I am not sure all of it can be attributed to the presence of an FPU. For instance, the MUL CMSIS-NN implementation is slower than the reference op even though the CMSIS-NN kernel uses SIMD instructions."]}, {"number": 43099, "title": "Allow symmetric TFLite quantization (no zero point/scale only) ", "body": "As far as I know, TFLite's quantization forces activations to have both scales and zero points. However, for some networks, symmetric quantization (no zero point) does not cause a significant loss in accuracy. It is therefore sufficient to use scale only. Please add support for symmetric quantization. ", "comments": []}, {"number": 43083, "title": "tf.data.experimental.make_csv_dataset, currently only supports one label; However, most datasets can have more than one label", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\ntest_dataset = tf.data.experimental.make_csv_dataset(\r\n    test_fp,\r\n    batch_size,\r\n    column_names=column_names,\r\n    label_name='species', M=<-- Currenly can only pass one label\r\n    num_epochs=1,\r\n    shuffle=False)\r\n\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.data.experimental.make_csv_dataset only accepts one label, to create the dataset. \r\n\r\n**Will this change the current api? How?**\r\nAPI, needs to accept list of strings as label_name or a string containing delimiters for each column name as label_name\r\n\r\n**Who will benefit with this feature?**\r\nThe community will benefit, by being able to generate more than one output label to the dataset, Otherwise, the API is useless for more than one output network\r\n**Any Other info.**\r\nSee issue #43074 for sample test case.\r\n", "comments": ["The same feature was also requested at https://stackoverflow.com/questions/60713541/how-to-read-multiple-columns-as-labels-using-make-csv-dataset-in-tensorflow-2", "Above Stackoverflow proposed solution, referenced  beats the purpose why tf.data is being used, i.e. for tensorflow to handle the distributive data pre processing. If we use pd frame (in memory data from file), then distributive processing from csv files is lost, you are stuck with one node to process....", "Supporting multiple labels seems reasonable to me. In the short term, you can achieve the same thing by re-organizing the data with a `map`. This just rearranges the Tensors, it doesn't require any copying or any expensive operations.\r\n\r\n```\r\nlabel_cols = ['label1', 'label2']\r\nfeature_cols = ['col1', 'col2']\r\ntest_dataset = tf.data.experimental.make_csv_dataset(\r\n    test_fp,\r\n    batch_size,\r\n    column_names=label_cols+feature_cols\r\n    num_epochs=1,\r\n    shuffle=False)\r\n\r\ndef make_features_labels_tuple(x):\r\n  labels = {k: x[k] for k in x if k in label_cols}\r\n  features = {k: x[k] for k in x if k in feature_cols}\r\n  return (features, labels)\r\n\r\ntest_dataset = test_dataset.map(lambda x: make_features_labels_tuple(x))\r\n```", "Related: https://github.com/tensorflow/tensorflow/issues/51571"]}, {"number": 43073, "title": "interperter with GPU Delegate blocks Android from rendering on Main thread.", "body": "\r\n**System information**\r\n- Have I written custom code: Yes.\r\n- OS Platform and Distribution: Android 27, 28, 29, and more\r\n- Mobile device Samsong Galaxy S10, A30, J6, Pixel 3, 4a, ....\r\n- TensorFlow imported as a gradle dependancy. \r\n- TensorFlow version 2.2.0 / 2.3.0\r\n\r\n\r\n**Describe the current behavior**\r\nI've created an interperter, and ran it without the `GpuDelegate`, everyting works fine.\r\nboth creation and `interperter.run` are running **not** on the main thread, but on another thread.\r\nthis thread has it's own GL context, which is **not** the same as the main thread.\r\n\r\nWhen i add the `GpuDelegate` to the interperter options, aside from the model running much faster,\r\nit blocks me from rendering animations in the main thread. \r\nthis happens with Lottie animations, View animations, and simple android views like `ProgressBar`.\r\nall the animations freeze while the interperter is running.\r\n\r\nthe included image shows the main thread and the tread running the interperter.\r\nthe tiny stack in the left in the main therad, is a Lottie animation view render call, and the main thread is stuck in `syncAndDrawFrame` until the interperter is finished (which in this case is  ~200ms on a flagship device, which is super fast for this interperter, but an eternity for the main thread.)\r\n![202009092340696141122502934](https://user-images.githubusercontent.com/37263856/92591818-4045d500-f2a7-11ea-8f52-53da69a74f5a.jpg)\r\n\r\nI assume the `GpuDelegate` consumes all the GPU resources for my interperter.\r\n\r\n**Describe the expected behavior**\r\nthe interperter invocation should not interfere with other threads, or at least other GL context, and should not block the main thread.", "comments": ["Can you take a look at https://github.com/tensorflow/tensorflow/issues/25657 about  GL context handling?", "AFAIK, there are 3 issues there:\r\n* blocking the calling thread (when it is not the same as the one creating the interpreter)\r\n* overheating \r\n* frame rate dropping *after* the inrefence.\r\n\r\nhere, i have a different thread (and GL context), that creates and runs the interpreter,\r\nand the Delegate blocks the *main* thread.\r\ni'm pretty sure it has something to do with the GL Context, but i didn't find something usefull there. ", "HI @lrdxgm, can you take a look?", "I'm not really familiar with OpenGL contexts in multithreaded environment, so I'm no good if the issue is related to that.\r\n\r\nAre you using OpenGL backend? (The alternative is the OpenCL backend.) Is the data source for the inference an OpenGL object (e.g. a texture)? If the answer to both questions is no, you can try building TFLite and the GPU delegate with -DCL_DELEGATE_NO_GL to disable the OpenGL ES integration. This compile flag was introduced to target desktop Linux, but it should also work on Android.\r\n\r\nAlso do note that some GPUs are not capable doing compute and graphics concurrently; As far as I understand this includes Qualcomm Adrenos, so long-running kernels will affect graphics. Make sure threads running GPU inference are lower priority than threads doing graphics, so context switches are possible.", "we just imported the regular artifact using gradle, so i assume it's GL backend. `org.tensorflow:tensorflow-lite-gpu:2.2.0`\r\nthe data is not an openGL object, but a direct buffer allocated in JAVA.\r\n\r\nchanging proirities of threads does not have any effect. \r\nmost of our devices have Adernos GPU, but i'l check if it happens also on Mali.", "Hello, I have a similar issue, and I'm wondering if we can get any more clarity on how the GpuDelegate works. \r\n\r\nI'm currently trying a fairly heavyweight feature extraction model with GpuDelegate from tf lite `2.4.0`. I have a simple CameraX setup that analyzes images async, so I can see how it performs over repetitions. On my Samsung S10+, the model takes about 300ms with the delegate, but the entire OS graphics layer will jank very badly. I turned on their GpuWatch feature in developer settings to see Gpu usage, and running it in this fashion will lead to 100% Gpu spikes and slow down the entire OS graphical output. I tried setting low thread priority on the analyzer thread that runs the delegate as well, which didn't seem to do anything. Alternatively, running on CPU instead and setting 4 threads leads to about 80% CPU utilization at highest, albeit much slower inference times.\r\n\r\nI'm wondering then, will using GpuDelegate with a long running model starve the OS graphics layer of frame updates or is there maybe a way to de-prioritize or batch the model's GPU computations? I had been hoping there's some built in mechanism of some sort that the system can prefer frame updates over heavy computations like this.", "@noamfreeman It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43073\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43073\">No</a>\n", "I am using the latest TF 2.5 and the problem still exists. @sushreebarsa could you re-open this and check the issue, thanks. ", "@piisku78 I believe this is at least partly due to how at least some GPU work as @lrdxgm alluded to as well. Taking a look this [Adreno GPU doc](https://developer.qualcomm.com/qfile/33472/80-nb295-11_a.pdf) (one of the most common GPUs) in section 3.4 it says that long-running GPU compute kernels can block the graphics pipeline, so they recommend making sure the work is in the 10s of milliseconds. We similarly had an issue with a model that ran for 150ms or longer depending on the device and it was totally unusable on GPU because of the jank that it caused on the entire OS UI. I get the sense that using GPU is unviable for long-running models at this time.\r\n\r\nNot entirely sure what TF Lite could do then, but maybe it's possible work could be batched for GPU, but that might get pretty complicated. That doc is also for OpenCL, so not sure if using OpenGL would make any difference in how the work is distributed on GPU.", "@noamfreeman @piisku78 Reopening this issue as the problem still persists as per the [comment](https://github.com/tensorflow/tensorflow/issues/43073#issuecomment-910000033).Thanks!"]}]