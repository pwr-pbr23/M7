[{"number": 45800, "title": "[INTEL MKL] Fix for 'Horovod>0.19.5' installation", "body": "`Horovod` install was recently broken on main branch and `make/cmake` are now required to install the package.\r\nOtherwise the following error happens during install:\r\n\r\n```\r\nStep 22/24 : RUN ${PYTHON} -m pip install --no-cache-dir horovod${HOROVOD_VERSION:+==${HOROVOD_VERSION}}\r\n ---> Running in 33b945e5cbb9\r\nCollecting horovod==0.21.0\r\n  Downloading horovod-0.21.0.tar.gz (3.2 MB)\r\nCollecting cloudpickle\r\n  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\r\nCollecting dataclasses\r\n  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\r\nCollecting psutil\r\n  Downloading psutil-5.7.3.tar.gz (465 kB)\r\nCollecting pyyaml\r\n  Downloading PyYAML-5.3.1.tar.gz (269 kB)\r\nBuilding wheels for collected packages: horovod, psutil, pyyaml\r\n  Building wheel for horovod (setup.py): started\r\n  Building wheel for horovod (setup.py): finished with status 'error'\r\n  Running setup.py clean for horovod\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/local/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-m3fizszw/horovod_5339227cd66141c2a05dc6b44fc7dd16/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-m3fizszw/horovod_5339227cd66141c2a05dc6b44fc7dd16/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-va5ikcqf\r\n       cwd: /tmp/pip-install-m3fizszw/horovod_5339227cd66141c2a05dc6b44fc7dd16/\r\n  Complete output (211 lines):\r\n  WARNING: Skipping page https://download.pytorch.org/whl/torch_stable.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  WARNING: Skipping page https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  WARNING: Skipping page https://download.pytorch.org/whl/torch_stable.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  WARNING: Skipping page https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  WARNING: Skipping page https://download.pytorch.org/whl/torch_stable.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  WARNING: Skipping page https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  WARNING: Skipping page https://download.pytorch.org/whl/torch_stable.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  WARNING: Skipping page https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html because the GET request got Content-Type: binary/octet-stream.The only supported Content-Type is text/html\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  creating build/lib.linux-x86_64-3.6/horovod\r\n  copying horovod/__init__.py -> build/lib.linux-x86_64-3.6/horovod\r\n  creating build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/compression.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/optimizer.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/functions.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  copying horovod/torch/sync_batch_norm.py -> build/lib.linux-x86_64-3.6/horovod/torch\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark\r\n  copying horovod/spark/conf.py -> build/lib.linux-x86_64-3.6/horovod/spark\r\n  copying horovod/spark/gloo_run.py -> build/lib.linux-x86_64-3.6/horovod/spark\r\n  copying horovod/spark/mpi_run.py -> build/lib.linux-x86_64-3.6/horovod/spark\r\n  copying horovod/spark/runner.py -> build/lib.linux-x86_64-3.6/horovod/spark\r\n  copying horovod/spark/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark\r\n  creating build/lib.linux-x86_64-3.6/horovod/ray\r\n  copying horovod/ray/elastic.py -> build/lib.linux-x86_64-3.6/horovod/ray\r\n  copying horovod/ray/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/ray\r\n  copying horovod/ray/runner.py -> build/lib.linux-x86_64-3.6/horovod/ray\r\n  copying horovod/ray/__init__.py -> build/lib.linux-x86_64-3.6/horovod/ray\r\n  creating build/lib.linux-x86_64-3.6/horovod/keras\r\n  copying horovod/keras/elastic.py -> build/lib.linux-x86_64-3.6/horovod/keras\r\n  copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/keras\r\n  copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/keras\r\n  creating build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/elastic.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/basics.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/exceptions.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  copying horovod/common/util.py -> build/lib.linux-x86_64-3.6/horovod/common\r\n  creating build/lib.linux-x86_64-3.6/horovod/mxnet\r\n  copying horovod/mxnet/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\r\n  copying horovod/mxnet/__init__.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\r\n  copying horovod/mxnet/functions.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner\r\n  copying horovod/runner/js_run.py -> build/lib.linux-x86_64-3.6/horovod/runner\r\n  copying horovod/runner/run_task.py -> build/lib.linux-x86_64-3.6/horovod/runner\r\n  copying horovod/runner/gloo_run.py -> build/lib.linux-x86_64-3.6/horovod/runner\r\n  copying horovod/runner/mpi_run.py -> build/lib.linux-x86_64-3.6/horovod/runner\r\n  copying horovod/runner/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner\r\n  copying horovod/runner/launch.py -> build/lib.linux-x86_64-3.6/horovod/runner\r\n  copying horovod/runner/task_fn.py -> build/lib.linux-x86_64-3.6/horovod/runner\r\n  creating build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/compression.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/elastic.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/gradient_aggregation.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/gradient_aggregation_eager.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/functions.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/sync_batch_norm.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  copying horovod/tensorflow/util.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\r\n  creating build/lib.linux-x86_64-3.6/horovod/_keras\r\n  copying horovod/_keras/elastic.py -> build/lib.linux-x86_64-3.6/horovod/_keras\r\n  copying horovod/_keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/_keras\r\n  copying horovod/_keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/_keras\r\n  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\r\n  copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\r\n  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\r\n  copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\r\n  creating build/lib.linux-x86_64-3.6/horovod/torch/elastic\r\n  copying horovod/torch/elastic/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/elastic\r\n  copying horovod/torch/elastic/state.py -> build/lib.linux-x86_64-3.6/horovod/torch/elastic\r\n  copying horovod/torch/elastic/sampler.py -> build/lib.linux-x86_64-3.6/horovod/torch/elastic\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/remote.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  copying horovod/spark/torch/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/gloo_exec_fn.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/mpirun_exec_fn.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  copying horovod/spark/task/task_info.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/optimizer.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/bare.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/tensorflow.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/remote.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  copying horovod/spark/keras/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/_namedtuple_fix.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/store.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/params.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/backend.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/constants.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/cache.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/serialization.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  copying horovod/spark/common/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/common\r\n  creating build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/host_discovery.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/rsh.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/job_id.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/mpirun_rsh.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  copying horovod/spark/driver/rendezvous.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/task\r\n  copying horovod/runner/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/runner/task\r\n  copying horovod/runner/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/task\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/http\r\n  copying horovod/runner/http/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/http\r\n  copying horovod/runner/http/http_server.py -> build/lib.linux-x86_64-3.6/horovod/runner/http\r\n  copying horovod/runner/http/http_client.py -> build/lib.linux-x86_64-3.6/horovod/runner/http\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/util\r\n  copying horovod/runner/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/util\r\n  copying horovod/runner/util/threads.py -> build/lib.linux-x86_64-3.6/horovod/runner/util\r\n  copying horovod/runner/util/network.py -> build/lib.linux-x86_64-3.6/horovod/runner/util\r\n  copying horovod/runner/util/cache.py -> build/lib.linux-x86_64-3.6/horovod/runner/util\r\n  copying horovod/runner/util/remote.py -> build/lib.linux-x86_64-3.6/horovod/runner/util\r\n  copying horovod/runner/util/lsf.py -> build/lib.linux-x86_64-3.6/horovod/runner/util\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/common\r\n  copying horovod/runner/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/common\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/driver\r\n  copying horovod/runner/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/runner/driver\r\n  copying horovod/runner/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/driver\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/settings.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/driver.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/discovery.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/worker.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/rendezvous.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/constants.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  copying horovod/runner/elastic/registration.py -> build/lib.linux-x86_64-3.6/horovod/runner/elastic\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/settings.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/host_hash.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/codec.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/timeout.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/safe_shell_exec.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/tiny_shell_exec.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/hosts.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/network.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/env.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/secret.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  copying horovod/runner/common/util/config_parser.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/util\r\n  creating build/lib.linux-x86_64-3.6/horovod/runner/common/service\r\n  copying horovod/runner/common/service/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/service\r\n  copying horovod/runner/common/service/task_service.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/service\r\n  copying horovod/runner/common/service/__init__.py -> build/lib.linux-x86_64-3.6/horovod/runner/common/service\r\n  creating build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\r\n  copying horovod/tensorflow/keras/elastic.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\r\n  copying horovod/tensorflow/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\r\n  copying horovod/tensorflow/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\r\n  running build_ext\r\n  Traceback (most recent call last):\r\n    File \"/tmp/pip-install-m3fizszw/horovod_5339227cd66141c2a05dc6b44fc7dd16/setup.py\", line 89, in build_extensions\r\n      cwd=self.build_temp)\r\n    File \"/usr/lib64/python3.6/subprocess.py\", line 306, in check_call\r\n      retcode = call(*popenargs, **kwargs)\r\n    File \"/usr/lib64/python3.6/subprocess.py\", line 287, in call\r\n      with Popen(*popenargs, **kwargs) as p:\r\n    File \"/usr/lib64/python3.6/subprocess.py\", line 729, in __init__\r\n      restore_signals, start_new_session)\r\n    File \"/usr/lib64/python3.6/subprocess.py\", line 1364, in _execute_child\r\n      raise child_exception_type(errno_num, err_msg, err_filename)\r\n  FileNotFoundError: [Errno 2] No such file or directory: 'cmake': 'cmake'\r\n  \r\n  During handling of the above exception, another exception occurred:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"<string>\", line 1, in <module>\r\n    File \"/tmp/pip-install-m3fizszw/horovod_5339227cd66141c2a05dc6b44fc7dd16/setup.py\", line 193, in <module>\r\n      'horovodrun = horovod.runner.launch:run_commandline'\r\n    File \"/usr/local/lib/python3.6/site-packages/setuptools/__init__.py\", line 153, in setup\r\n      return distutils.core.setup(**attrs)\r\n    File \"/usr/lib64/python3.6/distutils/core.py\", line 148, in setup\r\n      dist.run_commands()\r\n    File \"/usr/lib64/python3.6/distutils/dist.py\", line 955, in run_commands\r\n      self.run_command(cmd)\r\n    File \"/usr/lib64/python3.6/distutils/dist.py\", line 974, in run_command\r\n      cmd_obj.run()\r\n    File \"/usr/local/lib/python3.6/site-packages/wheel/bdist_wheel.py\", line 299, in run\r\n      self.run_command('build')\r\n    File \"/usr/lib64/python3.6/distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/usr/lib64/python3.6/distutils/dist.py\", line 974, in run_command\r\n      cmd_obj.run()\r\n    File \"/usr/lib64/python3.6/distutils/command/build.py\", line 135, in run\r\n      self.run_command(cmd_name)\r\n    File \"/usr/lib64/python3.6/distutils/cmd.py\", line 313, in run_command\r\n      self.distribution.run_command(command)\r\n    File \"/usr/lib64/python3.6/distutils/dist.py\", line 974, in run_command\r\n      cmd_obj.run()\r\n    File \"/usr/local/lib/python3.6/site-packages/setuptools/command/build_ext.py\", line 79, in run\r\n      _build_ext.run(self)\r\n    File \"/usr/lib64/python3.6/distutils/command/build_ext.py\", line 339, in run\r\n      self.build_extensions()\r\n    File \"/tmp/pip-install-m3fizszw/horovod_5339227cd66141c2a05dc6b44fc7dd16/setup.py\", line 93, in build_extensions\r\n      raise RuntimeError('CMake failed: {}'.format(str(e)))\r\n  RuntimeError: CMake failed: [Errno 2] No such file or directory: 'cmake': 'cmake'\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for horovod\r\n```", "comments": ["@angerson we also need some parts of this hotfix on [r2.4](https://github.com/tensorflow/tensorflow/tree/r2.4) as well.\r\nSo I created a cherry-pick here: #45806 \r\n", "`PowerTools` repo name is also subject to licensing issue, so that's fixed in this PR as well.", "/assign @angerson "]}, {"number": 45799, "title": "Update the external license check to be close to the internal check.", "body": "Manually updated the licenses in the files that are not excluded from a license check in the internal CI system, and removed these exclusions in the external CI system as well.\r\n\r\nThe remaining external exclusions that are not excluded internally are:\r\n * .bzl\r\n * .h5\r\n * .tpl\r\n * .properties\r\n\r\nWill will fix these remaining discrepancies incrementally, as and when a PR touches such a file and runs afoul of the internal checks.\r\n\r\nSee http://b/175315163#comment7 for more details.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45798, "title": "Cleanup `tensorflow/c/experimental/gradients/` part 2", "body": "/cc @saxenasaurabh ", "comments": ["@saxenasaurabh could you take a look at this PR? Thank you !", "@saxenasaurabh Could you take a look ? Thank you !", "@saxenasaurabh \r\n\r\n> could we simplify `CompareNumericalAndAutodiffGradients` to only take the forward model fn?\r\n\r\nIt is harder than I thought. I could not find a way to register the gradients function. What do you think ?\r\n\r\n"]}, {"number": 45797, "title": "saved_model does not support RaggedTensors", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ninp = layers.Input(shape=(None, 4), ragged=True)\r\nx = layers.Lambda(lambda x: x)(inp)\r\nmodel = keras.Model(inp, x)\r\nmodel.compile(loss=\"mse\")\r\n\r\nmodel.save(\"test\", save_traces=True)\r\n```\r\n\r\nhttps://colab.research.google.com/drive/1hGrYVPrTy6vDbZPAHAPn2XGmpQMXQmQX?usp=sharing\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nColab\r\n\r\n- TensorFlow version (use command below):\r\ntf-nightly (~=2.5.0.a)\r\n\r\n**Describe the current behavior**\r\n```keras.Model.save()``` raises an exception when a layer returns a RaggedTensor.\r\n\r\nThe exception message states that the object returned from tracing a function must be a tf.Tensor. RaggedTensors are not instances of tf.Tensor.\r\n\r\n**Describe the expected behavior**\r\nIt should be possible to save and load models that take RaggedTensors as inputs and return RaggedTensors as outputs.\r\n\r\n**Workaround**\r\nThe model can be saved when the option ```save_traces=False``` is used.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1hGrYVPrTy6vDbZPAHAPn2XGmpQMXQmQX?usp=sharing\r\n\r\nIssue #41034 Reports a similar issue with loading a model that uses a RaggedTensor as input. This issue is for the specific problem of a RaggedTensor not being accepted as the return value of a keras node / tf.function as far as the saved_model package is concerned.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py:158 signature_wrapper  *\r\n        structured_outputs, signature_function.name, signature_key)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_serialization.py:219 _normalize_outputs  **\r\n        .format(value, key, compat.as_str_any(function_name), signature_key))\r\n\r\n    ValueError: Got a non-Tensor value tf.RaggedTensor(values=Tensor(\"PartitionedCall:0\", shape=(None, 4), dtype=float32), row_splits=Tensor(\"PartitionedCall:1\", shape=(None,), dtype=int64)) for key 'lambda' in the output of the function __inference__wrapped_model_173 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor.\r\n```\r\n\r\nAs far as I can tell this relates to the following function in python/saved_model/signature_serializatioon.py\r\n\r\n ```python\r\ndef _normalize_outputs(outputs, function_name, signature_key):\r\n  \"\"\"Construct an output dictionary from unnormalized function outputs.\"\"\"\r\n  # Convert `outputs` to a dictionary (if it's not one already).\r\n  if not isinstance(outputs, collections_abc.Mapping):\r\n    if not isinstance(outputs, collections_abc.Sequence):\r\n      outputs = [outputs]\r\n    outputs = {(\"output_{}\".format(output_index)): output\r\n               for output_index, output\r\n               in enumerate(outputs)}\r\n\r\n  # Check that the keys of `outputs` are strings and the values are Tensors.\r\n  for key, value in outputs.items():\r\n    if not isinstance(key, compat.bytes_or_text_types):\r\n      raise ValueError(\r\n          (\"Got a dictionary with a non-string key {!r} in the output of the \"\r\n           \"function {} used to generate the SavedModel signature {!r}.\")\r\n          .format(key, compat.as_str_any(function_name), signature_key))\r\n    if not isinstance(value, ops.Tensor):\r\n      raise ValueError(\r\n          (\"Got a non-Tensor value {!r} for key {!r} in the output of the \"\r\n           \"function {} used to generate the SavedModel signature {!r}. \"\r\n           \"Outputs for functions used as signatures must be a single Tensor, \"\r\n           \"a sequence of Tensors, or a dictionary from string to Tensor.\")\r\n          .format(value, key, compat.as_str_any(function_name), signature_key))\r\n\r\n  return outputs\r\n```\r\n\r\nThe test that requires the return value to be an instance of tf.Tensor is too restrictive.", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/36183e588ab823b46da963668fc8429c/45797-2-3.ipynb), [TF v2.4](https://colab.research.google.com/gist/amahendrakar/f7339fc475dd6101cd31898fa9dc6cbf/45797.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/328ae01930ae030a4edc2d21e1b0f2fe/45797-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!\r\n", "@pedro-r-marques,\r\nThe error can be resolved by replacing the line,  **`x = layers.Lambda(lambda x: x)(inp)`** with \r\n**`x = layers.Lambda(lambda x: x.to_tensor())(inp)`**.\r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/cd735dc7a4c5b1a09a4a206a9b336548/45797.ipynb) of the working code. Thanks!\r\n", "@rmothukuru The operation ```to_tensor``` converts a RaggedTensor to a dense tensor (i.e. tf.Tensor). If you apply that operation the node no longer returns a RaggedTensor and thus graph saving works. This issue is to track the fact that graph saving does not support graphs with nodes that return RaggedTensors, which I'm sure you agree, is undesirable.", "The issue seems to be on the keras side since using only the savedmodel api works in TF2.4:\r\n````\r\nclass TestOutputRagged(tf.Module):\r\n  @tf.function()\r\n  def test(self, a):\r\n    return a\r\n\r\na = TestOutputRagged()\r\na.test(tf.ragged.constant([[2], [6, 8]]))\r\ntf.saved_model.save(a, '/tmp/test')\r\ntf.saved_model.load('/tmp/test').test(tf.ragged.constant([[3], [6, 8]]))\r\n````\r\n", "If you want to replicate the issue without Keras, the following would do the trick:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass TestOutputRagged(tf.Module):\r\n  @tf.function()\r\n  def test(self, a):\r\n    return a\r\n\r\na = TestOutputRagged()\r\nrt = tf.ragged.constant([[2], [6, 8]])\r\ntf.saved_model.save(a, '/tmp/test', signatures=a.test.get_concrete_function(tf.type_spec_from_value(rt)))\r\n```\r\n\r\nBy default, Keras model save attempts to save the signatures of the functions. As per the original description above, the problem doesn't occur when not saving signatures. But the stack trace above is in the saved model code which currently rejects as function signature is the returned value is not a tf.Tensor. AFAIK it should accept any CompositeTensor.", "Fixed by https://github.com/tensorflow/tensorflow/commit/b68d87647ca12d585b4daaa597a450bf83bc3ea4", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45797\">No</a>\n"]}, {"number": 45796, "title": "Keras compile(loss=) missing gradient for captured tensors", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04.5 LTS (Dockerized)\r\n- TensorFlow installed from : Docker tensorflow/tensorflow:latest-gpu-jupyter\r\n- TensorFlow version : 2.3.1\r\n- Python version: 3.6.9\r\n- GPU model and memory: No GPU\r\n\r\n**Describe the current behavior**\r\n\r\nA gradient is not being recorded for tensors captured by a loss function passed to .compile() \r\n\r\nThis results in a ValueError: No gradients provided for any variable ...\r\n\r\n**Describe the expected behavior**\r\n\r\n.compile(loss=) should correctly integrate the gradients for all tensors used in the loss computation.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://gist.github.com/TheBeaNerd/78844dacf2d236eb50188062036a1082\r\n", "comments": ["@TheBeaNerd,\r\nCould you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "@amahendrakar\r\nThe example still does not work, but I'm seeing a different error under both 2.4.0 and nightly:\r\n\r\n  TypeError: Cannot convert a symbolic Keras input/output to a numpy array.\r\n\r\nAs I note in my example, however, using add_loss() rather than .compile(loss=) works under 2.3.1, 2.4.0 and nightly.\r\n\r\n(I have updated my gist to reflect this experiment)", "Was able to reproduce the issue. Running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/4a4292b7148fc3e3b8effccebeea95d7/45796-2-3.ipynb#scrollTo=MEU-6CSjvLen), throws a `ValueError` stating `No gradients provided for any variable`.\r\n\r\nWhereas with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/b66448a64c709065ec989dbc047b2add/45796.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2a43ee3aab29e02270b395f82ffb4f6d/45796-tf-nightly.ipynb#scrollTo=MEU-6CSjvLen), the error is `TypeError: Cannot convert a symbolic Keras input/output to a numpy array`. Please check the linked gist for reference. Thanks!", "The code above is TF1 code. It relies on the existence of backgrounds static computation graph. \r\nIf you want to run it, revert to TF 1.x. The gradient error is expected.\r\n\r\nHere's the equivalent code in TF 2.x: https://keras.io/examples/generative/vae/\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45796\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45796\">No</a>\n", "> The gradient error is expected.\r\n\r\n  As noted above, under TF 2.4/nightly, the gradient error has been replaced with:\r\n\r\n  TypeError: Cannot convert a symbolic Keras input/output to a numpy array.\r\n\r\n> Here's the equivalent code in TF 2.x: https://keras.io/examples/generative/vae/\r\n\r\n  The linked example doesn't use .compile(loss=) to integrate loss with the model.\r\n\r\n  Thus, it doesn't address the issue:\r\n\r\n> **Describe the expected behavior**\r\n>\r\n> .compile(loss=) should correctly integrate the gradients for all tensors used in the loss computation.\r\n\r\n"]}, {"number": 45791, "title": "Official Example For TensorFlow Lite Model Maker doesn't work", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n**No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Mac Book Pro 2017(2.3 GHz Dual-Core Intel Core i5, 8 GB 2133 MHz LPDDR3), MacOS Big Sur 11.0.1,\r\nBrowser: Version 87.0.4280.88 (Official Build) (x86_64),\r\nbut I guess the above info does not matter as I run the example in Google Colab [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_question_answer.ipynb)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n**OnePlus 6 OxygenOS 10.3.7, Android 10**\r\n- TensorFlow installed from (source or binary):\r\n**binary**\r\n- TensorFlow version (use command below):\r\n**In Colab 2.5.0-dev20201217, in Android 0.0.0-nightly**\r\n- Python version:\r\n**Python 3.6.9**\r\n- Bazel version (if compiling from source):\r\n**N/A**\r\n- GCC/Compiler version (if compiling from source):\r\n**N/A**\r\n- CUDA/cuDNN version:\r\n**nvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243**\r\n- GPU model and memory:\r\nI cannot check this as I run the Colab provided in official Tensorflow page on the provided Google Colab Environment, which you can see in the following url: [https://www.tensorflow.org/lite/tutorials/model_maker_question_answer](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer)\r\n\r\n\r\n**Describe the current behavior**\r\nIn order to have the full context of the issue, please see [the following issue](https://github.com/tensorflow/tensorflow/issues/45541)\r\n\r\n\r\nThe specific problem described in #45541  is resolved on nightly build, but overall the official example is not working:\r\n\r\nhere are the issues:\r\n\r\n1. When I do `!pip install tflite-model-maker-nightly` I see the following error in the log:\r\n```\r\nERROR: tensorflow 2.3.0 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\r\nERROR: tf-nightly 2.5.0.dev20201215 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\r\n```\r\nThough this does not stop the execution and I can continue.\r\n\r\n2. When i try to use the created model with TensorFlow Lite Android v 2.3.0 I get the following error when I try to create interpreter with this model:\r\nhttps://gist.github.com/ando0689/1874b6881dc7af3a31519e1bb2813386\r\n\r\n3. And when I increase the version of TensorFlow Lite on Android to `0.0.0-nightly` I get the following error when I try to do inference:\r\nhttps://gist.github.com/ando0689/b25f00443249da833af08cb8a6235a95\r\n\r\n4. Just a note: The provided dataset with 8000 entities seems to be too much for this example, as running on the free Colab, it takes about 6 hours, and usually fails with some error.. I was only able to train it by reducing the dataset to be 1000 entities, I guess it's enough for example.\r\n\r\n**Describe the expected behavior**\r\nThe example project provided in Official website of TensorFlow should run without any problem on the officially provided Colab Environment and also the produced model should work on client side as well.\r\n\r\n**Standalone code to reproduce the issue**\r\n1. Please try to make TFLite model using Official Colab [here](https://www.tensorflow.org/lite/tutorials/model_maker_question_answer)\r\n2. Following the resolution of #45541  issue described [here](https://github.com/tensorflow/tensorflow/issues/45541) install tflit_model_maker from nightly channel  using `!pip install tflite-model-maker-nightly`\r\n3. On client side use the official TensorFlow Lite Android example app for BERT QA, find it [here](https://github.com/tensorflow/examples/tree/master/lite/examples/bert_qa/android)\r\n\r\n**Other info / logs** \r\nProvided in issue description above\r\n", "comments": ["Thank you for the report! We will take a look at it in details from our side.\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'FULLY_CONNECTED' version '9'\r\n    \r\nRegistration failed.\r\n```\r\n\r\nAs you find the error message, can you check it what TensorFlow version do you use exactly? Is it latest TF tensorflow (like 2.3, 2.4) or tf-nightly (2.5.0.dev20201215)? Thank you!", "Hi @lintian06 ,\r\n\r\nThanks for reply.\r\n\r\nAs I mentioned in the report, this exact error is received with TFLite 2.3.0 on Android, changing the version of TFLite to tf-nightly (2.5.0.dev20201215) resolves this exact problem, and the Interpreter gets initialized, but during inference it throws another error, you can see the log here:\r\n[https://gist.github.com/ando0689/b25f00443249da833af08cb8a6235a95](https://gist.github.com/ando0689/b25f00443249da833af08cb8a6235a95)", "Thank you for the log message! The crashed call stack doesn't show the information as detailed as I need.\r\n\r\nCan you try changing [this line](https://github.com/tensorflow/examples/blob/22f86fd27e5688152772e06db80da03d552bfd10/lite/examples/bert_qa/android/app/src/main/java/org/tensorflow/lite/examples/bertqa/ml/QaClient.java#L151) to\r\n```\r\nObject[] inputs = {segmentIds, inputMask, inputIds};\r\n```\r\n\r\nI just suspect your model may have different tensors, compared with the existing one.\r\n", "Thanks. \r\nIt's strange but now I started to receive the following exception:\r\nhttps://gist.github.com/ando0689/23077fe85f7961b07ee0d2e550b42538\r\n\r\nB.t.w, if it would be more convenient for you, here is the model itself, I use the official TFLite BertQA example app, the only change I do is the change of the model and the version of TF. so may be if you can reproduce the problem it would be easier to debug it?\r\nhttps://drive.google.com/file/d/13tBmq6oTqPC3pyiElJuRVzks1z1xmEja/view?usp=sharing\r\n ", "Hi @ando0689 , the file contains zero bytes, which doesn't seem correct.", "@lintian06 , sorry for that, last time I tried with a corrupted model, and the last error was because of that.\r\nI retrained the model and tried how you suggested with changing \r\nthis `Object[] inputs = {inputIds, inputMask, segmentIds};`\r\nto this `Object[] inputs = {segmentIds, inputMask, inputIds};`\r\n\r\nand seems it worked! Thank you very much!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45791\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45791\">No</a>\n", "Great! FYI, we are going to polish the code to adapt the tensor order in the near future. "]}, {"number": 45790, "title": "Fixes added for TfLite Micro related TCs", "body": "This PR include fixes for 3 TfLite Micro test cases.\r\nA brief description of fixes:\r\n1. Allocation size values are measured and updated for s390x in memoy_arena test file. These values were hard-coded and only measured for x86 as per the comments in the file. I used the logs for memory_arena_test to map the values to be updated.\r\n2. Values for kBufferAlignment is updated as it was observed that when this value is set to 8, then it actually aligns tensor buffers value to 16 which is the requirement for SIMD extensions. If this value is left to 16, then micro_allocator and interpreter TCs fail and both test cases give warning of losing bytes of data due to misalignment.\r\n3. Calling of CorrectTensorEndianness function is removed as the `FlatBufferVectorToTfLiteTypeArray` already converts flatbuffer tensor data from little endian to big endian during `StartModelAllocation` function call within `AllocateTensors` function.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Thanks for your contributions. Please note that much of the TfLite Micro team will be on vacation so our responses will be delayed until the first week of Jan 2021.\r\n\r\nI will strongly encourage you to break down this PR into multiple individual PRs and also have github issues associated with each of these per [our contribution guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md#general-pull-request-guidelines). I will close the current PR, but am happy to continue the conversation over github issues and the more focused PRs that I am requesting.\r\n\r\nMore specific comments and suggestions below.\r\n\r\n> This PR include fixes for 3 TfLite Micro test cases.\r\n> A brief description of fixes:\r\n> \r\n> 1. Allocation size values are measured and updated for s390x in memoy_arena test file. These values were hard-coded and only measured for x86 as per the comments in the file. I used the logs for memory_arena_test to map the values to be updated.\r\n\r\nWe won't be able to accept changes to the memory_arena_threshold_test at this time. While this test has value in detecting some regressions, it is also quite brittle in practice and the thresholds change based on the architecture, what optimized kernel implementations are used ...\r\n\r\nThe direction that we would like to take such profiling is to have it be part of continous builds and graph the arena size over time, rather than be a unit test.\r\n\r\n> 2. Values for kBufferAlignment is updated as it was observed that when this value is set to 8, then it actually aligns tensor buffers value to 16 which is the requirement for SIMD extensions. If this value is left to 16, then micro_allocator and interpreter TCs fail and both test cases give warning of losing bytes of data due to misalignment.\r\n\r\nI think we need to root cause what is happening here. If setting the value to 8 results in an alignment of 16, then that is a bug that we need to fix. Would you be willing to make a github issue describing the steps that you took to reproduce this error?\r\n\r\n> 3. Calling of CorrectTensorEndianness function is removed as the `FlatBufferVectorToTfLiteTypeArray` already converts flatbuffer tensor data from little endian to big endian during `StartModelAllocation` function call within `AllocateTensors` function.\r\n\r\nThis sounds reasonable. Support for big endian is on a best effort basis so there can certainly be bugs here. Please make a separate pull request and corresponding github issue so that this can be fixed in isolation. I will also pull in other external contributors who care about big-endian support into that discussion."]}, {"number": 45788, "title": "Allow inverse-normalize data using tf.normalize() function", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\nI am working a Regression(seq-2-seq) problem using a custom model, and while building my Data Pipeline I used `tf.keras.utilities.normalize` which automatically normalizes a numpy array in a feauture range not specified by the user, it works very well in normalizing the data.\r\n\r\nHowever, after training a model on the normalized data (Where the `labels` were normalized using this utility), I am getting predictions as \"normalized\" labels. Apparently, when I run the `.evaluate()` function to test out my model, I am getting an `RMSE: 0.00089` which indicates that the model is performing well and is **de-normalizing the labels during evaluation**. However when I predict on 1 sample, the predictions are not normalized.\r\n\r\nSo, in short, can a feature be introduced that allows the function to de-normalize a numpy array manually (Like a Boolean argument) perhaps inheriting the required metadata from the `model` object? I think it would be very helpful for everyone using this function and would aid in quick&clean normalization.\r\n\r\n\r\n\r\n", "comments": ["Can I help in coding this feature ? ", "@amahendrakar @rmothukuru Any workaround for the time being?", "@neel04,\r\nIn the below statement:\r\n\r\n> However when I predict on 1 sample, the predictions are not normalized. \r\n\r\ndo you mean the Predictions are **not Denormalized**?\r\n\r\nAlso, In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@rmothukuru There is no issue - it is just that if I normalize data with that function, there is simply no way to recover the original data (before normalization)", "To denormalize an array, you need to know the mean and variance that were used to normalize it in the first place. This is not the case here.\r\n\r\nAlso note that should not use `evaluate()` to compute RMSE because `evaluate()` computes metrics sample-wise then averages the per-sample results. However, the square root of a sum is not the sum of the square roots."]}, {"number": 45787, "title": "Make TFLu renode test script target platform independent", "body": "This replaces `test_bluepill_binary.sh` and `test_stm32f4_binary.sh` with `test_with_renode.sh`.\r\n\r\n\r\nWith this change, the unit tests for stm32f4 are also emabled as part of the CI. These are the first set of unit tests of the cmsis-nn kernels.\r\n\r\nSimilar to the bluepill + renode test suite, the STM32F4 test suite takes ~30 seconds to run on my local machine.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@freddan80, @mansnils: once this PR is merged, we will have the STM32F4 + Renode + CMSIS tests be part of the CI for the very first time!\r\n\r\n@piotrzierhoffer, @mgielda: with this our first round of Renode integration will be complete, and very successful too!\r\n\r\nThe renode download and installation is improved with https://github.com/tensorflow/tensorflow/pull/45812"]}, {"number": 45786, "title": "Keras docs wrongly advise not to pass tf.keras.layers activations to a layer creation", "body": "## URL(s) with the issue:\r\n\r\nhttps://keras.io/api/layers/activations/\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn tf.keras, there are 2** ways of adding activation functions to your models. In the first instance, objects from `tf.keras.activations` can be passed as the `activation` argument in the creation of a layer as in: \r\n```python\r\nmodel.add(layers.Dense(64, activation=tf.nn.tanh))\r\n```\r\n\r\nThe 2nd API for activations is to explicitly add them as a layer, as in:\r\n```python\r\nx = layers.Dense(10)(x)\r\nx = layers.LeakyReLU()(x)\r\n```\r\n\r\nThe problem with the docs is that they explicitly say (at https://keras.io/api/layers/activations/) that:\r\n> you should not pass activation layers instances as the activation argument of a layer. They're meant to be used just like regular layers\r\n\r\nThis runs contrary to current usage in tf2, where can can do exactly this. See, for instance, [this stackoverflow answer](https://stackoverflow.com/a/56869141/7858285) which advises the following:\r\n```python\r\nmodel.add(Conv2D(..., activation=tf.keras.layers.LeakyReLU(alpha=0.1), ...)\r\n```\r\nin place of the `lambda` function that is otherwise needed if one is to follow the keras docs advice.\r\n\r\n**you might say there are 3 ways, as in the '1st' method one can also request an activation by its string identifier. \r\n\r\n### Submit a pull request?\r\nUnless I'm mistaken, the https://keras.io/ docs are not open source. The keras repo currently requests that all issues be opened here in the tf repo. ", "comments": ["@bibbygoodwin,\r\nThis is an issue related to `**Keras documentation**` and not **`TF Keras documentation`**.  The statement:\r\n\r\n> you should not pass activation layers instances as the activation argument of a layer. They're meant to be used just like regular layers\r\n\r\nis neither present in the TF Keras Documentation for [LeakyRelu](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU) nor in [PReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU).\r\n\r\nSo, request you to raise an issue in [Keras Github Repository](https://github.com/keras-team/keras/issues/new). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rmothukuru Thank you for your reply. The README of the keras repo currently states: \r\n\r\n> In the near future, this repository will be used once again for developing the Keras codebase. For the time being, the Keras codebase is being developed at tensorflow/tensorflow, and any PR or issue should be directed there.\r\n\r\nThat was why I created this issue here and not in the keras repo.\r\n\r\nAlso, the docs I linked to which have the problem (https://keras.io/api/layers/activations/) are indeed the keras documentation, but are for `tf.keras`, as you can see on the linked page. Thus, I think there is ample scope for confusion here if things are not ammended.\r\n\r\nThanks again.", "It was always possible to do so (if you use the layer as such, it's used like a function), but it leads to potential issues (most importantly the issue that your activation layer *could* include trainable weights, which would be ignored by the model if you used it like this).\r\n\r\nLayers are meant to be nodes in a graph of layers, and so should be used as such, like in the example:\r\n\r\n```\r\nx = layers.Dense(10)(x)\r\nx = layers.LeakyReLU()(x)\r\n```\r\n\r\n> Unless I'm mistaken, the https://keras.io/ docs are not open source. The keras repo currently requests that all issues be opened here in the tf repo.\r\n\r\nKeras docs are open-source and can be modified at https://github.com/keras-team/keras-io\r\n"]}, {"number": 45785, "title": "GRPCIO v1.32.0 hard requirement results in clashes with other versions", "body": "Tensorflow currently has a hard dependency on `grpcio~=1.32.0`, which results in clashes with packages that depend in newer versions of GRPC. This issue encompasses the requirement to update the GRPCIO library to the latest 1.34.0 version (or later).", "comments": ["https://github.com/tensorflow/tensorflow/pull/46019 may fix this.", "#46019 landed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45785\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45785\">No</a>\n", "We've just stumbled on this issue with TF 2.4.1. It seems that TF 2.4.1 still depends on 1.32.0: https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/tools/pip_package/setup.py#L121\r\n", "That is true. 2.5 branch cut is today.", "> That is true. 2.5 branch cut is today.\r\n\r\nThanks @mihaimaruseac , can you confirm this will be addressed in TF 2.5? Since this issue is closed, do you have another issue to track its progress?", "I've created a new issue just in case: https://github.com/tensorflow/tensorflow/issues/48109"]}, {"number": 45784, "title": "[PluggableDevice] PluggableDevice mechanism implementation", "body": "Add PluggableDevice mechanism implementation, including plugin initialization, PluggableDevice create and compute, and also add a new flag(is_pluggable_device) in DeviceFactory to do some low-level device specialization. ", "comments": ["@penpornk @annarev I have submit the pluggable device implementation PR, please help to have a review. thanks very much!", "@kulinseth  @wchao1115  @pawelpiskorski FYI\r\n", "@penpornk @sanjoy I have addressed the comments, could you help to give a further review? Any comments will be appreciated. Thanks very much,.", "Is this targeted for 2.5 release ?", "@sub-mod Yes, we are trying to get this into 2.5 release.", "@penpornk I have addressed all the comments, please help to review. Thanks very much.", "@penpornk I have revert the device_spec.py and device_test.py, please have a review. Thanks.", "@penpornk I fixed a  sanity check failure(builderfiler format issue), sorry for bring the trouble.\r\nand I'm not sure whether  win_gen_out is relaeted with this PR, I will continue to check this. Thanks.\r\n```\r\n2021-03-10 14:35:51.678303: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at iterator_ops.cc:1191 : Internal: Could not parse: 64 as uint64\r\nERROR:tensorflow:Could not parse: 64 as uint64\r\n\t [[node save/DeserializeIterator (defined at \\\\?\\T:\\tmp\\Bazel.runfiles_hpqk5btx\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\data\\experimental\\kernel_tests\\snapshot_test.py:1201) ]]\r\n\r\nOriginal stack trace for 'save/DeserializeIterator':\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_hpqk5btx\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\data\\experimental\\kernel_tests\\snapshot_test.py\", line 1284, in <module>\r\n    test.main()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\test.py\", line 58, in main\r\n    return _googletest.main(argv)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\googletest.py\", line 66, in main\r\n    benchmark.benchmarks_main(true_main=main_wrapper, argv=argv)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\benchmark.py\", line 518, in benchmarks_main\r\n    true_main()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\platform\\googletest.py\",\r\n```", "@penpornk Windows Bazel GPU report visual studio error c2039 'iota' is not a member of 'std' in device_util_id.h, I added \"#include numeric\" in that headfile, I am not sure whether it can resolve, I check other files who also used this std function include this file. Thanks. ", "@penpornk Seems still one checks failed(MacOs CPU python3), It seems a strange error(one is direct_session, another is grappler), I am not sure whether it is related with this PR since I have no MacOS machine to reproduce, can you help to check this also? I will also check the possible issue. Thanks very much!", "@jzhoulon This check failed 4 hours ago too (same errors) when it passed elsewhere. So I reran it. It's still failing so it's likely related to this PR. I'll try to find how I can reproduce this from my side as well.", "> @jzhoulon This check failed 4 hours ago too (same errors) when it passed elsewhere. So I reran it. It's still failing so it's likely related to this PR. I'll try to find how I can reproduce this from my side as well.\r\n\r\n@jzhoulon and @penpornk , I have pulled in the latest patches and will try to repro on my side. I will update if I find something locally.\r\n", "> > @jzhoulon This check failed 4 hours ago too (same errors) when it passed elsewhere. So I reran it. It's still failing so it's likely related to this PR. I'll try to find how I can reproduce this from my side as well.\r\n> \r\n> @jzhoulon and @penpornk , I have pulled in the latest patches and will try to repro on my side. I will update if I find something locally.\r\n\r\n@kulinseth  Thanks for the help! I have some findings , though I still didn't figure out why only MacOS has this issue\r\n it was caused by framework:allocator be built into several modules, and make the static variable (cpu_allocator_collect_full_stats) duplicated and have inconsistent stats in several modules. I delete the dependence of framework:allocator in common_runtime:bfc_allocator and device:device_mem_allocator), the test can pass now. \r\n\r\nDetails:\r\nGPU and PluggableDevice's bfc_allocator deps on common_runtime:bfc_allocator and device:device_mem_allocator,  both of these modules(common_runtime:bfc_allocator && device::device_mem_allocator) depends on framework:allocator. In framework/allocator.cc, it has static variable(cpu_allocator_collect_full_stats), it is supposed to be global unique, however, I printed its address, it has two address(means it has duplicated variable) when the test case failure, as a result, cpu_alloc is always an old allocator(not new TrackingAllocator) in cpu_allocator_base. And I remove the dependence (framework::allocator) from common_runtime:bfc_allocator and device:device_mem_allocator, the build seems also pass(already depends on framework headers), and the test case pass.\r\n```\r\nCode:\r\n56 // If true, cpu allocator collects full stats.\r\n57 static bool cpu_allocator_collect_full_stats = false;\r\n58\r\n59 void EnableCPUAllocatorFullStats() { cpu_allocator_collect_full_stats = true; \r\n      printf(\"set cpu_allocator_collect_full_state_ptr = %p\\n\", &cpu_allocator_collect_full_stats);\r\n}\r\n\r\n68 Allocator* cpu_allocator_base() {\r\n69   static Allocator* cpu_alloc =\r\n70       AllocatorFactoryRegistry::singleton()->GetAllocator();\r\n71   // TODO(tucker): This really seems wrong.  It's only going to be effective on\r\n72   // the first call in a process (but the desired effect is associated with a\r\n73   // session), and we probably ought to be tracking the highest level Allocator,\r\n74   // not the lowest.  Revisit the advertised semantics of the triggering option.\r\n75   printf(\"get cpu_allocator_collect_full_stats_ptr = %p\\n\",  &cpu_allocator_collect_full_stats);\r\n76\r\n77   if (cpu_allocator_collect_full_stats && !cpu_alloc->TracksAllocationSizes()) {\r\n78     cpu_alloc = new TrackingAllocator(cpu_alloc, true);\r\n79   }\r\n80   return cpu_alloc;\r\n81 }\r\n```\r\ndebug log:\r\n```\r\n2021-03-12 14:09:32.971918: F tensorflow/core/grappler/clusters/single_machine_test.cc:55] Non-OK-status: cluster_->Provision() status: Invalid argument: Tracking allocation is not enabled.\r\nset cpu_allocator_collect_full_state_ptr = 0x11629b330\r\n\r\nget cpu_allocator_collect_full_stats_ptr = 0x11fc59b90\r\n*** Received signal 6 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n```\r\nfix patch:\r\n```\r\n--- a/tensorflow/core/common_runtime/BUILD\r\n+++ b/tensorflow/core/common_runtime/BUILD\r\n@@ -1697,7 +1697,7 @@ cc_library(\r\n         \"//tensorflow/core:lib\",\r\n         \"//tensorflow/core:lib_internal\",\r\n         \"//tensorflow/core:protos_all_cc\",\r\n-        \"//tensorflow/core/framework:allocator\",\r\n+        #\"//tensorflow/core/framework:allocator\",\r\n         \"//tensorflow/core/profiler/lib:traceme\",\r\n         \"@com_google_absl//absl/container:flat_hash_set\",\r\n        \"@com_google_absl//absl/strings\",\r\ndiff --git a/tensorflow/core/common_runtime/device/BUILD b/tensorflow/core/common_runtime/device/BUILD\r\nindex 3a49500053c..f0cd9030954 100644\r\n--- a/tensorflow/core/common_runtime/device/BUILD\r\n+++ b/tensorflow/core/common_runtime/device/BUILD\r\n@@ -75,7 +75,7 @@ tf_cuda_library(\r\n         \":device_id\",\r\n         \"//tensorflow/core:lib\",\r\n         \"//tensorflow/core:lib_internal\",\r\n-        \"//tensorflow/core/framework:allocator\",\r\n+        #\"//tensorflow/core/framework:allocator\",\r\n         \"//tensorflow/core/platform:stream_executor\",\r\n\r\n```\r\n\r\n(edited by penpornk@ to fix markdown formatting error which makes the patch hard to read)", "@jzhoulon Thank you for the quick findings! I think the duplicate symbol might be because both PluggableDevice runtime and GPU runtime are linked statically. Could you please help test linking PluggableDevice runtime dynamically? Our [tpu_runtime](https://github.com/tensorflow/tensorflow/blob/e0f5efab558c19dae5c8fc09b73da03a343a6773/tensorflow/core/tpu/BUILD#L286-L295) is also linked dynamically.\r\n\r\n@kulinseth Thank you for your help as well!\r\n\r\nI've pulled the PR in to test internally and am fixing other failures (which obscure this failure on Mac OS CI). I'll get to this once I'm done with other failures.", "> @jzhoulon Thank you for the quick findings! I think the duplicate symbol might be because both PluggableDevice runtime and GPU runtime are linked statically. Could you please help test linking PluggableDevice runtime dynamically? Our [tpu_runtime](https://github.com/tensorflow/tensorflow/blob/e0f5efab558c19dae5c8fc09b73da03a343a6773/tensorflow/core/tpu/BUILD#L286-L295) is also linked dynamically.\r\n> \r\n> @kulinseth Thank you for your help as well!\r\n> \r\n> I've pulled the PR in to test internally and am fixing other failures (which obscure this failure on Mac OS CI). I'll get to this once I'm done with other failures.\r\n\r\nThanks @jzhoulon for the findings. I was also curious why this was Mac only failure. I looked at the tests and they have the \"no_gpu\" tag to it (although Ubuntu CPU would have caught it too). Its also possible that the Xcode toolchain with compiler flags have different behavior with duplicate symbols. ", "@jzhoulon I had a question regarding the Pluggable impl. Did you test the pluggable implementation with host_memory_allocate/deallocate APIs being exercised ? I am locally not seeing them getting used, was curious if we need to set some special flag.", "> @jzhoulon I had a question regarding the Pluggable impl. Did you test the pluggable implementation with host_memory_allocate/deallocate APIs being exercised ? I am locally not seeing them getting used, was curious if we need to set some special flag.\r\n\r\n@kulinseth Thanks for the review. the host_memory_allocate is used in `PluggableDeviceProcessState::GetPluggableDeviceHostAllocator`, it will call DeviceHostAllocator to create host allocator, DeviceHostAllocator will call streamexecutor's HostMemAllocator.  no, you don't need to pass any flags in plugin, only to register the host_memory_allocate func ptr\r\n```\r\n    SubAllocator* sub_allocator = new DeviceHostAllocator(\r\n        se, numa_node, pluggable_device_host_alloc_visitors_[numa_node],\r\n        pluggable_device_host_free_visitors_[numa_node]);\r\n    int64 pluggable_device_host_mem_limit_in_mb = -1;\r\n```", "> @kulinseth Thanks for the review. the host_memory_allocate is used in `PluggableDeviceProcessState::GetPluggableDeviceHostAllocator`, it will call DeviceHostAllocator to create host allocator, DeviceHostAllocator will call streamexecutor's HostMemAllocator. no, you don't need to pass any flags in plugin, only to register the host_memory_allocate func ptr\r\n> \r\n> ```\r\n>     SubAllocator* sub_allocator = new DeviceHostAllocator(\r\n>         se, numa_node, pluggable_device_host_alloc_visitors_[numa_node],\r\n>         pluggable_device_host_free_visitors_[numa_node]);\r\n>     int64 pluggable_device_host_mem_limit_in_mb = -1;\r\n> ```\r\n\r\nThanks @jzhoulon. I do see the DeviceHostAllocator getting called (using the BFCAllocator) and registered, and we have the host_memory_allocate and deallocate functions registered but they are not getting invoked (rest of the alloc functions are working fine, like device mem allocate()/deallocate()) . Are we missing any `set_on_host()` or `force_gpu_compatible` flags to be set ? Is there any dependence on the numa configuration or memory stats.\r\n\r\nI am curious in your backend impl did you see any issues with using host_memory allocation features.", "> > @kulinseth Thanks for the review. the host_memory_allocate is used in `PluggableDeviceProcessState::GetPluggableDeviceHostAllocator`, it will call DeviceHostAllocator to create host allocator, DeviceHostAllocator will call streamexecutor's HostMemAllocator. no, you don't need to pass any flags in plugin, only to register the host_memory_allocate func ptr\r\n> > ```\r\n> >     SubAllocator* sub_allocator = new DeviceHostAllocator(\r\n> >         se, numa_node, pluggable_device_host_alloc_visitors_[numa_node],\r\n> >         pluggable_device_host_free_visitors_[numa_node]);\r\n> >     int64 pluggable_device_host_mem_limit_in_mb = -1;\r\n> > ```\r\n> \r\n> Thanks @jzhoulon. I do see the DeviceHostAllocator getting called (using the BFCAllocator) and registered, and we have the host_memory_allocate and deallocate functions registered but they are not getting invoked (rest of the alloc functions are working fine, like device mem allocate()/deallocate()) . Are we missing any `set_on_host()` or `force_gpu_compatible` flags to be set ? Is there any dependence on the numa configuration or memory stats.\r\n> \r\n> I am curious in your backend impl did you see any issues with using host_memory allocation features.\r\n\r\n@kulinseth   I just confirmed that host_memory_allocate can be invoked.(I add printf in hsot_memory_allocate and printed), can you try `force_gpu_compatible` to see whether it can be invoke? although I didn't find we use this option in our model, but it seems be invoked..\r\n```\r\nAllocator* PluggableDevice::GetAllocator(AllocatorAttributes attr) {\r\n  DCHECK(cpu_allocator_) << \"CPU allocator must be set\";\r\n  if (attr.on_host()) {\r\n    if (attr.gpu_compatible() || force_gpu_compatible_) {\r\n      PluggableDeviceProcessState* ps =\r\n          PluggableDeviceProcessState::singleton(device_type(), platform_name_);\r\n      return ps->GetPluggableDeviceHostAllocator(0);\r\n    } else {\r\n      return cpu_allocator_;\r\n    }\r\n  } else {\r\n    return device_allocator_;\r\n  }\r\n}\r\n\r\n```", "This PR was merged in https://github.com/tensorflow/tensorflow/commit/3a3878ff2dba3169c49991552bc1981f53f10099. I'm closing the PR now. Thank you very much everyone for the hard work!\r\n(We'll see in a day or two whether it will stick.)", "> \r\n> @kulinseth I just confirmed that host_memory_allocate can be invoked.(I add printf in hsot_memory_allocate and printed), can you try `force_gpu_compatible` to see whether it can be invoke? although I didn't find we use this option in our model, but it seems be invoked..\r\n\r\nThanks so much for checking."]}, {"number": 45783, "title": "Huge performance degradation on different CPU", "body": "I have a model with ResNet50 which is trained by Keras.\r\nThis model can be run on CPU \"Intel(R) Xeon(R) Gold 6138T CPU @ 2.00GHz\" in 1.2 second, but when predict on \"E5 2620\" it need 15s.\r\nIs there any idea to solve the performance degradation? Thanks.\r\nP.S. Both of the 2 cases is in virtual mechine.", "comments": ["@Dockyin,\r\nCould you please provide the TensorFlow version you are using and a minimal code snippet to reproduce the issue?\r\n\r\nPlease go through [this](https://www.tensorflow.org/guide/profiler) guide to optimize TensorFlow performance and also take a look at the guide to [debug performance bottlenecks](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#debug_performance_bottlenecks) and check if it helps. Thanks!", "E5 2620 are broadwell architecture systems while Intel Xeon Gold 6138 is Skylake which is newer generation than Broadwell (added AVX512 instruction set support). So it is expected that 6138 would be better than E5 2620. \r\n\r\nBut, are you using the \"pip install tensorflow-mkl\" wheel and with that, set the correct OpenMP threads? \r\n\r\nAdding @TensorFlow-MKL  for further trouble-shooting. ", "> @Dockyin,\r\n> Could you please provide the TensorFlow version you are using and a minimal code snippet to reproduce the issue?\r\n> \r\n> Please go through [this](https://www.tensorflow.org/guide/profiler) guide to optimize TensorFlow performance and also take a look at the guide to [debug performance bottlenecks](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras#debug_performance_bottlenecks) and check if it helps. Thanks!\r\n\r\n@amahendrakar \r\nThanks for your response. My tensorflow is 1.15.2, which running in python 3.7.8.\r\nmy code is:\r\n**Load model:**\r\n```\r\nself.sess = tf.compat.v1.Session()\r\nf = gfile.FastGFile(model_path, 'rb')\r\ngraph_def = tf.compat.v1.GraphDef()\r\ngraph_def.ParseFromString(f.read())\r\ntf.import_graph_def(graph_def, name='')\r\nf.close()\r\n```\r\n**Invoke model:**\r\n```\r\nimages_array = np.asarray(images) \r\ninput_1 = self.sess.graph.get_tensor_by_name('input_1:0') \r\npred = self.sess.graph.get_tensor_by_name('dense_1/Softmax:0') \r\npreds = self.sess.run(pred, feed_dict={input_1: images_array}) \r\n```", "@Dockyin,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to v2.4 and check if you are facing the same issue. Thanks!", "@Dockyin \r\nE5 2620 is 2012 product and with 6 cores, 32nm, support AVX.\r\nGold 6138T is 2017 product and with 20 cores, 14nm, support AVX512.\r\n\r\nThey have big gap in performance. So the result of your case is normal.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Solved!\r\nThe degradation is caused by virtual layer settings.!!!--_--", "@Dockyin \r\n\r\nIt's great to find the root cause!\r\n\r\nThank you!"]}, {"number": 45782, "title": "Support for AMD Graphics Card", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.x\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIf possible can the support for AMD graphics be provided as this is the issue either way if the system has good features/configuration just because it has an AMD graphics card the TensorFlow won't work as smooth as it works in Nvidia graphics card? \r\n\r\n**Will this change the current API? How?**\r\nI don't know about this.\r\n\r\n**Who will benefit from this feature?**\r\nEveryone who uses AMD graphics card.\r\n\r\n**Any Other info.**\r\n", "comments": ["@shvamabps,\r\nPlease go through simliar issue [#32358](https://github.com/tensorflow/tensorflow/issues/32358) and take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/32358#issuecomment-530234081) from a member of the TensorFlow team and let us know if it helps. Thanks!", "Thanks @amahendrakar \nIt helped..", "@shvamabps You can check out `PlaidML` by Intel which is great for using AMD GPU's and other non-Nvidia devices to be used in training and works very well with `Keras`.", "@shvamabps,\r\nThank you for the update. Marking this issue as closed, please feel free to reopen if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45782\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45782\">No</a>\n"]}, {"number": 45781, "title": "`Model` subclasses cannot be loaded without \"losing\" their class. ", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes.  I have included a short, self-contained, correct example in a Colab below.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nReproducible on Colab instances.  Originally identified on Debian 10, running on Google Cloud \r\nLinux jamlong-gpu-4 4.9.0-12-amd64 #1 SMP Debian 4.9.210-1 (2020-01-20) x86_64 GNU/Linux\r\n\r\n- TensorFlow installed from (source or binary):\r\nBinary (pip install tensorflow==2.4.0)\r\n\r\n- TensorFlow version (use command below):\r\nv2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n\r\n- Python version:\r\nPython 3.7.6\r\n\r\n- CUDA/cuDNN version:\r\nN/A - reproducible on Colab without GPU\r\n\r\n- GPU model and memory:\r\nN/A - reproducible on Colab without GPU\r\n\r\n**Describe the current behavior**\r\nLoading a custom `MySubClass` model (subclass of `Model`) from a SavedModel yields a `Functional` instead of a `MySubClass`.\r\n\r\n\r\n**Describe the expected behavior**\r\nI'd expect when I define a `MySubClass` class as a subclass of Model, and then reload an instance of the model, I would get a `MySubClass`\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nColab demonstrating an SSCCE:\r\n\r\nhttps://colab.research.google.com/drive/12ShtOJ7oBkQsGsn_N801qHeMv3__POv4?usp=sharing\r\n\r\n\r\n**Other info / logs** \r\n\r\nThe issue appears to be that inheritance is effectively \"broken\" for `Model.from_config` as a result of the introduction of the `Functional` class, which `Model` is proxying to, in somewhat baffling (to an outsider, at least) fashion. \r\n\r\nIf a user creates `MySubClass` as subclass of `Model`, and doesn't define a `from_config`, the implementation falls back to `Model.from_config`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L2302\r\n```\r\n @classmethod\r\n  def from_config(cls, config, custom_objects=None):\r\n    # Since only FunctionalModel produces config, the model can only\r\n    # be constructed for FunctionalModel\r\n    from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\r\n    return functional.Functional.from_config(\r\n        config, custom_objects=custom_objects)\r\n```\r\n\r\nHowever, this implementation assumes that everything is really a `Functional` (since any model with a config is a `Functional`), which is what appears to be breaking us in this case. `Model.from_config(MySubClass, config...)` gets called, and it delegates to an explicit `Functional.from_config(config...)`, discarding the fact that cls=MySubClass in the process.  \r\n\r\nThe problem is, `Model.from_config` (or, more accurately, `Functional.from_config` is doing _all of the work_ in reloading the model, so we're completely reliant on it. However, the fact that the `Model` / `Functional` differentiation exists completely breaks the ability to use it.\r\n\r\n* It doesn't work \"out of the box\", which it ideally would without the `Model` / `Functional` divide.\r\n* Normally you'd just do any pre-work, defer to `super().from_config()`, and do any post-processing. However, the super-call would trigger the \"Force it to be `Functional`\" code, which still breaks it.\r\n* Because `Functional` is part of `tf.python.keras` and not exported as part of the public API, we can't simply emulate the logic of `Functional.from_config` (which makes call to `tf.python.keras.engine.functional`,  delegates to the `cls` constructor, and then another call to do some post-processing)\r\n* Of note: if I ignore the fact that it's _also_ completely the wrong thing to do, and have my class subclass  `tf.python.keras.engine.functional.Functional` instead of `Model`, it appears to work. It looks like it may _just_ be the fact that `Model.from_config` discards the `cls`.  But again, not something we can do here since it's not part of the public API.\r\n\r\nIn short: unless I've missed something obvious, model subclassing is currently broken w.r.t. serialization because of the `Model` vs. `Functional` divide. \r\n\r\n", "comments": ["@jamlong,\r\nI do not have access to the Colab notebook you have linked. Could you please provide the required permissions to view the file. Thanks!", "Updated the link to something more accessible:\r\nhttps://colab.research.google.com/drive/12ShtOJ7oBkQsGsn_N801qHeMv3__POv4?usp=sharing\r\n", "Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/328db4c6cafbc39a3aeb6926764f37a3/45781.ipynb). Thanks!", "Hi--\r\n\r\nEverything you say is correct, and thank you for bringing this up-- I am looking into it. In the meantime, though, I would hesitate to say that \"model subclassing is currently broken w.r.t. serialization because of the Model vs. Functional divide\", since subclassed model can be revived as the correct class if you register the class as a custom object:\r\n```\r\nreloaded_model = tf.keras.models.load_model(tmpdir, custom_objects={\"MyModelSubclass\": MyModelSubclass})\r\nassert isinstance(reloaded_model, MyModelSubclass)\r\n```\r\n", "I am interested in getting a user's perspective here, @jamlong.\r\n\r\nSince the below works:\r\n```\r\nreloaded_model = tf.keras.models.load_model(tmpdir, custom_objects={\"MyModelSubclass\": MyModelSubclass})\r\nassert isinstance(reloaded_model, MyModelSubclass)\r\n```\r\ndo you still believe that model inheritance is broken?\r\n\r\nAlso, another question:\r\n```\r\nclass MyModelSubclass(tf.keras.models.Model):\r\n    pass\r\nmodel = MyModelSubclass()\r\nmodel.save(tmpdir)\r\ndel MyModelSubclass  # delete class definition since user may not have access to it in a new runtime\r\n\r\nreloaded_model = tf.keras.models.load_model(tmpdir)\r\nprint(type(reloaded_model))\r\n```\r\nWhat do you expect the last line to print?\r\n", "> I would hesitate to say that \"model subclassing is currently broken\nw.r.t. serialization because of the Model vs. Functional divide\", since\nsubclassed model can be revived as the correct class if you register the\nclass as a custom object:\n\nAh. The example I provided is missing an important detail.  The original\ncode had the subclass decorated with\n@tf.keras.utils.register_keras_serializable, which we'd expect to be\nsufficient based on its documentation:\n\nThis decorator injects the decorated class or function into the Keras\ncustom object dictionary,* so that it can be serialized and deserialized\nwithout needing an entry in the user-provided custom object dict.*\n\n\nIs this not a sufficient approach in this case for model subclasses, for\nsome reason?\n\nIf you update the example to use it, you still run into the same issue.  I\nrewrote this as a standalone script to rule out colab issues.  Updated\nGist:\nhttps://gist.github.com/jamlong/d1d5cf61128d11191631ca71a8825a52\n\nIf you run the gist under tf 2.4.0, the first run will define/register the\nsubclass, create a model, save it, and then reload it within the same\nprocess. This is fine, and produces output like:\n\nReloaded model:\n\n<__main__.MyModelSubclass object at 0x7f5f6ee2f490>\n\nRestart the Runtime & run all\n\nIf you run it a second time from a separate process (as in: save the gist\nas a script and run it twice, or \"Restart Runtime And Run All\" in colab),\nthe second time it defines the subclass and registers it, sees a model file\nexists, then simply reloads the model from disk.  This still results in the\nModel being reloaded as a `Functional` instead of `MySubClass` despite\nMySubClass being defined and registered with the decorator.\n\n<tensorflow.python.keras.engine.functional.Functional object at\n0x7f78bb3d4ad0>\n\nTraceback (most recent call last):\n\n  File \"test.py\", line 20, in <module>\n\n    f\"Reloaded instance of type {type(reloaded_model)}, but should also be\nMyModelSubclass\"\n\nAssertionError: Reloaded instance of type <class\n'tensorflow.python.keras.engine.functional.Functional'>, but should also be\nMyModelSubclass\n\n> do you still believe that model inheritance is broken?\n\nYes-ish. I'd expect the new gist to work, and it doesn't.\n\nAt this point it's unclear to me which bit isn't working, whether that's an\nissue with the inheritance (because if I derive from the non-public\n`Functional` class directly instead of `Model` it works...) or the\nserialization (register_keras_serializable/load_model), or my *expectations* of\nregister_keras_serializable working in this case.\n\n\nOn Fri, Jan 29, 2021 at 3:40 PM Monica Song <notifications@github.com>\nwrote:\n\n> I am interested in getting a user's perspective here, @jamlong\n> <https://github.com/jamlong>.\n>\n> Since the below works:\n>\n> reloaded_model = tf.keras.models.load_model(tmpdir, custom_objects={\"MyModelSubclass\": MyModelSubclass})\n> assert isinstance(reloaded_model, MyModelSubclass)\n>\n> do you still believe that model inheritance is broken?\n>\n> Also, another question:\n>\n> class MyModelSubclass(tf.keras.models.Model):\n>     pass\n> model = MyModelSubclass()\n> model.save(tmpdir)\n> del MyModelSubclass  # delete class definition since user may not have access to it in a new runtime\n>\n> reloaded_model = tf.keras.models.load_model(tmpdir)\n> print(type(reloaded_model))\n>\n> What do you expect the last line to print?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/45781#issuecomment-770106477>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABE5IDW5IOXSFPOMNICCL3DS4NBNLANCNFSM4U7BGI4A>\n> .\n>\n", "I confirm jamlong's observations. The following does not work for me; the reloaded model is of type `Functional`.\r\nMy use case: I train a custom model in one script, and I try to load the trained model in another script.\r\n\r\nI'm using TF 2.3.0 on Python 3.6.12\r\n\r\n**Update** After some further playing around, I found that 'subclass-loading' is broken in TF 2.3.x, but works in TF 2.4.\r\n\r\n> ```\r\n> reloaded_model = tf.keras.models.load_model(tmpdir, custom_objects={\"MyModelSubclass\": MyModelSubclass})\r\n> assert isinstance(reloaded_model, MyModelSubclass)  # ASSERT --> reloaded_model = <tensorflow.python.keras.engine.functional.Functional object at 0x7f40d2908c50>\r\n> ```\r\n", "As a TensorFlow user, I concur with @jamlong as to the fact that the identified behavior is indeed an issue. I encounter it both with TF 2.3 and TF 2.4 (using Python 3.6 and 3.8 on various Linux-based systems).\r\n\r\nI have implemented a custom Model (let's call it `CustomModel`) subclass to override some Functional behaviors (namely, to implement gradient accumulation and hack loss/metrics containers into supporting RaggedTensor inputs), and run into the exact same issue: in spite of it being registered (and even if I explicitly pass a `custom_objects` dict to reference it), depending on whether such a `CustomModel` has already been instantiated in the current thread, de-serialization from a save file and/or config dict will either produce a `FunctionalModel` or `CustomModel`.\r\n\r\nThis causes serious issues as depending on the restored class, the model will or will not implement behaviors that are absolutely necessary for my code to run. At the moment I am considering using a ugly hack that involves instantiating a `CustomModel` based on reloaded layers, optimizer, etc. gatherd from the `FunctionalModel` when the issue happens, but this is impractical since I would basically have to write such a hack for each and every of my custom Model classes - and I have quite a few - and would add unwanted elements to the computation graph, which is not nice for TensorBoard visualization (although this is rather minor).", "Hi all--\r\n\r\nThanks for all your thoughtful comments.\r\n\r\nAn update:\r\n(1) Re @jamlong 's issue, I believe it is now fixed in the latest tf-nightly, see the link here: https://colab.research.google.com/gist/monicadsong/af9b7085f97d00b94894a460daa78a33/45781-tf-nightly.ipynb\r\n\r\n(2) @ds-steventondeur: reloading a subclass Functional model should work now in `tf-nightly` without fail.\r\n\r\n(3) @pandrey-fr: this issue is now fixed in `tf-nightly` and will be in TF 2.5. If you would like to continue using your current version of TF without updating, you are correct in that will need to instantiate a `CustomModel` in your current thread. Maybe `tf.compat.v1.reset_default_graph` will clear these these dummy models from the computational graph.\r\n\r\nYou can also try doing something like: \r\n```\r\n# Change CustomModel to inherit from `Functional`\r\nCustomModel.__bases__ = (tensorflow.python.keras.engine.functional.Functional,)\r\n```\r\n\r\nI am closing this issue now, but please reopen if I overlooked something!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45781\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45781\">No</a>\n", "Hi @monicadsong, thank you very much for your answer!\r\nI will try and implement your suggested fix while waiting for TF 2.5 to land :)\r\n"]}, {"number": 45780, "title": "Extract a function for parsing op SPACE_TO_BATCH_ND", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nThis PR-1 in the effort to port operator SPACE_TO_BATCH_ND\r\nfrom lite to micro, as tracked in #45765. ", "comments": ["Withdrawing this PR, as it was a duplicate of work being performed under #45693."]}, {"number": 45779, "title": "Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED", "body": "**System information**\r\n- OS: Windows 10\r\n- TensorFlow installed from (source or binary):  \"pip install tensorflow\"\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.6.8\r\n- Installed using: pip\r\n- CUDA/cuDNN version: CUDA 11.0, cuDNN v8.0.4 (September 28th, 2020) for CUDA 11.0\r\n- GPU model and memory: NVidia RTX2080Ti 11gb\r\n\r\nHi, \r\n\r\nI'm having a problem installing the new version of TF and some help would be greatly appreciated\r\n\r\nWhen I launch TensorFlow, everything works as expected:\r\n\r\n`2020-12-17 02:38:06.341560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-17 02:38:08.362868: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-12-17 02:38:08.363763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-17 02:38:08.393157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-12-17 02:38:08.393327: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-17 02:38:08.402693: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-17 02:38:08.402832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-17 02:38:08.408295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-17 02:38:08.410941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-17 02:38:08.420106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-17 02:38:08.424734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-17 02:38:08.426865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-17 02:38:08.427022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n2020-12-17 02:38:08.443738: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-17 02:38:08.444619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-12-17 02:38:08.444716: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-17 02:38:08.445101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-17 02:38:08.446152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-17 02:38:08.446937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-17 02:38:08.447301: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-17 02:38:08.447647: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-17 02:38:08.448059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-17 02:38:08.448403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-17 02:38:08.448795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-12-17 02:38:08.969074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-17 02:38:08.969175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2020-12-17 02:38:08.970017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2020-12-17 02:38:08.970724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9416 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-12-17 02:38:08.971475: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set`\r\n\r\nBut then, when I begin training the model:\r\n\r\n`2020-12-17 02:38:10.285052: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-12-17 02:38:10.484223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-17 02:38:10.925209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-17 02:38:10.929739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-17 02:38:11.425989: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-12-17 02:38:11.426101: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-12-17 02:38:11.428154: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2020-12-17 02:38:11.428256: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2020-12-17 02:38:11.428655: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:697 : Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\nTraceback (most recent call last):\r\n  File \"style.py\", line 226, in <module>\r\n    combination_image, base_image, style_reference_image\r\n  File \"C:\\Users\\Jordan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\Jordan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 895, in _call\r\n    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\Jordan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Users\\Jordan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\Jordan\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n         [[node model/block1_conv1/Relu (defined at style.py:159) ]] [Op:__inference_compute_loss_and_grads_1267]\r\n\r\nFunction call stack:\r\ncompute_loss_and_grads`\r\n\r\n\r\n\r\nI used to use older versions of tensorflow-gpu on my GTX980 but had to update for my new RTX2080Ti, I've installed the CUDA and cuDNN versions noted here: https://www.tensorflow.org/install/gpu\r\n\r\nUnfortunately using this combination doesn't seem to work, help would be greatly appreciated. The above errors occur when attempting to run the Keras style transfer example https://keras.io/examples/generative/neural_style_transfer/ during the final block of code:\r\n`for i in range(1, iterations + 1):\r\n\tloss, grads = compute_loss_and_grads(\r\n\t\tcombination_image, base_image, style_reference_image\r\n\t)\r\n\toptimizer.apply_gradients([(grads, combination_image)])\r\n\tprint(i)\r\n\tif i % 100 == 0:\r\n\t\tprint(\"Iteration %d: loss=%.2f\" % (i, loss))\r\n\t\timg = deprocess_image(combination_image.numpy())\r\n\t\tfname = result_prefix + \"_at_iteration_%d.png\" % i\r\n\t\tkeras.preprocessing.image.save_img(fname, img)`\r\n\r\nThanks,\r\nJordan", "comments": ["GitHub is breaking lines so I've pasted an easier to read console output here: https://pastebin.com/7Fu9Kezs\r\n\r\nAfter \"Training...\" is printed is when the model begins to train and the errors occur", "@jordan-bird,\r\nPlease try setting a hard limit on the total GPU memory as shown in [this guide](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if it helps. \r\n\r\n> GitHub is breaking lines so I've pasted an easier to read console output here\r\n\r\nAnd to preserve the formatting in GitHub please add ``` (i.e. three back ticks) before and after the error log.\r\n\r\nThanks!", "Thank you. Works like a charm! \r\n\r\nFor anyone who also comes across this problem, the first solution listed on the guide linked by @amahendrakar produced the same error messages as in my OP but the second block of code below it fixed the issues and the training is now being performed by the GPU :\r\n\r\n```gpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45779\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45779\">No</a>\n", "Thank you soo much!!!! this solution worked perfectly \ud83d\udc4d ", "I have the same problem and I have only one GPU. I tried all 3 solutions recommended in the link and none of them worked. I still suffer from memory allocation problems. Here is the error I get:\r\n\r\n `2021-03-12 00:28:29.804317: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 3 Chunks of size 58491904 totalling 167.35MiB\r\n2021-03-12 00:28:29.815991: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 59466752 totalling 56.71MiB\r\n2021-03-12 00:28:29.816114: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 74294272 totalling 70.85MiB\r\n2021-03-12 00:28:29.816233: I tensorflow/core/common_runtime/bfc_allocator.cc:1036] 1 Chunks of size 122827776 totalling 117.14MiB\r\n2021-03-12 00:28:29.816355: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 1002.91MiB\r\n2021-03-12 00:28:29.816469: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 1073741824 memory_limit_: 1073741824 available bytes: 0 curr_region_allocation_bytes_: 2147483648\r\n2021-03-12 00:28:29.816722: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: \r\nLimit:                      1073741824\r\nInUse:                      1051624448\r\nMaxInUse:                   1073741824\r\nNumAllocs:                      825054\r\nMaxAllocSize:                569103872\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2021-03-12 00:28:29.817160: W tensorflow/core/common_runtime/bfc_allocator.cc:441] *********************************************_************************************x***************xx\r\n2021-03-12 00:28:29.817274: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at fused_batch_norm_op.cc:1448 : Resource exhausted: OOM when allocating tensor with shape[8,32,237,237] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\baran\\.conda\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\baran\\.conda\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1359, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"C:\\Users\\baran\\.conda\\envs\\tf2.4\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1451, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[8,56,237,237] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node batch_normalization_3/FusedBatchNorm}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n`", "@baranaldemir,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 45778, "title": "[Doc bug] tf.sparse.concat doesn't have argument `expand_nonconcat_dim`", "body": "\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/sparse/concat\r\n\r\n## Description of issue (what needs changing):\r\nThe signature of API  `tf.sparse.concat ` doesn't have argument `expand_nonconcat_dim`\r\n![image](https://user-images.githubusercontent.com/24580222/102425051-90d17a80-3fda-11eb-88fb-8da8d44f381c.png)\r\n\r\nBut in the Argument section, it describes argument `expand_nonconcat_dim`\r\n![image](https://user-images.githubusercontent.com/24580222/102425085-a34bb400-3fda-11eb-8d57-5834a8a9f9a7.png)\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.3.0\r\n- **Python version**: 3.7.6", "comments": ["Added a PR #46040 for the fix."]}, {"number": 45776, "title": "Update version numbers for TensorFlow 1.15.5", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 1 -> 1\nMinor: 15 -> 15\nPatch: 4 -> 5\n\nNo lingering old version strings \"1.15.4\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"1.15.4\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 45775, "title": "Update version numbers for TensorFlow 2.1.3", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 1 -> 1\nPatch: 2 -> 3\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.2\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py.orig:50:2.1.2\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.1.2\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py.orig:50:2.1.2\n```", "comments": []}, {"number": 45774, "title": "Update version numbers for TensorFlow 2.3.2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 3 -> 3\nPatch: 1 -> 2\n\nNo lingering old version strings \"2.3.1\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.3.1\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 45773, "title": "Update version numbers for TensorFlow 2.2.2", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 2 -> 2\nPatch: 1 -> 2\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.1\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/java/maven/proto/pom.xml:37:2.2.1\ntensorflow/java/maven/libtensorflow/pom.xml:29:2.2.1\ntensorflow/java/maven/spark-tensorflow-connector/pom.xml:196:2.2.1\ntensorflow/java/maven/tensorflow-hadoop/pom.xml:58:2.2.1\ntensorflow/workspace.bzl:521:2.2.1\ntensorflow/workspace.bzl:523:2.2.1\ntensorflow/workspace.bzl:524:2.2.1\ntensorflow/core/profiler/convert/trace_events_to_json_test.cc:77:2.2.1\ntensorflow/core/profiler/convert/trace_events_to_json_test.cc:112:2.2.1\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.2.1\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/java/maven/proto/pom.xml:37:2.2.1\ntensorflow/java/maven/libtensorflow/pom.xml:29:2.2.1\ntensorflow/java/maven/spark-tensorflow-connector/pom.xml:196:2.2.1\ntensorflow/java/maven/tensorflow-hadoop/pom.xml:58:2.2.1\ntensorflow/workspace.bzl:521:2.2.1\ntensorflow/workspace.bzl:523:2.2.1\ntensorflow/workspace.bzl:524:2.2.1\ntensorflow/core/profiler/convert/trace_events_to_json_test.cc:77:2.2.1\ntensorflow/core/profiler/convert/trace_events_to_json_test.cc:112:2.2.1\n```", "comments": []}, {"number": 45772, "title": "Update version numbers for TensorFlow 2.0.4", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 0 -> 0\nPatch: 3 -> 4\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.3\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:93:2.0.3\ntensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.xml:1:2.0.3\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.0.3\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/python/tpu/profiler/capture_tpu_profile.py:93:2.0.3\ntensorflow/contrib/verbs/verbs_with_0_copies_phase1_protocol.xml:1:2.0.3\n```", "comments": []}, {"number": 45771, "title": "Floating point exception in tf.truncatemod when x is boundary value", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\nFloating point exception in `tf.truncatemod` when `x` is a boundary value of int64\r\n\r\n**Describe the expected behavior**\r\nexpect no crash \r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\ntf.truncatemod(x=-9223372036854775808, y=[-1])\r\n~~~\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f05e9b4020b3aca1d08aa3c65f089f3b/45771-2-4.ipynb). Thanks!", "@DNXie Closing the issue since it was resolved in TF-Nightly version(2.6). Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/cf44b74f16ec85f49f6be35373f82bef/untitled90.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45771\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45771\">No</a>\n"]}, {"number": 45770, "title": "Segmentation fault in tf.histogram_fixed_width", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.histogram_fixed_width` crashes (segmentation fault) when `values` contain nan\r\n\r\n**Describe the expected behavior**\r\nExpect no crash \r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.histogram_fixed_width(values=np.nan, value_range=[1,2])\r\n~~~\r\n\r\n", "comments": ["I have tried in colab with TF 2.3, nightly version(`2.5.0-dev20201216`) and was able to reproduce the issue.Please,find the gist [here](https://colab.research.google.com/gist/ravikyram/c73c149a0723b67cec2772459f542a73/untitled582.ipynb). Thanks!", "Does this also reproduce with TF 2.4?", "@mihaimaruseac \r\n\r\nI have tried in colab with TF 2.4 and i was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e36fe89c33a3477c484b62248b30a02d/untitled584.ipynb). Thanks!", "Looks like `values` don't expect `nan` values. Should I open a PR that raises an error if there are `nan` in `values`?", "I see that the example crashes if executed on cpu with TF 2.4 where as on gpu execution with TF 2.4 it passes with following output.\r\n```python\r\n<tf.Tensor: shape=(100,), dtype=int32, numpy=\r\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>\r\n```", "It is still replicating in CPU mode for [TF 2.8](https://colab.sandbox.google.com/gist/mohantym/0f41a7f797bf930813d8383dc1d955a3/github_45770.ipynb#scrollTo=F8ttmtF-X2zd).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45770\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45770\">No</a>\n"]}, {"number": 45769, "title": "Update release notes for TensorFlow 2.3.2", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.3.2\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 45768, "title": "Update release notes for TensorFlow 2.2.2", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.2.2\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 45767, "title": "Update release notes for TensorFlow 2.0.4", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.0.4\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 45766, "title": "Update release notes for TensorFlow 2.1.3", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.1.3\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": []}, {"number": 45765, "title": "micro: port op SPACE_TO_BATCH_ND from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator SPACE_TO_BATCH_ND from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan in the style of #45306:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro without making any changes or including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test", "comments": ["There is already work in progress on this issue in #45693, maybe you can contribute there if you want?", "> There is already work in progress on this issue in #45693, maybe you can contribute there if you want?\r\n\r\nOops, thanks for pointing this out, @stephanboner. I'll leave this op to you. Please let me know if you want any help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45765\">No</a>\n", "> > There is already work in progress on this issue in #45693, maybe you can contribute there if you want?\r\n> \r\n> Oops, thanks for pointing this out, @stephanboner. I'll leave this op to you. Please let me know if you want any help.\r\n\r\nI will, thank you!"]}]