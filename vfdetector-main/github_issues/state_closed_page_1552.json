[{"number": 6360, "title": "Locking mechanisms", "body": "Especially when integrating TensorFlow into an exiting multi-threaded application, it's not always easy to use queues for synchronization. Currently, we must use Python locks to lock the `sess.run(...)` calls from different threads. Exposing a TensorFlow lock interface could allow to synchronize access only to needed values of the session:\r\n\r\n```python\r\nx = tf.placeholder(tf.float32, [None, 784])\r\ny = tf.placeholder(tf.float32, [None, 10])\r\nW = tf.Variable(tf.zeros([784, 10]))\r\nb = tf.Variable(tf.zeros([10]))\r\n\r\nlock = tf.Lock()\r\nwith lock:\r\n  pred = tf.nn.softmax(tf.matmul(x, W) + b)\r\nloss = -tf.reduce_mean(tf.reduce_sum(y * tf.log(pred), 1))\r\nwith lock:\r\n  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\r\n\r\ndef inference_thread():\r\n  while True:\r\n    # Generate data...\r\n    sess.run(pred, data)\r\n\r\ndef training_thread():\r\n  while True:\r\n    # Generate data...\r\n    sess.run(train_step, data)\r\n```\r\n\r\nOr simpler:\r\n\r\n```python\r\nwith tf.Lock():\r\n  W = tf.Variable(tf.zeros([784, 10]))\r\n# ...\r\n```", "comments": ["Why do you need to use locking? Session is designed to support multiple\n.run calls in parallel. For instance, Supervisor and queue_runners launch\nenqueue ops in parallel with main training loop without locking.\n\nThere are some special cases -- ops like tf.assign_add have \"read\nvariable\"/\"update variable\" parts. If you don't want another thread to\nmodify the value between these two steps, there's\n tf.assign_add(...,use_locking=True) option\n\nOn Fri, Dec 16, 2016 at 5:45 AM, Danijar Hafner <notifications@github.com>\nwrote:\n\n> Especially when integrating TensorFlow into an exiting multi-threaded\n> application, it's not always easy to use queues for synchronization.\n> Currently, we must use Python locks to lock the sess.run(...) calls.\n> Exposing a TensorFlow lock interface could allow to synchronize access only\n> to needed values of the session:\n>\n> x = tf.placeholder(tf.float32, [None, 784])\n> y = tf.placeholder(tf.float32, [None, 10])W = tf.Variable(tf.zeros([784, 10]))\n> b = tf.Variable(tf.zeros([10]))\n>\n> lock = tf.Lock()with lock:\n>   pred = tf.nn.softmax(tf.matmul(x, W) + b)\n> loss = -tf.reduce_mean(tf.reduce_sum(y * tf.log(pred), 1))with lock:\n>   train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n> def inference_thread():\n>   while True:\n>     # Generate data...\n>     sess.run(pred, data)\n> def training_thread():\n>   while True:\n>     # Generate data...\n>     sess.run(train_step, data)\n>\n> Or even simpler:\n>\n> with tf.Lock():\n>   W = tf.Variable(tf.zeros([784, 10]))# ...\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6360>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHHDatqz7ifXWt50kVKAhv3hGhl21ks5rIpXrgaJpZM4LPMvQ>\n> .\n>\n", "So access to variables is already protected by a lock that allows multiple reads or one write? My understanding was that assigning to a variable while using it in a computation from another `sess.run()` can cause interference.", "The lock is activated by `use_locking` option. Here's a simple experiment to confirm -- https://gist.github.com/yaroslavvb/3a53132dcddc8ff47e4e919579b98c6a\r\n\r\nIf you turn on `use_locking`, then each read takes at least as long as each write. If you turn it off, the reads are near instant.", "Okay, thanks for putting together an example. Seems like we might not need locking primitives then. One more question, without `use_locking` are the reading threads guaranteed to read consistent versions of the variable? By consistent, I mean that they are either not yet updated or already fully updated.", "No -- in the example I gave without `use_locking`, the writes take >1 seconds each, while reads take 0.0005 seconds, so they must be looking at partially updated states. ", "Actually, it seems some scenarios are not covered by `use_locking=True`.\r\n\r\nIf `a.assign_add(b, use_locking=True)` gets evaluated while `c.assign(a, use_locking=True)` is executing, then `c` will end up with partially updated value.\r\n\r\nUpdated gist with experiment \r\nhttps://gist.github.com/yaroslavvb/3a53132dcddc8ff47e4e919579b98c6a\r\n", "You're right, here is a slightly more compact example to reproduce the issue. The writer thread increases the elements of the matrix exponentially using locking. The reader thread checks for all elements of the matrix being identical at any point, which fails. Isn't this a bug in TensorFlow's `use_locking`?\r\n\r\n```python\r\nimport threading\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nvariable = tf.Variable(np.ones((100, 100)), trainable=False)\r\nwrite_op = variable.assign_add(variable, use_locking=True)\r\nsess = tf.Session()\r\nrunning = True\r\n\r\ndef writing():\r\n  global running\r\n  for _ in range(1000):\r\n    sess.run(write_op)\r\n    time.sleep(0.001)\r\n  running = False\r\n\r\ndef reading():\r\n  while running:\r\n    value = sess.run(variable)\r\n    assert (value == value.flat[0]).all()  # This fails!\r\n\r\nif __name__ == '__main__':\r\n  sess.run(tf.global_variables_initializer())\r\n  threading.Thread(target=writing).start()\r\n  threading.Thread(target=reading).start()\r\n```", "Hm, looks like `use_locking` sets a write-lock rather than read lock? cc @mrry for his expertise", "Exactly, but it seems quite counter-intuitive to me. I'd expect a write lock to block both reads and writes. There are some people on Github who tried to implement A3C (Mnih et al.; 2016) and ran into this issue. They ended up using processes and aggregating updates on the Python side with Numpy. Would be interesting to hear an opinoin from @mrry.", "At present, all TensorFlow variables (creating using the `tf.Variable` API) are associated with an [individual mutex](https://github.com/tensorflow/tensorflow/blob/2e22f1b20fdfa77b1332c518617391dc32359c5b/tensorflow/core/kernels/variable_ops.h#L33), which is acquired when that variable is the lvalue input to one of the assignment ops (like `\"Assign\"`) or one of the training ops that updates a set of related variables (like `\"ApplyGradientDesign\"`), **and** `use_locking = true` is set on that op.\r\n\r\nIn principle these locks could protect reads as well (e.g. if there were a `\"Snapshot\"` op that took a copy of the variable under the lock), but this doesn't happen in practice because TensorFlow automatically converts variable \"reference tensors\" to `Tensor` values, and regular `Tensor` values don't have a mutex, so the ops that operate on them aren't able to acquire the lock to protect reading. \r\n\r\nSince this is a sad state of affairs, we've been revising the memory model for variables, so that reads are [explicit](https://github.com/tensorflow/tensorflow/blob/2e22f1b20fdfa77b1332c518617391dc32359c5b/tensorflow/core/ops/resource_variable_ops.cc#L55) (and safely protected by a mutex), and values don't alias mutable buffers. This is still a work in progress, but @michaelisard or @alextp could comment more.", "@mrry Thanks for the update. What do you think about explicit locking like the example in the first post?", "@alextp has been leading the design and implementation of the new variables and can give you details about status and immediate future plans.\r\n\r\nWe have in the past discussed high-level locks like the one you describe, but we have struggled to come up with the use cases to help design what is required. We in general like to keep as much of TensorFlow as possible functional, because it simplifies automatic optimizations, and placement onto a variety of devices. The most common original use case for variables was asynchronous updates, which by their nature are expected to be somewhat racy. We have talked about putting support in the graph for \"short\" transactional updates of multiple variables, but for your initial example where you hold a lock around an entire training step it might make more sense to do higher-level serialization from the session API, as you are doing currently.\r\n\r\nWe would definitely be happy to hear more details about what you are trying to achieve, to help us plan!", "hi @mrry , you said \r\n\r\n> ...because TensorFlow automatically converts variable \"reference tensors\" to Tensor values...\r\n\r\nI don't quite understand what that means. Could you give an example to show such \"automatical conversion\"? And could you please explain what is \"reference tensors\"? Thanks!", "Here's a simple example that appears quite frequently: the `tf.identity()` removes the ref-ness from its input, but doesn't make a copy, so the result, which has `tf.float32`, is actually a mutable buffer(!):\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> v = tf.Variable(37.0)\r\n>>> v.dtype\r\ntf.float32_ref\r\n>>> id_v = tf.identity(v)\r\n>>> id_v.dtype\r\ntf.float32\r\n```", "thanks for your kind response @mrry. But could you please explain more about in which cases we might need to remove the ref-ness from input? I have found no docs introducing the differences of `reference` and `non-reference` `dtype`, thus I have no idea when I should use which one of them.\r\n\r\nThanks for your time!", "@AIROBOTAI you shouldn't have to worry about keeping / removing refness from inputs. While now it's not currently true that it's safe to treat all tensors as constant, I'm working on changing the system to make this true, so the notion of ref won't be necessary.\r\n\r\nThat said, removing ref from inputs is not something you do; the type system allows ref tensors to be treated as non-ref tensors, but not the other way around (so you cannot assign to a constant, but can use the value of a variable in any operation you'd like).", "Wow, cool! Thanks @alextp! Also, sorry for deviating from the topic of this issue :-P", "@michaelisard so turns out @danijar and I were both working on an A3C, and have similar use-case -- use-case is for `Snapshot` type op that lets you obtain a variable between updates. More specifically, our A3C implementation in https://github.com/openai/universe-starter-agent has agents trying to make prediction using parameters stored on a parameter server, which may be in the process of being updated by other agents, hence inconsistent. We'd like to be able to get a consistent version of parameters. A complication is that network parameters are typically split over several variables. \r\n\r\nSo maybe something like this, when updating params stored on parameter server.\r\n\r\n`ops.apply_gradients(grads_and_vars, snapshot_vars)\r\n`\r\n\r\nThen on workers, when transferring central parameters to a local copy. \r\n`tf.copy_snapshots(snapshot_vars, local_vars)`\r\n\r\nThe last op would be guaranteed to place a consistent version of global parameter variables into local parameter variables.", "Yes this is a use case that we thought about when @alextp was designing the new variable implementation, and will be relatively straightforward. I don't know the timeframe but the idea is that a lot of this functionality will be filled in after TF 1.0 is out.", "@alextp Okay cool. Will this change the last paragraph of the `tf.Variable.value()` docs? I think that's the unexpected behavior we ran into for A3C. It's definitely not safe behavior when running sessions from multiple threads.\r\n\r\n> tf.Variable.value()\r\n>\r\n> Returns the last snapshot of this variable.\r\n>\r\n> You usually do not need to call this method as all ops that need the value of the variable call it automatically through a convert_to_tensor() call.\r\n>\r\n> Returns a Tensor which holds the value of the variable. You can not assign a new value to this tensor as it is not a reference to the variable. See ref() if you want to get a reference to the variable.\r\n>\r\n> To avoid copies, if the consumer of the returned value is on the same device as the variable, this actually returns the live value of the variable, not a copy. Updates to the variable are seen by the consumer. If the consumer is on a different device it will get a copy of the variable.", "Instead there will be a sister class to variable (now named\nResourceVariable, already in the codebase but hidden as it's still\nincompatible with a lot of things) with different behavior. It will have a\nRead op where a Read is guaranteed to see the result of all writes on which\nit depends and to not see the results of writes which depend on it. It'll\nalso be possible to add operations with stronger contracts, such as the\ngroup snapshot read Yaroslav mentioned (which I might add from the start).\n\nOn Tue, Jan 10, 2017 at 3:57 PM, Danijar Hafner <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Okay cool. Will this change the last\n> paragraph of the tf.Variable.value() docs? I think that's the unexpected\n> behavior we ran into for A3C. It's definitely not safe behavior when\n> running sessions from multiple threads.\n>\n> tf.Variable.value()\n>\n> Returns the last snapshot of this variable.\n>\n> You usually do not need to call this method as all ops that need the value\n> of the variable call it automatically through a convert_to_tensor() call.\n>\n> Returns a Tensor which holds the value of the variable. You can not assign\n> a new value to this tensor as it is not a reference to the variable. See\n> ref() if you want to get a reference to the variable.\n>\n> To avoid copies, if the consumer of the returned value is on the same\n> device as the variable, this actually returns the live value of the\n> variable, not a copy. Updates to the variable are seen by the consumer. If\n> the consumer is on a different device it will get a copy of the variable.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6360#issuecomment-271736413>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZrtuYPX1_IsBHJq3N_nxDS3iR-3ks5rRBsHgaJpZM4LPMvQ>\n> .\n>\n\n\n\n-- \n - Alex\n", "BTW, there's another potential use-case for locking that came up [here](http://stackoverflow.com/questions/41920371/tensorflow-multi-threaded-queuerunner)\r\n\r\nYou have two aligned queues of images and labels. Then you have something like `f(images.dequeue(), labels.dequeue()`. If you run this function in two parallel run calls, the queues can get out of sync. A user-facing interface might be to have `f` be a Function node, and have a property `use_locking` which adds a lock on all of its inputs on start of execution, and releases a lock at the end.", "For queues I recommend using a single queue for both images and labels. This is why all queues support tuples of Tensors.\r\n\r\nWhat's the advantage of separate queues, if the enqueues and dequeues need to match, and so need any internal shuffling and whatnot?", "Not sure what the advantage is. I guess this is similar to the use-case that was mentioned earlier, you may have function `f(a,b)` which modifies variables `a`,`b` and you don't want that function to see `a`,`b` variable in a partially updated state another copy of `f` is being executed concurrently", "I have a different use case and/or question -- which may need another thread. I am trying to implement a custom op to run a L-BFGS solver. On the C++, I have a co-routine using threads: one runs the solver detached, the other manages the TF control. It operates inside a TF while_loop with the `compute` call serving as a defacto generator function to fetch data when needed. I can run this on a single node, but it seems to lock up the graph somewhere when I run it in distributed. Can I prevent another thread from trying to acquire a lock and/or can I manage the locks to update the variable when needed and release the lock otherwise?", "@benring, I need more information to understand what is going on. Can your\ncustom op be trying to access state on another machine or something?\n\nOn Mon, Mar 6, 2017 at 12:26 AM, benring <notifications@github.com> wrote:\n\n> I have a different use case and/or question -- which may need another\n> thread. I am trying to implement a custom op to run a L-BFGS solver. On the\n> C++, I have a co-routine using threads: one runs the solver detached, the\n> other manages the TF control. It operates inside a TF while_loop with the\n> compute call serving as a defacto generator function to fetch data when\n> needed. I can run this on a single node, but it seems to lock up the graph\n> somewhere when I run it in distributed. Can I prevent another thread from\n> trying to acquire a lock and/or can I manage the locks to update the\n> variable when needed and release the lock otherwise?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6360#issuecomment-284331206>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdynpYZsNle_AvPvFSTtSy8_xjA9ks5ri8NBgaJpZM4LPMvQ>\n> .\n>\n\n\n\n-- \n - Alex\n", "@benring the \"lock up the graph\" behavior would be a TensorFlow bug unless you introduced a circular dependency in the graph (like in https://github.com/tensorflow/tensorflow/issues/6961), or your graph is waiting for input.\r\n\r\nIf you can rule out the latter with session timeout, it might be worth filing a bug with reproducible example\r\n\r\n```\r\n  config = tf.ConfigProto()\r\n  config.operation_timeout_in_ms=50000   # terminate on long hangs\r\n  sess = tf.InteractiveSession(\"\", config=config)\r\n\r\n```", "I rechecked and this is not a distributed issue -- my single node version I tried yesterday used a different model. I tried the config setting and the session never times out. Does this imply a circular dependency in the graph? I can execute the custom op on a toy example, but am having a hard time reproducing the error to demonstrate it. Our model is pretty complex -- it's an extension of GPFlow -- so tracing the graph is no simple task. Is there  a way I can crank up the logging? Also: I am using TF r0.12 ", "Can you get a backtrace to see where it's stuck? (gdb attach, then `bt` or `thread apply all bt` for all threads)  You can get stuck for reasons unrelated to tensorflow, for instance, I've had a model stuck because a GPU op was calling Nvidia drivers, which was trying to store some temporary files on NFS which was hanging (solution was to set `CUDA_CACHE_PATH` to local directory). You can get more logging with `export TF_CPP_MIN_VLOG_LEVEL=1`, although it's going to be a deluge. Also, I'd recommend trying to reproduce with CPU-only.", "We finally resolved this. We had a tf.cond() buried in the objective function which caused our while_loop to hang. We wound up doing some re-factoring using the tf.where(). Is there a better op which can implement a simple conditional which accepts a single bool scalar (vice a bool mask) and returns a tensor?", "I think `tf.cond` is such conditional, it shouldn't be producing a hang, can you give a reproducible example so this could be fixed?", "Give me  bit to simplify an example. If/when -- shall I post it as a new issue?", "yes, new issue", "cc @yuanbyu to verify there are no known cases where executing `tf.cond` is supposed to cause a hang", "Periodically evaluating model performance on unseen test data while training is an important use case.  Any updates that occur to the model during testing invalidate the test's results.", "Is this issue ready to close?", "@alextp said something about plans for a mechanism to update a group of variables together, did anything happen with it?", "I think @ebrevdo might be taking this on. Assigning to him so he can confirm or deny....", "@danijar Indeed I'm working on a locking mechanism; we can talk about it offline.  There's an early design, and some code, but I'm not sure if the version I have in mind now will be the version we end up with in the end.  It relies on using ResourceVariables instead of regular Variables.  @sguada is also appraised on the situation.", "Note as of now, the locking mechanism is restricted to all ops and associated [Resource]Variables sitting on the same device.  And one initial limitation might be that it must be a CPU.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I don't need this feature myself anymore. But several people in this thread mentioned interest in it so I'm keeping this open. @ebrevdo can you please provide it update here once you have one?", "An initial implementation has just been pushed to tf.contrib.framework.  I'm adding a comprehensive python wrapper in the next few days.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ebrevdo Any updates, or ready to close?", "Yes; see `tf.contrib.framework.CriticalSection`."]}, {"number": 6359, "title": "Additional change to pull request #6014", "body": "Additional change to pull request #6014 ", "comments": ["Can one of the admins verify this patch?", "Please add the changes from #6014 here. Thanks!", "@martinwicke Done, 2 files together. ", "Jenkins, test this please.", "Thanks for the fix!"]}, {"number": 6358, "title": "Fix Java API Typo Errors", "body": "I think it is necessary to use capital letter when it refers to Object just like Tensor.", "comments": ["Can one of the admins verify this patch?", "Lots of typos in Tensorflow but those ones are not typos for me.", "Oh sorry I shoud not use the word 'typo',\r\nAre there any reasons why some of comments are 'Tensor' and others are 'tensor'?", "Thanks for the contribution! Closing because the cases are intentional there. If you strongly oppose, please reopen with a comment explaining your reasoning."]}, {"number": 6357, "title": "TF for Embedded Devices", "body": "Has there been any effort to port the TensorFlow inference routines to lightweight cores commonly found in embedded devices (i.e. ARM Cortex M4F)?  Memory footprints are often small, but with efficient use of small datatypes, it seems that this could be fruitful.  Thanks.", "comments": ["There have been ports to Rasberry PI's that run full Linux stacks see https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md\r\n\r\nHowever, I am not aware of any ports to MCUs that run RTOS' or bare metal. There may be some features in the soon to be released XLA that help deploy individual inference routines derived from TensorFlow.\r\n\r\n@petewarden, do you have anything else to add?\r\n", "The work @samjabrahams is actively doing on bringing TensorFlow to Raspberry Pi seems like a satisfactory response to this question. If you would like to request TensorFlow for a specific platform, e.g. ARM Cortex M4F, or have the aforementioned project merged into TensorFlow, then please open a new issue. Or I can re-open and rename this one.", "Just saw this thread: I figured I may as well chip in a few bits of info:\r\n\r\nRegarding Makefile support:\r\n\r\n* If you're simply looking for any kind of inference support, the \"most officially supported\" thing to do is to try and modify the [Makefile instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile) to work on whatever system you're trying to build. This leaves you with a static library that can run your model.\r\n* You could try following the [Raspberry Pi Makefile tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples) to figure out the necessary flags to work on your system.\r\n* Once you've got a decent idea of how to compile, you could replace the pre-trained Inception model from the tutorial with whatever trained model you want to use \r\n    * You'll want to [freeze]() and [optimize]() your exported model so that it's contained in a single protobuf file (which is what the Inception model looks like)\r\n* All this said, I can't say for sure if ARMv7-M would compile successfully (I'm fairly confident ARMv6 doesn't have the right instructions to work properly with the current codebase)\r\n\r\nRegarding my repository:\r\n\r\n* If you want full TensorFlow functionality and/or the ability to run it from Python, you're going to have to get Bazel installed on whatever system you're running (the majority of the work done for my repo is figuring out how to compile new versions of Bazel)\r\n* You'll also want to figure out a reasonable cross-compilation setup, if possible. I'm still looking for a consistent way to avoid compiling everything directly on the Raspberry Pi\r\n* I'm a little behind on the current versions of TensorFlow/Bazel- holidays + vacation + work has led to much less time going toward the repo. Hopefully I can get it together by 1.0.0-final!", "[Bender](https://github.com/xmartlabs/Bender) supports running TensorFlow models on iOS. Maybe it's interesting for you.", "@samjabrahams great work on this. we could be interested to contribute more also as part of my course (@Oxford Data Science for IoT) - has there been more updates re the TF on Pi? also .. does this come under the TF embedded / mobile tensorflow umbrella? finally which use cases are you considering for Pi(ex image or beyond) many thanks! Ajit", "@dsiot There has been some work towards making official support in #5729 - that may be where you want to look next/continue the discussion!", "thanks for the prompt reply!"]}, {"number": 6356, "title": "Build errors for libtensorflow_inference.so", "body": "I am trying to build `libtensorflow_inference.so`, in order to run a TF model in an Android app.  I found the following command in issue #6166, however the command is failing.  I reported an earlier error message in that issue, but now the error message has changed and it fails completely.  \r\nIt is worth noting that I am able to compile the Inception demo app with no trouble (from inside Android Studio), so I think the full tool chain is functional.\r\nI added the verbose option to try to troubleshoot the issue (not that it helped):\r\n\r\n`$  bazel build //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a --verbose_failures\r\n`\r\nI am running on MacOS (10.12.1) with TF0.12 (SHA=46b7b6).\r\nInstalled version of CUDA and cuDNN: None\r\nBazel Verion:\r\n```\r\nBuild label: 0.4.2-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 8 03:17:26 2016 (1481167046)\r\nBuild timestamp: 1481167046\r\nBuild timestamp as int: 1481167046\r\n```\r\n\r\nHere is the error message:\r\n```\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\n<snip>\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nERROR: /Users/kevin/tensorflow/tensorflow/core/BUILD:163:1: C++ compilation of rule '//tensorflow/core:protos_all_cc' failed: sandbox-exec failed: error executing command \r\n  (cd /private/var/tmp/_bazel_kevin/a09094529daf7e22afa1fbacb9484a3a/bazel-sandbox/4c850042-6906-474b-9554-e958a1aadaeb-3/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Users/kevin/anaconda2/bin:/Users/kevin/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    TMPDIR=/var/folders/cm/1r6x6rbj7hn_51qvfjzj7nx80000gn/T/ \\\r\n  /usr/bin/sandbox-exec -f /private/var/tmp/_bazel_kevin/a09094529daf7e22afa1fbacb9484a3a/bazel-sandbox/4c850042-6906-474b-9554-e958a1aadaeb-3/sandbox.sb /bin/false -MD -MF bazel-out/stub_armeabi-v7a-fastbuild/bin/tensorflow/core/_objs/protos_all_cc/tensorflow/core/framework/device_attributes.pb.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-fastbuild/bin/tensorflow/core/_objs/protos_all_cc/tensorflow/core/framework/device_attributes.pb.pic.o' -fPIC -iquote . -iquote bazel-out/stub_armeabi-v7a-fastbuild/genfiles -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-fastbuild/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-fastbuild/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-fastbuild/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-unknown-warning-option -Wno-unused-but-set-variable -Wno-sign-compare -c bazel-out/stub_armeabi-v7a-fastbuild/genfiles/tensorflow/core/framework/device_attributes.pb.cc -o bazel-out/stub_armeabi-v7a-fastbuild/bin/tensorflow/core/_objs/protos_all_cc/tensorflow/core/framework/device_attributes.pb.pic.o).\r\nsandbox-exec: /bin/false: No such file or directory\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 44.632s, Critical Path: 2.86s\r\n```", "comments": ["BTW, I chose to generate `libtensorflow_inference.so` because I only need to compute an inference based on an existing graph, from within an Android app.  The app is to take a 128 element numeric vector and classify a result to one of six classes.  Is this the correct .so to generate?", "@kevinashaw Yes, this is the library you want, along with the corresponding Java src that //tensorflow/contrib/android:android_tensorflow_inference_java will place in a Jar file for you.\r\n\r\nOn OSX false may be found in /usr/bin/, not /bin, though I'm not sure if that's the root cause of the problem. Can you try `bazel clean` and then rebuild with --sandbox_debug enabled? That may provide a more informative error.", "I used the `bazel clean` command and the .so file compiled on the first try.  Thank you.\r\n\r\nHowever, this lead me to the next question:\r\nWhere should I put the .so file so that Android Studio can find it?  \r\nI placed it in both `~project/libs` and `~project/src/app/src/main/jniLibs`, but I have been unable to get Android Studio / Gradle to recognize it.  For example, in the following code, `org.tensorflow` is not resolving:\r\n```\r\npackage com.foobar.tfdemo; \r\n\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n\r\npublic class TensorFlowClassifier implements Classifier {\r\n// do something\r\n}\r\n```\r\nI'm wondering if there is some special location or process for bringing the .so files in to a project?\r\n I've tried to fully document the issue in SO: [http://stackoverflow.com/questions/41176321/tensorflow-android-studio-and-bazel-setting-up-a-new-project](url)\r\nThanks for the help! \r\n", "@kevinshaw You can try using [build.gradle](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/build.gradle) to have AS automatically build and install the native libs with gradle (calling out to bazel).\r\n\r\nIf you're doing it manually the .so will probably need to go in libs/armeabi-v7a. You'll also need to copy over the Jar produced from building //tensorflow/contrib/android:android_tensorflow_inference_java, or alternatively just copy over the Java file itself.", "Closing; please update if there are any remaining issues.\r\n\r\nIt's also worth mentioning that we are planning on providing an AAR solution soon that should make integrating TF into Android apps in Android Studio much simpler if you do not need to modify native C/C++ code.", "I have a similar problem on OSX when running\r\n\r\n`bazel build //tensorflow/contrib/android:libtensorflow_inference.so    --rosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a --sandbox_debug --verbose_failures`\r\n\r\nI get the following error\r\n\r\n```\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_jean-baptistebuisson/230f1e1bd98ef893dd2fae3f5b464a27/external/protobuf/BUILD:73:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: false failed: error executing command \r\n  (cd /private/var/tmp/_bazel_jean-baptistebuisson/230f1e1bd98ef893dd2fae3f5b464a27/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Users/jean-baptistebuisson/anaconda/bin:/Library/Frameworks/Python.framework/Versions/3.6/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n    TMPDIR=/var/folders/cz/w7p3l65n35g2dd96x3pdfm1h0000gn/T/ \\\r\n  /bin/false -MD -MF bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/generated_message_util.pic.d '-frandom-seed=bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/generated_message_util.pic.o' -fPIC -iquote external/protobuf -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/bazel_tools -isystem external/protobuf/src -isystem bazel-out/stub_armeabi-v7a-py3-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -DHAVE_PTHREAD -Wall -Wwrite-strings -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -c external/protobuf/src/google/protobuf/generated_message_util.cc -o bazel-out/stub_armeabi-v7a-py3-opt/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/generated_message_util.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nprocess-wrapper: execvp(\"/bin/false\", ...): No such file or directory\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 9.860s, Critical Path: 0.63s\r\n```\r\n\r\nAnd bazel clean did not help", "Hi,\r\n\r\nI am having the same issue....The bazel build \r\n\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a\r\n\r\nruns for a very long time and throws up error :(\r\nbazel clean doesnt help...\r\n\r\nCan some one please help....\r\n![screen shot 2017-05-07 at 9 06 34 pm](https://cloud.githubusercontent.com/assets/25388406/25797880/b98aefcc-33e7-11e7-9523-5d476a504f89.png)\r\n![screen shot 2017-05-08 at 10 49 40 am](https://cloud.githubusercontent.com/assets/25388406/25797881/b98bf87c-33e7-11e7-8434-49c77fb89d4c.png)\r\n", "@azainab What is your actual error? It's not included in the screenshots you posted.", "It runs for a whole very long time and has a big list of lines...Is there a way I can see the 3 errors? I tried the --verbose_failures but it says 0 targets found....\r\n\r\nIf yes, I run a command to see the errors and post it...\r\n\r\nFew warnings and errors I got....\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:devices.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:devices.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:grappler_item.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:grappler_item.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:op_types.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:op_types.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler:utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/clusters:cluster.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/clusters:cluster.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/inputs:utils.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/inputs:utils.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:auto_parallel.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:auto_parallel.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:constant_folding.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:constant_folding.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_rewriter.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:graph_rewriter.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:layout_optimizer.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:layout_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:memory_optimizer.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:memory_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:meta_optimizer.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:meta_optimizer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:model_pruner.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/core/BUILD:917:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/grappler/optimizers:model_pruner.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\n\r\nINFO: From Compiling external/protobuf/src/google/protobuf/compiler/js/embed.cc [for host]:\r\nexternal/protobuf/src/google/protobuf/compiler/js/embed.cc:37:12: warning: unused variable 'output_file' [-Wunused-const-variable]\r\nconst char output_file[] = \"well_known_types_embed.cc\";\r\n           ^", "@azainab These are warnings, not the error, which would be just above the section from your screenshot.\r\n\r\nAre you using NDK r12b?", "I am using r14b...\r\n![screen shot 2017-05-08 at 5 39 28 pm](https://cloud.githubusercontent.com/assets/25388406/25809376/693875ac-3415-11e7-864f-028661ab4214.png)\r\n", "That's the problem then. We recommend r12b as r14b currently has incompatibilities with TF due to the different default flags used by Clang.", "Error I got...\r\n\r\nERROR: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/contrib/android/BUILD:29:1: C++ compilation of rule '//tensorflow/contrib/android:android_tensorflow_inference_jni' failed: Process exited with status 1 [sandboxed].\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:97:26: error: use of undeclared identifier 'AAsset_seek64'; did you mean 'AAsset_seek'?\r\n    off64_t new_offset = AAsset_seek64(asset.get(), offset, SEEK_SET);\r\n                         ^~~~~~~~~~~~~\r\n                         AAsset_seek\r\nexternal/androidndk/ndk/platforms/android-12/arch-arm/usr/include/android/asset_manager.h:96:7: note: 'AAsset_seek' declared here\r\noff_t AAsset_seek(AAsset* asset, off_t offset, int whence);\r\n      ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:98:22: error: use of undeclared identifier 'AAsset_getLength64'; did you mean 'AAsset_getLength'?\r\n    off64_t length = AAsset_getLength64(asset.get());\r\n                     ^~~~~~~~~~~~~~~~~~\r\n                     AAsset_getLength\r\nexternal/androidndk/ndk/platforms/android-12/arch-arm/usr/include/android/asset_manager.h:113:7: note: 'AAsset_getLength' declared here\r\noff_t AAsset_getLength(AAsset* asset);\r\n      ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:158:12: error: use of undeclared identifier 'AAsset_openFileDescriptor64'; did you mean 'AAsset_openFileDescriptor'?\r\n  int fd = AAsset_openFileDescriptor64(asset.get(), &start, &length);\r\n           ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           AAsset_openFileDescriptor\r\nexternal/androidndk/ndk/platforms/android-12/arch-arm/usr/include/android/asset_manager.h:126:5: note: 'AAsset_openFileDescriptor' declared here\r\nint AAsset_openFileDescriptor(AAsset* asset, off_t* outStart, off_t* outLength);\r\n    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:158:53: error: cannot initialize a parameter of type 'off_t *' (aka 'long *') with an rvalue of type 'off64_t *' (aka 'long long *')\r\n  int fd = AAsset_openFileDescriptor64(asset.get(), &start, &length);\r\n                                                    ^~~~~~\r\nexternal/androidndk/ndk/platforms/android-12/arch-arm/usr/include/android/asset_manager.h:126:53: note: passing argument to parameter 'outStart' here\r\nint AAsset_openFileDescriptor(AAsset* asset, off_t* outStart, off_t* outLength);\r\n                                                    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:173:14: error: use of undeclared identifier 'AAsset_getLength64'; did you mean 'AAsset_getLength'?\r\n    length = AAsset_getLength64(asset.get());\r\n             ^~~~~~~~~~~~~~~~~~\r\n             AAsset_getLength\r\nexternal/androidndk/ndk/platforms/android-12/arch-arm/usr/include/android/asset_manager.h:113:7: note: 'AAsset_getLength' declared here\r\noff_t AAsset_getLength(AAsset* asset);\r\n      ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:214:8: error: use of undeclared identifier 'AAsset_getLength64'; did you mean 'AAsset_getLength'?\r\n  *s = AAsset_getLength64(asset.get());\r\n       ^~~~~~~~~~~~~~~~~~\r\n       AAsset_getLength\r\nexternal/androidndk/ndk/platforms/android-12/arch-arm/usr/include/android/asset_manager.h:113:7: note: 'AAsset_getLength' declared here\r\noff_t AAsset_getLength(AAsset* asset);\r\n      ^\r\n6 errors generated.\r\nUse --strategy=CppCompile=standalone to disable sandboxing for the failing actions.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "Can you give more details? What ndk are you using? What command are you using to build?", "I posted the command in my previous posts..\r\n\r\nHere it is:\r\n\r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \r\n--crosstool_top=//external:android/crosstool \r\n--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \r\n--cpu=armeabi-v7a\r\n\r\nndk version - r14b.. As suggested I changed it to r12b and running it... It takes a really long time to run... ", "I ran using ndk r12b I got the error below:\r\n\r\nERROR: /Users/zuni/Documents/androidstudy/Juanjors_AndroidMeneame/tensorflow/contrib/android/BUILD:29:1: C++ compilation of rule '//tensorflow/contrib/android:android_tensorflow_inference_jni' failed: Process exited with status 1 [sandboxed].\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::{anonymous}::RandomAccessFileFromAsset::Read(tensorflow::uint64, size_t, tensorflow::StringPiece*, char*) const':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:97:69: error: 'AAsset_seek64' was not declared in this scope\r\n     off64_t new_offset = AAsset_seek64(asset.get(), offset, SEEK_SET);\r\n                                                                     ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:98:52: error: 'AAsset_getLength64' was not declared in this scope\r\n     off64_t length = AAsset_getLength64(asset.get());\r\n                                                    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::NewReadOnlyMemoryRegionFromFile(const string&, std::unique_ptr<tensorflow::ReadOnlyMemoryRegion>*)':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:158:68: error: 'AAsset_openFileDescriptor64' was not declared in this scope\r\n   int fd = AAsset_openFileDescriptor64(asset.get(), &start, &length);\r\n                                                                    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:173:44: error: 'AAsset_getLength64' was not declared in this scope\r\n     length = AAsset_getLength64(asset.get());\r\n                                            ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::GetFileSize(const string&, tensorflow::uint64*)':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:214:38: error: 'AAsset_getLength64' was not declared in this scope\r\n   *s = AAsset_getLength64(asset.get());\r\n                                      ^\r\nUse --strategy=CppCompile=standalone to disable sandboxing for the failing actions.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "The Tensorflow TF Detect for android example draws boxes around humans seen by the camera, however I would like to train a model to detect things other than humans.\ufeff", "```\r\nvagrant@vagrant:~/tensorflow$ ../.bazel/bin/bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" //tensorflow/contrib/android:libtensorflow_inference.so --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --verbose_failures\r\nWARNING: /home/vagrant/.cache/bazel/_bazel_vagrant/2608380002b7e82b3d07fef59e49e485/external/protobuf_archive/WORKSPACE:1: Workspace name in /home/vagrant/.cache/bazel/_bazel_vagrant/2608380002b7e82b3d07fef59e49e485/external/protobuf_archive/WORKSPACE (@com_google_protobuf) does not match the name given in the repository's definition (@protobuf_archive); this will cause a build error in future versions\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:batch_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_volume_patch.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /home/vagrant/tensorflow/tensorflow/core/BUILD:1525:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there\r\nINFO: Analysed target //tensorflow/contrib/android:libtensorflow_inference.so (29 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/vagrant/.cache/bazel/_bazel_vagrant/2608380002b7e82b3d07fef59e49e485/external/fft2d/BUILD.bazel:21:1: C++ compilation of rule '@fft2d//:fft2d' failed (Exit 1): false failed: error executing command \r\n  (cd /home/vagrant/.cache/bazel/_bazel_vagrant/2608380002b7e82b3d07fef59e49e485/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PWD=/proc/self/cwd \\\r\n  /bin/false -MD -MF bazel-out/armeabi-v7a-opt/bin/external/fft2d/_objs/fft2d/external/fft2d/fft/fftsg.pic.d -fPIC -iquote external/fft2d -iquote bazel-out/armeabi-v7a-opt/genfiles/external/fft2d -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -DSELECTIVE_REGISTRATION -DSUPPORT_SELECTIVE_REGISTRATION -c external/fft2d/fft/fftsg.c -o bazel-out/armeabi-v7a-opt/bin/external/fft2d/_objs/fft2d/external/fft2d/fft/fftsg.pic.o)\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 211.430s, Critical Path: 13.76s\r\nINFO: 190 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nI got this error even after cleaning the bazel.\r\nPlease anyone could help thanks.", "I cleaned bazel and executed the following command\r\n\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a --cxxopt=-std=c++11  --verbose_failures --sandbox_debug`\r\n\r\nit gave me the following error:\r\n\r\n`ERROR: /private/var/tmp/_bazel_root/23994111945cccf7235ecf4d9dc9fbce/external/com_google_absl/absl/numeric/BUILD.bazel:26:1: C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed (Exit 1): clang failed: error executing command \r\n  (cd /private/var/tmp/_bazel_root/23994111945cccf7235ecf4d9dc9fbce/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    ANDROID_BUILD_TOOLS_VERSION=28.0.3 \\\r\n    ANDROID_NDK_API_LEVEL=18 \\\r\n    ANDROID_NDK_HOME=/Users/ivlab/Downloads/android-ndk-r14b/ \\\r\n    ANDROID_SDK_API_LEVEL=28 \\\r\n    ANDROID_SDK_HOME=/Users/ivlab/library/Android/Sdk \\\r\n    BAZEL_USE_CPP_ONLY_TOOLCHAIN=1 \\\r\n    PATH=/Library/Frameworks/Python.framework/Versions/3.6/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/ivlab/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/Library/Frameworks/Python.framework/Versions/3.6/bin/python3 \\\r\n    PYTHON_LIB_PATH=/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_DOWNLOAD_CLANG=1 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -fno-integrated-as -target armv7-none-linux-androideabi '-march=armv7-a' '-mfloat-abi=softfp' '-mfpu=vfpv3-d16' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.d '-frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o' -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl '-std=c++11' -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-missing-field-initializers -Wno-sign-compare '--sysroot=external/androidndk/ndk/platforms/android-18/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/com_google_absl/absl/numeric/int128.cc -o bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/com_google_absl/absl/numeric/int128.cc:139:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w0 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-18/arch-arm/usr/include/math.h:284:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\nexternal/com_google_absl/absl/numeric/int128.cc:141:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w1 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-18/arch-arm/usr/include/math.h:284:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\nexternal/com_google_absl/absl/numeric/int128.cc:143:59: error: no member named 'trunc' in namespace 'std'; did you mean simply 'trunc'?\r\n  uint64_t w2 = static_cast<uint64_t>(static_cast<double>(std::trunc(v)));\r\n                                                          ^~~~~~~~~~\r\n                                                          trunc\r\nexternal/androidndk/ndk/platforms/android-18/arch-arm/usr/include/math.h:284:8: note: 'trunc' declared here\r\ndouble  trunc(double);\r\n        ^\r\n3 errors generated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 134.270s, Critical Path: 17.16s\r\nINFO: 235 processes: 235 local.\r\nFAILED: Build did NOT complete successfully`\r\n\r\nPlease help.\r\n", "> I posted the command in my previous posts..\r\n> \r\n> Here it is:\r\n> \r\n> bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so\r\n> --crosstool_top=//external:android/crosstool\r\n> --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n> --cpu=armeabi-v7a\r\n> \r\n> ndk version - r14b.. As suggested I changed it to r12b and running it... It takes a really long time to run...\r\n\r\nGoogle Should concentrate on Supporting Higher Version of NDK rather 14b ,\r\nNDK 19 and NDk20 compiler are more otimized and inference code runs much faster..\r\nThere is considerable slowness if you use older NDK( i.e 14b) .. but its very painful to compile with newer ndk as google official dont support them"]}, {"number": 6355, "title": "TensorBoard command unable to handle the path properly in Windows", "body": "If you try to save the graph in a specific path eg `C:\\documents`\r\n`writer = tf.train.SummaryWriter('C:/documents/my_graph', sess.graph)`\r\n\r\nand try to show the graph with TensorBoard\r\n\r\n`tensorboard --logdir=\"C:/documents/my_graph\"`\r\n\r\nTensorBoard will not find any graph because is unable to load the path and looks always for the graph files in the default user path` C:\\Users\\andrew\\mygraph\\` if I run console as user or `C:\\Windows\\System32` if I run console as administrator. This is a bug and should be fixed.\r\n\r\nTensorflow version 0.12.0rc1 pip installation\r\nOS: Windows 10 x64\r\n\r\nRelated Stackoverflow question\r\nhttp://stackoverflow.com/questions/41157645/tensorflow-tensorboard-on-windows-shows-a-blank-page?noredirect=1#comment69557752_41157645", "comments": ["@AndreaFar Have you tried enabling more debug output by passing --debug switch to tensorboard command? This may help in investigations :) ", "The problem has been fixed in the latest final release. Upgrade your rc (release candidate) version to final 0.12.0 ", "@AndreaFar  please confirm that @SMH17's suggestion to upgrade resolves your issues. Thanks!", "this issue has been fixed now I have another issue ", "We're glad to hear the issue is resolved. As for the other issue, that appears to be resolved in https://github.com/tensorflow/tensorflow/issues/6748."]}, {"number": 6354, "title": "fix cuda DNN lib path for mac", "body": "https://github.com/tensorflow/tensorflow/commit/ddedb9e094fe3c0e942fdf9d70c21d4df87f8c35 introduced an error providing a non-existent lib.\r\n\r\nFrom the Tensorflow docs:\r\nhttps://www.tensorflow.org/get_started/os_setup under the section: Optional: Setup GPU for Mac:\r\n\r\n> Finally, you will also want to install the CUDA Deep Neural Network (cuDNN v5) library which currently requires an Accelerated Computing Developer Program account. Once you have it downloaded locally, you can unzip and move the header and libraries to your local CUDA Toolkit folder:\r\n> \r\n> ```\r\n> $ sudo mv include/cudnn.h /Developer/NVIDIA/CUDA-8.0/include/\r\n> $ sudo mv lib/libcudnn* /Developer/NVIDIA/CUDA-8.0/lib\r\n> $ sudo ln -s /Developer/NVIDIA/CUDA-8.0/lib/libcudnn* /usr/local/cuda/lib/\r\n> ```\r\n\r\nThe cuDNN v5 library provided by Nvidia at https://developer.nvidia.com/rdp/cudnn-download, `cudnn-8.0-osx-x64-v5.1.tgz`, has `libcudnn.5.dylib`, not `libcudnn.5`. \r\n\r\n\r\nHere's the output of ./configure, run on a Macbook Pro 10.11.6:\r\n\r\nBefore (buggy) version that gets stuck on specifying the cuDNN path: \r\n\r\n```\r\n\u279c  tensorflow git:(master) ./configure\r\n~/os/tensorflow ~/os/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\r\n  /Library/Python/2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python]\r\n\r\nUsing python library path: /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]:\r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /Developer/NVIDIA/CUDA-8.0/\r\nInvalid path to cuDNN  toolkit. Neither of the following two files can be found:\r\n/Developer/NVIDIA/CUDA-8.0/lib/libcudnn.5\r\n/Developer/NVIDIA/CUDA-8.0/libcudnn.5\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]:\r\n```\r\n\r\nAfter my changes:\r\n\r\n\r\n```\r\n\u279c  tensorflow git:(master) ./configure\r\n~/os/tensorflow ~/os/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\r\n  /Library/Python/2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python]\r\n\r\nUsing python library path: /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]:\r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /Developer/NVIDIA/CUDA-8.0/\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.0\r\n............\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n........\r\n____Loading package: tensorflow\r\n____Loading package: tensorflow/contrib/tfprof/python/tools/tfprof\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 157,769 bytes\r\n[truncated]\r\nConfiguration finished\r\n```\r\n", "comments": ["Can one of the admins verify this patch?", "This bug exists only if the answer to the question \"Please specify the Cudnn version you want to use. [Leave empty to use system default]: \" is non-empty. This PR would solve the problem in that case, but would introduce another bug for the case where the answer is empty. Can you make changes around line 292 please?", "Ping!\r\nAny progress here?\r\nShould we take over this?", "I'll be happy to take this on myself in case @K75gTJnoDY0e is unavailable to work on it.", "Hi, my bash scripting knowledge is insufficient here. I'll try to learn and fix up my PR this week.", "The underlying issue should have been fixed by PR https://github.com/tensorflow/tensorflow/pull/6641. Closing this PR."]}, {"number": 6353, "title": "online add worker to  Running a distributed training session", "body": "When you use distributed tensorflow for training tasks, if you start your quest, it has been running for a long time. At this point you find that the number of workers to start too little, so can not be a good convergence.\r\n\r\nAt this point, you should increase the worker online to the distributed training tasks.\r\n\r\nexample:\r\n# On ps0.example.com:\r\n$ python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 0\r\n# On ps1.example.com:\r\n$ python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 1\r\n# On worker0.example.com:\r\n$ python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 0\r\n# On worker1.example.com:\r\n$ python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 1\r\n\r\nWhen you start, then, you need to increase worker2.example.com:2222, should do so:\r\n\r\n# Write a python script:,note must call all ps servers\r\nimport tensorflow as tf\r\ndef main (_):\r\n\u00a0\u00a0\u00a0\u00a0with tf.Session ('grpc: //ps0.example.com:2222') as sess:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sess.add_onlineworker (2, 'worker2.example.com: 2222')\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0with tf.Session ('grpc: //ps1.example.com:2222') as sess:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sess.add_onlineworker (2, 'worker2.example.com: 2222')\r\n\r\nif __name__ == \"__main__\":\r\n\u00a0\u00a0\u00a0\u00a0tf.app.run ()\r\n\r\nThen  run  this  python script:,\r\n\r\n# On worker2.example.com:\r\n$ python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222, \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = worker --task_index = 2\r\n\r\n\r\n#Then you can see worrker has started training\r\n\r\n#If ps  nodes in the cluster restart, remember to start with the worker2.\r\n\r\nFor example ps0 restart, you must start:\r\n# On ps0.example.com:\r\n$ python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222, \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = ps --task_index = 0\r\n\r\nif  ps1 restart, you must start:\r\n# On ps1.example.com:\r\n$ python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222, \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = ps --task_index = 1\r\n\r\n\r\n\r\n\r\nCan continue to train.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "yes,you can verify this patch", "@tensorflow-jenkins ,,yes,you can verify this patch", "I signed the CLA (under my primary email guowl18702995996@gmail.com).", "CLAs look good, thanks!\n\n<!-- ok -->", "Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@tensorflow-jenkins      Well, of course.", "@mrry    why  donnot  deal  this   pull  request?", "@guoying1030, thanks for your contribution! This is an interesting feature, and something that we would like to support eventually.\r\n\r\nHowever, as @vrv mentioned on #6352, for a change of this magnitude (modifying public APIs such as the C API, the `tensorflow::Session` API, and the `MasterService` RPC interface), we would typically have a design review before starting the implementation. Having looked through the changes in this PR, we can't accept them in their current form. I'm going to close this PR for now, but we would be glad to have an open design discussion on a new issue about this feature.", "@mrry      So what should I do? You can accept my code\uff1f", "@mrry ,,what is PR mean?"]}, {"number": 6352, "title": "how to commit code to tensorflow??I ", "body": "When using distributed tensorflow, once the start of the cluster task, you can not online expansion of the number of workers trained, I achieved this feature, would like to submit to tensorflow\u3002\u3002\u3002\r\n\r\nhow do it?", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md\r\nhttps://guides.github.com/activities/contributing-to-open-source/\r\n", "@yaroslavvb ...thank you very much", "Thanks Yaroslav.\r\n\r\n@guoying1030 if this is a big feature change, you should file another issue describing your idea so it go through a design review before spending too much time getting the PR ready.  We wouldn't want you to spend too much time setting up the PR if we wouldn't be able to accept the change for other technical reasons :)", "@vrv ,,,how to write this design???Is there a template?", "There isn't exactly a template yet.   It might be helpful to follow the beginning steps from something like  https://github.com/golang/proposal#readme: file an issue with the feature request, proposing in detail the scenario that you would like to enable.  We can then go through the process of discussing the options for implementing them, and depending on the scope of that work, the next steps might be either a design doc, or a simple PR."]}, {"number": 6351, "title": "What is login password in the jupyter notebook?", "body": "What is login password in the jupyter notebook?", "comments": ["in Docker", "hi jessyuan24,\r\n\r\nI'm looking for the password too.I find the password is added on commit [5f45249][commit_url] as described at  [#2378][issue_url].It seems the password is passed when we download the image as commond `docker run -it -p 8888:8888 $CUDA_SO $DEVICES -e PASSWORD=pass gcr.io/tensorflow/tensorflow-gpu` shown.\r\n\r\nBut when I run the image step by tensorflow guide,I didn't tap -e option.so it's really confused me. \r\n\r\n[commit_url]:https://github.com/tensorflow/tensorflow/commit/5f4524928af5ba5727fa97a35ddd5da72be0c476\r\n[issue_url]:https://github.com/tensorflow/tensorflow/pull/2378\r\n\r\n", "yes. I got it.-e option is bash shell parameter option. ", "-e is a docker parameter to export the environment variable to override the docker image. It seems like that works. We should definitely add this to the docker instructions @caisq.", "Thank you!!", "Finally,I find a way to access jupyter notebook.Recreate a container and specify a password.\r\n```\r\ndocker create -it -p 8888:8888 -e PASSWORD=JovialGates --name jovial_gates gcr.io/tensorflow/tensorflow\r\ndocker start jovial_gates\r\n```\r\nthen I can enter the notebook.\r\n\r\n", "This should be re-open since the use of jupyter_notebook_config.py was removed in cae5763 and the PASSWORD env variable is now ignored.", "I recommend opening a new issue instead, filling in the corresponding issue template. Not many developers are looking at closed issues.", "Has anyone working on this -e PASSWORD=pass? Is it fixed?"]}, {"number": 6350, "title": "Fix CUDNN extension error in configure for MacOS", "body": "", "comments": []}, {"number": 6349, "title": "compile tensorflow with option \"-xMIC-AVX512\" on INTEL XEON PHI PROCESSORS failed\uff5e\uff5e", "body": "Hi,All,Thanks for your time.\r\nI'm guest of a server with INTEL XEON PHI PROCESSORS(KNL) with CentOS7.\r\nI set my own env at first and compile bazel(0.3.2) and tensorflow(v0.9.0rc0) successfully(CC=gcc).\r\n(1) I set my own env as follows(most of it):\r\nexport HOME=/home/guest\r\nexport HOME_USR=$HOME/usr\r\nexport JAVA_HOME=$HOME/jdk1.8.0_112\r\nexport PATH=$HOME/intel/bin:$HOME/bazel_bin:$HOME_USR/bin:$JAVA_HOME/bin:$PATH\r\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\r\nexport CPATH=$HOME_USR/include:$HOME/intel/include\r\nexport C_INCLUDE_PATH=$CPATH\r\nexport CPLUS_INCLUDE_PATH=$CPATH\r\nexport LIBRARY_PATH=$HOME_USR/lib:$HOME_USR/lib64:$HOME/intel/lib/intel64_lin_mic/\r\nexport LD_LIBRARY_PATH=$LIBRARY_PATH\r\nexport LD_RUN_PATH=$LIBRARY_PATH\r\nexport COMPILER_PATH=$PATH\r\nexport CC=gcc\r\nexport PKG_CONFIG_PATH=$HOME_USR/lib/pkgconfig:$PKG_CONFIG_PATH\r\nexport PERL5LIB=$HOME_USR\r\n(2)bazel is OK:  \r\n./compile.sh\r\n(3)tensorflow is OK:   \r\n./configure \r\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n(4)Then the problem comes,I change the env CC=icpc , and run:\r\nbazel build -c opt --copt=-xMIC-AVX512 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n1. ERROR: /home/guest/.cache/bazel/_bazel_guest/ac941aabf1be9475583be832ae449128/external/gif_archive/BUILD:6:1: C++ compilation of rule '@gif_archive//:gif' failed: icpc failed: error executing command \r\n2.   (cd /home/guest/.cache/bazel/_bazel_guest/ac941aabf1be9475583be832ae449128/execroot/tensorflow && \\\r\n3.   exec env - \\\r\n4.     LD_LIBRARY_PATH=/home/guest/usr/lib:/home/guest/usr/lib64:/home/guest/cuda/lib64:/home/guest/cuda/lib64:/home/guest/cudnn:/home/guest/intel/lib/intel64_lin_mic/ \\\r\n5.     PATH=/home/guest/intel/bin:/home/guest/bazel_bin:/home/guest/usr/bin:/home/guest/cuda/bin:/home/guest/jdk1.8.0_112/bin:/usr/lib64/mpich/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/guest/.local/bin:/home/guest/bin \\\r\n6.   /home/guest/intel/bin/icpc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -B/home/guest/intel/bin -B/usr/bin -Wunused-but-set-parameter -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 -MD -MF bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/gif_font.d -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/gif_archive -isystem bazel-out/host/genfiles/external/gif_archive -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/gif_archive/gif_font.c -o bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/gif_font.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\r\n7. icpc: command line warning #10006: ignoring unknown option '-Wno-builtin-macro-redefined'\r\n8. Warning #2011: predefined meaning of \"__DATE__\" discarded\r\n9. \r\n10. Warning #2011: predefined meaning of \"__TIMESTAMP__\" discarded\r\n11. \r\n12. Warning #2011: predefined meaning of \"__TIME__\" discarded\r\n13. \r\n14. external/gif_archive/gif_font.c(231): error: a value of type \"void *\" cannot be assigned to an entity of type \"char *\"\r\n15.       dup = malloc(strlen(legend)+1);\r\n16.           ^\r\n17. \r\n18. compilation aborted for external/gif_archive/gif_font.c (code 2)\r\n19. Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n(5)I've found a similar problem here :\r\nhttps://github.com/tensorflow/tensorflow/issues/4775\r\nbut I don't know how to solve this problem?\r\nI update bazel by \"git checkout tags/0.4.2\" under bazel, and try to ./compile.sh again(CC=icpc),but it doesn't work.", "comments": ["@andydavis1 , @benoitsteiner , do you know if anyone has gotten tensorflow running on Xeon phi. I'm not aware of anybody. If, it may take some work to get it to work @craftlk. ", "Please also follow #7116 ", "Looks like #7116 was closed as fixed.\r\nI will also close this issue."]}, {"number": 6348, "title": "Tensorflow library for python 3 not available from the suggested docker image", "body": "I noticed that tensorflow is not available from the standard `gcr.io/tensorflow/tensorflow` docker image.\r\n\r\nI spun a docker container:\r\n\r\n    docker run -it --rm gcr.io/tensorflow/tensorflow bash\r\n\r\nthen I tried importing the Tensorflow from within python3:\r\n\r\n```\r\nroot@dc7f436ca5e9:/notebooks# python3\r\nPython 3.4.3 (default, Oct 14 2015, 20:28:29)\r\n[GCC 4.8.4] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named 'tensorflow'\r\n```\r\n\r\nbut the library is not available. Is this a bug or is this the intended feature? Does this mean I need to install tensorflow (for python3) myself with pip3 then in my own image? or maybe its a small bug?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttp://stackoverflow.com/questions/41170610/what-is-the-correct-way-to-have-tensorflow-available-in-a-docker-container-or-do\r\n\r\n### Environment info\r\nOperating System: MAC OS X sierra 10.12.1 \r\n", "comments": ["@caisq can you take a look. @brando90, you can just use `docker run -it tensorflow/tensorflow bash` to use docker hub. Also, you should almost always use a tag of the version you want to use... rather than tensorflow/tensorflow alone, because the semantics of latest are confusing.", "@brando90, when you use the image name `gcr.io/tensorflow/tensorflow` without any tags at the end, you are implicitly pointing to the `latest` tag, which is for python2 only. The tag `latest-py3` is most likely the one you want. So just do:\r\n\r\n```\r\ndocker run -it --rm gcr.io/tensorflow/tensorflow:latest-py3 bash\r\n```\r\n\r\nAs @aselle pointed out, you could also download the image from Docker Hub, instead of gcr.io:\r\n\r\n```\r\ndocker run -it --rm tensorflow/tensorflow:latest-py3 bash\r\n```\r\n", "I'm closing this issue. Please let me know if you have any remaining questions. "]}, {"number": 6347, "title": "A few more changes for release.", "body": "", "comments": []}, {"number": 6346, "title": "Windows 10 -> tensorflow-gpu -> TCC mode = Hangs forever", "body": "Anybody getting this to work in windows while in TCC CUDA mode? Seems to work fine in WDDM driver mode but WDDM cripples performance. To replicate issue, run on machine with Tesla class GPU in TCC mode, then try to create a session (sess = tf.Session()). The python process consumes a CPU core indefinitely and the session is never created.", "comments": ["@johnxllis Do you have physical access to the machine, or do you use remote desktop?\r\n\r\nCC @mrry @vit-stepanovs @guschmue ", "@gunan This is physical machine on my desktop. However, I am now seeing evidence that this issue is larger than just Tensorflow as it seems to be affecting Theano as well. The problem is most likely in CUDA or the configuration of the machine. I am closing the issue until I can isolate further. "]}, {"number": 6345, "title": "Udacity LSTM: module 'tensorflow' has no attribute 'concat_v2'", "body": "### Environment info\r\nOperating System: OSX 10.12.1\r\n\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n\u279c  ~ `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nzsh: command not found: 0.12.0-rc1.\r\n```\r\n\r\nI'm going through the udacity deep learning class and getting an error when evaluating\r\n\r\n`logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)`\r\n\r\nwhich was changed in https://github.com/tensorflow/tensorflow/commit/d4eb834824d79c6a64a3c4a1c4a88b434b73e63e#diff-e4eb32d9f7b7f06d7547050e1ce3d937\r\n\r\n```\r\n>>> import tensorflow\r\n>>> tensorflow.__version__\r\n'0.12.0-rc1'\r\n>>> tensorflow.concat_v2\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'concat_v2'\r\n>>> tensorflow.concat\r\n<function concat at 0x10bc03730>\r\n>>>\r\n```\r\n\r\nI installed and am running the latest release rc1 where `concat_v2` was added in https://github.com/tensorflow/tensorflow/commit/4564322e249017c030123b249451614bc2ff7658, has anyone else seen this?  Thanks in advance!\r\n\r\n\r\n\r\n", "comments": ["I ran into the same error yesterday when attempting to run the [code](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb) for the PTB LSTM language model example from the TensorFlow tutorials. Also running 0.12.0-rc1. I haven't been able to find a fix for it yet and cannot find any information about it in the docs. :/", "Same problem happened to me. Please give a solution.", "concat_v2 is added in the master branch. I believe you can run these code by switch to master branch.", "Hi Guys,\r\nI had the same problem, i just used the previous version of concat because i read here https://www.tensorflow.org/versions/master/api_docs/python/array_ops/slicing_and_joining#concat_v2 that their goal is identical.\r\nThe only thing is that the order of the parameters is inverse.\r\nWhere concat_v2 takes (values, axis), concat takes (axis,values), so i reversed the parameters calling\r\ntf.concat(0,outputs)\r\nI hope it is right and that it will save you some time!", "@danyz91 : I don't understand. The link that you gave shows that `concat` function is deprecated and it's encouraged for us to use `concat_v2`. So which one should we use in the long run?\r\n\r\ncc @aselle ", "In the short-term (before 1.0 final happens) concat_v2 will be the function to use. Once 1.0 final happens, concat_v2() will be renamed to concat() and then what used to be concat() will cease to exist.", "@aselle: Got it, thank you for the explanation!", "~/ `export TF_BINARY_URL=export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl1`\r\n\r\n~/`sudo pip3 install --upgrade $TF_BINARY_URL`", "Yes !  `concat_v2` -> `concat` for TF 1.0"]}, {"number": 6344, "title": "Fix split_ops_test for new import convention", "body": "Fixes bad merge earlier.", "comments": []}, {"number": 6343, "title": "0.12.0rc1 windows package for Python 3.5 (x64) does not include lib or include files", "body": "I installed the 0.12.0rc1 tensorflow package for Python 3.5 (64 bit) on Windows 10.  \r\n\r\nI started to try to add running trained models into an existing C++ software package, but the include and lib files specified by the documentation and found with `tensorflow.sysconfig.get_include()` and `.get_lib()` don't exist They return values:\r\n\r\n```\r\nIn [5]: tensorflow.sysconfig.get_include()\r\nOut[5]: 'e:\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\include'\r\n\r\nIn [6]: tensorflow.sysconfig.get_lib()\r\nOut[6]: 'e:\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\core'\r\n```\r\n\r\nBut these directories do not exist. I do not know if there is a problem with the package or the documentation.", "comments": ["@mrry may have seen this one before?", "Yes, it's true that we don't include those files with the Windows build, because the DLL we generate isn't useful for building C++ applications (it doesn't export the necessary symbols).\r\n\r\nRight now, for building C++ applications, your best bet is to follow the [CMake rules](https://github.com/tensorflow/tensorflow/blob/2477b50bf49d2cd6b8eac23071424c31dd05a05e/tensorflow/contrib/cmake/tf_tutorials.cmake) for building [`example_trainer.cc`](https://github.com/tensorflow/tensorflow/blob/2477b50bf49d2cd6b8eac23071424c31dd05a05e/tensorflow/cc/tutorials/example_trainer.cc); or using Bazel on Windows to build your application.\r\n\r\nIn the future, I expect we will distribute a separate `TensorFlow.dll` that exposes the [C API](https://github.com/tensorflow/tensorflow/blob/2477b50bf49d2cd6b8eac23071424c31dd05a05e/tensorflow/c/c_api.h), because it is relatively small and easy to link against from various different languages. Ideally it would be possible to build the TensorFlow Python extension (`_pywrap_tensorflow.pyd`) as a wrapper around such a DLL, but this would be longer-term change."]}, {"number": 6342, "title": "TypeError: Cannot create initializer for non-floating point type", "body": "Operating System: Mac OS [10.11.6]\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0, cudnn-8.0-osx-x64-v5.1 \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n 8 lrwxr-xr-x  1 root    admin    13B Dec 15 13:34 /usr/local/cuda/lib/libcuda.1.dylib@ -> libcuda.dylib\r\n    32 -rwxr-xr-x  1 root    wheel    13K Nov  3 22:09 /usr/local/cuda/lib/libcuda.dylib*\r\n     8 lrwxr-xr-x  1 root    wheel    45B Nov  3 22:10 /usr/local/cuda/lib/libcudadevrt.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\n     8 lrwxr-xr-x  1 root    wheel    50B Nov  3 22:10 /usr/local/cuda/lib/libcudart.8.0.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\n     8 lrwxr-xr-x  1 root    wheel    46B Nov  3 22:10 /usr/local/cuda/lib/libcudart.dylib@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\n     8 lrwxr-xr-x  1 root    wheel    49B Nov  3 22:10 /usr/local/cuda/lib/libcudart_static.a@ -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\n151304 -rwxr-xr-x@ 1 AmirHJ  admin    74M Jul 27 11:48 /usr/local/cuda/lib/libcudnn.5.dylib*\r\n     8 lrwxr-xr-x  1 AmirHJ  admin    16B Jul 27 11:51 /usr/local/cuda/lib/libcudnn.dylib@ -> libcudnn.5.dylib\r\n128152 -rw-r--r--@ 1 AmirHJ  admin    63M Jul 27 11:48 /usr/local/cuda/lib/libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: tensorflow-gpu==0.12.0rc1\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.12.0-rc1\r\n\r\nRunning [Text Classification Using Convolutional Neural Networks on Characters](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification_character_cnn.py):\r\n\r\n> python test.py\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally\r\n> WARNING:tensorflow:Using temporary folder as model directory: /var/folders/gy/035w5b717yn01k9qlwvtcp1h0000gn/T/tmpU1WjvQ\r\n> WARNING:tensorflow:From test.py:105 in main.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\n> Instructions for updating:\r\n> Estimator is decoupled from Scikit Learn interface by moving into\r\n> separate class SKCompat. Arguments x, y and batch_size are only\r\n> available in the SKCompat class, Estimator will only accept input_fn.\r\n> Example conversion:\r\n>   est = Estimator(...) -> est = SKCompat(Estimator(...))\r\n> WARNING:tensorflow:From test.py:105 in main.: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\n> Instructions for updating:\r\n> Estimator is decoupled from Scikit Learn interface by moving into\r\n> separate class SKCompat. Arguments x, y and batch_size are only\r\n> available in the SKCompat class, Estimator will only accept input_fn.\r\n> Example conversion:\r\n>   est = Estimator(...) -> est = SKCompat(Estimator(...))\r\n> Traceback (most recent call last):\r\n>   File \"test.py\", line 121, in <module>\r\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n>     sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n>   File \"test.py\", line 105, in main\r\n>     classifier.fit(x_train, y_train, steps=100)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 191, in new_func\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 355, in fit\r\n>     max_steps=max_steps)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 699, in _train_model\r\n>     train_ops = self._get_train_ops(features, labels)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1052, in _get_train_ops\r\n>     return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1023, in _call_model_fn\r\n>     model_fn_results = self._model_fn(features, labels)\r\n>   File \"test.py\", line 59, in char_cnn_model\r\n>     byte_list, N_FILTERS, FILTER_SHAPE1, padding='VALID')\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n>     return func(*args, **current_args)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 840, in convolution\r\n>     trainable=trainable)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n>     return func(*args, **current_args)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 244, in model_variable\r\n>     caching_device=caching_device, device=device)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n>     return func(*args, **current_args)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 208, in variable\r\n>     caching_device=caching_device)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\r\n>     custom_getter=custom_getter)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\r\n>     custom_getter=custom_getter)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\r\n>     validate_shape=validate_shape)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\r\n>     caching_device=caching_device, validate_shape=validate_shape)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 677, in _get_single_variable\r\n>     expected_shape=shape)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\r\n>     expected_shape=expected_shape)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 327, in _init_from_args\r\n>     initial_value(), name=\"initial_value\", dtype=dtype)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 665, in <lambda>\r\n>     shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n>   File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/initializers.py\", line 120, in _initializer\r\n>     raise TypeError('Cannot create initializer for non-floating point type.')\r\n> TypeError: Cannot create initializer for non-floating point type.", "comments": ["I'm able to reproduce this @martinwicke. Any thoughts?\r\n", "I see this from TF-SLIM , OSX  \r\n\r\n net = slim.conv2d(inputs, 32, [7, 7], stride=2, scope=end_point)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 900, in convolution\r\n    outputs = layer.apply(inputs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 293, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 259, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/layers/convolutional.py\", line 138, in build\r\n    dtype=self.dtype)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1063, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 340, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 249, in variable_getter\r\n    variable_getter=functools.partial(getter, **kwargs))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 200, in _add_variable\r\n    trainable=trainable and self.trainable)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1303, in layer_variable_getter\r\n    return _model_variable_getter(getter, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1292, in _model_variable_getter\r\n    custom_getter=getter)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 268, in model_variable\r\n    partitioner=partitioner, custom_getter=custom_getter)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 177, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 225, in variable\r\n    partitioner=partitioner)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 683, in _get_single_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 225, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 322, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 672, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/initializers.py\", line 120, in _initializer\r\n    raise TypeError('Cannot create initializer for non-floating point type.')\r\nTypeError: Cannot create initializer for non-floating point type.\r\n", "image = tf.to_float(image) fixes it for now. ", "Could someone check #7318 to see whether the fix works?"]}, {"number": 6341, "title": "ImportError: No module named google.protobuf", "body": "**Problem description**\r\nThis issue happens randomly. For same machine, same CPU,works randomly. I run it for 10 times, 6 times succeed, 4 times fail. Same code is running for every time. The code is \r\n```\r\nimport tensorflow as tf\r\n# with tf.device('/cpu:0'):\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\r\n\tprint sess.run(c)\r\n```\r\n\r\n**System information**\r\nInformation of cluster:\r\nBasically our cluster is made of IBM Power System S822LC for commercial computing\r\n\r\n    OS: Red Hat Enterprise Linux Server release 7.2 (Maipo)\r\n    TF version: 0.10.0rc0\r\n    protobuf: 3.0.0b2\r\n    Python: 2.7.5\r\n\r\n**Error**\r\n```\r\n>>> import os\r\n>>> cwd = os.getcwd()\r\n>>> cwd\r\n'/gpfs/gpfs0/groups/duraisamy/shawnpan/test/script'\r\n>>> import tensorflow\r\nimport tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.1.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/gpfs/gpfs0/software/rhel72/packages/tensorflow_gpu/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/gpfs/gpfs0/software/rhel72/packages/tensorflow_gpu/tensorflow/python/__init__.py\", line 58, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/gpfs/gpfs0/software/rhel72/packages/tensorflow_gpu/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/gpfs/gpfs0/software/rhel72/packages/tensorflow_gpu/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\nImportError: No module named google.protobuf\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```", "comments": ["Could it be an issue with the filesystem? (ie, Python is not able to import google.protobuf). You could perhaps debug by running distributed test script which does \"import google.protobuf\" and nothing else and seeing how often that fails", "I believe this is an instance of https://github.com/google/protobuf/issues/1296. Which is annoying, but there's not much we can do about in in TensorFlow. \r\n\r\nThere's a workaround in that thread, but it's some work for sysadmins that really should be inside protobuf.\r\n", "problem resolved. it turns out to be googleprobuf corrupted on some nodes.\r\n", "On Mac OS - Installing tensorflow 1.3 - it will automatically remove other protobuf installs and install protobuf 3.4. However, this does not work and neither does installing or downgrading to any other protobuf version.\r\n\r\nHowever I found a solution.\r\nNot sure why this works - but on Mac OS this solved it.\r\n\r\n    pip install google", "I install tensorflow with virtualenv.\r\npip uninstall protobuf\r\npip uninstall google\r\npip install google\r\npip install protobuf\r\nThat is ok,but I don't know why", "pip install google doesn't work for me\r\nI just\r\npip install google-cloud\r\nand it works", "pip install google-cloud worked for me as well! :)\r\n", "after installed google and google-cloud, I still got this:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"helloworld.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\nImportError: No module named google.protobuf\r\n```", "the `__init__.py` file is missing in site-packages/google directory. creating an empty `__init__.py` file there should work. thanks. @fangbq", "On the Power System I faced the same error while installing tensorflow 1.5\r\nSurprisingly, even though the protobuf module was there in the system, python did not pick it up.\r\n\r\nAfter installing google and google-cloud with the following:\r\n```\r\npip install google-cloud\r\npip install google\r\n```\r\nit worked fine.\r\n", "pip install google-cloud worked for our windows dev who was having problems. ", "the __init__.py file is missing, save my time thanks! @malithakabir ", "There is no need to install `google-cloud` or `google`. Neither the missing`__init__.py` will solve this issue!\r\n\r\nI have protobuf installed\r\n\r\n```sh\r\n(.env) ip-192-168-22-127:Wave-U-Net loretoparisi$ pip show protobuf\r\nName: protobuf\r\nVersion: 3.6.1\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: 3-Clause BSD License\r\nLocation: /Library/Python/2.7/site-packages\r\nRequires: six, setuptools\r\nRequired-by: tensorflow, tensorboard\r\n```\r\nin `virtualenv` but the import fails!", "@malithakabir  can you say where exactly i should place the __init__.py...there is no google directory in site packages.", "on my end, none of these solutions were working...\r\nam using Parrot linux", "pip install google-cloud-bigquery works for me , I am using windows, python 3.5 and tensorflow-gpu 1.4", "Finally installed with conda\r\n`conda install protobuf`", "Note: In Python >= 3.3 there **must not be**  a `__init__.py` in the google folder: http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap", "Do not install the google package since that will not solve your problem. You might have to uninstall it since it conflicts with the protobuf namespace. It's from a some other developer and not Google official!\r\n\r\nName: google\r\nVersion: 2.0.2\r\nSummary: Python bindings to the Google search engine.\r\nHome-page: http://breakingcode.wordpress.com/\r\nAuthor: Mario Vilas\r\nAuthor-email: mvilas@gmail.com\r\nLicense: UNKNOWN\r\nRequires: beautifulsoup4\r\n", "I'm following [this guide](https://jkjung-avt.github.io/build-tensorflow-1.12.2/) and every step worked fine except of the last one, When i try to run the benchmark i get this error. \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\", line 25, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/zwazel/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/zwazel/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/home/zwazel/.local/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\nModuleNotFoundError: No module named 'google.protobuf'\r\n```\r\n\r\nI tried all of the above things and nothing worked. Can someone help me or should i open a new issue for this?\r\nI'm running it on a Jetson Nano with Ubuntu 18.04 LTS", "> I install tensorflow with virtualenv.\r\n> pip uninstall protobuf\r\n> pip uninstall google\r\n> pip install google\r\n> pip install protobuf\r\n> That is ok,but I don't know why\r\n\r\ngood job,I also solve it in this way .\r\nenviroment: Ubuntu 1.18", "Again: Double-check that there is no `__init__.py` in your site-packages/google folder or delete it if there is. Easier than the uninstall dance which may or may not work. BTW: You probably do NOT want to install the \"google\" package: That is the Python bindings to google search engine. Why would you want that in this context? Oh and it trashes your namespace s mentioned in https://github.com/tensorflow/tensorflow/issues/6341#issuecomment-553769366", "> Finally installed with conda\r\n> `conda install protobuf`\r\n\r\nIt was the solution in my case. Thank you. pip was the problem. I am using conda environment. Use conda install!", "> I install tensorflow with virtualenv.\r\n> pip uninstall protobuf\r\n> pip uninstall google\r\n> pip install google\r\n> pip install protobuf\r\n> That is ok,but I don't know why\r\n\r\nThis works for me too.\r\nBefore uninstall, the location of protobuf is \\site-packages\\protobuf-4.0.0rc2-py3.8.egg\\google\\protobuf, after reinstall, the location is changed to site-packages\\google\\protobuf. Probably this solves the problem.", "> I install tensorflow with virtualenv.\r\n> pip uninstall protobuf\r\n> pip uninstall google\r\n> pip install google\r\n> pip install protobuf\r\n> That is ok,but I don't know why\r\n\r\nThis does work for me, but is indeed quite odd", "> Again: Double-check that there is no `__init__.py` in your site-packages/google folder or delete it if there is. Easier than the uninstall dance which may or may not work. BTW: You probably do NOT want to install the \"google\" package: That is the Python bindings to google search engine. Why would you want that in this context? Oh and it trashes your namespace s mentioned in [#6341 (comment)](https://github.com/tensorflow/tensorflow/issues/6341#issuecomment-553769366)\r\n\r\nI can confirm that removing `__init__.py ` does work! I am working on Jetson Nano with the latest JetPack 4.5.1 and installed Tensorflow 2.5.0 and protobuf 3.17.3. In my case, I use a virtual environment called `cv` so I ran the following commands:\r\ncd ~/cv/lib/python3.6/site-packages/protobuf-3.17.3-py3.6-linux-aarch64.egg/google/\r\nrm `__init__.py`"]}, {"number": 6340, "title": "cannot enable peer access for GPU cards on one machine", "body": "**Error:**\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 2\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 3\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 2\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 3\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 2 to device ordinal 0\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 2 to device ordinal 1\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 3 to device ordinal 0\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 3 to device ordinal 1\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y N N \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y N N \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   N N Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   N N Y Y \r\n```\r\n\r\n**Information of GPU**\r\n```\r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0002:01:00.0\r\nTotal memory: 15.90GiB\r\nFree memory: 15.62GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x10037db0cd0\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0003:01:00.0\r\nTotal memory: 15.90GiB\r\nFree memory: 15.62GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3effffc22000\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties: \r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0006:01:00.0\r\nTotal memory: 15.90GiB\r\nFree memory: 15.62GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x3dffffb856e0\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties: \r\nname: Tesla P100-SXM2-16GB\r\nmajor: 6 minor: 0 memoryClockRate (GHz) 1.4805\r\npciBusID 0007:01:00.0\r\nTotal memory: 15.90GiB\r\nFree memory: 15.62GiB\r\n\r\n```\r\n**Information of cluster:**\r\nBasically our cluster is made of IBM Power System S822LC for commercial computing\r\n\r\n    OS: Red Hat Enterprise Linux Server release 7.2 (Maipo)\r\n    TF version: 0.10.0rc0\r\n    protobuf: 3.0.0b2\r\n    Python: 2.7.5\r\n", "comments": ["This can occur all of the GPUs are not on the same PCI bus. Please ask this question on stackoverflow, where there may be someone that can help with your particular hardware configuration."]}, {"number": 6339, "title": "Branch 142140440", "body": "", "comments": ["Small number of conflicts in the merge.", "Jenkins, test this please?", "aborted? please don't abort...", "@mrry, can I get your help on the cmake failure? I suspect it's mostly about a new legacy_seq2seq module that's not properly included. But the conv test failure is different.", "Looks like a missing Python module in tf.contrib :(. Adding the following lines [here](https://github.com/martinwicke/tensorflow/blob/0daa5a0a2ce3c695b94a7008b191780343010bdc/tensorflow/contrib/cmake/tf_python.cmake#L313) should get you there:\r\n\r\n```\r\nadd_python_module(\"tensorflow/contrib/legacy_seq2seq/python\")\r\nadd_python_module(\"tensorflow/contrib/legacy_seq2seq/python/ops\")\r\n```", "GPU failure looks like a flake."]}, {"number": 6338, "title": "Cherry pick updated TensorBoard binary and python3 bug", "body": "Pull request of commit 43b29aa1763541a2255f876ac228bdfa5310ac34 into the r0.12 branch", "comments": ["@tensorflow-jenkins test this please.", "Failed mac test shouldn't be related. Retest to make sure.", "@tensorflow-jenkins test this please.", "@yifeif any idea why the tests failed?\r\n\r\n@tensorflow-jenkins test this please.", "let's try again... @tensorflow-jenkins test this please."]}, {"number": 6337, "title": "Can't develop TensorBoard under macOS", "body": "I read [DEVELOPMENT.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/DEVELOPMENT.md)\r\nAfter npm run prepare finished, gulp come up with error:\r\n\r\n> [23:42:32] Using gulpfile ~/Desktop/Going/tensorflow-master/tensorflow/tensorboard/gulpfile.js\r\n(node:20770) DeprecationWarning: os.tmpDir() is deprecated. Use os.tmpdir() instead.\r\n[23:42:32] Starting 'watch'...\r\n[23:42:32] Finished 'watch' after 129 ms\r\n[23:42:32] Starting 'first-compile'...\r\n[23:42:41] Finished 'first-compile' after 9.16 s\r\n[23:42:41] Starting 'server'...\r\n[23:42:41] Finished 'server' after 17 ms\r\n[23:42:41] Starting 'default'...\r\n[23:42:41] Finished 'default' after 18 \u03bcs\r\n[23:42:41] Webserver started at http://0.0.0.0:8000\r\n2016-12-15 23:42 gulp[20770] (FSEvents.framework) FSEventStreamStart: register_with_server: ERROR: f2d_register_rpc() => (null) (-22)\r\nevents.js:160\r\n      throw er; // Unhandled 'error' event\r\n      ^\r\nError: Error watching file for changes: EMFILE\r\n    at exports._errnoException (util.js:1022:11)\r\n    at FSEvent.FSWatcher._handle.onchange (fs.js:1282:11)\r\n\r\n\r\nmacOS 10.12.1\r\n\u279c  tensorboard gulp -v\r\n[23:44:58] CLI version 3.9.1\r\n[23:44:58] Local version 3.9.1\r\n\u279c  tensorboard node -v\r\nv7.2.1\r\n\u279c  tensorboard npm -v\r\n3.10.10\r\n\u279c  tensorboard bower -v\r\n1.8.0\r\n\r\n", "comments": ["Due to the great efforts of @jart, you no longer need gulp, bower, or any of their co-conspirators to develop or run TensorBoard! Perhaps try again using our new repository at https://github.com/tensorflow/tensorboard/blob/master/README.md?\r\n\r\n"]}, {"number": 6336, "title": "session_bundle in TensorFlow 0.12.0-rc1 cannot restore v2 checkpoint", "body": "I used [example/export_half_plus_two.py](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/contrib/session_bundle/example/export_half_plus_two.py) to test session_bundle. I run the below command to write v2 checkpoint files in export path,\r\n```shell\r\nroot\u00a7b47d8eea090e:/tensorflow/model-export/session_bundle# python export_half_plus_two.py --use_checkpoint_v2=True\r\ncopying asset files to: /tmp/half_plus_two/00000123-tmp/assets\r\ncopying asset file: hello2.txt\r\ncopying asset file: hello1.txt\r\n```\r\n\r\nThe export directory is \r\n```shell\r\nroot@b47d8eea090e:/tmp/half_plus_two/00000123# ls -l\r\ntotal 32\r\ndrwxr-xr-x 2 root root  4096 Dec 15 13:59 assets\r\n-rw-r--r-- 1 root root   133 Dec 15 13:59 checkpoint\r\n-rw-r--r-- 1 root root     8 Dec 15 13:59 export.data-00000-of-00001\r\n-rw-r--r-- 1 root root   134 Dec 15 13:59 export.index\r\n-rw-r--r-- 1 root root 14980 Dec 15 13:59 export.meta\r\n```\r\n\r\nThen I write a import python file, the content is\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.contrib.session_bundle import session_bundle\r\nfrom tensorflow.contrib.session_bundle import constants\r\nfrom tensorflow.contrib.session_bundle import manifest_pb2\r\n\r\nexport_dir = \"/tmp/half_plus_two/00000123\"\r\n\r\ntf.reset_default_graph()\r\n\r\nsess, meta_graph_def = session_bundle.load_session_bundle_from_path(\r\n    export_dir, target=\"\", config=tf.ConfigProto(device_count={\"CPU\": 2}))\r\n\r\nwith sess.as_default():\r\n    collection_def = meta_graph_def.collection_def\r\n    signatures_any = collection_def[constants.SIGNATURES_KEY].any_list.value\r\n    print(\"signatures length: %d\\n\" % len(signatures_any))\r\n\r\n    signatures = manifest_pb2.Signatures()\r\n    signatures_any[0].Unpack(signatures)\r\n\r\n    named_signatures = signatures.named_signatures\r\n    input_name = (named_signatures[\"inputs\"].generic_signature.map[\"x\"].tensor_name)\r\n    output_name = (named_signatures[\"outputs\"].generic_signature.map[\"y\"].tensor_name)\r\n\r\n    y = sess.run([output_name], {input_name: np.array([[0], [1], [2], [3]])})\r\n    print(y[0])\r\n```\r\n\r\nIt will tell me the variables are uninitialized,\r\n```shell\r\nroot\u00a7b47d8eea090e:/tensorflow/model-export/session_bundle# python import_half_plus_two.py \r\nsignatures length: 1\r\n\r\nTraceback (most recent call last):\r\n  File \"import_half_plus_two.py\", line 26, in <module>\r\n    y = sess.run(\u00c4output_name\u00dc, \u00e4input_name: np.array(\u00c4\u00c40\u00dc, \u00c41\u00dc, \u00c42\u00dc, \u00c43\u00dc\u00dc)\u00a8)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value b\r\n\t \u00c4\u00c4Node: b/read = Identity\u00c4T=DT_FLOAT, _class=\u00c4\"loc:\u00a7b\"\u00dc, _device=\"/job:localhost/replica:0/task:0/cpu:0\"\u00dc(b)\u00dc\u00dc\r\n\r\nCaused by op u'b/read', defined at:\r\n  File \"import_half_plus_two.py\", line 12, in <module>\r\n    export_dir, target=\"\", config=tf.ConfigProto(device_count=\u00e4\"CPU\": 2\u00a8))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/session_bundle/session_bundle.py\", line 95, in load_session_bundle_from_path\r\n    saver = tf.train.import_meta_graph(meta_graph_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1526, in import_meta_graph\r\n    **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 502, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value b\r\n\t \u00c4\u00c4Node: b/read = Identity\u00c4T=DT_FLOAT, _class=\u00c4\"loc:\u00a7b\"\u00dc, _device=\"/job:localhost/replica:0/task:0/cpu:0\"\u00dc(b)\u00dc\u00dc\r\n```\r\n\r\nIf I changed the \"use_checkpoint_v2\" option to \"False\", there would be no error.\r\n\r\n```shell\r\nroot\u00a7b47d8eea090e:/tensorflow/model-export/session_bundle# python export_half_plus_two.py --use_checkpoint_v2=False\r\nWARNING:tensorflow:*******************************************************\r\nWARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\r\nWARNING:tensorflow:Consider switching to the more efficient V2 format:\r\nWARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\r\nWARNING:tensorflow:now on by default.\r\nWARNING:tensorflow:*******************************************************\r\ncopying asset files to: /tmp/half_plus_two/00000123-tmp/assets\r\ncopying asset file: hello2.txt\r\ncopying asset file: hello1.txt\r\nroot\u00a7b47d8eea090e:/tensorflow/model-export/session_bundle# \r\nroot\u00a7b47d8eea090e:/tensorflow/model-export/session_bundle# python import_half_plus_two.py                          \r\nsignatures length: 1\r\n\r\n\u00c4\u00c4 2. \u00dc\r\n \u00c4 2.5\u00dc\r\n \u00c4 3. \u00dc\r\n \u00c4 3.5\u00dc\u00dc\r\nroot\u00a7b47d8eea090e:/tensorflow/model-export/session_bundle# \r\n```\r\n\r\nAt this time, the export directory is\r\n\r\n```shell\r\nroot@b47d8eea090e:/tmp/half_plus_two/00000123# ls -l\r\ntotal 28\r\ndrwxr-xr-x 2 root root  4096 Dec 15 14:07 assets\r\n-rw-r--r-- 1 root root   163 Dec 15 14:07 checkpoint\r\n-rw-r--r-- 1 root root   169 Dec 15 14:07 export-00000-of-00001\r\n-rw-r--r-- 1 root root 13740 Dec 15 14:07 export.meta\r\n```\r\n\r\nI found that the diff between v2 checkpoint file and v1 checkpoint is a substring \".data\" in v2 checkpoint filename. Just see the above example, the v2 checkpoint filename is \"export.data-00000-of-00001\" and the v1 checkpoint filename is \"export-00000-of-00001\". \r\n\r\nSo, I moved to read the load function \"load_session_bundle_from_path\", I found a bug at [L64](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/contrib/session_bundle/session_bundle.py#L64). See following code snippet\r\n\r\n```python\r\nif not file_io.file_exists(variables_filename):\r\n    variables_filename = os.path.join(\r\n        export_dir, constants.VARIABLES_FILENAME_PATTERN)\r\n    if not file_io.get_matching_files(variables_filename):\r\n      # If graph_util.convert_variables_to_constants() is called on a model\r\n      # it won't have any variables, and that's OK.\r\n      #\r\n      # TODO(yxshi): verify that the graph_def in fact does not have any\r\n      # reachable variables.\r\n      variables_filename = None\r\n```\r\n\r\nwhere the constant \"constants.VARIABLES_FILENAME_PATTERN\"( defined in [constant.py](https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/contrib/session_bundle/constants.py#L28)) is \"export-????-of-????\". It cannot match the v2 checkpoint filename, so the load function cannot restore the v2 checkpoint file.\r\n", "comments": ["I am OOO at the moment.  Could you please add Sukriti?\n\nOn Fri, Dec 16, 2016 at 00:58 andydavis1 <notifications@github.com> wrote:\n\n> Assigned #6336 <https://github.com/tensorflow/tensorflow/issues/6336> to\n> @concretevitamin <https://github.com/concretevitamin>.\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6336#event-895524644>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAkLHvUBxrxBbLMJfS7kuOt7hKuGkfV_ks5rIXG5gaJpZM4LOJ2Q>\n> .\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n", "session_bundle is not compatible with v2 checkpoints. Please use saved_model.", "@gunan I am seeing a similar issue loading saved checkpoints (saved using `tensorflow.contrib.session_bundl.exporter`) with `session_bundle.load_session_bundle_from_path`. This used to work. What should I change so that I can save and then load a given model?\r\n\r\nThe files that saved_model seems to be looking for: https://github.com/tensorflow/tensorflow/blob/27711108b5fce2e1692f9440631a183b3808fa01/tensorflow/python/saved_model/loader.py#L91-L97 are different from what I see created by the checkpointer.", "This seems to be fixed with `bundle_shim.load_session_bundle_or_saved_model_bundle_from_path`."]}, {"number": 6335, "title": "Fix bazel fetch in ./configure on Windows", "body": "On Windows, `$(bazel query //...)` hangs when running in a new workspace.\r\n\r\nhttps://github.com/bazelbuild/bazel/issues/2248", "comments": []}, {"number": 6334, "title": "Offering a small TensorFlow runtime for production environments", "body": "I noticed in https://github.com/tensorflow/tensorflow/pull/1109 that some attempts to reduce the PyPI package have been made,  but since 0.9 the package has grown quite a bit.\r\n\r\nAre there any ongoing attempts of creating a smaller TensorFlow runtime that can be used for putting graphs into production? For example, the most common use case for TensorFlow is probably running a machine learning model's forward pass. For that, all of the gradient related code could be excluded from the package, no?\r\n\r\nPreferably I'd love a utility that takes in a graph protobuf and spits out a pip installable TensorFlow version with only the required parts of the codebase. Is this at all doable?", "comments": ["@petewarden", "I think this could be closed in favor of https://www.tensorflow.org/performance/xla/tfcompile", "I think it should not be closed in favor of XLA AOT because it supports only limited set of the operators.\r\nWe need `tensorflow-runtime` package which can be installed on small devices like Raspberry Pi.\r\nWhy to install full 80MB package if I'm not going to train the model on Raspberry Pi.", "A sweet solution nowadays is to use TensorFlow Lite. It's worked nicely for me. At the time of creating this issue tflite wasn't available yet. :koala: "]}, {"number": 6333, "title": "Very low GPU usage", "body": "I try to use single dynamic_rnn to process very long sequence for classification task.\r\nHere are some parameters:\r\nrnn_size=500, seq_max_length=2500, batch_size=50, embedding_size=64, softmax_size=1600.\r\n\r\nthe code is as below:\r\n```\r\nx_vec = tf.nn.embedding_lookup(embedding_matrix_variable, self.x)\r\nlstm_fw_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = word_dim)\r\nlstm_fw_cell = rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob, input_keep_prob=self.dropout_keep_prob)\r\noutputs, _ = rnn.dynamic_rnn(lstm_fw_cell, x, dtype=tf.float32, sequence_length=real_length, swap_memory=False)\r\n\r\noutputs = tf.transpose(outputs, [1, 0, 2])\r\noutputs = tf.unpack(outputs)\r\n\r\noutput = outputs[0]\r\none = tf.ones([1, hidden_unit], tf.float32)\r\nwith tf.variable_scope(\"output\"):\r\n    tf.get_variable_scope().reuse_variables()\r\n        for i in range(1, len(outputs_6)):\r\n            ind = self.real_length < (i+1)\r\n            ind = tf.to_float(ind)\r\n             ind = tf.expand_dims(ind, -1)\r\n             mat = tf.matmul(ind, one)\r\n             output=tf.add(tf.mul(output, mat), tf.mul(outputs[i], 1.0-mat))\r\n\r\n\r\ny_prediction = tf.matmul(output, w_h2y) + b_h2y\r\ny_prediction = tf.nn.softmax(y_prediction)\t\r\n\t\t\r\nweight_decay = L2  * ( tf.nn.l2_loss(w_h2y) + tf.nn.l2_loss(b_h2y) )\r\nself.cost = tf.reduce_mean( -tf.reduce_sum(self.y*tf.log(y_prediction + 1e-10)) ) + weight_decay\r\nself.optimizer = tf.train.AdamOptimizer(alpha).minimize(self.cost)\r\n```\r\n\r\n\r\nThe usage of GPU on TITAN is only 5%.\r\nThe usage of CPU  is about 150%.\r\nI am not sure what's the problem.", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 6332, "title": "_linear scope bug: use the scope if provided", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed the CLA (under my primary email sasha.v.novikov@gmail.com).", "CLAs look good, thanks!\n\n<!-- ok -->", "Please remove the scope argument instead.  This function is not meant for public use and the scope argument is a bug.", "Done.", "Lukasz, can you do the final approval?  ooo.", "@tensorflow-jenkins test this please", "Mind fixing the lint errors? (The _scope arg is used in several places in that file)", "My bad, fixed", "Jenkins, test this please", "Trying again...\r\nJenkins, test this please"]}, {"number": 6331, "title": "Tensorflow installation Error in Mac Sierra due to numpy", "body": "Hi,\r\n\r\nI want to install TensorFlow on Mac. Currently I am using MacOS Sierra (Version 10.12.1).\r\nPython was already installed in this current version. My Python version is 2.7.10.\r\n\r\nI followed the instructions given on the webpage of Tensorflow to install it on my Mac. When I entered the command \r\n$ pip install tensorflow then I have the following error:\r\n\r\n\r\n\r\n\r\n$ pip install tensorflow\r\nCollecting tensorflow\r\n  Downloading tensorflow-0.12.0rc1-cp27-cp27m-macosx_10_11_x86_64.whl (38.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 38.5MB 23kB/s \r\nCollecting numpy>=1.11.0 (from tensorflow)\r\n  Downloading numpy-1.11.2-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.9MB 263kB/s \r\nRequirement already satisfied: six>=1.10.0 in /Library/Python/2.7/site-packages/six-1.10.0-py2.7.egg (from tensorflow)\r\nCollecting mock>=2.0.0 (from tensorflow)\r\n  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 4.9MB/s \r\nCollecting wheel (from tensorflow)\r\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 3.3MB/s \r\nCollecting protobuf==3.1.0 (from tensorflow)\r\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 348kB 2.1MB/s \r\nCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow)\r\n  Downloading funcsigs-1.0.2-py2.py3-none-any.whl\r\nCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow)\r\n  Downloading pbr-1.10.0-py2.py3-none-any.whl (96kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 7.4MB/s \r\nRequirement already satisfied: setuptools in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from protobuf==3.1.0->tensorflow)\r\nInstalling collected packages: numpy, funcsigs, pbr, mock, wheel, protobuf, tensorflow\r\n  Found existing installation: numpy 1.8.0rc1\r\n    DEPRECATION: Uninstalling a distutils installed project (numpy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\r\n    Uninstalling numpy-1.8.0rc1:\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_set.py\", line 778, in install\r\n    requirement.uninstall(auto_confirm=True)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_install.py\", line 754, in uninstall\r\n    paths_to_remove.remove(auto_confirm)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/req/req_uninstall.py\", line 115, in remove\r\n    renames(path, new_path)\r\n  File \"/Library/Python/2.7/site-packages/pip-9.0.1-py2.7.egg/pip/utils/__init__.py\", line 267, in renames\r\n    shutil.move(old, new)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 302, in move\r\n    copy2(src, real_dst)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 131, in copy2\r\n    copystat(src, dst)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 103, in copystat\r\n    os.chflags(dst, st.st_flags)\r\nOSError: [Errno 1] Operation not permitted: '/var/folders/jk/s2ngqqls71v9vj7z02p79w880000gn/T/pip-hpgFvE-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy-1.8.0rc1-py2.7.egg-info'\r\n\r\n\r\n\r\n\r\n\r\nI think there is some issue with numpy. My current numpy version is 1.8.0rc1 and it was already installed with MacOS Sierra. I tried to uninstall it so that Tensorflow install the new version by itself but no success.\r\n\r\nI tried different solutions available on the web to solve this issue but no success.\r\n\r\nCould anybody tell me that how I can solve this issue and install Tensorfow.\r\n\r\nThanks.\r\nZeeshan", "comments": ["Try use homebrew python instead", " I agree, using `homebrew` to install python is the best way to go. \r\nIf you want detailed steps: https://github.com/mokpro/tensorflow_examples/blob/master/README.md", "1-You usually need \"sudo\" to run pip installations, unless you are running inside a virtualenv.\r\n\r\n2-There is also a problem where if you both have macOS installed python and brew installed python, you can install any packages on one, but run the other. MacOS installed python resides in `/usr/bin/python` while brew installs it under `/usr/local/bin/python`.\r\nI think you need to run the following to make your system default to the brew installed python.\r\n`brew link python`.\r\nBut SIP (System Integrity Protection) on macOS may complicate things.\r\n\r\nTherefore I suggest you to create a virtualenv and install everything under that.\r\nIt should have no performance implications at all.\r\nAlso, any python environment changes you need will all be in a single folder, owned by you with no need for extra privileges.", "I have same error, how to solve this problem without homebrew.", "@jessyuan24\r\n\r\nHomebrew is the best way. Otherwise You can try Virtualenv\r\n\r\nIf you want to use the macos python, maybe you need to disable SIP and use 'sudo pip install'", "Because of System integrity protection, I think only way to install software on macOS natively is through the appstore.\r\n\r\nSo, as @comicchang suggested, your options are:\r\n1-disable SIP (the instructions are easy to find with a google search) then sudo pip install\r\n2-use virtualenv\r\n3-use homebrew.\r\n\r\n@zshareef did running `brew link python` help resolve your problem?", "@comicchang Thank you so much. it works. But I have some problem didn't understand.", "Also \r\n```\r\nsudo pip install --ignore-installed numpy\r\n```\r\n\r\nHad to do this with the `six` dependency as well", "Only this solution worked for me.", "For me too! Thanks.", "@otech47 Thanks", "I fixed this by\r\n\r\n    vim /usr/local/bin/pip\r\n\r\nwith\r\n\r\n    #!/usr/local/bin/python2.7\r\n\r\nat first line, as I already have `/usr/local/bin` ahead of `/usr/bin`.\r\n", "--virtualenv \r\nif your python's version is 2.7,try this:\r\nvirtualenv python27\r\nsource python27/bin/activate", "@AdaZhangLi \r\n\r\nThank you! That did it for me!", "Thanks @otech47 . Your sudo pip install --ignore-installed numpy worked for me. Let's see what happens down the road :-).", "> Also\r\n> \r\n> ```\r\n> sudo pip install --ignore-installed numpy\r\n> ```\r\n> Had to do this with the `six` dependency as well\r\n\r\nthis work for me "]}]