[{"number": 7477, "title": "Add support for custom queue runners", "body": "I have a custom implementation of `QueueRunner` but right now TF complains when serializing:\r\n\r\n> WARNING - tensorflow - Error encountered when serializing queue_runners.\r\n> Type is unsupported, or the types of the items don't match field type in CollectionDef.\r\n> unbound method to_proto() must be called with QueueRunner instance as first argument (got IteratorRunner instance instead)\r\n\r\nThe use case for the implementation is to take advantage of automatic thread starting from `Experiment`, `Supervisor`, `managed_session`, etc.\r\n\r\nI could inherit from `QueueRunner` but a `QueueRunnerBase` would be ideal as much of the existing implementation assumes the use of `enqueue_ops`.\r\n\r\nAs an example, I have [`GeneratorRunner`](https://gist.github.com/jimfleming/d1118cc630f5c883223a4b4645cc2e7b) which runs a generator in a thread and enqueues the outputs. This is more flexible than the existing `QueueRunner` since it  supports placeholders with feed dictionaries.", "comments": ["@jimfleming do you have a solution in mind? (ie, is it an issue of an internal check being unnecessarily strict?)", "That may be one issue and possibly a feature request for a `QueueRunnerBase` or for a more general thread runner. I haven't yet looked at what is involved with serialization so the check may be valid, in the case of my custom runner, particularly as it has not been tested in a distributed setting.\r\n\r\nI'm open to suggestions for how to proceed and if a pull request is warranted I can put one together.", "An alternative to \"queuerunnerbase\" is to implement it as new object and use duck typing to make it work with tensorflow serialization. The amount of stuff in QueueRunner is quite small, I'm not sure if it's worth using inheritance as opposed to \"copy paste the needed bits\"", "I agree that it's quite small. It looks like [`add_queue_runner`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/queue_runner_impl.py#L365) and [`start_queue_runners`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/queue_runner_impl.py#L383) wouldn't be affected. [`QueueRunnerDef`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/queue_runner.proto#L17) appears to expect a list of `enqueue_op_name` which breaks from my current use case as the generator performs the enqueue directly.", "btw, things seem to be moving in the direction of single threaded stuff using \"Hooks\" ... not sure if there's a future for queue runners ... IE, instead of having multiple threads issuing run calls in parallel (and causing unreproducible memory errors when the scheduling gets unlucky), do everything in a sequence, see https://github.com/tensorflow/tensorflow/blob/4c192f060cf9ff897911d240c140299d6db257b6/tensorflow/python/training/basic_session_run_hooks.py\r\n\r\nBTW, here's current existing code-base handles passing feed_dict to existing queue runners\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/799e31f3840c21322e380e1ec6e5bacb95d016fa/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py", "In particular, it seems \"StreamingDataFeeder\" is similar to your use case\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/799e31f3840c21322e380e1ec6e5bacb95d016fa/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py#L549", "Interesting. I agree that queue runners have their issues, especially wrt reproducibility. I'll look at using run hooks to replicate this without threads.\r\n\r\nSomething like `StreamingDataFeeder` seems to fit. It's not clear from the tests how it would be used with something like `Estimator`/`Experiment` which does not accept a `feed_dict`, but rather an `input_fn` which returns tensors fed by a queue. I guess I could combine with something like `FeedFnHook` but this feels hacky.\r\n\r\nSince it sounds like queue runners are going away the original issue is no longer accurate. If this is moving towards a usage question of existing functionality I can file on SO.", "@mrry, could you comment on this issue. I think we can close and move to SO based on discussion, but you should take a look. Thank you!", "I'm not sure what's a possible action is here... or what the utility of a new class hierarchy would be. There's already a way to register serialization and deserialization functions for custom graph-collection objects using [`register_proto_function()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4219), so perhaps you could use that? I don't think there's any additional magic we can bring for your `GeneratorRunner` since we don't have (AFAIK) a standard way to serialize a Python generator.\r\n\r\n(As an aside, we're trying to find ways to reduce the need for queue runners, but they're in the API for 1.0, so they'll be around for a while but I doubt we'll invest much effort in improving them.)", "It seems this kind of modification is easier to do as a library on top of TensorFlow rather than part of TensorFlow itself (Conway's Law). A change to core needs to have some benefit that's not easily achievable when outside of core. Now that TF 1.0 is out and API is stable, I'm hoping 3rd party libraries will emerge which will collect things like that into toolboxes that enhance usability of TensorFlow. I think this generator pattern is useful, here's a recent user than wanted to know how to use a generator as TensorFlow input -- http://stackoverflow.com/questions/42322176/tensorflow-custom-data-reader-using-py-func"]}, {"number": 7476, "title": "Implement Batch Renormalization in TF", "body": "See paper: https://arxiv.org/pdf/1702.03275.pdf\r\nKeras implementation: https://github.com/ajbrock/Neural-Photo-Editor/blob/master/layers.py#L35", "comments": ["Looks like it has been implemented in tensorpack: https://github.com/ppwwyyxx/tensorpack/blob/3f238a015b941041e58ed43e63d5306dbae979fc/tensorpack/models/batch_norm.py#L208", "how can we set the values of rmax and dmax?\r\ne.g. rmax=2, dmax=2 will be sufficient for most use?\r\n", "@cancan101 @hengck23 [Here](https://github.com/shiyemin/shuttleNet/blob/master/ops/ops.py#L55-L308) is my implementation. I also provide my code for setting the RMAX and DMAX.\r\n\r\nBasic usage:\r\nbatch_renorm_decay=0.9\r\nbatch_renorm_epsilon=0.001\r\n\r\nRMAX_decay = ops.adjust_max(200, 800, 1, 3, name='RMAXDECAY')\r\nDMAX_decay = ops.adjust_max(200, 600, 1, 5, name='DMAXDECAY')\r\nbatch_renorm_params = {\r\n    'renorm': True,\r\n    'RMAX': RMAX_decay,\r\n    'DMAX': DMAX_decay,\r\n    'decay': batch_renorm_decay,\r\n    'epsilon': batch_renorm_epsilon,\r\n}\r\nwith slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,\r\n                    normalizer_fn=ops.fused_batch_norm,\r\n                    normalizer_params=batch_renorm_params) as scope:\r\n\r\n\r\nBTW, my implementation is modified based on TF's fused_batch_norm and should be fully compatible with it.", "I have submitted a quick PR with the renorm parameter", "Looks like this got added in https://github.com/tensorflow/tensorflow/pull/9059/commits/55c809b62d688637813c828301e528cb2fbcc54b.", "But no support for fused batch norm :(", "This issue solved. Any future discussion re: fused batch re-norm can be found at: https://github.com/tensorflow/tensorflow/issues/9141."]}, {"number": 7475, "title": "Replace deprecated tf.op_scope by tf.name_scope", "body": "Since tf.op_scope is deprecated (see https://www.tensorflow.org/api_docs/python/framework/defining_new_operations#op_scope) it should probably be removed from the style guide.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 7474, "title": "Gradient of gamma log pdf is broken", "body": "```\r\nIn [1]: import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nd\r\nIn [2]: dist = tf.contrib.distributions\r\n\r\nIn [3]: mu = tf.get_variable('mean_arg', [], 'float32')\r\n\r\nIn [4]: m = tf.nn.softplus(mu)\r\n\r\nIn [5]: q = dist.Gamma(0.01, 0.01/mu)\r\n\r\nIn [6]: sess = tf.InteractiveSession()\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 780 Ti\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\r\npciBusID 0000:02:00.0\r\nTotal memory: 2.98GiB\r\nFree memory: 2.90GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 780 Ti, pci bus id: 0000:02:00.0)\r\n\r\nIn [7]: sess.run(tf.global_variables_initializer())\r\n\r\nIn [8]: sess.run(tf.gradients(q.log_pdf(q.sample()), mu))\r\nOut[8]: [0.74267536]\r\n\r\nIn [9]: sess.run(tf.gradients(q.log_pdf(q.sample()), mu))\r\nOut[9]: [nan]\r\n\r\nIn [10]: sess.run(tf.gradients(q.log_pdf(q.sample()), mu))\r\nOut[10]: [nan]\r\n\r\nIn [20]: tf.__version__\r\nOut[20]: '0.12.0-rc1'\r\n```\r\nIt looks like this happens because Gamma samples can be negative:\r\n```\r\nIn [80]: z = q.sample()\r\n\r\nIn [81]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))\r\nOut[81]: [-0.0050091296, 0.74267536]\r\n\r\nIn [82]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))\r\nOut[82]: [-0.0, nan]\r\n\r\nIn [83]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))\r\nOut[83]: [-0.0, nan]\r\n\r\nIn [84]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))\r\nOut[84]: [-0.0, nan]\r\n\r\nIn [85]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))\r\nOut[85]: [-0.01258331, 0.74267519]\r\n\r\nIn [86]: sess.run([z] + tf.gradients(q.log_pdf(z), mu))\r\nOut[86]: [-1.0715475e-21, 0.74267536]\r\n```\r\nHappy to run more tests to figure out this issue.\r\n\r\nUpdate: quick fix thanks to @ebrevdo - \r\n```z = z + np.finfo(z.dtype.as_numpy_dtype).tiny```", "comments": ["So, for clarity, you just add epsilon to your input to avoid inputting zero? Can you close the issue if it is satisfactorily resolved?", "We're adding this directly to the gamma random sampler.  Can leave it open\nuntil we push the change.\n\nOn Feb 14, 2017 5:05 PM, \"Andrew Selle\" <notifications@github.com> wrote:\n\n> So, for clarity, you just add epsilon to your input to avoid inputting\n> zero? Can you close the issue if it is satisfactorily resolved?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7474#issuecomment-279887055>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1y6KNWlhjK1vw6flXGWu8XOkyaxks5rck9mgaJpZM4L_s4p>\n> .\n>\n"]}, {"number": 7473, "title": "Jupyter notebook: Kernel dies when running timeline trace [Nvidia-docker]", "body": "I am trying to use timeline to profile GPU memory usage on an EC2 instance using Tensorflow-Gpu Nvidia-Docker. When I add a few lines to my notebook which runs a convNet, it keeps restarting with a \"Kernel died\" message.\r\n\r\nSome of the changes I made:\r\n```\r\nfrom tensorflow.python.client import timeline\r\n```\r\n\r\n```\r\nrun_metadata = tf.RunMetadata()\r\n\r\nwith tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:\r\n    tf.global_variables_initializer().run()\r\n    for step in range(num_steps):\r\n        batch_data, batch_labels = generate_batch(\r\n          batch_size, num_skips, skip_window)\r\n        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\r\n        if step == 1000:\r\n            _, l = session.run([optimizer, loss], feed_dict=feed_dict, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n            trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n            with open('timeline.ctf.json', 'w') as trace_file:\r\n                trace_file.write(trace.generate_chrome_trace_format())\r\n        else:\r\n            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\r\n        ...\r\n```", "comments": ["There should be an actual error from tensorflow in your logs somewhere. I would first suspect that it can't find CUPTI library\r\n\r\n`export LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\"\r\n`", "Thanks @yaroslavvb; @tfboyd, do you have anything additional?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7472, "title": "Upgrade TensorFlow Docker images from ubuntu 14.04 to 16.04", "body": "This leads to Python 3 version increase from 3.4 to 3.5.\r\n\r\nDO NOT MERGE YET: Pending Jenkins configuration update, which is blocked by Python 3.5 build failure we are experiencing in nightly.", "comments": ["TESTED (Jenkins login required to view logs):\r\nExperimental GPU Docker run:\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-docker-gpu/1/\r\n\r\nExperimental CPU Docker run:\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-docker-cpu/2/", "We need to fix the ongoing Python 3.5 build failure in nightly before the Jenkins configuration can be updated for nightly-docker-cpu and nightly-docker-gpu and this can be merged.\r\nFor example, see http://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/", "CC @jhseu ", "I'll send you a change for the nightly.", "I'm merging this PR. The new CPU pip build based on jhsue@'s change has passed. The new GPU build is still failing. I'll get those fixed to unblock both the CPU and GPU docker builds.", "Great! When will the released Docker images be updated for this change? Will it be with the next TF release?", "@taion, you will first see the effect of these changes on the nightly* docker images that we build and push onto Docker Hub every night. Currently there are some blockers that prevent the nightly-gpu* images to be built successfully. But the non-gpu ones should already be 16.04.\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags/\r\n\r\nThe next TF release (1.1) will incorporate these changes.", "It does not look like the nightlies are built / pushed to gcr: gcr.io/tensorflow/tensorflow", "@cancan101 We currently push nightly images only to Docker Hub: \r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags/\r\n\r\nImages are pushed to gcr.io only during releases:\r\n"]}, {"number": 7471, "title": "Branch 147365276", "body": "", "comments": ["Sorry, mind redoing the push? We need at least cl/147369186 so that we don't break the build on g++ 5/6", "yeah there are two failures.  I'll repush after they are fixed."]}, {"number": 7470, "title": "Android: Multiple dex files define Lorg/tensorflow/contrib/android/TensorFlowInferenceInterface", "body": "I am getting the following error regarding the TensorFlowInferenceInterface only when I try to build an APK in Android Studio (on MacOS).  But when I instead just \"Run app\", I do not get the error and the app compiles, installs and runs just fine.  (There is also an unrelated warning shown for a CircularQueue that I am working to fix; I include it for completeness)\r\n```\r\nInformation:0 warnings\r\nError:associated EnclosingMethod attribute. This class was probably produced by a\r\nError:indicate that it is *not* an inner class.\r\nInformation:See complete output in console\r\nError:Execution failed for task ':app:transformClassesWithDexForFastBuildDebug'.\r\n> com.android.build.api.transform.TransformException: com.android.ide.common.process.ProcessException: java.util.concurrent.ExecutionException: java.lang.UnsupportedOperationException\r\nError:(com.bea.xml.stream.util.CircularQueue$1) that doesn't come with an\r\nError:warning: Ignoring InnerClasses attribute for an anonymous inner class\r\nError:Error converting bytecode to dex:\r\nCause: com.android.dex.DexException: Multiple dex files define Lorg/tensorflow/contrib/android/TensorFlowInferenceInterface;\r\nError:this warning is that reflective operations on this class will incorrectly\r\nError:and without specifying any \"-target\" type options. The consequence of ignoring\r\nInformation:Gradle tasks [:ai2_common:assembleDebug, :app:assembleFastBuildDebug, :common:assembleDebug]\r\nInformation:10 errors\r\nInformation:BUILD FAILED\r\nInformation:Total time: 54.509 secs\r\nError:solution is to recompile the class from source, using an up-to-date compiler\r\nError:compiler that did not target the modern .class file format. The recommended\r\n```\r\n\r\nI am using the following TensorFlowInferenceInterface.java file:\r\n```\r\n//\r\n// Source code recreated from a .class file by IntelliJ IDEA\r\n// (powered by Fernflower decompiler)\r\n//\r\n\r\npackage org.tensorflow.contrib.android;\r\n\r\nimport android.content.res.AssetManager;\r\nimport android.util.Log;\r\nimport java.util.Random;\r\n\r\npublic class TensorFlowInferenceInterface {\r\n    private static final String TAG = \"TensorFlowInferenceInterface\";\r\n    private final long id = (new Random()).nextLong();\r\n\r\n    public TensorFlowInferenceInterface() {\r\n        try {\r\n            this.testLoaded();\r\n            Log.i(\"TensorFlowInferenceInterface\", \"Native methods already loaded.\");\r\n        } catch (UnsatisfiedLinkError var4) {\r\n            Log.i(\"TensorFlowInferenceInterface\", \"Loading tensorflow_inference.\");\r\n\r\n            try {\r\n                System.loadLibrary(\"tensorflow_inference\");\r\n            } catch (UnsatisfiedLinkError var3) {\r\n                throw new RuntimeException(\"Native TF methods not found; check that the correct native libraries are present and loaded.\");\r\n            }\r\n        }\r\n\r\n    }\r\n\r\n    public native int initializeTensorFlow(AssetManager var1, String var2);\r\n    public native int runInference(String[] var1);\r\n    public native void enableStatLogging(boolean var1);\r\n    public native String getStatString();\r\n    public native void close();\r\n    public native void fillNodeFloat(String var1, int[] var2, float[] var3);\r\n    public native void fillNodeInt(String var1, int[] var2, int[] var3);\r\n    public native void fillNodeDouble(String var1, int[] var2, double[] var3);\r\n    public native void fillNodeByte(String var1, int[] var2, byte[] var3);\r\n    public native void readNodeFloat(String var1, float[] var2);\r\n    public native void readNodeInt(String var1, int[] var2);\r\n    public native void readNodeDouble(String var1, double[] var2);\r\n    public native void readNodeByte(String var1, byte[] var2);\r\n    private native void testLoaded();\r\n}\r\n```\r\nWhen I search for all references to `TensorFlowInferenceInterface` in the path, I only get these:\r\n```\r\nimport org.tensorflow.contrib.android.TensorFlowInferenceInterface;\r\n```\r\nand\r\n```\r\n        public TensorFlowInferenceInterface inferenceInterface;\r\n```\r\nand\r\n```\r\n        inferenceInterface = new TensorFlowInferenceInterface();\r\n```\r\nNo other place is it even mentioned.\r\nHas anyone seen this before?", "comments": ["It sounds like you have a compiled version of TensorFlowInferenceInterface also on your classpath somehow, which would mean it wouldn't show up when you search your .java files -- probably the very .class file you've decompiled here?\r\n\r\nBtw It's not necessary to recreate the src file manually: you can grab the source from contrib/tensorflow/android/java/... or just download the [prebuilt jar](https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/libandroid_tensorflow_inference_java.jar) to drop in your app.", "Thanks.  I'll see if I can find the file tonight.  \r\nI'll probably see you at the TF Dev conference tomorrow!  :-)", "Great find.  There was indeed an extra copy of the .java file.  I removed it and it compiled perfectly.  Thanks!"]}, {"number": 7469, "title": "docs: batch normalization usage in slim", "body": "How to use batch normalization in the testing phase?\r\n\r\nI tried to use batch normalization to train a model like this:\r\n```\r\nbn = lambda x: slim.batch_norm(x, is_training=is_training)\r\nconv = slim.conv2d(images, 64, [3, 3], 1, normalizer_fn=bn, padding='SAME', scope='conv')\r\n```\r\nBut when I finished training and restored my model from checkpoint files, the model's performance on the testing set was poor, just like random guessing.\r\n\r\nIf these parameters are not dumped as model variables, is it possible to make an example to illustrate how to use batch normalization in slim, esp. for inference?", "comments": ["When training are you using: `slim.learning.create_train_op(loss, optimizer)` ?", "Thanks for your reply.\r\nI used `train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)` in my code. After I saw your comment, I changed it into `slim.learning.create_train_op`, but it didn't work either.\r\n\r\nThis is my current [code](https://github.com/soloice/mnist-bn/blob/master/mnist_bn.py) (I've figured out the canonical way to use a normalizer is to config its parameters in `normalizer_params`, so I replaced the lambda expression with `normalizer_params`). The most relevant part might be those in function `model()`:\r\n\r\n```\r\ndef model():\r\n    # Create the model\r\n    x = tf.placeholder(tf.float32, [None, 784])\r\n    keep_prob = tf.placeholder(tf.float32, [])\r\n    y_ = tf.placeholder(tf.float32, [None, 10])\r\n    is_training = tf.placeholder(tf.bool, [])\r\n\r\n    x_image = tf.reshape(x, [-1, 28, 28, 1])\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                        normalizer_fn=slim.batch_norm,\r\n                        normalizer_params={'is_training': is_training}):\r\n        conv1 = slim.conv2d(x_image, 32, [5, 5], scope='conv1')\r\n        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\r\n        conv2 = slim.conv2d(pool1, 64, [5, 5], scope='conv2')\r\n        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\r\n        flatten = slim.flatten(pool2)\r\n        fc = slim.fully_connected(flatten, 1024, scope='fc1')\r\n        drop = slim.dropout(fc, keep_prob=keep_prob)\r\n        logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\r\n\r\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n    cross_entropy = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    if update_ops:\r\n        updates = tf.group(*update_ops)\r\n        cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\r\n\r\n    # train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n    train_op = slim.learning.create_train_op(cross_entropy, optimizer)\r\n\r\n    return {'x': x,\r\n            'y_': y_,\r\n            'keep_prob': keep_prob,\r\n            'is_training': is_training,\r\n            'train_step': train_op,\r\n            'accuracy': accuracy,\r\n            'cross_entropy': cross_entropy}\r\n```\r\n\r\nIf `FLAGS.phase` is \"train\", the model is trained on the training set. But when I try to evaluate the model on the validation set and pass a `False` to placeholder `is_training`, the performance on validation set looks weird (it should be about 98% or higher).\r\n\r\nIf `FLAGS.phase` is \"test\", the model restores a pre-trained model from checkpoint and evaluate on the test set. Again, performance is really poor.\r\n\r\nAm I using passing parameters to 'slim.batch_norm' incorrectly? How to use batch normalization for inference?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Yes, the problem I met is more like a StackOverflow type question. \r\nThough, I'm wondering if the usage of slim.batch_norm layer could be added into slim readme.md or somewhere else to make it clearer.", "It's my bad. I haven't fully understood the dynamics of batch normalization layer during training.\r\n\r\n[This](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-280325584) is what happened.\r\n\r\nI feel very sorry for bothering you with my carelessness.", "Don't worry abou tit @soloice, glad you got things working.", "Hi @soloice \r\n\r\nI also met similar problem as yours while using `slim.batch_norm`, and solved by following \r\n\r\n> **Another important thing is**, be sure to use slim.learning.create_train_op to create train op. Do not use tf native tf.train.GradientDescentOptimizer(0.1).minimize(loss).\r\n\r\nThanks a lot! However, do you know the reason why we need to use `slim.create_train_op` instead of native `tf.train.GradientDescentOptimizer(0.1).minimize(loss)`? ", "@jacky841102  I don't know yet. I haven't checked source code in module `slim`.", "@soloice \r\nfrom slim.create_train_op source code, you can see it calls update_ops for you. So, I think you don't really need to call update_op manually by using slim.create_train_op\r\n\r\n# Update ops use GraphKeys.UPDATE_OPS collection if update_ops is None.\r\n global_update_ops = set(ops.get_collection(ops.GraphKeys.UPDATE_OPS))\r\n if update_ops is None:\r\n    update_ops = global_update_ops\r\n  else:\r\n    update_ops = set(update_ops)\r\n  if not global_update_ops.issubset(update_ops):\r\n    logging.warning('update_ops in create_train_op does not contain all the '\r\n                    ' update_ops in GraphKeys.UPDATE_OPS')\r\n\r\n  # Make sure update_ops are computed before total_loss.\r\n  if update_ops:\r\n    with ops.control_dependencies(update_ops):\r\n      barrier = control_flow_ops.no_op(name='update_barrier')\r\n    total_loss = control_flow_ops.with_dependencies([barrier], total_loss)", "When you use slim.batch_norm,be sure to use \"slim.learning.create_train_op\" instead of \"tf.train.GradientDecentOptimizer(lr).minimize(loss)\" or other optimizer. Try it to see if it works! ", "@soloice @cancan101 @tasx0823  Could you please explain why using `train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)` with a model defined with `tf.slim.batch_norm` is wrong? Thank you for pointing this out.\r\n\r\nContrastingly, I am using the native `train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)` method and seems to work fine(i am finetuning resnet50).\r\n\r\nI defined my training and the validation outputs as following,\r\n\r\n\r\n`with slim.arg_scope(self.network_arg_scope()):`\r\n\r\n            self.network_logits,_     = resnet_v1_50(inputs=inputs) \r\n\r\n            self.network_logits_val,_ = resnet_v1_50(inputs=inputs, reuse=True, is_training=False) ## Define a new output for testing but reuse the same model variables`\r\n\r\n One thing I can't explain is that I get two blocks in the TensorBoard as `resnet_v1_50` and `resnet_v1_50_1`. However, now I fear there is something wrong with this as for your comments. Have you any idea of what is happening here?\r\n\r\n![See the TensorBoard Graph](https://user-images.githubusercontent.com/34502974/34708987-6f795c36-f569-11e7-9f99-791a8f8d2975.png) \r\n\r\n \r\nThank you in advance :)", "@soloice @cancan101 @tasx0823 .... Thank you for the lead. Using `create_train_op` is giving something very close to the validation performance I was expecting. :)\r\n\r\nBtw this is also explained in the documentation : \r\n\r\n> By default, slim.learning.create_train_op includes all update ops that are\r\n> part of the `tf.GraphKeys.UPDATE_OPS` collection. Additionally, TF-Slim's\r\n> slim.batch_norm function adds the moving mean and moving variance updates to\r\n> this collection. Consequently, users who want to use slim.batch_norm will not\r\n> need to take any additional steps in order to have the moving mean and moving\r\n> variance updates be computed. \r\n\r\nHowever, I still have two blocks as `resnet_v1_50` and  `resnet_v1_50_1` in my TensorBoard visualization. Is this due to the distinct operators in the `BatchNorm` layer at train and validation times?\r\n![updated graph](https://user-images.githubusercontent.com/34502974/34750354-c6aa92f4-f5fa-11e7-83b2-93366223bc19.png)\r\n\r\n\r\n\r\n\r\n\r\n", "since you have\r\n\r\n> @soloice @cancan101 @tasx0823 Could you please explain why using `train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)` with a model defined with `tf.slim.batch_norm` is wrong? Thank you for pointing this out.\r\n> \r\n> Contrastingly, I am using the native `train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)` method and seems to work fine(i am finetuning resnet50).\r\n> \r\n> I defined my training and the validation outputs as following,\r\n> \r\n> `with slim.arg_scope(self.network_arg_scope()):`\r\n> \r\n> ```\r\n>         self.network_logits,_     = resnet_v1_50(inputs=inputs) \r\n> \r\n>         self.network_logits_val,_ = resnet_v1_50(inputs=inputs, reuse=True, is_training=False) ## Define a new output for testing but reuse the same model variables`\r\n> ```\r\n> One thing I can't explain is that I get two blocks in the TensorBoard as `resnet_v1_50` and `resnet_v1_50_1`. However, now I fear there is something wrong  with this as for your comments. Have you any idea of what is happening here?\r\n> \r\n> ![See the TensorBoard Graph](https://user-images.githubusercontent.com/34502974/34708987-6f795c36-f569-11e7-9f99-791a8f8d2975.png)\r\n> \r\n> Thank you in advance :)\r\nI have the same question? why can't we manually add update_op to the graph before computing the loss?\r\nthe poster already add this to the code:\r\n```\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    if update_ops:\r\n        updates = tf.group(*update_ops)\r\n        cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\r\n```", "> When you use slim.batch_norm,be sure to use \"slim.learning.create_train_op\" instead of \"tf.train.GradientDecentOptimizer(lr).minimize(loss)\" or other optimizer. Try it to see if it works!\r\n\r\nWhat if we manually add the dependency:\r\n```\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n      train_op = ...\r\n```\r\nWill this work? Thanks!", "> Thanks for your reply.\r\n> I used `train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)` in my code. After I saw your comment, I changed it into `slim.learning.create_train_op`, but it didn't work either.\r\n> \r\n> This is my current [code](https://github.com/soloice/mnist-bn/blob/master/mnist_bn.py) (I've figured out the canonical way to use a normalizer is to config its parameters in `normalizer_params`, so I replaced the lambda expression with `normalizer_params`). The most relevant part might be those in function `model()`:\r\n> \r\n> ```\r\n> def model():\r\n>     # Create the model\r\n>     x = tf.placeholder(tf.float32, [None, 784])\r\n>     keep_prob = tf.placeholder(tf.float32, [])\r\n>     y_ = tf.placeholder(tf.float32, [None, 10])\r\n>     is_training = tf.placeholder(tf.bool, [])\r\n> \r\n>     x_image = tf.reshape(x, [-1, 28, 28, 1])\r\n>     with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n>                         normalizer_fn=slim.batch_norm,\r\n>                         normalizer_params={'is_training': is_training}):\r\n>         conv1 = slim.conv2d(x_image, 32, [5, 5], scope='conv1')\r\n>         pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\r\n>         conv2 = slim.conv2d(pool1, 64, [5, 5], scope='conv2')\r\n>         pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\r\n>         flatten = slim.flatten(pool2)\r\n>         fc = slim.fully_connected(flatten, 1024, scope='fc1')\r\n>         drop = slim.dropout(fc, keep_prob=keep_prob)\r\n>         logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\r\n> \r\n>     correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\r\n>     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n>     cross_entropy = tf.reduce_mean(\r\n>         tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\r\n> \r\n>     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n>     if update_ops:\r\n>         updates = tf.group(*update_ops)\r\n>         cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\r\n> \r\n>     # train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n> \r\n>     optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n>     train_op = slim.learning.create_train_op(cross_entropy, optimizer)\r\n> \r\n>     return {'x': x,\r\n>             'y_': y_,\r\n>             'keep_prob': keep_prob,\r\n>             'is_training': is_training,\r\n>             'train_step': train_op,\r\n>             'accuracy': accuracy,\r\n>             'cross_entropy': cross_entropy}\r\n> ```\r\n> If `FLAGS.phase` is \"train\", the model is trained on the training set. But when I try to evaluate the model on the validation set and pass a `False` to placeholder `is_training`, the performance on validation set looks weird (it should be about 98% or higher).\r\n> \r\n> If `FLAGS.phase` is \"test\", the model restores a pre-trained model from checkpoint and evaluate on the test set. Again, performance is really poor.\r\n> \r\n> Am I using passing parameters to 'slim.batch_norm' incorrectly? How to use batch normalization for inference?\r\n\r\nIs it because  is_training should be a python boolean rather than a placeholder?", "for slim.batch norm, there is another parameters call trainable, which is true as default. When you do evaluation, turn off that. Also, for small data, it's possible to need more steps than the one without batch_norm.", "> for slim.batch norm, there is another parameters call trainable, which is true as default. When you do evaluation, turn off that. Also, for small data, it's possible to need more steps than the one without batch_norm.\r\n\r\n@balansky Hi, if I set the is_training=None in slim.batch_norm but everything else is the default, is it still on the training phase?"]}, {"number": 7468, "title": "Update README.md", "body": "added hahahaha", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "\ud83d\udc4d "]}, {"number": 7467, "title": "Feature request: Add early stopping mechanism to slim.evaluation_loop", "body": "Would it be possible to add early stopping mechanism to slim.evaluation_loop?\r\n\r\n", "comments": ["@sguada, @nathansilberman  could you comment? Could this be contributed by external users? Is there a way to do it already?", "+1 <bump>", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I think add support for hooks like \r\n\r\n> tf.train.MonitoredTrainingSession\r\n\r\nwould be a generic way to support this kind of requirements.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @sguada: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @sguada: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @sguada: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @sguada: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @sguada: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing until someone wants to contribute it."]}, {"number": 7466, "title": "Integrate gRPC changes to fix #6116", "body": "The gRPC won't build for some reasons, first make a hackish fix just for the review.", "comments": ["Can one of the admins verify this patch?", "This is not complete, do not merge.", "rpcbench_test results:\r\n* without the grpc fix\r\n```\r\nBM_ShardedProgram/1/1         125990       5604\t3 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/3         515428       1000\t5 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/5         673779        987\t7 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/15       1366261        456\t17 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/60       5010130        100\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/15/1        873925        837\t17 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/15/3       3341377        212\t47 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/15/5       5391330        100\t77 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/1       1588803        437\t32 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/2       6168890        100\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/3      11095890        100\t92 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/5      20620620        100\t152 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/60/1       2921452        230\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/60/3      44074260        100\t182 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/60/5      83405230        100\t302 nodes; Multi device; tensor bytes/send: 8\r\nBM_RPC/30/2                  6221740        100\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_RPC/30/1000               7289430        100\t62 nodes; Multi device; tensor bytes/send: 4000\r\nBM_RPC/30/100000            98748280        100\t62 nodes; Multi device; tensor bytes/send: 400000\r\nBM_RPC/1/100000000        4321044180        100\t4 nodes; Multi device; tensor bytes/send: 400000000\r\nBM_SingleDevice/1/1           106932       6345\t3 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/30/2          102905       5610\t62 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/60/5          150717       5097\t302 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/4/10000      9268440        100\t40002 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/1/1000000  448853650        100\t1000002 nodes; Single device; tensor bytes/send: 8\r\n```\r\n* with the grpc fix\r\n```\r\nBM_ShardedProgram/1/1         105292       6073\t3 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/3         823504       1000\t5 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/5         698704        961\t7 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/15       1412678        460\t17 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/1/60       5179623        146\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/15/1        960009        738\t17 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/15/3       3826291        182\t47 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/15/5       6762430        100\t77 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/1       1621082        404\t32 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/2       7305350        100\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/3      16084080        100\t92 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/30/5      26449080        100\t152 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/60/1       3076433        208\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/60/3      55058800        100\t182 nodes; Multi device; tensor bytes/send: 8\r\nBM_ShardedProgram/60/5     111056120        100\t302 nodes; Multi device; tensor bytes/send: 8\r\nBM_RPC/30/2                  6928160        100\t62 nodes; Multi device; tensor bytes/send: 8\r\nBM_RPC/30/1000               8062080        100\t62 nodes; Multi device; tensor bytes/send: 4000\r\nBM_RPC/30/100000            95994550        100\t62 nodes; Multi device; tensor bytes/send: 400000\r\nBM_RPC/1/100000000        1332521740        100\t4 nodes; Multi device; tensor bytes/send: 400000000\r\nBM_SingleDevice/1/1           112106       5918\t3 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/30/2          134409       4993\t62 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/60/5          154851       4168\t302 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/4/10000     13725570        100\t40002 nodes; Single device; tensor bytes/send: 8\r\nBM_SingleDevice/1/1000000  446881410        100\t1000002 nodes; Single device; tensor bytes/send: 8\r\n```\r\nActually this benchmark test looks quite noisy and unstable, just pay attention of this case (newly added in this patch):\r\n```\r\nBM_RPC/1/100000000        4321044180        100\t4 nodes; Multi device; tensor bytes/send: 400000000\r\n```\r\n```\r\nBM_RPC/1/100000000        1332521740        100\t4 nodes; Multi device; tensor bytes/send: 400000000\r\n```", "Changes look good to me (besides the one comment). Let me know when it's ready for merging.", "Merged with the master for the protobuf update.\r\n@jhseu I made a quick and dirty fix over grpc.BUILD, but I think it can be merged now. We can update it when grpc side fix it's build file.", "(Testing first)\r\nJenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "@mrry this fails the windows build, but passes everything else. Presumably https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/patches/grpc/CMakeLists.txt needs to be updated.\r\n\r\nNot planning to learn cmake at the moment :) Derek, how do you want to proceed?", "Jenkins, test this please", "There's not too much to change:\r\n\r\n1. Update [this line](https://github.com/tensorflow/tensorflow/blob/89059e6f0b2788b624e744afc21ba2472523a250/tensorflow/contrib/cmake/external/grpc.cmake#L6) to refer to the new commit for gRPC.\r\n2. Copy each of the file lists from the corresponding `cc_library` in `grpc.BUILD` to the `add_library()` blocks in [`grpc/CMakeLists.txt`](https://github.com/tensorflow/tensorflow/blob/89059e6f0b2788b624e744afc21ba2472523a250/tensorflow/contrib/cmake/patches/grpc/CMakeLists.txt).", "Ah, ok, I'll make an attempt later today.", "Jenkins, test this please", "@mrry So the latest gRPC strictly requires SSL through the census code. It might be possible to unravel by patching the code, but I'm not sure it's worth it.\r\n\r\nI updated the CMake files to make boringssl required, and the build works (on Linux at least), but adds additional build-time dependencies for Windows. You can see that list here:\r\nhttps://github.com/google/boringssl/blob/master/BUILDING.md\r\n\r\nSince it's rare for people to build for Windows from scratch (they'd more likely use the package we build), should we just install those dependencies on our Windows machines and require boringssl from now on?", "Hmm, the build of boringssl on Windows is going to be pretty tedious. I don't know much about Census, but it looks like the only SSL reference in its code is [here](https://github.com/grpc/grpc/blob/36ffc6e0f73a1b13a79bf120b7a2517136b8e5d5/src/core/ext/census/tracing.c#L39), in a file with a bunch of no-op implementations that doesn't appear to use any of the included headers.\r\n\r\nTo avoid changing the test infrastructure (and making the build more complicated for a library we don't end up using), could we do one of the following:\r\n\r\n* Provide an empty stub for `<openssl/rand.h>`, like we do for [`<unistd.h>`](https://github.com/tensorflow/tensorflow/blob/43c71a03380d8de18202cc399563814b2f438cd2/tensorflow/contrib/cmake/patches/gif/unistd.h) when building giflib?\r\n* Avoid building the Census source altogether, since IIUC it isn't referenced from any other part of TensorFlow or gRPC?", "Jenkins, test this please", "Jenkins, test this please", "@mrry Thanks! The empty file worked.", "Note: currently blocked on a large RPCs bug for gRPC on Windows. See https://github.com/grpc/grpc/pull/9826", "@llhe please resolve changes when you get a chance, thanks!", "Jenkins, test this please", "Jenkins, test this please.", "License check failed. Please add the copyright notice to all files. Thanks!", "Note that we're blocked on a few gRPC issues that @vjpai is working on. I'll update this pull request when it's ready.", "Note that the bazel issues in grpc should be taken care of by the recently merged grpc/grpc#10639 . We're still working on the large message issue on Windows.\r\n", "@jhseu is this waiting for another grpc fix os is this it?", "@martinwicke We're waiting on one more fix.", "@jhseu is there an ETA on the fix?  Is there a PR already open for it?", "@vrv We're waiting on a fix for https://github.com/grpc/grpc/issues/10161\r\n\r\nI'll try syncing this pull request to the latest gRPC commit later today, though.", "(Note to self) Windows testing happening here:\r\nhttps://github.com/grpc/grpc/pull/10716", "@jhseu what's the current status?", "@ctiller will be picking up the windows debugging issue.\n\nOn Thu, May 4, 2017, 7:30 PM Vijay Vasudevan <notifications@github.com>\nwrote:\n\n> @jhseu <https://github.com/jhseu> what's the current status?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7466#issuecomment-299356772>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIdEkAcez6-Y24fLwqMjAAqo4PkojKNSks5r2onYgaJpZM4L_QY->\n> .\n>\n", "(in other words, we're still waiting on some fix to gRPC?)", "@vrv Yep", "Can one of the admins verify this patch?", "@llhe could you rebase to fix conflicts? \r\nWhat is the status here? Are we still waiting for grpc fixes? i'll mark this as stalled for now.", "@rmlarsen  We are still waiting changes from gRPC.", "Any news? This would have to be rebased. I'm tempted to close this until the gRPC changes are ready.", "I think this PR is important enough (3x faster large tensor copies) to remain tracked until it is merged. By the way why does it take so long in gRPC side? It's literally been over 6 months and it is supposed to be merged into v1.0 [initially](https://github.com/tensorflow/tensorflow/issues/6116#issuecomment-269716302).", "Periodic ping to check status.", "Update as of 22-JUN.  The gRPC team is assigning someone someone to clear this issue.  This issue was escalated and is being watch closely.  If updates are not seen feel free to ping this thread/issue.    ", "> Update as of 22-JUN. The gRPC team is assigning someone someone to clear this issue.\r\n\r\nDo we know where this stands, or is there a GRPC issue we can follow?", "@promiseofcake we're waiting on https://github.com/grpc/grpc/issues/10161 ", "Will this: https://github.com/tensorflow/tensorflow/pull/11768, render this change moot?", "@promiseofcake Yep, closing this pull request in favor of the new change. It was easier to redo from scratch than to bring this change up-to-date.", "@jhseu Just to make sure, has the fix [#6116](https://github.com/tensorflow/tensorflow/issues/6116) been checked into master branch of tensorflow? Thanks.", "Yep"]}, {"number": 7465, "title": "Library not loaded: @rpath/libcudart.8.0.dylib", "body": "Hi! Tensorflow is not properlly installed. \r\n\r\nHere's the output of: python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n\r\ndyld: warning, LC_RPATH $ORIGIN/../../_solib_darwin/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\r\ndyld: warning, LC_RPATH ../local_config_cuda/cuda/lib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\r\ndyld: warning, LC_RPATH ../local_config_cuda/cuda/extras/CUPTI/lib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: dlopen(/Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\n\r\nOS: OSX 10.11.6 \r\nTensorflow pip version: tensorflow-gpu==0.12.1\r\n\r\nMy \"cuda\" libs under /usr/local/cuda/lib/* \r\n\r\n/usr/local/cuda/lib/libcublas.8.0.dylib\t\t/usr/local/cuda/lib/libcusparse.8.0.dylib\t/usr/local/cuda/lib/libnppim.8.0.dylib\r\n/usr/local/cuda/lib/libcublas.dylib\t\t/usr/local/cuda/lib/libcusparse.dylib\t\t/usr/local/cuda/lib/libnppim.dylib\r\n/usr/local/cuda/lib/libcublas_device.a\t\t/usr/local/cuda/lib/libcusparse_static.a\t/usr/local/cuda/lib/libnppim_static.a\r\n/usr/local/cuda/lib/libcublas_static.a\t\t/usr/local/cuda/lib/libnppc.8.0.dylib\t\t/usr/local/cuda/lib/libnppist.8.0.dylib\r\n/usr/local/cuda/lib/libcuda.dylib\t\t/usr/local/cuda/lib/libnppc.dylib\t\t/usr/local/cuda/lib/libnppist.dylib\r\n/usr/local/cuda/lib/libcudadevrt.a\t\t/usr/local/cuda/lib/libnppc_static.a\t\t/usr/local/cuda/lib/libnppist_static.a\r\n/usr/local/cuda/lib/libcudart.8.0.dylib\t\t/usr/local/cuda/lib/libnppi.8.0.dylib\t\t/usr/local/cuda/lib/libnppisu.8.0.dylib\r\n/usr/local/cuda/lib/libcudart.dylib\t\t/usr/local/cuda/lib/libnppi.dylib\t\t/usr/local/cuda/lib/libnppisu.dylib\r\n/usr/local/cuda/lib/libcudart_static.a\t\t/usr/local/cuda/lib/libnppi_static.a\t\t/usr/local/cuda/lib/libnppisu_static.a\r\n/usr/local/cuda/lib/libcudnn.5.dylib\t\t/usr/local/cuda/lib/libnppial.8.0.dylib\t\t/usr/local/cuda/lib/libnppitc.8.0.dylib\r\n/usr/local/cuda/lib/libcudnn.dylib\t\t/usr/local/cuda/lib/libnppial.dylib\t\t/usr/local/cuda/lib/libnppitc.dylib\r\n/usr/local/cuda/lib/libcudnn_static.a\t\t/usr/local/cuda/lib/libnppial_static.a\t\t/usr/local/cuda/lib/libnppitc_static.a\r\n/usr/local/cuda/lib/libcufft.8.0.dylib\t\t/usr/local/cuda/lib/libnppicc.8.0.dylib\t\t/usr/local/cuda/lib/libnpps.8.0.dylib\r\n/usr/local/cuda/lib/libcufft.dylib\t\t/usr/local/cuda/lib/libnppicc.dylib\t\t/usr/local/cuda/lib/libnpps.dylib\r\n/usr/local/cuda/lib/libcufft_static.a\t\t/usr/local/cuda/lib/libnppicc_static.a\t\t/usr/local/cuda/lib/libnpps_static.a\r\n/usr/local/cuda/lib/libcufftw.8.0.dylib\t\t/usr/local/cuda/lib/libnppicom.8.0.dylib\t/usr/local/cuda/lib/libnvToolsExt.1.dylib\r\n/usr/local/cuda/lib/libcufftw.dylib\t\t/usr/local/cuda/lib/libnppicom.dylib\t\t/usr/local/cuda/lib/libnvToolsExt.dylib\r\n/usr/local/cuda/lib/libcufftw_static.a\t\t/usr/local/cuda/lib/libnppicom_static.a\t\t/usr/local/cuda/lib/libnvblas.8.0.dylib\r\n/usr/local/cuda/lib/libcuinj.8.0.dylib\t\t/usr/local/cuda/lib/libnppidei.8.0.dylib\t/usr/local/cuda/lib/libnvblas.dylib\r\n/usr/local/cuda/lib/libcuinj.dylib\t\t/usr/local/cuda/lib/libnppidei.dylib\t\t/usr/local/cuda/lib/libnvgraph.8.0.dylib\r\n/usr/local/cuda/lib/libculibos.a\t\t/usr/local/cuda/lib/libnppidei_static.a\t\t/usr/local/cuda/lib/libnvgraph.dylib\r\n/usr/local/cuda/lib/libcurand.8.0.dylib\t\t/usr/local/cuda/lib/libnppif.8.0.dylib\t\t/usr/local/cuda/lib/libnvgraph_static.a\r\n/usr/local/cuda/lib/libcurand.dylib\t\t/usr/local/cuda/lib/libnppif.dylib\t\t/usr/local/cuda/lib/libnvrtc-builtins.8.0.dylib\r\n/usr/local/cuda/lib/libcurand_static.a\t\t/usr/local/cuda/lib/libnppif_static.a\t\t/usr/local/cuda/lib/libnvrtc-builtins.dylib\r\n/usr/local/cuda/lib/libcusolver.8.0.dylib\t/usr/local/cuda/lib/libnppig.8.0.dylib\t\t/usr/local/cuda/lib/libnvrtc.8.0.dylib\r\n/usr/local/cuda/lib/libcusolver.dylib\t\t/usr/local/cuda/lib/libnppig.dylib\t\t/usr/local/cuda/lib/libnvrtc.dylib\r\n/usr/local/cuda/lib/libcusolver_static.a\t/usr/local/cuda/lib/libnppig_static.a\r\n", "comments": ["This issue has been reported before, if you search issues you'll find solutions (disable SIP, set DYLD_LIBRARY_PATH, etc)", "Closing it as duplicate of https://github.com/tensorflow/tensorflow/issues/6729"]}, {"number": 7464, "title": "Android Camera Demo sometimes doesn't works", "body": "I have built Tensorflow Android Camera Demo with a custom model.\r\nSometimes the app works correctly on my phone, but at other times it gives error and is terminated.\r\nThese are my adb logcat:\r\n```\r\n02-13 14:53:44.751 22219 22234 F libc    : Fatal signal 11 (SIGSEGV), code 2, fault addr 0xee9c2000 in tid 22234 (ImageListener)\r\n02-13 14:53:44.801  3038  3038 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n02-13 14:53:44.801  3038  3038 F DEBUG   : Build fingerprint: 'samsung/zeroltexx/zerolte:6.0.1/MMB29K/G925FXXU4DPGW:user/release-keys'\r\n02-13 14:53:44.801  3038  3038 F DEBUG   : Revision: '10'\r\n02-13 14:53:44.801  3038  3038 F DEBUG   : ABI: 'arm'\r\n02-13 14:53:44.811  3038  3038 F DEBUG   : pid: 22219, tid: 22234, name: ImageListener  >>> org.tensorflow.demo <<<\r\n02-13 14:53:44.811  3038  3038 F DEBUG   : signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xee9c2000\r\n02-13 14:53:44.811  3038  3038 F DEBUG   :     r0 000004a8  r1 eec1000c  r2 ee9b800c  r3 00009ff4\r\n02-13 14:53:44.811  3038  3038 F DEBUG   :     r4 00000000  r5 ffffff80  r6 00000662  r7 00000c80\r\n02-13 14:53:44.811  3038  3038 F DEBUG   :     r8 fffffcbf  r9 de0605d8  sl 00009ec0  fp 000001e0\r\n02-13 14:53:44.811  3038  3038 F DEBUG   :     ip 0003ffff  sp f3722408  lr 00000135  pc decf5ee6  cpsr 800e0030\r\n02-13 14:53:44.821  3038  3038 F DEBUG   :\r\n02-13 14:53:44.821  3038  3038 F DEBUG   : backtrace:\r\n02-13 14:53:44.821  3038  3038 F DEBUG   :     #00 pc 00090ee6  /data/app/org.tensorflow.demo-1/lib/arm/libtensorflow_demo.so\r\n02-13 14:53:44.821  3038  3038 F DEBUG   :     #01 pc 00086767  /data/app/org.tensorflow.demo-1/lib/arm/libtensorflow_demo.so (Java_org_tensorflow_demo_env_ImageUtils_convertYUV420ToARGB8888+142)\r\n02-13 14:53:44.821  3038  3038 F DEBUG   :     #02 pc 000523e7  /data/app/org.tensorflow.demo-1/oat/arm/base.odex (offset 0x2b000) (void org.tensorflow.demo.env.ImageUtils.convertYUV420ToARGB8888(byte[], byte[], byte[], int[], int, int, int, int, int, boolean)+218)\r\n02-13 14:53:44.821  3038  3038 F DEBUG   :     #03 pc 0003fdad  /data/app/org.tensorflow.demo-1/oat/arm/base.odex (offset 0x2b000) (void org.tensorflow.demo.ClassifierActivity.onImageAvailable(android.media.ImageReader)+1064)\r\n02-13 14:53:44.821  3038  3038 F DEBUG   :     #04 pc 03c37c61  /system/framework/arm/boot.oat (offset 0x2f37000)\r\n02-13 14:53:45.011  3038  3038 F DEBUG   :\r\n02-13 14:53:45.011  3038  3038 F DEBUG   : Tombstone written to: /data/tombstones/tombstone_07\r\n```\r\nHow can I solve this problem?", "comments": ["It looks like the yuv>RGB conversion is breaking. What sort of device are you using? On some 5.0 devices only the Y plane gets returned, resulting in a greenish image.", "I'm using Android 6.0. The strange thing is that I have this error only sometimes", "Thats odd then. When you turn on debug mode (volume button), does the thumbnail image appear normal?", "I don't know if thumbnail image appear normal. This is what I see: https://storage.googleapis.com/tensorflow_android_demo/Screenshot_20170213-154631.png", "That's definitely abnormal. The stride is wrong and the green noise at the bottom is uninitialized data. It's crashing when it tries to access data that it shouldn't be reading from, apparently because the image is a different res than it thinks it is. It's hard to tell just from this image, but does the preview seem stretched on screen as well?\r\n\r\nThe question is why there's a mismatch. You might try forcing it to a preview resolution like 640x480 and see if that helps.\r\n\r\nCan you paste the full log please? And what is the exact model of your device? Sometimes camera drivers have odd behavior at particular resolutions.\r\n", "Here you can find the full log, with all error and warning:\r\nhttps://storage.googleapis.com/tensorflow_android_demo/log.txt\r\nand here the log, with all error, warning and info:\r\nhttps://storage.googleapis.com/tensorflow_android_demo/log_info.txt\r\n\r\nMy device is a Samsung Galaxy S6.\r\n\r\nMaybe these errors are a problem of my phone. If I use others devices the app seems to work correctly and the thumbnail image appears normal. ", "Does the preview appear to be rotated? If so, it's probably treating it as a 320x480 image rather than a 480x320 image.\r\n\r\nCan you try hardcoding sensorOrientation in [ClassifierActivity.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java#L147) to be 0? My guess is that this device is returning incorrect info about the rotation of the preview frames.", "@davideb91 Any luck on this? Also, is there any chance you've made changes to the activity orientation settings in AndroidManifest.xml?", "@andrewharp Hi Andrew, long time no see :). Sorry to bug you in another thread, but we also have come up to this issue. When testing our little app on my friends phone (which is also Galaxy S6) we are also getting this error (SIGSEV 11 when ImageUtils.convertYUV420ToARGB8888 is executed) and this is happening only sometimes.\r\n\r\nApart of this, twisted bitmap is obtained when we are showing to user what was examined by TF. See in attachement. This only happens on S6. Regarding sensor orientation, we are getting same rotation value as on another phones, so this probably is not the root cause. Maybe some issue with armeabi-v7a and arm64 compatibility of .so libraries? I only compiled armeabi-v7a version assuming that they will work properly on arm64 (on S6) as well.\r\n\r\n![s6](https://cloud.githubusercontent.com/assets/19685528/23383936/eed2cb9e-fd48-11e6-83f7-7c45daa21912.png)\r\n\r\nOn the other devices which are available to us for testing: Galaxy S5, Xiaomi Mi4, Galaxy J5 and Galaxy Tab S2 9.7, it works fine and displays correct snapshot.", "@bazinac Yeah, seems to be some issue with the Samsung Galaxy S6. Would you mind stepping through higher preview resolutions to see if any of them work?\r\n\r\nI doubt it's actually a TF issue, as the preview image shown on the screen is pasted there directly by the camera. Though if the S6 is consistently bad we might want to update the demo to accommodate it anyway. My assumption is that the sigsegv happens because a bug in the camera code is causing it to provide a preview frame that is not the resolution it claimed.", "Closing due to lack of activity.", "@andrewharp issue exists on multiple devices like the nexus 6p, it not limited to Samsung s6. Can we reopen this issue and find a fix?", "@andrewharp i tried higher resolution (720) on Galaxy S6 however output still does not look good. So it seems that issue is not related to preview size.\r\n![screenshot_20170303-194946](https://cloud.githubusercontent.com/assets/22727681/23848469/57998934-07cf-11e7-96b6-5f2fadc6cd26.png)\r\n", "@vi7us @bazinac You're still using the same camera code and color conversion code as the demo, correct? Does a new build of the TF demo also show the same behavior on the S6 for you? (you can check by turning on debug mode with the volume key)\r\n\r\n@OverratedGman Are you seeing this in the same app as @vi7us and @bazinac ?", "@andrewharp we are using code from tf demo however not the latest build. Right now have quite limited access on internet as we are on holidays and will be able to test using latest tf demo only after 21st March. Is that OK or do you need feedback from us earlier?", "@davideb91 @bazinac @OverratedGman @vi7us \r\nWould you mind logging your device's `android.info.supportedHardwareLevel`? I'm wondering if this is a common factor in all of this.", "Hi @andrewharp, my android.info.supportedHardwareLevel on Galaxy S5 (which work ok) is 2 (this is INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY probably), and on @vi7us's S6 it is 1 (which is INFO_SUPPORTED_HARDWARE_LEVEL_FULL). Then clearly there is difference. On Galaxy S5 it uses legacy camera HAL. It could be interesting, if someone like @OverratedGman who has Nexus 6p could check it too...\r\n\r\n\r\nWe will try to run fresh clean TF android demo and check whether it works now. **To explain**:  Our application just originally took some parts of the demo, but it is heavilly modified, so there is no debug mode. I have just discovered that captured bitmaps are not cropped properly on Galaxy S6, resulting into artifacts. I suspect method _ImageUtils.convertYUV420ToARGB8888_, that should manipulate rgbBytes variable (which is the part I took from the demo). This method also sometimes caused app to crash on Galaxy S6.\r\n\r\n@vi7us is currently building latest TF to see whether mint demo works properly.\r\n\r\nSorry to have such a long reply latency, we were on the vacation and tried to avoid computers completely :)", "@andrewharp, all,\r\n\r\nI tried using both latest tf demo and our application using latest tf however it seems that thumbnail(s) are still distorted with application crashing occasionally. Attached are screenshot from latest demo and screen\r\n![screenshot_20170323-003440](https://cloud.githubusercontent.com/assets/22727681/24225814/dc9382e4-0f62-11e7-8e1c-e71f85758e27.png)\r\nshot of android studio when application crashed (using our application), maybe it could help.\r\n\r\n![applicationcrash](https://cloud.githubusercontent.com/assets/22727681/24225760/87365ca4-0f62-11e7-8749-f6d8087070dc.png)\r\n ", "@vi7us This screenshot is like inception of the Inception model :)\r\n\r\nOk, so it's failing on some devices with android.info.supportedHardwareLevel of 1, and some of 2 it seems. Unfortunate we can't use that as a discriminating factor. \r\n\r\nA couple more questions:\r\n- What Android API levels are these devices?\r\n- If you hardcode the res to something like 640x480, does that help?\r\n\r\nThere is an older conversion of the demo from the Camera2 API to use the original android.hardware.Camera here: https://github.com/hamidb/tensorflow/blob/api20/tensorflow/examples/android/src/org/tensorflow/demo/CameraConnectionFragment.java\r\n\r\nI feel this may address your problems -- if it does, contributions welcome to patch the demo to use android.hardware.Camera on lower API levels.", "@andrewharp, @bazinac, @OverratedGman  I tried suggested and here is what i got\r\nFirst, when some of resolutions are hardcoded, thumbnails look good and application does not crash :1st_place_medal:  from what i observed, more common res. like 320x240, 640x480, 1280x720, 1920x1080 work, less common like 480x320, 720x480 or 1088x1088 does not. So thanks for this suggestion! Unfortunately I'm not able to confirm if issue is only the resolution as none of \"faulty\" resolutions can be used on Galaxy S5 device.\r\n\r\nFor the hardware level, actually it works on the device with 2 and fails on device with level 1 (however it could by caused by the thumbnail size again)\r\n\r\nLast thing, I was not able to make work the older version of camera class in current demo, however not sure if it is needed.\r\n\r\n\r\n", "Great! Thanks for checking that out.\r\n\r\nMaybe it would make sense to set preferred sizes of 640x480 for classification/detection and 1920x1080 for stylization, and have it fall back to the current logic only if they're not available.\r\n\r\nJust to note, the only reason I have it pick the smallest sizes possible right now is to minimize CPU  spent converting yuv->RGB and then rescaling. The actual frame size TF sees is always the same. The conversion is relatively fast, though, so doubling the number of pixels is not a huge cost.", "Yes you're right, even when using higher resolutions, application is still smooth so we will probably just filter resolutions to ones that work and use non-supported only in case of none of supported resolutions are available. I do not expect that fall back will happen on many devices. \r\n\r\nBtw i do not think that issue is caused by small resolution as 320*240 works as well, maybe it could be ratio of screen width / height?  ", "Possibly, who knows what black magic is going on inside the camera drivers :p\r\n\r\nI've fixed this internally; should show up in the next internal->external push.", "The demo now uses 640x480 for detection/classification and 1280x720 for stylization, which should be more widely supported resolutions.\r\n\r\nIf anybody has more issues, can you please replay with your device model/API level/supportedHardwareLevel?\r\n\r\nRemaining problem cases should be covered by #8736, so it would be good to know if there are any devices still experiencing problems with INFO_SUPPORTED_HARDWARE_LEVEL_FULL or greater.\r\n\r\n", "Closing as fixed; please reply with your device model/Android version/supportedHardwareLevel if you continue to experience any similar issues."]}, {"number": 7463, "title": "Initialize error in 0.12.1", "body": "I run my code well under Tensorflow 0.10, but after I update the version to 0.12.1, all the variables throws a `FailedPreconditionError (see above for traceback): Attempting to use uninitialized value W` error. And my code is  unchanged except I use` tf.global_variables_initializer()` instead of `tf.initialize_all_variables()`. I tried to add `tf.GraphKeys.VARIABLES = tf.GraphKeys.GLOBAL_VARIABLES` under `import tensorflow as tf` but it didn't work. So I opened this issue because I don't know where the bug is.\r\n\r\nMy code is as below. Omitted some details to be more readable.\r\nAnd the error is different every time. All tf.Variable defined variables are mentioned wrong.\r\n\r\n```\r\nclass model(object):\r\n    def __init__(self, paras):\r\n        self.D = tf.constant(D, dtype = tf.float32)\r\n        self.Q = tf.constant(Q, dtype = tf.float32)\r\n\r\n        if self.USE_FEATURE:\r\n            self.CF = tf.Variable((np.random.rand(self.rank, d_F) - 0.5) / self.rank, \\\r\n                                  dtype = tf.float32, name = 'CF')  #error here\r\n        self.W = tf.Variable((np.random.rand(self.rank, sample_num) - 0.5) / self.rank / 200, \\\r\n                                  dtype = tf.float32, name = 'W')   #error here\r\n        self.C = tf.Variable((np.random.rand(context_num, self.rank) - 0.5) / self.rank, \\\r\n                                   dtype = tf.float32, name = 'C')  #error here\r\n        \r\n        ED = tf.transpose(self.Q) * (1.0 / (1.0 + tf.exp(- tf.matmul(self.C, self.W))))\r\n        recons = self.D - ED\r\n        W_grad = tf.matmul(tf.transpose(self.C), recons)\r\n        self.W_grad = tf.Variable(W_grad, dtype = tf.float32)  #error here\r\n        \r\n        self._build_update_W_grad()\r\n        if not self.USE_FEATURE:\r\n            self._build_alter_W()\r\n        else:\r\n            self._build_W_with_F()\r\n        self._build_alter_C()\r\n\r\n    def _run(self, sess):\r\n        tf.initialize_all_variables().run()  #where the error throws\r\n\r\n        for i in xrange(self.max_iter):\r\n            if (i + 1) % self.prun_step == 0:\r\n                self.mu = self.mu * self.prun_rate\r\n            if (i + 1) % 2 == 1:\r\n                for j in xrange(self.inner_maxiter):\r\n                    if not self.USE_FEATURE:\r\n                        self.up_W_grad.run()\r\n                        self.up_W.run()\r\n                    else:\r\n                        self.up_W_grad.run()\r\n                        for k in xrange(self.sgd_batch):\r\n                            self.up_W_CF.run()\r\n                            #raise NotImplementedError\r\n            else:\r\n                for j in xrange(self.inner_maxiter):\r\n                    self.up_C.run()\r\n        \r\n        W = self.W.eval()\r\n        C = self.C.eval()\r\n        print 'end training. save W and C'\r\n        return W, C\r\n\r\n    def _build_alter_W(self):\r\n        #codes\r\n        self.up_W = tf.group(updata_W)\r\n        \r\n    def _build_W_with_F(self):    \r\n        #codes\r\n        self.up_W_CF = tf.group(updata_W, updata_CF)\r\n        #raise NotImplementedError\r\n   \r\n    def _build_alter_C(self):\r\n        #codes\r\n        self.up_C = tf.group(updata_C)\r\n   \r\n    def _build_update_W_grad(self):\r\n        #codes\r\n        self.up_W_grad = tf.group(update_W_grad)\r\n\r\n#main program\r\ntrain_epoch = model(paras)\r\nwith tf.Session(config = config) as sess:\r\n    W, C = train_epoch._run(sess)\r\n```\r\n\r\nCan anybody help? The program worked well in 0.10 but crashed after I updated to 0.12.1. I changed nothing but `tf.initialize_all_variables().run()` to `tf.global_variables_initializer().run()`. ", "comments": ["I suspect the bug is in your code rather than in tensorflow. One debugging tip is to list all of your variables before you run initializer, and make sure that 1) you don't have variable initializers depending on other variables 2) you don't have any unexpected variables in there", "Please try what @yaroslavvb suggests. Thanks!", "Solved. Thanks a lot! @yaroslavvb "]}, {"number": 7462, "title": "ValueError: Variable d_bn1/d_bn1_2/d_bn1_2/moments/moments_1/mean/ExponentialMovingAverage/biased does not exist", "body": "I am trying to run the model from here, http://bamos.github.io/2016/08/09/deep-completion/ , but i am facing this issue. Kindly suggest some approach.\r\nTraceback (most recent call last):\r\n  File \"train-dcgan.py\", line 39, in <module>\r\n    is_crop=False, checkpoint_dir=FLAGS.checkpoint_dir)\r\n  File \"/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/model.py\", line 65, in __init__\r\n    self.build_model()\r\n  File \"/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/model.py\", line 81, in build_model\r\n    self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\r\n  File \"/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/model.py\", line 312, in discriminator\r\n    h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))\r\n  File \"/Users/asuma2/Code/GAN/ImageCompletion/dcgan-completion.tensorflow/ops.py\", line 34, in __call__\r\n    ema_apply_op = self.ema.apply([batch_mean, batch_var])\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 391, in apply\r\n    self._averages[var], var, decay, zero_debias=zero_debias))\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 70, in assign_moving_average\r\n    update_delta = _zero_debias(variable, value, decay)\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/moving_averages.py\", line 177, in _zero_debias\r\n    trainable=False)\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 1024, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 850, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 346, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 331, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/Users/asuma2/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 650, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable d_bn1/d_bn1_2/d_bn1_2/moments/moments_1/mean/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n", "comments": ["In general we cannot support other TensorFlow examples written by others. On the other hand, this question should be asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "you should change the version of tensorflow "]}, {"number": 7461, "title": "Add aliases to make layer names more consistent?", "body": "In the new [layers API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/layers.py) the convolutional layers have short names (conv1d, conv3d, etc.) while the pooling layers have full names (max_pooling1d, average_pooling3d, etc.) which seems inconsistent. tf.contrib.layers had aliases. Could something like that be added in to the core layers too?\r\n\r\nE.g.\r\n```\r\nmax_pool1d = max_pooling1d\r\nconvolution1d = conv1d\r\n```\r\nand so on.", "comments": ["Or preferably just settle on a naming convention (full names) instead, and never shorten it, which is in line with the rest of TensorFlow (`tf.nn.sigmoid_cross_entropy_with_logits` comes to mind).", "cc @fchollet who added that file", "Both full names and short aliases are valid. E.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/pooling.py#L613\r\n\r\nNot all of these aliases are currently publicly exposed, but they will soon be.", "Sweet. :+1:"]}, {"number": 7460, "title": "Check failed GetConvolveAlgorithms() on Windows", "body": "I'm following through the set of Udacity Tensorflow examples. \r\n\r\nI can run the first three without any problems, but when I use the code found in the [fourth example,](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb) Python crashes and I receive a track trace that leads to \r\n```\r\nc:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\n\r\nI'm using tensorflow-gpu which I've installed via: `pip install tensorflow-gpu`\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.VERSION)\r\n```\r\n\r\nReports the version as 0.12.1\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI see there's issue #6509 which mentions `tf.one_hot()` and #6822 which mentions `MatrixDiag` and `OneHot` but I am not using either of these in my code so I'm not sure if they're directly related.\r\n\r\n### Environment info\r\n- OS: Windows 10 Pro 64-bit (10.0, Build 14393) \r\n- CUDA 8\r\n- TensorFlow 0.12.1\r\n- Nvidia GeForce 860M GPU\r\n\r\n### If possible, provide a minimal reproducible example \r\n\r\n1. Run the source provided on [tensorflow/examples/udacity/1_notmnist.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb)\r\n2. Run the source provided on [tensorflow/examples/udacity/4_convolutions.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb)\r\n\r\n### Logs or other output that would be helpful\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\nTraining set (200000, 28, 28) (200000,)\r\nValidation set (10000, 28, 28) (10000,)\r\nTest set (10000, 28, 28) (10000,)\r\nTraining set (200000, 28, 28, 1) (200000, 10)\r\nValidation set (10000, 28, 28, 1) (10000, 10)\r\nTest set (10000, 28, 28, 1) (10000, 10)\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GPU\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 0.993\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.00GiB\r\nFree memory: 830.97MiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GPU, pci bus id: 0000:01:00.0)\r\nInitialized\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:378] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\kernels\\conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\n```\r\n", "comments": ["After posting I re-read the error log and noticed that it suggest I upgrade CuDNN. I've upgraded to version 5.1 but I'm now seeing the following error instead:\r\n\r\n```\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cudnn64_5.dll\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:3459] Unable to load cuDNN DSO\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\n0.12.1\r\nTraining set (200000, 28, 28) (200000,)\r\nValidation set (10000, 28, 28) (10000,)\r\nTest set (10000, 28, 28) (10000,)\r\nTraining set (200000, 28, 28, 1) (200000, 10)\r\nValidation set (10000, 28, 28, 1) (10000, 10)\r\nTest set (10000, 28, 28, 1) (10000, 10)\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GPU\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 0.993\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.00GiB\r\nFree memory: 830.97MiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GPU, pci bus id: 0000:01:00.0)\r\nInitialized\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:221] Check failed: s.ok() could not find cudnnCreate in cudnn DSO; dlerror: cudnnCreate not found\r\n```", "The last error `Check failed: s.ok() could not find cudnnCreate in cudnn DSO; dlerror: cudnnCreate not found` was due to a bad copy on my part so CUDA couldn't find the libraries it needed. \r\n\r\nTo recap for future viewers:\r\n 1. I had to make sure I had the proper version of CuDNN downloaded. (In my case version 5.1)\r\n 2. Once I had downloaded and extracted CuDNN, I had to ensure that `cuda/bin` was discoverable on my `PATH`."]}, {"number": 7459, "title": "Decode csv failed to decode csv file with commas in cell properly.", "body": "### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled CUDA and cuDNN: \r\n\r\nTensorflow is build from source.\r\n\r\nTensorflow version: 1.0.0-rc2\r\n\r\n### Problem\r\n```python\r\nimport tensorflow as tf\r\nimport os\r\nfilename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(),\"train.csv\")])\r\nreader = tf.TextLineReader(skip_header_lines=1)\r\nkey, value = reader.read(filename_queue)\r\ndecoded = tf.decode_csv(value, record_defaults = [[0.0], [0.0], [0], [\"\"],[\"\"], [0.0], [0.0], [0.0], [\"\"], [0.0], [\"\"], [\"\"]])\r\npassenger_id, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked = tf.train.shuffle_batch(decoded, batch_size=9, capacity=450, min_after_dequeue=9)\"\r\n```\r\n\r\nThe data to import:\r\n```\r\nPassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\r\n1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\r\n2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C\r\n3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S\r\n4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S\r\n5,0,3,\"Allen, Mr. William Henry\",male,35,0,0,373450,8.05,,S\r\n6,0,3,\"Moran, Mr. James\",male,,0,0,330877,8.4583,,Q\r\n```\r\n\r\nThe column \"name\" contains comma. The tenforflow reports invalid arguments.\r\n", "comments": ["I changed your example to actually run\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport os\r\nwith tf.device(\"/cpu:0\"):\r\n\r\n  filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(),\"train.csv\")])\r\n  reader = tf.TextLineReader(skip_header_lines=1)\r\n  key, value = reader.read(filename_queue)\r\n  decoded = tf.decode_csv(value, record_defaults = [[0.0], [0.0], [0], [\"\"],[\"\"], [0.0], [0.0], [0.0], [\"\"], [0.0], [\"\"], [\"\"]])\r\n  passenger_id, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked = tf.train.shuffle_batch(decoded, batch_size=9, capacity=450, min_after_dequeue=9)\r\n  with tf.Session() as session:\r\n      coordinator = tf.train.Coordinator()\r\n      tf.train.start_queue_runners(session, coord=coordinator)\r\n\r\n\r\n      print (session.run(name))\r\n\r\n      coordinator.request_stop()\r\n      coordinator.join()\r\n\r\n```\r\n\r\nThis worked fine and produced the following output.\r\n```\r\n[b'Futrelle, Mrs. Jacques Heath (Lily May Peel)'\r\n b'Futrelle, Mrs. Jacques Heath (Lily May Peel)' b'Heikkinen, Miss. Laina'\r\n b'Cumings, Mrs. John Bradley (Florence Briggs Thayer)'\r\n b'Cumings, Mrs. John Bradley (Florence Briggs Thayer)'\r\n b'Heikkinen, Miss. Laina' b'Allen, Mr. William Henry' b'Moran, Mr. James'\r\n b'Cumings, Mrs. John Bradley (Florence Briggs Thayer)']\r\n```\r\n\r\nThis stackoverflow might be helpful. \r\nhttp://stackoverflow.com/questions/42156479/can-tensorflow-handle-categorical-features-with-multiple-inputs-within-one-colum/42168658#42168658\r\n\r\nBarring a reproducible test case, I will need to close the issue. Thanks!\r\n\r\n", "Thanks."]}, {"number": 7458, "title": "Feature Request: Default project", "body": "New default project, for anyone new to these things.\r\n# watcher# \r\n>ip camera+tensorflow in order to watch your yard.\r\n>requirements: camera+RPi+internet(pc?) \r\n>default project that is somewhat 'advanced' for 'I know how to apt-** people'\r\n>install, teach, and it works", "comments": []}, {"number": 7457, "title": "UnboundLocalError: local variable 'status' referenced before assignment", "body": "The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:\r\n\r\n python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.12.1\r\n\r\n**Here's the error**:\r\n\r\nEpoch 2/2\r\n19125/19125 [==============================] - 78s - loss: 0.4568 - acc: 0.8681 - val_loss: 2.1682 - val_acc: 0.4104\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f747161e160>>\r\nTraceback (most recent call last):\r\n  File \"/home/p3/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 581, in __del__\r\nUnboundLocalError: local variable 'status' referenced before assignment\r\n", "comments": ["This should've been fixed in https://github.com/tensorflow/tensorflow/pull/7386 , can you see if error persists in latest nightly version?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I've got this in tensorflow (1.0.1).", "Still persists.", "Happens intermittently with `tensorflow/tensorflow:1.0.1-gpu-py3` docker image and keras.\r\n\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f3a69e8c470>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 582, in __del__\r\nUnboundLocalError: local variable 'status' referenced before assignment\r\n\r\n```", "Also happened for me upon saving a model in Ubuntu 16.0.4, Python3, and Keras:\r\n```python\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f7c3e1fb2b0>>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 582, in __del__\r\nUnboundLocalError: local variable 'status' referenced before assignment\r\n```\r\n\r\n```python\r\n>>> tf.__version__\r\n'1.0.1'\r\n>>> keras.__version__\r\n'2.0.4'\r\n```", "Happend to me too today on our GPU-Cluster:\r\n```\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7f145ff41e10>>\r\nTraceback (most recent call last):\r\n  File \"/work/kielholz/envs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 582, in __del__\r\nUnboundLocalError: local variable 'status' referenced before assignment\r\n```\r\n\r\n```\r\nuname -a\r\nLinux [...] 4.8.0-46-generic #49~16.04.1-Ubuntu SMP Fri Mar 31 14:51:03 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\nOn a GeForce GTX 980 Ti.\r\n\r\n```\r\npython --version\r\nPython 3.5.3 :: Continuum Analytics, Inc.\r\n\r\n```\r\n```\r\n\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n>>> tensorflow.__version__\r\n'1.0.1'\r\n>>> import keras\r\nUsing TensorFlow backend.\r\n>>> keras.__version__\r\n'2.0.3'\r\n```\r\n\r\n", "@zheng-xq , could you take a look?", "The exception comes from session.py. Passing to Derek fro now.", "I'm pretty sure this has been fixed since the 1.1 release. I'm closing this issue for now, but feel free to reopen it if you can reproduce with a newer version of TensorFlow.", "I got this error today while training my network on keras.\r\n\r\nEpoch 10/10\r\n19308/19286 [==============================] - 62s - loss: 0.0220 - acc: 0.1795 - val_loss: 0.0231 - val_acc: 0.1846\r\ndict_keys(['val_acc', 'val_loss', 'acc', 'loss'])\r\nAcc\r\n[0.1782905629139073, 0.1796146675076869, 0.18056705298013245, 0.17775015535748767, 0.18656870860927152, 0.17137973897756442, 0.17694536423841059, 0.18142738762061394, 0.17860099337748345, 0.17951108349797049]\r\nLoss\r\n[0.032948918208874613, 0.029692213859633675, 0.028213497242020654, 0.027055541726004047, 0.025659046015572666, 0.025640698771453779, 0.024098622677416005, 0.023113698205617934, 0.02143701246557202, 0.02202132222925083]\r\nValidation Loss\r\n[0.027665206672329651, 0.026977688277141799, 0.026762272280297782, 0.025648139429476319, 0.024419905385002494, 0.024124456143497338, 0.024165717970677895, 0.022718481179619013, 0.021766701916234244, 0.023056170290434046]\r\nException ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.Session object at 0x7fc9535b4a90>>\r\nTraceback (most recent call last):\r\n  File \"/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 581, in __del__\r\nUnboundLocalError: local variable 'status' referenced before assignment\r\n", "I also get this error, version 1.5\r\n\r\n`\r\n2018-03-19 02:51:05.913 EDT\r\nThe replica master 0 exited with a non-zero status of 1. Termination reason: Error. Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main \"__main__\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code exec code in run_globals File \"/root/.local/lib/python2.7/site-packages/object_detection/train.py\", line 167, in <module> tf.app.run() File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 124, in run _sys.exit(main(argv)) File \"/root/.local/lib/python2.7/site-packages/object_detection/train.py\", line 163, in main worker_job_name, is_chief, FLAGS.train_dir) File \"/root/.local/lib/python2.7/site-packages/object_detection/trainer.py\", line 366, in train saver=saver) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 790, in train return total_loss UnboundLocalError: local variable 'total_loss' referenced before assignment To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=301256752837&resource=ml_job%2Fjob_id%2Fshreya_object_detection_1521441960&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22shreya_object_detection_1521441960%22\r\n`", "i also get this error, version 1.6\r\n\r\n`Traceback (most recent call last):\r\n  File \"object_detection/train.py\", line 167, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/train.py\", line 163, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/research/object_detection/trainer.py\", line 360, in train\r\n    saver=saver)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 791, in train\r\n    return total_loss\r\nUnboundLocalError: local variable 'total_loss' referenced before assignment\r\n`", "I also get a similar error when attempting to train using the object detection API (identical to the comment above)\r\n\r\n`Traceback (most recent call last):\r\n  File \"object_detection/train.py\", line 167, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/train.py\", line 163, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/home/dave/tensorflow-master/models/research/object_detection/trainer.py\", line 370, in train\r\n    saver=saver)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 791, in train\r\n    return total_loss\r\nUnboundLocalError: local variable 'total_loss' referenced before assignment`"]}, {"number": 7456, "title": "Training using multiple GPUs returns Inf values for loss and Nan for grads. ", "body": "I have two Tesla K80 cards (2 GPUs per card) and I spent few days testing a MNIST classification model using multiple GPUs. What I found is that the training process would always diverge (got Nan for grads and Inf for loss) when I use two GPUs which are in the same card, however when I allocated two GPUs to my training operation from different cards, it would lead to convergence. By the way, everything worked well on a single GPU. \r\n\r\nI am not sure about how GPUs compute those networks and it is really weird two GPUs from the same card make my model diverge and from different cards can make it converge.\r\n\r\nThe output for divergence is like the below:\r\n```man\r\n2017-02-13 12:30:11.255323: step 10, loss = 5799703749333771039308345507840.00 (980.7 examples/sec; 0.102 sec/batch)\r\n2017-02-13 12:30:14.131089: step 20, loss = 2102245862526597403246592.00 (793.5 examples/sec; 0.126 sec/batch)\r\n2017-02-13 12:30:16.995940: step 30, loss = 2.30 (787.6 examples/sec; 0.127 sec/batch)\r\nW tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Nan in summary histogram for: layer2/weights_1\r\n\t [[Node: layer2/weights_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](layer2/weights_1/tag, layer2/weights/read/_117)]]\r\n```\r\nI used cpu to preprocess the input data read from tfreords. My code for computing the average grads:\r\n```man\r\ndef average_gradients(tower_grads):\r\n    average_grads = []\r\n    for grad_and_vars in zip(*tower_grads):\r\n        grads = []\r\n        for g, _ in grad_and_vars:\r\n            expanded_g = tf.expand_dims(g, 0)\r\n            grads.append(expanded_g)\r\n        grad = tf.concat(0, grads)\r\n        grad = tf.reduce_mean(grad, 0)\r\n        v = grad_and_vars[0][1]\r\n        grad_and_var = (grad, v)\r\n        average_grads.append(grad_and_var)\r\n    return average_grads \r\n```\r\n\r\nThe training_op:\r\n```man\r\ndef main(argv=None): \r\n    with tf.Graph().as_default(), tf.device('/cpu:0'):\r\n        x, y_ = get_input()\r\n        regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\r\n        \r\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\r\n        learning_rate = tf.train.exponential_decay(\r\n            LEARNING_RATE_BASE, global_step, 60000 / BATCH_SIZE, LEARNING_RATE_DECAY)       \r\n        \r\n        opt = tf.train.GradientDescentOptimizer(learning_rate)\r\n        \r\n        tower_grads = []\r\n        for i in range(N_GPU):\r\n            with tf.device('/gpu:%d' % i):\r\n                with tf.name_scope('GPU_%d' % i) as scope:\r\n                    cur_loss = get_loss(x, y_, regularizer, scope)\r\n                    tf.get_variable_scope().reuse_variables()\r\n                    grads = opt.compute_gradients(cur_loss)\r\n                    tower_grads.append(grads)\r\n        \r\n        grads = average_gradients(tower_grads)\r\n        for grad, var in grads:\r\n            if grad is not None:\r\n            \ttf.summary.histogram('gradients_on_average/%s' % var.op.name, grad)\r\n\r\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\r\n        for var in tf.trainable_variables():\r\n            tf.summary.histogram(var.op.name, var)\r\n\r\n        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\r\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\r\n```\r\n", "comments": ["What's your peer-to-peer matrix like (printed when you use tensorflow). I expect that if you use two chips from same card, it would configure it for peer to peer transfer and use DMA to transfer data. So perhaps there's some difference when using DMA that causes the discrepancy", "Peer to peer matrix, GPU 0 and 1 caused the discrepancy, 1 and 2 worked well.\r\n```man\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y\r\n```", "One possibility is that there's something happening with variable sync. IE, if TensorFlow is reading from GPU variable while another op is writing to it at the same time. Normally, in such situation, the variable would be partially updated, but updates do not cross boundaries of individual components. @poxvoculi is there an easy way to rule out GPU variable read reading a scalar in the middle of DMA write happening to it?", "@zheng-xq \r\n\r\nI have experienced a similar problem that I have not yet been able to fully diagnose.  Briefly, I've experienced NaNs on k80s and p100s that I suspect are due to simultaneous read/write operations on the same memory area resulting in corrupt values.  But I don't really have proof of this.  In my case, it occurs when using GPUDirect to DMA out of a GPU while the GPU is also doing compute.  My best understanding of these devices is that memory operations should be atomic at the cache line level: if DMA reads are asynchronous with writes, the read should get the value prior to the write or after, but never a mixture of before/after bits.  Since I'm seeing NaNs downstream in the gradient computations I guess in principle it's possible that they result from too much asynchrony in the computation (i.e. the computation sees legitimate values mixed from multiple phases, rather than corrupt values), but I'm doubtful.\r\n\r\nMy working solution is to make a temporary local copy on the GPU of a value prior to DMA'ing it off device.  This causes the NaNs to go away, at the cost of extra memory use. \r\n\r\nAccording to the NVIDIA documentation I've seen, the two GPU dies on one k80 card \"should\" act like two separate GPUs, and communication between them still uses DMA via the PCI bus.  So, the differences you see may be due to timing rather than fundamentally different memory access behavior.\r\n\r\nYou might try the temporary copy technique to see whether it avoids the problem.  I'd be interested in any further insights you gain into this problem.", "I also trained the full imagenet model on the same machine using multiple gpus, but this problem did not occur. The only difference in my view is the processing speeds of MNIST model and inception. ", "@poxvoculi Regarding your working solution, could you be more specific? Did you change the TensorFlow code or just changed some system configure?", "In my case I changed the TensorFlow source code in C++.   I'm working on a part of the internal system that is not (yet) open source, so I can't give you a code snippet to apply, but the basic idea is to copy values into a temporary write-once buffer and use that for the source whenever a Tensor is to be used by an Op on another device..     There is not a system config in TF to do this,  If it turns out to be a general problem affecting many users, I could make one. \r\n\r\nThe easiest way to try this would be to examine your program and look for every place where the output of an Op on one device becomes an input to an Op on a different device.  In each such case, create a distinct, identical value not with Id (because that doesn't force a copy), but by adding 0 (or mul by 1.0), then use that copy value as the input to the other Op.  Make sure that the copy is local to the  device where the first value was produced, so the DMA from one device to another reads from the copy.", "PS: I had a brief discussion with @zheng-xq about this, and he says that ops typically don't write to GPU directly across GPU boundaries, even with DMA enabled. IE, if you do GPU0 assign to GPU1 variable, it'll allocate a temporary tensor on GPU1, use DMA to initialize it, then copy temporary tensor on GPU1 to variable on GPU1", "@yaroslavvb Thanks very much! I am not familiar with memory and GPU, could you please be more specific about how to implement it?", "@YiMX I think Paul is suggesting to do the following, instead of\r\n\r\n```\r\nwith tf.device(\"gpu:1\"):\r\n  a =   gpu_0_tensor + b\r\n```\r\nto do this\r\n\r\n```\r\nwith tf.device(\"gpu:1\"):\r\n  gpu_0_tensor_moved = gpu_0 + 0\r\n  a = gpu_0_tensor_moved + b\r\n```\r\n\r\nThe `gpu_0_tensor_moved` is a copy of `gpu_0_tensor` created on `gpu1`, so that now `a+b` works on Tensors on the same device. Do this to enough tensors so that every other operation only works on tensors from the same GPU", "Actually what I'm suggesting is this:\r\n\r\nInstead of\r\n```\r\nwith tf.device(\"gpu:0\"):\r\n    a = Foo()\r\n    with tf.device(\"gpu:1\"):\r\n        b = Bar()\r\n        c = Baz(a, b)\r\n\r\n```\r\nDo\r\n```\r\nwith tf.device(\"gpu:0\"):\r\n    a = Foo()\r\n    a_copy = a + 0.0\r\n    with tf.device(\"gpu:1\"):\r\n        b = Bar()\r\n        c = Baz(a_copy, b)\r\n```\r\n\r\nThe idea being to create an immediate one-time-use, read-only copy of any value that's going to be used on another device.  The copy should be local to the same device as the original value. \r\n", "@YiMX , let us know if @poxvoculi's suggestion solves your problem. Thanks!", "I tried your ideas to specify those variables, but the problem still exists, even if I split the loop into the below form to let every variable have a unique name.\r\n```man\r\nwith tf.device('/gpu:0'):\r\n     with tf.name_scope('GPU_0') as scope:\r\n        loss0 = get_loss(x_splits[0], y_splits[0], regularizer, scope, reuse_variables)\r\n        reuse_variables = True\r\n        grads0 = opt.compute_gradients(loss0)\r\n        tower_grads.append(grads0)\r\n\r\nwith tf.device('/gpu:1'):\r\n    with tf.name_scope('GPU_1') as scope:\r\n        loss1 = get_loss(x_splits[1], y_splits[1], regularizer, scope, reuse_variables)\r\n        reuse_variables = True\r\n        grads1 = opt.compute_gradients(loss1)\r\n        tower_grads.append(grads1)\r\n```\r\n", "I think you've made explicit what the original code did, but haven't introduced any new temporary local copies.  What I'm suggesting is intrusive to the code, to the degree that values go back and forth between devices.\r\n\r\nMy apologies if I don't understand your example well, I rarely work with TF at this level.  I think 'tower_grads' is located on the CPU, and a separate instance of 'grads' is located on each GPU.\r\nThe line\r\n   ```tower_grads.append(grads)```\r\nis going to force an inter-device copy from each GPU to the CPU.  Try substituting\r\n ```\r\n       grads_copy = grads + 0.0\r\n       tower_grads.append(grads_copy)\r\n```\r\n\r\nI think the average_gradients function is computing entirely on the CPU, if I'm wrong, you'll need some tmp copies in there too.  \r\n\r\nFrankly, it doesn't look likely that this is going to solve your problem.  I'd be more hopeful if the source of the inter-device copy were a Var.  Is is possible that opt.compute_gradients() or opt.apply_gradients is hiding some inter-device copies?", "I added some copies, but there was something wrong if I did this to grads because it is a list not a value (I got errors of NotimplementError at apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)). I am still working on this, but something I found might be useful. This time I changed \r\n```man\r\ngrads = average_gradients(tower_grads)\r\n```\r\nto\r\n```man\r\ngrads = tower_grads[0]\r\n```\r\nIt worked but grads = tower_grads[1] still caused discrepancy. The problem I think might be caused by the second GPU.", "Any update? I'm currently having this problem. Worked fine on CPU but NaN on GPUs.", "Can you clarify: What program are you running, on which release, hardware platform, etc.?", "Apologies, my problem is actually much better described in this thread https://github.com/tensorflow/tensorflow/issues/2037 - I'll follow up there.", "Any update? I've been struggling with this problem for a while. I have 8 M40 cards and the problem is very similar with @YiMX. Everything works well on single card but diverging when using multiple cards together. The most weird part is that when I allocate computation on two different gpus between which the peer to peer access is not supported, everything works well again. For example, it works well on gpu0 with gpu4 or 5 or 6, but fails on gpu0 with gpu1 or 2 or 3.\r\n\r\nThe peer to peer matrix \r\nDMA  \r\n~~0 1 2 3 4 5 6 7 \r\n    0:   Y Y Y Y N N N N \r\n    1:   Y Y Y Y N N N N \r\n    2:   Y Y Y Y N N N N \r\n    3:   Y Y Y Y N N N N \r\n    4:   N N N N Y Y Y Y \r\n    5:   N N N N Y Y Y Y \r\n    6:   N N N N Y Y Y Y \r\n    7:   N N N N Y Y Y Y \r\n", "Ok, it finally got solved.\r\nIn my case, The bug is pear to pear data access between gpu0 and gpu1,2,3 doesn't work. This abnormalty can be verified with \"simpleP2P\" test. The test reports an \"verification error\" with nan in my case. But the reason behind this behaviour could be various. For me, it's because there's a conflict of virtual memory existing between one of my cpu and gpu0, which can be  solved by simply disabling the VT-d function. After that, the simplep2p test got pased and NAN problem for the training disapeared.", "I had a same problem and solved by disable ACSCtl.  \r\nCheck your ACSCtl status. \r\n\r\nRef:\r\nhttps://devtalk.nvidia.com/default/topic/883054/multi-gpu-peer-to-peer-access-failing-on-tesla-k80-/\r\nhttps://github.com/twitter/torch-ipc/issues/17\r\n", "Is there any news? I have a similar problem as @YiMX  with 8 Tesla P100. \r\nI pass the simpleP2P test. I use TensorFlow 1.4.1 and Cuda 8.0.\r\n\r\nHere are two of the error messages (shortened) I get,\r\n\r\nwith `tf.add_check_numerics_ops()`:\r\n```\r\n...\r\nInvalidArgumentError (see above for traceback): tower_0/local3/L2Loss:0 : \r\nTensor had Inf values\r\n[[Node: CheckNumerics_173 = CheckNumerics[T=DT_FLOAT, \r\nmessage=\"tower_0/local3/L2Loss:0\", \r\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]\r\n(tower_0/local3/L2Loss/_209, ^CheckNumerics_172)]]\r\n```\r\nand with `tf.check_numerics()` :\r\n```\r\n...\r\nInvalidArgumentError (see above for traceback): NaN: average_gradients(expanded_g) : \r\nTensor had Inf and NaN values\r\n [[Node: CheckNumerics_30 = CheckNumerics[T=DT_FLOAT, \r\nmessage=\"NaN: average_gradients(expanded_g)\", \r\n_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ExpandDims_30)]]\r\n[[Node: tower_6/total_loss/_2216 = _Send[T=DT_FLOAT, \r\nclient_terminated=false, \r\nrecv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", \r\nsend_device=\"/job:localhost/replica:0/task:0/device:GPU:6\", \r\nsend_device_incarnation=1, \r\ntensor_name=\"edge_4923_tower_6/total_loss\",\r\n _device=\"/job:localhost/replica:0/task:0/device:GPU:6\"](tower_6/total_loss)]]\r\n```\r\n\r\nThe output from simpleP2P:\r\n```\r\n`[./simpleP2P] - Starting...\r\nChecking for multiple GPUs...\r\nCUDA-capable device count: 8\r\n> GPU0 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n> GPU1 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n> GPU2 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n> GPU3 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n> GPU4 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n> GPU5 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n> GPU6 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n> GPU7 = \"Tesla P100-PCIE-16GB\" IS  capable of Peer-to-Peer (P2P)\r\n\r\nChecking GPU(s) for support of peer to peer memory access...\r\n> Peer access from Tesla P100-PCIE-16GB (GPU0) -> Tesla P100-PCIE-16GB (GPU1) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU0) -> Tesla P100-PCIE-16GB (GPU2) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU0) -> Tesla P100-PCIE-16GB (GPU3) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU0) -> Tesla P100-PCIE-16GB (GPU4) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU0) -> Tesla P100-PCIE-16GB (GPU5) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU0) -> Tesla P100-PCIE-16GB (GPU6) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU0) -> Tesla P100-PCIE-16GB (GPU7) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU1) -> Tesla P100-PCIE-16GB (GPU0) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU1) -> Tesla P100-PCIE-16GB (GPU2) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU1) -> Tesla P100-PCIE-16GB (GPU3) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU1) -> Tesla P100-PCIE-16GB (GPU4) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU1) -> Tesla P100-PCIE-16GB (GPU5) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU1) -> Tesla P100-PCIE-16GB (GPU6) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU1) -> Tesla P100-PCIE-16GB (GPU7) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU2) -> Tesla P100-PCIE-16GB (GPU0) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU2) -> Tesla P100-PCIE-16GB (GPU1) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU2) -> Tesla P100-PCIE-16GB (GPU3) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU2) -> Tesla P100-PCIE-16GB (GPU4) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU2) -> Tesla P100-PCIE-16GB (GPU5) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU2) -> Tesla P100-PCIE-16GB (GPU6) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU2) -> Tesla P100-PCIE-16GB (GPU7) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU3) -> Tesla P100-PCIE-16GB (GPU0) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU3) -> Tesla P100-PCIE-16GB (GPU1) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU3) -> Tesla P100-PCIE-16GB (GPU2) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU3) -> Tesla P100-PCIE-16GB (GPU4) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU3) -> Tesla P100-PCIE-16GB (GPU5) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU3) -> Tesla P100-PCIE-16GB (GPU6) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU3) -> Tesla P100-PCIE-16GB (GPU7) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU4) -> Tesla P100-PCIE-16GB (GPU0) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU4) -> Tesla P100-PCIE-16GB (GPU1) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU4) -> Tesla P100-PCIE-16GB (GPU2) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU4) -> Tesla P100-PCIE-16GB (GPU3) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU4) -> Tesla P100-PCIE-16GB (GPU5) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU4) -> Tesla P100-PCIE-16GB (GPU6) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU4) -> Tesla P100-PCIE-16GB (GPU7) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU5) -> Tesla P100-PCIE-16GB (GPU0) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU5) -> Tesla P100-PCIE-16GB (GPU1) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU5) -> Tesla P100-PCIE-16GB (GPU2) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU5) -> Tesla P100-PCIE-16GB (GPU3) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU5) -> Tesla P100-PCIE-16GB (GPU4) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU5) -> Tesla P100-PCIE-16GB (GPU6) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU5) -> Tesla P100-PCIE-16GB (GPU7) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU6) -> Tesla P100-PCIE-16GB (GPU0) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU6) -> Tesla P100-PCIE-16GB (GPU1) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU6) -> Tesla P100-PCIE-16GB (GPU2) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU6) -> Tesla P100-PCIE-16GB (GPU3) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU6) -> Tesla P100-PCIE-16GB (GPU4) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU6) -> Tesla P100-PCIE-16GB (GPU5) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU6) -> Tesla P100-PCIE-16GB (GPU7) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU7) -> Tesla P100-PCIE-16GB (GPU0) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU7) -> Tesla P100-PCIE-16GB (GPU1) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU7) -> Tesla P100-PCIE-16GB (GPU2) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU7) -> Tesla P100-PCIE-16GB (GPU3) : No\r\n> Peer access from Tesla P100-PCIE-16GB (GPU7) -> Tesla P100-PCIE-16GB (GPU4) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU7) -> Tesla P100-PCIE-16GB (GPU5) : Yes\r\n> Peer access from Tesla P100-PCIE-16GB (GPU7) -> Tesla P100-PCIE-16GB (GPU6) : Yes\r\nEnabling peer access between GPU0 and GPU1...\r\nChecking GPU0 and GPU1 for UVA capabilities...\r\n> Tesla P100-PCIE-16GB (GPU0) supports UVA: Yes\r\n> Tesla P100-PCIE-16GB (GPU1) supports UVA: Yes\r\nBoth GPUs can support UVA, enabling...\r\nAllocating buffers (64MB on GPU0, GPU1 and CPU Host)...\r\nCreating event handles...\r\ncudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 12.16GB/s\r\nPreparing host buffer and memcpy to GPU0...\r\nRun kernel on GPU1, taking source data from GPU0 and writing to GPU1...\r\nRun kernel on GPU0, taking source data from GPU1 and writing to GPU0...\r\nCopy data back to host from GPU0 and verify results...\r\nDisabling peer access...\r\nShutting down...\r\nTest passed\r\n```", "I got similar problems earlier and solved it by removing nccl from `LD_LIBRARY_PATH`. I'm not sure what's going on, but it seems to be conflicting with the NCCL version that TF was compiled with. Also, certain versions of cudnn can also give NaN.", "Thanks for the fast answer. I think I've solved my problem. It wasn't the graphics card but the optimization algorithm that was responsible for the errors. I changed it from `tf.train.GradientDescentOptimizer()` with `tf.train.exponential_decay()` to `tf.train.AdamOptimizer()`. Now I don't get any more NaN errors.", "@milanfeind why do you think SGD had problem with multiple GPUs? ", "I also meet this problem with the offical lexample\r\nhttps://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py\r\n\r\nI try to write another program follow the `cifar10_multi_gpu_train.py`, but I get NaN loss error.\r\n\r\n```\r\ntf version 1.12.0\r\nUbuntu 16.04.5 LTS\r\n2 1080 Ti cards\r\nCUDA-9.1/cuDNN-7.0.5\r\n```", "I have the same problem. The environment is: Ubuntu 16.04, 2 GTX 1080Ti cards, tensorflow 1.15, compiled from source with cuda 10.0+cudnn 7.6.4.\r\n(1) When using one card with `batch_size=2`, the training is OK.\r\n(2) When uisng two cards with `batch_size=2`, the training is OK.\r\n\r\nThen I copied the code to another machine (which has V100, four GPU cards.\r\n(3) training with four cards with `batch_size=4`, the training will stop with `Nan` after several iterations."]}, {"number": 7455, "title": "Tensorflow value error when setting up training_data", "body": "Hey, I'm trying to feed a csv to tf.contrib.learn and running into a ValueError over and over again. Here's my code for the training_set variable, plus the ensuing error:\r\n\r\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n\tfilename = MET_TRAINING,\r\n\ttarget_dtype = np.float,\r\n\tfeatures_dtype = np.str)\r\nTraceback (most recent call last):\r\n  File \"<pyshell#15>\", line 4, in <module>\r\n    features_dtype = np.str)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 48, in load_csv_with_header\r\n    n_samples = int(header[0])\r\nValueError: invalid literal for int() with base 10: 'conflict'\r\n\r\nThe .csv file contains mostly strings, but post training I'd like tf to give me either a '0' or '1' depending on the category an text example ought to fall in. Suggestions on what to do from here?", "comments": ["Not sure if I understand your question, but this may more suitable for a SO question instead of an issue.\r\n\r\nSeems you are trying to convert the text string `conflict` to an integer. You may want to write your own parser instead of simply using the `int()` function since you said you will have categorical values. \r\n\r\nDo something like `n_samples = 0 if header[0] == 'conflict' else 1`.\r\n\r\nIf you are struggling with the csv header, you could just remove that line to get rid of the string column names and only keep the numerical values.", "You need to have a line in your csv that has # of features and # of samples. Go see the source code.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/base.py#L48\r\n\r\nArguably that function is not greatly documented. There is also another variant that doesn't expect that header (see the function directly below). That being said, contrib is not officially supported. Asking on StackOverflow would be a great thing here, because then it will be documented for future users. Thanks!\r\n", "@zaqari I am also getting the same issue can you please suggest me some solution of it...??", "Hey @Deepakchawla ! This was a while back, and I don't remember precisely what I did to fix it, but one of the things I did was to make sure that the values in my labels column were all int() by adding the the following single line after setting up the pandas dataframes for my test and train data. Assume you have to do this for both of them:\r\n\r\ndf['label']=df['label'].astype(int)\r\n\r\nThat worked like a charm.", "@zaqari Actually, I am using below code to split the data\r\n\r\n`from sklearn.cross_validation import train_test_split\r\nimport pandas as pd\r\n\r\ndataset = pd.read_excel('Iris.xls')\r\n\r\ntrain, test = train_test_split(dataset, test_size = 0.2)\r\ntrain.to_csv(\"train.xls\", index=False, index_label=False, header=True)\r\ntest.to_csv(\"test.xls\", index=False, index_label=False, header=True)\r\n\r\nprint (train.data)`\r\n\r\nand this is to load the data\r\n\r\n`import tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.contrib.learn.python.learn.datasets import base\r\n\r\nIRIS_TRAINING = \"iris_training.xls\"\r\nIRIS_TEST = \"iris_test.xls\"\r\n\r\ntraining_set = base.load_csv_with_header(filename=IRIS_TRAINING,features_dtype=np.float32,target_dtype=np.float32)\r\ntest_set = base.load_csv_with_header(filename=IRIS_TEST,features_dtype=np.float32,target_dtype=np.float32)\r\n\r\nprint(training_set.data)\r\nprint(training_set.target)`\r\n\r\nNow can you tell where I am doing the mistake.", "Hmm. I usually just load all of my data in via pandas as opposed to using the functions provided by tensorflow. However, looking at what you just shot me, I think the problem is in the target_dtype value--you have it set for np.float32 where you want it as np.int16 or np.int64 \r\n\r\nTry changing that and see what happens!", "@zaqari  When I gave int16 or int32 then it will show me below error:\r\n`Traceback (most recent call last):\r\n  File \"LoadData.py\", line 10, in <module>\r\n    training_set = base.load_csv_with_header(filename=IRIS_TRAINING,features_dtype=np.float32,target_dtype=int32)\r\nNameError: name 'int32' is not defined`\r\nbut when I changed to it in int then it will give something like this..\r\n`Traceback (most recent call last):\r\n  File \"LoadData.py\", line 10, in <module>\r\n    training_set = base.load_csv_with_header(filename=IRIS_TRAINING,features_dtype=np.float32,target_dtype=np.int)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py\", line 48, in load_csv_with_header\r\n    n_samples = int(header[0])\r\nValueError: invalid literal for int() with base 10: '5.6'`\r\n\r\nI think the data which I am loading which is saved in csv file it is taking as a string which is not acceptable in load_csv_with_header function of tensor flow.\r\n"]}, {"number": 7454, "title": "(iOS Camera Example from GitHub) 'unsupported/Eigen/CXX11/Tensor' file not found", "body": "\r\n<p>I have just downloaded the tensorflow iOS Camera Example from GitHub and when I run the project it crashes with the error \"'unsupported/Eigen/CXX11/Tensor' file not found\".  I have added an example and would really like some help as soon as possible.</p> <br>\r\n\r\n<p>All the best, James</p>\r\n\r\n<br>\r\n\r\n<img width=\"877\" alt=\"screen shot 2017-02-12 at 20 11 26\" src=\"https://cloud.githubusercontent.com/assets/18594256/22865597/7abd2032-f15f-11e6-801d-7f7909952041.png\">\r\n", "comments": ["What are you doing to \"run the project\". After which step of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples\r\ndoes it fail?\r\n", "Closing due to lack of activity.  Please reopen if necessary.", "the some bugs is in camera  ios_examples. the mobile direction is incorrect. you can find the correct result only home_key left."]}, {"number": 7453, "title": "error while building on windows with cuda 7.5", "body": "I have spent days to build tensorflow on windows with cuda 7.5 but met a lot of error. I first followed https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake/readme.md. I modified CmakeList.txt from cuda 8 to cuda 7.5. But nvcc version 7.5 do not support visual studio 2015. When I change to visual studio 2013. It failed with constexpr because visual studio 2013 do not support it. When I remove constexpr it got a lot of error of noexcept, uint16 and so on. So is it possible to build tensorflow with cuda 7.5? In a lot of cases, it's not possible to use cuda 8 . So can it be seen as a feature request to support cuda 7.5 on windows? Thanks.", "comments": ["Ill let @mrry @guschmue comment.\r\nI think there is an incompatibility (due to the versions of visual studio they are compiled with) between cuda versions anything lower than 8 and python.", "With vs2015 only supporting the cuda toolkit 8.0 you'd need to use vs2013.\r\nThe c99 support in vs2013 was incomplete and tensorflow makes lots of use of c99. When we started porting tensorflow we used vs2015.update1 and even that was a royal pain to get things compile - lots of little things that required code changes. With vs2015.update3 all of this went away.\r\nMaybe not impossible to get this to work but it would be very painful. The code changes itself would be somewhat hard to push back into the master because there would be lots of ifdef.\r\nIf you need 7.5 on the box and can't install the cuda 8.0 you could try to copy the cuda 8.0 dlls into a directory and make sure tensorflow finds that directory in the path before it finds 7.5 dlls. As long you have the latest nvidia driver I think this should work.\r\n", "Thank you for your response, I will try copying dlls and update drivers.", "I will close this issue with the explanation @guschmue provided (cannot use cuda 7.5 due to no c99 support in appropriate VS version)."]}, {"number": 7452, "title": "Bugs in TensorFlowYoloDetector", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: Windows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n099ef38a99d069b9a1f6d09a289b2df69eaee276\r\n2. The output of `bazel version`\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:13 2016 (1481136433)\r\nBuild timestamp: 1481136433\r\nBuild timestamp as int: 1481136433\r\n\r\n----\r\nIt looks like there are bugs in [TensorfFlowYoloDetector](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java).\r\n\r\n1. **The confidence calculation is broken**\r\nIn line 218: \r\n\r\n` final float confidence = output[offset + 4];`\r\n\r\nShould be \r\n\r\n` final float confidence = expit( output[offset + 4] );`\r\n\r\nYou can see line 42 at [DarkFlow file](https://github.com/thtrieu/darkflow/blob/master/net/yolov2/test.py)\r\n\r\n2. **The RGB values might be read as BGR in line 152:**\r\n```\r\n    for (int i = 0; i < intValues.length; ++i) {\r\n      floatValues[i * 3 + 0] = (intValues[i] & 0xFF) / 255.0f;\r\n      floatValues[i * 3 + 1] = ((intValues[i] >> 8) & 0xFF) / 255.0f;\r\n      floatValues[i * 3 + 2] = ((intValues[i] >> 16) & 0xFF) / 255.0f;\r\n    }\r\n```\r\n\r\nUnless there is a bug in DarkFlow, in order to have the same results, one must change to:\r\n```\r\n    for (int i = 0; i < intValues.length; ++i) {\r\n      floatValues[i * 3 + 2] = (intValues[i] & 0xFF) / 255.0f;\r\n      floatValues[i * 3 + 1] = ((intValues[i] >> 8) & 0xFF) / 255.0f;\r\n      floatValues[i * 3 + 0] = ((intValues[i] >> 16) & 0xFF) / 255.0f;\r\n    }\r\n```\r\n\r\n3. **The NMS calculation is completely different from DarkFlow and DarkNet**\r\nSee line 242, it just return top K results, without any non-maximum suppression\r\n\r\nHere is the code from the file:\r\n```\r\n    final ArrayList<Recognition> recognitions = new ArrayList<Recognition>();\r\n    for (int i = 0; i < Math.min(pq.size(), MAX_RESULTS); ++i) {\r\n      recognitions.add(pq.poll());\r\n    }\r\n```\r\n\r\n\r\nHere is the correct code from [DarkFlow, line 52](https://github.com/thtrieu/darkflow/blob/master/net/yolov2/test.py):\r\n```\r\n\t# non max suppress boxes\r\n\tfor c in range(C):\r\n\t\tfor i in range(len(boxes)):\r\n\t\t\tboxes[i].class_num = c\r\n\t\tboxes = sorted(boxes, key = prob_compare)\r\n\t\tfor i in range(len(boxes)):\r\n\t\t\tboxi = boxes[i]\r\n\t\t\tif boxi.probs[c] == 0: continue\r\n\t\t\tfor j in range(i + 1, len(boxes)):\r\n\t\t\t\tboxj = boxes[j]\r\n\t\t\t\tif box_iou(boxi, boxj) >= .4:\r\n\t\t\t\t\tboxes[j].probs[c] = 0.\r\n```", "comments": ["cc @andrewharp ", "@AndreyRub Thanks for the catches! I had not known what to expect from the tiny YOLO graph, so I just chalked any performance issues up to it being much smaller than the regular graph. Fix incoming.\r\n\r\nRe: # 3, the non-max suppression is performed in DetectorActivity using the same logic as the default MultiBox detector."]}, {"number": 7451, "title": "Getting the following error :/usr/prachi/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 944, in _run     % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape()))) ValueError: Cannot feed value of shape (0,) for Tensor u'input/BottleneckInputPlaceholder:0', which has shape '(?, 2048)'", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I'm 98% sure this is a user-error, which is best helped by nice people on stackoverflow"]}, {"number": 7450, "title": "android demo app: tf_detect using  yolo instead of multibox_detect", "body": "Does anyone know how to build android demo app which uses yolo detector instead of multibox detector ?\r\n\r\nThanks,,\r\n", "comments": ["You need to add the graph to assets/ and flip the boolean USE_YOLO to true. There are instructions here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L65\r\n\r\nClosing as there is nothing actionable here; for further general TF usage queries please try on StackOverflow.", "@kaishijeng  hi\uff0care you successful\uff1f"]}, {"number": 7449, "title": "bazel build --copt=-march=native not using available CPU instructions", "body": "**Update 2017-12-06:**\r\nThe current version of my [tensorflow.sh install script](https://github.com/ahundt/robotics_setup/blob/master/tensorflow.sh) has been working well for me, and the update from tf 1.3 to tf 1.4 required only a one character change!\r\n\r\n**Original Post:**\r\nHere are the key lines in my install script with a quote from the tensorflow docs:\r\n```\r\n# To be compatible with as wide a range of machines as possible, TensorFlow defaults to only using SSE4.1 SIMD instructions on x86 machines. Most modern PCs and Macs support more advanced instructions, so if you're building a binary that you'll only be running on your own machine, you can enable these by using --copt=-march=native in your bazel build command.\r\n\r\nbazel build --copt=-march=native -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n### Even with `--copt=-march=native` I get the following warnings about the CPU instruction set, contradicting the above statement:\r\n\r\n```\r\n\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\nHere is the exact script I used to build tensorflow:\r\nhttps://github.com/ahundt/robotics_setup/blob/b5ee71f262ec36f8dbc8374ed2503c0812fb0f47/tensorflow.sh\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttp://stackoverflow.com/a/41520266/99379\r\n\r\nOperating System:\r\nUbuntu 16.04\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n```\r\npython -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\n```\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n07bb8ea2379bd459832b23951fb20ec47f3fdbd4\r\n\r\n2. The output of `bazel version`\r\n\r\n```\r\n\u00b1 bazel version\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n```\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\n\r\npython -c 'import tensorflow as tf; print(tf.__version__); sess = tf.InteractiveSession(); sess.close();'\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.0\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:02:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:02:00.0)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nThis person tried some other things: http://stackoverflow.com/a/41520266/99379\r\n\r\n", "comments": ["This command should break down the optimizations that are turned on with march=native\r\n \r\n`\r\ngcc -march=native -Q --help=target`\r\n\r\nAre AVX/FMA optimizations in there?\r\n\r\nAlso, `march=native` is a long way of writing\r\n\r\n`bazel build --config=opt --config=cuda `", "Here is the full output of that command:\r\nhttps://gist.github.com/ahundt/ec233276360962b1317a36c2054d933c\r\n\r\nKey lines:\r\n```\r\n  -mavx                                 [enabled]\r\n  -mavx2                                [enabled]\r\n```\r\n\r\n\r\n>Also, march=native is a long way of writing\r\n>\r\n>bazel build --config=opt --config=cuda\r\n\r\nThanks, I didn't know that about the bazel flags, I usually use cmake so I'm not super sure of the particulars of bazel.\r\n\r\nBased on what you are saying this may also have something to do with either the gcc version or how my gcc is configured/compiled and not just which flags are passed? I had assumed based on those quoted docs that everything could be detected and configured appropriately based on compiler flags alone.", "cc @martinwicke who troubleshooted (troubleshot?) similar issues in the past", "@gunan, could you take a look at this please?", "@gunan is out. I'll look.", "Maybe you have to use `--cxxopt=-march=native` as well? It's safer to use `--config=opt`, which does that for you. ", "Looks like the following is a workaround according to https://github.com/wangyum/Anaconda/issues/15\r\n\r\n    bazel build --linkopt='-lrt' -c opt --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 -k //tensorflow/tools/pip_package:build_pip_package", "@ahundt does using just --config=opt work for you?", "@martinwicke @ahundt It works for me.\r\n![tensorflow-7449](https://cloud.githubusercontent.com/assets/5399861/23003869/62184440-f42e-11e6-8f3e-152520186bf2.jpg)\r\n\r\nThe following options are enabled target specific:\r\n```\r\n$ gcc -march=native -Q --help=target | grep enable\r\n  -m64                                  [enabled]\r\n  -m80387                               [enabled]\r\n  -m96bit-long-double                   [enabled]\r\n  -maes                                 [enabled]\r\n  -malign-stringops                     [enabled]\r\n  -mavx                                 [enabled]\r\n  -mcx16                                [enabled]\r\n  -mfancy-math-387                      [enabled]\r\n  -mfp-ret-in-387                       [enabled]\r\n  -mfused-madd                          [enabled]\r\n  -mglibc                               [enabled]\r\n  -mhard-float                          [enabled]\r\n  -mieee-fp                             [enabled]\r\n  -mpclmul                              [enabled]\r\n  -mpopcnt                              [enabled]\r\n  -mpush-args                           [enabled]\r\n  -mred-zone                            [enabled]\r\n  -msahf                                [enabled]\r\n  -msse                                 [enabled]\r\n  -msse2                                [enabled]\r\n  -msse3                                [enabled]\r\n  -msse4                                [enabled]\r\n  -msse4.1                              [enabled]\r\n  -msse4.2                              [enabled]\r\n  -mssse3                               [enabled]\r\n  -mstackrealign                        [enabled]\r\n  -mtls-direct-seg-refs                 [enabled]\r\n```", "I had this exact issue. However when I used the following to build from source the messages no longer appear. @ahundt suggestion removed a few of them however it was missing fma and avx2. \r\n\r\n`bazel build --linkopt='-lrt' -c opt --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 --copt=-mavx2 --copt=-mfma -k //tensorflow/tools/pip_package:build_pip_package`", "Dear @devfubar. Is `--linkopt='-lrt'` required option or it is you own customization ?", "@vskubriev I merely extended @ahundt suggested options from this [comment](https://github.com/tensorflow/tensorflow/issues/7449#issuecomment-280158846)\r\n\r\nNot sure what it does personally. Perhaps @ahundt could elaborate on its purpose? I have noticed it mentioned in various documents but never explained.", "I am very confused by this and I'd like to find out whether there's a problem in TF somewhere or whether some compiler (versions) don't properly interpret `-march=native`. \r\n\r\nCan someone who built with `--config=opt`, and who's running on the same machine they built on, and who nevertheless does get warnings about unused available optimizations please post the exact compiler and version (both the compiler version as well as the version tensorflow reports) they built with?", "I just tried the following, and I was able to build an optimized binary:\r\n```\r\nbazel build --config=opt tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nSo I will close this issue as not reproducible.", "@gunan was that with the versions I specified or is there a later commit than either bazel 0.4.4 or tf1.0 07bb8ea that may have resolved the issue?", "@ahundt I think this could have more to do more with your gcc than bazel. IE, it seems as if you run `gcc -march=native -Q` and it promises to turn on `-mavx` flag for `-march=native`, but then it doesn't. Maybe you could validate it by building something else with `gcc` directly and seeing if problem persists?", "@gunan I'm not sure closing this is appropriate as there is obviously an issue for some people, maybe not for you but others. I'm pretty sure I could half my issue inbox if I just tried something and then said I couldn't reproduce. Not exactly diligent behaviour.", "My hunch is that this is some kind of gcc/bazel interaction for a configuration that's not available/common inside Google, so \"community support\" label seems appropriate", "@yaroslavvb thanks for re-opening. \r\n\r\n@martinwicke ;\r\n```\r\ngcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n```\r\n```\r\nbazel version\r\nBuild label: 0.4.4\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n```\r\n```\r\nTensor flow tag: v1.0.0\r\n```\r\n", "Thanks @devfubar. You said earlier, it works for you when building with \r\n\r\n```\r\nbazel build --linkopt='-lrt' -c opt --copt=-mavx --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 --copt=-mavx2 --copt=-mfma -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\ncorrect?\r\n\r\nIn that case, can you paste the contents of your `tools/bazel.rc` file?", "@martinwicke yes that is the command I used to stop the warning messages.\r\n```\r\n./configure\r\n```\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] \r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] \r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\n.........\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.........\r\nINFO: All external dependencies fetched successfully.\r\n```\r\n```\r\ncat tools/bazel.rc\r\n```\r\n```\r\n# Autogenerated by configure: DO NOT EDIT\r\nbuild:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nbuild:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\r\nbuild:win-cuda --define=using_cuda=true --define=using_cuda_nvcc=true\r\n\r\nbuild:sycl --crosstool_top=@local_config_sycl//crosstool:toolchain\r\nbuild:sycl --define=using_sycl=true\r\n\r\nbuild:sycl_asan --crosstool_top=@local_config_sycl//crosstool:toolchain\r\nbuild:sycl_asan --define=using_sycl=true --copt -fno-omit-frame-pointer --copt -fsanitize-coverage=3 --copt -fsanitize=address --copt -DGPR_NO_DIRECT_SYSCALLS --linkopt -fPIC --linkopt -lasan\r\n\r\nbuild --force_python=py3\r\nbuild --host_force_python=py3\r\nbuild --python3_path=\"/usr/bin/python3\"\r\nbuild --define=use_fast_cpp_protos=true\r\nbuild --define=allow_oversize_protos=true\r\n\r\nbuild --define PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\ntest --define PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\ntest --force_python=py3\r\ntest --host_force_python=py3\r\nrun --define PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n\r\nbuild --spawn_strategy=standalone\r\ntest --spawn_strategy=standalone\r\nrun --spawn_strategy=standalone\r\n\r\nbuild --genrule_strategy=standalone\r\ntest --genrule_strategy=standalone\r\nrun --genrule_strategy=standalone\r\n\r\nbuild -c opt\r\ntest -c opt\r\nrun -c opt\r\n\r\nbuild:opt --cxxopt=-march=native --copt=-march=native\r\n```", "Interesting I have the same gcc version. Perhaps I should confirm that tf and bazel are completely on the 1.0 release version.\r\n```\r\ngcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n```", "@devfubar Sorry for preemptively closing the issue with little information. I tested our build instructions on all our supported platforms.\r\n\r\n@ahundt After looking at what you are running, I think you are not installing the TF you just built.\r\nhttps://github.com/ahundt/robotics_setup/blob/b5ee71f262ec36f8dbc8374ed2503c0812fb0f47/tensorflow.sh\r\n\r\nIn the above script, could you remove line 75\r\nand modify line 87 to say:\r\n\r\n```\r\npip install --upgrade /tmp/tensorflow_pkg/tensorflow-*\r\n```\r\n\r\nand try again?", "Ping!\r\n@ahundt, were you able to try the modified pip install command above?\r\nDid it resolve your problem?\r\n\r\n@devfubar Could you also share the exact commands you ran all the way from building TF from source, to installing the pip package and seeing the warning messages?", "Hi @gunan \r\n\r\nSo I created a brand new vanilla ubuntu machine to do some testing over the weekend with mixed results. First time I did not get the warning messages but on a second clean machine I did. I am just trying to narrow in on what exactly I did to either get the messages or what I did to not get them.", "This worked for me! In my case this is resolved. Sorry that it ended up being a bug in my installation procedure and thus I was encountering something that was already fixed. Thanks!", "@ahundt Thanks for the feedback. I am glad the issue is resolved for you.\r\n\r\n@devfubar I highly suspect you are also having a problem with installing the correct pip package after you build it. Please try the following commands to build and run TF.\r\nPlease do not modify the script except for the commented lines.\r\nYou can try using docker to simulate having clean machines.\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit pull\r\ngit checkout r1.0\r\n\r\n# Writing instructions for CPU, feel free to run configure manually here, \r\n# without any modifications to the optimization flags.\r\nyes \"\" | ./configure\r\n\r\n# Add config=cuda (config flag can be set multiple times) If you need GPU.\r\nbazel build --config=opt tensorflow/tools/pip_package:build_pip_package\r\nmkdir pip_pkg\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package `pwd`/pip_pkg\r\n\r\ncd pip_pkg\r\npip install --upgrade tensorflow-*\r\npython -c 'import tensorflow as tf; print(tf.__version__); sess = tf.InteractiveSession(); sess.close();'\r\n```\r\n\r\nAfter running the above script without any modifications if you still see the warning messages I can continue investigating. But at the moment, I am convinced that there are no issues in TF.\r\nThe problem seems to be on the user side.\r\n\r\n", "Closing this. Thanks for the sleuthing, @gunan!", "(of course\u00a0and as always, comment to reopen)", "*argument unused during compilation : '-march=native' *tells me your\ncompiler (what is your compiler?) does not like -march=native. What is the\ncompiler you are using?\n\nOn Fri, Jul 7, 2017 at 6:02 PM, TethysSun <notifications@github.com> wrote:\n\n> @martinwicke <https://github.com/martinwicke> Hi, I have the same problem\n> when I was trying to install it on Mac OS.\n>\n> When./configure, I used the default -march=native. However, the warning \u201c*argument\n> unused during compilation : '-march=native'* \u201cshows up when calling bazel\n> build with --config=opt. Do you know why does that happen?\n>\n> I also tried suggestions in #7778\n> <https://github.com/tensorflow/tensorflow/issues/7778> by using sudo\n> bazel build --config opt --copt=-msse4.1 --copt=-msse4.1 --copt=-mavx\n> --copt=-mavx2 --copt=-mfma //tensorflow/tools/pip_\n> package:build_pip_package , but after building, \"*The TensorFlow library\n> wasn't compiled to use SSE instructions*\" still appears when running the\n> helloTensorflow.\n>\n> And bazel build --linkopt='-lrt' -c opt --copt=-mavx --copt=-msse4.2\n> --copt=-msse4.1 --copt=-msse3 --copt=-mavx2 --copt=-mfma -k\n> //tensorflow/tools/pip_package:build_pip_packaged suggested by @devfubar\n> <https://github.com/devfubar> and suggested wheels from\n> https://github.com/yaroslavvb/tensorflow-community-wheels <http://url>\n> doesn't work for me too.\n>\n> And tried bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma\n> --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_\n> package:build_pip_package as suggested in https://stackoverflow.com/\n> questions/41293077/how-to-compile-tensorflow-with-sse4-\n> 2-and-avx-instructions <http://url>.\n>\n> It took me a long time to uninstall, reinstall and rebuilding from source,\n> unfortunately I don't find the answer for me yet. For The rest steps when\n> installing from source, I followed the official instruction\n> https://www.tensorflow.org/install/install_sources here.\n>\n> Any idea what's wrong?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7449#issuecomment-313722867>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_TDUkcGNZp4PGxjejEWCRKDe2RRLks5sLlaWgaJpZM4L-Zzg>\n> .\n>\n", "At the end, I get that:\r\n\r\n> p3.6_smartchemixam23@pt-mguittet:~/tensorflow$ sudo pip install /tmp/tensorflow_pkg/tensorflow-1.3.0-cp36-cp36m-macosx_10_12_x86_64.whl \r\n> Password:\r\n> The directory '/Users/emixam23/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\n> The directory '/Users/emixam23/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\n> Requirement already satisfied: tensorflow==1.3.0 from file:///tmp/tensorflow_pkg/tensorflow-1.3.0-cp36-cp36m-macosx_10_12_x86_64.whl in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages\r\n> Requirement already satisfied: numpy>=1.11.0 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow==1.3.0)\r\n> Requirement already satisfied: wheel>=0.26 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow==1.3.0)\r\n> Requirement already satisfied: six>=1.10.0 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow==1.3.0)\r\n> Requirement already satisfied: tensorflow-tensorboard<0.2.0,>=0.1.0 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow==1.3.0)\r\n> Requirement already satisfied: protobuf>=3.3.0 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow==1.3.0)\r\n> Requirement already satisfied: html5lib==0.9999999 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\n> Requirement already satisfied: werkzeug>=0.11.10 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\n> Requirement already satisfied: bleach==1.5.0 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\n> Requirement already satisfied: markdown>=2.6.8 in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow==1.3.0)\r\n> Requirement already satisfied: setuptools in /Users/emixam23/.local/share/virtualenvs/p3.6_smartch/lib/python3.6/site-packages (from protobuf>=3.3.0->tensorflow==1.3.0)\r\n", "You have to add the `--upgrade` flag to the `pip install ....` command if tensorflow is already installed on your system.", "Wow it works, but I still get `2017-10-02 22:40:06.689056: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA` Thank you so much, I'm searching since so long ! ><\r\nBest !", "@Emixam23 you have to compile yourself, or use one of the pre-built optimized versions (ie, from https://github.com/yaroslavvb/tensorflow-community-wheels)", "I already compiled it, that's why the warnings went away. But I got a new one, that's why I'm confused :/", "see the same kind of issue on SKX if I use -march=native during configure\r\nubuntu 16.04 and history output below shows the installation\r\n\r\n2004  git clone https://github.com/tensorflow/tensorflow \r\n 2005  cd tensorflow\r\n 2006  git checkout r1.4\r\n 2007  sudo apt-get install openjdk-8-jdk\r\n 2008  sudo echo \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list\r\n 2009  curl https://bazel.build/bazel-release.pub.gpg | sudo apt-key add -\r\n 2010  sudo apt-get update && sudo apt-get install bazel\r\n 2011  sudo apt-get upgrade bazel\r\n 2012  sudo apt-get install python-numpy python-dev python-pip python-wheel\r\n 2013  ./configure \r\n 2014  bazel build --config=mkl -c opt -c opt //tensorflow/tools/pip_package:build_pip_package > tf_build.log 2>&1\r\n 2015  vi tf_build.log \r\n 2016  mkdir ../tf_r1.4_mkl\r\n 2017  sudo bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n 2018  ls /tmp/tensorflow_pkg/\r\n 2019  sudo pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl -t ~/tf_r1.4_mkl/\r\n 2020  cp /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27mu-linux_x86_64.whl ~/tf_r1.4_mkl/\r\nThread model: posix\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) \r\n 2025  export OMP_NUM_THREADS=52\r\n 2026  python tf_cnn_benchmarks.py --device=cpu --mkl=True --kmp_settings=1 --batch_size=64 --model=alexnet --forward_only=True --num_warmup_batches=10  --num_inter_threads 2 --num_intra_threads 56 > alexnet_skx_thr.log 2>&1\r\ntop of alexnet_skx_thr.log\r\n2017-12-02 11:17:35.612237: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n", "When you add `-march=native` during configure, it wont automatically get added to your build.\r\nalso, `-c` is short for `--compilation_mode`, not `--config`\r\nYou will need to modify your build command as follows:\r\n```\r\nbazel build --config=mkl --config opt -c opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nHowever, this will almost surely fail because there are known compilation issues with avx512+TF. You may need to debug and fix your build to get it working."]}, {"number": 7448, "title": "tf.summary.FileWriter crashes with AlreadyExistsError", "body": "We have a cluster with ~20 GPUs that we often use to train multiple networks in parallel, and we use `tf.summary.Filewriter` to keep track of the networks' progress. However, some jobs are crashing when they attempt to create their `FileWriter`s with the following stack trace:\r\n\r\n```\r\n[...]\r\n  File \"/ubc/cs/research/tracking-raid/julm/eyescream/tensorflow/pose_estimation/linear_model.py\", line 141, in __init__\r\n    self.train_writer = tf.summary.FileWriter( os.path.join(summaries_dir, 'train' ))\r\n  File \"/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 308, in __init__\r\n    event_writer = EventFileWriter(logdir, max_queue, flush_secs)\r\n  File \"/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/summary/writer/event_file_writer.py\", line 69, in __init__\r\n    gfile.MakeDirs(self._logdir)\r\n  File \"/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 299, in recursive_create_dir\r\n    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)\r\n  File \"/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/ubc/cs/research/tracking-raid/julm/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError: /global/scratch/julm/3d_experiments\r\n```\r\n\r\nI am writing the `Filewriter`s' progress under `/global/scratch/julm/3d_experiments/`, and creating multiple subdirectories depending on the hyperparameters that each network is using.\r\n\r\nThe error seems to suggest that the `FileWriter` is trying to create the directory `/global/scratch/julm/3d_experiments/` and crashing because the directory already exists. Moreover, only around 1 in 5 jobs crashes with this error.\r\n\r\nDo you know if I could somehow ignore this error? I don't think the fact that the directory exists should trigger an error for the user.\r\n\r\nOur cluster runs under OpenSUSE 42.2.", "comments": ["I think you have a race condition. If you start /global/scratch/julm/3d_experiments/ before starting any experiments, does it create any errors. What happens otherwise is the first process looks and doesn't see it and then between the time it looked somebody else looked and created it. ", "I also get this race condition. The cluster has Red Hat Enterprise Linux 7 and Slurm job scheduler. Using TF 0.12. Creating the directory before submitting the job fixes the problem. But, handling the race condition within `FileWriter` might be beneficial?", "Running\r\n```python\r\nos.system('mkdir -p {}'.format( summaries_directory ))\r\n```\r\ndoes get rid of the race condition -- or, rather, lets linux handle the race condition, and that fixes my particular problem.\r\n\r\nI agree with @jsseely that it'd be super nice to have TF handle this internally though.", "@una-dinosauria, it's unlikely that this will be done by us in the short term or long term, but feel free to contribute a PR to TensorFlow. Thanks!", "@aselle Thanks! I'd love to contribute to TF, and this sounds like it has a fairly straightforward fix. \r\nIt's unlikely that I'll find the time to do it before mid-March though -- I guess we should keep this open just to track the issue and as a reference for others who might run into this problem.", "Closing due to lack of recent activity. Please reopen and update the issue if you submit a PR for the change. Thanks!\r\n"]}]