[{"number": 31370, "title": "tf.data.experimental.make_csv_dataset header flag not working as described", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n`Custom code`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```os: Linux\r\nos kernel version: #22-Ubuntu SMP Tue Jul 2 13:27:33 UTC 2019\r\nos release version: 5.0.0-21-generic\r\nos platform: Linux-5.0.0-21-generic-x86_64-with-debian-buster-sid\r\nlinux distribution: ('debian', 'buster/sid', '')\r\nlinux os distribution: ('debian', 'buster/sid', '')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='dev-XPS-13-9343', release='5.0.0-21-generic', version='#22-Ubuntu SMP Tue Jul 2 13:27:33 UTC 2019', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\nGNU/Linux\r\n```\r\n- TensorFlow installed from (source or binary):\r\n`binary using pip`\r\n- TensorFlow version (use command below):\r\n\r\n```Name: tensorflow\r\nVersion: 2.0.0b1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/dev/miniconda3/envs/pythonapu/lib/python3.6/site-packages\r\nRequired-by: \r\n```\r\n\r\n- Python version:\r\n```\r\nPython 3.6.8 :: Anaconda, Inc.\r\npython version: 3.6.8\r\npython branch: \r\npython build version: ('default', 'Dec 30 2018 01:22:34')\r\npython compiler version: GCC 7.3.0\r\npython implementation: CPython\r\n```\r\n**Describe the current behavior**\r\n\r\n`tf.data.experimental.make_csv_dataset(header=True)` includes the header data in the dataset\r\n\r\n**Describe the expected behavior**\r\n\r\nAccording to the docs:\r\n```\r\nheader: A bool that indicates whether the first rows of provided CSV files\r\n      correspond to header lines with column names, and should not be included\r\n      in the data.\r\n```\r\nThe data should not be included in the data.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI have a dataset in csv `/tmp/foo.csv`:\r\n```\r\nA,B,C\r\n1,NA,1\r\n1,NA,1\r\n1,NA,1\r\n1,NA,1\r\n```\r\nI can run something like with `header=True`\r\n```\r\n dataset_file = tf.keras.utils.get_file(\"foo2\" + str(uuid.uuid4()) + \".csv\", \"file:///tmp/foo.csv\")\r\ndataset = tf.data.experimental.make_csv_dataset(\r\n        dataset_file, batch_size=4, header=False,\r\n        label_name=\"A\", na_value='NA', column_names=[\"A\", \"B\", \"C\"],\r\n        field_delim=',')\r\n    for feature_batch, label_batch in dataset.take(1):\r\n        print(label_batch)\r\n        print(\"features:\")\r\n        for key, value in feature_batch.items():\r\n            print(key + ' ' + value)\r\n```\r\n\r\n\r\n\r\nwhich gives:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/xyz/workspace/pythonapi/main/services/dataloader.py\", line 149, in <module>\r\n    load()\r\n  File \"/home/xyz/workspace/pythonapi/main/services/dataloader.py\", line 143, in load\r\n    print(key + ' ' + value)\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 909, in r_binary_op_wrapper\r\n    x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\")\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1100, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1158, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1237, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 305, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 246, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 254, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/xyz/miniconda3/envs/pythonapu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 115, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, handle, device, dtype)\r\nTypeError: Cannot convert provided value to EagerTensor. Provided value: C  Requested dtype: int32\r\n```\r\nWhen C should not be in the dataset.\r\n\r\n**Other info / logs**\r\n\r\nWith `header=False`\r\n\r\nworks fine but not as described in the documentation as this suggests the csv does not contain the header and prints \r\n```\r\ntf.Tensor([b'1' b'1' b'1' b'A'], shape=(4,), dtype=string)\r\nfeatures:\r\ntf.Tensor([b'B ' b'B ' b'B ' b'B B'], shape=(4,), dtype=string)\r\ntf.Tensor([b'C 1' b'C 1' b'C 1' b'C C'], shape=(4,), dtype=string)\r\n```", "comments": ["@gridcellcoder \r\nI have created dataset foo.csv using pandas for reproducing the issue.While executing the code i am getting below `error:ValueError: unknown url type: 'fOO.csv'`Request you to help me with the reproducable code.Thanks!", "To reproduce:\r\n\r\n1) Create a file in /tmp/foo.csv Note: Not `f00.csv` as in your message\r\n2) Copy this into the file you created in 1) note: do not use pandas etc just a simple text editor\r\n\r\n```\r\nA,B,C\r\n1,NA,1\r\n1,NA,1\r\n1,NA,1\r\n1,NA,1\r\n```\r\n\r\n3) Run this in a python file after importing keras and tf beta2.0\r\n\r\n```\r\ndataset_file = tf.keras.utils.get_file(\"foo2\" + str(uuid.uuid4()) + \".csv\", \"file:///tmp/foo.csv\")\r\ndataset = tf.data.experimental.make_csv_dataset(\r\n        dataset_file, batch_size=4, header=False,\r\n        label_name=\"A\", na_value='NA', column_names=[\"A\", \"B\", \"C\"],\r\n        field_delim=',')\r\n    for feature_batch, label_batch in dataset.take(1):\r\n        print(label_batch)\r\n        print(\"features:\")\r\n        for key, value in feature_batch.items():\r\n            print(key + ' ' + value)\r\n```\r\n\r\nthat should recreate the issue above.\r\n\r\n", "I have tried on Jupyter notebook  with TF version 2.0 beta1 and 2.0.0-dev20190807 and was able to reproduce the issue.Please, find the gist in attachment.Thanks!\r\n[test.tar.gz](https://github.com/tensorflow/tensorflow/files/3480131/test.tar.gz)\r\n", "Apologies for the delayed response. \r\n\r\nHere's an explanation of what you're seeing. Since you don't provide the `column_defaults` parameter, `make_csv_dataset` tries to infer the type of your data.\r\n\r\nWhen the `header=True` flag is set, it ignores the first line of data.\r\nTherefore, it infers that column A contains ints, B contains strings, C contains ints.\r\nThe issue here is that in line \r\n```\r\nprint(key + ' ' + value)\r\n```\r\n\r\nkey is a string tensor, and value is an int tensor (in the A and C case). These types cannot be added together, which is why you get the above error.\r\n\r\n\r\nIn the case where `header=False`, the first (header) line is not ignored. If you read the documentation, the header parameter is \"A bool that indicates whether the first rows of provided CSV files correspond to header lines with column names, and should not be included in the data.\" -- so when header=False, the header line IS included in the data, i.e. it interprets the header line as regular data that's part of your dataset. Because this line has the data \"A,B,C\", it infers that all your columns are strings. Since string tensors can be added together, you don't get an error.\r\n\r\nDoes that make sense? Let me know how we can improve the documentation to make the behavior of `header` clearer.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31370\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31370\">No</a>\n"]}, {"number": 31369, "title": "TensorflowLite Runtime Installation Doesn't provide Interpreter Package", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: \r\n[Platform 1]\r\nNAME=\"Ubuntu\"\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 16.04.5 LTS\"\r\nVERSION_ID=\"16.04\"\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\n[Platform 2]\r\nPRETTY_NAME=\"Mendel GNU/Linux 3 (Chef)\"\r\nNAME=\"Mendel GNU/Linux\"\r\nVERSION_ID=\"3\"\r\nVERSION=\"3 (chef)\"\r\nID=mendel\r\nID_LIKE=debian\r\n\r\n- TensorFlow installed from (source or binary): {Didn't install trying to use tflite_runtime} [TFLITE RUNTIME](https://www.tensorflow.org/lite/guide/python#run_an_inference_using_tflite_runtime)\r\n- TensorFlow version: N/A\r\n- Python version: Mendel 3.5.3 / Ubuntu 16.04 3.6.9\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source):  Mendel (gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516), Ubuntu 16.04 (gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609)\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI tried to install the TFLITE Runtime using \r\n\r\nhttps://www.tensorflow.org/lite/guide/python#run_an_inference_using_tflite_runtime\r\n\r\nWhen I tried to import as follows.\r\n\r\n`from tflite_runtime import Interpreter`\r\n\r\nI get the following error in both devices (Ubuntu 16.04 and Mendel)\r\n\r\n`>>> from tflite_runtime import Interpreter\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n`\r\n\r\nBut when I just do \r\n\r\n`import tflite_runtime`\r\n\r\nDoesn't give me an error in either platform. \r\nI tried using IntelliSense and it shows me no Interpreter API. \r\n\r\nAm I doing something wrong? How to fix this issue? \r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi I found a solution\r\n\r\n`from tflite_runtime.interpreter import Interpreter`\r\n\r\nCan you update your documentation? I still need to check whether this works fine. ", "And also \r\n\r\n`from tflite_runtime import load_deleget` is not working\r\nBut\r\n\r\n`from tflite_runtime.interpreter import load_delegate` is working. \r\n\r\nAny reason why the API is not consistent in docs and in the package? \r\nAm I doing something wrong? ", "On Ubuntu Server 18.04, `from tflite_runtime import Interpreter` raises _`cannot import name 'Interpreter' from 'tflite_runtime'`_. However, `from tflite_runtime.interpreter import Interpreter` works just fine.\r\n\r\nOn the other hand, I couldn't get `interpreter` working in any way on Amazon Linux 2. `from tflite_runtime import Interpreter` raises _`cannot import name 'Interpreter' from 'tflite_runtime'`_ and `from tflite_runtime.interpreter import Interpreter` raises _`No module named '_interpreter_wrapper'`_.\r\n\r\nIs this perhaps a related issues? Any help would be much appreciated.", "Interpreter wrapper issue comes when you try to install this in a non-edge-tpu device (when using the API in such a way). I also got it. \r\nI use the BasicEngine API to do my inference on such occasions. \r\nI think it is an issue with APIs, but not 100% sure. I think they are related and I am not sure how to solve it. ", "Just to clear up my issues with Amazon Linux: it seems that TensorflowLite Runtime 1.14.0 requires GLIBC_2.27, whereas Amazon Linux (2) only supports GLIBC_2.26 and lower.\r\n\r\nIt still seems to be the case that one has to from `tflite_runtime.interpreter import Interpreter`, no matter what OS.", "I am facing the same issue with official install. I am running on rpi zero and trying to install tflite stand alone.\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_rpi\r\n\r\nError I am getting is\r\nModuleNotFoundError: No module named 'tflite_runtime'\r\n", "@vibhatha Looks like this was resolved. The imports are updated as shown below\r\n\r\n```\r\nimport tflite_runtime.interpreter as tflite\r\n\r\ninterpreter = tflite.Interpreter(model_path=args.model_file)\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> On Ubuntu Server 18.04, `from tflite_runtime import Interpreter` raises _`cannot import name 'Interpreter' from 'tflite_runtime'`_. However, `from tflite_runtime.interpreter import Interpreter` works just fine.\r\n> \r\n> On the other hand, I couldn't get `interpreter` working in any way on Amazon Linux 2. `from tflite_runtime import Interpreter` raises _`cannot import name 'Interpreter' from 'tflite_runtime'`_ and `from tflite_runtime.interpreter import Interpreter` raises _`No module named '_interpreter_wrapper'`_.\r\n> \r\n> Is this perhaps a related issues? Any help would be much appreciated.\r\n\r\n@martin-marek  i am getting the same issue on ubuntu 16. how was it solved? "]}, {"number": 31368, "title": "Internal compiler error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): Docker image latest-gpu-py3\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: RTX 2080 Ti / 12 GB\r\n\r\nI have created a fully-quantized tf lite model from a saved model. But trying to compile it with the edgetpu_compiler, I get an error:\r\n\r\n```\r\nuser@ubuntu:~/tf/tensorflow1_14$ edgetpu_compiler saved_converted_linearmodel_tpu_1.14.0.tflite \r\nEdge TPU Compiler version 2.0.258810407\r\nINFO: Initialized TensorFlow Lite runtime.\r\n\r\nInternal compiler error. Aborting!\r\n```\r\nError message is unfortunately not very helpful. The non-compiled version is loadable and produces the correct results.\r\n\r\nI have attached the model that I try to compile, as well as its visualization (via visualize.py).\r\n\r\n[litemodel.tar.gz](https://github.com/tensorflow/tensorflow/files/3472515/litemodel.tar.gz)\r\n", "comments": ["@DocDriven I have same issue.\r\nhave you solved it? could you share your solution?", "@cuongdv1 Unfortunately, I haven't figured it out yet because as far as I know, the source code for the compiler is not open source. Therefore, I couldn't debug it. Best bet is to wait for a new release of the compiler and try again.", "I'll add that I get this error when I try to compile an object detection tflite model produced by Google Cloud AutoML.  Also using Edge TPU Compiler version 2.0.258810407", "Are there any updates on this topic? I have come across this problem multiple times now, even with networks that are shipped with keras (e.g. VGG16). The test code for this is below.\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.applications import mobilenet, resnet50, inception_v3, vgg16\r\nfrom tensorflow.keras.preprocessing.image import load_img\r\nfrom tensorflow.keras.preprocessing.image import img_to_array\r\nfrom tensorflow.keras.applications.resnet50 import decode_predictions\r\n\r\n\r\n### Load and test model\r\n\r\nimagenet_dir = './tiny-imagenet-200/test/images'\r\n\r\nvgg16_model = vgg16.VGG16(weights='imagenet')\r\nprint(vgg16_model.summary())\r\n\r\nfilename = './bird.jpg'\r\noriginal = load_img(filename, target_size=(224, 224))\r\nnumpy_image = img_to_array(original)\r\nimage_batch = np.expand_dims(numpy_image, axis=0)\r\nprocessed_image = vgg16.preprocess_input(image_batch.copy())\r\n\r\npredictions = vgg16_model.predict(processed_image)\r\nlabel = decode_predictions(predictions)\r\nprint(label)\r\n\r\nkeras_file = 'vgg16.h5'\r\nkeras.models.save_model(vgg16_model, keras_file)\r\n\r\n\r\n### TF lite conversion\r\n\r\ndef representative_dataset_gen():\r\n\tfor image in os.listdir('./tiny-imagenet-200/test/images')[:500]:\r\n\t\toriginal = load_img(os.path.join('./tiny-imagenet-200/test/images', image), target_size=(224, 224))\r\n\t\tnumpy_image = img_to_array(original)\r\n\t\timage_batch = np.expand_dims(numpy_image, axis=0)\r\n\t\tprocessed_image = vgg16.preprocess_input(image_batch.copy())\r\n\t\tprint(processed_image.shape)\r\n\t\tprint(type(processed_image))\r\n\t\tyield [processed_image]\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ntflite_model = converter.convert()\r\nopen(\"vgg16_fiq.tflite\", \"wb\").write(tflite_model)\r\n\r\n\r\n### Test tflite model\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"vgg16.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\ninput_detail = interpreter.get_input_details()[0]\r\noutput_detail = interpreter.get_output_details()[0]\r\nprint('Input detail: ', input_detail)\r\nprint('Output detail: ', output_detail)\r\n\r\ninterpreter.set_tensor(input_detail['index'], processed_image)\r\ninterpreter.invoke()\r\npred_litemodel = interpreter.get_tensor(output_detail['index'])\r\nlabel_lite = decode_predictions(pred_litemodel)\r\nprint(label_lite)\r\n```\r\nI used the [Tiny ImageNet dataset](https://tiny-imagenet.herokuapp.com/) for the post training quantization. Also, my test picture is the one from the Coral demo, which I have attached. It should output magpie ([bird.jpg](https://user-images.githubusercontent.com/30296124/64694362-cb746b00-d499-11e9-84d7-b8023c736e68.jpg)).\r\n\r\nI can produce a tflite file with this code, but the TPU compiler throws the \"Internal compiler error\" again. Can you please confirm to me, if this is reproducable?\r\n", "I have the same error using tensorflow 2.0 nightly and tensorflow 1.0 nightly. Any update on this? Since the error is very generic, it is very hard to debug...", "@Lap1n \r\nAt least for the VGG16 model in my previous post, I was able to compile it with the new compiler version 2.0.267685300. Unfortunately, this did not resolve the original problem for me. Tested it with the TF 1.15 nightly docker image.", "I had the same error using the MobileNet v2 model in Keras with the tiny-imagenet-200 dataset. The TPU compiler version was 2.0.267685300. The quantized tflite file was produced successfully, but it cannot be compiled.", "@ynorz can you show me the code you used for converting and quantizing your model?", "> @ynorz can you show me the code you used for converting and quantizing your model?\r\n\r\n```python\r\ndef get_label(file_path):\r\n  # convert the path to a list of path components\r\n  parts = tf.strings.split(file_path, '/')\r\n  # The second to last is the class-directory\r\n  return parts[-3] == CLASS_NAMES\r\n\r\ndef decode_img(img):\r\n  # convert the compressed string to a 3D uint8 tensor\r\n  img = tf.image.decode_jpeg(img, channels=3)\r\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\r\n  img = tf.image.convert_image_dtype(img, tf.float32)\r\n  # resize the image to the desired size.\r\n  return tf.image.resize(img, [224, 224])\r\n\r\ndef process_path(file_path):\r\n  label = get_label(file_path)\r\n  # load the raw data from the file as a string\r\n  img = tf.io.read_file(file_path)\r\n  img = decode_img(img)\r\n  return label, img\r\n\r\ndata_dir = '/my_data_dir'\r\ndata_dir = pathlib.Path(data_dir)\r\nlist_ds = tf.data.Dataset.list_files(str(data_dir/'*/*/*'))\r\nimage_count = len(list(data_dir.glob('*/*/*.JPEG')))\r\nCLASS_NAMES = np.array([item.name for item in data_dir.glob('*')])\r\nlabeled_ds = list_ds.map(process_path, num_parallel_calls=100)\r\ntf.compat.v1.enable_eager_execution()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ndef representative_data_gen():\r\n    for _,image in labeled_ds.take(100):\r\n        image = tf.expand_dims(image, 0)\r\n        yield [image]\r\n\r\nconverter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverted_tflite_model = converter.convert()\r\nopen(TFLITE_MODEL,\"wb\").write(converted_tflite_model)\r\n```", "@bhavitvyamalik \r\nThe tflite model I got from this code could run on CPU but compiling it would trigger the compiler error.", "There can be 2 possibilities why your model is giving internal compiler error.\r\n\r\n1) Most basic of all is you have not done your modeling right and some operators are not supported by Edge TPU while you are compiling it. I remember my custom model giving the same error when it had only basic operators which are supported by Edge TPU. So make sure your model is supported which you are converting. If it applies then move on to next point.\r\n\r\n2) Versioning error. I tried converting my code in Tensorflow2.0 which didn't run in Edge TPU even after compiling. Make sure you are using Tensorflow 1.15.0 for converting and quantizing. This was my code for the same:\r\n```\r\ndef representative_data_gen():\r\n  for input_value in mnist_data[:100]:     //mnist_data list contains images which were resized during appending\r\n    data = np.array([input_value])\r\n    yield [data]\r\n\r\nopt = tf.lite.Optimize.DEFAULT\r\nops = tf.lite.OpsSet.TFLITE_BUILTINS_INT8\r\ndtype = tf.uint8\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(model_path)\r\n\r\nconverter.optimizations = [opt]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [ops]\r\nconverter.inference_input_type = dtype\r\nconverter.inference_output_type = dtype\r\n\r\ntflite_quant_model = converter.convert()\r\nopen(\"model_quantised.tflite\", \"wb\").write(tflite_quant_model)\r\n```", "https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\r\n\r\nI tried to do the post-training integer quantization following the official guide on MNIST. And this guide can only run on tensorflow 1.15.0, which, to some extent, did prove your point that tensorflow 1.15 worked better. However, I still get the internal compiler error with compiler version: 2.0.267685300.", "If you are getting internal compiler error then your operations are supported by the Edge TPU during compilation. However it can still run on the CPU of your Edge TPU but it'll increase the inference time to a large extent. \r\n\r\nMost importantly, you can compile only these models successfully on Edge TPU:\r\n- Mobilenet_v1\r\n- Mobilenet_v2\r\n- Inception_v3\r\n- ResNet50\r\n\r\nIf you'll use any other model, it might not work properly. Try using one of these models followed by quantization using the code I posted earlier. It should work flawlessly.\r\n", "Figured out a solution that sounds stupid. I moved the folder 'models' with .tflite file to '/home/username/edgetpu', and then the compiler works with the same compile code provided on the official website. This 'edgetpu' folder was created through a beginner object detection retrain example using dataset of American bulldog and  Abyssinian provided on the official website.\r\n\r\nMy setup: custom dataset, mobilenet_v1 or mobilenet_v2 downloaded from coral website, coral accelerator. ", "> Figured out a solution that sounds stupid. I moved the folder 'models' with .tflite file to '/home/username/edgetpu', and then the compiler works with the same compile code provided on the official website. This 'edgetpu' folder was created through a beginner object detection retrain example using dataset of American bulldog and Abyssinian provided on the official website.\r\n> \r\n> My setup: custom dataset, mobilenet_v1 or mobilenet_v2 downloaded from coral website, coral accelerator.\r\n\r\nOn this same example code, I initially received this error after compiling with **edgetpu_compiler output_tflite_graph.tflite**:\r\n\r\n```\r\nEdge TPU Compiler version 2.0.291256449\r\n\r\nModel compiled successfully in 276 ms.\r\n\r\nInput model: output_tflite_graph.tflite\r\nInput size: 5.34MiB\r\nOutput model: output_tflite_graph_edgetpu.tflite\r\nOutput size: 5.75MiB\r\nOn-chip memory available for caching model parameters: 7.62MiB\r\nOn-chip memory used for caching model parameters: 5.66MiB\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 1\r\nTotal number of operations: 64\r\nOperation log: output_tflite_graph_edgetpu.log\r\n\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 63\r\nNumber of operations that will run on CPU: 1\r\nSee the operation log file for individual operation details.\r\nError opening file for writing: output_tflite_graph_edgetpu.tflite\r\n\r\nInternal compiler error. Aborting!\r\n```\r\n\r\nBut was able to get around it after I ran with **sudo**, which gives the following output:\r\n\r\n```\r\nEdge TPU Compiler version 2.0.291256449\r\n\r\nModel compiled successfully in 341 ms.\r\n\r\nInput model: output_tflite_graph.tflite\r\nInput size: 5.34MiB\r\nOutput model: output_tflite_graph_edgetpu.tflite\r\nOutput size: 5.75MiB\r\nOn-chip memory available for caching model parameters: 7.62MiB\r\nOn-chip memory used for caching model parameters: 5.66MiB\r\nOff-chip memory used for streaming uncached model parameters: 0.00B\r\nNumber of Edge TPU subgraphs: 1\r\nTotal number of operations: 64\r\nOperation log: output_tflite_graph_edgetpu.log\r\n\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 63\r\nNumber of operations that will run on CPU: 1\r\nSee the operation log file for individual operation details.\r\n```\r\n\r\nNote: I didn't have to move the files around as mentioned in the previous post.", "I'am having same issue with a tflite model with **transpose convolution**. Tensorflow 1.x does not seem to support transpose convolution. With latest tf2.0-nighlty quantized tflite model it gives the error: **'Internal compiler error. Aborting!**. It would be helpful, if the compiler exactly  prints the cause of failure i.e. if some operators or it's version is not supported. It seem to works until some random convolutional layer(602) and produces compiler error after its inclusion!!!\r\n\r\n[pnet_test.tflite.zip](https://github.com/tensorflow/tensorflow/files/4225324/pnet_test.tflite.zip)\r\n", "> I'am having same issue with a tflite model with **transpose convolution**. Tensorflow 1.x does not seem to support transpose convolution. With latest tf2.0-nighlty quantized tflite model it gives the error: **'Internal compiler error. Aborting!**. It would be helpful, if the compiler exactly prints the cause of failure i.e. if some operators or it's version is not supported. It seem to works until some random convolutional layer(602) and produces compiler error after its inclusion!!!\r\n> \r\n> [pnet_test.tflite.zip](https://github.com/tensorflow/tensorflow/files/4225324/pnet_test.tflite.zip)\r\n\r\nI am also having a similar issue. I have a custom model which uses transpose convolution that I want to compile for edge tpu. Was there any solution?", "> I'am having same issue with a tflite model with **transpose convolution**. Tensorflow 1.x does not seem to support transpose convolution. With latest tf2.0-nighlty quantized tflite model it gives the error: **'Internal compiler error. Aborting!**. It would be helpful, if the compiler exactly prints the cause of failure i.e. if some operators or it's version is not supported. It seem to works until some random convolutional layer(602) and produces compiler error after its inclusion!!!\r\n> \r\n> [pnet_test.tflite.zip](https://github.com/tensorflow/tensorflow/files/4225324/pnet_test.tflite.zip)\r\n\r\nHave you tried `edgetpu_compiler -s your_tflite_graph.tflite`?\r\nIt prints out the log", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31368\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31368\">No</a>\n"]}, {"number": 31367, "title": "Operation 'ExtractImagePatches' has no attr named '_XlaCompile'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):b'unknown' 1.12.0\r\n- Python version:Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0 7.0.5\r\n- GPU model and memory: GTX 1080 8G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nCan't take gradients, fails with InvalidArgumentError: Operation 'ExtractImagePatches' has no attr named '_XlaCompile'\r\n**Describe the expected behavior**\r\nGetting gradients without error\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ninputs = np.array([[[x + y * 10 for x in [1,2]] for y in range(5)]])\r\ninputs = inputs.reshape((1,1,inputs.shape[1],inputs.shape[2]))\r\nprint(\"inputs:\", inputs)\r\n\r\nimport tensorflow as tf\r\ntf.reset_default_graph()\r\nwith tf.Graph().as_default() as g:\r\n    x = tf.placeholder(\"float\", [1, 1, None, 2])\r\n    y = tf.extract_image_patches(images=x, ksizes=[1, 1, 3, 1], strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding='VALID')\r\n    gradient = tf.gradients(y, x)\r\n    init = tf.global_variables_initializer()\r\n    \r\nwith tf.Session(graph=g) as sess:\r\n    sess.run(init)\r\n    print(sess.run([y], feed_dict = {x:inputs}))\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 1.12 [gist](https://colab.research.google.com/drive/1a1lDY5ZsPsHI3ZRk3ePWO3lP_r-k2Y2D) ,1.13,1.14,nightly versions [gist ](https://colab.research.google.com/drive/1B-1ObJLXLNpuOA_Tx_WUtwkq15oiqTMt)and was able to reproduce the issue.However in Nightly version it is throwing different error ```\r\n\r\n```\r\nValueError: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"init\"\r\nop: \"NoOp\"\r\n is not an element of this graph\r\n```\r\n", "@mkravchik I cannot reproduce the error if you remove init op. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/179569689d746cc40770799bb952024f/tf_31367.ipynb). Thanks!", "Thanks probably because you are using the nightly build. With my version it happens regardless the init op. I think that the root cause for this is the following issue https://github.com/tensorflow/tensorflow/issues/11651 which have been fixed recently, but not released", "@mkravchik You could use `tf-nightly` for now and in the next couple of months new stable version will be released. I am closing the issue for now. Please feel free to open the issue if it persists with the new version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31367\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31367\">No</a>\n"]}, {"number": 31366, "title": "New Updated curl to 7.65.1", "body": "Updated curl to latest version 7.65.1", "comments": ["@shubham769 can you please check build failures ?"]}, {"number": 31365, "title": "tf.summary.histogram broken in tf-nightly-gpu-2.0-preview", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0-dev20190806\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nCalling `tf.summary.histogram` on a non eager tensor results in an exception: ```OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\ninputs = tf.keras.layers.Input(shape=(300,))\r\ntf.summary.histogram(\"in\", inputs)\r\n```\r\n\r\n\r\n", "comments": ["Re produced this issue. Please find the colab notebook [here](https://colab.sandbox.google.com/gist/gowthamkpr/8fe198df1611ed750d2a98e4ab6f6b1a/untitled76.ipynb)", "@ialdencoots The second line in your code [`tf.summary.histogram`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram) is expecting a real numeric tensor as an input but you are providing non-eager tensor `tf.keras.layers.Input`. In the TF website, it was clearly mentioned that \r\n\r\n`values: A real numeric Tensor. Any shape. Values to use to build the histogram`\r\n\r\nThe following with a numeric tensor works well\r\n```\r\nimport tensorflow as tf\r\n# inputs = tf.keras.layers.Input(shape=(300,))\r\ninputs=tf.constant([1.,2.,3.,4.,5.,6.])\r\ntf.summary.histogram(\"in\", inputs)\r\n```\r\nI am closing the issue as it was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31365\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31365\">No</a>\n", "Do you mean to tell me you can't get summary statistics for any tensor that is the result of an operation of a tensor fed into a graph? So basically weights are the only tensors you can get histograms for? If that's not a bug, it's definitely functionality that ought to exist.", "@ialdencoots All I am saying is the function `tf.summary.histogram` can take only numerical tensor by design. `Input()` is used to instantiate a Keras graph tensor whereas `tf.summary.histogram` takes only a numerical tensor. `weights` are not the only tensors that can get histogram. Please check an [example](https://www.tensorflow.org/guide/tensorboard_histograms#some_more_distributions) from TF website where it uses results of an operation of a tensor fed into a graph. Please let me know whether it helped you or not. Thanks!", "Yeah, ok @jvishnuvardhan , that was just a minimal example. Say you have an arbitrary graph with inputs and you want to calculate literally anything and get a histogram of it. The error still persists.\r\n```\r\nimport tensorflow as tf\r\ninputs = tf.keras.layers.Input(shape=(300,))\r\nmult = 3 * inputs\r\ntf.summary.histogram(\"hist\", mult)\r\n```\r\nDo you mean to tell me that `mult` is not a numerical tensor?", "@ialdencoots Still `mult` is a graph tensor which cannot be given as input to `tf.summary.histogram`. Please check my implementation that works without any issue. Please let me know if you have any comments. Thanks!\r\n```\r\n!pip install tf-nightly-2.0-preview\r\nimport tensorflow as tf\r\ninputs = tf.keras.layers.Input(shape=(300,))\r\nmult = 3 * inputs\r\n# mult = tf.keras.layers.Lambda(lambda x:3*x)(inputs) # you could do this also\r\nmodel=tf.keras.Model(inputs=inputs, outputs=mult)\r\nmult_from_model=model(tf.random.uniform(minval=-1,maxval=1,shape=(300,),dtype=tf.float32))\r\ntf.summary.histogram(\"hist\", mult_from_model)\r\n```", "Ok, so histogram can't be called on anything internal to a model? If I want to check the distribution of tensors in internal layers, I have to produce a model with multiple outputs?", "This problem persists in 2.0! Is the only way of getting out summaries to TensorBoard during training with the builtin training loop from tf.keras to avoid the functional API and use model subclassing instead?\r\n\r\nThe subclassing API has subpar support in other parts of the TensorFlow ecosystem so I'd prefer to stick with the functional API if possible."]}, {"number": 31364, "title": "Tons of warnings just by importing tf (2.0.0-beta1)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nI just created a virtual environnement to test tensorflow 2.0.0-beta1 with no other package installed. Just calling `import tensorflow` launched many warning messages. It is strange that I already use 2.0.0-beta1 in other virtual environnements and never saw these warnings before.\r\n\r\n**Code to reproduce the issue**\r\n\r\nIn bash:\r\n```\r\npython3 -m venv testenv\r\nsource testenv/bin/activate\r\npip install tensorflow==2.0.0-beta1\r\npython\r\n```\r\nThen in python:\r\n```\r\nimport tensorflow as tf\r\n# print(tf.version.GIT_VERSION, tf.version.VERSION)\r\n ```\r\n\r\n**Warning messages**\r\n```\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/path-to-testenv/testenv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n```\r\n", "comments": ["@durandg12 \r\n\r\nCan you please let us know which numpy version you are using. Thanks!", "@ravikyram I do not install numpy myself in the virtual environnement. However, when I install tensorflow 2.0.0-beta1 it installs numpy 1.17.0.\r\n\r\n```\r\n$pip install tensorflow==2.0.0-beta1\r\n...\r\nCollecting numpy<2.0,>=1.14.5 (from tensorflow==2.0.0-beta1)\r\n  Using cached https://files.pythonhosted.org/packages/be/e8/45079ae05c4dda4a67bc51578ae5e75feda0a79c2836d477d676e7a58efb/numpy-1.17.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\r\n...\r\n```\r\n\r\n```\r\n$ pip freeze\r\nabsl-py==0.7.1\r\nastor==0.8.0\r\ngast==0.2.2\r\ngoogle-pasta==0.1.7\r\ngrpcio==1.22.0\r\nh5py==2.9.0\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nMarkdown==3.1.1\r\nnumpy==1.17.0\r\nprotobuf==3.9.1\r\nsix==1.12.0\r\ntb-nightly==1.14.0a20190603\r\ntensorflow==2.0.0b1\r\ntermcolor==1.1.0\r\ntf-estimator-nightly==1.14.0.dev2019060501\r\nWerkzeug==0.15.5\r\nwrapt==1.11.2\r\n```", "\r\nI think TF 2.0.b1 comes with numpy dependency 1.16.4 and it shows no warning. \r\n\r\nHowever, numpy 1.17.0 ,tensorflow could be updated to make it compatible with numpy 1.17. Created a PR #30559 for the fix.\r\nAlso, please refer #30427 for more insights.Thanks!", "From what I understand, TF 2.0.b1 is designed to work with numpy 1.16.4, but it installs 1.17. Isn't that a little illogical ? Anyway, since you already created a PR that has been merged, I assume that the warnings will disappear in the near future and that we can close this issue.", "See also https://github.com/tensorflow/tensorflow/issues/31249 and https://github.com/tensorflow/tensorflow/pull/30559\r\n\r\nTL;DR: Both the RC0 for TF2.0 and the patch 1.14.1 will have these warnings solved already.", "What does RC0 mean ?", "release candidate 0, the next step after the betas.\r\n\r\nThere will be a few RCs and then 2.0 will be officially launched in a final form", "Thank you. I can close this issue then.", "You can simply revert back to the numpy 1.16.1 version, by \r\n\u2192 pip install numpy==1.16.1\r\n", "Or you can test with the newest released RC0 for TF 2.0\r\n\r\nWe recommend testing the release candidates as after the final release we don't do patch releases except for security related issues", "> Or you can test with the newest released RC0 for TF 2.0\r\n> \r\n> We recommend testing the release candidates as after the final release we don't do patch releases except for security related issues\r\n\r\n@mihaimaruseac I did it and the warnings are gone, thanks! So this issue can stay closed.\r\nStrangely, the warnings from [this issue](https://github.com/tensorflow/tensorflow/issues/29881) are not gone whereas the issue has been closed 2 months ago by [the following commit](https://github.com/tensorflow/tensorflow/commit/d7e858192d1de827bc03705ac62e1bd38daf06d8)."]}, {"number": 31363, "title": "tf.function changes behavior of += operator on tf.Tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install (wheel?)\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.7.3\r\n- GPU model and memory: CPU\r\n\r\n\r\n**Describe the current behavior**\r\nBehaviour of += operator on a tensor is different when called via tf.function and when called directly\r\n\r\n**Describe the expected behavior**\r\ntf.function does not change the behavior of any operations on tensors.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nclass First:\r\n    def __init__(self, initial: int):\r\n        self.value = tf.constant(initial)\r\n\r\n    def increment(self):\r\n        self.value += 1\r\n\r\n    def __str__(self):\r\n        return f'Object with tensor = {self.value}'\r\n\r\n@tf.function\r\ndef increment(obj: First):\r\n    obj.increment()\r\n\r\nc1 = First(100)\r\n\r\nc1.increment()\r\nprint(c1)\r\n\r\nincrement(c1)\r\nprint(c1)\r\n```\r\n\r\n**Other info / logs**\r\noutput:\r\n```\r\nObject with tensor = 101\r\nObject with tensor = Tensor(\"add:0\", shape=(), dtype=int32)\r\n```\r\n\r\nExpected output:\r\n\r\n```\r\nObject with tensor = 101\r\nObject with tensor = 102\r\n```\r\n", "comments": ["I tried the code on Colab and was able to reproduce the issue with Tensorflow 2.0.0beta1. Please see the gist [here](https://colab.research.google.com/drive/13h9wPKWxCkWP0L97VqkoIFor4N3wT8N3). Thanks!  ", "It's not about the behavior of +=, it's about the fact that you're assigning a tensor inside a function graph to an object which outlives that function.\r\n\r\nIf you need to change the value of something inside a tf.function and you want that to persist either make it a return value or make it a tf.Variable, as otherwise tf has no way of knowing what you want.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31363\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31363\">No</a>\n", "@alextp \r\nThen it is a leaky abstraction. As a user, I don't want to know how tensorflow makes its magic. I am doing a simple tensor operation inside a tf.function, why should it break?\r\n\r\ntf has a way of knowing what I want, as it works outside tf.function."]}, {"number": 31362, "title": "Non-tensorflow code gets executed only on the first run of tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install (wheel?)\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.7.3\r\n- GPU model and memory: CPU\r\n\r\n**Describe the current behavior**\r\nNon-tensorflow code gets executed only on the first run of tf.function\r\n\r\n**Describe the expected behavior**\r\nEither exception is thrown or the code is executed every time. As it is, it just welcomes bugs.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nclass MyObj:\r\n    def __init__(self):\r\n        self.value = 0\r\n\r\nobj = MyObj()\r\n\r\n@tf.function\r\ndef with_py_side_effect(tensorflow_stuff, o):\r\n\r\n    # do my complex tf stuff\r\n    ...\r\n\r\n    o.value += 1\r\n    return tensorflow_stuff\r\n\r\nfor i in range(5):\r\n    print(i, obj.value)\r\n    a = with_py_side_effect(None, obj)\r\n```\r\n\r\n**Other info / logs**\r\nMy output:\r\n```\r\n/Users/ikkamens/.pyenv/versions/3.7.3/bin/python /Users/ikkamens/Library/Preferences/PyCharmCE2019.2/scratches/tf_foo_python.py\r\n2019-08-06 11:53:11.888584: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2.0.0-beta1\r\n0 0\r\n1 1\r\n2 1\r\n3 1\r\n4 1\r\n\r\nProcess finished with exit code 0\r\n```", "comments": ["@ikamensh ,\r\nWhen tried executing the given code with TF version 2.0.0beta1 I did not receive any warning, please take a look [gist](https://colab.sandbox.google.com/drive/1LDYziezlHD2QHZTq_gI-JfWpra3tCjD2#scrollTo=CRBQ-MorJb7I) of collab.Thanks!", "@anush-o \r\nI am not complaining about a warning. My code should increment the o.value on each iteration, but as seen from the output it does so only on the first iteration. A warning would be desired, but is not shown!\r\n\r\nExpected output:\r\n```\r\n\r\n2.0.0-beta1\r\n0 0\r\n1 1\r\n2 2\r\n3 3\r\n4 4\r\n\r\n```", "@kkimdev @alextp @dynamicwebpaige @lamberta @martinwicke FYI\r\n\r\n`tf.function` allows you to express TensorFlow computations using a subset of Python, however it is not a general-purpose execution back-end for the Python language, at least not in this version; perhaps we should call this out more clearly in the docs.\r\n\r\nOne of the side effects if this mechanism is the behavior you experienced. In general, you should try to write all computation using tf functions to make sure it's executed the way you expect.\r\n\r\nFor a bit more background, please have a look at the [API docs](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function) and the [autograph reference](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/index.md).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31362\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31362\">No</a>\n"]}, {"number": 31361, "title": "UniqueV2 reports incorrect output shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04.6 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory: Tesla K40C, 11441MiB\r\n\r\n**Describe the current behavior**\r\nWhen not using eager execution, UniqueV2 always reports its first output to have rank 1.\r\n\r\n**Describe the expected behavior**\r\nUniqueV2 should report its first output to have the same rank as its input.\r\n\r\n**Code to reproduce the issue**\r\nThe bug can be exposed by forcing non-eager execution through `tf.function` or `tf.compat.v1.disable_eager_execution()`. The former is demonstrated below:\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import gen_array_ops\r\n\r\ndef unique_rank(x, axis):\r\n    unique = gen_array_ops.unique_v2(x, axis)\r\n    return tf.rank(unique[0])\r\n\r\n# 2D input should produce a 2D output\r\nx = tf.ones([2, 2])\r\nprint(\"UniqueV2 output 0 rank:\", tf.function(unique_rank)(x, [0]))\r\nprint(\"UniqueV2 output 0 rank executing eagerly:\", unique_rank(x, [0]))\r\n```\r\nThis outputs\r\n```\r\nUniqueV2 output 0 rank: tf.Tensor(1, shape=(), dtype=int32)\r\nUniqueV2 output 0 rank executing eagerly: tf.Tensor(2, shape=(), dtype=int32)\r\n```\r\nbut should output\r\n```\r\nUniqueV2 output 0 rank: tf.Tensor(2, shape=(), dtype=int32)\r\nUniqueV2 output 0 rank executing eagerly: tf.Tensor(2, shape=(), dtype=int32)\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.0 beta1,TF 2.0.0-dev20190805 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1RpI4ba9Dg7Elu2yr4jjdOkyQoJVc6f67) here.Thanks!", "@yongtang I think this was broken since https://github.com/tensorflow/tensorflow/commit/7603480f3 when uniquev2 was added.\r\n\r\nDo you have time to work on this?", "Tatiana, can you shepherd this from the TF ops side? The issue is that the shape inference function for uniquev2 ignores the fact that when axis is specified it preserves rank", "I think this falls into Rasmus's domain of expertise.", "With the latest tf-nightly the result works correctly:\r\n```python\r\n# python v.py \r\n('UniqueV2 output 0 rank:', <tf.Tensor: shape=(), dtype=int32, numpy=2>)\r\n('UniqueV2 output 0 rank executing eagerly:', <tf.Tensor: shape=(), dtype=int32, numpy=2>)\r\n# python -c 'import tensorflow as tf; print(tf.version.VERSION)'\r\n2.1.0-dev20191010\r\n```\r\n\r\nI think the issue has been resolved.", "As @yongtang mentioned, this was resolved in `tf-nightly`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/04693a821f4c3e3eef2a1b38088af9e5/untitled574.ipynb) is the gist for your reference.\r\n\r\nI am closing the issue as it was resolved in `tf-nightly`. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31361\">No</a>\n"]}, {"number": 31360, "title": "Using tf.feature_columns in exported estimators fails when using tf.feature_columns.indicator_column", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n### System information\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: CPU\r\n\r\nTensorflow Serving information:\r\nUsing tensorflow serving docker image, tag latest, as of 05/08/2019 (August 5th):\r\n- TensorFlow ModelServer: 1.14.0-rc0\r\n- TensorFlow Library: 1.14.0\r\n\r\n### Describe the current behavior\r\n_[I originally opened an issue at tensorflow/serving](https://github.com/tensorflow/serving/issues/1409), but I got redirected here as it is a tensorflow issue._\r\n\r\nTensorflow serving doesn't handle a 'feature_columns input layer' in the Estimator model_fn.\r\nWhen using tf.feature_columns.input_layer or tf.keras.layer.DenseFeatures to process feature_columns in the model_fn: If you have a feature_column that is a categorical_column wrapped by an indicator_column, Tensorflow serving fails.\r\n\r\nTensorflow serving doesn't seem to properly handle the indicator_column. It responds with:\r\n```python\r\n{\r\n    \"error\": \"Input to reshape is a tensor with <n> values, but the requested shape has <n squared>\\n\\t [[{{node input_layer/<feature name>_indicator/Reshape}}]]\"\r\n}\r\n```\r\n[The reshape op where the error is thrown](https://github.com/tensorflow/tensorflow/blob/29ecfbf1e7ab2f073e69770753174667079d10b5/tensorflow/core/kernels/reshape_op.h#L92)\r\n\r\n[I asked around on stackoverflow](https://stackoverflow.com/questions/57327655/is-there-a-way-to-export-custom-tensorflow-r1-14-estimators-that-are-able-to-p), if there were workarounds, no response so far.\r\n\r\nThe main advantage of tf.feature_columns happens to be the indicator_column (which allows for easy one-hot encoding of features in the model code). It is also pushed in multiple Tensorflow guides as something that's used. **I think this bug blocks practical use of the tf.feature_columns module in production.**\r\n\r\n_When not using the indicator_column as a feature_column, all seems well_\r\n\r\n### Describe the expected behavior\r\nTensorflow serving should just serve inference requests, (as it does when not using indicator_column)\r\n\r\n### Code to reproduce the issue\r\nScript to export saved_models from estimators that use feature_columns:\r\n```\r\n\"\"\"Code for testing tensorflow serving reshape bug\"\"\"\r\n\r\nimport tensorflow as tf\r\n\r\nfeature_columns = [\r\n    # Feature columns that use indicator column\r\n    tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity('test', 2))\r\n]\r\n\r\nestimator_params = {\r\n    'feature_columns': feature_columns\r\n}\r\n\r\n\r\ndef model_fn(features, labels=None, mode=None, params=None):\r\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n    inputs = tf.feature_column.input_layer(features, params['feature_columns'])\r\n\r\n    if not is_training:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode,\r\n            predictions=inputs\r\n        )\r\n\r\n    a = tf.Variable(1, dtype=tf.float32, trainable=True)\r\n\r\n    # Doesn't need to train, but the model needs to be trainable for exporting to work\r\n    loss = tf.reduce_mean(a * inputs)\r\n\r\n    optimizer = params.get('optimizer', None) or tf.train.AdamOptimizer(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        loss=loss,\r\n        train_op=train_op\r\n    )\r\n\r\n\r\ndef input_fn():\r\n    return {'test': tf.constant([1, 0], dtype=tf.int64)}, tf.constant([3, 2], dtype=tf.float32)\r\n\r\n\r\ndef serving_input_fn():\r\n    receiver_tensors = {\r\n        'test': tf.placeholder(tf.int64, shape=[None, 1], name='test')\r\n    }\r\n\r\n    return tf.estimator.export.ServingInputReceiver(receiver_tensors, receiver_tensors)\r\n\r\n\r\n# Custom estimator\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, params=estimator_params)\r\n# Canned estimator\r\n# estimator = tf.estimator.DNNRegressor([2, 2, 1], feature_columns=feature_columns)\r\n\r\nestimator.train(input_fn=input_fn, steps=5)\r\n\r\nestimator.export_saved_model('./', serving_input_fn)\r\n```\r\nServe the generated saved_model with Tensorflow serving.\r\nNow make requests to it.\r\n\r\nExample body for custom estimators:\r\n```\r\n{\r\n\t\"inputs\": {\r\n\t\t\"test\": [0, 1]\r\n\t}\r\n}\r\n```\r\nExample body for the canned estimator:\r\n```\r\n{\r\n\t\"signature_name\": \"predict\",\r\n\t\"inputs\": {\r\n\t\t\"test\": [0, 1]\r\n\t}\r\n}\r\n```\r\n\r\n### Source code / logs\r\nAll I get as response from tensorflow serving:\r\n```\r\n{\r\n    \"error\": \"Input to reshape is a tensor with 2 values, but the requested shape has 4\\n\\t [[{{node input_layer/test_indicator/Reshape}}]]\"\r\n}\r\n```\r\nIt seems to always be: \"... with _n_ values\", \"... requested shape has _n squared_\". Always in the reshape op.\r\n\r\nI think it may be due to improper (de)serialization to or from Saved Model when using tf.keras.DenseFeatures or tf.feature_column.input_layer.\r\nIt may also be some bugged tensorflow op (just guessing)\r\n\r\n", "comments": ["@nielsgroen, Will it be possible to provide the code, which i can reproduce the reported issue on colab or jupyter notebook. Thanks!", "@gadagashwini \r\n**If you must run it in google colab:**\r\n1. Put the code in a cell in google colab.\r\n2. When in Colab, on the left you see an arrow to open a sidebar, open it.\r\n3. Go to 'files'. (This puts you in the '/content' folder)\r\n4. Once you ran the notebook, refresh the files. (The saved_model should be in /content folder)\r\n\r\n**Or just run it locally:**\r\n1. copy and paste the python code in a virtual environment with tf-1.14.0 installed.\r\n2. run it (the saved_model appears in the same folder as that python file you pasted the code to)\r\n\r\n**Serve the saved_model**\r\nNow, since it is an issue which requires serving the said saved_model (remember, it is an issue with exported estimators, so we need to serve it up), we are going to serve the saved_model.\r\n_**This requires [tensorflow serving](https://www.tensorflow.org/tfx/serving/docker)**_.\r\n\r\nFollow [the instructions](https://www.tensorflow.org/tfx/serving/docker) to serve the model.\r\n\r\nMake a POST request to tensorflow serving with header application/json and the body provided in the original post ^^.\r\n\r\nIf you feel like this is outside of your scope to reproduce, could you please reassign?\r\nThanks for your help.\r\n", "I tried replicating the issue with Tensorflow 1.14.0 on Colab. I didn't get any error. Please see the colab [gist](https://colab.research.google.com/drive/1HNt1FjiPEXAqGMUPjlQS3M7yITYuhUGt). Thanks!", "### As is clearly stated in the issue: The script does not produce an error.\r\n### The script produces a saved_model.\r\n[(what is a saved_model)](https://www.tensorflow.org/guide/saved_model)\r\n### When serving the saved_model:\r\nTensorflow serving produces the error (mentioned above) when a request is made (request body mentioned above).\r\n\r\n_With all due respect, someone who doesn't understand what the issue is about should not assign it to themselves. Whether it be due to a lack of a firm grasp of English (which is totally understandable) or due to other, less desirable, reasons._\r\n", "Closing as there is a easy workaround found.\r\n\r\nIt turns out, when using the indicator_column, making the inputs strictly two-dimensional will work.\r\n```\r\n{\r\n\t\"signature_name\": \"predict\",\r\n\t\"inputs\": {\r\n\t\t\"test\": [[0], [1]]  // note the extra dimension\r\n\t}\r\n}\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31360\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31360\">No</a>\n"]}, {"number": 31359, "title": "tflite output different result with pbfile when using only one convolutional layer ?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): In a Ubuntu18.04 docker container.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow==1.14\r\n- TensorFlow version (use command below): tf-cpu==1.14.0\r\n- Python version: python3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No.\r\n- GPU model and memory: No.\r\n\r\n**Describe the current behavior**\r\ntflite output different result with pbfile when using only one convolutional layer ?\r\n\r\n**Describe the expected behavior**\r\ntflite should output the same result with pbfile when using only one convolutional layer.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nimport warnings\r\nwarnings.filterwarnings(action='ignore', category=FutureWarning)\r\n\r\n######################################################################\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools.freeze_graph import freeze_graph\r\n\r\nCONFIG = tf.compat.v1.ConfigProto()\r\nCONFIG.gpu_options.allow_growth = True\r\n\r\n######################################################################\r\n\r\ninputs_debug = [\"inpT\"]\r\noutputs_debug = [\"outT\"]\r\nshape_debug = (1, 32, 240, 1)\r\ndtype_debug = np.float32\r\npbPath_debug = './debug/fuck.pb'\r\nlitePath_debug = './debug/fuck.lite'\r\ncheckpoint_debug = './debug/fuck.ckpt'\r\n\r\n######################################################################\r\n\r\ndef convert(pbPath, litePath, inputs, outputs):\r\n    from_frozen_graph = tf.lite.TFLiteConverter.from_frozen_graph\r\n    converter = from_frozen_graph(pbPath, inputs, outputs)\r\n\r\n    tflite_model = converter.convert()\r\n    open(litePath, \"wb\").write(tflite_model)\r\n\r\n######################################################################\r\n\r\ndef generate_data(shape, dtype=np.float32):\r\n    data = np.array(np.random.randint(0, 255, shape), dtype=dtype)\r\n    if dtype == np.float32: data = (data - 127.0) / 128.0\r\n\r\n    return [data]\r\n\r\n######################################################################\r\n\r\ndef get_tf_engine(pbPath, inpNs, outNs):\r\n    inpNs = [inpN + \":0\" for inpN in inpNs]\r\n    outNs = [outN + \":0\" for outN in outNs]\r\n\r\n    graph = tf.compat.v1.Graph()\r\n    with graph.as_default():\r\n        f = tf.io.gfile.GFile(pbPath, \"rb\")\r\n        graphDef = tf.compat.v1.GraphDef()\r\n        graphDef.ParseFromString(f.read())\r\n\r\n        Ts = tf.import_graph_def(\r\n            graphDef, name='',\r\n            return_elements=inpNs + outNs,\r\n        )\r\n        inpT, outT = Ts[:len(inpNs)], Ts[len(inpNs):]\r\n        session = tf.compat.v1.Session(config=CONFIG)\r\n\r\n    def get_tf_target(data):\r\n        return session.run(\r\n            outT,\r\n            {iT: d for (iT, d) in zip(inpT, data)}\r\n        )\r\n\r\n    return get_tf_target\r\n\r\n######################################################################\r\n\r\ndef get_lite_engine(litePath):\r\n    interpreter = tf.lite.Interpreter(litePath)\r\n    interpreter.allocate_tensors()\r\n\r\n    input_details = interpreter.get_input_details()\r\n    outputs_details = interpreter.get_output_details()\r\n\r\n    def get_lite_output(data):\r\n        for (input_detail, d) in zip(input_details, data):\r\n            interpreter.set_tensor(input_detail['index'], d)\r\n        interpreter.invoke()\r\n\r\n        return [\r\n            interpreter.get_tensor(outputs_detail['index'])\r\n            for outputs_detail in outputs_details\r\n        ]\r\n\r\n    return get_lite_output\r\n\r\n######################################################################\r\n\r\nif __name__ == '__main__':\r\n    if not os.path.exists(checkpoint_debug+\".meta\"):\r\n        with tf.compat.v1.Graph().as_default():\r\n            inpT = tf.compat.v1.placeholder(\r\n                dtype_debug, shape_debug, inputs_debug[0])\r\n\r\n            x = inpT\r\n            x = tf.layers.conv2d(x, 64, (3,3), (1,1), 'same', dilation_rate=(1,1))\r\n\r\n            outT = tf.identity(x, outputs_debug[0])\r\n\r\n            saver = tf.compat.v1.train.Saver()\r\n            with tf.compat.v1.Session(config=CONFIG) as session:\r\n                session.run(tf.compat.v1.global_variables_initializer())\r\n\r\n                saver.save(session, checkpoint_debug)\r\n\r\n    if not os.path.exists(pbPath_debug):\r\n        freeze_graph(\r\n            input_graph=None,\r\n            input_saver=None,\r\n            input_binary=True,\r\n            input_checkpoint=checkpoint_debug,\r\n            output_node_names=','.join(outputs_debug),\r\n            restore_op_name=None,\r\n            filename_tensor_name=None,\r\n            output_graph=pbPath_debug,\r\n            clear_devices=True,\r\n            initializer_nodes=None,\r\n            variable_names_whitelist=\"\",\r\n            variable_names_blacklist=\"\",\r\n            input_meta_graph=checkpoint_debug+\".meta\",\r\n            input_saved_model_dir=None,\r\n        )\r\n\r\n    if not os.path.exists(litePath_debug):\r\n        convert(pbPath_debug, litePath_debug, inputs_debug, outputs_debug)\r\n\r\n    get_tf_target = get_tf_engine(\r\n        pbPath_debug, inputs_debug, outputs_debug)\r\n    get_lite_output = get_lite_engine(litePath_debug)\r\n\r\n    for i in range(10):\r\n        data = generate_data(shape_debug, dtype_debug)\r\n\r\n        targets = get_tf_target(data)\r\n        outputs = get_lite_output(data)\r\n\r\n        for (target, output) in zip(targets, outputs):\r\n            print(target.shape, output.shape, end='\\t')\r\n            print(np.allclose(target, output, 1e-5, 1e-8), end='\\n')\r\n\r\n```\r\n\r\n**I use the code above to generate a pb file with only one convolutional layer, and convert it to a tflite file. And the output of the pb and tflite files are different as following. I wonder why ?**\r\n\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n(1, 32, 240, 64) (1, 32, 240, 64)       False\r\n\r\n```\r\n", "comments": ["@rmothukuru @karimnosseir @aselle \r\nDo you know what's the problem, thank you ? ", "I found that when installing tensorflow using 'pip install tensorflow==1.14.0', the problem occurs; when installing by building from source and 'pip install tensorflow-1.14.1-cp36-cp36m-linux_x86_64.whl', the problem disappear.\r\n\r\nI wonder this is a problem in tf==1.14.0 and has been fixed in tf==1.14.1 ?", "I found another question:\r\n\r\nAdding one layer to the network makes the result different between pb file and lite file...\r\n\r\n```\r\nx = tf.layers.conv2d(x, 64, (3,3), (1,1), 'same', dilation_rate=(1,1))\r\nx = tf.layers.conv2d(x, 64, (3,3), (1,1), 'same', dilation_rate=(1,1)) # Adding one convolutional layer.\r\n```", "@tensorflowbutler ", "@aselle I found that using **tensorflow==1.13.2**, the output of tensorflow with **multi-layer cnn** is the same with the output of tflite using the .lite file converted from .pb file.\r\n\r\nHowever, tensorflow==1.14.0 and tensorflow==1.14.1 both output wrong results.\r\n\r\nAnd, in tensorflow==1.13.2, the output of .pb file is not the same as that of .lite file **when using dilation, no matter whether the w and h is the same in the dilation rate**.", "The diff is loss in accuracy. For example it is passing with\r\nprint(np.allclose(target, output, 1e-5, 1e-7))\r\n\r\nWill look when this small diff started showing.", "@karimnosseir I wonder why converting from `pb` to `tflite` would introduce loss in accuracy ?\r\nAnd I found the loss is small in the first several layers, but is huge in the last several layers, and causing the model with tflie output wrong result !\r\n\r\nI think that converting `pb` to `tflite` both with `float32` data type should not introduce error or loss in accuracy.\r\n", "@jiarenyf This might be an error in the C++ code.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31359\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31359\">No</a>\n"]}, {"number": 31358, "title": "[freeze_graph]Node def expected inputs do not match with tf.keras layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Anaconda binary (```tensorflow-gpu```)\r\n- TensorFlow version (use command below): ```b'unknown' 1.13.1```\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A (Because of conda binary)\r\n- GPU model and memory: GTX 1080Ti, 11Gb\r\n\r\n**Describe the current behavior**\r\nWhen I try to freeze my TF graph that uses ```tf.keras``` layers I get the following exception.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 426, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node model/batch_normalization_v1/cond/Const_2}}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/stefan/Documents/work/asr/reproduce_bug/bug.py\", line 100, in <module>\r\n    export()\r\n  File \"/home/stefan/Documents/work/asr/reproduce_bug/bug.py\", line 95, in export\r\n    filename_tensor_name='')\r\n  File \"/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py\", line 146, in freeze_graph_with_def_protos\r\n    _ = importer.import_graph_def(input_graph_def, name=\"\")\r\n  File \"/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/stefan/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 430, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node model/batch_normalization_v1/cond/Const_2}}\r\n```\r\n\r\nIt seems to be related to the ```tf.keras.layers.BatchNormalization```. When I pass the ```training=True``` or ```training=False``` to it the error goes away.\r\n\r\n**Describe the expected behavior**\r\nThe graph should freeze no matter the ```training``` parameter.\r\n\r\n**Code to reproduce the issue**\r\nStandalone script required to reproduce.\r\n```\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.datasets import mnist\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\n\r\nclass Model:\r\n    def __init__(self):\r\n        self._conv = tf.keras.layers.Conv2D(filters=64, kernel_size=5, activation=tf.nn.relu)\r\n        self._batch_norm = tf.keras.layers.BatchNormalization()\r\n        self._flatten = tf.keras.layers.Flatten()\r\n        self._logits = tf.keras.layers.Dense(units=10)\r\n\r\n    def forward(self, inputs):\r\n        with tf.name_scope('model'):\r\n            conv_out = self._conv(inputs)\r\n            norm_out = self._batch_norm(conv_out)\r\n            flat_out = self._flatten(norm_out)\r\n            logits = self._logits(flat_out)\r\n\r\n        return tf.identity(logits, name='logits')\r\n\r\n    @staticmethod\r\n    def loss(logits, labels):\r\n        with tf.name_scope('loss'):\r\n            loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n        return loss\r\n\r\n\r\ndef train():\r\n    def map_fn(x, y):\r\n        return tf.expand_dims(tf.math.divide(tf.cast(x, tf.float32), 255), axis=2), tf.cast(y, tf.int32)\r\n\r\n    (x_train, y_train), (_, _) = mnist.load_data()\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n    dataset = dataset.map(map_fn).batch(128).repeat(10)\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    inputs, labels = iterator.get_next()\r\n\r\n    model = Model()\r\n    logits = model.forward(inputs)\r\n    loss = model.loss(logits, labels)\r\n\r\n    with tf.name_scope('optimizer'):\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=1e4).minimize(loss,\r\n                                                                       global_step=tf.train.get_or_create_global_step())\r\n\r\n    with tf.train.MonitoredTrainingSession(checkpoint_dir='./tmp') as sess:\r\n        while not sess.should_stop():\r\n            print(sess.run([optimizer, loss])[1])\r\n\r\n\r\ndef export():\r\n    tf.reset_default_graph()\r\n\r\n    export_dir = './tmp/frozen'\r\n    model = Model()\r\n\r\n    inputs = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1], name='input_placeholder')\r\n    _ = model.forward(inputs)\r\n\r\n    with open('./tmp/checkpoint') as f:\r\n        line = f.readline()\r\n        checkpoint = line[line.find('\"') + 1:line.rfind('\"')]\r\n        print(checkpoint)\r\n\r\n    with tf.Session(graph=tf.get_default_graph()) as sess:\r\n        saver = tf.train.Saver()\r\n\r\n        sess.run(tf.global_variables_initializer())\r\n        print(os.path.join('./tmp', checkpoint))\r\n        saver.restore(sess, os.path.join('./tmp', checkpoint))\r\n\r\n        builder = tf.saved_model.Builder('./tmp/frozen')\r\n        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING], strip_default_attrs=True)\r\n        builder.save()\r\n\r\n        graph = tf.graph_util.remove_training_nodes(sess.graph.as_graph_def(), protected_nodes=['logits'])\r\n\r\n        freeze_graph.freeze_graph_with_def_protos(\r\n            input_graph_def=graph,\r\n            input_saver_def=None,\r\n            input_saved_model_dir=export_dir,\r\n            saved_model_tags=[tf.saved_model.tag_constants.SERVING],\r\n            input_checkpoint=os.path.join('./tmp', checkpoint),\r\n            output_node_names='logits',\r\n            output_graph=os.path.join('./tmp', 'frozen.pb'),\r\n            clear_devices=True,\r\n            initializer_nodes='',\r\n            restore_op_name='',\r\n            filename_tensor_name='')\r\n\r\n\r\nif __name__ == '__main__':\r\n    train()\r\n    export()\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 1.13.1 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1iQnd2Psdc__CByVhjj8yBsKcMaI2Q7pw) here.Tried reproducing the issue with TF 1.14,nightly versions and 2.0 beta versions we see the different errors.\r\n\r\n", "I might be a bit late but setting `tf.keras.backend.set_learning_phase(0)` helps", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31358\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31358\">No</a>\n"]}, {"number": 31357, "title": "Android Studio using build.gradle appears Failed to find 'android-25'", "body": "**System information**\r\n- OS Platform and Distribution :Windows 10\r\n- Mobile device : Xiao Mi Mix2\r\n- TensorFlow version: 1.10.0\r\n- Python version: 3.6\r\n\r\n**Describe the problem**\r\nAndroid Studio:\r\n\r\n6:47:20 Gradle sync started\r\n6:47:32 Gradle sync failed: Failed to find target with hash string 'android-25' in: D:\\software\\SDK\r\n        Consult IDE log for more details (Help | Show Log)\r\n\r\nWhen I use the 'build.gradle', it delivers this problem\r\nSo I change buildToolVersion to 21, it delivers as the follows:\r\n\r\n* What went wrong:\r\nA problem occurred evaluating root project 'cmake'.\r\n> Plugin with id 'com.android.library' not found.\r\n\r\n* Try:\r\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.\r\n\r\nIn the past , we success build the apk(not me), but not anymore. \r\n\r\nrelative source:\r\nhttps://github.com/LGXNOTLGX/androidAudioRecg\r\n\r\nhelp me please", "comments": ["@LGXNOTLGX, Provide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "ok, i will show you relevant items:\r\n1. I run the code from the https://github.com/saturn-lab/audioPlot get the basic data\r\n2. Then next step is from https://github.com/saturn-lab/audioNet\r\n3. Final, the main problem, it is from https://github.com/saturn-lab/androidAudioRecg\r\nabove is the whole steps about our project, if you want to get the relevant data, you may run the \"1\" first.\r\n\r\nHere's where the problem went wrong \r\n![problem](https://user-images.githubusercontent.com/52983152/62666970-3a046b80-b9b8-11e9-9f61-0ff77d846701.JPG)\r\nWhen I open the Android Studio, it always report 'android-25' failed to find. \r\nI have download all of the packages from SDK manager, it stills reported\r\nso I want to change the buildToolVersion, but I can not solve it!\r\nThanks!", "I don't believe this failure is in any way related to TensorFlow. Please file an issue against the saturn-lab projects."]}, {"number": 31356, "title": "system libs cherry-picks for v1.14.1", "body": "Here are a few cherry-picks for the v1.14.1 release.\r\nThey are going into master separately in the following PRs:\r\nhttps://github.com/tensorflow/tensorflow/pull/31313 enum34, jsoncpp\r\nhttps://github.com/tensorflow/tensorflow/pull/31316 install_headers\r\nhttps://github.com/tensorflow/tensorflow/pull/31314 pkg-config\r\n\r\n", "comments": []}, {"number": 31355, "title": "Custom OPS with registering gradient functions error in eager backprop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): PIP\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: V10.0.130 / V7.0.5\r\n- GPU model and memory: NVIDIA TITAN Xp\r\n\r\n**Describe the current behavior**\r\nIf I run the code with directly\r\n```\r\n2019-08-06 11:32:33.687311: E tensorflow/stream_executor/cuda/cuda_driver.cc:1003] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-08-06 11:32:33.687356: E tensorflow/stream_executor/gpu/gpu_timer.cc:78] Invalid argument: error recording CUDA event on stream 0x7fce8a062590: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-08-06 11:32:33.687558: F ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:644] Non-OK-status: CudaLaunchKernel( ColumnReduceKernel<IN_T, T*, Op>, grid_dim, block_dim, 0, cu_stream, in, (T*)temp_storage.flat<int8_t>().data(), extent_x, extent_y, op, init) status: Internal: an illegal memory access was encountered\r\nAborted (core dumped)\r\n```\r\n\r\nIf I run the code with `cuda-memcheck --binary-patching no python train_simple.py`\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_simple.py\", line 71, in <module>\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n  File \"/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 1002, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 76, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/media/disk1/fordata/web_server/kangfu/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 137, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/media/disk4/fordata/web_server/kangfu/HDRNet-TF/hdrnet_ops.py\", line 24, in _bilateral_slice_grad\r\n    grid_tensor, guide_tensor, input_tensor, grad, has_offset=has_offset)\r\n  File \"<string>\", line 355, in bilateral_slice_apply_grad\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Failed launch BilateralSliceApplyGradKernel. [Op:BilateralSliceApplyGrad]\r\n```\r\n**Describe the expected behavior**\r\nThe code should calculate gradient properly without error. The custom ops run correctly in inferring mode, however, it throws bug during `model.fit` or below:\r\n```\r\nwith tf.GradientTape() as tape:\r\n    output = model(_full_res)\r\n    loss = tf.keras.losses.mse(output, _full_res)\r\ngrads = tape.gradient(loss, model.trainable_variables)\r\n```\r\n\r\n**Code to reproduce the issue**\r\nI define the custom ops like below:\r\n\r\n```\r\nREGISTER_KERNEL_BUILDER(Name(\"BilateralSlice\").Device(DEVICE_GPU), BilateralSliceOp);\r\nREGISTER_KERNEL_BUILDER(Name(\"BilateralSliceGrad\").Device(DEVICE_GPU), BilateralSliceGradOp);\r\nREGISTER_KERNEL_BUILDER(Name(\"BilateralSliceApply\").Device(DEVICE_GPU), BilateralSliceApplyOp);\r\nREGISTER_KERNEL_BUILDER(Name(\"BilateralSliceApplyGrad\").Device(DEVICE_GPU), BilateralSliceApplyGradOp);\r\n```\r\nIt seems like the custom ops not work OK in eager mode\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@ravikyram I commit the code used to reproduce the error in https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.\r\n\r\nTo get the error, you can:\r\n```bash\r\ngit clone https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.git\r\ncd Tensorflow-Bug-Reproduce\r\nmake\r\npython run.py\r\n```\r\nthen it outputs:\r\n```\r\n2019-08-08 17:50:50.689623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-08 17:50:51.605504: E tensorflow/stream_executor/cuda/cuda_driver.cc:1003] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-08-08 17:50:51.605560: E tensorflow/stream_executor/gpu/gpu_timer.cc:78] Invalid argument: error recording CUDA event on stream 0x7fe3505d0550: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-08-08 17:50:51.605661: F ./tensorflow/core/kernels/reduction_gpu_kernels.cu.h:605] Non-OK-status: CudaLaunchKernel(ColumnReduceMax16ColumnsKernel<IN_T, T*, Op>, grid_dim, block_dim, 0, cu_stream, in, (T*)temp_storage.flat<int8_t>().data(), extent_x, extent_y, op, init) status: Internal: an illegal memory access was encountered\r\nAborted (core dumped)\r\n```", "@MKFMIKU \r\nI am trying to reproduce the issue with your instructions.\r\nI am able to run the below two commands:\r\ngit clone https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.git\r\ncd Tensorflow-Bug-Reproduce\r\nbut make command is throwing some error\r\n![image](https://user-images.githubusercontent.com/51902062/62756296-cbe1a680-ba94-11e9-89e4-d83ab28fb658.png)\r\nThanks!", "@ravikyram Seems like you don't have `nvcc` in your environment. The `nvidia-cuda-toolkit` should be installed before run `make`. ", "@MKFMIKU,\r\nIs this still an issue? Could you please upgrade to TensorFlow v2.3 and check if you are still facing the error?\r\n\r\n>git clone https://github.com/MKFMIKU/Tensorflow-Bug-Reproduce.git\r\n>cd Tensorflow-Bug-Reproduce\r\n>make\r\n\r\nAlso while trying to reproduce the issue, on running the `make` step I am facing an error stating `Makefile:28: recipe for target 'build/bilateral_slice.cu.o' failed`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/cf1d67a0b1595ce316946c04e93aa6d0/31355.ipynb#scrollTo=4iyuE954ERri). Thanks!", "It is solved after upgrade", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31355\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31355\">No</a>\n"]}, {"number": 31354, "title": "Tensorflow library query API", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\nMac / Windows / Linux - x86 + Aarch64 \r\n\r\n- TensorFlow version (you are using):\r\n1.11.0 and upwards\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nIf I can help I will gladly\r\n\r\n**Describe the feature and the current behavior/state.**\r\nPresently tensorflow_framework and it's friends (e.g. the _jni version etc) use cpu_feature_guard / cpu_info to check if your CPU supports the instruction set the build was performed with and complains if you're missing features / extra features.\r\n\r\nThere should be a way to programmatically query the library and CPU (+ GPU???) to discover what feature set the library was compiled with etc.\r\n\r\nPresently there are a couple of un-exposed methods that do some of the desired functionality...\r\n\r\nInfoAboutUnusedCPUFeatures and TestCPUFeature are examples of useful features that could be exposed via a c_api extension\r\n\r\nWhat I propose is that the library expose the features used to compile it and the features the host is capable of supporting with various overlapping calls allowing the caller to identify the following cases...\r\n\r\n1) The library matches the host perfectly (very rare)\r\n2) The library includes features not supported by the host\r\n3) The library works on the host but does not use all available features (a-la feature-guard)\r\n \r\n(2) + (3) should return a set of unsupported / extra features\r\n\r\n**Will this change the current api? How?**\r\nNo, it can be implemented as an extra C API\r\n\r\n**Who will benefit with this feature?**\r\nPotentially any user who experiences a crash from unsupported features or could reasonably expect better performance from a rebuild\r\n\r\nMy personal experience showed a 100% improvement in speed on my Mac and Linux machines using CPU-only builds. On GPU builds the change is, of course, trivial\r\n\r\nAs Tensorflow is increasingly becoming a hobbyist interest - \"Build a brain out of a Raspberry Pi\" fluff pieces it seems advisable to cater for the non academic / commercial user\r\n\r\n**Any Other info.**\r\nPotentially this feature would allow external API developers to check for host issues before starting rather than, at worst, dump the core (see this happen several times)", "comments": ["@peardox We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions.refer **[link](https://www.tensorflow.org/api_docs)** Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31354\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31354\">No</a>\n"]}, {"number": 31353, "title": "avoid running condition at every iteration", "body": "This PR is to avoid checking the condition at every iteration (but, there is a duplicate block, if it's worth doing it, I'll come up with a solution fixing the duplicate part later.)", "comments": ["What's the motivation behind this PR? I feel the code prior to the change was significantly more readable.", "Thanks @gargn, I'll close this. "]}, {"number": 31352, "title": "(Windows) ptxas_utils.cc(54): 'tensorflow::SubProcess' unimplemented class?", "body": "tags: 2.0.0-beta0 subtype:windows type:build/install\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device: -\r\n- TensorFlow installed from: Source\r\n- TensorFlow version: r2.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: NO, pip is installed, NO\r\n- Bazel version: 26.0\r\n- GCC/Compiler version: -\r\n- CUDA/cuDNN version: 10.1 / 7.6.2\r\n- GPU model and memory: GTX 1060 3GB\r\n- CPU: i7-860 (AVX unsupported) (64-bit)\r\n\r\n**Describe the problem**\r\nI know CUDA 10.1/VS2019 is unsupported for 2.0 currently, but the error may have nothing to do with that? Tensorflow's SubProcess class is not implemented for Windows? Is this a necessity, how is Tensorflow-gpu else compiled for Windows?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`python ./configure`\r\n```\r\nbazel build --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nC:/users/admin/documents/git-tf/tensorflow/tensorflow/stream_executor/cuda/BUILD:97:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:ptxas_utils' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/admin/_bazel_admin/52rkf7yz/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\Admin\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\Admin\\AppData\\Local\\Temp\r\n  C:/Program Files/Python36/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-py2-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-py2-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-py2-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-py2-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-py2-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-py2-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-py2-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-py2-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Iexternal/nsync/public /Ibazel-out/x64_windows-py2-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-py2-opt/bin/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-py2-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-py2-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-py2-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-py2-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-py2-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-py2-opt/bin/external/local_config_cuda/cuda/cuda/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI -nvcc_options=disable-warnings /Fobazel-out/x64_windows-py2-opt/bin/tensorflow/stream_executor/cuda/_objs/ptxas_utils/ptxas_utils.o /c tensorflow/stream_executor/cuda/ptxas_utils.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(54): error C2039: 'SetProgram': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(55): error C2039: 'SetChannelAction': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(56): error C2039: 'Start': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(62): error C2039: 'Communicate': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(192): error C2039: 'SetProgram': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(193): error C2039: 'SetChannelAction': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(195): error C2039: 'Start': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\ntensorflow/stream_executor/cuda/ptxas_utils.cc(199): error C2039: 'Communicate': is not a member of 'tensorflow::SubProcess'\r\n.\\tensorflow/core/platform/windows/subprocess.h(27): note: see declaration of 'tensorflow::SubProcess'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1863.507s, Critical Path: 326.27s\r\nINFO: 2580 processes: 2580 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow](https://www.tensorflow.org/install/source_windows) website .Please, let us know. Thanks!", "Yup I followed the instructions, the only thing is that I dont run with --config=opt since I want to compile tensorflow without using the AVX instruction (since i7-860 doesn't support AVX instructions) ", "@ravikyram, please assign this issue to a TensorFlow team member. I won't have time to look into this recently. Thanks!", "@andrewharp ?", "bump", "@TheNewSound Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "@TheNewSound,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "I can't test it on Windows at the moment, but will give it a try this weekend.\r\n\r\nHowever, in general, Tensorflow-gpu is (maybe _was_) a pain to compile, both on Windows and Linux. On Linux it's the NVIDIA drivers that need older compiler versions (using Ubuntu 19.10) and on Windows the issue is closer to Tensorflow because of the VS2017 build tools instead of the latest VS2019 (but I see this has changed now).\r\n\r\n", "> I can't test it on Windows at the moment, but will give it a try this weekend.\r\n\r\n@TheNewSound,\r\nAny updates regarding this? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31352\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31352\">No</a>\n"]}, {"number": 31351, "title": "tf.gradients with \"ValueError: Cannot create a tensor proto whose content is larger than 2GB.\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary(pip tensorflow-gpu)\r\n- TensorFlow version (use command below): 1.14.0 (v1.14.0-rc1-22-gaf24dc91b5)\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): Noe\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: 10.1/7.6.1\r\n- GPU model and memory: 1080ti (12GB)\r\n\r\n**Describe the current behavior**\r\nHi, \r\nI tried to use tf.gradeints to calculate `integrated-gradients` which is one of the Explainability Methods. To do that, I have to treat ~30,000 x ~30,000 matrix(pred) like the following code. My task is Knowledge-Graph.\r\n\r\n```python\r\npred = sess.run(model, feed_dict=feed_dict)\r\ntf_grads = tf.gradients(pred, placeholders)\r\n```\r\n\r\nHowever, I encountered `ValueError: Cannot create a tensor proto whose content is larger than 2GB.`. Is this bug? or Are there some methods to avoid this issue?\r\n\r\nThank you in advance,\r\n\r\nref) https://github.com/ankurtaly/Integrated-Gradients\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\n```\r\nFile \"/home/ono/ws/GraphCNN_kg/gcn_modules/visualization.py\", line 302, in cal_feature_IG_for_kg                                                                                                                                                                                                                                                                        \r\n    visualizer.cal_integrated_gradients(sess, placeholders, out_prediction, divide_number)                                                                                                                                                                                                                                                                                \r\n  File \"/home/ono/ws/GraphCNN_kg/gcn_modules/visualization.py\", line 264, in cal_integrated_gradients                                                                                                                                                                                                                                                                     \r\n    out = tf.reduce_sum(prediction)                                                                                                                                                                                                                                                                                                                                       \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func                                                                                                                                                                                                                                            \r\n    return func(*args, **kwargs)                                                                                                                                                                                                                                                                                                                                          \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1410, in reduce_sum_v1                                                                                                                                                                                                                                          \r\n    return reduce_sum(input_tensor, axis, keepdims, name)                                                                                                                                                                                                                                                                                                                 \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper                                                                                                                                                                                                                                                \r\n    return target(*args, **kwargs)                                                                                                                                                                                                                                                                                                                                        \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1457, in reduce_sum                                                                                                                                                                                                                                             \r\n    input_tensor, _ReductionDims(input_tensor, axis), keepdims,                                                                                                                                                                                                                                                                                                           \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1345, in _ReductionDims                                                                                                                                                                                                                                         \r\n    return range(0, array_ops.rank(x))                                                                                                                                                                                                                                                                                                                                    \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper                                                                                                                                                                                                                                                \r\n    return target(*args, **kwargs)                                                                                                                                                                                                                                                                                                                                        \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 486, in rank                                                                                                                                                                                                                                                   \r\n    return rank_internal(input, name, optimize=True)                                                                                                                                                                                                                                                                                                                      \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 506, in rank_internal                                                                                                                                                                                                                                          \r\n    input_tensor = ops.convert_to_tensor(input)                                                                                                                                                                                                                                                                                                                           \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1087, in convert_to_tensor                                                                                                                                                                                                                                     \r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)                                                                                                                                                                                                                                                                                                      \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1145, in convert_to_tensor_v2                                                                                                                                                                                                                                  \r\n    as_ref=False)                                                                                                                                                                                                                                                                                                                                                         \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1224, in internal_convert_to_tensor                                                                                                                                                                                                                            \r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)                                                                                                                                                                                                                                                                                                   \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 305, in _constant_tensor_conversion_function                                                                                                                                                                                                           \r\n    return constant(v, dtype=dtype, name=name)                                                                                                                                                                                                                                                                                                                            \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 246, in constant                                                                                                                                                                                                                                       \r\n    allow_broadcast=True)                                                                                                                                                                                                                                                                                                                                                 \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 284, in _constant_impl                                                                                                                                                                                                                                 \r\n    allow_broadcast=allow_broadcast))                                                                                                                                                                                                                                                                                                                                     \r\n  File \"/home/ono/.miniconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 537, in make_tensor_proto                                                                                                                                                                                                                              \r\n    \"Cannot create a tensor proto whose content is larger than 2GB.\")                                                                                                                                                                                                                                                                                                     \r\nValueError: Cannot create a tensor proto whose content is larger than 2GB. \r\n```\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@0h-n0, Will it be possible to provide the minimal code to reproduce the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 31350, "title": " ModuleNotFoundError: No module named 'tensorflow.contrib'", "body": "Have spent a couple days troubleshooting.\r\nAm getting the following error\r\n\r\n> ModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nThe offending line is\r\n\r\n> import tensorflow.contrib.tensorrt as trt\r\n\r\nHere are my setup specs\r\n\r\n> Windows 10\r\n> \r\n> Python 3.6.8\r\n> \r\n> CUDA 10.0\r\n> \r\n> cuDNN v 7.6.2\r\n> \r\n> Tensorflow (gpu) 1.14.0\r\n> \r\n> GeForce GTX 960M\r\n> \r\n> Driver version 431.60\r\n> \r\n> Intel Core i7-6700HQ 2.6 GHz*\r\n> \r\n\r\nAny feedback or troubleshooting steps appreciated!\r\n", "comments": ["\"In TF 1.14, TF-TRT was moved to the core from contrib.\r\n\r\nYou need to import it like this: `from tensorflow.python.compiler.tensorrt import trt_convert as trt`\r\n\r\nhttps://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/image-classification/image_classification.py#L22 \"\r\n\r\nfrom [StackOverflow](https://stackoverflow.com/a/56550206)", "Thanks. Actually tried that already but was then getting another error \r\n\"ModuleNotFoundError: No module named 'tensorflow.python.compiler.tensorrt\"", "Is it possible tensorflow is in correctly installed? When I do a $pip list I get \r\n\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntensorflow-gpu       2.0.0b1", "I get almost identical error if instead of cmd I run  using Anaconda prompt with Python 3.7.3 \"No module named 'tensorflow.python.compiler.\"", "Also tried manually installing the TensorRt module from Nvidia but not clear how to properly install in Windows.", "`tf.contrib` doesn't exist in 2.0.", "Thanks. Then how do I access the tensorrt module?\r\nOr at least confirm it is installad?", "@jBachalo ,\r\nCan you please go through this [link](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html)?Thanks.", "Thanks. Great info.\n\nOn Wed, Aug 7, 2019 at 2:58 AM anush-o <notifications@github.com> wrote:\n\n> @jBachalo <https://github.com/jBachalo> ,\n> Can you please go through this link\n> <https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html>\n> ?Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31350?email_source=notifications&email_token=AAB4P2CCQF42TLOZFPJOAKLQDJXCNA5CNFSM4IJP57UKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3XMQEQ#issuecomment-518965266>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAB4P2CPKA2DEEQWGBARFLTQDJXCNANCNFSM4IJP57UA>\n> .\n>\n\n\n-- \n[c] 416.306.0036\n[v] vimeo.com/bachalo\n[t] @eco_bach\n--------------------------------------------\n\"...all improvisation is life in search of a style.\"\n            - Bruce Mau,'LifeStyle'\n", "Let us know if the link provided by @anush-o solved the issue so we can close.", "No\r\nIssue still exists though am able to get code running by commenting out all tensorrt import statements.", "I have same issue too! Is there somebody who solved this? ", "@jBachalo Did you try `from tensorflow.python.compiler import tensorrt as trt `? I have checked it in colab with `TF1.14.0` and `TF2.0b1`. Thanks!", "I have tried as you said, but it did not work.\r\n`>>> from tensorflow.python.compiler import tensorrt as trt`\r\n````\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name 'tensorrt'\r\n````\r\n\r\nI have checked every possible package directory that can have tensorrt folder, but there are not tensort folder. I removed and reinstalled quite several time and changed to another versions too, but did not work. ", "@I-Love-IU Can you try running this [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/6cd003898b725ae4385d82c7dbab5b62/tf_31350.ipynb#scrollTo=LBWAGqcXQpHt). It works for me without any error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31350\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31350\">No</a>\n", "I am trying to \"net=tflearn.input_data(shape=[None,len(training[0])])\" but as I could not import tflearn so I used \"from tensorflow.python.compiler import tensorrt as trt\" to import it but still I am not able to use the input-data function .\r\nplease help", "@rohansethi1999430 Please open a new issue with plaform details and a standalone code to reproduce the issue. Thanks!", "import tensorflow.contrib.eager as tfe\r\nget the following error:\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\nhow can I solve this problem?", "@missleepless can you please check TF version you have installed? I think it is TF2.0. There is no `contrib` module in TF2.0. If you want `contrib` then install `TF1.15.0rc3`. If you have TF2.0, then you could find some of the contrib modules in the `addons` repo under `tensorflow repository`.  Thanks!", "Hi , I tried with TF 2.0 , 1.15 ,1.14 \r\n\r\nThe below import statement does not work \r\n\r\n`import tensorflow.contrib.tensorrt as trt`\r\n\r\nI am getting the below error :\r\n\r\n import tensorflow.contrib.tensorrt as trt\r\nModuleNotFoundError: No module named 'tensorflow.contrib.tensorrt'\r\n\r\n", "> Hi , I tried with TF 2.0 , 1.15 ,1.14\r\n> \r\n> The below import statement does not work\r\n> \r\n> `import tensorflow.contrib.tensorrt as trt`\r\n> \r\n> I am getting the below error :\r\n> \r\n> import tensorflow.contrib.tensorrt as trt\r\n> ModuleNotFoundError: No module named 'tensorflow.contrib.tensorrt'\r\n\r\n```\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n```\r\n\r\nworked for me", "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets    \r\n\r\ntoo much Import issues", "> > Hi , I tried with TF 2.0 , 1.15 ,1.14\r\n> > The below import statement does not work\r\n> > `import tensorflow.contrib.tensorrt as trt`\r\n> > I am getting the below error :\r\n> > import tensorflow.contrib.tensorrt as trt\r\n> > ModuleNotFoundError: No module named 'tensorflow.contrib.tensorrt'\r\n> \r\n> ```\r\n> from tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n> ```\r\n> \r\n> worked for me\r\n\r\n\r\nHey, Hi! Thats good to hear. May I know which TF version did this work in ?", "This issue is closed, if you have new issues please open new issues and fill in the template. Closed issues are rarely looked at again after they are closed.", "> oesn't exist in 2.0.\r\n\r\nhow to solve it.thank you", "```\r\n## https://github.com/tensorflow/tensorflow/issues/31350\r\n## ... tensorflow.contrib doesn't exist in TensorFlow 2.0\r\n\r\n# from tensorflow.contrib.tensorboard.plugins.projector import (\r\nfrom tensorboard.plugins.projector import ...\r\n...\r\n```\r\n\r\nE.g.:\r\n\r\n```\r\n(py3.7) [victoria@victoria spacy]$ P\r\nPython 3.7.4 (default, Nov 20 2019, 11:36:53) \r\n[GCC 9.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n>>> from tensorflow.contrib.tensorboard.plugins.projector import *\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\n>>> from tensorboard.plugins.projector import (\r\n...     visualize_embeddings,\r\n...     ProjectorConfig,\r\n... )\r\n>>>\r\n```", "\r\n  File \"C:\\Users\\USER\\Desktop\\ph.d work\\PROTEIN STRUCTURE PREDICTION\\FINAL\\rgn-master\\model\\model.py\", line 18, in <module>\r\n    import tensorflow.contrib.layers as layers\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib'", "> File \"C:\\Users\\USER\\Desktop\\ph.d work\\PROTEIN STRUCTURE PREDICTION\\FINAL\\rgn-master\\model\\model.py\", line 18, in\r\n> import tensorflow.contrib.layers as layers\r\n> \r\n> ModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nso how do you solve this problem?", "This is not a big issue its just change of tenserflow version you are using \r\njust Uninstall the installed the current version and install 1.8.0\r\nBecause in latest realese tensorflow does not contain the package called Contrib", "Can u please tell the code to install tensorflow 1.18.\n\n\nOn Mon, Dec 16, 2019, 4:37 PM Divanshu Tak <notifications@github.com> wrote:\n\n> This is not a big issue its just change of tenserflow version you are using\n> just Uninstall the installed the current version and install 1.8.0\n> Because in latest realese tensorflow does not contain the package called\n> Contrib\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31350?email_source=notifications&email_token=AN4XXYBORMZMGCTULZXDHLTQY5OP5A5CNFSM4IJP57UKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG6LGOI#issuecomment-566014777>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN4XXYDW6OP5SQOZABX3T63QY5OP5ANCNFSM4IJP57UA>\n> .\n>\n", "Guys. `tf.contrib` no longer exists in post-2.0 world. It has been moved (partially) to TF Addons.\r\n\r\nIf you really want `tf.contrib`, you can use TF 1.15 for the next 3 years, but after that you will have to switch as there won't be a TF 1.16 or later", "> Have spent a couple days troubleshooting.\r\n> Am getting the following error\r\n> \r\n> > ModuleNotFoundError: No module named 'tensorflow.contrib'\r\n> \r\n> The offending line is\r\n> \r\n> > import tensorflow.contrib.tensorrt as trt\r\n> \r\n> Here are my setup specs\r\n> \r\n> > Windows 10\r\n> > Python 3.6.8\r\n> > CUDA 10.0\r\n> > cuDNN v 7.6.2\r\n> > Tensorflow (gpu) 1.14.0\r\n> > GeForce GTX 960M\r\n> > Driver version 431.60\r\n> > Intel Core i7-6700HQ 2.6 GHz*\r\n> \r\n> Any feedback or troubleshooting steps appreciated!\r\n\r\nTry with \r\n**import tensorflow.keras as keras**\r\n", "Locking conversation as resolved: `tf.contrib` only exists before TF2.0. If you get the error in the title please run `pip list` and uninstall all tensorflow ecosystem packages that are not in sync (either all should be 1.15 or all should be after 2.0)"]}, {"number": 31349, "title": "Adding custom op in 1.14", "body": "I noticed there was a same topic #30632. The starter of the issue switches to tf 1.12 and solves the problem. But the problem is still not resolved. When I follow the instruction and try to compile the cuda kernel, I change the cuda_kernel_helper.h to gpu_kernel_helper.h and run into the same problem.\r\n\r\npython2.7/site-packages/tensorflow/include/tensorflow/core/util/gpu_kernel_helper.h:22:10: fatal error: third_party/gpus/cuda/include/cuda_fp16.h: No such file or directory\r\n #include \"third_party/gpus/cuda/include/cuda_fp16.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nCan someone figure out a reason behind this error? I pip installed tensorflow. Should I build from source?\r\n\r\n", "comments": ["I think you can edit the code from `#include \"third_party/gpus/cuda/include/cuda_fp16.h\"` into `#include \"cuda_fp16.h\"`. This solved my problem", "@MKFMIKU Thanks, it works. But, still, we should have an update in the tutorial. ", "@BMG-JTIAN \r\nCan you please let us know which tutorial you are referring to.Also, please let us know what platform you are using (operating system, architecture) .", "@BMG-JTIAN I ran into the same issue. This commit on custom-op should fix the issue: https://github.com/tensorflow/custom-op/commit/b267cc546365f30963ef2f6b690ff0a4e7391913. Notice the `cuda_header_library` has the prefix `third_party/gpus` which is how the file gets discovered", "@BMG-JTIAN Could you check whether it was resolved or not? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31349\">No</a>\n", "Hello,\r\n\r\nHas the proposed solution been implemented? If not, i would like to send a pull request. :+1: \r\n", "I met the same problem. As @MKFMIKU suggested, to change `#include \"third_party/gpus/cuda/include/cuda_fp16.h\"` to `#include \"cuda_fp16.h\"` . However, this would ONLY solve the first issue, in this same file `/site-packages/tensorflow_core/include/tensorflow/core/util/gpu_kernel_helper.h`, it has included `#include \"tensorflow/core/util/gpu_device_functions.h\"`, and in this file, it would still include other files from `third_party/gpus/cuda/include`, e.g.  `third_party/gpus/cuda/include/cuComplex.h`, so every time there is a include statement to include files from this path \"`third_party/gpus/cuda/include`\", you will encounter this `No such file or directory` error.", "> @BMG-JTIAN I ran into the same issue. This commit on custom-op should fix the issue: [tensorflow/custom-op@b267cc5](https://github.com/tensorflow/custom-op/commit/b267cc546365f30963ef2f6b690ff0a4e7391913). Notice the `cuda_header_library` has the prefix `third_party/gpus` which is how the file gets discovered\r\n\r\nSir, so you are referring to the method using bazel to build the custom gpu op? Is there a way to just use `nvcc` command?", "> I met the same problem. As @MKFMIKU suggested, to change `#include \"third_party/gpus/cuda/include/cuda_fp16.h\"` to `#include \"cuda_fp16.h\"` . However, this would ONLY solve the first issue, in this same file `/site-packages/tensorflow_core/include/tensorflow/core/util/gpu_kernel_helper.h`, it has included `#include \"tensorflow/core/util/gpu_device_functions.h\"`, and in this file, it would still include other files from `third_party/gpus/cuda/include`, e.g. `third_party/gpus/cuda/include/cuComplex.h`, so every time there is a include statement to include files from this path \"`third_party/gpus/cuda/include`\", you will encounter this `No such file or directory` error.\r\n\r\nI met the same condition to you with an enviroment of TF1.15+CUDA10.0. Have you found a solition?\r\n@ravikyram @jvishnuvardhan Is there any official way to solve it? \r\nThank you in advance.", "try link cuda headers to tensorflow path, for example:\r\n\r\n```\r\nmkdir -p /usr/local/lib/python3.8/dist-packages/tensorflow/include/third_party/gpus\r\nln -s /usr/local/cuda/include /usr/local/lib/python3.8/dist-packages/tensorflow/include/third_party/gpus/\r\n```\r\n\r\n\r\n> > I met the same problem. As @MKFMIKU suggested, to change `#include \"third_party/gpus/cuda/include/cuda_fp16.h\"` to `#include \"cuda_fp16.h\"` . However, this would ONLY solve the first issue, in this same file `/site-packages/tensorflow_core/include/tensorflow/core/util/gpu_kernel_helper.h`, it has included `#include \"tensorflow/core/util/gpu_device_functions.h\"`, and in this file, it would still include other files from `third_party/gpus/cuda/include`, e.g. `third_party/gpus/cuda/include/cuComplex.h`, so every time there is a include statement to include files from this path \"`third_party/gpus/cuda/include`\", you will encounter this `No such file or directory` error.\r\n> \r\n> I met the same condition to you with an enviroment of TF1.15+CUDA10.0. Have you found a solition? @ravikyram @jvishnuvardhan Is there any official way to solve it? Thank you in advance.\r\n\r\n"]}, {"number": 31348, "title": "Create mnist example with embedding projector", "body": "As specified in Tensorboard issue [#86](https://github.com/tensorflow/tensorboard/issues/86), the Embedding Projector does not have a code example. This pull request has a code example of Embedding usage based on [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py) and [TF Dev Summit 2017 TensorBoard tutorial](https://github.com/martinwicke/tf-dev-summit-tensorboard-tutorial/blob/master/mnist.py).", "comments": ["Thanks for making the new PR. \r\n\r\nLet's close this one."]}, {"number": 31347, "title": "Jvishnuvardhan patch 2", "body": "", "comments": []}, {"number": 31346, "title": "Try using Bazel 0.26.1", "body": "", "comments": []}, {"number": 31345, "title": "Add Ubuntu16.04 toolchain", "body": "Trying to build with the new build definitions in parallel with the old ones and seeing which one succeeds first", "comments": []}, {"number": 31344, "title": "Refactor {Batch&Map}DatasetOpTest", "body": "This PR refactors `BatchDatasetOpTest` and `MapDatasetOpTest` to make the unit tests more modular.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa Another style of tests for `BatchDataset` is submitted here(https://github.com/tensorflow/tensorflow/pull/31344/commits/86aea7b0d3843f60de00b2c14c5309efb77cf7) to reduce the repeated code. \r\n\r\nThe idea is that \r\n1) Move`dataset_kernel`, `dataset_context`, `dataset`, `iterator_context`, and `iterator` to `BatchDatasetOpTest` as the private member variables; \r\n\r\n2) Add the following function  to make `dataset_kernel`, `dataset_context`, `dataset`, `iterator_context`, `iterator` so that the boilerplate code can be avoid in each unit test.\r\n```C++\r\nStatus MakeDatasetAndIterator(BatchDatasetParams batch_dataset_params, bool create_iterator)\r\n```\r\n\r\n3) The code in every test can be simplied as below:\r\n```C++\r\nTEST_P(ParameterizedGetNextTest, GetNext) {\r\n  auto test_case = GetParam();\r\n  TF_ASSERT_OK(SetupTestEnv(/*thread_num=*/2, /*cpu_num=*/2, {}));\r\n  TF_ASSERT_OK(MakeDatasetAndIterator(test_case.dataset_params, true));\r\n  TF_ASSERT_OK(\r\n      CheckIteratorGetNext(test_case.expected_outputs, /*compare_order=*/true));\r\n}\r\n```\r\n\r\n4) Generate the test cases as a vector instead of one by one:\r\n```C++\r\nstd::vector<CardinalityTestCase<BatchDatasetParams>> CardinalityTestCases() {\r\n  return {{/*dataset_params=*/BatchDataset1(), /*expected_cardinality=*/3},\r\n          {/*dataset_params=*/BatchDataset2(), /*expected_cardinality=*/3},\r\n          {/*dataset_params=*/BatchDataset3(), /*expected_cardinality=*/4},\r\n          {/*dataset_params=*/BatchDataset4(), /*expected_cardinality=*/3},\r\n          {/*dataset_params=*/BatchDataset5(), /*expected_cardinality=*/0},\r\n          {/*dataset_params=*/BatchDataset6(), /*expected_cardinality=*/1},\r\n          {/*dataset_params=*/BatchDataset7(), /*expected_cardinality=*/0}};\r\n}\r\n\r\nINSTANTIATE_TEST_SUITE_P(\r\n    BatchDatasetOpTest, ParameterizedCardinalityTest,\r\n    ::testing::ValuesIn(std::vector<CardinalityTestCase<BatchDatasetParams>>(\r\n        CardinalityTestCases())));\r\n```", "> Could you please comment on what is it that you would like me to review? I have not been following your changes and am confused why are there two versions of the batch test.\r\n\r\n@jsimsa Could you please only review the latest commit (https://github.com/tensorflow/tensorflow/commit/86aea7b0d3843f60de00b2c14c5309efb77cf7)? This commit adds another version of the batch test. Once the solution is finalized, I will merge it into the original batch test. \r\n\r\nThe reason that I create another version is to make the code clean for review, as there are big changes in the code. Once the review finishes, it will merged to the original batch test. If it is not needed, I can delete the second version and add the changes to the original batch test. ", "@jsimsa Thanks for the quick review! The comments are addressed here(https://github.com/tensorflow/tensorflow/pull/31344/commits/bf730e7a10fe1f45e4ceac431b801a0404256683). Could you please take a look at the changes when you have time?", "@jsimsa `DatasetOpsTestBaseV2` is added and {Range, Batch, Map}DatasetOpTests have been updated accordingly. Please take another look (https://github.com/tensorflow/tensorflow/pull/31344/commits/d214efe549e7dd5ed3ff2a770dd923d84a6b4004)!", "@jsimsa This PR is rebased to resolve the conflicts with this commit (https://github.com/tensorflow/tensorflow/commit/6d8f05acd72df61e5f4e5b4c72837b7caed3e942) by removing `Save()` and `IsStateful()`-related checks in the test base class. Could you please re-approve this PR?", "Internal tests fail with:\r\n\r\n```\r\nI0807 23:15:47.460148    4166 cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA\r\nF0807 23:15:47.663839    4166 refcount.h:90] Check failed: ref_.load() == 0 (2 vs. 0) \r\n*** Check failure stack trace: ***\r\n    @     0x7f3f932e5642  absl::logging_internal::LogMessage::DieIfFatal()\r\n    @     0x7f3f932e428c  absl::logging_internal::LogMessage::Flush()\r\n    @     0x7f3f932e7049  absl::logging_internal::LogMessageFatal::~LogMessageFatal()\r\n    @     0x560844985ab8  tensorflow::data::BatchDatasetOp::Dataset::~Dataset()\r\n    @     0x560844985ace  tensorflow::data::BatchDatasetOp::Dataset::~Dataset()\r\n    @     0x7f3f9be0a4d8  tensorflow::data::DatasetOpsTestBase::~DatasetOpsTestBase()\r\n    @     0x7f3f9be09b6e  tensorflow::data::(anonymous namespace)::BatchDatasetOpTest_DatasetNodeName_Test::~BatchDatasetOpTest_DatasetNodeName_Test()\r\n    @     0x7f3f9b890c50  testing::TestInfo::Run()\r\n    @     0x7f3f9b8915c7  testing::TestSuite::Run()\r\n    @     0x7f3f9b8a0f57  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x7f3f9b8a0555  testing::UnitTest::Run()\r\n    @     0x7f3f9bab830e  main\r\n    @     0x7f3f92975bbd  __libc_start_main\r\n    @     0x560844984c49  ../sysdeps/x86_64/start.S:108 _start\r\n```\r\n\r\nwhich suggests you are leaking references someplace.", "@jsimsa This issue is caused by [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/dataset_test_base.cc#L363) which increases the reference but `Unref()` is missing.\r\n\r\nAs `DatasetBase` has its own reference count, `std::unique_ptr<DatasetBase> dataset_;` is changed to `DatasetBase* dataset_ = nullptr;` and add `dataset_->Unref();` to the deconstructor of the dataset test base. Could you please have a look at the change (https://github.com/tensorflow/tensorflow/pull/31344/commits/ece48329c7915b6ace6424fbf757f47657c64553)? "]}, {"number": 31343, "title": "Behavior of tf.data.Dataset when `steps_per_epoch` is set", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n[tf.data.dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)\r\n[model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\r\n\r\n## Description of issue (what needs changing):\r\nMy `tf.data.Dataset` does not have `repeat` set which means it should go forever. At the end of `steps_per_epoch`, does the `tf.data.Dataset` shuffle itself? Or does it pick up from where it left off? Or does it reset? \r\n\r\nI couldn't find a clear explanation online from the googling I did. My dataset is about 14 million examples, and the loss seems to be decreasing between epochs (with `steps_per_epoch` set). I'm just worried that it's fitting on the same X samples again and again\r\n\r\nIt's not entirely clear to me what is happening in the background with `fit`", "comments": ["Hi,\r\n\r\nIn this case, the dataset will indeed be consumed sequentially from epoch to epoch without going back through it. You can notably see in the `fit` method's docstring that the `shuffle` argument (triggering shuffling of the data between epochs) is inactive when `steps_per_epoch` is set to something else than `None`.\r\n\r\nA simple way to see it is to set a fake model and data with deterministic loss outputs and see whether you get the expected loss values ; _e.g._:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Set up a pseudo model with no trainable weights.\r\ninputs = tf.keras.Input(tuple(), dtype=tf.float32)\r\nmodel = tf.keras.Model(inputs, inputs + 1)\r\nmodel.compile('adam', 'mse')  # the optimizer will actually not fit anything\r\n\r\n# Set up a mock dataset.\r\ndata = tf.data.Dataset.from_tensor_slices(\r\n    (tf.range(50, dtype=tf.float32), tf.zeros(50, dtype=tf.float32))\r\n)\r\n\r\n# \"Fit\" the model - basically, compute the mean sum of squared batched inputs.\r\nmodel.fit(data.batch(1), epochs=5, steps_per_epoch=10)\r\n# You should actually get a warning about the dataset not being shuffled.\r\n# \"Losses\" at each epoch are 38.5, 248.5, 658.5, 1268.5 and 2078.5\r\n\r\n# Compute the expected losses, see that they are the same.\r\n[np.square(np.arange(i, i + 10) + 1).mean() for i in range(0, 50, 5)]\r\n```", "Thank you very much! ", "You are welcome :-)"]}, {"number": 31342, "title": "Rebase r2.0 branch to Aug 3rd to prepare for TF 2.0.0 release", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31342) for more info**.\n\n<!-- need_author_consent -->", "Is this after your rebase or before?"]}, {"number": 31341, "title": "Cherry-pick needed change", "body": "#31336 brought usages of `REGISTER_INPUT_COLOCATION_EXEMPTION` into the `r1.14` branch but we're missing  c7a136634925d2057c38e4b5834c451dd2d1ac17 which defines the macro. Hence, cherry-picking it now so that builds succeed.\r\n\r\nCherry-picked commit is:\r\n```\r\nCreating a registry for ops that should be excluded from input resource collocation constraints and making use of it in both graph-mode and eager-mode.\r\n\r\nPiperOrigin-RevId: 250518974\r\n```", "comments": []}]