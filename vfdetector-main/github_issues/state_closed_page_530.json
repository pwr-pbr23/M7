[{"number": 37831, "title": "port TF Lite micro speech example to CEVA-DSP BX1", "body": "This commit is a port of TF Lite Micro Speech example to CEVA-DSP BX1 processor.\r\nThe commit adds project settings for CEVA's IDE as well as some code changes and additions, and demo input file.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37831) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@idog-ceva Thank you for your contribution. Can you please sign CLA? Thanks!", "@googlebot I signed it!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37831) for more info**.\n\n<!-- ok -->", "@idog-ceva Can you please check @petewarden's comments and keep us posted ? Thanks!", "> @idog-ceva Can you please check @petewarden's comments and keep us posted ? Thanks!\r\n\r\n@gbaned Hi, I'll take this over on CEVA's side and deal with the issues @petewarden raised in the next few days, thanks :)", "@yair-ehrenwald, @idog-ceva Any update on this PR? Please. Thanks!", "@yair-ehrenwald, @idog-ceva Any update on this PR? Please. Thanks!", "@yair-ehrenwald, @idog-ceva  Any update on this PR? and please resolve conflicts. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "> I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n\r\nOk. I opened a new PR for this a couple of weeks ago:\r\nhttps://github.com/tensorflow/tensorflow/pull/42414\r\nthis one should have been closed, sorry about that."]}, {"number": 37830, "title": "Replace VarHandleOp in Keras with VariableV2", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: https://www.tensorflow.org/api_docs/python/tf/Variable\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing): \r\n\r\n### Clear description\r\n\r\nTF 1.13.1\r\nI can't find anything detail about differences between VarHandlOp and VariableV2. \r\nAs far as I know it seems that VarHandleOp implement in Keras and VariableV2 in TF 1.X. \r\nHow can I convert VarHandleOp to VariableV2 in tf.keras?\r\nI'm using some model processing tool that can only run under VariableV2 ops.\r\n\r\n### Correct links\r\n\r\nhttps://git.codingcafe.org/Mirrors/tensorflow/tensorflow/commit/e4a5c5356063d7f7b324a5771fe296bb199b532c\r\nSomelink above is all I could find.\r\n\r\n### Parameters defined\r\n\r\n---\r\n\r\n### Returns defined\r\n\r\n---\r\n\r\n### Raises listed and defined\r\n\r\n---\r\n\r\n### Usage example\r\n\r\nIs this a usage example?\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/VariableV2?hl=pl\r\nIs that a kind of example?\r\n\r\n### Request visuals, if applicable\r\n\r\n---\r\n\r\n### Submit a pull request?\r\n---\r\n", "comments": ["@summelon, Can you provide any keras doc link for VarHandleOp since I am not able to find it anywhere?", "@ashutosh1919 Thanks for your reply.\r\nI don't know where is it in keras doc. But it is something in TF doc like this https://www.tensorflow.org/api_docs/python/tf/raw_ops/VarHandleOp?hl=ko .\r\nMy empirical observation is that it's used in tf.keras defined model arch(VarHandleOp). \r\nOn the other hand, VariableV2 in tensorflow defined model arch.\r\nI can't figure out differences between these two.\r\nAnd how to replace VarHandleOp with VariableV2?\r\n\r\nI would be grateful for any help you are able to provide.", "@summelon , Please checkout the PR #37855 .", "> @summelon , Please checkout the PR #37855 .\r\n\r\n@summelon,\r\nCould you please check @ashutosh1919's comment and let us know if it helps. Thanks!\r\n", "@amahendrakar Oh! My bad. I missed the message you provided in \"Files change\".\r\nThanks for your example.\r\nHowever it still got me confused because of components of these two.\r\nWhen I running my prunning code, it led to different result(work VariableV2).\r\nI can't figure out why or provide my source code.\r\nIt would be helpful if you guys can give me some hints.\r\n\r\nThank you!", "By the way, My framework is under TF 1.13.1, which I think may be  different with TF 2.1.", "Update: After recognizing VarHandleOp and VariableV2, I found the problem of my code is still the difference between Op type. In TF-slim defined model, the program identify prunable Ops by scope with \"*/read\" and their type are \"Identify\". while in Keras, it seems that Ops are turn into \"*/ReadVariableOp\" and their type are also \"ReadVariableOp\".\r\nWould you like to tell me some helpful information about these two kinds of Ops?", "cc @fchollet \r\n\r\n> Update: After recognizing VarHandleOp and VariableV2, I found the problem of my code is still the difference between Op type. In TF-slim defined model, the program identify prunable Ops by scope with \"_/read\" and their type are \"Identify\". while in Keras, it seems that Ops are turn into \"_/ReadVariableOp\" and their type are also \"ReadVariableOp\".\r\n> Would you like to tell me some helpful information about these two kinds of Ops?\r\n\r\ncc @fchollet ", "Update: It seems that \"Identify\" type ops(e.g. \"resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights/read\" in tf-slim) and ReadVariableOp type ops(e.g.  \"res4f_branch2a/kernel/Read/ReadVariableOp\" in Keras) have similar characteristic when I check their type or shape.\r\n\r\nI tried to detect \"ReadVariableOp\" instead of \"Identify\" Ops. However now another problem occured. \r\n\r\nIn TF-Slim, outputs of \"*/weights\" and \"*/weights/read\" are \"<tf.Tensor 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights:0' shape=(1, 1, 64, 64) dtype=float32_ref>\" and \"<tf.Tensor 'resnet_v1_50/block1/unit_1/bottleneck_v1/conv1/weights/read:0' shape=(1, 1, 64, 64) dtype=float32>\" respectively.\r\nOn the other hand, in Keras, outputs of \"*/kernel\" and \"*/kernel/Read/ReadVariableOp\" are \"<tf.Tensor 'res4f_branch2a/kernel:0' shape=() dtype=resource>\" and \"<tf.Tensor 'res4f_branch2a/kernel/Read/ReadVariableOp:0' shape=(1, 1, 1024, 256) dtype=float32>\" respectively.\r\n\r\nWhen I tried to use \"tf.compat.v1.assign()\" function on \"*/weights\" in TF-Slim, it worked. However in Keras it failed on \"*/kernel\".\r\n\r\nIs there any mistake in my usage? Or I can Simply use \"*/kernel/Read/ReadVariable\" in Keras as both \"*/weight\" and \"*/weight/read\" in TF-Slim?\r\n\r\nThank you!", "@summelon,\r\nCould you please provide a sample code to reproduce the issue you are facing? Thanks!", "@amahendrakar I'm sorry, the code is not open source.\r\nInstead, let me clarify and simplify my question.\r\n\r\nIn TF-1.13.1, there are differences between \"VariableV2\" and \"VarHandleOp\" indeed.\r\nThis is the part which you can reproduce.\r\nAfter created a ResNet_v1_50 by TF-Slim and Keras, you can get the weight variable, \"*/weights\"(...V2) and \"*/kernel\"(...Op) respectively.\r\nThen, applying tf.compat.v1.assign(ref=variable.outputs[0], ...), it works on VariableV2 but VarHandlOp failed.\r\n\r\nI'm confuing about this.\r\nI would appreciate if you can provide any helpful solution.", "@summelon Hi. I'm facing the same problem and after many days of trying solutions here and there, I still cannot merge a keras model and a tensorflow1 model and then train the whole thing. I even manually changed VarHandleOp in the Keras nodes to VariableV2 in the graph, but I'm still getting immutable tensors without _ref at the end. I also used 'tf.variable' which creates new _ref nodes which don't substitute the previous ones, so it's of no use. As you said there's also \"ReadVariableOp\". I was wondering if you've found a workaround for this. Thank you :) \r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 42792, "title": "[ja] Automatic proofreading on GitHub Actions", "body": "We, Japanese community members, are using a [proofreading tool](https://github.com/tfug/proofreading).\r\n\r\nWe want to apply the proofreading tool on CI and make Japanese contributors to pass it.\r\nThus, our questions are below:\r\n\r\n**1. Could we create `docs-l10n/.github/workflows/ja.yml`?**\r\n\r\nWe want to define a GitHub Actions's workflow for Japanese documents proofreading.\r\nThe setting below will not affect non-japanese documents:\r\n\r\n```yaml\r\non:\r\n  push:\r\n    paths:\r\n      - 'site/ja/**'\r\n```\r\n\r\n**2. Could we use `docs-l10n/tools` directory to develop proofreading tool?**\r\n\r\nI guess it's natural that the proofreading tool is in `docs-l10n/tools`.\r\n\r\nIf it's \"no\", however, we are planning to checkout [tfug/proofreading](https://github.com/tfug/proofreading) in GitHub Actions and apply it.\r\nActually, [current workflow](https://github.com/tensorflow/docs-l10n/blob/6cd120c79237361422f83df4f22c2f7760f5fa87/.github/workflows/ci.yaml) for tensorflow/docs-l10n checkout [tensorflow/docs](https://github.com/tensorflow/docs) repository and use `docs/tools/nbfmt.py`.\r\n\r\n", "comments": ["Sounds good to me. But still figuring out adding `nbfmt.py` as CI using GitHub Actions. It seems there are a lot (security) restrictions if you want to automatically add commits on a pull requests. But if you just want to use it for pass/fail tests and surface that in the PR, that is more straightforward.", "Thanks.\r\n\r\n> It seems there are a lot (security) restrictions if you want to automatically add commits on a pull requests\r\n\r\nWe are not planning to automatically add commits on CI. Just applying our proofreading tool for pass/fail tests. Translators should pass the test in their commits and PRs.\r\n\r\nIf it's okay, we are willing to make a PR to add `docs-l10n/.github/workflows/ja.yml`.", "Cool. Yeah, please submit a PR and we can go through it. I'm not familiar with all of your tooling, so will need you (or someone in the community) to maintain it. But sounds good to me \ud83d\ude00", "PR tensorflow/docs-l10n#185 is for this issue and it merged."]}, {"number": 37829, "title": "Input tensor has different type from described in the comments.", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started.md#validate-input-shape\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nAt following lines of the second code of [this section](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/microcontrollers/get_started.md#validate-input-shape), it is claimed that input is a 2D tensor.\r\n\r\n```c++\r\n// The property \"dims\" tells us the tensor's shape. It has one element for\r\n// each dimension. Our input is a 2D tensor containing 1 element, so \"dims\"\r\n// should have size 2.\r\nTF_LITE_MICRO_EXPECT_EQ(2, input->dims->size);\r\n```\r\n\r\nHowever, based on [the notebook where the model defined](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb), it should have 1D tensor for input.\r\n\r\n### Submit a pull request?\r\n\r\nNo.", "comments": ["Sorry, this issue came from my misunderstanding. I checked the input shape of the `model_2` in [this notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb) but what I have to check truly is the input shape of `sine_model`. So, I close this issue."]}, {"number": 37828, "title": "Fix metrics.mean_cosine_distance", "body": "Fix #37827", "comments": []}, {"number": 37827, "title": "tf.metrics.mean_cosine_distance fails during distributed evaluation", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): `No`\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): `Linux Debian GNU/Linux 9`\r\n- TensorFlow installed from (source or\r\nbinary): `binary`\r\n- TensorFlow version (use command below): `v1.15.2-1-g61ff2cb 1.15.2`\r\n- Python version: `3.7.6`\r\n- CUDA/cuDNN version: `CUDA 10, cuDNN 7.6.5`\r\n- GPU model and memory: `2x Tesla K80`\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.metrics.mean_cosine_distance` fails at the end of distributed evaluation with `MirroredStrategy`:\r\n\r\n```\r\nTypeError: Fetch argument PerReplica:{\r\n  0 /replica:0/task:0/device:GPU:0: <tf.Tensor 'Sub:0' shape=() dtype=float32>,\r\n  1 /replica:0/task:0/device:GPU:1: <tf.Tensor 'replica_1/Sub:0' shape=() dtype=float32>\r\n} has invalid type <class 'tensorflow.python.distribute.values.PerReplica'>, must be a string or Tensor. (Can not convert a PerReplica into a Tensor or Operation.)\r\n```\r\n\r\nNon-distributed evaluation (that is, with `RunConfig.eval_distribute=None` or with a single GPU only) finishes without errors.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef model_fn(features, labels, mode):\r\n    predictions = tf.layers.dense(features, 2)\r\n    metrics = {'cos': tf.metrics.mean_cosine_distance(labels, predictions, 1)}\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        predictions=predictions,\r\n        loss=tf.constant(0.1),\r\n        train_op=None,\r\n        eval_metric_ops=metrics)\r\n\r\n\r\ndef input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices(\r\n        (np.array([[1., 1.]]), np.array([[2., 2.]])))\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(1, drop_remainder=True)\r\n    return dataset\r\n\r\n\r\nif __name__ == '__main__':\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    assert len(gpus) > 1, 'Need >1 GPUs to run'\r\n    strategy = tf.distribute.MirroredStrategy()\r\n    run_config = tf.estimator.RunConfig(train_distribute=strategy,\r\n                                        eval_distribute=strategy)\r\n\r\n    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\r\n    print(estimator.evaluate(input_fn, steps=5))\r\n```\r\n\r\n**Other info / logs**: \r\n[logs_1_15.txt](https://github.com/tensorflow/tensorflow/files/4368761/logs_1_15.txt)\r\n", "comments": ["@master \r\nAs we see there is a pr related to this issue, can we move this to closed status, as this will be monitored in pr #37828", "Sounds good, closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37827\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37827\">No</a>\n"]}, {"number": 37826, "title": "Op type not registered 'HashTableV2' in binary running on 5f00e250dfa5", "body": "when I load a model of CRNN,I find this peoblem.\r\nERROR: Creating graph in session failed...Not found: Op type not registered 'HashTableV2' in binary running on 5f00e250dfa5. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\n\r\nI use this code:\r\n` \r\n#include <fstream>\r\n#include <utility>\r\n#include <vector>\r\n#include <Eigen/Core>\r\n#include <Eigen/Dense>\r\n\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\n#include <iostream>\r\n\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\nint main()\r\n{\r\n    Session* session;\r\n    Status status = NewSession(SessionOptions(), &session);\r\n\r\n    if (!status.ok()) {\r\n        cout << status.ToString() << \"\\n\";\r\n        return 1;\r\n    }\r\n    cout << \"Session successfully created.\\n\";\r\n    string model_path=\"/tensorflow_cc/mnist/crnn_frozen_model.pb\";\r\n    GraphDef graphdef; //Graph Definition for current model\r\n\r\n\r\n    Status status_load = ReadBinaryProto(Env::Default(), model_path, &graphdef); / \r\n    if (!status_load.ok()) {\r\n        std::cout << \"ERROR: Loading model failed...\" << model_path << std::endl;\r\n        std::cout << status_load.ToString() << \"\\n\";\r\n        return -1;\r\n    }\r\n    Status status_create = session->Create(graphdef);  \r\n    if (!status_create.ok()) {\r\n        std::cout << \"ERROR: Creating graph in session failed...\" << status_create.ToString() << std::endl;\r\n        return -1;\r\n    }\r\n    cout << \"Session successfully created.\"<< endl;\r\n}\r\n`", "comments": ["@tianhongbao \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!\r\n\r\n", "@ravikyram ubuntu,tensorflow c++ 1.13.0", "@tianhongbao Can you please try with tensorflow versions >=1.14.0 and let me know if the issue still persists.", "Closing this issue as it has been inactive for more than 2 weeks. Please add additional comments for us to open this issue again. Thanks!"]}, {"number": 37825, "title": "Issue with installation of Tensorflow in Windows10", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution:Windows\r\n- TensorFlow installed from (source or binary): https://www.tensorflow.org/install\r\n- TensorFlow version:2.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?: Created virtualenv using conda. used pip to install tensorflow\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: GTX 1650\r\n\r\n**Describe the problem**\r\nTensorflow is getting installed in virtual environment but cannot import it.When I try to import, I get the following error\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nTensorFlowTensorF\r\n*Steps followed**\r\n1. Uninstall all NVIDIA Drivers/Software and delete all NVIDIA files from program files (x86 aswell)\r\n\r\n2. Install Visual Studio 2017 (from link above)\r\n\r\n3. Install CUDA (first check compatible versions using link above)\r\n\r\n4. Download cuDNN (again check version is same as CUDA version)\r\n\r\n5. Extract the cuDNN zip folder to your desktop. Open a new windows explorer and navigate to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\Vx.x\\ . Now copy the contents of the downloaded and extracted cuDNN folder into the appropriate folders (files from cuDNN bin go into the new windows explorer bin folder etc.).\r\n\r\n6. Navigate to your system enviorment variables and edit the path. Add the following two directories into your path:\r\n-  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\r\n- C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp\r\n\r\n7. Download and Install Anaconda (from link above)\r\n\r\n8. Set up a virtual enviorment using python 3.5\r\n- conda create -n [name] python=3.5\r\n\r\n9. Activate the virtual enviorment\r\n- activate [name]\r\n\r\n10. Install packages\r\n- pip install --ignore-installed --upgrade tensorflow-gpu\r\n- pip install keras\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\indra\\anaconda3\\envs\\imgreco\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@indranibose1981 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Thanks!", "Please use ` ``` ` around code blocks on future issue reports. Also, please check for duplicates.\r\n\r\nClosing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37825\">No</a>\n", "I am using a GPU. I was able to install 1.5 version but 2.0 of tensor flow still does not work.\n\n\nSent from my iPhone\n\n> On 23-Mar-2020, at 4:49 PM, ravikyram <notifications@github.com> wrote:\n> \n> \ufeff\n> @indranibose1981\n> \n> What is make/model of your cpu?\n> I suspect your cpu model does not support AVX instructions sets.See hardware requirements\n> Make sure to download the latest microsoft visual c++ redistributable from here.\n> .Also, please follow the instructions from to install from Tensorflow website.\n> \n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n"]}, {"number": 37824, "title": "out of memory issue in running face-api.js", "body": "hii \r\nwhile running face-api.js node examples (node faceRecognition.js) am getting the OOM error while running 2 hour video face recognition. how can solve this issue.\r\n \r\n2020-03-20 16:12:39.728434: W tensorflow/core/framework/op_kernel.cc:1651] OP_RE\r\nQUIRES failed at cwise_ops_common.cc:82 : Resource exhausted: OOM when allocatin\r\ng tensor with shape[1,32,32,512] and type float on /job:localhost/replica:0/task\r\n:0/device:CPU:0 by allocator cpu\r\nException code=0xc0000005 flags=0x0 at 0x00007FF957AEA277. Access violation - at\r\ntempting to read data at address 0x0000000000000010", "comments": ["@Rajeshwari210,\r\nPlease Provide the exact sequence of commands / steps that you executed before running into the problem. Thanks", "$ tsc --skipLibCheck faceRecognition.ts\r\n node faceRecognition.js\r\nam running these two commands and i'm uploading 1hr video as input which creates the frame(through ffmpeg extract frames) and compares with face-api module and print the confidence score. after 750 frames am getting **OMM** (out of memory) issue.", "@Rajeshwari210, This issue is not related to Tensorflow can you post this issue in relevant repo TFjs. Thanks", "@Rajeshwari210, Please update for the above comment. Thanks", "Closing this issue since its not related to Tensorflow. Please post this issue in TFJS repo.\r\nThanks"]}, {"number": 37823, "title": "How to solve this problem\uff1f\uff08c++ tensorflow+opencv \uff09", "body": "/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4350:30: note: suggested alternative: \u2018GOOGLE_DLOG\u2019\r\n   operation_timeout_in_ms_ = GOOGLE_LONGLONG(0);\r\n                              ^~~~~~~~~~~~~~~\r\n                              GOOGLE_DLOG\r\n/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h: In member function \u2018void tensorflow::RunOptions_Experimental::clear_collective_graph_key()\u2019:\r\n/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4571:27: error: \u2018GOOGLE_LONGLONG\u2019 was not declared in this scope\r\n   collective_graph_key_ = GOOGLE_LONGLONG(0);\r\n                           ^~~~~~~~~~~~~~~\r\n/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4571:27: note: suggested alternative: \u2018GOOGLE_DLOG\u2019\r\n   collective_graph_key_ = GOOGLE_LONGLONG(0);\r\n                           ^~~~~~~~~~~~~~~\r\n                           GOOGLE_DLOG\r\n/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h: In member function \u2018void tensorflow::RunOptions::clear_timeout_in_ms()\u2019:\r\n/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4617:20: error: \u2018GOOGLE_LONGLONG\u2019 was not declared in this scope\r\n   timeout_in_ms_ = GOOGLE_LONGLONG(0);\r\n                    ^~~~~~~~~~~~~~~\r\n/home/sy/tensorflow/bazel-genfiles/tensorflow/core/protobuf/config.pb.h:4617:20: note: suggested alternative: \u2018GOOGLE_DLOG\u2019\r\n   timeout_in_ms_ = GOOGLE_LONGLONG(0);\r\n                    ^~~~~~~~~~~~~~~\r\n                    GOOGLE_DLOG\r\n", "comments": ["@heartraeh \r\ncould you please share how you encountered this problem, a simple stand alone code for us to replicate the issue faced along with the tensor flow version.", "#include <iostream&gt;\r\n&nbsp;\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n&nbsp;\r\n#include <opencv2/opencv.hpp&gt;\r\n#include <cv.h&gt;\r\n#include <highgui.h&gt;\r\n#include <Eigen/Core&gt;\r\n#include <Eigen/Dense&gt;\r\n&nbsp;\r\nusing namespace std;\r\nusing namespace cv;\r\nusing namespace tensorflow;\r\n&nbsp;\r\n&nbsp;\r\n&nbsp;\r\n// \u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u8bb2OpenCV\u7684Mat\u6570\u636e\u8f6c\u5316\u4e3atensor\uff0cpython\u91cc\u9762\u53ea\u8981\u5bf9cv2.read\u8bfb\u8fdb\u6765\u7684\u77e9\u9635\u8fdb\u884cnp.reshape\u4e4b\u540e\uff0c\r\n// \u6570\u636e\u7c7b\u578b\u5c31\u6210\u4e86\u4e00\u4e2atensor\uff0c\u5373tensor\u4e0e\u77e9\u9635\u4e00\u6837\uff0c\u7136\u540e\u5c31\u53ef\u4ee5\u8f93\u5165\u5230\u7f51\u7edc\u7684\u5165\u53e3\u4e86\uff0c\u4f46\u662fC++\u7248\u672c\uff0c\u6211\u4eec\u7f51\u7edc\u5f00\u653e\u7684\u5165\u53e3\r\n// \u4e5f\u9700\u8981\u5c06\u8f93\u5165\u56fe\u7247\u8f6c\u5316\u6210\u4e00\u4e2atensor\uff0c\u6240\u4ee5\u5982\u679c\u7528OpenCV\u8bfb\u53d6\u56fe\u7247\u7684\u8bdd\uff0c\u5c31\u662f\u4e00\u4e2aMat\uff0c\u7136\u540e\u5c31\u8981\u8003\u8651\u600e\u4e48\u5c06Mat\u8f6c\u5316\u4e3a\r\n// Tensor\u4e86\r\nvoid CVMat_to_Tensor(Mat img,Tensor* output_tensor,int input_rows,int input_cols)\r\n{\r\n&nbsp; &nbsp; //imshow(\"input image\",img);\r\n&nbsp; &nbsp; //\u56fe\u50cf\u8fdb\u884cresize\u5904\u7406\r\n&nbsp; &nbsp; resize(img,img,cv::Size(input_cols,input_rows));\r\n&nbsp; &nbsp; //imshow(\"resized image\",img);\r\n&nbsp;\r\n&nbsp; &nbsp; //\u5f52\u4e00\u5316\r\n&nbsp; &nbsp; img.convertTo(img,CV_8UC3);&nbsp; // CV_32FC3\r\n&nbsp; &nbsp; //img=1-img/255;\r\n&nbsp;\r\n&nbsp; &nbsp; //\u521b\u5efa\u4e00\u4e2a\u6307\u5411tensor\u7684\u5185\u5bb9\u7684\u6307\u9488\r\n&nbsp; &nbsp; uint8 *p = output_tensor-&gt;flat<uint8&gt;().data();\r\n&nbsp;\r\n&nbsp; &nbsp; //\u521b\u5efa\u4e00\u4e2aMat\uff0c\u4e0etensor\u7684\u6307\u9488\u7ed1\u5b9a,\u6539\u53d8\u8fd9\u4e2aMat\u7684\u503c\uff0c\u5c31\u76f8\u5f53\u4e8e\u6539\u53d8tensor\u7684\u503c\r\n&nbsp; &nbsp; cv::Mat tempMat(input_rows, input_cols, CV_8UC3, p);\r\n&nbsp; &nbsp; img.convertTo(tempMat,CV_8UC3);\r\n&nbsp;\r\n&nbsp;//&nbsp; &nbsp; waitKey(0);\r\n&nbsp;\r\n}\r\n&nbsp;\r\nint main()\r\n{\r\n&nbsp; &nbsp; /*--------------------------------\u914d\u7f6e\u5173\u952e\u4fe1\u606f------------------------------*/\r\n&nbsp; &nbsp; string model_path=\"../model/coco.pb\";\r\n&nbsp; &nbsp; string image_path=\"../test.jpg\";\r\n&nbsp; &nbsp; int input_height = 1000;\r\n&nbsp; &nbsp; int input_width = 1000;\r\n&nbsp; &nbsp; string input_tensor_name=\"image_tensor\";\r\n&nbsp; &nbsp; vector<string&gt; out_put_nodes;&nbsp; //\u6ce8\u610f\uff0c\u5728object detection\u4e2d\u8f93\u51fa\u7684\u4e09\u4e2a\u8282\u70b9\u540d\u79f0\u4e3a\u4ee5\u4e0b\u4e09\u4e2a\r\n&nbsp; &nbsp; out_put_nodes.push_back(\"detection_scores\");&nbsp; //detection_scores&nbsp; detection_classes&nbsp; detection_boxes\r\n&nbsp; &nbsp; out_put_nodes.push_back(\"detection_classes\");\r\n&nbsp; &nbsp; out_put_nodes.push_back(\"detection_boxes\");\r\n&nbsp;\r\n&nbsp; &nbsp; /*--------------------------------\u521b\u5efasession------------------------------*/\r\n&nbsp; &nbsp; Session* session;\r\n&nbsp; &nbsp; Status status = NewSession(SessionOptions(), &amp;session);//\u521b\u5efa\u65b0\u4f1a\u8bddSession\r\n&nbsp;\r\n&nbsp; &nbsp; /*--------------------------------\u4ecepb\u6587\u4ef6\u4e2d\u8bfb\u53d6\u6a21\u578b--------------------------------*/\r\n&nbsp; &nbsp; GraphDef graphdef; //Graph Definition for current model\r\n&nbsp;\r\n&nbsp; &nbsp; Status status_load = ReadBinaryProto(Env::Default(), model_path, &amp;graphdef); //\u4ecepb\u6587\u4ef6\u4e2d\u8bfb\u53d6\u56fe\u6a21\u578b;\r\n&nbsp; &nbsp; if (!status_load.ok()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"ERROR: Loading model failed...\" << model_path << std::endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << status_load.ToString() << \"\\n\";\r\n&nbsp; &nbsp; &nbsp; &nbsp; return -1;\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; Status status_create = session-&gt;Create(graphdef); //\u5c06\u6a21\u578b\u5bfc\u5165\u4f1a\u8bddSession\u4e2d;\r\n&nbsp; &nbsp; if (!status_create.ok()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"ERROR: Creating graph in session failed...\" << status_create.ToString() << std::endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; return -1;\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; cout << \"<----Successfully created session and load graph.-------&gt;\"<< endl;\r\n&nbsp;\r\n&nbsp; &nbsp; /*---------------------------------\u8f7d\u5165\u6d4b\u8bd5\u56fe\u7247-------------------------------------*/\r\n&nbsp; &nbsp; cout<<endl<<\"<------------loading test_image--------------&gt;\"<<endl;\r\n&nbsp; &nbsp; Mat img;\r\n&nbsp; &nbsp; img = imread(image_path);\r\n&nbsp; &nbsp; cvtColor(img, img, CV_BGR2RGB);\r\n&nbsp; &nbsp; if(img.empty())\r\n&nbsp; &nbsp; {\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout<<\"can't open the image!!!!!!!\"<<endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; return -1;\r\n&nbsp; &nbsp; }\r\n&nbsp;\r\n&nbsp; &nbsp; //\u521b\u5efa\u4e00\u4e2atensor\u4f5c\u4e3a\u8f93\u5165\u7f51\u7edc\u7684\u63a5\u53e3\r\n&nbsp; &nbsp; Tensor resized_tensor(DT_UINT8, TensorShape({1,input_height,input_width,3})); //DT_FLOAT\r\n&nbsp;\r\n&nbsp; &nbsp; //\u5c06Opencv\u7684Mat\u683c\u5f0f\u7684\u56fe\u7247\u5b58\u5165tensor\r\n&nbsp; &nbsp; CVMat_to_Tensor(img,&amp;resized_tensor,input_height,input_width);\r\n&nbsp;\r\n&nbsp; &nbsp; cout << resized_tensor.DebugString()<<endl;\r\n&nbsp;\r\n&nbsp; &nbsp; /*-----------------------------------\u7528\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5-----------------------------------------*/\r\n&nbsp; &nbsp; cout<<endl<<\"<-------------Running the model with test_image---------------&gt;\"<<endl;\r\n&nbsp; &nbsp; //\u524d\u5411\u8fd0\u884c\uff0c\u8f93\u51fa\u7ed3\u679c\u4e00\u5b9a\u662f\u4e00\u4e2atensor\u7684vector\r\n&nbsp; &nbsp; vector<tensorflow::Tensor&gt; outputs;\r\n&nbsp;\r\n&nbsp; &nbsp; Status status_run = session-&gt;Run({{input_tensor_name, resized_tensor}}, {out_put_nodes}, {}, &amp;outputs);\r\n&nbsp;\r\n&nbsp; &nbsp; if (!status_run.ok()) {\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"ERROR: RUN failed...\"&nbsp; << std::endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << status_run.ToString() << \"\\n\";\r\n&nbsp; &nbsp; &nbsp; &nbsp; return -1;\r\n&nbsp; &nbsp; }\r\n&nbsp;\r\n&nbsp; &nbsp; //\u628a\u8f93\u51fa\u503c\u7ed9\u63d0\u53d6\u51fa\r\n&nbsp; &nbsp; cout << \"Output tensor size:\" << outputs.size() << std::endl;&nbsp; //3\r\n&nbsp; &nbsp; for (int i = 0; i < outputs.size(); i++)\r\n&nbsp; &nbsp; {\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << outputs[i].DebugString()<<endl;&nbsp; &nbsp;// [1, 50], [1, 50], [1, 50, 4]\r\n&nbsp; &nbsp; }\r\n&nbsp;\r\n&nbsp; &nbsp; cvtColor(img, img, CV_RGB2BGR);&nbsp; // opencv\u8bfb\u5165\u7684\u662fBGR\u683c\u5f0f\u8f93\u5165\u7f51\u7edc\u524d\u8f6c\u4e3aRGB\r\n&nbsp; &nbsp; resize(img,img,cv::Size(1000,1000));&nbsp; // \u6a21\u578b\u8f93\u5165\u56fe\u50cf\u5927\u5c0f\r\n&nbsp; &nbsp; int pre_num = outputs[0].dim_size(1);&nbsp; // 50&nbsp; \u6a21\u578b\u9884\u6d4b\u7684\u76ee\u6807\u6570\u91cf\r\n&nbsp; &nbsp; auto tmap_pro = outputs[0].tensor<float, 2&gt;();&nbsp; //\u7b2c\u4e00\u4e2a\u662fscore\u8f93\u51fashape\u4e3a[1,50]\r\n&nbsp; &nbsp; auto tmap_clas = outputs[1].tensor<float, 2&gt;();&nbsp; //\u7b2c\u4e8c\u4e2a\u662fclass\u8f93\u51fashape\u4e3a[1,50]\r\n&nbsp; &nbsp; auto tmap_coor = outputs[2].tensor<float, 3&gt;();&nbsp; //\u7b2c\u4e09\u4e2a\u662fcoordinate\u8f93\u51fashape\u4e3a[1,50,4]\r\n&nbsp; &nbsp; float probability = 0.5;&nbsp; //\u81ea\u5df1\u8bbe\u5b9a\u7684score\u9608\u503c\r\n&nbsp; &nbsp; for (int pre_i = 0; pre_i < pre_num; pre_i++)\r\n&nbsp; &nbsp; {\r\n&nbsp; &nbsp; &nbsp; &nbsp; if (tmap_pro(0, pre_i) < probability)\r\n&nbsp; &nbsp; &nbsp; &nbsp; {\r\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break;\r\n&nbsp; &nbsp; &nbsp; &nbsp; }\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"Class ID: \" << tmap_clas(0, pre_i) << endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"Probability: \" << tmap_pro(0, pre_i) << endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; string id = to_string(int(tmap_clas(0, pre_i)));\r\n&nbsp; &nbsp; &nbsp; &nbsp; int xmin = int(tmap_coor(0, pre_i, 1) * input_width);\r\n&nbsp; &nbsp; &nbsp; &nbsp; int ymin = int(tmap_coor(0, pre_i, 0) * input_height);\r\n&nbsp; &nbsp; &nbsp; &nbsp; int xmax = int(tmap_coor(0, pre_i, 3) * input_width);\r\n&nbsp; &nbsp; &nbsp; &nbsp; int ymax = int(tmap_coor(0, pre_i, 2) * input_height);\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"Xmin is: \" << xmin << endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"Ymin is: \" << ymin << endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"Xmax is: \" << xmax << endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; cout << \"Ymax is: \" << ymax << endl;\r\n&nbsp; &nbsp; &nbsp; &nbsp; rectangle(img, cvPoint(xmin, ymin), cvPoint(xmax, ymax), Scalar(255, 0, 0), 1, 1, 0);\r\n&nbsp; &nbsp; &nbsp; &nbsp; putText(img, id, cvPoint(xmin, ymin), FONT_HERSHEY_COMPLEX, 1.0, Scalar(255,0,0), 1);\r\n&nbsp; &nbsp; }\r\n&nbsp; &nbsp; imshow(\"1\", img);\r\n&nbsp; &nbsp; cvWaitKey(0);\r\n&nbsp;\r\n&nbsp; &nbsp; return 0;\r\n}\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:&nbsp;\"Saduf2019\"<notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2020\u5e743\u670823\u65e5(\u661f\u671f\u4e00) \u4e0b\u53485:09\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"\u6b66\u6587\u535a\"<wuwenboxyz@qq.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] How to solve this problem\uff1f\uff08c++ tensorflow+opencv \uff09 (#37823)\r\n\r\n\r\n\r\n\r\n\r\n \r\n@heartraeh\r\n could you please share how you encountered this problem, a simple stand alone code for us to replicate the issue faced along with the tensor flow version.\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@heartraeh \r\nplease update tensorflow version", "@heartraeh\r\nplease update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 37822, "title": "tf.fill(), This function does not handle the case of the path where all inputs are not already EagerTensors", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.20rc0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: - GPU model and memory:CPU only\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\n# I run this in eager mode\r\n# input: [batch_size, features]\r\ndef get_indices(self, input, batch_size, max_len):\r\n\tnum_phones = input.get_shape().as_list()[1]\r\n\r\n\tout = []\r\n\tfor i in range(batch_size):\r\n\t\tcur_len = tf.reduce_sum(input[i])\r\n\t\tindices = tf.concat([tf.fill([input[i][idx]], idx)\r\n\t\t\t\t\t\t\t for idx in range(num_phones)], axis=-1)\r\n\t\t\t\t\t\t\t \r\n\treturn out\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3307, in fill\r\n    dims, value)\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/train.py\", line 168, in <module>\r\n    main()\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/train.py\", line 164, in main\r\n    train(log_dir, args)\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/train.py\", line 98, in train\r\n    mel_targets, linear_targets)\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/train.py\", line 30, in train_step\r\n    mel_outputs, linear_outputs = model((sequences, durations), training=True)\r\n  File \"/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 967, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py\", line 567, in call\r\n    training=training)\r\n  File \"/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 967, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py\", line 229, in call\r\n    indices = self.get_indices(durations, batch_size, max_len)\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py\", line 245, in get_indices\r\n    for idx in range(num_phones)], axis=-1)\r\n  File \"/mnt/c/SourceCode/tacotron-tf2/models/tacotron.py\", line 245, in <listcomp>\r\n    for idx in range(num_phones)], axis=-1)\r\n  File \"/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 234, in fill\r\n    result = gen_array_ops.fill(dims, value, name=name)\r\n  File \"/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3312, in fill\r\n    dims, value, name=name, ctx=_ctx)\r\n  File \"/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3339, in fill_eager_fallback\r\n    ctx=ctx, name=name)\r\n  File \"/home/yhb/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension -14 must be >= 0 [Op:Fill]\r\n```\r\nProcess finished with exit code 1\r\n\r\n", "comments": ["@xiaoyangnihao \r\n\r\nLooks like code is incomplete. Please, share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37822\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37822\">No</a>\n", "Why was this issue closed? Does anyone have a solution?"]}, {"number": 37821, "title": "Error converting Mobilenet model using TFLiteConverter", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): 2.1.1\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n model._set_inputs(x)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\n\r\n**Failure details**\r\n\r\nWhen I convert a model using TFLiteConverter with the settings above, it seems to create a uint8 node. However, when I try to run the converted model on my microcontroller using the Tensorflow Lite Micro the input node of the graph still seems to be the float version. Is there any way I can access the graph being created by Tensorflow Lite Converter or possibly set the input to the uint8 node?\r\n\r\n", "comments": ["@ajax98,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Any updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 37820, "title": "Make initial_accumulator_value in adagrad consistent", "body": "Fix #37229. 0.1 is used in all other cases.", "comments": ["Because the same code running under a new TF version will now return a\ndifferent answer. This silently changes the hyperparameters of many\nexisting models, making previous experiments not reproducible.\n\nOn Mon, Mar 23, 2020 at 8:48 AM Jin Dong <notifications@github.com> wrote:\n\n> *@djdongjin* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/keras/optimizer_v2/adagrad.py\n> <https://github.com/tensorflow/tensorflow/pull/37820#discussion_r396553149>\n> :\n>\n> > @@ -141,7 +141,7 @@ def from_config(cls, config, custom_objects=None):\n>          An optimizer instance.\n>      \"\"\"\n>      if 'initial_accumulator_value' not in config:\n> -      config['initial_accumulator_value'] = 0.\n>\n> why changing a value from 0.0 to 0.1 is a backwards-incompatible change?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37820#discussion_r396553149>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRI5FTPHA65ZQOLVTYDRI6AEZANCNFSM4LRUI2JA>\n> .\n>\n\n\n-- \n - Alex\n", "> Because the same code running under a new TF version will now return a different answer. This silently changes the hyperparameters of many existing models, making previous experiments not reproducible.\r\n> [\u2026](#)\r\n> On Mon, Mar 23, 2020 at 8:48 AM Jin Dong ***@***.***> wrote: ***@***.**** commented on this pull request. ------------------------------ In tensorflow/python/keras/optimizer_v2/adagrad.py <[#37820 (comment)](https://github.com/tensorflow/tensorflow/pull/37820#discussion_r396553149)> : > @@ -141,7 +141,7 @@ def from_config(cls, config, custom_objects=None): An optimizer instance. \"\"\" if 'initial_accumulator_value' not in config: - config['initial_accumulator_value'] = 0. why changing a value from 0.0 to 0.1 is a backwards-incompatible change? \u2014 You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <[#37820 (comment)](https://github.com/tensorflow/tensorflow/pull/37820#discussion_r396553149)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAABHRI5FTPHA65ZQOLVTYDRI6AEZANCNFSM4LRUI2JA> .\r\n> -- - Alex\r\n\r\nunderstood. thanks!"]}, {"number": 37819, "title": "modify the initial value of initial_accumulator_value in adagrad", "body": "Fix #37229, the 0.1 value is used in all other cases.", "comments": []}, {"number": 37818, "title": "[Feature Request][Keras] Allow loss function using multiple tensors as input", "body": "Some loss functions not only depend on `y_pred` and `y_true` but also depend on other tensors, such as a CTC loss (related to \r\n* https://github.com/ysoullard/CTCModel/blob/d9e87fb24a6f7446457de5c89004404a900ce00b/CTCModel.py#L108\r\n* https://github.com/robmsmt/KerasDeepSpeech/blob/553638821c996cb6049e34185babacdb8f00c215/model.py#L145\r\n* https://github.com/ypwhs/captcha_break/blob/master/ctc_2019.ipynb\r\n\r\n), \r\n\r\nYolo3 loss (related to \r\n* https://github.com/qqwweee/keras-yolo3/blob/e6598d13c703029b2686bc2eb8d5c09badf42992/train.py#L131\r\n* https://github.com/experiencor/keras-yolo3/blob/768c524f277adbfd26c2f44d73cb1826bbaf2d10/yolo.py#L361\r\n\r\n)\r\n\r\n and CRF loss function (relate to \r\n\r\n* tensorflow/addons#1363\r\n* tensorflow/addons#377\r\n\r\n). \r\n\r\nMaybe we can provide a container named `GroupedTensorContainer` to make it works like below:\r\n\r\n```python\r\nx_np, y_np = get_test_data()\r\n\r\nx_input = tf.keras.layers.Input(shape=x_np.shape[1:])\r\n\r\n# NOTE: tensors can from different layers\r\npred_y, loss_required_1, loss_required_2 = SomeLayer(5)(x_input)\r\n\r\ncanned_tensors = GroupedTensorContainer(visiable=pred_y, hidden=(loss_required_1, loss_required_2))\r\n\r\nmodel = tf.keras.Model(x_input, canned_tensors)\r\n\r\nmodel.compile(\"adam\", loss=SomeLoss())\r\nmodel.fit(x_np, y_np)\r\nmodel.evaluate(x_np, y_np)\r\n\r\ncontainer = model.predict(x_np)\r\ncontainer  # this is the visible tensor\r\n```\r\n\r\nwhile the `SomeLoss` has such a prototype:\r\n```python\r\nclass SomeLoss:\r\n    def __call__(self, y_true, y_pred, sample_weight=None):\r\n        pred_y = y_pred.visiable_tensor\r\n        loss_required_1, loss_required_2 = y_pred.hidden_tensors\r\n\r\n       loss = compute_loss(\r\n            y_true, pred_y, loss_required_1, loss_required_2\r\n        )\r\n\r\n        return loss\r\n```\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes", "comments": ["@pavithrasv Please take a look, thank you!", "@pavithrasv Could you please take a look?\r\nOr anyone from the official team willing to take a look? This issue already waiting for more than one year, but I am still want to figure out: the idea is good or not.", "@howl-anderson, Sorry for the late response. Is this still an issue for you?\r\n\r\nCan you please try recent TFv2.7 and let us know whether it is persisting. Also, share a simple standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37817, "title": "Can't convert .pb model file to .tflite", "body": "When I ran the following code:\r\n```\r\nimport tensorflow as tf\r\n\r\npath = \"C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/frozen_inference_graph-SteelRoll.pb\"\r\n\r\ninputs = [\"image_tensor\"]\r\noutputs = [\"detection_boxes\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs, input_shapes={\"image_tensor\":[1,640,360,3]})\r\nconverter.post_training_quantize = True\r\ntflite_model = converter.convert()\r\nopen(\"frozen_inference_graph-SteelRoll.tflite\", \"wb\").write(tflite_model)\r\n```\r\nI met a fatal error which said:\r\n`F tensorflow/lite/toco/tooling_util.cc:2258] Check failed: array.data_type == array.final_data_type Array \"image_tensor\" has mis-matching actual and final data types (data_type=uint8, final_data_type=float).\r\nFatal Python error: Aborted`\r\nHow do I solve this problem?\r\n", "comments": ["@LAWSSSS \r\nplease share the tensor flow version along with all dependencies to replicate the issue faced.\r\nplease find [the gist](https://colab.sandbox.google.com/gist/Saduf2019/cb02a2eec0989ee7264c9715cddbedc6/37817.ipynb) of the code shared, on nightly\r\n", "@Saduf2019 TF_CPU = 1.14.0", "@LAWSSSS \r\nplease check to this [issue](https://github.com/tensorflow/tensorflow/issues/35736#issuecomment-574073364) for reference as it has same error.", "@Saduf2019 Thanks for the solution. I changed my tensorflow to 2.0.0 and used the same method provided in the referenced issue(`tf.compat.v1.lite.TFLiteConverter.from_frozen_graph`), it worked out.\r\nThanks a lot.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37817\">No</a>\n"]}, {"number": 37816, "title": "Added description of effect of regularization on validation and test", "body": "Fixes #37792 .\r\n@mihaimaruseac , Please review this one.", "comments": []}, {"number": 37815, "title": "Error when train a model on Colab", "body": "**Code**\r\n\r\n```\r\nhistory = model.fit(train_dataset,\r\n                    epochs=EPOCHS,\r\n                    callbacks=[lr_schedule],\r\n                    steps_per_epoch=STEPS_PER_EPOCH,\r\n                    )\r\nfinal_stats = model.evaluate(validation_dataset, steps=1)\r\nprint(\"Validation accuracy: \", final_stats[1])\r\n```\r\n\r\n\r\n**Describe the problem**\r\nWhen I tried to train my model on **Colab** using above code\r\nI got the error like below mentioned:\r\n\r\n```\r\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\r\nEpoch `1/20`\r\nUnimplementedError                        Traceback (most recent call last)\r\n/tensorflow-1.15.0/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n10 frames\r\nUnimplementedError: From /job:worker/replica:0/task:0:\r\n{{function_node __inference_Dataset_map_decode_image_22}} File system scheme '[local]' not implemented (file: '/gdrive/My Drive/plant-pathology-2020-fgvc7/images/Train_187.jpg')\r\n\t [[{{node ReadFile}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional_9]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnimplementedError                        Traceback (most recent call last)\r\n/tensorflow-1.15.0/python3.6/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nUnimplementedError: From /job:worker/replica:0/task:0:\r\n File system scheme '[local]' not implemented (file: '/gdrive/My Drive/plant-pathology-2020-fgvc7/images/Train_187.jpg')\r\n\t [[{{node ReadFile}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNextAsOptional_9]]\r\n\r\n```", "comments": ["@Mallow15, can you share full code or link of colab file to reproduce mentioned error.", "https://colab.research.google.com/drive/1XRTvryTN6IzJOyLsye_4FcwYJ4c6aur_", "@Mallow15, please share correct link as above link is not working.", "sry,\r\nhttps://colab.research.google.com/drive/1XRTvryTN6IzJOyLsye_4FcwYJ4c6aur_", "please here underscore is not considered as a link\r\njust put it at the end of the link", "Just find **#train model** to find above code with the error", "@Mallow15, As i can see you have defined validation data as `valid_dataset` but you are passing as `validation_dataset` in `evaluate` method. So replace\r\n`final_stats = model.evaluate(validation_dataset, steps=1)`\r\nby\r\n`final_stats = model.evaluate(valid_dataset, steps=1)`.\r\nHope it helps.", "I changed it eventhough it produces same error\r\n", "You just see the link it shows the error \r\n", "@Mallow15, please refer this [issue](https://github.com/tensorflow/datasets/issues/486).", "@Mallow15 \r\n\r\nAny update on this issue please. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37815\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37815\">No</a>\n"]}, {"number": 37814, "title": "import_pb_to_tensorboard.py OSError", "body": "`python import_pb_to_tensorboard.py --model_dir C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/mnist_model_graph.pb --log_dir /tmp/tensorflow_logdir\r\nOSError: SavedModel file does not exist at: C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/mnist_model_graph.pb`\r\nBut I'm pretty sure that I put it in this path, how can I solve this problem?", "comments": ["If you are using TF 2.1, the `import_pb_to_tensorboard.py` expect `--model_dir` point to the location of SavedModel dir (not the `.pb` file).  The script will search `saved_model.pb` or `saved_model.pbtxt` in model dir. see #37529", "> If you are using TF 2.1, the `import_pb_to_tensorboard.py` expect `--model_dir` point to the location of SavedModel dir (not the `.pb` file). The script will search `saved_model.pb` or `saved_model.pbtxt` in model dir. see #37529\r\n\r\nHi, I'm using TF 1.14 and I do as you said, but I still got the same OSerror:\r\n`python import_pb_to_tensorboard.py --model_dir C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite --log_dir /log\r\nOSError: SavedModel file does not exist at: C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite`\r\nWhat am I doing wrong?", "@LAWSSSS I have read the code of TF 1.14, it's different from TF 2.0, you should pass the path of `.pb` file as `--model_dir`.", "@howl-anderson I did as you said but the error still showed up:\r\n`python import_pb_to_tensorboard.py --model_dir C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/mnist_model_graph.pb --log_dir /tmp/tensorflow_logdir`\r\n`OSError: SavedModel file does not exist at: C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/mnist_model_graph.pb`\r\nHowever, this code using the same model path works out just fine:\r\n```\r\nimport os\r\nfrom tensorflow.python.tools.import_pb_to_tensorboard import import_to_tensorboard\r\nmodel_dir = r'C:\\Users\\LAWSSSS\\Desktop\\convert_pb_2_tflite'\r\nmodel = os.path.join(model_dir, 'mnist_model_graph.pb')\r\nprint(model)\r\n\r\nimport_to_tensorboard(model_dir=model, log_dir='log_test/')\r\n```\r\nSince these two model paths are the same, I can't figure out what goes wrong in the first measure.", "@LAWSSSS The error msg `SavedModel file does not exist at ...` shows that you are NOT using tf 1.14, probably one of TF 2.x , please double check it.", "@howl-anderson  Sorry for the late response and thanks for the pointer.  I've checked the TF version, and I'm sure that it's TF 1.14.0. Any insight on this problem? ", "@LAWSSSS In your `python import_pb_to_tensorboard.py --model_dir C:/Users/LAWSSSS/Desktop/convert_pb_2_tflite/mnist_model_graph.pb --log_dir /tmp/tensorflow_logdir` where the file `import_pb_to_tensorboard.py` come from?", "@howl-anderson Thanks a lot. Following your instruction, I finally solve my problem. The `import_pb_to_tensorboard.py` is from a blog, not my original installed tensorflow folder. After replacing it with my own installed tensorflow file, the problem is solved."]}, {"number": 37813, "title": "[Autograph] Fix loop else support and add loop integration test", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nRight now, the autograph does not support loop else syntax like\r\n```python\r\nwhile x > 2:\r\n  x /= 2\r\nelse:\r\n  x += 1\r\n```\r\nIn fact, it would just neglect the else part, because the tf functional while op does not support \"else\" (which, in my opinion, is good, since the loop else is a rare syntax sugar that probably only appears in python). Therefore, this pr changed the loop from\r\n```python\r\nwhile test:\r\n  body\r\nelse:\r\n  orelse\r\n```\r\nto\r\n```python\r\nwhile test:\r\n  body\r\norelse\r\n```\r\nAnd for those loops that have `break` statements involve\r\n```python\r\nvar_name = False\r\nwhile ag__.and_(lambda: test, lambda: ag__.not_(var_name)):\r\n  body\r\nif ag__.not_(var_name):\r\n  orelse\r\n```\r\nFor the test, it turns out that the UT for break statement only could not detect this problem. In fact, the `test_loop_orelse` in `break_statement_test.py` does not work in current autograph but have passed the UT. Therefore, we added a integration test for the loop syntax, which will run the break_statement, continue_statement and control_flow converters one by one.\r\n\r\nThank you for your time on this review.", "comments": ["@mdanatg Thank you so much for your quick response! I haven't even finish my comment... As for the comment in break statement, break in for loop will still take effect only after the control_flow converter, so I think the UT makes sense and therefore didn't touch it.", "@mdanatg Could you help me merge this pr? thx!", "Sorry for the delay. The PR tripped a few internal tests. It looks like some changes got lost in the merge."]}, {"number": 37812, "title": "redefinition of cusolverStatus_t in tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc at lines 192 and 202", "body": "I tried building TF on my system (ubuntu x86_64, cuda 10.2). I came across this error after about 15 mins of compiling. \r\n\r\n<em>In file included from tensorflow/stream_executor/cuda/cusolver_stub.cc:61:0:\r\n./tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc: In function 'cusolverStatus_t cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t, cusolverDnIRSInfos_t, cusolver_int_t*)':\r\n./tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc:202:30: error: redefinition of 'cusolverStatus_t cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t, cusolverDnIRSInfos_t, cusolver_int_t*)'\r\n cusolverStatus_t CUSOLVERAPI cusolverDnIRSInfosGetNiters(\r\n                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc:192:30: note: 'cusolverStatus_t cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t, cusolverDnIRSInfos_t, cusolver_int_t*)' previously defined here\r\n cusolverStatus_t CUSOLVERAPI cusolverDnIRSInfosGetNiters(\r\n                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n</em>\r\nTwo definitions are right one after the other. Both functions seem to be identical. I am going to delete one of the definitions, and restart the build. Keeping my fingers crossed.", "comments": ["@ranavjk,\r\nPlease Provide the exact sequence of commands / steps that you executed before running into the problem and also provide the Tensorflow version which you are trying to build. Thanks!", "@gadagashwini \r\n1. set up config (./configure) -> all defaults except for cuda=yes and tensorRT=yes.\r\n2. When asked for cuda paths: cuda path = /usr/local/cuda-10.2, cudnn path = /usr/include,/usr/lib, tensorRT path =/usr/local/tensorRT \r\n3. bazel build --config=cuda --verbose_failures //tensorflow/tools/lib_package:libtensorflow\r\n4. As I pointed earlier, function `cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t, cusolverDnIRSInfos_t, cusolver_int_t*)` is defined twice on line#192 and 202 in the file `tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc` . I deleted one of those functions and build went fine from there. I did get a linking error at the end. Will work on that today.\r\n5. Tensorflow version: 2", "Can confirm this error. ", "Fixed here: https://github.com/tensorflow/tensorflow/pull/37843", "Fix is merged in master. Feel free to close the issue. ", "@ranavjk, Associated PR has been merged can you close this issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37812\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37812\">No</a>\n"]}, {"number": 37811, "title": "GET returned 401 Unauthorized for  repository 'upb", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nmacox 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource (github head)\r\n- TensorFlow version:\r\nmaster\r\n- Python version:\r\n2..7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n2.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n\r\n```\r\nINFO: Call stack for the definition of repository 'upb' which is a http_archive (rule definition at /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/com_github_grpc_grpc/bazel/grpc_deps.bzl:228:9\r\n - /Users/etsang/dev/src/github.com/tensorflow/tensorflow/WORKSPACE:121:1\r\nWARNING: Download from https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 401 Unauthorized\r\nERROR: An error occurred during the fetch of repository 'upb':\r\n   java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: GET returned 401 Unauthorized\r\nERROR: no such package '@upb//bazel': java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: GET returned 401 Unauthorized\r\nERROR: no such package '@upb//bazel': java.io.IOException: Error downloading [https://github.com/protocolbuffers/upb/archive/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz] to /private/var/tmp/_bazel_etsang/4c08b338ce3f5893c94fb26f4e062082/external/upb/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz: GET returned 401 Unauthorized\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\nanswer the questions interactively prompted form ./configure\r\nbazel build (no target to see what happens)\r\nErrorred out\r\nI already have .netrc setup under my home folder with the correct credentials\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["full error. It turned out it is personal token. github remove login and password for rest request. So just edit .netrc to use personal token instead of login password works. please document in faq accordingly", "In that case, can we close issue?", "yes please", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37811\">No</a>\n"]}, {"number": 37810, "title": "Fix filters with 0 dimensions in conv and conv_transpose", "body": "Fix #37764. Made modifications in python end. More detail can be seen in the issue page.", "comments": ["> Why do we need this? In many other cases 0 dimensions are supported in TF.\r\n\r\nIf so, the conv layer in keras api is not consistent, as changed in this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/1e102f63964365d82d7f22402b7ba21e0e0e64fe"]}, {"number": 37809, "title": "import error", "body": "DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["please use a perfect format or code to report\r\n", "Please fill in issue template on future issue reports. Also, please check for duplicates.\r\n\r\nClosing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37809\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37809\">No</a>\n"]}, {"number": 37807, "title": "Broken link", "body": "## URL(S) with issue:\r\nhttps://github.com/tensorflow/federated/blob/master/docs/federated_core.md\r\nIn the above readme file [MapsReduce](https://research.google/pubs/pub62.pdf/)\r\n\r\n### Pull Request\r\nI've corrected the link and by successfully merging [#813](https://github.com/tensorflow/federated/pull/813) it'll resolve this issue.\r\nHey, @mihaimaruseac Kindly review it.", "comments": ["@abhinavsp0730 As mentioned by @MarkDaoust [here](https://github.com/tensorflow/tensorflow/issues/37783#issuecomment-602776176), please mention the following in the description of the PR\r\n\r\n`fixes: https://github.com/tensorflow/tensorflow/issues/37807` which closes this issue automatically when the corresponding PR gets merged. Thanks!\r\n", "Done", "@abhinavsp0730 I see this issue of broken link was corrected already. Can you verify once and close the issue? thanks! ", "Hey, @jvishnuvardhan I've linked a PR with this issue, Even though it's merged successfully but don't know why this issue hasn't been closed automatically. Would please give me some insight in this.\r\nNevertheless closing this issue. "]}, {"number": 37805, "title": "How to force TF Eigen to use openBLAS library for GEMM calls?", "body": "Hi,\r\nI am trying to link OpenBLAS to Tensorflow Eigen, Basically I want to override Eigen gemm call to OpenBLAS gemm call using below command which is not working.\r\n\r\nCommand to building TF.\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --copt=-DEIGEN_USE_BLAS --linkopt=-L<path to BLAS> --linkopt=-lopenblas //tensorflow/tools/pip_package:build_pip_package   \r\n\r\nI did't find anything with TF documentation. Is there any way to achieve the same?\r\n", "comments": ["Please, refer #8350 and see if it helps you.This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37804, "title": "Failed to load the native TensorFlow runtime", "body": "\r\n**System information**\r\n- OS Platform and Distribution MAC version 10.10.5 \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.1\r\n- Python version: 3.7\r\n- Installed using  conda :\r\n- CUDA/cuDNN version: NVIDIA GeForce GT 330M 256 Mo\r\n- GPU model and memory: mi-2010, \r\n\r\n**Describe the problem**\r\n\r\nWhen i try to execut : import tensorflow as tf\r\n\r\ni have :\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/Marot/miniconda3/envs/chatbot/lib/python3.5/site-packages/tensorflow_core/python/../libtensorflow_framework.2.dylib\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nCan you help me.\r\n\r\nplease?\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "\r\nHey,\r\nI instaled tensorflow with :\r\n\r\nhttps://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.8.0-py3-none-any.whl\r\n\r\nAfter that a friend advised me to use this comand \r\n\r\nconda create -n tensorflow pip python=3\r\n\r\nAnd now it work now,\r\n\r\nThank you very must for your answer\r\n\r\nHave a good day\r\n\r\n", "@elyvision, \r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37804\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37804\">No</a>\n"]}, {"number": 37803, "title": "fix TFE_OpReset doesn't clear inputs.", "body": "I've got below error when running `c_api.TFE_Execute(op, retVals, ref num_retvals, status);`.\r\n```shell\r\nNodeDef expected inputs 'int32, int32' do not match 4 inputs specified; Op<name=Sub; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]>; NodeDef: {{node Sub}}\r\n```\r\nThe `op` is from reused and reset `op`. \r\nAfter investigating, I've found the `op` is not reset completed, at least the `inputs` is not clean.\r\n\r\n![image](https://user-images.githubusercontent.com/1705364/77254033-7a091d80-6c2c-11ea-8fe7-7448736713f9.png)\r\n\r\nIt went through after I added `op_to_reset->operation->Clear();`\r\n\r\nCould reproduce this issue from this [example](https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/BasicOperations.cs). I'm not sure if this is the appropriate fix for it.", "comments": ["Please open against master and then we can cherry-pick.\r\n\r\nAlso, please add unit test", "I think you can cherry-pick from `r2.2` to `master` as well.", "No, our procedure is `master` -> `r...` branch. We prefer to not mix the branches", "To give more context: code is integrated inside Google from the `master` branch. There is internal code at Google that only works with the `master` branch.\r\n\r\nWhen we do a release, we cut a `r...` branch and __only__ do cherry-picks to the branch to fix build/incomplete features.\r\n\r\nOnce release is done, we **don't** merge the branch back into master. Instead, we keep it there and if there is need for a patch release (usually only due to security vulnerabilities), we use the branch again.\r\n\r\nAll cherry-picks to the branch follow a strict process that involves both the branch and `master`.\r\n\r\nDue to all of the above reasons, we cannot accept PRs targetting other branches if they are not cherry-picking things in master (or exceptional cases, such as `r1.15` branch when code on that branch no longer exists on `master`)", "I'll close one and PR to `master` next time."]}, {"number": 37802, "title": "Cannot build TensorFlow 2.2 branch on Windows 10", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 Pro x64\r\n- Mobile device: None\r\n- TensorFlow installed from (source or binary): sources\r\n- TensorFlow version: branch r2.2, commit acf4951a2f5fdc181ed14c163381c0cf135d9ee6\r\n- Python version: 3.7.1\r\n- Installed using virtualenv? pip? conda?: virtualenv with pip, without conda\r\n- Bazel version (if compiling from source): the default version downloaded by Bazelisk\r\n- GCC/Compiler version (if compiling from source): MSVS 2019 v16.5.0\r\n- CUDA/cuDNN version: 10.2 / 7.6.5.32\r\n- GPU model and memory: Geforce GTX 1080\r\n\r\n**Describe the problem**\r\n\r\nCompilation stops with the following error:\r\n```\r\nERROR: D:/libs/ai_ml/tensorflow/tensorflow/lite/tools/optimize/BUILD:115:1: C++ compilation of rule '//tensorflow/lite/tools/optimize:operator_property' failed (Exit 2)\r\ntensorflow/lite/tools/optimize/operator_property.cc(207): error C2679: binary '=': no operator found which takes a right-hand operand of type 'initializer list' (or there is no acceptable conversion)\r\n.\\tensorflow/lite/tools/optimize/operator_property.h(34): note: could be 'tflite::optimize::operator_property::DerivedScale &tflite::optimize::operator_property::DerivedScale::operator =(tflite::optimize::operator_property::DerivedScale &&) noexcept(<expr>)'\r\n.\\tensorflow/lite/tools/optimize/operator_property.h(34): note: or       'tflite::optimize::operator_property::DerivedScale &tflite::optimize::operator_property::DerivedScale::operator =(const tflite::optimize::operator_property::DerivedScale &)'\r\ntensorflow/lite/tools/optimize/operator_property.cc(207): note: while trying to match the argument list '(tflite::optimize::operator_property::DerivedScale, initializer list)'\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\npython ./configure.py\r\nset path=C:\\Programs\\Build;C:\\msys64\\usr\\bin;%PATH%\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"\r\nset BAZEL_VC=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\"\r\nbazel build --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nNone\r\n", "comments": ["Can you build master instead? Trying to identify differences", "@mihaimaruseac , thanks for the suggestion - I'm trying this now, will keep you posted. Right from the beginning the problem with master branch is that `python ./configure.py` throws an exception in windows command line because the length of the command exceeds 32K characters. I workaround this by means of msys2. But I think it's better to reduce the command line so that `cmd.exe` can be used too.", "The master branch produces the following error for me:\r\n```\r\nERROR: D:/libs/ai_ml/tensorflow/tensorflow/stream_executor/cuda/BUILD:425:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cusolver_stub' failed (Exit 2)\r\n.\\tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc(204): error C2084: function 'cusolverStatus_t cusolverDnIRSInfosGetNiters(cusolverDnIRSParams_t,cusolverDnIRSInfos_t,cusolver_int_t *)' already has a body\r\n.\\tensorflow/stream_executor/cuda/cusolver_dense_10_2.inc(192): note: see previous definition of 'cusolverDnIRSInfosGetNiters'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nUPDATE: This looks like a merge issue. I've removed one of the 2 identical definitions and trying the compilation again.", "On master branch the compilation hangs for more than 10 hours:\r\n```\r\n[4,094 / 5,097] 16 actions running\r\n    Compiling tensorflow/core/kernels/cwise_op_sub.cc; 38326s local\r\n    Compiling tensorflow/core/kernels/cwise_op_div.cc; 37932s local\r\n    Compiling tensorflow/core/kernels/cwise_op_equal_to_1.cc; 37805s local\r\n    Compiling tensorflow/core/kernels/cwise_op_igammas.cc; 24395s local\r\n    Compiling tensorflow/core/kernels/cwise_op_left_shift.cc; 18171s local\r\n    Compiling tensorflow/core/kernels/cwise_op_less.cc; 18112s local\r\n    Compiling tensorflow/core/kernels/cwise_op_less_equal.cc; 15807s local\r\n    Compiling tensorflow/core/kernels/cwise_op_greater_equal.cc; 14404s local ...\r\n```", "Unfortunately, they are not hanging. Some kernels do take that long to compile.\r\nOne way to make it faster is to add the `--define=override_eigen_strong_inline=true` flag to your bazel command.\r\n\r\nFor me, the builds have been healthy, but I will retry.", "After I let these compilations run longer, I'm getting the following error:\r\n```\r\nERROR: D:/libs/ai_ml/tensorflow/tensorflow/core/kernels/BUILD:4084:1: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit -1)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 18869.605s, Critical Path: 18103.23s\r\nINFO: 1730 processes: 1730 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nNo details about any compilation error are provided above these lines.", "Finally, after updating to a later revision of master branch and disabling inlining in configure.py, I was able to build tensorflow, however, the following command then fails:\r\n```\r\n(BuildTF) D:\\Libs\\ai_ml\\tensorflow>bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package D:/Libs/ai_ml/Built/tensorflow_pkg\r\nTue Mar 24 15:29:07 BST 2020 : === Preparing sources in dir: /tmp/tmp.IxVMcIFyRl\r\nUnzipping simple_console_for_windows.zip to create runfiles tree...\r\n[./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]\r\n  End-of-central-directory signature not found.  Either this file is not\r\n  a zipfile, or it constitutes one disk of a multi-part archive.  In the\r\n  latter case the central directory and zipfile comment will be found on\r\n  the last disk(s) of this archive.\r\nunzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or\r\n        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.\r\n```", "I managed to workaround the above issue with https://stackoverflow.com/questions/52394305/creating-pip-package-for-tensorflow-with-gpu-support-results-in-0-byte-simple-co . However, I had to delete a lot of `.zip` files from getting into `simple_console_for_windows.zip`. Can someone elaborate on the consequences of deleting those files?", "yeah, the above issue you saw is also known.\r\nThe zip file gets larger than 2GB, and zip fails. Unfortunately, bazel does not report an actionable issue for us there.\r\nfor building the pip package, bazel team helped us create some flags where we can use only for building the pip package, which does not build some of those zip files. so as long as your pip package is working, you should be ok.\r\n", "@srogatch I have built TensorFlow 2.2 rc3 now successfully on Windows 10 with both Python 3.8 and 3.7, also CUDA 10.1 and CUDA 10.2. Without any tweaks to the source.\r\n\r\nPosted some notes on building to https://github.com/tensorflow/tensorflow/issues/38174#issuecomment-613722488 that might help with your build speed.\r\n\r\nDo you also get the 2GB zip issue with a no_tensorflow_py_deps flag?\r\n`bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`", "@ahtik , thanks for following up. Unfortunately, currently, Tensorflow experiments are not relevant to me. You can close the issue. And the command you gave should be a good resolution.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37802\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37802\">No</a>\n"]}, {"number": 37801, "title": "Remove expired forward compatibility horizons", "body": "This PR removes expired forward compatibility statements that always evaluate to `True`.", "comments": []}]