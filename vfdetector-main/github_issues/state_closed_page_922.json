[{"number": 25794, "title": "[TF 2.0 API Docs] tf.AggregationMethod", "body": "### Existing URLs containing the issue:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/AggregationMethod\r\n\r\n### Description of issue (what needs changing):\r\n\r\n* **Correct Links**\r\nhttps://github.com/tensorflow/tensorflow/blob/master/python/ops/gradients_util.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_util.py.\r\n\r\n* **Clear Description**\r\nThe description is not opinionated about when to use this symbol, and unclear on what aggregation methods for combining gradients would be useful for. \r\n\r\n* **Usage Example**\r\nNo usage example is provided.\r\n\r\n* **Parameters Defined**\r\nParameters are poorly defined, and not formatted appropriately.\r\n\r\n* **Returns Defined**\r\nReturns are not defined.\r\n\r\n* **Raises Listed and Defined**\r\nErrors are not defined.\r\n\r\n* **Visuals, if Applicable**\r\nNo visuals are included.", "comments": ["@dynamicwebpaige I'm interested in working on this issue!", "@dynamicwebpaige Can I please work on this?", "@Rajiv2605 @Ayush517 - Absolutely! Any contributions that improve our API docs are extremely welcome. \ud83d\ude0a\r\n", "@dynamicwebpaige \r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/AggregationMethod\r\nWhile opening the link on this page, I am getting redirected to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_util.py which is the correct link mentioned in the issue. It's working fine I think.\r\n\r\nSeems like most of [these issues](https://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+label%3Atype%3Adocs+%5BTF+2.0+API+Docs%5D+is%3Aopen) with broken links were solved.", "Is this issue still open? as the status is still open here can should I work on it? @dynamicwebpaige ", "How to use tf.AggregationMethod in tf2.0. \r\nIn tf1.0 it can be passthrough the \r\noptimizer.minimize(loss, method_aggregation =  tf.AggregationMethod.EXPERIMANTAL_TREE)\r\nAnd it can help if GPU is OOM.\r\nBut how to use that in TF 2.0?", "Hi @dynamicwebpaige ,\r\n\r\nIs this issue still Open? Asking so that I can work on it?\r\n\r\nThanks and Gratitude!", "Hi @chinmoysarangi, \r\n\r\nPaige has a [lot of these bugs](https://www.tensorflow.org/community/contribute/docs_ref), I doubt she working on this particular one right now.\r\n\r\nAlso if you're working on api pages be sure to have a look at [this doc](https://www.tensorflow.org/community/contribute/docs_ref).\r\n\r\n\r\n", "Is the issue still open? I'm interested to contribute", " can I work on this issue?", "The links on this page are fixed."]}, {"number": 25793, "title": "Disable TensorRT and fix some GPU typos", "body": "This disables TensorRT in the devel builds, which currently can't build\nin Docker's environment for some reason, and fixes some additional typos\nin the GPU devel build setup.\n\nFYI @tjakob, @tfboyd.", "comments": []}, {"number": 25792, "title": "Prefer literals over list and dict constructors", "body": "This is just some search and replace to improve readability:\r\n- `d = dict()` --> `d = {}`\r\n- `l = list()` --> `l = []`\r\n\r\nSome nice benefit of this is that literals in Python are quite a bit faster (though only 60 ns - 75 ns):\r\n```shell\r\nIn [1]: %timeit d = dict()\r\n101 ns \u00b1 0.0829 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\r\n\r\nIn [2]: %timeit dl = {}\r\n34.1 ns \u00b1 0.378 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\r\n\r\nIn [3]: %timeit l = list()\r\n101 ns \u00b1 0.541 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\r\n\r\nIn [4]: %timeit l = []\r\n25.4 ns \u00b1 0.346 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\r\n```", "comments": ["@lgeiger I think you don't need to request the entire team to review your PR next time, just let one of our people watching the PR queue assign you an appropriate reviewer :-)\r\n\r\nThat said, thanks for the change!", "> I think you don't need to request the entire team to review your PR next time, just let one of our people watching the PR queue assign you an appropriate reviewer :-)\r\n\r\nYes, I'm really sorry for the noise. It wasn't me, it was the bot \ud83e\udd16\r\nI don't even have access to request reviews from the tensorflow team.\r\n\r\nThe bot probably requested reviews from every [code owner](https://github.com/tensorflow/tensorflow/blob/master/CODEOWNERS) since this trivial change touches so many file.", "Rebased!"]}, {"number": 25791, "title": "1.13.0-rc2 cherry-pick request: Don't initialize tpu system twice for mesh-tensorflow.", "body": "PiperOrigin-RevId: 232234083\r\n", "comments": ["I don't think this is relevant any more so closing."]}, {"number": 25790, "title": "Incompatibility between keras and tf.keras for layer.build", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.0rc1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nHere is the code below which should work (it works with keras-team/keras).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code should run as expected, instead, we get the error message below. Note that changing the flag at the begining of the script makes the code work.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nuse_tf_keras = True\r\nif use_tf_keras:\r\n    from tensorflow.keras.layers import Layer, Dense\r\n    from tensorflow.keras import backend as K\r\n    from tensorflow.keras import Sequential\r\nelse:\r\n    from keras.layers import Layer, Dense\r\n    from keras import backend as K\r\n    from keras import Sequential\r\n\r\n\r\nclass MyLayer(Layer):\r\n\r\n  def build(self, input_shape):\r\n    print(type(input_shape))\r\n    input_dim = input_shape[-1]\r\n    output_dim = input_shape[-1] / 2\r\n\r\n    self.kernel = self.add_weight(shape=(input_dim, output_dim),\r\n                                  name='kernel',\r\n                                  initializer='ones')\r\n    super().build(input_shape)\r\n\r\n  def call(self, inputs):\r\n    return K.dot(inputs, self.kernel)\r\n\r\n  def compute_output_shape(self, input_shape):\r\n    input_shape = list(input_shape)\r\n    input_shape[-1] = input_shape[-1] // 2\r\n    return input_shape\r\n\r\nmodel = Sequential()\r\nmodel.add(Dense(8, input_shape=(20, 20)))\r\nmodel.add(MyLayer())\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n<class 'tensorflow.python.framework.tensor_shape.TensorShapeV1'>\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-255eda5c5005> in <module>()\r\n     32 model = Sequential()\r\n     33 model.add(Dense(8, input_shape=(20, 20)))\r\n---> 34 model.add(MyLayer())\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _method_wrapper(self, *args, **kwargs)\r\n    440     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    441     try:\r\n--> 442       method(self, *args, **kwargs)\r\n    443     finally:\r\n    444       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)\r\n    178       # If the model is being built continuously on top of an input layer:\r\n    179       # refresh its output.\r\n--> 180       output_tensor = layer(self.outputs[0])\r\n    181       if isinstance(output_tensor, list):\r\n    182         raise TypeError('All layers in a Sequential model '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    536       if not self.built:\r\n    537         # Build layer if applicable (if the `build` method has been overridden).\r\n--> 538         self._maybe_build(inputs)\r\n    539         # We must set self.built since user defined build functions are not\r\n    540         # constrained to set self.built.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   1601     # Only call `build` if the user has manually overridden the build method.\r\n   1602     if not hasattr(self.build, '_is_default'):\r\n-> 1603       self.build(input_shapes)\r\n   1604 \r\n   1605   def __setattr__(self, name, value):\r\n\r\n<ipython-input-2-255eda5c5005> in build(self, input_shape)\r\n     15     print(type(input_shape))\r\n     16     input_dim = input_shape[-1]\r\n---> 17     output_dim = input_shape[-1] / 2\r\n     18 \r\n     19     self.kernel = self.add_weight(shape=(input_dim, output_dim),\r\n\r\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'int'\r\n```\r\n\r\nif using keras-team/keras, we get:\r\n```\r\n<class 'tuple'>\r\n```\r\n\r\n", "comments": ["The issue is that `Dimension` only defines a `__div__`, not `__rdiv__` (or `__truediv__`/`__rtruediv__`):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_shape.py#L455\r\n\r\nI think adding `__rdiv__` will fix the issue of `TypeError: unsupported operand type(s) for /`.\r\n\r\nHowever, in the above code in tensorflow, it also mentioned that `__div__` is deprecated (in favor of `__floordiv__` or `//`). Actually, changing:\r\n`output_dim = input_shape[-1] / 2` to `output_dim = input_shape[-1] // 2` works.\r\n\r\nI think it probably makes sense to change `/` to `//`?\r\n\r\nOr, maybe it also makes sense to explicitly thrown out a better error message in `__div__`/`__rdiv__`?", "Created a PR #25821 to print a better error message in case the operation is not supported.", "Closing this issue since the associated PR has been merged. \r\nAlso can see clear error message with latest TF (tested with tf-nightly 1.15.0-dev201907250)\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25790\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25790\">No</a>\n"]}, {"number": 25789, "title": "Tensorflow 1.12.0 not working with CUDA 10.0", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n- TensorFlow version: version 1.12.0\r\n- Python version: 3.5.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: CUDA version 10.0\r\n- GPU model and memory: Tesla V100\r\n\r\nwhen I just try to import tensorflow I get this error message:\r\n\r\n\r\n```\r\nPython 3.5.5 |Anaconda custom (64-bit)| (default, May 13 2018, 21:12:35)\r\n[GCC 7.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/anaconda/envs/py35/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/anaconda/envs/py35/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/anaconda/envs/py35/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/anaconda/envs/py35/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["In order to use TensorFlow 1.12 with CUDA 10 you need to build TensorFlow from source. \r\nhttps://www.tensorflow.org/install/source\r\n\r\nYou may also try the 1.13.0rc1 release candidate build which is built with CUDA 10.\r\n\r\n`pip install tensorflow-gpu==1.13.0rc1`", "Closing this issue since the wdiron's explanation is correct. Feel free to post a new issue if having problems  in installing TF 1.13.0-rc1/1.13.0-rc2 both are built with CUDA 10. Thanks!", "Hello, Tensorflow 1.14 worked for me! wouldn't suggest to use release canditate (rc) version. \r\n\r\n> pip install tensorflow-gpu==1.14"]}, {"number": 25788, "title": "image shift and interpolation", "body": "I am trying to shift all images in a batch and have 'NEAREST' interpolation. For some reason, the resulting images are interpolated with black pixels... Is this a bug?\r\n\r\n`\r\n translations = [100,0] * len(img_batch_list)\r\n img_batch = tf.contrib.image.translate(img_batch,\r\n                                        translations,\r\n                                        interpolation='NEAREST',\r\n                                        name=\"shift\")\r\n`", "comments": ["Please take a look at [similar issue](https://github.com/tensorflow/tensorflow/issues/25591). Probably this will help.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25787, "title": "Warning or error about incompatible device when I use tf.nn.rnn_cell.DropoutWrapper on multiple GPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.11.0 and 1.12.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: 9.0/7.3.0\r\n- GPU model and memory: nvidia 1080ti, 11GB\r\n\r\n**Describe the current behavior**\r\nI write a very simple 2 layers LSTM model working on 2 GPUs. The model has no specific meaning, and I just want to test LSTM network. I randomly generalize some data as inputs, and take reduced sum of logits as loss. Then I compute the gradients and loss. I would get some warnings like below. And the same situation happens when I run [TensorFlow Neural Machine Translation Tutorial](https://github.com/tensorflow/nmt/tree/tf-1.4), which is the reason why I tested this simple code.\r\n\r\n```\r\nWARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.\r\n\r\nNode-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation:\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>\r\nNo device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation.\r\n\r\nNo node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.\r\nDevice assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:\r\n  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>\r\nWARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.\r\n```\r\nIf I don't set `allow_soft_placement=True`, I would get error.\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'\r\n\t [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[\"loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2\", \"loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div\"], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=\"/device:GPU:0\"]()]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nget no warning or make sure that this situation would affect result or performance.\r\n\r\n**Code to reproduce the issue**\r\n**The warning only emerges when I use `tf.nn.rnn_cell.DropoutWrapper`.**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.layers import core as layers_core\r\n\r\n\r\ndata_x = np.random.standard_normal([100, 5, 1000])\r\n\r\nsingle_cell_0 = tf.nn.rnn_cell.LSTMCell(1000, forget_bias=1)\r\nsingle_cell_0 = tf.nn.rnn_cell.DropoutWrapper(cell=single_cell_0, input_keep_prob=0.8, seed=285)\r\nsingle_cell_0 = tf.contrib.rnn.DeviceWrapper(single_cell_0, \"/gpu:0\")\r\n\r\nsingle_cell_1 = tf.nn.rnn_cell.LSTMCell(1000, forget_bias=1)\r\nsingle_cell_1 = tf.nn.rnn_cell.DropoutWrapper(cell=single_cell_1, input_keep_prob=0.8, seed=285)\r\nsingle_cell_1 = tf.contrib.rnn.DeviceWrapper(single_cell_1, \"/gpu:1\")\r\n\r\ncell = tf.contrib.rnn.MultiRNNCell([single_cell_0, single_cell_1])\r\n\r\ntarget_input = tf.placeholder(dtype=tf.float32, shape=[None, 5, 1000], name=\"input\")\r\n\r\noutput, final_state = tf.nn.dynamic_rnn(cell,\r\n                                        target_input,\r\n                                        dtype=tf.float32,\r\n                                        time_major=True)\r\noutput = tf.reshape(output, [-1, 5000])\r\nwith tf.device(\"/gpu:2\"):\r\n    logits = layers_core.Dense(100, use_bias=False, name=\"output_projection\")(output)\r\n    loss = tf.reduce_sum(logits)\r\n\r\nparams = tf.trainable_variables()\r\nfor param in params:\r\n    print(\"  %s, %s, %s\" % (param.name, str(param.get_shape()), param.op.device))\r\ngrad = tf.gradients(loss, params, colocate_gradients_with_ops=True)\r\n\r\nconfig_proto = tf.ConfigProto(\r\n    log_device_placement=False,\r\n    allow_soft_placement=False)\r\nconfig_proto.gpu_options.allow_growth = True\r\nwith tf.Session(config=config_proto) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    g, l = sess.run([grad, loss], {target_input: data_x})\r\n    print(g, l)\r\n```\r\n\r\n**Other info / logs**\r\nFull info with warning\r\n```\r\n  rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0, (2000, 4000), /device:GPU:0\r\n  rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0, (4000,), /device:GPU:0\r\n  rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0, (2000, 4000), /device:GPU:1\r\n  rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0, (4000,), /device:GPU:1\r\n  output_projection/kernel:0, (5000, 100), /device:GPU:2\r\nWARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.\r\n\r\nNode-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation:\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>\r\nNo device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation.\r\n\r\nNo node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.\r\nDevice assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:\r\n  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>\r\nWARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.\r\n\r\nNode-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation:\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>\r\nNo device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation.\r\n\r\nNo node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.\r\nDevice assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:\r\n  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>\r\n2019-02-16 00:15:18.804994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:18:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2019-02-16 00:15:19.010291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3b:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2019-02-16 00:15:19.227656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:86:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2019-02-16 00:15:19.408641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:af:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.36GiB\r\n2019-02-16 00:15:19.415867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-02-16 00:15:20.680526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-16 00:15:20.680570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 3 \r\n2019-02-16 00:15:20.680576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y Y \r\n2019-02-16 00:15:20.680595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y Y \r\n2019-02-16 00:15:20.680599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N Y \r\n2019-02-16 00:15:20.680603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 3:   Y Y Y N \r\n2019-02-16 00:15:20.681368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10390 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:18:00.0, compute capability: 6.1)\r\n2019-02-16 00:15:20.682959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10398 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1)\r\n2019-02-16 00:15:20.684307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10398 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:86:00.0, compute capability: 6.1)\r\n2019-02-16 00:15:20.685688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10013 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:af:00.0, compute capability: 6.1)\r\n```\r\n\r\nFull info with error, no `allow_soft_placement=True`.\r\n```\r\n  rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0, (2000, 4000), /device:GPU:0\r\n  rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0, (4000,), /device:GPU:0\r\n  rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0, (2000, 4000), /device:GPU:1\r\n  rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0, (4000,), /device:GPU:1\r\n  output_projection/kernel:0, (5000, 100), /device:GPU:2\r\nWARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' (defined at test.py:102) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:93) which had an incompatible device '/device:GPU:1'.\r\n\r\nNode-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation:\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>\r\nNo device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation.\r\n\r\nNo node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.\r\nDevice assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:\r\n  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>\r\nWARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' (defined at test.py:102) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:93) which had an incompatible device '/device:GPU:1'.\r\n\r\nNode-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation:\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>\r\n  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>\r\nNo device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation.\r\n\r\nNo node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.\r\nDevice assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:\r\n  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>\r\n2019-02-16 00:36:24.357307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:18:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2019-02-16 00:36:24.561462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3b:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2019-02-16 00:36:24.760426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:86:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.75GiB\r\n2019-02-16 00:36:24.941042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:af:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.36GiB\r\n2019-02-16 00:36:24.948169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-02-16 00:36:26.126392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-16 00:36:26.126439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 3 \r\n2019-02-16 00:36:26.126446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y Y \r\n2019-02-16 00:36:26.126451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y Y \r\n2019-02-16 00:36:26.126455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N Y \r\n2019-02-16 00:36:26.126459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 3:   Y Y Y N \r\n2019-02-16 00:36:26.127293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10390 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:18:00.0, compute capability: 6.1)\r\n2019-02-16 00:36:26.128911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10398 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1)\r\n2019-02-16 00:36:26.130298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10398 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:86:00.0, compute capability: 6.1)\r\n2019-02-16 00:36:26.131678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10013 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:af:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\r\n    return fn(*args)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1275, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1312, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'\r\n\t [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[\"loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2\", \"loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div\"], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=\"/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 109, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'\r\n\t [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[\"loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2\", \"loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div\"], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=\"/device:GPU:0\"]()]]\r\n\r\nCaused by op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const', defined at:\r\n  File \"test.py\", line 102, in <module>\r\n    grad = tf.gradients(loss, params, colocate_gradients_with_ops=True)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 776, in _GradientsHelper\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 398, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 776, in <lambda>\r\n    lambda: grad_fn(op, *out_grads))\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py\", line 972, in _RealDivGrad\r\n    grad * math_ops.realdiv(math_ops.realdiv(-x, y), y), ry), sy))\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5100, in neg\r\n    \"Neg\", x=x, name=name)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1805, in __init__\r\n    self._control_flow_post_processing()\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1816, in _control_flow_post_processing\r\n    self._control_flow_context.AddOp(self)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2471, in AddOp\r\n    self._AddOpInternal(op)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2492, in _AddOpInternal\r\n    real_x = self.AddValue(x)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2424, in AddValue\r\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1136, in GetRealValue\r\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 998, in AddForwardAccumulator\r\n    max_size = constant_op.constant(-1, dtypes.int32)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 213, in constant\r\n    name=name).outputs[0]\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n...which was originally created as op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div', defined at:\r\n  File \"test.py\", line 93, in <module>\r\n    time_major=True)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 664, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 868, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3274, in while_loop\r\n    return_same_structure)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2994, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2929, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3243, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 836, in _time_step\r\n    (output, new_state) = call_cell()\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 822, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 233, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 364, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 769, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1484, in call\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1391, in __call__\r\n    return self._cell(inputs, state, scope=scope)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1279, in __call__\r\n    self._input_keep_prob)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1260, in _dropout\r\n    *[shallow_filtered_substructure, values])\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1053, in _enumerated_map_structure_up_to\r\n    enumerated_fn, *args, **kwargs)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 643, in map_structure_up_to\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 643, in <listcomp>\r\n    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1049, in enumerated_fn\r\n    r = map_fn(ix[0], *inner_args, **inner_kwargs)\r\n  File \"/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1255, in dropout\r\n    v, keep_prob=keep_prob, seed=self._gen_seed(salt_prefix, i))\r\n\r\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'\r\n\t [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[\"loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2\", \"loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div\"], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=\"/device:GPU:0\"]()]]\r\n```", "comments": ["Thank you so much for the detailed report!\r\n\r\nFrom a quick look, it seems that the check was dropped in the following commit.\r\nhttps://github.com/tensorflow/tensorflow/commit/f8f17cebcc89921b5f1a204dab1f9fdb47b120c1\r\n\r\nThe commit is part of TensorFlow 1.13 release. Could you give it a try with 1.13 to see if that resolves the issue?", "Closing this issue but let us know if this is still an issue with TensorFlow 1.13 or later release.", "> Closing this issue but let us know if this is still an issue with TensorFlow 1.13 or later release.\r\n\r\n\r\n\r\n> Thank you so much for the detailed report!\r\n> \r\n> From a quick look, it seems that the check was dropped in the following commit.\r\n> [f8f17ce](https://github.com/tensorflow/tensorflow/commit/f8f17cebcc89921b5f1a204dab1f9fdb47b120c1)\r\n> \r\n> The commit is part of TensorFlow 1.13 release. Could you give it a try with 1.13 to see if that resolves the issue?\r\n\r\nThank you. Similar warnings don't exist with TF 1.13.1."]}, {"number": 25786, "title": "tf.keras.Model layers list is doubled in newer versions", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14/ Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 8.0 / cuDNN 7.1\r\n- GPU model and memory: Titan XP / 12Gb\r\n\r\n\r\nI am using tf.keras.Model to create my custom model class. When I call the class and build the model the layers properties, which is supposed to show the layers in the model, shows every layer twice. This behavior happens in 1.12.0, while it was fine in 1.9.0. I am not able to find documentation about this issue. \r\n\r\nThis is a simple class that I am encountering this issue:\r\n```python\r\nclass Lenet(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Lenet, self).__init__()\r\n        self.conv1 = tf.layers.Conv2D(filters=6,\r\n                                     kernel_size=5,\r\n                                     activation=tf.nn.relu)\r\n        self.max_pool1 = tf.layers.MaxPooling2D(pool_size=2, strides=2)\r\n\r\n        self.conv2 = tf.layers.Conv2D(filters=16,\r\n                                     kernel_size=5,\r\n                                     activation=tf.nn.relu)\r\n        self.max_pool2 = tf.layers.MaxPooling2D(pool_size=2, strides=2)\r\n\r\n        self.flatten = tf.layers.Flatten()\r\n\r\n        self.fc1 = tf.layers.Dense(units=120, activation=tf.nn.relu)\r\n        self.fc2 = tf.layers.Dense(units=84,  activation=tf.nn.relu)\r\n        self.fc3 = tf.layers.Dense(units=10,  activation=tf.nn.relu)\r\n\r\n        self.lenet_layers = [self.conv1, self.max_pool1, self.conv2, \r\n                                self.max_pool2, self.flatten, self.fc1, self.fc2, self.fc3]\r\n\r\n    def call(self, input):\r\n        out = input\r\n        for layer in self.lenet_layers:\r\n            out = layer(out)\r\n        return out\r\n```\r\nAfter creating my Model and call it and pass a placeholder as input, I call:\r\n```python\r\nlenet = Lenet()\r\nx = tf.placeholder(tf.float32,[None,32,32,1])\r\ny = lenet(x)\r\nlenet.layers\r\n```\r\nI expect to get a list with 8 layers' object on it, however, it returns a list with 16 layer objects( each layer twice on it). This was not the case back in the 1.9.0 version. I can simply ignore it and move on, but think it should not be like this. Maybe there is a purpose behind this behavior, which should be included in the documentation as well.\r\n", "comments": []}, {"number": 25785, "title": "Please, approve iOS DeepLab Tensorflow Lite GPU example.", "body": "Add TensorFlow Lite GPU example for iOS with DeepLab 257x257 model.", "comments": ["Dear @Takuro-Ito, sorry for that mistake, main .gitignore file removed that file from commit. \r\nPlease, review that pull request.    \r\nDon't forget:\r\n1) Use latest Xcode 10.1;\r\n2) Set your dev team in project settings; \r\n3) Download model to Resource folder (see README).", "@VolodymyrPavliukevych Thank you for dealing with it. Unfortunately I'm not authorized to review and merge pull request. \r\nBut your example was very helpful to get started with DeepLab model on iOS platform. I appreciate it.", "@Takuro-Ito, you are welcome!", "Dear @aselle, please review DeepLab model example  for Tensorflow Lite GPU.", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 45 days with no activity and the `awaiting review` label has been applied.", "Sorry for the late response. I've escalated it to @jdduke for review.", "This is really great!\r\n\r\nWe're still trying to determine our general policy for hosting examples. Note that we're also in the process of migrating all the TensorFlow Lite examples to the [new examples repo](https://github.com/tensorflow/examples/tree/master/lite).\r\n\r\nOne thing we're considering is a gallery page where we link to community examples/demos. If you want to put the example in your own repo for now, we can link to that, and get back to you after we've determined our general example hosting policy. Again, very nice work! Do you by chance have a screenshot?", "Tentatively closing this, but we'll reopen when we have a path forward. In the meantime, we're working on a mechanism for linking to community examples if you want to go ahead and host this on your own repo.", "@VolodymyrPavliukevych \r\nthe project tensorflow/tensorflow/lite/examples/ios/DeepLabApp/DeepLabApp.xcodeproj   doesn't exist."]}, {"number": 25784, "title": "Image resizing codes of iOS example camera apps might be wrong", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI think `in_x = (y * image_width) / wanted_input_width;` statements (mentioned below) would be `in_x = (x * image_width) / wanted_input_width;`. (The former perhaps rotates the input image?)\r\n\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/ios/camera/CameraExampleViewController.mm#L302\r\n\r\n```\r\n  for (int y = 0; y < wanted_input_height; ++y) {\r\n    float *out_row = out + (y * wanted_input_width * wanted_input_channels);\r\n    for (int x = 0; x < wanted_input_width; ++x) {\r\n      const int in_x = (y * image_width) / wanted_input_width; <--\r\n      const int in_y = (x * image_height) / wanted_input_height; <--\r\n      tensorflow::uint8 *in_pixel =\r\n          in + (in_y * image_width * image_channels) + (in_x * image_channels);\r\n      float *out_pixel = out_row + (x * wanted_input_channels);\r\n      for (int c = 0; c < wanted_input_channels; ++c) {\r\n        out_pixel[c] = (in_pixel[c] - input_mean) / input_std;\r\n      }\r\n    }\r\n```\r\n\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/ios/camera/CameraExampleViewController.mm#L133\r\n\r\n```\r\nvoid ProcessInputWithFloatModel(\r\n    uint8_t* input, float* buffer, int image_width, int image_height, int image_channels) {\r\n  for (int y = 0; y < wanted_input_height; ++y) {\r\n    float* out_row = buffer + (y * wanted_input_width * wanted_input_channels);\r\n    for (int x = 0; x < wanted_input_width; ++x) {\r\n      const int in_x = (y * image_width) / wanted_input_width; <--\r\n      const int in_y = (x * image_height) / wanted_input_height; <--\r\n      uint8_t* input_pixel =\r\n          input + (in_y * image_width * image_channels) + (in_x * image_channels);\r\n      float* out_pixel = out_row + (x * wanted_input_channels);\r\n      for (int c = 0; c < wanted_input_channels; ++c) {\r\n        out_pixel[c] = (input_pixel[c] - input_mean) / input_std;\r\n      }\r\n    }\r\n  }\r\n}\r\n```", "comments": ["@brownbro Could you provide any results showing the bug? Thanks!", "The x and y values of the inputs are flipped. Its affect is that the output are flipped as well. \r\n\r\nKeep in mind that the output of model is normalized. The boxes are scaled to width or height. \r\n\r\nTherefore, the images from original code will have predictions that is flipped according to the diagonal line. As shown below (an incorrect image): \r\n![original_5](https://user-images.githubusercontent.com/7332407/55710749-244a4f80-59a0-11e9-81c5-2a6987f5464b.PNG)\r\n\r\n**Images from original code (incorrect):**\r\n```\r\nconst int in_x = (y * image_width) / wanted_input_width;\r\nconst int in_y = (x * image_height) / wanted_input_height;\r\n```\r\n![original_4](https://user-images.githubusercontent.com/7332407/55710623-dd5c5a00-599f-11e9-9dc8-78ac2ad92a97.PNG)\r\n![original_3](https://user-images.githubusercontent.com/7332407/55710624-dd5c5a00-599f-11e9-8386-4211303a5171.PNG)\r\n![original_2](https://user-images.githubusercontent.com/7332407/55710625-ddf4f080-599f-11e9-90d9-e513604ccc61.PNG)\r\n![original_1](https://user-images.githubusercontent.com/7332407/55710626-ddf4f080-599f-11e9-843a-f871f3cd4295.PNG)\r\n\r\n\r\n**Images from corrected code:**\r\n```\r\nconst int in_x = (x * image_width) / wanted_input_width;\r\nconst int in_y = (y * image_height) / wanted_input_height;\r\n```\r\n![correct_4](https://user-images.githubusercontent.com/7332407/55710627-ddf4f080-599f-11e9-84ea-5cd3744b3427.PNG)\r\n![correct_3](https://user-images.githubusercontent.com/7332407/55710630-ddf4f080-599f-11e9-9eb9-89d558863b4b.PNG)\r\n![correct_2](https://user-images.githubusercontent.com/7332407/55710632-de8d8700-599f-11e9-8174-35e8cd3f0749.PNG)\r\n![correct_1](https://user-images.githubusercontent.com/7332407/55710635-de8d8700-599f-11e9-86da-5cf38b758136.PNG)\r\n\r\n**Other information:**\r\n* Demo from cloning the repository from GitHub. \r\n* Using SSD with post processing with export_tflite_ssd_graph.py and tflite_convert. Thus, the output will include bounding boxes. \r\n* Using TensorFlowLiteGpuExperimental 0.0.1\r\n", "Hi @brownbro! \r\nIt seems you are using  older versions(1.x versions) of Tensorflow. We recommend that you upgrade  your code base to 2.x  versions as many features and bug fixes has been done in newer versions and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25784\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25784\">No</a>\n"]}, {"number": 25783, "title": "Lite: Reverse Operator Multiple Axis support", "body": "1:> Reverse Operator: Multiple axis as input is supported\r\n2:> Testcases added for the same", "comments": ["@rthadur I don't know TF Lite very well. Can you find a more knowledgeable reviewer, please?", "> @rthadur I don't know TF Lite very well. Can you find a more knowledgeable reviewer, please?\r\n\r\nsure @angersson ", "@jianlijianli : Please help conclude this PR, it is been long pending, TIA!", "@jianlijianli : Please help conclude this PR, TIA!", "@jianlijianli : Gentle Reminder!", "@jianlijianli : Thanks for your valuable comments, i have addressed it, please check, Thanks!", "@jianlijianli : Gentle Reminder!!!", "@jdduke : Some comments i have handled, one need discuss with you, please my reply inline, Thanks!", "@jdduke : Your comments are addressed now, please check, Thanks!", "@jdduke , @jianlijianli : Gentle Reminder!!!", "@jdduke : Your comments are addressed now, please check, Thanks!", "@jdduke , @jianlijianli : Gentle Reminder!!!", "Sorry for the delay. My main hesitation in landing this is the increased complexity. Is there a specific model which requires this feature? Can you point to it? Thanks.", "> Sorry for the delay. My main hesitation in landing this is the increased complexity. Is there a specific model which requires this feature? Can you point to it? Thanks.\r\n\r\nSorry Duke! I could not recollect any model where i have multiple axes requirement. Usually it goes for  1 axis.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 25782, "title": "Update doc for he_normal and lecun_normal according to issue 25564", "body": "The PR updates `he_normal`'s and `lecun_normal`'s documentation according to issue #25564 and the fix for `glorot_normal` done in https://github.com/tensorflow/tensorflow/pull/25642. In particular, it clarifies that the specified standard deviation is the one after truncation.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25782).\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\nGooglers can find more info about SignCLA and this PR by [following this link](go/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F25782).\n\n<!-- ok -->", "cc @facaiy thanks."]}, {"number": 25781, "title": "How can i build static framework for tf-lite under 300kb for iOS?", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: master\r\n- Doc Link:https://www.tensorflow.org/lite/overview\r\n\r\n\r\n**Describe the documentation issue**\r\n\r\nDear supporter,\r\n   I found \"Smaller in size: TensorFlow Lite is smaller than 300KB when all supported operators are linked and less than 200KB when using only the operators needed for supporting InceptionV3 and Mobilenet.\" under TensorFlow Lite highlights. \r\n    I got libtensorflow-lite.a for each archtecture afte using build_ios_universal_lib.sh script. Binary size is far bigger, around 7.5MB. How can I get one smaller than 300KB?  Do I need to do any optimization on compiling and how?", "comments": ["I have the same problem\uff0chas been looking for a solution. I only need text forecast function, so that can reduce the size of the package?Or is there any way to reduce the size of the package \uff1f", "@jvishnuvardhan  Do we got any progress on this issue?  I still had no clue on this problem.", "@aselle  ?", "Have you looked at the tensorflow lite micro code?  tensorflow/lite/experimental/micro  Targeted at very small footprint implementations, but very new (and incomplete) at this point.  I'm not sure how to build a library of the size indicated in the docs.  Maybe @jdduke can comment?", "When you say \"binary size\" is bigger, are you measuring the size impact on the actual iOS app? Or just the static library? Looking at just the static library size is misleading, as it ignores all the size reduction that will occur from op stripping (selective registration) as well as symbol stripping (which is controlled by your build options).\r\n\r\nSee [this guide](https://www.tensorflow.org/lite/guide/inference#customizing_the_kernel_library) for information on how to trim some of the builtin operators. If you know exactly which ops are used by your model, you can create a custom `OpResolver` and register only the ops you need. When your app is actually built/linked, it should trim away unused code.", "@jdduke  hi, I mean static library.  there is no size impact on actual iOS app if I only drag the libtensorflow-lite.a into my Xcode project without using it. How can I quick test the size impact as all iOS examples using downloaded .framework ?   when the \"selective registration\" happens , who trigger it and how does it work?  which option you mean when you say \"controlled by your build options\", tf-lite build option? Xcode build option?", "The guide is pretty vague on how to trim the built-in operators; is there more specific documentation particularly for iOS/android? The size impact on the actual iOS app is about 1.2 MB just for the framework, so trimming it down would be very useful.", "Agreed that the documentation could be more explicit. You can start by creating a `MutableOpResolver`, and then adding only the kernels used by your model. There's an example of doing this [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/models/smartreply/predictor.cc#L76), which takes advantage of a bazel build rule for generating only the ops used by a specific model. If you can't use the bazel build rule to generate that code, you'll have to manually add each op yourself ([this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc) demonstrates how this is done for the `BuiltinOpResolver`).", "Thanks @jdduke I'll try that out", "Worked great! Thanks so much for your help @jdduke ", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25781\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25781\">No</a>\n", "@jdduke fellowed by your example\uff0cgot \u201cl17_ops_registration.cc\u201d which contains only 12 operators .  but it seems not reduce library size when i include it . How to use it properly? thanks!", "So, using the MutableOpResolver approach won't change the size of the *static* library, but it should shrink the size of the actual binary that gets produced in an app, as the unneeded code/symbols can be stripped.", "> \r\n> \r\n> So, using the MutableOpResolver approach won't change the size of the _static_ library, but it should shrink the size of the actual binary that gets produced in an app, as the unneeded code/symbols can be stripped.\r\n\r\nOh\uff0cis there another way to reduce static library?\r\nhere is my BUILD file.  Only a \"cc_binary + static\" .so can be loaded ok in unity(c#).\r\n\r\n> cc_binary(\r\n    name = \"Prophet.so\",\r\n    srcs = [\"minimal.h\", \"Prophet.cc\",   ],\r\n    linkopts = tflite_linkopts() + select({\r\n        \"//tensorflow:android\": [\r\n            \"-pie\",  # Android 5.0 and later supports only PIE\r\n            \"-lm\",  # some builtin ops, e.g., tanh, need -lm\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    deps = [\r\n        \"//tensorflow/lite:framework\",\r\n        \"//tensorflow/lite/c:c_api_internal\",\r\n        \"//tensorflow/lite/kernels:builtin_ops\",\r\n        \"//tensorflow/lite:builtin_op_data\",\r\n        \"//tensorflow/lite/schema:schema_fbs\",\r\n    ],\r\n    linkshared = True,\r\n)\r\n\r\n\r\n\r\n", "To reduce the static build you would either need to create your own cc_library target with just the kernel dependencies you need (see this [build rule](https://github.com/tensorflow/tensorflow/blob/cc35420e4a615fa635a5302e476ba2a6118daa62/tensorflow/lite/kernels/BUILD#L376)), or modify that `builtin_op_kernels` target directly for just the kernels you need, and also update the BuiltinOpResolver in the `builtin_ops` target.", "fellow your guide, i modified\"tensorflow/tensorflow/lite/kernels/register.cc \",and BUILD target \"builtin_op_kernels\". operators decrease from 88 to 77\uff0clib size reduces from 3.7m to 3.4m\u3002\r\nIs there a powerful  way to reduce size?\r\nit seems like whole dependicies are compiled. Is there efficient way to strip unused symbols automately\uff1f What does below mean?\r\n\r\n> def tflite_linkopts_unstripped():\r\n    \"\"\"Defines linker flags to reduce size of TFLite binary.\r\n\r\n       These are useful when trying to investigate the relative size of the\r\n       symbols in TFLite.\r\n\r\n    Returns:\r\n       a select object with proper linkopts\r\n    \"\"\"\r\n\r\n    # In case you wonder why there's no --icf is because the gains were\r\n    # negligible, and created potential compatibility problems.\r\n    return select({\r\n        \"//tensorflow:android\": [\r\n            \"-Wl,--no-export-dynamic\",  # Only inc syms referenced by dynamic obj.\r\n            \"-Wl,--gc-sections\",  # Eliminate unused code and data.\r\n            \"-Wl,--as-needed\",  # Don't link unused libs.\r\n        ],\r\n        \"//conditions:default\": [],\r\n    })\r\n\r\n> \r\n> \r\n> To reduce the static build you would either need to create your own cc_library target with just the kernel dependencies you need (see this [build rule](https://github.com/tensorflow/tensorflow/blob/cc35420e4a615fa635a5302e476ba2a6118daa62/tensorflow/lite/kernels/BUILD#L376)), or modify that `builtin_op_kernels` target directly for just the kernels you need, and also update the BuiltinOpResolver in the `builtin_ops` target.\r\n\r\n", "> \r\n> \r\n> Agreed that the documentation could be more explicit. You can start by creating a `MutableOpResolver`, and then adding only the kernels used by your model. There's an example of doing this [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/models/smartreply/predictor.cc#L76), which takes advantage of a bazel build rule for generating only the ops used by a specific model. If you can't use the bazel build rule to generate that code, you'll have to manually add each op yourself ([this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc) demonstrates how this is done for the `BuiltinOpResolver`).\r\n\r\nwhy cann't recently(8gb memory) on fresh source?\r\n\r\n> \r\n  bazel build tensorflow/python/tools:print_selective_registration_header && \\   \r\nbazel-bin/tensorflow/python/tools/print_selective_registration_header \\  \r\n   --graphs=models_arm64/1.pb > ops_to_register.h\r\n\r\n> <long int, 1>; int packet_size = 4; bool inner_dim_contiguous = false; bool inner_dim_reordered = true; int Alignment = 0]':\r\n./tensorflow/core/kernels/eigen_spatial_convolutions.h:308:11: warning: 'orig_r' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n./tensorflow/core/kernels/eigen_spatial_convolutions.h:308:11: warning: 'orig_r' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/python/tools:print_selective_registration_header failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 724.181s, Critical Path: 123.60s\r\nINFO: 2191 processes: 2191 local.\r\nFAILED: Build did NOT complete successfully\r\n"]}, {"number": 25780, "title": "Wrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2.", "body": "**System information**\r\n- OS Platform and Distribution: MacOs High Sierra\r\n- TensorFlow installed from: installed from pip\r\n- TensorFlow version: v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6\r\n\r\n**Problem**\r\nI think that tf.python.keras.Dense layer is not implemented correctly. The documentation states:\r\n\r\n>  Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with `kernel`.\r\n\r\nMy interpretation of this claim is: if we have input tensor with shape `(None, 10, 10)` then dense layer with 20 hidden units should produce output with a shape `(None, 20)` and the kernel should have a shape of `(10 * 10, 20)`, and input tensor should be flattened to shape `(None, 100)` before product with kernel. \r\n\r\n**But**,  implementation is different. \r\n\r\n1. Kernel is initialized to shape `(10, 20)` (for the example above)\r\n```python\r\nself.kernel = self.add_weight(    # method Dense#build\r\n        'kernel',\r\n        shape=[input_shape[-1].value, self.units],\r\n        initializer=self.kernel_initializer,\r\n        regularizer=self.kernel_regularizer,\r\n        constraint=self.kernel_constraint,\r\n        dtype=self.dtype,\r\n        trainable=True)\r\n```\r\n\r\n2. The output is computed as tensordot with summation over innermost dimensions\r\n```python\r\n# Broadcasting is required for the inputs.\r\noutputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]]) # method Dense#call\r\n```\r\n\r\nThis means that for `x.shape == (None, 10, 10)` and `units == 20` the output shape will be `(None, 10, 20)`, and that kernel is broadcasted for all other dimensions (this means weights are shared).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nx = tf.random.normal((10, 10, 10))\r\ny = tf.layers.dense(x, 20)\r\nprint(y.shape)      # Produces (10, 10, 20) not (10, 20) as expected.\r\n```\r\n\r\n**Discussion**\r\nThree possible things are happening here:\r\n\r\n1. Documentation is wrong and implementation is correct, and a dense layer is meant to accept only rank 2-dimensional input (batchSize, a dimension of input vector) and if we pass it higher rank it treats it as a stack of input vectors. The solution is to change the documentation (specifically the note part), it should state something like numpy's documentation for `matmul`:  \r\n> If either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly\r\n2. Implementation is wrong and there should be `flatten` before `tensordot` (no need for `tensordot`, we can now use `matmul`)\r\n3. I do not understand what does dense layer should do (unlikely because I asked my colleagues for their opinion and they agree with my view of what kind of operation does dense layer perform)\r\n\r\nI've ordered this list from most to least likely (in my opinion).", "comments": ["I have added an comment for the working of **tensordot** function here, however this does not state that the Problem has been solved. \r\n#### The size of the specified input_shape still does not raise error if the shape of the inputs (called by Dense) is not equal to input_shape.", "@fredo994 I agree with your solution n\u00b01 (remove or update the 'Note' part). \r\nI'm not an GitHub/PR expert, would you be able to open a PR with your proposition of changing the docstring?", "@fredo994,\r\nA **Note**, as shown below, has been added in the [Documentation of Dense Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense):\r\n\r\n> Note: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units). \r\n\r\nCan you please confirm if we can close this issue, as it has been resolved? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25780\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25780\">No</a>\n"]}, {"number": 25779, "title": "keras and tf.keras behave differently with custom loss function and fit_generator", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version: 10.0 / 7.4.2\r\n- GPU model and memory: GTX 1080 ti\r\n- Keras version: 2.2.4\r\n\r\n**Describe the current behavior**\r\nI am using Keras bundled with tensorflow. When I apply my own (rather obscure) loss I get a **NotFoundError**:\r\n```\r\nNotFoundError: Resource __per_step_7/_tensor_arraysloss/reshape_loss/map/TensorArray_1_1/N10tensorflow11TensorArrayE does not exist.\r\n\t [[{{node training/Adam/gradients/loss/reshape_loss/map/while/TensorArrayReadV3_1_grad/TensorArrayGrad/TensorArrayGradV3}}]]\r\n\t [[training/Adam/gradients/b_count_6/_233]]\r\n```\r\nwhen I use `fit_generator`, but when I use `fit` everything works ok. However, when I use vanilla Keras, both methods work just fine.\r\n\r\nAs a side question: my loss is rather slow, can someone give me some pointers in how to improve performance?\r\n\r\n**Describe the expected behavior**\r\nI would expect `fit_generator` to work just the same as in the vanilla-keras version or in the normal `fit` version.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n``` python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n# from tensorflow.keras import Input, Model\r\n# from tensorflow.keras.layers import Dense, Flatten, Reshape\r\nfrom keras import Input, Model\r\nfrom keras.layers import Dense, Flatten, Reshape\r\n\r\n\r\ndef get_limb_lengths(person, limbs):\r\n    \"\"\"\r\n    :param person: [ J * 3 ]\r\n    :param limbs: {tf.constant} [ (a, b), (b, c), ... ]\r\n    :return:\r\n    \"\"\"\r\n    person = tf.reshape(person, (-1, 3))\r\n    distances = tf.map_fn(\r\n        lambda limb: tf.sqrt(\r\n            tf.reduce_sum(\r\n                tf.square(person[limb[0]] - person[limb[1]]))),\r\n        limbs,\r\n        dtype=tf.float32\r\n    )\r\n    return distances\r\n\r\n\r\ndef mean_limb_length_per_sequence(y_true, y_pred, limbs):\r\n    \"\"\"\r\n    :param y_true: (n_frames, x J * 3)\r\n    :param y_pred: (n_frames, x J * 3)\r\n    :param limbs: {tf.constant}\r\n    :return:\r\n    \"\"\"\r\n    diff = tf.map_fn(\r\n        lambda x:\r\n        tf.reduce_mean(\r\n            tf.abs(get_limb_lengths(x[0], limbs) -\r\n                   get_limb_lengths(x[1], limbs))),\r\n        (y_true, y_pred),\r\n        dtype=tf.float32\r\n    )\r\n    return diff\r\n\r\n\r\ndef mean_limb_length_on_batch(y_true, y_pred, limbs):\r\n    \"\"\" This one is optimized for CMU-MoCap\r\n    :param y_true: (batchsize x n_frames x J * 3)\r\n    :param y_pred: (batchsize x n_frames x J * 3)\r\n    :return:\r\n    \"\"\"\r\n    loss = tf.map_fn(\r\n        lambda x: tf.reduce_mean(\r\n            mean_limb_length_per_sequence(x[0], x[1], limbs)),\r\n        (y_true, y_pred),\r\n        dtype=tf.float32\r\n    )\r\n    return tf.reduce_mean(loss)\r\n\r\n\r\ndef mean_limb_length(y_true, y_pred):\r\n    \"\"\" This one is optimized for CMU-MoCap\r\n    :param y_true: (batchsize x n_frames x J * 3)\r\n    :param y_pred: (batchsize x n_frames x J * 3)\r\n    :return:\r\n    \"\"\"\r\n    limbs = tf.constant([\r\n        (0, 1), (1, 2), (2, 3), (3, 4)\r\n    ])\r\n    return mean_limb_length_on_batch(y_true,\r\n                                     y_pred,\r\n                                     limbs)\r\n\r\n\r\nbs = 512\r\nf1 = 20\r\nf2 = 2\r\nJ = 5\r\ndim = 3\r\nX = np.random.random((bs, f1, J * dim))\r\nY = np.random.random((bs, f2, J * dim))\r\n\r\ndef create_gen():\r\n    while True:\r\n        yield X, Y\r\n\r\ngen = create_gen()\r\n\r\ninputs = Input(shape=X.shape[1:])\r\nx = Flatten()(inputs)\r\nx = Dense(f2 * J * dim)(x)\r\nx = Reshape((f2, J, dim))(x)\r\n\r\nmodel = Model(inputs=inputs, outputs=x)\r\nmodel.summary()\r\n\r\noptimizer = 'adam'\r\n\r\nmodel.compile(loss=mean_limb_length,\r\n              metrics=[mean_limb_length],\r\n              optimizer=optimizer)\r\n\r\n# model.fit(X, Y)\r\nmodel.fit_generator(generator=gen,\r\n                    validation_data=gen,\r\n                    steps_per_epoch=10,\r\n                    validation_steps=3,\r\n                    epochs=5)\r\n```\r\n\r\n**Other info / logs**\r\nError message:\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-3-1232392e144b> in <module>\r\n    104                     steps_per_epoch=10,\r\n    105                     validation_steps=3,\r\n--> 106                     epochs=5)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1500         use_multiprocessing=use_multiprocessing,\r\n   1501         shuffle=shuffle,\r\n-> 1502         initial_epoch=initial_epoch)\r\n   1503 \r\n   1504   def evaluate_generator(self,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\r\n    191       progbar.on_batch_begin(step, batch_logs)\r\n    192 \r\n--> 193       batch_outs = batch_function(*batch_data)\r\n    194       if not isinstance(batch_outs, list):\r\n    195         batch_outs = [batch_outs]\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1259       else:\r\n   1260         self._make_fit_function()\r\n-> 1261         outputs = self._fit_function(ins)  # pylint: disable=not-callable\r\n   1262 \r\n   1263     if reset_metrics:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)\r\n   3080 \r\n   3081     fetched = self._callable_fn(*array_vals,\r\n-> 3082                                 run_metadata=self.run_metadata)\r\n   3083     self._call_fetch_callbacks(fetched[-len(self._fetches):])\r\n   3084     return nest.pack_sequence_as(self._outputs_structure,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1438           ret = tf_session.TF_SessionRunCallable(\r\n   1439               self._session._session, self._handle, args, status,\r\n-> 1440               run_metadata_ptr)\r\n   1441         if run_metadata:\r\n   1442           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)\r\n    542             None, None,\r\n    543             compat.as_text(c_api.TF_Message(self.status.status)),\r\n--> 544             c_api.TF_GetCode(self.status.status))\r\n    545     # Delete the underlying status object from memory otherwise it stays alive\r\n    546     # as there is a reference to status from this from the traceback due to\r\nNotFoundError: Resource __per_step_7/_tensor_arraysloss/reshape_loss/map/TensorArray_1_1/N10tensorflow11TensorArrayE does not exist.\r\n\t [[{{node training/Adam/gradients/loss/reshape_loss/map/while/TensorArrayReadV3_1_grad/TensorArrayGrad/TensorArrayGradV3}}]]\r\n\t [[training/Adam/gradients/b_count_6/_233]]\r\n```\r\n\r\n", "comments": ["I was able to execute your code snippet using ```fit_generator``` successfully using tf.keras and keras in TF 1.13.0-rc1 and TF 1.12.0\r\nCan you please execute your code using [google.colab.sandbox](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true) and confirm?\r\nThanks!", "Thank you for your help, the code works fine in the colab.sandbox. I compiled tf myself so that I can use it with Cuda 10 and Python 3.7 so I assume something goes wrong there. I will investigate my setup and report back in case something shows up.", "You are welcome. I would like to tell you that TensorFlow 1.13.0-rc2 has been released!\r\nIt comes with GPU binaries built with CUDA 10 and also supports Python 3.7 \r\nYou may want to give it a try. I will close this issue since we cannot reproduce it and works fine with colab. Feel free to reopen if have any further problems. Thanks!"]}, {"number": 25778, "title": "Removed warning in sparse_tensor.h", "body": "Ensure the variable is initialised. And compilation warning is removed\r\n", "comments": []}, {"number": 25777, "title": "What does a dim tensor have a dimension with 0-size? such as tf.Tensor([], shape=(2, 2, 0), dtype=int32)", "body": "- tensorShapes may have an 0-size dimension, but when are they going to be applied? Could you give me an example? This troubles me a lot...  THANKS A LOT!\r\n\r\nfor example  `tf.one_hot`  \r\nOn the implementation side, Tensorflow turns out to be verifying that `depth>= 0`, which are going to generate the tensor with a dimension with 0-size", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 25776, "title": "convert yolov3 model to tflite", "body": "not able to convert yolov3 model to tflite format\r\n\r\nconverted the weights file to pb format\r\n\r\nthen\r\n\r\n./bazel-bin/tensorflow/lite/toco/toco --input_file=yolov3_finale_manga109.pb --output_file=foo.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=input --input_shapes=1,406,406,3 --output_arrays=output --allow_nonexistent_arrays\r\n\r\ngot error\r\n\r\n2019-02-15 17:23:48.560625: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2044 operators, 2743 arrays (0 quantized)\r\n2019-02-15 17:23:48.601097: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 0 operators, 2 arrays (0 quantized)\r\n2019-02-15 17:23:48.601134: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!\r\n2019-02-15 17:23:48.601217: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 0 operators, 2 arrays (0 quantized)\r\n2019-02-15 17:23:48.601228: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!\r\n2019-02-15 17:23:48.601237: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 0 operators, 2 arrays (0 quantized)\r\n2019-02-15 17:23:48.601243: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!\r\n2019-02-15 17:23:48.601249: W tensorflow/lite/toco/tooling_util.cc:1263] Fixing constant output array output by inserting a copy. This is not optimal.\r\n2019-02-15 17:23:48.601260: F ./tensorflow/lite/toco/model.h:2134] Check failed: has_shape() \r\n", "comments": ["@vainaijr Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@vainaijr  I did it here: https://github.com/peace195/tensorflow-lite-yolo-v3\r\n\r\nPlease try it. I would appreciate if you give me a star for this project :+1:", "You can see at here. I tried it and success.\r\nhttps://github.com/phanxuanduc1996/convert_yolo_weights"]}, {"number": 25775, "title": "tf.nn.dropout is not working properly with keep_rate 1.", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed with conda\r\n- TensorFlow version 1.12.0\r\n- Python version 3.6.6\r\n- CUDA/cuDNN version: 9.0/7\r\n- GPU model and memory: GTX 1080 TI 11GB\r\n\r\n**Describe the current behavior**\r\nwhen using ```tf.nn.dropout``` with ```placeholder_with_default(1)``` during forwarding validation data it still makes use of random which changes the state for the next train operation and so the training procedure is not deterministic. \r\n\r\n**Describe the expected behavior**\r\nMy opinion is that the following code: ```tensor_util.constant_value(keep_prob)``` should return the default value which is 1 and should return ```x```, but for some reason the value is ```None``` and the tf.seed changes for the next batches of train data, which cause the instability of training procedure", "comments": ["@sipan17  It would be great if you can provide a small code to reproduce the error. Thanks!", "Just run the code with use_validation True and False, and you can see that the results differ\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\n\r\ndef next_batch(data, batch_size):\r\n    if len(data) <= batch_size:\r\n        return [], np.array(data)\r\n    else:\r\n        return np.array(data[batch_size:]), np.array(data[:batch_size])\r\n\r\n\r\ndef batch_generator(num_datapoints, shuffle=False):\r\n    inds = np.arange(num_datapoints)\r\n    if shuffle:\r\n        np.random.shuffle(inds)\r\n    rem_inds, batch_inds = next_batch(inds, batch_size)\r\n\r\n    while len(batch_inds) > 0:\r\n        yield batch_inds\r\n        rem_inds, batch_inds = next_batch(rem_inds, batch_size)\r\n\r\n\r\nn_epochs = 10\r\ngpu_number = 0\r\nbatch_size = 256\r\ntrain_dropout_keep_rate = 0.5\r\nuse_validation = bool(int(sys.argv[1]))\r\nseed = 17\r\nnum_labels = 2\r\n\r\nnp.random.seed(seed)\r\ntf.set_random_seed(seed)\r\ndata = np.random.rand(10000, 32)\r\nlabels = np.zeros((len(data), 2))\r\nmsk = np.array(np.random.randint(2, size=len(data)), bool)\r\nlabels[:, 0][msk] = 1\r\nlabels[:, 1][~msk] = 1\r\nconfig = tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.1))\r\n\r\nwith tf.device(\"/gpu:{}\".format(gpu_number)):\r\n    with tf.Session(config=config) as sess:\r\n        input_ph = tf.placeholder(tf.float32, shape=[None, 32], name=\"input\")\r\n        labels_ph = tf.placeholder(tf.float32, shape=[None, num_labels], name=\"label\")\r\n        dropout_drop_rate_ph = tf.placeholder_with_default(1., shape=[], name=\"dropout_rate\")\r\n        fc1 = tf.layers.dense(input_ph, 16)\r\n        dr1 = tf.nn.dropout(fc1, dropout_drop_rate_ph)\r\n        fc2 = tf.layers.dense(dr1, 2)\r\n        logits = tf.nn.dropout(fc2, dropout_drop_rate_ph)\r\n        output = tf.nn.softmax(logits, name=\"prediction\")\r\n\r\n        total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels_ph,\r\n                                                                               logits=logits), name=\"ce_loss\")\r\n\r\n        train_op = tf.train.AdamOptimizer().minimize(total_loss)\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        for epoch in range(n_epochs):\r\n            print('epoch {} started'.format(epoch + 1))\r\n            losses = []\r\n            for batch_inds in batch_generator(len(data), True):\r\n                _, _loss = sess.run([train_op, total_loss],\r\n                                    feed_dict={input_ph: data[batch_inds],\r\n                                               labels_ph: labels[batch_inds],\r\n                                               dropout_drop_rate_ph: train_dropout_keep_rate})\r\n                losses.append(_loss)\r\n\r\n            print(\"epoch {} ended\".format(epoch + 1))\r\n            print(\"epoch moving mean loss: {}\".format(np.round(100 * np.mean(losses), 2)))\r\n\r\n            if use_validation:\r\n\r\n                for batch_inds in batch_generator(len(data)):\r\n                    _loss = sess.run(total_loss, feed_dict={input_ph: data[batch_inds],\r\n                                                            labels_ph: labels[batch_inds]})\r\n```", "@sipan17 I see the following errors with use_validation= True and False. When I run multiple times, the loss values are changing slightly. Do you expect these numbers to be same? Thanks!\r\n\r\nTrue\r\nepoch 10 ended\r\nepoch moving mean loss: 72.33\r\n\r\nFalse\r\nepoch 10 ended\r\nepoch moving mean loss: 72.17", "Of course, they should be the same as when you forward the validation data with keep rate 1 no changes should be made in the model or in the random seed, so the training accuracy should exactly match", "@sipan17,\r\nSorry for the delayed response. Slight `Non-Determinism` is possible in `Tensorflow`, especially while using `GPU`. Please find [this Article](https://www.twosigma.com/articles/a-workaround-for-non-determinism-in-tensorflow/) and this [Stack Overflow Answer](https://stackoverflow.com/questions/50744565/how-to-handle-non-determinism-when-training-on-a-gpu) on how to handle `Non-Determinism`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25775\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25775\">No</a>\n"]}, {"number": 25774, "title": "Update license year", "body": "Updated license year in LICENSE file.\r\n\r\n:octocat:", "comments": []}, {"number": 25773, "title": "Build error during TF build from the source with XLA enabled", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.12.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip without virtualenv\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): I'm not sure what version I'm using but, I'm using VS2015 and _MSC_VER is 1900\r\n- CUDA/cuDNN version: CUDA v9.0.176 / cuDNN v7.4.2.24\r\n- GPU model and memory: GTX960M (2GB)\r\n\r\n\r\n**Describe the problem**\r\nI want to build TF from source with XLA enabled.\r\nSo, I tried to build, configuring XLA to be enabled.\r\nBut the result was build failed and the log seems to say that 'xla_data.pb.h' file is having some trouble for TS build.\r\nI could not find the real file (xla_data.pb.h), even if the log points where the file exist (bazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h), because the path of the file is temporary during the build as I know.\r\nBecause even if I modify the bazel's temporary file, this problem would not be resolved radically, I want to find other way for this problem.\r\nCould you give me some help about this problem ?\r\n\r\nThanks!\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n> C:\\tensorflow>python ./configure.py\r\n> WARNING: Running Bazel server needs to be killed, because the startup options are different.\r\n> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\n> You have bazel 0.15.0 installed.\r\n> Please specify the location of python. [Default is C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\python.exe]:\r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\r\n> Please input the desired Python library path to use.  Default is [C:\\Users\\nickeys\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages]\r\n> \r\n> Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: N\r\n> No Apache Ignite support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with XLA JIT support? [y/N]: Y\r\n> XLA JIT support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]:\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: Y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:\r\n> \r\n> \r\n> Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:\r\n> \r\n> \r\n> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n> \r\n> \r\n> Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:\r\n> \r\n> \r\n> Please specify a list of comma-separated Cuda compute capabilities you want to build with.\r\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\n> Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.0\r\n> \r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n> \r\n> \r\n> Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\n> Eigen strong inline overridden.\r\n\r\nAnd then,\r\n\r\n> C:\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nINFO: From ProtoCompile tensorflow/core/protobuf/replay_log.pb.cc:\r\ntensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/protobuf/cluster.proto but not used.\r\ntensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/framework/graph.proto but not used.\r\nERROR: C:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:536:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/nickeys/_bazel_nickeys/xv6zejqw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.14393.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.14393.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.14393.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\nickeys\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\nickeys\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -DEIGEN_AVOID_STL_ARRAY /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.o /c tensorflow/compiler/xla/service/cpu/runtime_fft.cc\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h(242): error C2059: syntax error: 'constant'\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h(242): error C3805: 'constant': unexpected token, expected either '}' or a ','\r\nbazel-out/x64_windows-opt/genfiles\\tensorflow/compiler/xla/xla_data.pb.h(249): error C2065: 'TOKEN': undeclared identifier\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2765.994s, Critical Path: 2700.12s\r\nINFO: 2934 processes: 2934 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["looks like a global enum value conflicting with some already defined symbol", "This is a conflict between the generated enum entry OPAQUE in xla_data.pb.h (line 290)\r\n\r\n`OPAQUE = 14,`\r\n(generated out of tensorflow/compiler/xla/xla_data.proto - line 74)\r\n\r\n and an identically named preprocessor macro in the Windows 10 SDK (in my case ...\\Windows 10 SDK\\include\\10.0.10586.0\\um\\wingdi.h line 1872):\r\n\r\n`#define OPAQUE              2`\r\n\r\nThe include chain that leads to the error:\r\n\r\n .\\tensorflow/compiler/xla/util.h <- Root target causing the issue\r\n -.\\tensorflow/compiler/xla/status_macros.h\r\n --.\\tensorflow/compiler/xla/types.h\r\n ---.\\tensorflow/core/framework/numeric_types.h\r\n ----.\\third_party/eigen3/unsupported/Eigen/CXX11/Tensor\r\n -----external/eigen_archive\\unsupported/Eigen/CXX11/Tensor\r\n ------...\\Windows 10 SDK\\include\\10.0.10586.0\\um\\windows.h\r\n -------...\\Windows 10 SDK\\include\\10.0.10586.0\\um\\wingdi.h <- Preprocessor definition\r\n -.\\tensorflow/compiler/xla/xla_data.pb.h <- Location of error, generated by protoc\r\n\r\nI hope this helps in solving this issue.\r\n", "@alexrobomind thank you very much for the information!\r\n@tatianashp could you reassign this issue to an appropriate owner to make the needed changes?", "I would prefer to fix this in Eigen rather than modifying our protocol buffers not to clash with this most awful of headers.\r\n\r\nIt's not clear to me how much Eigen really needs to include windows.h from its main header file, as it seems we're only trying to define a few integer types?  An added bonus of not including windows.h would be faster build times for all of TF on Windows (since *everything* is going to pull in TF numeric_types.h and windows.h is huge).\r\n\r\nIf `Tensor` must include `windows.h` then I guess it's up to it to #undef the macros that we're clashing with.\r\n\r\n@rmlarsen are you the right owner?", "If Eigen for some reason absolutely needs windows.h they could also use\r\n\r\n```\r\n#define NOGDI\r\n#include <windows.h>\r\n#undef NOGDI\r\n```\r\nThis would disable the Graphics Device Interface, which is pretty much all of wingdi.h, including the offending macro definition, without disabling the rest of windows.h.", "Maybe we should make a pull request to [eigen](https://bitbucket.org/eigen/eigen/pull-requests/). Or modify the enum entry `OPAQUE` in `tensorflow/compiler/xla/xla_data.proto`. What do you think @jlebar and @rmlarsen ?", "Modifying the proto would be a breaking change for us.  We would strongly\nprefer that this be fixed in Eigen.\n\nOn Fri, Apr 12, 2019 at 7:49 AM Stepiii <notifications@github.com> wrote:\n\n> Maybe we should make a pull request to eigen\n> <https://bitbucket.org/eigen/eigen/pull-requests/>. Or modify the enum\n> entry OPAQUE in tensorflow/compiler/xla/xla_data.proto. What do you think\n> @jlebar <https://github.com/jlebar> and @rmlarsen\n> <https://github.com/rmlarsen> ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25773#issuecomment-482601486>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAJMh2B0htpqEoNmydRtOSFwzoPHp7ZEks5vgJ0SgaJpZM4a862U>\n> .\n>\n", "I would recommend sending a pull request to Eigen. Please add me as\nreviewer.\n\nOn Fri, Apr 12, 2019 at 7:48 AM Stepiii <notifications@github.com> wrote:\n\n> Maybe we should make a pull request to eigen\n> <https://bitbucket.org/eigen/eigen/pull-requests/>. Or modify the enum\n> entry OPAQUE in tensorflow/compiler/xla/xla_data.proto. What do you think\n> @jlebar <https://github.com/jlebar> and @rmlarsen\n> <https://github.com/rmlarsen> ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25773#issuecomment-482601486>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQH9DmO1ub84x44sa88HTmqAT9zmYT1Dks5vgJzVgaJpZM4a862U>\n> .\n>\n", "If Eigen cannot change their includes, one could also add an undef to third_party\\eigen3\\unsupported\\Eigen\\CXX11\\Tensor\r\n\r\n```\r\n...\r\n// On Windows, Eigen will include Windows.h, which defines various\r\n// macros that conflict with TensorFlow symbols. Undefine them here to\r\n// prevent clashes.\r\n#undef DeleteFile\r\n#undef ERROR\r\n#undef LoadLibrary\r\n...\r\n```\r\n\r\nIt seems like this is not the first time this problem appeared.", "@alexrobomind I have tried:\r\n```c++\r\n#undef OPAQUE\r\n```\r\nBut it does not work.", "@Stepiii\r\nIndeed, i do get the same error, but this time the include chain leading to wingdi.h is a different one. Also, it's a different target (//tensorflow/compiler/xla/service:hlo).\r\n\r\n .\\tensorflow/compiler/xla/service/dynamic_parameter_binding.h\r\n -external/com_google_absl\\absl/container/flat_hash_map.h\r\n --external/com_google_absl\\absl/container/internal/raw_hash_map.h\r\n  ---external/com_google_absl\\absl/container/internal/hashtablez_sampler.h\r\n  ----external/com_google_absl\\absl/synchronization/mutex.h\r\n  -----external/com_google_absl\\absl/synchronization/internal/kernel_timeout.h\r\n  ------external/com_google_absl\\absl/time/clock.h\r\n  -------external/com_google_absl\\absl/time/time.h\r\n  --------D:\\Programme\\Windows 10 SDK\\include\\10.0.10586.0\\um\\winsock2.h\r\n  ---------D:\\Programme\\Windows 10 SDK\\include\\10.0.10586.0\\um\\windows.h\r\n  ----------D:\\Programme\\Windows 10 SDK\\include\\10.0.10586.0\\um\\wingdi.h\r\n   .\\tensorflow/compiler/xla/xla_data.pb.h\r\n\r\nEdit: Formatting", "@alexrobomind It is more like:\r\n\r\n.\\tensorflow/compiler/xla/service/dynamic_parameter_binding.h\r\n-external/com_google_absl\\absl/container/flat_hash_map.h\r\n--external/com_google_absl\\absl/container/internal/raw_hash_map.h\r\n**---external/com_google_absl\\absl/container/internal/raw_hash_set.h**\r\n----external/com_google_absl\\absl/container/internal/hashtablez_sampler.h\r\n-----external/com_google_absl\\absl/synchronization/mutex.h\r\n------external/com_google_absl\\absl/synchronization/internal/kernel_timeout.h\r\n...\r\n\r\nBut it doesn't matter. It seems the error is more deeper than I expected.\r\n@jlebar and @rmlarsen How could we avoid including `wingdi.h` ?", "Ugh.  If absl is including Windows headers, I am not sure there is much hope and we may have to take the breaking change in XLA.  But let's check with ABSL folks.\r\n\r\n/me summons @misterg", "ABSL folks are open to changing ABSL not to include windows.h and other Windows headers from ABSL headers.\r\n\r\nIt sounds like @rmlarsen is not going to write the Eigen PR.  I'm wondering if one of you might be willing to do this?", "@alexrobomind can you give us more details to solve the errors step by step", "Upon further investigation with ABSL team, this is looking difficult to fix in ABSL.\r\n\r\nMoreover, they pointed out that because this proto is part of XLA's public API, even if we fixed ABSL and Eigen, that wouldn't unbreak the build for anyone else who includes this header plus windows.h.\r\n\r\nI've sent out a proposal internally to rename the OPAQUE enumerator and take the breaking change.", "cc @EricWF from ABSL team.", "Justin,\n\nThank you for handling this problem on XLA side!\n\n[catching up with the email after travel]\n\nOn Mon, Apr 15, 2019 at 9:41 PM Justin Lebar <notifications@github.com>\nwrote:\n\n> cc @EricWF <https://github.com/EricWF> from ABSL team.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25773#issuecomment-483507690>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA8ObAs_8lHrOA6cNLYHax2hwDskhZoCks5vhVRygaJpZM4a862U>\n> .\n>\n", "@crazyn2 The fix that I applied is exactly what @Stepiii uploaded in his commit(s). Unfortunately, this is only a partial fix, since Eigen is not the only route including windows.h (and therefore also wingdi.h). Sorry for the late response.", "@alexrobomind I've avioded this problem just cancling the XLA JIK and the Eigen support additionally.", "@crazyn2 It seems that you cannot build tensorflow with XLA on windows. That is the issue.", "I am working on renaming the enumerator to make this work.  Separately, we\nneed to turn on xla in our windows builds so that we catch issues like this.\n\nOne worry is that when I renames OPAQUE we will just run into another\nproblem.  I guess we'll see.\n\nOn Thu, Apr 18, 2019, 12:30 PM Stepiii <notifications@github.com> wrote:\n\n> @crazyn2 <https://github.com/crazyn2> It seems that you cannot build\n> tensorflow with XLA on windows. That is the issue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25773#issuecomment-484652650>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABEZB2FIY2SJZBCF37TORTPRDD47ANCNFSM4GXTVWKA>\n> .\n>\n", "I have renamed OPAQUE to OPAQUE_TYPE.  LMK if you run into additional problems."]}, {"number": 25772, "title": "Unable to set estimator.model_dir after estimator initialization", "body": "## System information:\r\nTensorflow version: 1.9.0 (installed from binary)\r\nPython version: 3.6.8\r\n\r\n## Problem\r\nSetting the value of `estimator.model_dir` _after_ the estimator has already been created does not work.  I specifically want to set the value of this after initialization, but only see a way to set it _during_ initialization.  Is there a reason that tensorflow does not support setting the `model_dir` after estimator creation? If not, is there a viable workaround?\r\n\r\n## Source code:\r\n```\r\nclassifier = tf.estimator.DNNClassifier(\r\n    feature_columns=some_feature_classifier,\r\n    hidden_units=[10, 10],\r\n    n_classes=3)\r\n# other code...\r\nclassifier.model_dir = \"/tmp/model_dir_file\"\r\n```\r\n\r\n## Error message:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nFile \"<stdin>\" in <module>\r\n----> 1 classifier.model_dir = \"/tmp/model_dir_file\"\r\n\r\nAttributeError: can't set attribute\r\n```\r\n\r\nThank you!", "comments": ["It is a variable in DNNClassfier's super class Estimator but not DNNClassifier. Hence you cannot access it through DNNClassifier. Moreover, these API's are all deprecated and you might want to consider migrating to the new ones.", "As suggested by @Sayan-Paul , @adt123 Could you use newer Tensorflow version and test your model? Thanks!", "Hmm, I tried running the same code in my original post with Tensorflow 1.12.0, and received the same error message when trying to set `classifier.model_dir` to some other value. Should I be setting this variable through the super class?\r\nThanks!", "The class DNNClassifier is deprecated so you should use the new estimator module which is at a different location tf.estimator. In the new one, you can give the model_dir as a parameter of `Estimator`.\r\n\r\n\r\nOn Feb 21 2019, at 5:05 pm, Aditi Nataraj <notifications@github.com> wrote:\r\n> Hmm, I tried running the same code in my original post with Tensorflow 1.12.0, and received the same error message when trying to set classifier.model_dir to some other value. Should I be setting this variable through the super class?\r\n> Thanks!\r\n>\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub (https://github.com/tensorflow/tensorflow/issues/25772#issuecomment-466231401), or mute the thread (https://github.com/notifications/unsubscribe-auth/AC2ZafF8c-6jalBQnEY3tDRgcjAFlMiIks5vP0JKgaJpZM4a8vBS).\r\n>\r\n>\r\n\r\n", "@adt123 could you use latest TF version and follow @Sayan-Paul suggestion to solve this issue. If it was already solved, please close the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 25771, "title": "TFTRT: Use IInt8EntropyCalibrator2 for TRT 5.1 onward", "body": "TRT 5.1 added IInt8EntropyCalibrator2 which is an improvement over IInt8EntropyCalibrator.\r\n\r\nA few differences:\r\n1) Calibration table is generated prior to fusions, so it can be used across\r\n   engines with different fusions.\r\n2) \"Per-tensor scaling\" (e.g for a concat, all inputs along with output will\r\n   share a single scale factor. Previously, all connections would have a\r\n   different scale factor requiring many extra scale operations with no\r\n   benefit).\r\n3) Pool op output scale = input scale (shown to produce better accuracy)", "comments": []}, {"number": 25770, "title": "[Intel MKL] Fix incorrect way to dump optimized graph", "body": "This PR fixes issue 25674. It simply adds a check to ensure that\r\na graph is valid before dumping it.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "CLA;:yes justified because original author wanted it in 1.13 and submitted it to master. https://github.com/tensorflow/tensorflow/pull/25726#issuecomment-463844211"]}, {"number": 25769, "title": "Fix regression in XLA in TensorFlow 1.13 RC0, RC1", "body": "Only convert BackpropFilterConv to depthwise convolution if format is\u2026 \u2026 NHWC.\r\n\r\nThe logic for the conversion has this assumption. This CL makes sure that this\r\nassumption holds, and adds tests for NCHW format.\r\n\r\nFixes #25734\r\n\r\nPiperOrigin-RevId: 225534508", "comments": []}, {"number": 25768, "title": "tf.contrib.quantize.create_eval_graph does not add min/max node to bias", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux archlinux 4.19.2-arch1-1-ARCH #1 SMP PREEMPT Tue Nov 13 21:16:19 UTC 2018 x86_64 GNU/Linux`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `binary`\r\n- TensorFlow version (use command below): `v1.12.0-0-ga6d8ffae09 1.12.0`\r\n- Python version: `3.6.7`\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: `none`\r\n- GPU model and memory: `none`\r\n\r\n**Describe the current behavior**\r\n\r\nI want to convert a model to TFLite with quantized uint8 as inference type. I add the min/max nodes to the graph by calling `tf.contrib.quantize.create_eval_graph`, then I load the trained values (both the weights and the min/max values). I try to convert the model with TFLiteConverter, but I get errors of this kind:\r\n\r\n `Array conv2d/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.`\r\n\r\nI look at my graph (the one modified by `tf.contrib.quantize.create_eval_graph`) and I don't see min/max nodes for biases.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe biases should have min/max nodes to be used by TFLiteConverter after calling `tf.contrib.quantize.create_eval_graph`.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis example can reproduce this bug using a single Conv2d layer. Note that for the sake of keeping the example simple, I simulate loading the weights and the values of the min/max nodes (learned during training) by simply running `tf.global_variables_initializer()`. \r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.lite as tflite\r\n\r\nsess = tf.keras.backend.get_session()\r\n\r\n# create model\r\nx = x_input = tf.keras.layers.Input([10,10,3])\r\nx = tf.keras.layers.Conv2D(1, 3)(x)\r\nmodel = tf.keras.Model(x_input, x)\r\n\r\n# add fake quantization nodes \r\ntf.contrib.quantize.create_eval_graph(sess.graph)\r\n\r\n# simulate loading weights and quantization min/max values\r\nsess.run(tf.global_variables_initializer())\r\n\r\n# try to quantize\r\ntoco_converter = tflite.TFLiteConverter.from_session(sess, model.inputs, model.outputs)\r\ntoco_converter.post_training_quantize = True\r\ntoco_converter.inference_type = tflite.constants.QUANTIZED_UINT8\r\ntoco_converter.inference_input_type = tflite.constants.QUANTIZED_UINT8\r\ntoco_converter.quantized_input_stats = {model.inputs[0].name.split(':')[0]: (0., 255.)}\r\nmodel_tflite_binary = toco_converter.convert()\r\n```\r\ngives the following error:\r\n```\r\n2019-02-14 15:12:39.980124: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 8 operators, 13 arrays (0 quantized)\r\n2019-02-14 15:12:39.980218: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 8 operators, 13 arrays (0 quantized)\r\n2019-02-14 15:12:39.980322: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (1 quantized)\r\n2019-02-14 15:12:39.980352: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 2 operators, 5 arrays (1 quantized)\r\n2019-02-14 15:12:39.980371: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 2 operators, 5 arrays (1 quantized)\r\n2019-02-14 15:12:39.980401: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:129] Array conv2d/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.\r\n```\r\n\r\nWhen I look at the content of tf.global_variables(),  I see nothing related to bias_quant: \r\n```\r\n[<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 1) dtype=float32>,\r\n <tf.Variable 'conv2d/bias:0' shape=(1,) dtype=float32>,\r\n <tf.Variable 'conv2d/weights_quant/min:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'conv2d/weights_quant/max:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'conv2d/act_quant/min:0' shape=() dtype=float32_ref>,\r\n <tf.Variable 'conv2d/act_quant/max:0' shape=() dtype=float32_ref>]\r\n```\r\n\r\nSame thing when I look as the graph in tensorboard:\r\n```python\r\nwith tf.summary.FileWriter('/tmp/conv') as fw:\r\n    fw.add_graph(sess.graph)\r\n```\r\n\r\n![picture-of-tensorboard](https://user-images.githubusercontent.com/14935326/52816460-48a84080-306f-11e9-9cb2-e3fb2a044f58.png)\r\n\r\n**Other info / logs**\r\n\r\nEvent file from FileWriter: [log.zip](https://github.com/tensorflow/tensorflow/files/2866713/log.zip)", "comments": ["As a side note, if I do `use_bias=False` in the Conv2d, no quantization node is added. The content of `tf.global_variables()` is simply `[<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 1) dtype=float32>]`.", "I went through the source code of eval graph api, while I was working on quantization. fake layers don't get added at all the nodes.\r\nfake layer gets added after (conv2d + bias) or (conv2d + relu) combo as act_quant/***\r\n\r\ntf1.13 has more support for quantization than tf1.12 has.\r\ntry using tf1.13 for more robustness.\r\n\r\nHowever, it all depends on the model you want to quantize.", "Yes, I noticed that. Also, you cannot use standalone \"keras\" because it does not work. I needed to stick with tf.keras. I could not use batch norm neither (from tf.keras or tf.layers). \r\n\r\nAs a side note, I was able convert the mobilenet models from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/models.md#image-classification-quantized-models), but the predictions from my .tflite models were different then theirs. I can't explain it.\r\n\r\nAnyways, I was able to quantize a really simple Conv2D + ReLU network. I may try with 1.13 later.", "Use batch_norm from tf.contrib.layers. This works as it considers fusedbatchnorm by default, which gets folded with the convolution before it when you call create_eval_graph(). create_training_graph() for training in a similar manner.\r\n\r\nWhen you use this batch norm, do not forget to add UPDATE_OPS to the control dependencies when you train", "Is it possible to do this with keras API? I mean, can we add control dependencies when training with `model.fit_generator`?", "> the\r\n\r\nhi, i meet the problem that the batchnormalization can not get the min max value\u3002how i set the UPDATE_OPS to the control dependencies when i train\u3002can you give a demo\uff1f", "Hi,\r\n\r\n########\r\nx = tf.placeholder(tf.float32, [None, 1], 'x')\r\ny = tf.layers.batch_normalization(x, training=is_training)\r\n\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    y = tf.identity(y)\r\n##########\r\n\r\nwill work.\r\nAnother way, you can create training step under \" with tf.control_dependencies(update_ops):  \"\r\n\r\n--- \r\n\r\n", "my codes just try add the with tf.control_dependencies(update_ops): when update the gradients like \r\n*****\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n       with tf.control_dependencies(update_ops):\r\n               self.train_op = optimizer.apply_gradients(avg_grads_var, global_step=global_step)\r\n*****\r\nis right\uff1f\uff1f\r\n\r\nor\r\ni need add \u201cupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\ny = tf.identity(y)\u201d\r\nin every back of bn layers\uff1f\uff1f", "> Hi,\r\n> \r\n> ########\r\n> x = tf.placeholder(tf.float32, [None, 1], 'x')\r\n> y = tf.layers.batch_normalization(x, training=is_training)\r\n> \r\n> update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n> with tf.control_dependencies(update_ops):\r\n> y = tf.identity(y)\r\n> ##########\r\n> \r\n> will work.\r\n> Another way, you can create training step under \" with tf.control_dependencies(update_ops): \"\r\n\r\nforgot to thanks for you first", "Yeah, any one works ", "> Yeah, any one works\r\n\r\ni also meet the problem \r\n\"\r\nArray kws_model/KWS_Model/tower_0/CNN_V1/first_bn/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array kws_model/KWS_Model/tower_0/CNN_V1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\n\r\n\"\r\ni have tried many methods to fix it. but didn't work.\r\n===========================\r\nonly i remove the batch_normalization layers or i add converter.default_ranges_stats=(0,6). but i know these are not the true way.\r\n\r\n", "> > Yeah, any one works\r\n> \r\n> # i also meet the problem\r\n> \"\r\n> Array kws_model/KWS_Model/tower_0/CNN_V1/first_bn/FusedBatchNorm_mul_0, which is an input to the Add operator producing the output array kws_model/KWS_Model/tower_0/CNN_V1/Relu, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\\n\r\n> \"\r\n> i have tried many methods to fix it. but didn't work.\r\n> only i remove the batch_normalization layers or i add converter.default_ranges_stats=(0,6). but i know these are not the true way.\r\n\r\nHey , Do you have any solution to deal the DEFAULT_RANGES_MIN/MAX now?\r\nI tried to add MinMax information into my model use Post-training method by (/tensorflow/bazel-bin/tensorflow/lite/toco/toco) . \r\nBut i don't know how to use the flage(\t--arrays_extra_info_file=\"\"      \tstring\tPath to an optional file containing a serialized ArraysExtraInfo proto allowing to pass extra information about arrays not specified in the input model file, such as extra MinMax information.).\r\nPlease give me some suggestion. thx\r\n\r\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25768\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25768\">No</a>\n"]}, {"number": 25767, "title": "Provide a warning for CUDA compute capabilities less than 3.5 when building.", "body": "PR for issue #23951. Let me know of any changes that are required, and I'll be happy to make them.", "comments": ["Thanks for working on this. @gunan can correct me if I'm wrong, but `configure.py` already has a small warning when the compute capability provided is less than 3.*. You could probably get rid of `warning_flag` and replace the whole construct with something like this:\r\n\r\n```python\r\nver = float(m.group(0))\r\nif ver < 3.5:\r\n  print('ERROR: TensorFlow only supports Cuda compute capabilities 3.5 or higher. '\r\n          'Please re-specify a list of compute capabilities, excluding version {}.'.format(ver))\r\n  all_valid = False\r\n ```\r\n\r\nThere's no warning in the prompt for a compute capability either. Can you change it to this?\r\n\r\n```\r\n...build time and binary size, and that TensorFlow only supports compute capabilities > 3.5.\r\n```", "> Thanks for working on this. @gunan can correct me if I'm wrong, but `configure.py` already has a small warning when the compute capability provided is less than 3.*. You could probably get rid of `warning_flag` and replace the whole construct with something like this:\r\n> \r\n> ```python\r\n> ver = float(m.group(0))\r\n> if ver < 3.5:\r\n>   print('ERROR: TensorFlow only supports Cuda compute capabilities 3.5 or higher. '\r\n>           'Please re-specify a list of compute capabilities, excluding version {}.'.format(ver))\r\n>   all_valid = False\r\n> ```\r\n> \r\nThis is what I had concluded after looking at the original issue...\r\n- **capabilities < 3.0**: TensorFlow cannot be build.\r\n- **3.0 < capabilities < 3.5**: TensorFlow can be build but would not work.\r\n- **capabilities >= 3.5** TensorFlow can be build and works as expected.\r\n\r\nActually, by looking at the discussions in the issue thread, I thought that we still want to allow the ability to build TensorFlow if (**3.0 < compute_capabilities < 3.5**) but we simply wanted to include a warning saying \"Hey, you can build with this capability, but it would not be useful\". This code is in accordance with that.\r\n\r\nI agree with what you have suggested. I don't see the point of allowing to build for capabilities less than 3.5 if TensorFlow is ultimately not usable with them. Should I proceed to make the appropriate changes?\r\n\r\n> There's no warning in the prompt for a compute capability either. Can you change it to this?\r\n> \r\n> ```\r\n> ...build time and binary size, and that TensorFlow only supports compute capabilities > 3.5.\r\n> ```\r\n\r\nAlright, I'll include this as well.\r\n", "@angersson, I have made the required changes, does the patch look okay?", "FYI - as mentioned in  #26394 - it looks like TF could be build with compute capability < 3.0 if XLA is not enabled."]}, {"number": 25766, "title": "TFTRT: Allow FP16 kernels in INT8 mode", "body": " \r\nChange deprecated setHalf2Mode -> setFp16Mode. setHalf2Mode was deprecated in TRT 4.0.\r\n\r\nAllow int8 mode to use fp16 kernels when they are more performant. Setting a precision mode is not a strict limitation. Instead, it simply allows TRT to consider kernels of the given precision. Previously, our int8 mode only considered fp32 and int8 kernels. But we can allow it to also consider fp16 kernels when they perform better. This will result in better performance overall.\r\n\r\nTRT also does not support INT8 mode for many layers, but does support them in FP16. Previously we would've been using the FP32 kernels for those layers but now we can use the FP16 kernels.", "comments": ["Thanks @smit-hinsu. I haven't actually tried it yet, but it was recommended by the TRT team. We do have a set of benchmark models which are tested monthly."]}, {"number": 25765, "title": "[INTEL MKL] Added support for Quantized-Conv2D and Pad fusion.", "body": "", "comments": ["@penpornk Thanks for quickly reviewing the PR! I have addressed your review comments. Let me know if you need anything else.", "@penpornk I have changed `padding_list_` to be of type `Tpadding` instead of `int32`.\r\n\r\nAlso, I have added an additional registration for when `Tbias` is a `float` since we support both `qint32` and `float` types for Tbias for `Quantized-Conv2D + bias + sum + relu + requantize` fusion.", "@pragyaak The unit test failures don't seem to be related to the changes made in this PR. Please let me know if you think otherwise.", "Thanks for the explanation, @penpornk! I tried modifying the API golden files per your suggestion, but only `tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt` ended up getting updated, and `tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt` remains unmodified. However, I do see that `bazel test tensorflow/tools/api/tests:api_compatibility_test` unit test passes locally even with just this change.", "@bhavani-subramanian We pulled a PR in from last week with just changes to `v2`, but it failed the internal test for `v1` and we had to add changes to `v1` anyway. So I think we should add it now. I'm not sure how to get the OSS test to modify `v1`, though. (Apart from modifying it manually from the diff.) \r\n\r\nI'm asking @annarev: \r\nDo you know how to make the api_compatibility_test also modify the `v1` file? Thank you!\r\n\r\n(I'll tag `API review` after also getting the changes for `v1`.)", "I submitted a fix for that issue last week. @bhavani-subramanian can you sync to head and check if api_compatibility_test updates both v1 and v2 files now?\r\n\r\nSorry for the trouble!", "Thanks for the quick response, @annarev. I was able to add the `v1` file after merging my branch with the master.\r\n\r\n@penpornk Please let me know if anything else is needed from my end. Thanks!", "@penpornk The two failures in Ubuntu contrib pass when run locally. Not sure how to reproduce the error at my end.", "@bhavani-subramanian I'll rerun the tests. I can't see the log of `Ubuntu Python 2`. The rest looks unrelated.", "@penpornk Sorry to bug you again. The unit test that fails in `MacOS Python2 and CC` (`//tensorflow/python/saved_model:load_test`) passes when run locally. From the error log, it looks like the test failed because it was unable to create a new thread either because the system either ran out of resources (or) due to hitting a system limit on the number of threads per process ([Reference](https://stackoverflow.com/questions/26691340/multi-threading-in-c-throws-thread-constructor-failed-resource-temporarily-un)).\r\n\r\nIt doesn't seem like this failure is related to my PR.", "@bhavani-subramanian Ah. Don't worry about them! All three failing tests right now are existing failures (`MacOS Python2 and CC`, `Windows Bazel`, and `Windows Bazel GPU`) I mainly reran the tests because `Ubuntu Python 2` and `Ubuntu Contrib` shouldn't be failing. They are passing now so I think we are fine.\r\n\r\nI haven't tagged `ready to pull` yet because this still needs API review (because we changed the golden files).", "@pragyaak gentle reminder regarding the API review for this PR.", "Apologies for the delay, but this PR finally got through the API review and is getting merged internally. But we got outdated files, specifically,\r\n```\r\n//tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt\r\n//tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt\r\n```\r\nCould you please help resolve these? Thank you!", "@penpornk I merged my branch with the master and ran the commands below. But the above files are still not getting updated:\r\n```\r\n $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\nAre there other commands I should be running instead?", "@bhavani-subramanian Do you have the changes from the latest commit on these two files?: https://github.com/tensorflow/tensorflow/commit/4294edea3e80c746eac400262628f852b66d02e5", "@bhavani-subramanian Never mind! It's getting merged now. :)", "Thanks @penpornk! I am somehow not able to generate the golden files despite having the `4294ede` commit. Anyway, glad that its getting merged now :)"]}]