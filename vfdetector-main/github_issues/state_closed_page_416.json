[{"number": 41430, "title": "Training hangs when performing an assign op on aggregated gradients in SyncReplicasOptimizer", "body": "I'm trying to fetch the aggregated gradients which are computed after aggregating all the replicas together in SyncReplicasOptimizer. To do this, I added an assign op on a variable that I created and assigned the aggregated gradients value (from the aggregated_grads_and_vars). Basically in sync_replicas_optimizer.py apply_gradients() call I did something like:\r\n\r\n```\r\nvar1 = tf.Variable(0.0, trainable=False)\r\ndef apply_gradients(grads_and_vars, global_step):\r\n   \r\n   *aggregated_grads_and_vars returned based on replicas_to_aggregate*\r\n   aggregated_grads_and_vars = zip(aggregated_grad, var_list)\r\n   \r\n   test_var = tf.assign(var1, aggregated_gradients)\r\n\r\n   *execute everything else in the fn and return train_op*\r\n```\r\n\r\nLater, I call *session.run(train_op)* to execute a training steps followed by  *session.run(test_var)*.\r\nMy training process goes through with the first training and then hangs. The problem is with *session.run(test_var)*. \r\n\r\nMy approach works when I use any other asynchronous optimizer (tested on *MomentumOptimizer*, *AdamOptimizer* and *GradientDescentOptimizer*) but not for *SyncReplicasOptimizer*.\r\n\r\nCould this have something to do with *SyncReplicasOptimizerHook* ? \r\n\r\nAny lead would be greatly appreciated.\r\n\r\n", "comments": ["@sahiltyagi4,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!\r\n", "I'm using Tensorflow r1.14 for distributed training.\r\nThe source code is available at [https://github.com/sahiltyagi4/tensorflow/tree/r1.14](url).\r\n\r\nIn particular, I'm adding some computation to *apply_gradients* fn in *sync_replicas_optimizer.py* from line [https://github.com/sahiltyagi4/tensorflow/blob/r1.14/tensorflow/python/training/sync_replicas_optimizer.py#L379](url), and doing an assign op at [https://github.com/sahiltyagi4/tensorflow/blob/r1.14/tensorflow/python/training/sync_replicas_optimizer.py#L385](url) to a local variable with some random initial value called *_grad_variance*.\r\n\r\nThe basic snippet is like:\r\n```\r\ndef apply_gradients(grads_and_vars):\r\n    self._grad_variance = *some local variable with some initial value*\r\n    aggregated_grads = *sync_replica_optimizers' gradient aggregation from multiple workers with accumulators*\r\n    variance_list = []\r\n    for g2 in aggregated_grad:\r\n          variance_list.append(tf.reshape(g2, [-1]))\r\n    vars_concat = tf.concat(variance_list, 0)\r\n    flattened_gradients = tf.reshape(vars_concat, [-1], name='gradientprint123')\r\n    gradient_length = tf.shape(flattened_gradients, name='gradientslength')\r\n    test_var2 = tf.assign(self._grad_variance, tf.math.reduce_variance(vars_concat), name='pqrstuv1234')\r\n\r\n```\r\n\r\nI build this tensorflow (WITHOUT CUDA, MPI, etc.) and use this in Resnet at [https://github.com/sahiltyagi4/tensorflow/blob/r1.14/models/official/resnet/resnet_regular.py](url).\r\n\r\nUsing estimators, I call *mon_sess.run* on the assign op at [https://github.com/sahiltyagi4/estimator/blob/dynamic_v2/tensorflow_estimator/python/estimator/estimator.py#L1543](url) (I build the estimator from source too btw). This is where my training comes to a halt (know this because previous *mon_sess.run* on train_op executes for the first step).\r\n\r\nCommenting out the assign op, my training executes as it should. Also, in  the apply_gradients fn, when I use the grad_and_vars (returned from compute_gradients fn) instead of aggregated_grads_and_vars (from SyncReplicasOptimizer), I'm able to execute the assign op and fetch its value successfully. But this returns the variance on per-worker calculated gradients, and not the aggregated gradients across all workers in distributed training\r\n\r\nThis issue is persistent only in synchronous training with SyncReplicasOptimizer. Doing the same in async with another optimizer (tested *Momentum*, *Adam* and *GradientDescent* optimizers) works successfully.\r\nI'm not sure but I'm guessing if this has something to do with *SyncReplicasOptimizerHook* maybe?\r\n", "@amahendrakar I've added some more comments explaining the problem in detail", "@sahiltyagi4,\r\nThank you for the update. The example is fairly complex, and it is hard for us to pinpoint the issue.\r\n\r\nCan you please get the example down to the simplest possible repro? That will allow to determine the source of the issue easily. Thanks!", "Also, is there a specific reason you are using TF v1.14? Please update TensorFlow to v2.2 or v1.15 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41429, "title": "Scalar shape", "body": "Added function to C API to create a scalar shape for shape inference. \r\n@annarev  @bmzhao ", "comments": []}, {"number": 41428, "title": "Python Keras code out of memory for no apparent reason", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Server 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.2 / 7.6.5\r\n- GPU model and memory: Geforce GTX Titan X 12 GB RAM, 16 GB system RAM\r\n\r\n**Describe the current behavior**\r\nSee this Stackoverflow question for details: https://stackoverflow.com/questions/62917349/python-keras-code-out-of-memory-for-no-apparent-reason\r\n\r\n**Describe the expected behavior**\r\nThe code shouldn't cause Python to crash with code 137. I have over 12 GB system RAM available and nothing else is using the GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCode causes Python to crash with exit code 137 around 1 (sometimes a bit more, sometimes a bit less) iteration of the outer for loop. System memory usage keeps increasing while the program is running.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom sklearn.datasets import fetch_openml\r\nfrom sklearn.utils import shuffle\r\n\r\ndata, targets = shuffle(*fetch_openml('CIFAR_10', version=1, return_X_y=True)) # same happens if I force these to be float32s\r\ntrain_sz = 50000\r\nX_train, X_test, y_train, y_test = data[:train_sz, :], data[train_sz:, :], np.asarray(targets[:train_sz], dtype=np.int), np.asarray(targets[train_sz:], dtype=np.int)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.Input(shape=(X_train.shape[1],)))\r\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(10))\r\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer='adam')\r\n\r\ns = 0\r\nfor _ in range(500):\r\n    for i in range(100):\r\n        layers = []\r\n        for layer in model.get_weights():\r\n            layers.append(np.random.normal(0, 1, layer.shape))\r\n        model.set_weights(layers)\r\n        eval = model.evaluate(X_train, y_train)\r\n        s += eval\r\n        print(f'Done {i}')\r\nprint(s)\r\n```", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1523dedba35e4dca42822053dce2512a/41428.ipynb). Thanks!", "@ivdorelian Thanks for the issue!\r\n\r\nCan you try running `tf.keras.backend.clear_session()` periodically when calling the `set_weights` in a loop? There are some global-graph-related things in Keras that we are working on getting rid of, but the `clear_session()` call usually helps to release the memory from these items", "@omalleyt12 thanks for the reply!\r\n\r\nUnfortunately, adding that at the end of the second for loop doesn't help.\r\n\r\nWhat did help though was doing it like this:\r\n\r\n```\r\npreds = model(X_train)\r\neval = my_loss(y_train, preds)\r\n```\r\n\r\nNote that calling `.predict()` followed by `my_loss(...)` results in the same issue.\r\n\r\nIn my case this isn't a huge issue, although it would be nice to be able to pass in a batch size to evaluate or predict.", "@ivdorelian Is this still an issue for you. There were some performance improvements over the last few months. I tested your code with `tf-nightly` and it was running without memory issues. i ran outer `for` loop for 10 iterations without any issue. As it was taking long time, i interrupted execution. \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/ccff56d41112909cdee938cb411cd518/41428.ipynb) is a gist for your reference. \r\n\r\nThe only change i did was adding `tf.keras.backend.clear_session()` as mentioned by @omalleyt12 and added `gc.collect()`\r\n\r\n```\r\ns = 0\r\nfor j in range(100):\r\n    for i in range(100):\r\n        layers = []\r\n        for layer in model.get_weights():\r\n            layers.append(np.random.normal(0, 1, layer.shape))\r\n        model.set_weights(layers)\r\n        eval = model.evaluate(X_train, y_train)\r\n        s += eval\r\n        tf.keras.backend.clear_session()\r\n        gc.collect()\r\n        print(f'Done {i}')\r\n    print('*'*100)\r\n    print(f'Done {j}')\r\n        \r\nprint(s)\r\n```\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41427, "title": "Added arg: name to tf.nn.swish", "body": "Resolves #40592\r\nAdded name argument for tf.nn.swish and ops.name_scope, also updated docs to cover those changes.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41427) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41427) for more info**.\n\n<!-- ok -->"]}, {"number": 41426, "title": "[Cherrypick r2.3] Fix critical bug with `add_loss` TFOpLayer graph construction that caused incorrect loss values and backprop issues", "body": "PiperOrigin-RevId: 320257330\nChange-Id: I0a030bc7632735b152454657fd15e41539b4e4bd", "comments": []}, {"number": 41425, "title": "Cannot Run Tensorflow2.0 saved model with Java ", "body": "In java, cannot feed the input tensor to the loaded model (model which is saved in tf2.0 in .pb file):\r\n\r\n### My model: (tf2.0)\r\n\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nnn_input (Flatten)           (None, 5)                0         \r\n_________________________________________________________________\r\n...\r\n... \r\n...  \r\n_________________________________________________________________\r\nnn_output (Dense)            (None, 1)                 513       \r\n=================================================================\r\n```\r\n\r\n### My java code:\r\n\r\n```\r\n        final int NUM_PREDICTIONS = 1;\r\n        final int INPUT_SIZE = 5;\r\n        try (SavedModelBundle b = SavedModelBundle.load(\"./nn_visualization/NNRegressor_saved_model/\", \"serve\")) {\r\n            Session sess = b.session();\r\n            Tensor x = Tensor.create(new long[] { INPUT_SIZE }, FloatBuffer.wrap(new float[] {10, 9, 23, 7, 1}));\r\n            float[] y = sess.runner().feed(\"nn_input\", x).fetch(\"nn_output\").run().get(0).copyTo(new float[NUM_PREDICTIONS]);\r\n            System.out.println(y[0]);\r\n        }\r\n\r\n```\r\n\r\n### error:\r\n\r\n```\r\nException in thread \"main\" java.lang.IllegalArgumentException: No Operation named [nn_input] in the Graph\r\n\tat org.tensorflow.Session$Runner.operationByName(Session.java:380)\r\n\tat org.tensorflow.Session$Runner.parseOutput(Session.java:389)\r\n\tat org.tensorflow.Session$Runner.feed(Session.java:131)\r\n\tat HelloTensorFlow.main(HelloTensorFlow.java:25)\r\n```\r\n### Instalations\r\n\r\n```\r\n$java\r\njdk version: 1.8\r\n\r\n$ tf model save version: \r\npython3 -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n$ java used tensorflow version:\r\nTensorFlow.version(): 1.14.0\r\n```\r\n\r\n### Assumptions\r\n\r\nI assume that the problem arises due to the difference in the tensorflow versions in which the model is saved (tf2.0) and in which the model is loaded (tf1.4)?\r\n\r\nI guess I'm wrong with the first argument of the feed method. Any help?\r\n\r\nThanks in advance,\r\nMilan", "comments": ["@MilanCugur \r\n\r\nDid you try with latest 2.x version for saving the model and then load the same model and see whether the issue persists or not.It will be great if you share us complete code snippet to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram Thanks for the fast response!\r\n\r\nDefinitely, I give the complete code below: \r\n\r\n# Model creation (usual procedure)\r\nModel was created in **Python version 3.8.2** and **Tensorflow version 2.2.0**\r\n\r\n# Model summary: \r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nnn_input (Flatten)           (None, 64)                0         \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 64)                256       \r\n_________________________________________________________________\r\ndense (Dense)                (None, 64)                4160      \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, 64)                256       \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 128)               8320      \r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (None, 128)               512       \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 256)               33024     \r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (None, 256)               1024      \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 256)               65792     \r\n_________________________________________________________________\r\nbatch_normalization_4 (Batch (None, 256)               1024      \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 256)               0         \r\n_________________________________________________________________\r\ndense_4 (Dense)              (None, 128)               32896     \r\n_________________________________________________________________\r\nbatch_normalization_5 (Batch (None, 128)               512       \r\n_________________________________________________________________\r\ndense_5 (Dense)              (None, 256)               33024     \r\n_________________________________________________________________\r\nbatch_normalization_6 (Batch (None, 256)               1024      \r\n_________________________________________________________________\r\ndense_6 (Dense)              (None, 512)               131584    \r\n_________________________________________________________________\r\nbatch_normalization_7 (Batch (None, 512)               2048      \r\n_________________________________________________________________\r\ndense_7 (Dense)              (None, 512)               262656    \r\n_________________________________________________________________\r\nbatch_normalization_8 (Batch (None, 512)               2048      \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 512)               0         \r\n_________________________________________________________________\r\nnn_output (Dense)            (None, 1)                 513       \r\n=================================================================\r\nTotal params: 580,673\r\nTrainable params: 576,321\r\nNon-trainable params: 4,352\r\n_________________________________________________________________\r\n```\r\n\r\n# Model save (using callbacks)\r\nAlso **Python version 3.8.2** and **Tensorflow version 2.2.0**\r\n\r\n```\r\ntf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False,\r\n                                                        monitor='val_mae', mode='min', save_best_only=True)\r\n```\r\n\r\n# Model load+predict (Python, wrking one)\r\nAlso **Python version 3.8.2** and **Tensorflow version 2.2.0**\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(\"tf.version: \", tf.__version__)\r\n\r\ndef printTensors(pb_file):\r\n    model = tf.keras.models.load_model(pb_file)\r\n    x = [10, 9, 23, 7, 1, 2, 0, 8, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 13, 5, 23, 24, 13, 0, 0, 0, 0, 0, 12, 2, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2]\r\n    print(model.predict([x]))\r\n    \r\nprintTensors(./NNRegressor_model/\")\r\n```\r\nProduce output: \r\n`[[0.2698144]]`\r\n\r\n# Model load + predict (Java, throws error)\r\n```\r\nimport org.tensorflow.Graph;\r\nimport org.tensorflow.Session;\r\nimport org.tensorflow.Tensor;\r\nimport org.tensorflow.TensorFlow;\r\nimport org.tensorflow.SavedModelBundle;\r\nimport java.nio.FloatBuffer;\r\n\r\npublic class HelloTensorFlow {\r\n    public static void main(String[] args) throws Exception {\r\n\r\n        // good idea to print the version number, 1.2.0 as of this writing\r\n        System.out.println(\"TensorFlow.version(): \"+TensorFlow.version());\r\n        final int NUM_PREDICTIONS = 1;\r\n        final int INPUT_SIZE = 64;\r\n\r\n        // load the model Bundle\r\n        try (SavedModelBundle b = SavedModelBundle.load(\"./NNRegressor_model/\", \"serve\")) {\r\n            System.out.println(\"SavedModelBundle: \" + b.metaGraphDef());\r\n            // create the session from the Bundle\r\n            Session sess = b.session();\r\n            // create an input Tensor\r\n            Tensor x = Tensor.create(new long[] { INPUT_SIZE }, FloatBuffer.wrap(new float[] {10, 9, 23, 7, 1, 2, 0, 8, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 13, 5, 23, 24, 13, 0, 0, 0, 0, 0, 12, 2, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2}));\r\n\r\n            // run the model and get the result\r\n            float[] y = sess.runner().feed(\"nn_input\", x).fetch(\"nn_output\").run().get(0).copyTo(new float[NUM_PREDICTIONS]);\r\n\r\n            // print out the result.\r\n            System.out.println(y[0]);\r\n        }\r\n   }\r\n}\r\n```\r\n\r\nCompile with:\r\n`javac -cp /home/cugur/Downloads/jbindings/jtf/libtensorflow-1.14.0.jar HelloTensorFlow.java`\r\n\r\nRun with: \r\n`java -cp /home/cugur/Downloads/jbindings/jtf/libtensorflow-1.14.0.jar:. -Djava.library.path=/home/cugur/Downloads/jbindings/jtf/libtensorflow_jni-cpu-linux-x86_64-1.14.0 HelloTensorFlow`\r\n\r\nWhere I use **jdk1.8** and tensorflow jar&jni downoladed withihn official [https://www.tensorflow.org/install/lang_java](https://www.tensorflow.org/install/lang_java)\r\n\r\nThrows out an error:\r\n```\r\nTensorFlow.version(): 1.14.0\r\n2020-07-16 09:32:51.532701: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /home/cugur/Desktop/graal_dataset/dataset_v3/nn_visualization/NNRegressor_model/\r\n2020-07-16 09:32:51.541743: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-07-16 09:32:51.552082: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-07-16 09:32:51.576523: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2599500000 Hz\r\n2020-07-16 09:32:51.576926: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc804a08e20 executing computations on platform Host. Devices:\r\n2020-07-16 09:32:51.576946: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-07-16 09:32:51.610653: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\r\n2020-07-16 09:32:51.753134: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2020-07-16 09:32:51.782056: I tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /home/cugur/Desktop/graal_dataset/dataset_v3/nn_visualization/NNRegressor_model/\r\n2020-07-16 09:32:51.836027: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 303331 microseconds.\r\nSavedModelBundle: [B@330bedb4\r\nException in thread \"main\" java.lang.IllegalArgumentException: No Operation named [nn_input] in the Graph\r\n\tat org.tensorflow.Session$Runner.operationByName(Session.java:380)\r\n\tat org.tensorflow.Session$Runner.parseOutput(Session.java:389)\r\n\tat org.tensorflow.Session$Runner.feed(Session.java:131)\r\n\tat HelloTensorFlow.main(HelloTensorFlow.java:25)\r\n```\r\n\r\nAddition: \r\n\r\nThe same error occurred when running (javac and java commands) with nightly lbraries (TensorFlow.version(): 2.2.0) which is downloaded from: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md.\r\n\r\n# In addition\r\nCommands **saved_model_cli show --dir ./NNRegressor_model/ --all** confirms that the input and output tensors are indeed called as stated.", "**Update:**\r\n\r\nThe problem happen due to bad tensors naming! \r\nIt turns out that the input and output tensors are named with **serving_default_input_1:0** and **StatefulPartitionedCall:0**, which is in contrast to the names that were defined when creating the model, as well as the names that **saved_model_cli** output.\r\n\r\nWhen the names are changed, the code works properly.", "Glad it worked. Will close this issue now that it's fixed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41425\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41425\">No</a>\n", "Hi @MilanCugur \r\nI'm a newbie to tensorflow and I met same problem but I still not sure how to solve it. Could you please provide more detail codes about how you build the model and save it with the name of input and output?"]}, {"number": 41423, "title": "Lazy loading for s3_client and executor", "body": "@mihaimaruseac\r\nThis PR adds lazy-loading for s3_client and executor. This is the same as #41410 but I forgot to make `GetExecutor` static.", "comments": []}, {"number": 41422, "title": "Tensorflow keras timeseries prediction with X and y having different shapes", "body": "\r\nI am trying to do time series prediction with tensorflow and keras with `X` and `y` having different dimensions:\r\n\r\n```\r\nX.shape = (5000, 12)\r\ny.shape = (5000, 3, 12)\r\n```\r\nWhen I do the following\r\n\r\n```\r\nn_input = 7\r\ngenerator = TimeseriesGenerator(X, y, length=n_input, batch_size=1)\r\n\r\nfor i in range(5):\r\n    x_, y_ = generator[i]\r\n    print(x_.shape)\r\n    print(y_.shape)\r\n```\r\nI get as desired the output\r\n\r\n```\r\n(1, 7, 12)\r\n(1, 3, 12)\r\n(1, 7, 12)\r\n(1, 3, 12)\r\n...\r\n```\r\nThis is because my data is meteorological, I have 5000 days, for training in the array `X` I use a sliding window of 7 days, with each day containing 12 features (air pressure, temperature, humidity a.o.). And in the target array `y` I have sliding windows of 3 days, trying to predict the next 3 days to each window of 7 days.\r\n\r\nBut then when I try to fit the model I get an error due to the mismatch in the shape of the `X` and `y` arrays:\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(LSTM(4, input_shape=(None, 12)))\r\nmodel.add(Dense(1))\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\nhistory = model.fit_generator(generator, epochs=3).history\r\n```\r\n\r\n`ValueError: A target array with shape (1, 3, 12) was passed for an output of shape (None, 1) while using as loss \"mean_squared_error\". This loss expects targets to have the same shape as the output.`\r\n\r\nSo is there a way to adjust the architecture for the mismatch in the dimensions? Or is there a way to reshape `X` and y to make them work with this architecture? I tried the late reshaping `X` into (5000, 7, 12), but this gave also a dimensionality error. Tnx", "comments": ["1. The reason for the *ValueError* you encountered is that the model you build outputs a tensor with shape (None, 1) while your data *y* has a shape (None, 3, 12), but they don't match!\r\n2. I guess your intension is to build a model with input_shape=(None, 7, 12) & output_shape=(None, 3, 12). The best way is to change the shape of *Y* to (None, 36), as followed:\r\n```\r\ny=np.reshape(y,(-1,y.shape[-1]*y.shape[-2]))\r\n\r\nn_input = 7\r\ngenerator = TimeseriesGenerator(X, y, length=n_input, batch_size=1)\r\n\r\nmodel = Sequential()\r\nmodel.add(LSTM(64, input_shape=(7, 12)))\r\nmodel.add(Dense(Y.shape[-1]))\r\nmodel.summary()\r\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\r\nhistory = model.fit_generator(generator, epochs=3).history\r\n```", "@Nestak2 \r\nPlease refer to [this link](https://stackoverflow.com/questions/58578374/how-can-i-change-my-data-to-make-it-work-with-loss-mean-squared-error) with same error and let us know. Thanks!", "@Dynmi Thanks for the suggestion, I found out that I could also use an architecture with max pooling for my problem, e.g. this would work \r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(LSTM(4, return_sequences=True, input_shape=(n_input, 12)))\r\nmodel.add(MaxPool1D(2)) # also AvgPool1D is ok\r\nmodel.add(Dense(12))\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n\r\nmodel.summary()\r\nmodel.fit(generator, epochs=2)\r\n```\r\n\r\nDo you have an opinion which of the 2, maxpooling vs. your suggestion, would be a better solution (give more accurate results)?\r\nSomehow I fear to follow your suggestion of turning the shape of `y` from (None, 3, 12) to (None, 36), because the neural network would lose track of what feature is what, but this is only an intuition of mine and not tested", "Hhh, You can try both of them and see which one works better.\r\nAfter all, Practice is the sole criterion for testing truth.", "@Nestak2 \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41422\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41422\">No</a>\n"]}, {"number": 41421, "title": "Add qint8 and qint16 support for FillOp", "body": "This PR tries to address the issue raised in https://github.com/tensorflow/tensorflow/issues/26069#issuecomment-658767837 where\r\nqint8 and qint16 were not supported for FillOp.\r\n\r\nThis PR add qint8 and qint16 support for FillOp.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can you make the test also test in eager mode?\r\n\r\nAlso, we are seeing the following error, both in graph mode and in eager mode (removing the session related parts of the test):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 331, in _AssertCompatible\r\n    fn(values)\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 257, in _check_quantized\r\n    _check_failed(values)\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 251, in _check_failed\r\n    raise ValueError(v)\r\nValueError: 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \".../tensorflow/python/kernel_tests/constant_op_test.py\", line 463, in testQintDtype\r\n    z = array_ops.zeros([2, 3], dtype=dtype)\r\n  File \".../tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \".../tensorflow/python/ops/array_ops.py\", line 2769, in wrapped\r\n    tensor = fun(*args, **kwargs)\r\n  File \".../tensorflow/python/ops/array_ops.py\", line 2828, in zeros\r\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\r\n  File \".../tensorflow/python/framework/constant_op.py\", line 266, in constant\r\n    allow_broadcast=True)\r\n  File \".../tensorflow/python/framework/constant_op.py\", line 284, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 458, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 338, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected qint8, got 0 of type 'int' instead.\r\n```", "Thanks @mihaimaruseac , the PR has been updated with session part removed. Also, I casts the qint types to int32 type before comparing it with numpy. Think this will resolve the error raised. Please take a look and let me know if the issue still persists.", "This still fails internally\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 331, in _AssertCompatible\r\n    fn(values)\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 257, in _check_quantized\r\n    _check_failed(values)\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 251, in _check_failed\r\n    raise ValueError(v)\r\nValueError: 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \".../tensorflow/python/kernel_tests/constant_op_test.py\", line 463, in testQintDtype\r\n    z = array_ops.zeros([2, 3], dtype=dtype)\r\n  File \".../tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \".../tensorflow/python/ops/array_ops.py\", line 2769, in wrapped\r\n    tensor = fun(*args, **kwargs)\r\n  File \".../tensorflow/python/ops/array_ops.py\", line 2828, in zeros\r\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\r\n  File \".../tensorflow/python/framework/constant_op.py\", line 266, in constant\r\n    allow_broadcast=True)\r\n  File \".../tensorflow/python/framework/constant_op.py\", line 284, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 458, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 338, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected qint8, got 0 of type 'int' instead.\r\n```", "@yongtang Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "@mihaimaruseac @gbaned I think I finally figured out the error thrown out during the internal test. The message is caused by `tf.qint8`'s `zero` could not use `0` directly as `0` is incompatible with tf.qint8. Instead I used the np.zeros and casted to tf.qint8 ()changed the following line:\r\n```\r\n+    elif dtype.is_quantized:\r\n+      zero = np.zeros([]).astype(dtype.as_numpy_dtype)\r\n```\r\n\r\nThe different is that qint8 is actually an `i1` object not an int in np:\r\n```\r\n>>> np.zeros([]).astype(tf.qint8.as_numpy_dtype)\r\narray((0,), dtype=[('qint8', 'i1')])\r\n```\r\n\r\nI have updated the PR and believe the issue should have been resolved. \r\n\r\nPlease give it another try and let me know if there are still any issues.", "@mihaimaruseac Another issue that is different, but similar to the internal test failure above is the `tf.ones` with quantized dtype. As they are not essential to this PR, I have created a separate PR #41603 with different test cases.", "Different failure now\r\n\r\n```\r\n  File \".../tensorflow/python/framework/fast_tensor_util.pyx\", line 97, in fast_tensor_util.AppendInt8ArrayToTensorProto\r\nValueError: Buffer dtype mismatch, expected 'int8_t' but got 'short'\r\n```", "Thanks @mihaimaruseac for the help. I think this is likely caused by dtype being reused (or graph being reused) in the internal test. I have split the tests into two to avoid this. The PR has been updated, I assume it will fix the error.\r\n\r\nPlease give another try and sorry for taking that long.", "Thank you for the prompt response and sorry that the mismatch between internal and open source tests makes this harder to merge.", "`//tensorflow/python/kernel_tests:constant_op_test` still fails :(\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \".../tensorflow/python/kernel_tests/constant_op_test.py\", line 471, in testQint16Dtype\r\n    z = array_ops.zeros([2, 3], dtype=dtype)\r\n  File \".../tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \".../tensorflow/python/ops/array_ops.py\", line 2769, in wrapped\r\n    tensor = fun(*args, **kwargs)\r\n  File \".../tensorflow/python/ops/array_ops.py\", line 2830, in zeros\r\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\r\n  File \".../tensorflow/python/framework/constant_op.py\", line 266, in constant\r\n    allow_broadcast=True)\r\n  File \".../tensorflow/python/framework/constant_op.py\", line 284, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \".../tensorflow/python/framework/tensor_util.py\", line 565, in make_tensor_proto\r\n    append_fn(tensor_proto, proto_values)\r\n  File \".../tensorflow/python/framework/fast_tensor_util.pyx\", line 97, in fast_tensor_util.AppendInt8ArrayToTensorProto\r\nValueError: Buffer dtype mismatch, expected 'int8_t' but got 'short'\r\n```", "Thanks @mihaimaruseac. I think the error is caused by the incorrect mapping of fast tensor <=> np which appear to be a bug unrelated. I have created a PR #41677 to address it.", "@mihaimaruseac With PR #41677 merged in, I have rebased and updated this PR. I think the internal test issue likely have been solved. Can you give it a try? Thanks a lot for the help in the process."]}, {"number": 41420, "title": "Docs: incomplete description of callbacks.ModelCheckpoint's save_best_only parameter", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\r\n\r\n## Description of issue (what needs changing):\r\n\r\n1. `mode`: \"{auto, min, max}\" should be `{auto, min, max}`, I guess (minor)\r\n2.  `save_best_only`: I believe the description is incomplete. With `save_best_only=True`, not only will \"the latest best model [...] not be overwritten\": but also the current model is not written at all, even if it has another filename than the \"latest best model\". This is kind of implied by the name of the parameter, but the description should include that, too.", "comments": ["Hi Can I create a PR to fix this?", "PR to fix this issue is here https://github.com/tensorflow/tensorflow/pull/41567\r\nPlease review, thank you!", "This is already fixed by https://github.com/tensorflow/tensorflow/pull/41567. Please check the updated `tf-nightly` page https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint?version=nightly.\r\n\r\nI am closing this issue as this was already resolved. Thanks!"]}, {"number": 41419, "title": "Output of tf.keras.utils.Sequence is automatically cast to Tensor even if it is a SparseTensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.5\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8\r\n\r\n\r\n**Describe the current behavior**\r\nAll outputs produced by `tensorflow.keras.utils.Sequence` are automatically cast to `Tensor` when calling `model.fit()`, even if they are of class `SparseTensor`.\r\nThis makes it impossible to use sparse operations in a model if the model is trained with the Keras training loop. \r\n\r\n**Describe the expected behavior**\r\n`model.fit()` should not automatically cast the inputs to `Tensor` if the inputs are supported by TensorFlow operations.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.utils import Sequence\r\n\r\n\r\nclass Dataset(Sequence):\r\n    def __init__(self):\r\n        pass\r\n\r\n    def __len__(self):\r\n        return 1\r\n\r\n    def __getitem__(self, idx):\r\n        return tf.SparseTensor([[0, 1], [1, 0]], [1, 1], (2, 2))\r\n\r\n\r\nclass Net(Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, x, **kwargs):\r\n        return x.values\r\n\r\n\r\nnet = Net()\r\nnet.compile()\r\nnet.fit(Dataset())\r\n```\r\n\r\n\r\nCauses the following error:\r\n```python\r\nAttributeError: 'Tensor' object has no attribute 'values'\r\n```\r\n", "comments": ["Possibly related issues: https://github.com/tensorflow/tensorflow/issues/16689, https://github.com/tensorflow/tensorflow/issues/41397\r\nIt seems like TF casts inputs to `Tensor` by default, even when it shouldn't", "Was able to reproduce the issue with TF v2.2, TF v2.3.0rc1 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/7b77c8a55e6003da0d01ca45ee7e129e/41419.ipynb). Thanks!", "The Keras Sequence utility works well with numpy arrays, but for TensorFlow composite tensor types like RaggedTensor and SparseTensor, we would recommend you use tf.data.Datasets. You can read more about how to use Datasets in your input pipeline here: https://www.tensorflow.org/guide/data", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41419\">No</a>\n"]}, {"number": 41417, "title": "Min_delta parameters on callbacks function of Keras model seems to not take effect", "body": "I am using the following callbacks function on a Keras model and I initialize the minimum delta to 0.002, so based on the documentation of Tensorflow/Keras any improvement in the validation loss function less than 0.002 won't be counted for an improvement. However, this seems to not get implemented in my case.\r\n\r\ncallback function:\r\n\r\n```\r\ndef callback(folder_path, saved_model_name, patience_value, logdir, hparams):\r\n    \r\n    # Initialize parameters\r\n    monitor_metric = 'val_loss'\r\n    minimum_delta = 0.002\r\n    patience_limit = patience_value\r\n    verbose_value = 1\r\n    mode_value = 'min'\r\n    weights_fname = os.path.join(os.getcwd(), '{0}/{1}.h5'.format(folder_path, saved_model_name))\r\n    \r\n    # Initialize callbacks\r\n    callbacks = [\r\n        \r\n        EarlyStopping(monitor=monitor_metric,\r\n                      min_delta=minimum_delta,\r\n                      patience=patience_limit,\r\n                      verbose=verbose_value,\r\n                      mode=mode_value,\r\n                      restore_best_weights=True),\r\n\r\n        ModelCheckpoint(filepath=weights_fname,\r\n                        monitor=monitor_metric,\r\n                        verbose=verbose_value,\r\n                        save_best_only=True,\r\n                        save_weights_only=True),\r\n        \r\n        tf.keras.callbacks.TensorBoard(logdir),\r\n        \r\n        hp.KerasCallback(logdir, hparams)\r\n    ]\r\n    \r\n    return callbacks\r\n```\r\n**Example 1**\r\n```\r\nEpoch 00016: val_loss improved from 0.02129 to 0.02015, saving model to /content/model_one/adam_v2_14072020/multi_input_keras_model_50dim_128batchsize_0.01lr_10decaymultiplier_14072020.h5\r\n```\r\n\r\n**Example 2**\r\n```\r\nEpoch 00019: val_loss improved from 0.01880 to 0.01803, saving model to /content/model_one/adam_v2_14072020/multi_input_keras_model_50dim_128batchsize_0.01lr_10decaymultiplier_14072020.h5\r\n```\r\nYou can see that in both examples the validation loss improved and the model's weight have been saved, while they shouldn't;t have:\r\n\r\n* 0.02129 - 0.02015 = 0.00114 < 0.002 (although it was counted as an improvement)\r\n* 0.01880 - 0.01803 = 0.00077 < 0.002 (also counted as an improvement in validation loss)\r\nWhat is going wrong?\r\n\r\nApologize for not posting more code since it's a thorough application. So, please check my [colab notebook](https://drive.google.com/file/d/1FqQjjB2xFFQCiAqZacvVsl5SU0kcJwGd/view?usp=sharing) to replicate the issue.\r\n\r\nPickled data saved on my drive [folder](https://drive.google.com/drive/folders/1Z0p2RVpJSn8uhZG5UvZBU1AS3aLqEVqv?usp=sharing)", "comments": ["@NikosSpanos \r\n\r\nCan you please provide supporting files as well (Eg: saved pickle files) to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "> @NikosSpanos\r\n> \r\n> Can you please provide supporting files as well (Eg: saved pickle files) to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nOfc @ravikyram , I added the relative folder at the end of my post", "@NikosSpanos Is this happening with just this model or is it happening with other models too. Can you please try this in a basic example and also using tf-nightly and let me know i the issue still persists. Thanks!", "@gowthamkpr Can you please validate that my statement is valid? Because I have posted in the past the same question on StackOverflow (link [here](https://stackoverflow.com/questions/62912769/min-delta-on-callbacks-function-of-keras-model-seems-to-not-take-effect)). \r\nBased on the answers given in the posted SO question, I figured out that the minimum_delta is not the difference between validation_loss of epochs, but rather is the number below which if no improvement is spotted for 10 consecutive epochs then the training stops. So in my posted example above, since minimum delta=0.002 and validation loss: 0.01803 > 0.002. The training continuous until validation loss < 0.002\r\n\r\nI believe that there is a bug in the TF code because Keras [documentation ](https://keras.io/api/callbacks/early_stopping/) states the following:\r\n\r\n_min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement._\r\n\r\nChange = difference (New Number - Original Number). However, based on what is written in my SO question this is not true.\r\n\r\nCan you validate this statement. If you verify that my intuition is wrong, I will post a simpler example of the problem. Also, how am I suppose to use tf-nightly? I use TensorFlow v2.2", "@NikosSpanos As answered in stack overflow and as mentioned in the [documentation](https://keras.io/api/callbacks/early_stopping/), its pretty clear that if you don't see the change (or improvement of min_delta) for the number of epochs of patience_value, the training stops.\r\n\r\nThis has nothing to do with ModelCheckpoint.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41417\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41417\">No</a>\n"]}, {"number": 41416, "title": "generate_TFrecord.py", "body": "I am using my mac, running Catalina 10.15.5, with Python 3.8.3, and TF (2.2.0) which I pip3 installed from github. I have come to the conclusion the issue is using TF 2 instead of 1, but I do not want to downgrade my TF. Additionally, I named some folders differently than him i.e. his \"images\" is my \"Camera pictures,\" and his \"test\" and \"train\" are my \"Test\" and \"Train\"\r\n\r\nI've been dealing with an issue converting my csv files to TFrecord, following this tutorial https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/\r\n\r\n\r\n\r\nHere is the issue I keep running into:\r\n![image](https://user-images.githubusercontent.com/68125365/87552143-46d91700-c67f-11ea-900f-5f80329358e9.png)\r\n\r\nMy directory looks like:\r\n\r\nDesktop\r\n\r\n- models\r\n- Object-Detection\r\n        - data/\r\n            -- Test_labels.csv\r\n            -- Train_labels.csv\r\n        - Camera pictures/\r\n            -- Test/\r\n            --- testingimages.jpg\r\n            -- Train/\r\n            --- testingimages.jpg\r\n            -- ...myimages.jpg\r\n        - training\r\n        - xml_to_csv.py\r\n        - generate_TFrecord.py'\r\n\r\n\r\nHere is my generate_TFrecord.py code: \r\n    \r\n    \"\"\"   \r\n    Usage:\r\n    # From tensorflow/models/\r\n    # Create train data:\r\n    python3 generate_tfrecord.py --csv_input=data/Test_labels.csv --output_path=data/test.record --image_dir=Camera Pictures/Test\r\n    # Create test data:\r\n    python3 generate_tfrecord.py --csv_input=data/Train_labels.csv --output_path=data/train.record --image_dir=Camera Pictures/Train \r\n    \"\"\"\r\n\r\n    from __future__ import division\r\n    from __future__ import print_function\r\n    from __future__ import absolute_import\r\n\r\n    import os\r\n    import io\r\n    import pandas as pd\r\n    import tensorflow.compat.v1 as tf\r\n\r\n    from PIL import Image\r\n    from object_detection.utils import dataset_util\r\n    from collections import namedtuple, OrderedDict\r\n\r\n    flags = tf.app.flags\r\n    flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\r\n    flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\r\n    flags.DEFINE_string('image_dir', '', 'Path to the images')\r\n    FLAGS = flags.FLAGS\r\n\r\n\r\n    # TO-DO replace this with label map\r\n    def class_text_to_int(row_label):\r\n        if row_label == 'Raspi':\r\n            return 1\r\n        else:\r\n            None\r\n\r\n\r\n    def split(df, group):\r\n        data = namedtuple('data', ['filename', 'object'])\r\n        gb = df.groupby(group)\r\n        return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\r\n\r\n\r\n    def create_tf_example(group, path):\r\n        with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\r\n            encoded_jpg = fid.read()\r\n        encoded_jpg_io = io.BytesIO(encoded_jpg)\r\n        image = Image.open(encoded_jpg_io)\r\n        width, height = image.size\r\n\r\n        filename = group.filename.encode('utf8')\r\n        image_format = b'jpg'\r\n        xmins = []\r\n        xmaxs = []\r\n        ymins = []\r\n        ymaxs = []\r\n        classes_text = []\r\n        classes = []\r\n\r\n        for index, row in group.object.iterrows():\r\n            xmins.append(row['xmin'] / width)\r\n            xmaxs.append(row['xmax'] / width)\r\n            ymins.append(row['ymin'] / height)\r\n            ymaxs.append(row['ymax'] / height)\r\n            classes_text.append(row['class'].encode('utf8'))\r\n            classes.append(class_text_to_int(row['class']))\r\n\r\n        tf_example = tf.train.Example(features=tf.train.Features(feature={\r\n            'image/height': dataset_util.int64_feature(height),\r\n            'image/width': dataset_util.int64_feature(width),\r\n            'image/filename': dataset_util.bytes_feature(filename),\r\n            'image/source_id': dataset_util.bytes_feature(filename),\r\n            'image/encoded': dataset_util.bytes_feature(encoded_jpg),\r\n            'image/format': dataset_util.bytes_feature(image_format),\r\n            'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\r\n            'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\r\n            'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\r\n            'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\r\n            'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\r\n            'image/object/class/label': dataset_util.int64_list_feature(classes),\r\n        }))\r\n        return tf_example\r\n\r\n\r\n    def main(_):\r\n        writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\r\n        path = os.path.join(FLAGS.image_dir)\r\n        examples = pd.read_csv(FLAGS.csv_input)\r\n        grouped = split(examples, 'filename')\r\n        for group in grouped:\r\n            tf_example = create_tf_example(group, path)\r\n            writer.write(tf_example.SerializeToString())\r\n\r\n        writer.close()\r\n        output_path = os.path.join(os.getcwd(), FLAGS.output_path)\r\n        print('Successfully created the TFRecords: {}'.format(output_path))\r\n\r\n\r\n    if __name__ == '__main__':\r\n        tf.app.run()\r\n\r\nPlease respond with exactly what needs to be changed in the code, as I am new to this. Thank you!", "comments": ["@jp3spinelli,\r\nAs per the error log, the program is looking for an image named `1-146-raspberry-pi-camera-module-v2.jpg` under the directory `Camera`. \r\n\r\nWhereas your directory structure contains a folder named `Camera pictures`. Please ensure that all the names are consistent throughout the program.\r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is a larger community that reads questions there. Thanks!", "I asked this question on Stack Overflow but have not received a response yet, so I came here. Additionally, how do I fix the issue with my computer looking in the \"Camera\" folder when it should be \"Camera pictures\"?", "@jp3spinelli,\r\nCould you please zip the contents of `models` and `Object-Detection` folders and share it with us so that we can look into this issue. Thanks!", "I resolved the issue. Found a solution in Youtube comments. Thank you!"]}, {"number": 41415, "title": "Build TensorFlow 2.3 from source for the Raspberry Pi (Python 3.7)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 LTS\r\n- TensorFlow installed from : Git repository\r\n- TensorFlow version: 2.3.0rc0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello, \r\n\r\nI would like to build a wheel file for my Raspberry Pi 3B (Buster) with TensorFlow version 2.3.0rc0.\r\nSo I run the following command : \r\n```CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\"     tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37     tensorflow/tools/ci_build/pi/build_raspberry_pi.sh```\r\n\r\nHowever, the output I obtain is the wheel file for Python 3.5 so it won't work for my RPI..\r\nSo is my command correct? \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nOutput can be found here:\r\noutput-artifacts\r\noutput-artifacts/tensorflow-2.3.0rc0-cp35-none-linux_armv7l.whl\r\noutput-artifacts/benchmark_model\r\noutput-artifacts/libtensorflow_framework.so\r\noutput-artifacts/libtensorflow.so", "comments": ["@vinorth05 \r\nCan you please refer to this issue with similar error and update.\r\n#35116", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@vinorth05 there was an issue with Python 3.7 build script. Could you try it again with master?\r\n\r\nNow the build command is simpler.\r\n```\r\n$  tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41415\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41415\">No</a>\n"]}, {"number": 41414, "title": "Adopt existing cpu frequency implementation over abseil one", "body": "Solving issue https://github.com/tensorflow/tensorflow/issues/41033\r\nRemove the abseil internal method of retrieving cpu frequency, and use a Tensorflow native one instead.", "comments": ["Hi @gunan , would you mind take a look at this PR? Thanks.", "Sanity check fails with the following suggestion:\r\n```\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/core/platform/default/BUILD:\r\n271d270\r\n<         \"//tensorflow/core/platform:snappy.h\",\r\n273a273\r\n>         \"//tensorflow/core/platform:snappy.h\",\r\nexit status 1\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```\r\nHowever, I could not see the difference here (including the whitespaces); besides, as could be seen from the file changes, I did not edit this line in this PR. Do you have any idea what is causing the error @gunan ? Thanks.", "It seems that one check fails but I could not see the error message. It will be great if anyone could share the error message with me so that I could try to fix it, thanks.", "I also cannot see the issue.\r\n@gbaned could you help us import this PR?"]}, {"number": 41413, "title": "Threshold to be set 0 when using binary accuracy with raw prediction values (from_logits=True)", "body": "This is mainly a documentation bug (official tensorflow tutorial), but it is a \"dangerous trap\" and might also happen in general to users, so see below my last sentence this could also be fixed in Tensorflow that it detects this automatically.\r\n\r\nIn this [tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning) raw prediction values (form_logit=True) are used. So we have negative values and positive values, while \r\n\r\n> \r\n\"prediction will be treated as a logit, or a raw prediction value. **Positive numbers predict class 1, negative numbers predict class 0.**\"\r\n\r\nHowever, the model.compile statement is as follows:\r\n\r\n```\r\nbase_learning_rate = 0.0001\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n```\r\n\r\nThis is wrong, as per default, threshold value to classify is 0.5:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/binary_accuracy\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy\r\n```\r\ntf.keras.metrics.binary_accuracy(\r\n    y_true, y_pred, threshold=0.5\r\n)\r\n```\r\n```\r\ntf.keras.metrics.BinaryAccuracy(\r\n    name='binary_accuracy', dtype=None, threshold=0.5\r\n)\r\n```\r\n\r\n> threshold | (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0.\r\n> -- | --\r\n\r\n\r\n\r\n\r\nThis leads to the wrong classifications. model.evaluate will also give false accuracy measure. Reason is that predicted values in range [0,0.49999] are wrongly classified as 0 (I am not sure what happens to a value of exactly 0.5), whereas they actually should be classified as 1!\r\n\r\nSo it needs to be corrected to:\r\n\r\n> model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n>               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n>               metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.0))\r\n\r\nWould be even better if this is corrected inside Tensorflow that it automatically detects that from_logits=True was set and then assumes that default threshold is not 0.5 anymore, but 0.0 (and maybe additional WARNING output).", "comments": ["This is fixed.", "Hi @MarkDaoust @HaraBeron ,\r\n\r\nSeems like the fixes are lost after being replaced by this commit https://github.com/tensorflow/docs/commit/adae1941b3a1b3c0b781b94d96f4476892a44620\r\n\r\nCould you please reopen this issue?\r\n\r\nAlso, rather than only fixing the documentation, I would consider to add a new `from_logits` argument for `BinaryAccuracy`, and set the default `threshold` to `None` (for automatically decide whether to use `0.5` or `0.0`)"]}, {"number": 41412, "title": "Reuse HumanString in HumanStringWithLayout", "body": "Thank you for your time on reviewing this pr.", "comments": ["@gbaned Could you add someone else for this pr, for it is unrelated to mlir? Thank you :)."]}, {"number": 41411, "title": "Update data_adapter.py", "body": "", "comments": []}, {"number": 41410, "title": "Lazy loading for s3_client and executor", "body": "@mihaimaruseac \r\nThis PR adds lazy-loading for s3_client and executor", "comments": ["Close because it does not reflect new commit"]}, {"number": 41409, "title": "Tensorflow Lite ARM64 build error: inlining failed in call to always_inline 'uint8x16_t vaesmcq_u8(uint8x16_t)': target specific option mismatch", "body": "**System information**\r\n- OS Platform and Distribution: \r\n  - Linux Ubuntu 18.04\r\n  - Alpine 3.10\r\n- TensorFlow version: v2.2.0\r\n- GCC/Compiler version (if compiling from source): \r\n  - g++ (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n  - g++ (Alpine 8.3.0) 8.3.0\r\n\r\n**Describe the problem**\r\n\r\nWhile trying to build natively on ARM64 following the instruction found [here](https://www.tensorflow.org/lite/guide/build_arm64#native_compiling) we encountered the following error during build.\r\n\r\n```sh\r\naarch64-linux-gnu-g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITH_RUY -march=armv8-a -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/tools/make/downloads/absl/absl/random/seed_gen_exception.cc -o /home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/seed_gen_exception.o\r\nIn file included from tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:225:0:\r\n/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h: In function \u2018Vector128 {anonymous}::AesRound(const Vector128&, const Vector128&)\u2019:\r\n/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:12440:1: error: inlining failed in call to always_inline \u2018uint8x16_t vaesmcq_u8(uint8x16_t)\u2019: target specific option mismatch\r\n vaesmcq_u8 (uint8x16_t data)\r\n ^~~~~~~~~~\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:251:20: note: called from here\r\n   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;\r\n          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:225:0:\r\n/usr/lib/gcc/aarch64-linux-gnu/7/include/arm_neon.h:12426:1: error: inlining failed in call to always_inline \u2018uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)\u2019: target specific option mismatch\r\n vaeseq_u8 (uint8x16_t data, uint8x16_t key)\r\n ^~~~~~~~~\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.cc:251:20: note: called from here\r\n   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;\r\n          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/tools/make/Makefile:269: recipe for target '/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.o' failed\r\nmake: *** [/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/internal/randen_hwaes.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nIn file included from tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.cc:16:0:\r\n/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.h:40:10: fatal error: gmock/gmock.h: No such file or directory\r\n #include \"gmock/gmock.h\"\r\n          ^~~~~~~~~~~~~~~\r\ncompilation terminated.\r\ntensorflow/lite/tools/make/Makefile:269: recipe for target '/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.o' failed\r\nmake: *** [/home/ubuntu/tf_lite_build/tensorflow/tensorflow/lite/tools/make/gen/linux_aarch64/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/mocking_bit_gen.o] Error 1\r\nmake: Leaving directory '/home/ubuntu/tf_lite_build/tensorflow'\r\n\r\n```\r\n\r\nThis happens both on the Ubuntu machine and inside the Alpine Docker container.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```sh\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\n$ cd tensorflow/\r\n$ git checkout v2.2.0\r\n$ cd tensorflow/lite/tools/make\r\n$ ./download_dependencies.sh\r\n$ ./build_aarch64_lib.sh\r\n```\r\n", "comments": ["**Update**: Using the master branch (commit: https://github.com/tensorflow/tensorflow/commit/d2d6c3f07a0b874e64a024c767deb7c9fb39b704) the build succeeds.\r\n", "It's fixed in between 2.2 and 2.3.\r\nIf you prefer to use a stable branch, please use 2.3.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41409\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41409\">No</a>\n", "@terryheo Ok thank you... we'll switch to 2.3", "I did a build today and got hit by same issue. I use manylinux2014 image for builds.\r\n\r\ntop commit:\r\n\r\n```\r\ncommit 4e8bba06da6c3212c1d4e19ef86c42d905d47c7a (grafted, HEAD -> master, origin/master, origin/HEAD)\r\nAuthor: TensorFlower Gardener <gardener@tensorflow.org>\r\nDate:   Mon Aug 30 03:37:48 2021 -0700\r\n\r\n    Merge pull request #50961 from nouiz:upstream-cudaMallocAsync-no_stats_error\r\n    \r\n    PiperOrigin-RevId: 393736407\r\n    Change-Id: I31f5ba2b885683cc587a30cdce70e0f20ffefef9\r\n```\r\n\r\nError:\r\n\r\n```\r\n      /opt/rh/devtoolset-10/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-o\r\npt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o' -fPIC -iquoteexternal/com_google_absl -iquotebazel-\r\nout/aarch64-opt/bin/external/com_google_absl -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwr\r\nite-strings -DNOMINMAX -Wno-pass-failed -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_google_absl/absl/random/internal/randen_hwaes.cc -o bazel-out/aarch64-opt/bin/external\r\n/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o)                                                                                                                                                                                                     \r\n    Execution platform: @local_execution_config_platform//:platform                                                                        \r\n    In file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:\r\n    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h: In function 'Vector128 {anonymous}::AesRound(const Vector128&, const Vector128&)':\r\n    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h:12332:1: error: inlining failed in call to 'always_inline' 'uint8x16_t vaesmcq_u8(uint8x16_t)': target specific option mismatch\r\n    12332 | vaesmcq_u8 (uint8x16_t data)                                                                                                                                                                                                                                              \r\n          | ^~~~~~~~~~                                                                                                                                                                                                                                                                \r\n    external/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here                                                                                                                                                                                      \r\n      255 |   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;                                                                                                                                                                                                          \r\n          |          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                                                                       \r\n    In file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:                                                                                                                                                                                          \r\n    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h:12318:1: error: inlining failed in call to 'always_inline' 'uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)': target specific option mismatch                                                     \r\n    12318 | vaeseq_u8 (uint8x16_t data, uint8x16_t key)                                                                                                                                                                                                                               \r\n          | ^~~~~~~~~                                                                                                                                                                                                                                                                 \r\n    external/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here                                                                                                                                                                                      \r\n      255 |   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;                                                                                                                                                                                                          \r\n          |          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                                                                       \r\n    At global scope:                                                                                                                                                                                                                                                                  \r\n    cc1plus: note: unrecognized command-line option '-Wno-pass-failed' may have been intended to silence earlier diagnostics                                                                                                                                                          \r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build                                                                                                                                                                                                           \r\n    INFO: Elapsed time: 5671.673s, Critical Path: 500.29s                                                                                                                                                                                                                             \r\n    INFO: 9206 processes: 996 internal, 8210 local.                                                                                                                                                                                                                                   \r\n\r\n```"]}, {"number": 41408, "title": "Update data_adapter.py", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41408) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41408) for more info**.\n\n<!-- ok -->"]}, {"number": 41407, "title": "TFLM: To add Himax WE1 EVB examples", "body": "This pull request adds support magic wand and micro speech examples on Himax WE1 EVB", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41407) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41407) for more info**.\n\n<!-- ok -->", "> Can you remove the animation GIF file? It's 1MB, which is too large for this repo. Hopefully you can host it on a different server and use that URL instead?\r\n\r\nThanks for the suggestion, GIF now use an external link.", "@CKHSUinHimax Can you please resolve conflicts? Thanks!"]}, {"number": 41406, "title": "Ran into this issue while importing the tflearn library", "body": "D:\\Softwares\\anaconda\\lib\\site-packages\\tflearn\\__init__.py in <module>\r\n      2 \r\n      3 # Config\r\n----> 4 from . import config\r\n      5 from .config import is_training, get_training_mode, init_graph\r\n      6 \r\n\r\nD:\\Softwares\\anaconda\\lib\\site-packages\\tflearn\\config.py in <module>\r\n      3 import tensorflow as tf\r\n      4 \r\n----> 5 from .variables import variable\r\n      6 \r\n      7 # -------------------\r\n\r\nD:\\Softwares\\anaconda\\lib\\site-packages\\tflearn\\variables.py in <module>\r\n      5 import tflearn\r\n      6 \r\n----> 7 from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\n      8 from tensorflow.python.framework import ops\r\n      9 from tensorflow.python.ops import variable_scope\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\n\r\nI have installed the library", "comments": []}, {"number": 41404, "title": "memory leak in tf(2.2).keras.Model.predict function", "body": "pre:\r\n1. train detection model  using tf.keras under tf2.2 version\r\n2. inference using tf.compat.v1.keras \r\n config = tf.compat.v1.ConfigProto()\r\n config.gpu_options.per_process_gpu_memory_fraction = 0.25\r\n  sess = tf.compat.v1.Session(config = config)\r\n  tf.compat.v1.keras.backend.set_session(sess)\r\n\r\n3. load model: \r\n   model = tf.keras.models.load_model(self.modelpath, compile=False)\r\n\r\nfound memory leak using [model.predict(image_np)] to get prediction\r\n\r\nis this a serious bug in tf.keras.model.predict function???", "comments": ["finally I found  using [model(image_np)] can avoid memory leak", "> finally I found using [model(image_np)] can avoid memory leak\r\n\r\n@bruceliuxing,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "look forward to be fixed in the future version"]}, {"number": 41403, "title": "AttributeError: Tensor.op is meaningless when eager execution is enabled when using multiple feature inputs in Tensorflow federated", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Red Hat 7.7\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: Binary\r\n-   **TensorFlow version (use command below)**: TF 2.2\r\n-   **Python version**: 3.7\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\"AttributeError: Tensor.op is meaningless when eager execution is enabled\" when using multiple feature inputs in Tensorflow federated. \r\n\r\nI have a problem when inputting multiple feature inputs as follows:\r\n\r\n```\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_cols)\r\n\r\nfeature_layer_inputs = {}\r\nfeature_layer_inputs['a'] = tf.keras.Input(shape=(1,), name='a', dtype=tf.int32)\r\nfeature_layer_inputs['b'] = tf.keras.Input(shape=(1,), name='b', dtype=tf.int32)\r\n\r\nmodel = feature_layer(feature_layer_inputs)\r\n\r\nfor units in [64, 64]:\r\n    model = tf.keras.layers.Dense(units, activation='relu')(model)\r\nc_pred = tf.keras.layers.Dense(1, activation='sigmoid')(model)\r\n\r\nkeras_model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=c_pred)\r\n\r\ninput_spec = collections.OrderedDict(\r\n        x=collections.OrderedDict(\r\n            a=tf.TensorSpec(shape=[None,], dtype=tf.int32),\r\n            b=tf.TensorSpec(shape=[None,], dtype=tf.int32),\r\n        y=tf.TensorSpec(shape=[None,], dtype=tf.int32))\r\n\r\ndef model_fn():\r\n  # We _must_ create a new model here, and _not_ capture it from an external\r\n  # scope. TFF will call this within different graph contexts.\r\n\r\n  return tff.learning.from_keras_model(\r\n      keras_model,\r\n      #input_spec=preprocessed_sample_dataset.element_spec,\r\n      input_spec=input_spec,\r\n      loss=losses.SparseCategoricalCrossentropy(),\r\n      metrics=[metrics.SparseCategoricalAccuracy()])\r\n\r\n\r\niterative_process = tff.learning.build_federated_averaging_process(\r\n    model_fn,\r\n    client_optimizer_fn=lambda: optimizers.Adam(learning_rate=client_lr),\r\n    server_optimizer_fn=lambda: optimizers.SGD(learning_rate=server_lr))\r\n\r\nstate = iterative_process.initialize()\r\n\r\nfor round_num in range(1, NUM_ROUNDS+1):\r\n    #state, tff_metrics = iterative_process.next(state, federated_train_data)\r\n    state, tff_metrics = iterative_process.next(state, train_data)\r\n    eval_model = keras_model\r\n    eval_model.compile(optimizer=optimizers.Adam(learning_rate=client_lr),\r\n                       loss=losses.SparseCategoricalCrossentropy(),\r\n                       metrics=[metrics.SparseCategoricalAccuracy()])\r\n\r\n    tff.learning.assign_weights_to_keras_model(eval_model, state.model)\r\n\r\n    ev_result = eval_model.evaluate(x_test, y_test, verbose=0)\r\n```\r\n\r\nHowever, I got the full traceback as follows. Eventually, I get an error 'AttributeError: Tensor.op is meaningless when eager execution'. It seems that there is something wrong with the built model especially the inputs inside the tf.keras.model function.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"accident_modify_uk_final3b.py\", line 323, in <module>\r\n    server_optimizer_fn=lambda: optimizers.SGD(learning_rate=server_lr))\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/federated_averaging.py\", line 212, in build_federated_averaging_process\r\n    stateful_delta_aggregate_fn, stateful_model_broadcast_fn)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py\", line 360, in build_model_delta_optimizer_process\r\n    @tff.tf_computation\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/api/computations.py\", line 152, in tf_computation\r\n    return computation_wrapper_instances.tensorflow_wrapper(*args)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper.py\", line 333, in __call__\r\n    self._wrapper_fn)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper.py\", line 91, in _wrap\r\n    concrete_fn = wrapper_fn(fn, parameter_type, unpack=None)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/wrappers/computation_wrapper_instances.py\", line 52, in _tf_wrapper_fn\r\n    target_fn, parameter_type, ctx_stack)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/tensorflow_serialization.py\", line 275, in serialize_py_fn_as_tf_computation\r\n    result = target(*args)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/core/impl/utils/function_utils.py\", line 455, in <lambda>\r\n    return lambda: fn()  # pylint: disable=unnecessary-lambda\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py\", line 364, in tf_init_fn\r\n    stateful_model_broadcast_fn.initialize())\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py\", line 227, in server_init\r\n    _, optimizer_vars = _build_server_optimizer(model, optimizer)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py\", line 123, in _build_server_optimizer\r\n    apply_delta(delta=weights_delta)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nAttributeError: in user code:\r\n\r\n    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow_federated/python/learning/framework/optimizer_utils.py:112 apply_delta  *\r\n        optimizer.apply_gradients(grads_and_vars, name='server_update')\r\n    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:508 apply_gradients  **\r\n        \"name\": name,\r\n    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2427 _merge_call\r\n        return merge_fn(self._strategy, *args, **kwargs)\r\n    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:590 _distributed_apply  **\r\n        \"update_\" + var.op.name, skip_on_eager=True):\r\n    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:581 op\r\n        return self._handle.op\r\n    /home/anaconda3/envs/env1_TF2.1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1113 op\r\n        \"Tensor.op is meaningless when eager execution is enabled.\")\r\n\r\n    AttributeError: Tensor.op is meaningless when eager execution is enabled.\r\n```\r\n\r\nHow to solve this issue?\r\n", "comments": ["@ymsaputra \r\nPlease follow [this link](https://github.com/tensorflow/tensorflow/issues/27739#issuecomment-483139126) with same error and let us know.\r\n\r\nSimilar resoved issues:\r\n #33423 #28070 [link](https://stackoverflow.com/questions/62161420/attributeerror-tensor-op-is-meaningless-when-eager-execution-is-enabled) ", "@Saduf2019 Based on my understanding, those solved issues are not relevant to my issues. The StackOverflow link does not solve the problem. I think my issue is related to the model building when considering multiple inputs. ", "I have solved this issue by moving the model definition inside def model_fn(). It seems that the model cannot be captured globally"]}, {"number": 41402, "title": "How to find the number of iterations while using tf.compat.v1.train.AdamOptimizer?", "body": "I am actually. using the VGGish model to train my data and I am using the tf.compat.v1.train.AdamOptimizer to minimize my error. Is there a way in which I can print the number of iterations that the model does before printing the loss. Below is the code that I am using to train the model.\r\n\r\nMy definition of the optimizer -\r\n\r\n```\r\noptimizer = tf.train.AdamOptimizer(\r\n            # time steps\r\n            learning_rate=vggish_params.LEARNING_RATE,\r\n            # tolerance level \r\n            epsilon=vggish_params.ADAM_EPSILON)\r\n        optimizer.minimize(loss, global_step=global_step, name='train_op')\r\n```\r\n\r\nLoop where I am feeding the data each time. I want to print the number of iterations for each time I feed the data.\r\n\r\n```\r\nfor _ in range(FLAGS.num_batches):\r\n      #for normal batch training\r\n      (features, labels) = _get_examples_batch()\r\n      [num_steps, loss, _] = sess.run(\r\n        # here the first array that we are passing in is called fetches\r\n        # it is a list of graph elements\r\n          [global_step_tensor, loss_tensor, train_op],\r\n          # one has to be of the form of a tensor and the other in the form of a numpy array\r\n          feed_dict={features_tensor: features, labels_tensor: labels})\r\n      print('Step %d: loss %g' % (num_steps, loss))\r\n```\r\n", "comments": ["@tusharpoddar \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This is the python file that I am using to train my model. It has the definition of the model. It uses Adam optimizer. All I want to do is that each time when it prints the loss, I want to print the number of iterations that the model does before printing the loss. \r\n\r\n\r\n[vggish_train_demo.py.zip](https://github.com/tensorflow/tensorflow/files/4930310/vggish_train_demo.py.zip)\r\n", "@tusharpoddar \r\n\r\nI think this is more related to models repo.Please, raise an issue in models repo by filling issue template from [here](https://github.com/tensorflow/models/issues/new/choose).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41401, "title": "RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version", "body": "I want to run tensorflow with CUDA 10.0 or 10.1 on Windows.\r\n\r\n- abstract\r\n\r\n-- error message\r\nRuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version\r\n\r\n-- system infomeation\r\ntensorflow-gpu 2.2.0\r\nPython 3.7.1\r\nWindows 10 Pro\r\nCUDA 10.1.105\r\ncuDNN 7.6.4\r\nNVIDIA driver 451.48\r\nVisual Studio 2017\r\nGeForce GTX 1080 Ti\r\n\r\n- detail\r\n\r\n-- NVIDIA driver\r\n\r\nnvidia-smi\r\n\r\n```\r\nWed Jul 15 15:14:29 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 451.48       Driver Version: 451.48       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108... WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| 25%   30C    P8     9W / 250W |   1310MiB / 11264MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2228    C+G   ...es.TextInput.InputApp.exe    N/A      |\r\n|    0   N/A  N/A      9272    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A      9372    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A     11260    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n|    0   N/A  N/A     11740    C+G   ...artMenuExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A     12340    C+G   ...w5n1h2txyewy\\SearchUI.exe    N/A      |\r\n|    0   N/A  N/A     12952    C+G   ...zf8qxf38zg5c\\SkypeApp.exe    N/A      |\r\n|    0   N/A  N/A     13272    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A     15784    C+G   ...me\\Application\\chrome.exe    N/A      |\r\n|    0   N/A  N/A     16300    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n-- CUDA\r\n\r\nnvcc -V\r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation Built on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019\r\nCuda compilation tools, release 10.1, V10.1.105\r\n```\r\n-- cuDNN\r\n\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include\\cudnn.h\r\n\r\n```\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 6\r\n#define CUDNN_PATCHLEVEL 4\r\n\r\n```\r\n\r\n-- Environment Variable\r\n\r\n```\r\nCUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1 CUDA_PATH_V10_1=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1 CUDNN_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1 Path=\r\n    ...\r\n    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;\r\n    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvp;\r\n    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\libx64;\r\n    C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include;\r\n    ...\r\n```\r\n\r\n-- python\r\n\r\npython -V\r\n\r\n```\r\nPython 3.7.1\r\n```\r\n\r\n-- pip list\r\n\r\n```\r\nKeras                    2.4.3\r\nKeras-Preprocessing      1.1.2\r\n...\r\ntensorboard              2.2.2\r\ntensorboard-plugin-wit   1.7.0\r\ntensorflow-estimator     2.2.0\r\ntensorflow-gpu           2.2.0\r\ntensorflow-gpu-estimator 2.2.0\r\n```\r\n\r\n-- Error message\r\n\r\npython -c \"from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())\"\r\n\r\n```\r\n2020-07-15 15:00:36.136510: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-15 15:00:37.931078: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-15 15:00:37.946142: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x148edb04360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-15 15:00:37.957182: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-15 15:00:37.965143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-15 15:00:37.996562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-07-15 15:00:38.013444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-15 15:00:38.026126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-15 15:00:38.038540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-15 15:00:38.049231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-15 15:00:38.061932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-15 15:00:38.074267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-15 15:00:38.091257: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-15 15:00:38.100188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0 Traceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"D:\\****\\lib\\site-packages\\tensorflow\\python\\client\\device_lib.py\", line 43, in list_local_devices\r\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\nRuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version\r\n```\r\n", "comments": ["@take0212,\r\nCould you please take a look at the software requirements listed [here](https://www.tensorflow.org/install/gpu#software_requirements) and make sure you have the compatible version installed. Thanks!", "Thank you for your advice.\r\nBut it would be helpful if you could point out which combination is bad.\r\n\r\nI think the content of the error message is invalid.\r\n\r\nI have tried variously with reference to various sites.\r\nI did not solve it, so I posted  this.\r\n\r\nThere is a strange thing.\r\nWhen I run it with debugging in Visual Studio, it works fine.\r\nIf I run it without debugging, I get the error.\r\n", "@take0212,\r\n\r\nAs per the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu) TensorFlow v2.2 works with CUDA 10.1 and cuDNN 7.4.\r\n\r\nThe output of nvidia-smi shows CUDA Version as 11.0, whereas nvcc -v shows the version as V10.1.105. There seems to be a driver mismatch error here.\r\nPlease check [this](https://stackoverflow.com/a/55351774) similar StackOverflow comment and [this](https://forums.developer.nvidia.com/t/solved-why-is-my-cuda-driver-version-is-insufficient-for-cuda-runtime-version/112251) thread on NVIDIA forums and let us know if it helps. \r\n\r\nAlso, could you please specify if you're running TensorFlow from Anaconda environment. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I have fixed the bug.\r\n\r\nThe cause was unexpected and it was ANSI64.dll.\r\nThe solution was to uninstall ANSI64.dll.\r\n\r\nThe script could run in debug mode of Visual Studio.\r\nSo I checked loaded DLLs and using Process Explorer.\r\n\r\nI compared loaded DLLs between in case of that the script could run and that it couldn't run.\r\nThen I noticed that ANSI64.dll was loaded.\r\n\r\nANSI64.dll is special and it is not important information.\r\nThe survey procedure will be helpful.\r\n\r\nThe combination of versions of each module was irrelevant.\r\nThe difference between pip and conda was irrelevant.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41401\">No</a>\n"]}, {"number": 41400, "title": "Bigger Tflite micro model", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- Tensorflow version (commit SHA if source): 2.2.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): \r\n\r\n**Describe the problem**\r\nI downloaded tflite Inception_v4 model from https://www.tensorflow.org/lite/guide/hosted_models. The model had a size of 42.9MB. I converted to tflite micro using the xxd command and the .cc file I got after conversion had a size of 264MB. Does the model conversion to cc file increase the tflite model size by 5x times? What I understand is that, tflite micro should give a smaller sized model for execution in microcontrollers. Is my understanding correct. Can you help me in solving the problem?\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nxxd -i inception_v4_299_quant.tflite > model.cc\r\n\r\n", "comments": ["I downloaded the `inception_v4.tflite` from https://www.tensorflow.org/lite/guide/hosted_models after extraction it is `170.7mb`\r\nIn order to obtain smallest possible model size, You may want to try performing [Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) on TF Lite Flat Buffer before converting it into C.\r\nWe should also note that inception_v4 is fairly heavy architecture so the file size can be relatively bigger.", "What is the size of the *compiled* code? I'm not sure you can use the .cc file size as a direct measure of how it will actually impact binary size.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "\r\n\r\n> I downloaded the `inception_v4.tflite` from https://www.tensorflow.org/lite/guide/hosted_models after extraction it is `170.7mb`\r\n> In order to obtain smallest possible model size, You may want to try performing [Quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) on TF Lite Flat Buffer before converting it into C.\r\n> We should also note that inception_v4 is fairly heavy architecture so the file size can be relatively bigger.\r\n\r\nThanks Yasir. But the size of tflite micro model more than the tflite model raised the confusion\r\n", "> What is the size of the _compiled_ code? I'm not sure you can use the .cc file size as a direct measure of how it will actually impact binary size.\r\n\r\nThanks Duke. I will check that. Meanwhile one more question. Can we use python script for the same. Or is it possible with cpp code only?", "Note that using xxd converts the tflite model (flatbuffer binary format) into a C array. The model is unchanged and hence the binary size will also be the same.\r\n\r\nHere are the steps that I followed to verify what I say above:\r\n  * Downloaded tflite Inception_v4 model from https://www.tensorflow.org/lite/guide/hosted_models\r\n  * Confirmed the size of the tflite model is ```42855504 bytes``` with ```ls -l inception_v4_299_quant.tflite```\r\n  * Used xxd to convert to a C array with ```xxd -i inception_v4_299_quant.tflite > inception_v4_299_quant.cc ```\r\n  * Looked at the last line of  inception_v4_299_quant.cc and confirmed that the C array takes the same number of bytes.\r\n```tail -n1 inception_v4_299_quant.cc``` gave me ```unsigned int inception_v4_299_quant_tflite_len = 42855504;```\r\n\r\nI'm going to close this issue, but feel free to reopen if I missed something.\r\n\r\nNote that in general, a 42MB model is likely not suited for TF Micro. There will always be exceptions but usual latency and power requirements mean that models <1MB are more suited for systems running TF Micro.", "> * tail -n1 inception_v4_299_quant.cc\r\n\r\nThanks for the clarification @advaitjain, much appreciated !"]}, {"number": 41399, "title": "[Grappler] Fix bug in UpdateSqueezeDims", "body": "The dim logic in generic_layout_transposer's SqueezeTransposer was incorrect, which caused two problems:\r\n- Negative dims were transformed incorrectly.\r\n- The dim check incorrectly failed when dim = rank - 1 (which can occur when transforming NCHW to NHWC).\r\n\r\nThis PR fixes the dim logic and adds tests to check these cases.\r\n\r\ncc @nluehr ", "comments": ["@benbarsdell thank you for the fix!"]}, {"number": 41398, "title": "GPU memory consumption soars up when \"device_record_tensor_accesses_\" in executor.cc .", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): TensorFlow 1.13\r\n- Python version: Python 3.6.10\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA Tesla V100, 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThere is flag named \"device_record_tensor_accesses_\" which is set by \"RequiresRecordingAccessedTensors()\" function in gpu_device.cc. It sets record accessed tensors and delay tensor release time until actual GPU execution is finished. Comment said it may be useful for multi stream situation which never happened. Hence, it is set to false by default. I need this sort of feature so test it, changing it into true. When I turned it on, weirdly GPU memory consumption soars up. I used Resnet50 with batch 64 from tf_cnn_benchmark. It used around 8GB but when \"device_record_tensor_accesses_\" is turned on, it uses more than 15GB and keeps growing during training. I saw source code quite a long time to figure out where this problem comes from but I still can't.\r\n\r\nI looked up why it happens and found out that there are actually more tensor allocation.\r\n\r\nI saw that this feature is removed many part of related codes from recent TensorFlow since it has never been used.\r\n\r\nRelated files: executor.cc, gpu_device.cc, gpu_event_mgr.h/cc, unique_tensor_references.h/cc\r\n\r\n**Describe the expected behavior**\r\n\r\nI guess it should have stayed using the same amount of GPU memory.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nI simply use tf_cnn_benchmark's Resnet50 model training job.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis shows tensor allocation from that operator(kernel) which didn't exist when \"device_record_tensor_accesses_\" is turned off.\r\nSo, they are newly appearing tensor allocation. (There are more but I just post some of them due to space limit.)\r\n\r\n![image](https://user-images.githubusercontent.com/20127356/87491362-e8159c80-c682-11ea-9837-39724853e6be.png)\r\n\r\nand I grep one of those newly created operator \"AssignMovingAvg\". And it appears in a strange path.\r\n\r\nI just turned on \"device_record_tensor_accesses_\", but why are those operator added??\r\nIs there any reason or is it bug?\r\n\r\n![image](https://user-images.githubusercontent.com/20127356/87505594-30918200-c6a4-11ea-9e64-3f4143d1da9c.png)\r\n\r\nIs it supposed to change the \r\n\r\nI guess this is bug. I believe there is no reason that the peak GPU memory consumption goes up.\r\n\r\nThank you in advance! \r\n", "comments": ["Thanks for your issue. Perhaps tensorflow/benchmarks repo can be a good platform to raise this issue since your issue talks about [tf_cnn_benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks).\r\nPlease try posting it to [tensorflow/benchmarks](https://github.com/tensorflow/benchmarks/issues).", "Thank you for replying!"]}]