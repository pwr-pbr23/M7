[{"number": 13472, "title": "Branch 170899880", "body": "", "comments": ["Jenkins, test this please.", "Abandoning PR -- contains bad CL 170892257, will re-sync after the roll back (170912304) is submitted internally."]}, {"number": 13471, "title": "Issue when the saving the model when the session is made by with statement", "body": " i use tensorflow to train LSTM network. The training run well but when i want to save the model, i get error below.\r\n```\r\nStep 1, Minibatch Loss= 0.0146, Training Accuracy= 1.000\r\nStep 1, Minibatch Loss= 0.0129, Training Accuracy= 1.000\r\nOptimization Finished!\r\nTraceback (most recent call last):\r\n  File \".\\lstm.py\", line 169, in <module>\r\n    save_path = saver.save(sess, \"modelslstm/\" + str(time.strftime(\"%d-%m-%Y-%H-%M-%S\")) + \".ckpt\")\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1314, in __exit__\r\n    self._default_graph_context_manager.__exit__(exec_type, exec_value, exec_tb)\r\n  File \"C:\\Python35\\lib\\contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3815, in get_controller\r\n    if self.stack[-1] is not default:\r\nIndexError: list index out of range\r\n```\r\n\r\nMy Code :\r\n```\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    saver = tf.train.Saver()\r\n   ....\r\n   save_path = saver.save(sess, \"modelslstm/\" + str(time.strftime(\"%d-%m-%Y-%H-%M-%S\")) + \".ckpt\")\r\n```\r\nAfter i changed the `with` statement with the `sess = tf.Session()` the problem gone", "comments": ["Sounds like you are trying to use the session outside of the `with` block which is not allowed. You can just stick to `sess=tf.Session()` line, the `with` block is only useful if you are trying to use multiple sessions at the same time", "I think @yaroslavvb is right but when I try to call `saver.save` with a closed session, I get the error message \"RuntimeError: Attempted to use a closed Session\".\r\n\r\n@tancejang can you post a small, full example that reproduces the problem?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 13470, "title": "GetConvolveAlgorithms fixup take 2", "body": "Attention @tfboyd \r\n\r\nMove loop to toggle tensor_ops inside GetConvolveAlgorithms functions. Also\r\ntensor_ops are not included in the returned list if they are not supported by\r\nthe cuDNN or GPU architecture versions.\r\n\r\nThis is a re-submit of PR 13252 which seems to have been accidentally squashed\r\nduring the merge at hash 37800b9.", "comments": ["Can one of the admins verify this patch?", "Hold off on this for 24-48 hours while the build team figures out what happened with the merge.  This PR is useful to illustrate what was lost as a reference.  ", "@yifeif   Do you mind merging after the tests pass (assuming they pass).  Reed verified this is the same change as before.  ", "For sure. @tensorflow-jenkins test this please.", "FYI for sync @frankchn ", "makefile rerun at http://ci.tensorflow.org/job/tensorflow-pull-requests-makefile/11014/console", "@yifeif I assume you'll be merging this in after the tests passes, etc?"]}, {"number": 13469, "title": "Merge pull request #1 from tensorflow/master", "body": "Mise \u00e0 jour", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 13468, "title": "Issues in training the model on macOS Sierra", "body": "hello, \r\n\r\ni want to create a AI-chatbot using the tensorflow seq2seq model. So, when i try to train the model there are two issues that i want to ask about:\r\n\r\n1- why i'm getting this WARNING \r\n`WARNING:tensorflow:From /Users/emansaad/Desktop/AI-chtabot/Victor/eng-victor/seq2seq_model.py:129 in __init__.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.`\r\n\r\n2-the temperature of my MAC book pro (macOS Sierra) is getting very high and the fans are running to cool the device , is that normal?\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said:\r\n- The warning comes from the fact that `seq2seq_model.py` is using a deprecated API call\r\n- It is possible that the training process is exercising your CPU enough to make it hot and cause the fans to run"]}, {"number": 13467, "title": "cannot create an object of the class tf.layers.Dense()", "body": "I want to create an object of the class tf.layers.Dense(), but don't want to do the feedforward step immediately (i.e. not tf.layers.dense(input, units)). Because I want to first declare these modules/layers in a class, and then to have several member functions apply1(x, y), apply2(x,y) to use these layers. But when I did in tensorflow tf.layers.Dense(units), it returned:\r\n\r\nlayer = tf.layers.Dense(100)\r\n AttributeError: 'module' object has no attribute 'Dense'\r\n\r\nHow can I fix that?\r\nThanks", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\n(In particular, `tf.layers.Dense` is not an exported symbol in TensorFlow releases 1.3 and prior. The next release will have them or you need to build from source. Hope that helps)"]}, {"number": 13466, "title": "contrib/verbs only works on GPU", "body": "### System information\r\n- **OS Linux readhat** 7.3\r\n- **TensorFlow** installed from source\r\n- **TensorFlow version** 1.3.0\r\n- **Python version**:  2.7.5\r\n- **Bazel version**:\r\nBuild label: 0.5.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Aug 25 10:00:00 2017 (1503655200)\r\nBuild timestamp: 1503655200\r\nBuild timestamp as int: 1503655200\r\n\r\n- **CUDA/cuDNN version**: no CUDA\r\n- **GPU model and memory**: no GPU\r\n- **Exact command to reproduce**: Compilation with the following command fails:\r\n\r\n bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nERROR: /root/tensorflow/tensorflow/contrib/verbs/BUILD:133:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma' failed (Exit 1).\r\nIn file included from ./tensorflow/core/platform/stream_executor.h:26:0,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_util.h:23,\r\n                 from tensorflow/contrib/verbs/rdma.cc:24:\r\n./tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory\r\n #include \"cuda/cuda_config.h\"\r\n                              ^\r\ncompilation terminated.\r\n\r\nThe ./configure is done with all the defaults except enabling verbs.\r\n\r\nReverting the following commit fixes compilation:\r\ncommit 0d864630161d9f3b9eaef0b7c6ce7443654df97a\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Fri Sep 22 13:48:33 2017 -0700\r\n\r\n    Move GPU-specific dependencies of core/grappler:devices into\r\n    cuda_deps.\r\n\r\n    Fix #includes and deps of contrib/verbs:verbs_util, in particular\r\n    removing an unnecessary #include of gpu_util.h that relied on a\r\n    transitive dependency through :devices.\r\n\r\n    PiperOrigin-RevId: 169732234\r\n\r\n@junshi15 ", "comments": ["The existing verbs implementation assumes GPU usage. For example the following line calls GPUUtil.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L714\r\n\r\nYou can make it work with CPU-only cases, with some code change, I suppose.", "@dariavel Thanks for filing the issue.\r\n@junshi15 Thanks for the quick update.\r\n\r\nNote that packages under `contrib` are less well supported by the core TensorFlow team.\r\nI'm marking this as contributions welcome, in case somebody in the community would like to attempt to add verbs support for non-GPU.", "PR #14290 fixed the issue."]}, {"number": 13465, "title": "Remove unnecessary specification for default kernel name", "body": "This line doesn't effect in any cases.\r\n\r\nThe docker container for latest-py2 is working well even without this line.\r\nAlthough it's written `= 'python2'`, the docker container for latest-py3 is working.\r\n\r\n`parameterized_docker_build.sh` or other scripts are not overwriting the line.\r\nIt should be removed, or overwritten when the images are built.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13464, "title": "Tensorflow Windows 64bit build for Python 2.7", "body": "Dear All,\r\n\r\nI happened to have seemingly successfully built **tensorflow 1.3.1 (CPU only) for Python 2.7 64bit** with Visual Studio 2015 on Windows.\r\n\r\nApart from numerous minor manual tweaks, the main things that I had to do are the following:\r\n\r\n-  modify _tensorflow-1.3.1\\tensorflow\\contrib\\cmake\\tools\\create_def_file.py_\r\n-  modify _tensorflow-1.3.1\\tensorflow\\tools\\git\\gen_git_source.py_. \r\n-  modify some of the automatically generated _*.py_ files.\r\n-  copy some folders like _tensorflow\\core\\profiler_ to _tensorflow\\python\\profiler_ for the final distribution package.\r\n- add blank ___init__.py_ files in some folders for the final distribution package.\r\n- translate some Cmake build routines to Matlab (as it was easier for me to debug, :p )\r\n\r\nI have tested some example codes and found to be running as expected. I however believe to make thorough tests to claim whether my build is 100% okay or not.\r\n\r\nI have found in different forums that many users had struggled to build for Python 2.7 64bit in Windows and even if built, ended up with import errors, missing files, etc. I don't know whether they had finally succeeded or not.\r\n\r\nSo, for those who are really desperate to have one, I have shared the wheel package [here](http://mohammadulhaque.alotspace.com/download.php?id=56287).\r\n\r\n**Please beware that there is a high possibility of ABI mismatch, if used.**\r\n\r\nAcknowledgement: _I would like to acknowledge [Wingware](https://wingware.com/) for providing me a professional license for Wing IDE, which I have used, and in general use, extensively for debugging purposes in Python._\r\n\r\nDisclaimer:  _The above shared wheel package is for illustrative purposes only which, I hope, will provide some useful information regarding the build process. It is supplied \"AS IS\" without any warranties and support. I assume no responsibility or liability for its use of any kind._\r\n\r\n\r\n\r\n", "comments": ["Glad to hear that you were able to get this going @mohammadul .\r\nWe are unlikely to have the bandwidth to support any official Python 2.7 binary releases, but if you'd like to contribute the changes you had to made that would be great as it may help others that need to use Python 2.7 on Windows. Feel free to send a pull request.\r\n\r\nClosing this out since it isn't a bug or feature request.", "Hi @mohammadul ,\r\nAs i want to use python 2.7 with tensorflow on windows.\r\nI am not able to access your given link for whl file. Can you please provide me it will be quite helpful.\r\n"]}, {"number": 13463, "title": "DataLossError (see above for traceback): corrupted record at 12", "body": "I have a big problem, I use the  tfrecord file to import data for my tensorflow program. But, when the program run a period of time\uff0c it displays the DataLossError:\r\n------------------------\r\n\r\n### System information\r\nOS Platform and Distribution : Linux Ubuntu 14.04\r\nTensorFlow installed from : Anaconda\r\nTensorFlow version : 1.3.0\r\nPython version:  2.7.13\r\nCUDA/cuDNN version: 8.0 / 6.0  \r\nGPU model and memory: Pascal TITAN X\r\n\r\n### Describe the problem\r\n 2017-10-03 19:45:43.854601: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: corrupted record at 12\r\nTraceback (most recent call last):\r\n  File \"east_quad_train_backup.py\", line 416, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"east_quad_train_backup.py\", line 330, in main\r\n    Training()\r\n  File \"east_quad_train_backup.py\", line 312, in Training\r\n    feed_dict={learning_rate: lr})\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 12\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,512,512,3], [?,128,128,9]], output_types=[DT_UINT8, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n\t [[Node: gradients/Tile_grad/Shape/_23 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_442_gradients/Tile_grad/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op u'IteratorGetNext', defined at:\r\n  File \"east_quad_train_backup.py\", line 416, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"east_quad_train_backup.py\", line 330, in main\r\n    Training()\r\n  File \"east_quad_train_backup.py\", line 251, in Training\r\n    batch_image, batch_label = iterator.get_next()\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 304, in get_next\r\n    name=name))\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 379, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/t/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): corrupted record at 12\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,512,512,3], [?,128,128,9]], output_types=[DT_UINT8, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Iterator)]]\r\n\t [[Node: gradients/Tile_grad/Shape/_23 = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_442_gradients/Tile_grad/Shape\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\n\r\nThanks anyone to answer this question.", "comments": ["@huangrandong is this problem repeatable, or did it happen just one time? ", "@cy89 , thank you for your response\u3002This problem happened many times,and it will come out whenever i run my program. The problem can not be repeatable. the reason can be the problem of my computer configuration. my program can run on another machine and don't display the error.", "Can you post a small example that will cause the `DataLossError` after running it for a while, so that we can see what the problem is?", "@reedwm  my code is used to put the numpy array into  the TFrecord file and read the it from the same file ,this is my code:\r\n# create the TFrecord file function:\r\nimg_tfrecord_name = image_base_name + \".tfrecord\"\r\nwriter = tf.python_io.TFRecordWriter(new_label_path + img_tfrecord_name)\r\nlabel_concate = np.concatenate((score_map, x1_offset, y1_offset,\r\n                                                x2_offset, y2_offset, x3_offset,\r\n                                                y3_offset, x4_offset, y4_offset), axis = -1)\r\norg_train_image = cv2.imread(org_train_images_path + img_name)\r\norg_train_image_resize = cv2.resize(org_train_image,\r\n                                        (input_image_size, input_image_size))\r\nassert org_train_image_resize.shape == (512,512,3)\r\norg_train_image_resize = org_train_image_resize.astype(np.uint8)\r\norg_train_image_resize_raw = org_train_image_resize.tostring()\r\nlabel_concate = label_concate.astype(np.float32)\r\nlabel_concate_raw = label_concate.tostring()\r\nexample = tf.train.Example(\r\n                     features = tf.train.Features(\r\n                     feature = {'image':tf.train.Feature(bytes_list = \r\n                                       tf.train.BytesList(value[org_train_image_resize_raw])),\r\n                                      'label':tf.train.Feature(bytes_list = tf.train.BytesList(value=[label_concate_raw]))}))\r\nserialized = example.SerializeToString()\r\nwriter.write(serialized)\r\nprint 'writer  ',img_name,'  DOWN!'\r\nwriter.close()\r\n\r\n# read the TFrecord file \uff1a\r\ndef _parse_function_for_train(example_proto):\r\n       features = {'image': tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n                          'label': tf.FixedLenFeature((), tf.string, default_value=\"\")}\r\n        parsed_features = tf.parse_single_example(example_proto, features)\r\n        image_raw_out = parsed_features['image']\r\n        label_raw_out = parsed_features['label']\r\n        image_out = tf.decode_raw(image_raw_out, tf.uint8)\r\n        label_out = tf.decode_raw(label_raw_out, tf.float32)\r\n        image_out = tf.reshape(image_out, [512, 512, 3])\r\n        label_out = tf.reshape(label_out, [128,128,9])\r\n        return image_out, label_out\r\n\r\ndef CreateTrainDataset():\r\n      train_image_label_tfrecord_list = [\"t1.tfrecord\", \"t2.tfrecord\",......]\r\n      train_dataset = tf.contrib.data.TFRecordDataset(train_image_label_tfrecord_list)\r\n      train_dataset = train_dataset.map(_parse_function_for_train)\r\n      batched_train_dataset = train_dataset.batch(512)\r\n      return batched_train_dataset\r\n\r\nbatched_train_dataset = CreateTrainDataset()\r\niterator = batched_train_dataset.make_initializable_iterator()\r\nbatch_image, batch_label = iterator.get_next()\r\nwith tf.Session() as sess:\r\n          sess.run(iterator.initializer)\r\nWhen the above code run some iterations, the DataLossError will com out\r\n\r\n", "@huangrandong can you post a complete, self-contained example I can copy to a text file and run? In the code above, `image_base_name` is not defined.\r\n\r\n@saxenasaurabh @vrv, any idea what the problem could be?\r\n", "@reedwm you can define the variable which the code didn't define. and the code is used to put a numpy array of image and  another label array into the tfrecord file. Then, reading the two array from the tfrecord file", "It's much easier to quickly reproduce these issues if I have a self-contained example without having to define variables. Perhaps the issue only occurs for certain values of `x1_offset`, for example. So can you please add a complete example?", "I also had reports of this error which appears to occur randomly during the training. It happened on multiple occasions and with different reported offsets (see https://github.com/OpenNMT/OpenNMT-tf/issues/19).\r\n\r\nTo investigate the issue, I wrote a small script that repeatedly loops over the **same** *TFRecord* dataset that threw the error and applies the same processing as done during the training. However, I was not able to reproduce it, indicating that no records are corrupted in the file and something else is going one during the training.\r\n\r\nAny pointers to better investigate the issue would be appreciated.", "Same problem here. For several different sets of TFRecord files we get this error at random times during training.", "I have reproduced the error at the same record location. The first and third got the error in the middle of 'Filling up shuffle buffer' and the second got the error in the beginning of that. In my case, this error looks highly relevant with the buffer shuffling process although different size of buffer didn't work. I hope this would be helpful for debugging.\r\n\r\n\r\n```\r\n[kwon@ssi-dnn-slave-002 wsj_kaldi_tf]$ grep DataLossError wsj.log\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 3449023918\r\nDataLossError (see above for traceback): corrupted record at 3449023918\r\n[kwon@ssi-dnn-slave-002 wsj_kaldi_tf]$ grep DataLossError wsj.log1\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 3449023918\r\nDataLossError (see above for traceback): corrupted record at 3449023918\r\n[kwon@ssi-dnn-slave-002 wsj_kaldi_tf]$ grep DataLossError wsj.log2\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 3449023918\r\nDataLossError (see above for traceback): corrupted record at 3449023918\r\n```", "Allow me to further complicate matters. (Although I am not 100% sure that it is the same issue)\r\n\r\nI have some custom data and know that the TFRecord is not corrupt, because I've iterated over it (using the same code) successfully before. Now I've encountered the same situation that homink described.\r\nAfter restarting my machine it is again working as intended.\r\n\r\nAssuming that it is related, is there any caching involved when reading the .tfrecord? Either from tensorflow, python or the OS? (I am currently running it on Win10)", "@FirefoxMetzger I am too having this issue so I tried restarting my machine, as you did, and it did not fix the problem.  I'm using Ubuntu 16.04.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "/CC @mrry @saxenasaurabh, any ideas what the issue could be? This is hard to debug without a small example that reproduces the issue.", "AFAICT, this problem only affects ZLIB-compressed TFRecord files (because that is the [sole source of `\"corrupted record at\"`](https://github.com/tensorflow/tensorflow/blob/ccb687421b085110c88c101b94d6c83569148459/tensorflow/core/lib/io/record_reader.cc#L114) in an error message). The source indicates a CRC mismatch. I'm a little surprised that none of the code snippets mention ZLIB compression.\r\n\r\n/CC @saxenasaurabh @rohan100jain, who last touched the ZLIB-related code in that file.", "I confirm that the issue was encountered without any compression configured, unless it is the default (which is not AFAIK).", "Pardon my mistake, indeed there are other code paths that can print that message, and each of them is related to a CRC mismatch.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Anymore thoughts on this? It's a big issue for me but I don't know where to start debugging. Each time I reprocess my data the errors appear in different locations.  Sometimes it takes a couple training epochs to occur. ", "/sub\r\n\r\nThis is happening to us as well, any ideas?\r\n\r\nEdit to add:  We are using zlib compression, reading a bunch of files off GCS with interleave and shuffling them into one large Dataset; as a result, there's no way to catch the error and try and carry on.\r\n\r\nIs it possible this is some GCS transient?  I'm also having trouble repeating it with the same data.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@huangrandong @guillaumekln @tjvandal @homink @rjbruin @FirefoxMetzger have any of you been able to create a small example that reproduces the problem?  That would help us track down the problem.", "FYI: for my part I have not been having the issue anymore for a while now. I am running multiple training scripts every day on TensorFlow 1.3 without issue. We are using the same set-up as before, where the TFRecords are accessed over a NFS mount and we are still using the same sets of TFRecords as before.\r\n\r\nEDIT: scratch that, it just happened again today. I'll try to post a small example soon!", "## System information\r\n \r\nOS Platform and Distribution : Linux Ubuntu 16.04\r\nTensorFlow installed from : pip\r\nTensorFlow version : 1.5.0\r\nPython version: 2.7.12\r\nIntel i5-4590\r\n\r\nI have the same problem since I changed the system. As a small example that reproduces the problem is the small repository [robot-grasp-detection](https://github.com/tnikolla/robot-grasp-detection).\r\n\r\nTo make the code run\r\n\r\n* download [data](http://pr.cs.cornell.edu/grasping/rect_data/data.php)\r\n* run `./build_cgd_dataset.py`\r\n* run `./grasp_det.py --train_or_validation=train --model_path=./models/grasp/m4/m4.ckpt --batch_size=16 --num_epochs=10`\r\n\r\nIt doesn't show the error when validating `./grasp_det.py --train_or_validation=validation --model_path=./models/grasp/m4/m4.ckpt --batch_size=1 --num_epochs=1`.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tnikolla meaning no disrespect, but for isolating and debugging issues, what we mean by \"small\" is code that's been minimized to exhibit the bug in as few lines as possible, rather than a whole model. If you've got a deterministic exhibitor, the binary search to minimize it should be that much work for the author, but is a lot of work for someone unfamiliar with the code. If you could do that, we'd be able to converge rapidly. \r\n\r\nI'm going to close this bug for lack of activity; please reopen if we can get a small exhibitor to work with.", "We just ran into this error and it turned out to be legitimately caused by a corrupted record in our tfrecord file. Just threw these functions together to help check for this in future, thought they may be useful to others: https://gist.github.com/ed-alertedh/9f49bfc6216585f520c7c7723d20d951", "I also ran into this error in a totally different context. The error occurs randomly with the same code/tfrecord. What seems constant, is that this occurs only with very large tfrecord (>5Gb)", "I also ran into this error and my tfrecord files are also >5Gb (~55Gb) without compression.", "Hi. I am also running into the corrupted record error. According to the functions made by @ed-alertedh, my tfrecord file is perfectly fine without any crc mismatch. I figured out that you can momentarily get rid of the corrupted error by cleaning the linux memory cache with the command `sudo sh -c \"sync; echo 1 > /proc/sys/vm/drop_caches\"`. This might indicate that the records are getting corrupted in memory or during the read from drivers.", "Can we re-open this? I have a minimal example:\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_iterator():\r\n\r\n    output_buffer_size = 1000\r\n\r\n    pattern = 'test.txt.gz'\r\n\r\n    filenames = tf.data.Dataset.list_files(pattern).repeat()\r\n\r\n    dataset = filenames.apply(\r\n        tf.contrib.data.parallel_interleave(\r\n            lambda filename: tf.data.TFRecordDataset(filename, compression_type='GZIP'),\r\n            cycle_length=8,\r\n        )\r\n    )\r\n\r\n    dataset = dataset.map(\r\n        lambda src: tf.string_split([src]).values,\r\n        num_parallel_calls=8\r\n    ).prefetch(output_buffer_size)\r\n\r\n    iterator = dataset.make_initializable_iterator()\r\n\r\n    source = iterator.get_next()\r\n\r\n    return iterator, source\r\n\r\n\r\ndef main():\r\n\r\n\r\n    graph = tf.Graph()\r\n\r\n    with graph.as_default():\r\n        iterator, source = get_iterator()\r\n\r\n    with tf.Session(graph=graph) as sess:\r\n        table_initializer = tf.tables_initializer()\r\n        sess.run(table_initializer)\r\n        sess.run(iterator.initializer)\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        for __ in range(100):\r\n\r\n            value = sess.run([source])\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nHere's the necessary file: [test.txt.gz](https://github.com/tensorflow/tensorflow/files/2058033/test.txt.gz)\r\n\r\nHere's the output:\r\n\r\n```\r\n2018-05-31 06:38:55.956213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-05-31 06:38:55.956679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \r\nname: GeForce GTX 680 major: 3 minor: 0 memoryClockRate(GHz): 1.163\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 1.95GiB freeMemory: 1.58GiB\r\n2018-05-31 06:38:55.956700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-05-31 06:38:56.186321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-31 06:38:56.186355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 \r\n2018-05-31 06:38:56.186366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N \r\n2018-05-31 06:38:56.186483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1355 MB memory) -> physical GPU (device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\nTraceback (most recent call last):\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"example.py\", line 54, in <module>\r\n    main()\r\n  File \"example.py\", line 51, in main\r\n    value = sess.run([source])\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"example.py\", line 54, in <module>\r\n    main()\r\n  File \"example.py\", line 41, in main\r\n    iterator, source = get_iterator()\r\n  File \"example.py\", line 30, in get_iterator\r\n    source = iterator.get_next()\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 373, in get_next\r\n    name=name)), self._output_types,\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1666, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3417, in create_op\r\n    op_def=op_def)\r\n  File \"/home/john/Code/venv/tensorflow-rnn/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1743, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): corrupted record at 0\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n```\r\n\r\nMy production code is a lot more complex, obviously. This is just the \"minimal\" example that was requested. Everything works fine if I use a regular `interleave` instead of the `parallel_interleave`.\r\n\r\n## Edit\r\n\r\nI just tried it with an uncompressed txt and without the `compression_type='GZIP'` flag and it failed as well.\r\n\r\nMaybe there's something I don't understand about `parallel_interleave`?\r\n\r\nThanks for your all your hard work!", "You are trying to open a compressed text-file, which doesn't work with a `TFRecordReader`.\r\nPossible solutions would be, to save your data as [TFRecord](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py), or read your data with with a [generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) (of course, there are also other solutions.. )", "@lhlmgr Thank you for pointing out my silly mistake.\r\n\r\nI changed the relevant code from `TFRecordReader` to `TextLineDataset` and everything seems to be working just fine.", "can confirm that after cleaning the cache / restarting the computer @igorgad , problem solved temporarily", "Problem occurs more often on large dataset, I am using the shuffling function. ", "I also encountered this problem these days. At last, I found that it is due to I have not revised the code downloaded from other's github. Then, python will run the *.pyc file by default and my root directory lack the '__pycache__' directory, which supposed to contained those *.pyc file. I hope it will help.", "Having the same problem, with large dataset files.  It often happens after multiple successful training runs. \r\n\r\nI can fix it by copying the same file (according to diff, hash, size) back from my backup server.", "I am seeing this on large dataset too - and it is repeatable. Tried clearing up my cache and reboots, also tweaking around parallel_interleave(cycle_length), num_parallel_calls and buffer_size of my shuffle_and_repeat. - Everything's like a temporary fix but the underlying problem is still there.  ", "I get this problem on a dataset where each example has about 200k 32-bit floats, and it definitely seems related to the shuffle buffer. Without any shuffling, it works fine. With a small buffer as well. But when I want to increase the shuffle buffer size from 500 to 1500, this error comes up, with the same message every time:\r\n\r\nDataLossError (see above for traceback): truncated record at 1718960034\r\n\r\nEDIT: Encountering this issue with a small buffer size too, but later sometime during the training. I regenerated my dataset but to no avail. ", "same problem with tfrecord > 10gb. tf version 1.10, ubuntu 17, python 3.6.\r\n\r\nRebooting only solve the problem temporarily :(", "Things that solved this issue for me:\r\n- write the tfrecords onto SSD or HDD on your local machine \r\n- reduce the buffer size for shuffle_and_repeat & also check your core size and adjust num_parallel_calls to be not more than number of cpu cores ", "Having the same problem\r\n\r\nThe setup is as follows. \r\nI'm running inside a nvidia-docker container, TF version 1.10, Ubuntu 16.04.\r\nMy TFRecords are 161GB in size.\r\n\r\nSince the code is sensitive I can't post it but I'll explain what goes on and how I can repeat this exception.\r\n\r\nAfter restarting the machine and rebuilding the image (still TF 1.10) before training I'll actually go over the entire dataset, and count its size using `train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))` - so far no problems - notice this is a full iteration over that tfrecords.\r\nThen while iterating through a session that accepts the handle string, training will start normally and will suddenly fail indicating `DataLossError: corrupted record at X`.\r\nIf then I try to run the same script again, it will fail on the `train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))` immediately  indicating the corrupted record at exactly same value (same `X`)\r\n\r\nThis is now repeatable forever.\r\n\r\nIf I restart the machine it the whole process starts over, the first run will randomly fail during training, at an unexpected record number (this time a different number) and then no matter how many times I rerun the script I will fail on the first call on the same record.\r\n\r\nI think this should be reopened and solved - TFRecords are very important for big data training.\r\n", "I encountered the same problem. The DataLossError: corrupted record at X failed randomly when training on a large dataset.\r\ntf.version is 1.10, ubuntu 16.04, python 3.6.\r\nCan anyone help to solve the problem?", "I also encountered the same problem. There really was an issue with one of the TFRecord files I had. The way that I found it was to loop over all TFRecords and create a TFRecordDataset. Then I parsed them and just tried to access the data. I just printed the name of TFRecord files and then found the ones which were problematic. For me, it was because I was using a very large dataset and my hard drive limit on the cluster was met and my code created some corrupted TFRecords.", "This is not the same problem. I can CRC check every record several times, and then suddenly one record, which could previously be read, fails.\n\nRunning\n\n> sudo sh -c 'echo 3 >/proc/sys/vm/drop_caches'\n\ntemporally solve the problem, which show that the corruption happen in memory.\n\n\n________________________________\nFrom: siavash-khodadadeh <notifications@github.com>\nSent: Monday, October 1, 2018 17:13\nTo: tensorflow/tensorflow\nCc: stillwalker1234; Comment\nSubject: Re: [tensorflow/tensorflow] DataLossError (see above for traceback): corrupted record at 12 (#13463)\n\n\nI also encountered the same problem. There really was an issue with one of the TFRecord files I had. The way that I found it was to loop over all TFRecords and create a TFRecordDataset. Then I parsed them and just tried to access the data. I just printed the name of TFRecord files and then found the ones which were problematic. For me, it was because I was using a very large dataset and my hard drive limit on the cluster was met and my code created some corrupted TFRecords.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<https://nam01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F13463%23issuecomment-425941563&data=02%7C01%7C%7Cec70ab772ab147e2aa2908d627b07302%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636740036127479228&sdata=YfOWaokyXKfro0mLkEGcQcnBwDYyiQNmRJWXXQqWhaM%3D&reserved=0>, or mute the thread<https://nam01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAISpV-eRY4JfbSKkJMlZZ8h0Atx3FOZqks5ugjEWgaJpZM4PsAfN&data=02%7C01%7C%7Cec70ab772ab147e2aa2908d627b07302%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636740036127479228&sdata=bpYVak0Srjc7fhlptLmwHoM15%2FPXod0bvm1ZXw51Jw0%3D&reserved=0>.\n", "I just found the problem was caused from the unstable RAM (using the CRC checking for tfrecord here: https://gist.github.com/ed-alertedh/9f49bfc6216585f520c7c7723d20d951). After I downgrade the memory frequency from 3066 to 2800 in BIOS, it works fine now. ", "In my experience it helps\r\n* to keep the dataset files rather small (also helps for shuffling the data properly)\r\n* and strictly restrict the number of threads and parallel reads to the number of processors which are available on the system.\r\n\r\nStill, it happens occasionally (but far less frequent). So please fix it ;-)", "Me too :\r\nTensorflow 1.10.1 (recompiled locally using unchanged github source at tags/v1.10.1)\r\nPython 3.6\r\nKernel : 4.15.0-34-generic Ubuntu 18.04 LTS\r\nNo NFS, just linux raid (mdadm)\r\nTFrecord : 10 file of 1.8GB\r\nRAM: 62GB \r\n\r\nDropping cache fix the corruption temporary. I was running tensorflow 1.4.0 and there was no crash/corruption during all my training.\r\n\r\nI suspect it have to do with the shuffle operation as I do also have random segfault/error just before or after \"Filling up shuffle buffer\" or \"Shuffle buffer filled.\" ( I am using unchanged object detection pipeline)\r\n\r\nEdit: Same thing happen with tensorflow 1.11 (pip install).\r\nThere is 2 differents error message that I believe related to the same problem :\r\n- The \"DataLossError\" and \"corrupted record\"\r\n- And \"InvalidArgumentError: Invalid PNG data, size \" (I use png files)\r\n\r\nI saved md5sum of my tfrecords before training. When it crash with the error above,and I run md5sum: one of my tfrecord changes. Once I dropped the cache, the md5sum is back to what it was before training.", "If someone from the TensorFlow team is still reading this thread, would it be possible to optionally ignore records that fail the CRC check instead of raising an exception? This would be an acceptable behavior during training.", "I am not sure this is acceptable behavior : we are lucky that the CRC check failed, otherwise you will be silently training with corrupted data which is kind of scary. Some may say this is a kind data augmentation? ;-) ", "I guess it should skip them them\u2026 not return them \ud83d\ude09\r\n________________________________\r\nFrom: mhtrinh <notifications@github.com>\r\nSent: Wednesday, October 10, 2018 22:09\r\nTo: tensorflow/tensorflow\r\nCc: stillwalker1234; Comment\r\nSubject: Re: [tensorflow/tensorflow] DataLossError (see above for traceback): corrupted record at 12 (#13463)\r\n\r\n\r\nI am not sure this is acceptable behavior : we are lucky that the CRC check failed, otherwise you will be silently training with corrupted data which is kind of scary. Some may say this is a kind data augmentation? ;-)\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F13463%23issuecomment-428710406&data=02%7C01%7C%7C77712d69c0d84ab4916f08d62eec5982%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636747989974383638&sdata=Hp0XAHbihMlM9%2Fy3mm4MPoaxMqRT9L5ZF5B6lKjDjLs%3D&reserved=0>, or mute the thread<https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAISpV3q7kq1MUXf9zdurItroWQwsjxebks5ujlQQgaJpZM4PsAfN&data=02%7C01%7C%7C77712d69c0d84ab4916f08d62eec5982%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636747989974383638&sdata=UsIfdnQe4cgvxAHDK3J2nKyFJST0tebWzkljJ0R1BE0%3D&reserved=0>.\r\n", "Hi all, I ran into the same error several times. Sometimes it gets fixed after restarting the machine (weird, I know). Updated the drivers as well - still remains. Any update from TF folks?\r\n\r\n", "I was having a similar error for my tfrecord files. Then I went back to the script that converts my images to tfrecords format. I rerun the code with a single thread instead of multiple threads, and this fixed the problem. Running the conversion script with multiple threads makes it slower and results in corrupted files for me. ", "My problem with this is that I really had a corrupted tfrecord file: I was sending it to the other machine, sending process was stopped, but the file remained there. I didn't notice that it was just a part of a file ...\r\n\r\nSo you could check you tfrecords file with simple processing:\r\n```\r\nimport tensorflow as tf\r\nimport glob\r\n\r\ntrain_files = sorted(glob.glob('./train*.tfrecord'))\r\nfor f_i, file in enumerate(train_files): \r\n    print(f_i) \r\n    total_images += sum([1 for _ in tf.python_io.tf_record_iterator(file)])\r\n```\r\n\r\nThis code raises an exception when it reaches the corrupted tfrecord file (exception was triggered in `tf.python_io.tf_record_iterator(file)`).", "> My problem with this is that I really had a corrupted tfrecord file: I was sending it to the other machine, sending process was stopped, but the file remained there. I didn't notice that it was just a part of a file ...\r\n\r\nI'm sure mine is not corrupted, it was generated using single threaded not multiprocess.\r\n\r\nMy case is simply, the following works fine\r\n\r\n```\r\nit=tf.python_io.tf_record_iterator(filename)\r\nwhile True:\r\n  try: s=it.next()\r\n  except StopIteration: break\r\n```\r\n\r\nbut the following raise  DataLossError instead of StopIteration and triggering `repeat()`\r\n\r\n```\r\n  dataset = tf.data.TFRecordDataset(tf_files)\r\n  dataset = dataset.repeat()\r\n  dataset = dataset.shuffle(buffer_size) # buffer\r\n  dataset = dataset.batch(batch_size, drop_remainder=True)\r\n  dataset = dataset.map(get_preprocessor(pad_size))\r\n```\r\n", "> My problem with this is that I really had a corrupted tfrecord file: I was sending it to the other machine, sending process was stopped, but the file remained there. I didn't notice that it was just a part of a file ...\r\n> \r\n> So you could check you tfrecords file with simple processing:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import glob\r\n> \r\n> train_files = sorted(glob.glob('./train*.tfrecord'))\r\n> for f_i, file in enumerate(train_files): \r\n>     print(f_i) \r\n>     total_images += sum([1 for _ in tf.python_io.tf_record_iterator(file)])\r\n> ```\r\n> \r\n> This code raises an exception when it reaches the corrupted tfrecord file (exception was triggered in `tf.python_io.tf_record_iterator(file)`).\r\n\r\nThis is really gold to find the corrupted files. Thanks", "> but the following raise DataLossError instead of StopIteration and triggering `repeat()`\r\n\r\nthe DataLossError happens when on GPU (ex. colab)\r\nif you download the samefile and run the below it would work fine\r\n\r\n```\r\n_ = sum([1 for _ in tf.python_io.tf_record_iterator(file)])\r\n```", "The recently introduced [`tf.data.experimental.ignore_errors`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/ignore_errors) transformation could be a solution for the rare corruption that happens during the training.", "does the `.repeat()` understand that?\r\n\r\n```\r\n  dataset = dataset.repeat()\r\n```", "@guillaumekln thanks for the pointer to [`tf.data.experimental.ignore_errors`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/ignore_errors). I do have a follow-up question on that:\r\n\r\nHow does it handle `tf.errors.OutOfRangeError` - does it ignore that too? I use this to track the end of my dataset (validation). When I `ignore errors`, it seems that the validation loop is stuck upon reaching the end and `sess.run` doesn't yield anything at that point.\r\n```\r\ndataset = dataset.apply(tf.data.experimental.ignore_errors())\r\n```", "> does the `.repeat()` understand that?\r\n\r\nI think it does.\r\n\r\n> How does it handle `tf.errors.OutOfRangeError` - does it ignore that too? I use this to track the end of my dataset (validation). When I `ignore errors`, it seems that the validation loop is stuck upon reaching the end and `sess.run` doesn't yield anything at that point.\r\n\r\nNot sure sure about this. The following snippet does raise the `OutOfRangeError` exception:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndataset = tf.data.Dataset.range(10)\r\ndataset = dataset.apply(tf.data.experimental.ignore_errors())\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\nwith tf.Session() as sess:\r\n    while True:\r\n        print(sess.run(next_element))\r\n```", "@guillaumekln You're right, what I'm seeing may not have to do with `OutofRangeError`, but the execution stalls indefinitely when it encounters corrupt data within a TFRecord, despite using `tf.data.experimental.ignore_errors()`. I've created #25700 with the minimal code to reproduce what I'm seeing. Have you encountered this before?\r\n\r\nUPDATE: https://github.com/tensorflow/tensorflow/issues/25700#issuecomment-462959084\r\n> I think the issue was that, when ignore_errors is used, the same file will repeat as the file_index is not moved forward to completion.\r\n\r\nBug fix by @yongtang in #25705 \r\n", "\r\n\r\n\r\n> @reedwm  my code is used to put the numpy array into  the TFrecord file and read the it from the same file ,this is my code:\r\n> create the TFrecord file function:\r\n> img_tfrecord_name = image_base_name + \".tfrecord\"\r\n> writer = tf.python_io.TFRecordWriter(new_label_path + img_tfrecord_name)\r\n> label_concate = np.concatenate((score_map, x1_offset, y1_offset,\r\n> x2_offset, y2_offset, x3_offset,\r\n> y3_offset, x4_offset, y4_offset), axis = -1)\r\n> org_train_image = cv2.imread(org_train_images_path + img_name)\r\n> org_train_image_resize = cv2.resize(org_train_image,\r\n> (input_image_size, input_image_size))\r\n> assert org_train_image_resize.shape == (512,512,3)\r\n> org_train_image_resize = org_train_image_resize.astype(np.uint8)\r\n> org_train_image_resize_raw = org_train_image_resize.tostring()\r\n> label_concate = label_concate.astype(np.float32)\r\n> label_concate_raw = label_concate.tostring()\r\n> example = tf.train.Example(\r\n> features = tf.train.Features(\r\n> feature = {'image':tf.train.Feature(bytes_list =\r\n> tf.train.BytesList(value[org_train_image_resize_raw])),\r\n> 'label':tf.train.Feature(bytes_list = tf.train.BytesList(value=[label_concate_raw]))}))\r\n> serialized = example.SerializeToString()\r\n> writer.write(serialized)\r\n> print 'writer  ',img_name,'  DOWN!'\r\n> writer.close()\r\n> read the TFrecord file \uff1a\r\n> def _parse_function_for_train(example_proto):\r\n> features = {'image': tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n> 'label': tf.FixedLenFeature((), tf.string, default_value=\"\")}\r\n> parsed_features = tf.parse_single_example(example_proto, features)\r\n> image_raw_out = parsed_features['image']\r\n> label_raw_out = parsed_features['label']\r\n> image_out = tf.decode_raw(image_raw_out, tf.uint8)\r\n> label_out = tf.decode_raw(label_raw_out, tf.float32)\r\n> image_out = tf.reshape(image_out, [512, 512, 3])\r\n> label_out = tf.reshape(label_out, [128,128,9])\r\n> return image_out, label_out\r\n> def CreateTrainDataset():\r\n> train_image_label_tfrecord_list = [\"t1.tfrecord\", \"t2.tfrecord\",......]\r\n> train_dataset = tf.contrib.data.TFRecordDataset(train_image_label_tfrecord_list)\r\n> train_dataset = train_dataset.map(_parse_function_for_train)\r\n> batched_train_dataset = train_dataset.batch(512)\r\n> return batched_train_dataset\r\n> batched_train_dataset = CreateTrainDataset()\r\n> iterator = batched_train_dataset.make_initializable_iterator()\r\n> batch_image, batch_label = iterator.get_next()\r\n> with tf.Session() as sess:\r\n> sess.run(iterator.initializer) \r\n> When the above code run some iterations, the DataLossError will com out\r\n\r\nDon't use org_train_image_resize_raw = org_train_image_resize.tostring(), \r\nJust use org_train_image_resize_raw = org_train_image_resize.tobytes(). \r\nIn my case, change this will solve the problem", "> > @reedwm  my code is used to put the numpy array into  the TFrecord file and read the it from the same file ,this is my code:\r\n> > create the TFrecord file function:\r\n> > img_tfrecord_name = image_base_name + \".tfrecord\"\r\n> > writer = tf.python_io.TFRecordWriter(new_label_path + img_tfrecord_name)\r\n> > label_concate = np.concatenate((score_map, x1_offset, y1_offset,\r\n> > x2_offset, y2_offset, x3_offset,\r\n> > y3_offset, x4_offset, y4_offset), axis = -1)\r\n> > org_train_image = cv2.imread(org_train_images_path + img_name)\r\n> > org_train_image_resize = cv2.resize(org_train_image,\r\n> > (input_image_size, input_image_size))\r\n> > assert org_train_image_resize.shape == (512,512,3)\r\n> > org_train_image_resize = org_train_image_resize.astype(np.uint8)\r\n> > org_train_image_resize_raw = org_train_image_resize.tostring()\r\n> > label_concate = label_concate.astype(np.float32)\r\n> > label_concate_raw = label_concate.tostring()\r\n> > example = tf.train.Example(\r\n> > features = tf.train.Features(\r\n> > feature = {'image':tf.train.Feature(bytes_list =\r\n> > tf.train.BytesList(value[org_train_image_resize_raw])),\r\n> > 'label':tf.train.Feature(bytes_list = tf.train.BytesList(value=[label_concate_raw]))}))\r\n> > serialized = example.SerializeToString()\r\n> > writer.write(serialized)\r\n> > print 'writer  ',img_name,'  DOWN!'\r\n> > writer.close()\r\n> > read the TFrecord file \uff1a\r\n> > def _parse_function_for_train(example_proto):\r\n> > features = {'image': tf.FixedLenFeature((), tf.string, default_value=\"\"),\r\n> > 'label': tf.FixedLenFeature((), tf.string, default_value=\"\")}\r\n> > parsed_features = tf.parse_single_example(example_proto, features)\r\n> > image_raw_out = parsed_features['image']\r\n> > label_raw_out = parsed_features['label']\r\n> > image_out = tf.decode_raw(image_raw_out, tf.uint8)\r\n> > label_out = tf.decode_raw(label_raw_out, tf.float32)\r\n> > image_out = tf.reshape(image_out, [512, 512, 3])\r\n> > label_out = tf.reshape(label_out, [128,128,9])\r\n> > return image_out, label_out\r\n> > def CreateTrainDataset():\r\n> > train_image_label_tfrecord_list = [\"t1.tfrecord\", \"t2.tfrecord\",......]\r\n> > train_dataset = tf.contrib.data.TFRecordDataset(train_image_label_tfrecord_list)\r\n> > train_dataset = train_dataset.map(_parse_function_for_train)\r\n> > batched_train_dataset = train_dataset.batch(512)\r\n> > return batched_train_dataset\r\n> > batched_train_dataset = CreateTrainDataset()\r\n> > iterator = batched_train_dataset.make_initializable_iterator()\r\n> > batch_image, batch_label = iterator.get_next()\r\n> > with tf.Session() as sess:\r\n> > sess.run(iterator.initializer)\r\n> > When the above code run some iterations, the DataLossError will com out\r\n> \r\n> Don't use org_train_image_resize_raw = org_train_image_resize.tostring(),\r\n> Just use org_train_image_resize_raw = org_train_image_resize.tobytes().\r\n> In my case, change this will solve the problem\r\n\r\nwhy this modification can solve the problem?", "In my case, I solved this problem in this way:\r\nhttps://gist.github.com/ed-alertedh/9f49bfc6216585f520c7c7723d20d951\r\nseveral tfrecord files are corrupted and can be found using above code.\r\nAfter removing this corrupted files, everyting goes well in training.\r\nThe checking process prints like this:\r\nvalidating  train_feat/391072.tfrecord\r\nerror in train_feat/391072.tfrecord at record 391064\r\ncorrupted record at 12\r\nvalidating  train_feat/391073.tfrecord\r\nvalidating  train_feat/391074.tfrecord\r\nvalidating  train_feat/391075.tfrecord\r\nvalidating  train_feat/391076.tfrecord", "I ran into this problem once today while I was using Google Colab GPU Version and I fixed this just restart my notebook. Here is the address of my note book <br> https://colab.research.google.com/drive/1aqKWeqKGSDUiTJFmmlS47MVd8XiFEJWe#scrollTo=jU3twrhvfe9r", "I had this issue.  After execute sudo sh -c \"sync; echo 1 > /proc/sys/vm/drop_caches\" or restart my machine, it fix the problem temporally and can run a few epochs. However, this issue happened sometime later. Finally, I found it may relate to my rams and used memtest86 (https://www.youtube.com/watch?v=9_xFNojChNA) to test each of them. It turned out that one of my rams was faulty. Never have this problem again after plugging out the faulty ram. ", "I just restarted my computer and it works. dk what the problem is. could be memory issue.", "Fixed by increasing the number of tf record shards.\r\n\r\nTo check tf record files:\r\n```\r\ntotal_images = 0\r\ntrain_files = sorted(glob.glob(os.path.join(tfrecord_path, '*')))\r\n    for idx, file in enumerate(train_files):\r\n        try:\r\n            total_images += sum([1 for _ in tf.io.tf_record_iterator(file)]) # Check corrupted tf records\r\n        except:\r\n            print(\"{}: {} is corrupted\".format(idx, file))\r\n    print(\"Succeed, no corrupted tf records found for {} images\".format(total_images))\r\n```", "i encountered this problem in MAC system, the reason is, in my directory , it has a .DS_store file\uff0cbut my code noly need the records data, so when i filter the .DS_store, it run sucessed", "I encountered an issue where TFRecords where occasionally genuinely corrupt (verified with the posted code above) when producing them in parallel, but not when producing them in a single process. It turned out to be an issue with the fact that the list of TFRecords to be produced was not unique, and that would occasionally make two processes write to disk at the same time, causing corruption. So if you encounter this issue and you are using parallelism, double check that your dataset doesn't contain duplicate items.", "I also encountered `DataLossError corrupted record at 0`, when I used generated gzip `tfrecord` to build a train dataset. After I removed the compression gzip during the writing tfrecord process, the error was gone. I assumed this error was related to gzip compression type when generating tfrecord.", "I was getting DataLossError (see above for traceback): Attempted to pad to a smaller size than the input element.\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[8], [8,100], [8,100,4], [8,100,4], [8,100], [8,100], [8,100], [8,100], [8,512,512,3], [8], [8], [8], [8,3]], output_types=[DT_STRING, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64, DT_INT64, DT_BOOL, DT_FLOAT, DT_FLOAT, DT_STRING, DT_INT32, DT_STRING, DT_INT32], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\t [[Node: RandomShuffle_14/_12093 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_8060_RandomShuffle_14\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nI did tried everything that was given on this thread but none of them work. First of all using iterate tf record i was not able detect any corrupt record. Now records were not corrupt but i was still getting this error. After multiple trial and errors I found that my current training set had max 324 boxes for few images. So so all i have to do was to update the train for max box param.\r\ntrain_config: {\r\n  batch_size: 4\r\n  max_number_of_boxes: 325\r\nAnd this solved the problem. In few older version or other variant of OD this param pay be under input_Reader.\r\nHope this will help.\r\nI used below code for record validations.\r\n\r\ndef val_fun1(filenames):\r\n  dataset = tf.data.TFRecordDataset(filenames)\r\n  #dataset = dataset.apply(tf.data.experimental.ignore_errors())\r\n  dataset = dataset.batch(64)\r\n  dataset = dataset.repeat(1)\r\n  \r\n  iterator = dataset.make_one_shot_iterator()\r\n  next_element = iterator.get_next()\r\n  \r\n  with tf.Session() as sess:\r\n      i = 0\r\n      while True:\r\n          try:\r\n              print(i, sess.run(next_element).shape)\r\n              i = i + 1\r\n          except tf.errors.OutOfRangeError:\r\n              print(\"Dataset complete\")\r\n              break\r\n\r\nthanks...Pankaj\r\n", "> In my case, I solved this problem in this way:\r\n> https://gist.github.com/ed-alertedh/9f49bfc6216585f520c7c7723d20d951\r\n> several tfrecord files are corrupted and can be found using above code.\r\n> After removing this corrupted files, everyting goes well in training.\r\n> The checking process prints like this:\r\n> validating train_feat/391072.tfrecord\r\n> error in train_feat/391072.tfrecord at record 391064\r\n> corrupted record at 12\r\n> validating train_feat/391073.tfrecord\r\n> validating train_feat/391074.tfrecord\r\n> validating train_feat/391075.tfrecord\r\n> validating train_feat/391076.tfrecord\r\n\r\nhello my friend I am new to python, would just help me how can I run this script?", "I was facing the same problem , but it's happened when I training several epochs.....", "> I had this issue. After execute sudo sh -c \"sync; echo 1 > /proc/sys/vm/drop_caches\" or restart my machine, it fix the problem temporally and can run a few epochs. However, this issue happened sometime later. Finally, I found it may relate to my rams and used memtest86 (https://www.youtube.com/watch?v=9_xFNojChNA) to test each of them. It turned out that one of my rams was faulty. Never have this problem again after plugging out the faulty ram.\r\n\r\nEncounter the same problem. \"sync; echo 1 > /proc/sys/vm/drop_caches\" works for me.", "> I also encountered `DataLossError corrupted record at 0`, when I used generated gzip `tfrecord` to build a train dataset. After I removed the compression gzip during the writing tfrecord process, the error was gone. I assumed this error was related to gzip compression type when generating tfrecord.\r\n\r\nThank you, your answer resolved my problem that bothered me three hours.", "> I also encountered `DataLossError corrupted record at 0`, when I used generated gzip `tfrecord` to build a train dataset. After I removed the compression gzip during the writing tfrecord process, the error was gone. I assumed this error was related to gzip compression type when generating tfrecord.\r\n\r\nI had a similar problem as I was preparing my data in gzip files to later use them on training a bert model. \r\nThe problem was solved by simply gunzipping (gunzip -r pre*) and then I could train the bert model without a problem. However, if I created the data without the gzip, it did work so for some reason. I had to first create the data with gzip and unzip, it was not possible to create tfrecord files directly. ", "> > My problem with this is that I really had a corrupted tfrecord file: I was sending it to the other machine, sending process was stopped, but the file remained there. I didn't notice that it was just a part of a file ...\r\n> > So you could check you tfrecords file with simple processing:\r\n> > ```\r\n> > import tensorflow as tf\r\n> > import glob\r\n> > \r\n> > train_files = sorted(glob.glob('./train*.tfrecord'))\r\n> > for f_i, file in enumerate(train_files): \r\n> >     print(f_i) \r\n> >     total_images += sum([1 for _ in tf.python_io.tf_record_iterator(file)])\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > This code raises an exception when it reaches the corrupted tfrecord file (exception was triggered in `tf.python_io.tf_record_iterator(file)`).\r\n> \r\n> This is really gold to find the corrupted files. Thanks\r\n\r\nwhere to add this code?\r\nbefore training?\r\nI am really new to this, so i apologize if its too basic."]}, {"number": 13462, "title": "Feature Request: recompute gradient with updated weights within a graph", "body": "Hi,\r\n\r\nI wonder could there could be some new features to recompute gradients with updated weights within a graph or if there is any better way to do this. For example, for estimating hessian norm, we need to compute\r\n\r\ndelta ~ N(0, I)\r\nhessian_norm = 1/M \\sum_{1}^{M}  gradient(f(x+delta))- gradient(f(x-delta))/(2delta)\r\n\r\nwe need to gradient value on x+delta. Currently we will get None type if we use tf.gradient on var+delta directly. \r\n\r\nThank you very much.", "comments": ["Similar operations can be implemented by pytorch:\r\n\r\nhttps://github.com/ucla-vision/entropy-sgd/blob/master/python/optim.py line63-84. gradients are computed on updated weights within a graph. It might be worthwhile for tensorflow to have similar abilities to do this.\r\n\r\n\r\n", "This is too vague for a feature request. It's already possible in tensorflow, StackOverflow is a better place to ask. If you send me a link to your SO question I'll post one possible solution", "https://stackoverflow.com/questions/46672168/how-to-use-tensorflow-to-approximate-hessian-matrixs-norm", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "closing for vagueness"]}, {"number": 13461, "title": "using ExponentialMovingAverage differs in CPU & GPU", "body": "I use BN in cnn,here is the code\r\n\r\n```\r\ndef batch_norm(x, beta, gamma, train_phase, scope='bn', decay=0.9, eps=1e-5):\r\n\r\n    with tf.variable_scope(scope):        \r\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\r\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\r\n\r\n        def mean_var_with_update():\r\n            ema_apply_op = ema.apply([batch_mean, batch_var])\r\n            with tf.control_dependencies([ema_apply_op]):\r\n                return tf.identity(batch_mean), tf.identity(batch_var)\r\n\r\n        mean, var = tf.cond(train_phase, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\r\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\r\n    return normed\r\n```\r\n\r\nwhen i print variables in CPU & GPU, it differs:\r\nIn CPU it shows:\r\n`bn_1/bn_1/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [32]\r\nbn_1/bn_1/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [32]\r\nbn_2/bn_2/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_2/bn_2/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_3/bn_3/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_3/bn_3/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_4/bn_4/moments/Squeeze/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_4/bn_4/moments/Squeeze_1/ExponentialMovingAverage (DT_FLOAT) [64]`\r\nwhile in GPU it shows:\r\n`bn_1/bn_1/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [32]\r\nbn_1/bn_1/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [32]\r\nbn_2/bn_2/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_2/bn_2/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_3/bn_3/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_3/bn_3/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_4/bn_4/moments/moments_1/mean/ExponentialMovingAverage (DT_FLOAT) [64]\r\nbn_4/bn_4/moments/moments_1/variance/ExponentialMovingAverage (DT_FLOAT) [64]`\r\n\r\ntherefore, when i load a CPU-trainned ckeckpoint in GPU, it turns out some errors like \"variables not found in file\" etc. Can someone solve this problem? thanks a lot.", "comments": ["@asimshankar it's not clear to me whether this behavior is working as intended (you can't read a CPU-trained checkpoint on a GPU), or whether this is supposed to work. Can you please comment?", "The checkpoint should certainly be loadable. The provided snippet isn't sufficient to help reproduce the problem. @DarcyCode could you please fill in the complete issue template, including instructions to reproduce the problem?\r\n\r\nSimply using the `batch_norm` function you've provided above and with the following snippet, I do not see any difference in variables names with or without GPU:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef batch_norm(x, beta, gamma, train_phase, scope='bn', decay=0.9, eps=1e-5):\r\n    with tf.variable_scope(scope):        \r\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\r\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\r\n\r\n        def mean_var_with_update():\r\n            ema_apply_op = ema.apply([batch_mean, batch_var])\r\n            with tf.control_dependencies([ema_apply_op]):\r\n                return tf.identity(batch_mean), tf.identity(batch_var)\r\n\r\n        mean, var = tf.cond(train_phase, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\r\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\r\n    return normed\r\n\r\ndef build_graph():\r\n  x = tf.ones([1, 1, 1])\r\n  train_phase = tf.constant(True)\r\n  batch_norm(x, 1., 1., train_phase)\r\n  print(tf.global_variables())\r\n\r\nwith tf.Graph().as_default():\r\n  with tf.device('/device:CPU:0'):\r\n    build_graph()\r\n\r\nprint('-----')\r\n\r\nwith tf.Graph().as_default():\r\n  with tf.device('/device:GPU:0'):\r\n    build_graph()\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@asimshankar Thanks for answering my questions. I define a CNN with BN on a computer(no-GPU) with CPU-version tensorflow r1.3. Code is below:\r\n``` python\r\nimport tensorflow as tf\r\n\r\nX = tf.placeholder(tf.float32, [None, LENGTH],name='X')\r\nY = tf.placeholder(tf.float32, [None, MAX_CAPTCHA*CHAR_SET_LEN],name='Y')\r\nkeep_prob = tf.placeholder(tf.float32,name='keep_prob') # dropout\r\ntrain_phase = tf.placeholder(tf.bool, name='train_phase')\r\n \r\ndef batch_norm(x, beta, gamma, train_phase, scope='bn', decay=0.9, eps=1e-5):\r\n    with tf.variable_scope(scope):\r\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\r\n        ema = tf.train.ExponentialMovingAverage(decay=decay) \r\n        def mean_var_with_update():\r\n            ema_apply_op = ema.apply([batch_mean, batch_var])\r\n            with tf.control_dependencies([ema_apply_op]):\r\n                return tf.identity(batch_mean), tf.identity(batch_var)\r\n        mean, var = tf.cond(train_phase, mean_var_with_update, lambda: (ema.average(batch_mean), ema.average(batch_var)))\r\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\r\n    return normed\r\n \r\ndef crack_captcha_cnn(w_alpha=0.01, b_alpha=0.1):\r\n    x = tf.reshape(X, shape=[-1, IMAGE_HEIGHT, IMAGE_WIDTH, 1])\r\n    # 4 conv layer\r\n    w_c1 = tf.Variable(w_alpha*tf.random_normal([5, 5, 1, 32]))\r\n    b_c1 = tf.Variable(b_alpha*tf.random_normal([32]))\r\n    conv1 = tf.nn.bias_add(tf.nn.conv2d(x, w_c1, strides=[1, 1, 1, 1], padding='SAME'), b_c1)\r\n    conv1 = batch_norm(conv1, tf.constant(0.0, shape=[32]), tf.random_normal(shape=[32], mean=1.0, stddev=0.02), train_phase, scope='bn_1')\r\n    conv1 = tf.nn.relu(conv1)\r\n    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv1 = tf.nn.dropout(conv1, keep_prob)\r\n \r\n    w_c2 = tf.Variable(w_alpha*tf.random_normal([5, 5, 32, 64]))\r\n    b_c2 = tf.Variable(b_alpha*tf.random_normal([64]))\r\n    conv2 = tf.nn.bias_add(tf.nn.conv2d(conv1, w_c2, strides=[1, 1, 1, 1], padding='SAME'), b_c2)\r\n    conv2 = batch_norm(conv2, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_2')\r\n    conv2 = tf.nn.relu(conv2)\r\n    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv2 = tf.nn.dropout(conv2, keep_prob)\r\n \r\n    w_c3 = tf.Variable(w_alpha*tf.random_normal([3, 3, 64, 64]))\r\n    b_c3 = tf.Variable(b_alpha*tf.random_normal([64]))\r\n    conv3 = tf.nn.bias_add(tf.nn.conv2d(conv2, w_c3, strides=[1, 1, 1, 1], padding='SAME'), b_c3)\r\n    conv3 = batch_norm(conv3, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_3')\r\n    conv3 = tf.nn.relu(conv3)\r\n    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv3 = tf.nn.dropout(conv3, keep_prob)\r\n\r\n    w_c4 = tf.Variable(w_alpha*tf.random_normal([3, 3, 64, 64]))\r\n    b_c4 = tf.Variable(b_alpha*tf.random_normal([64]))\r\n    conv4 = tf.nn.bias_add(tf.nn.conv2d(conv3, w_c4, strides=[1, 1, 1, 1], padding='SAME'), b_c4)\r\n    conv4 = batch_norm(conv4, tf.constant(0.0, shape=[64]), tf.random_normal(shape=[64], mean=1.0, stddev=0.02), train_phase, scope='bn_4')\r\n    conv4 = tf.nn.relu(conv4)\r\n    conv4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\r\n    conv4 = tf.nn.dropout(conv4, keep_prob)\r\n     \r\n    # Fully connected layer\r\n    w_d = tf.Variable(w_alpha*tf.random_normal([int(LENGTH/4), 1024]))\r\n    b_d = tf.Variable(b_alpha*tf.random_normal([1024]))\r\n    dense = tf.reshape(conv4, [-1, w_d.get_shape().as_list()[0]])\r\n    dense = tf.nn.relu(tf.add(tf.matmul(dense, w_d), b_d))\r\n    dense = tf.nn.dropout(dense, keep_prob)\r\n \r\n    w_out = tf.Variable(w_alpha*tf.random_normal([1024, MAX_CAPTCHA*CHAR_SET_LEN]))\r\n    b_out = tf.Variable(b_alpha*tf.random_normal([MAX_CAPTCHA*CHAR_SET_LEN]))\r\n    out = tf.add(tf.matmul(dense, w_out), b_out, name='out')\r\n    return out\r\n\r\ndef train_crack_captcha_cnn():\r\n    output = crack_captcha_cnn()\r\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=Y))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\r\n \r\n    predict = tf.reshape(output, [-1, MAX_CAPTCHA, CHAR_SET_LEN],name='predict')\r\n    max_idx_p = tf.argmax(predict, 2)\r\n    max_idx_l = tf.argmax(tf.reshape(Y, [-1, MAX_CAPTCHA, CHAR_SET_LEN]), 2)\r\n    correct_pred = tf.equal(max_idx_p, max_idx_l)\r\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\r\n \r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for i in tf.global_variables():\r\n            print(i)\r\n        return\r\n```\r\nWhen print tf.global_variables(), it prints\r\n```<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\r\n<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/Squeeze/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/Squeeze/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/Squeeze_1/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref>\r\n<tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>\r\n<tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref>\r\n<tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref>\r\n<tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>\r\n<tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>\r\n```\r\n\r\nHowever, when i define the same net on a computer(a GPU) with GPU-version tensorflow r1.2, it prints:\r\n\r\n```\r\n<tf.Variable 'Variable:0' shape=(5, 5, 1, 32) dtype=float32_ref>\r\n<tf.Variable 'Variable_1:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'bn_1/bn_1/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(32,) dtype=float32_ref>\r\n<tf.Variable 'Variable_2:0' shape=(5, 5, 32, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_3:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_2/bn_2/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_4:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_5:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_3/bn_3/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_6:0' shape=(3, 3, 64, 64) dtype=float32_ref>\r\n<tf.Variable 'Variable_7:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/moments_1/mean/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'bn_4/bn_4/moments/moments_1/variance/ExponentialMovingAverage:0' shape=(64,) dtype=float32_ref>\r\n<tf.Variable 'Variable_8:0' shape=(3200, 1024) dtype=float32_ref>\r\n<tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>\r\n<tf.Variable 'Variable_10:0' shape=(1024, 200) dtype=float32_ref>\r\n<tf.Variable 'Variable_11:0' shape=(200,) dtype=float32_ref>\r\n```\r\nHow could I solve this problem? Thanks again!", "@DarcyCode : The snippet you provided above doesn't seem to be self-contained, so I'm not sure if there are other things at play here (because of code not included in the snippet).\r\n\r\nCould you provide a small, self-contained snippet that reproduces the problem?", "All code is pasted above. I think the problem lies in function batch_norm(). It is saved differently in different machine. Or my computer is too old for tensorflow....", "Automatically closing due to lack of recent activity (and because it doesn't look much like a bug). Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 13459, "title": "remove warning for forward decl", "body": "replace `struct` with `class` to remove warning.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13458, "title": "How to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)", "body": "I want to limit the total memory of each GPU in mnist,\r\nhttps://www.tensorflow.org/tutorials/using_gpu\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\r\nsession = tf.Session(config=config, ...)\r\n\r\nand I added the above code to the mnis.py\r\nhttps://github.com/tensorflow/models/tree/master/official/mnist\r\n\r\nhere is the modified code in mnis.py :\r\ndef main(unused_argv):\r\n\r\n\r\n  config = tf.ConfigProto()\r\n  config.gpu_options.per_process_gpu_memory_fraction = 0.4\r\n\r\n  mnist_classifier = tf.estimator.Estimator(\r\n      model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\r\n\r\n\r\n\r\nbut I get the below error:\r\n\r\n\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"mnist.py\", line 231, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"mnist.py\", line 206, in main\r\n    model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,config=config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 142, in __init__\r\n    config)\r\nValueError: config must be an instance of RunConfig, but provided gpu_options {\r\n  per_process_gpu_memory_fraction: 0.4\r\n}\r\n.\r\n\r\n\r\nMy question is :\r\nHow to use config.gpu_options.per_process_gpu_memory_fraction in tf.estimator.Estimator(config=config)", "comments": ["`tf.estimator.Estimator` [expects a `RunConfig`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#class_estimator) object as its argument. [`RunConfig`](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig) in-turn contains the `ConfigProto` as `session_config`. So you could do something like the following:\r\n\r\n```python\r\nmnist_classifier = tf.estimator.Estimator(\r\n  model_fn=mnist_model_fn, \r\n  model_dir=FLAGS.model_dir,\r\n  config=tf.contrib.learn.RunConfig(session_config=config))\r\n```\r\n\r\nSee docs for [`tf.contrib.learn.RunConfig`](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig) for details.\r\n\r\nThough, I believe this complexity is in the process of being reduced.\r\n@ispirmustafa : Does one have to use `tf.contrib.learn.RunConfig` to create an estimator with a specific `ConfigProto` as mentioned above? Or is there an alternative?\r\n\r\nHope that helps.\r\n", "it works.\r\nthanks for your great help.", "Yes, RunConfig is the way to add change arguments such as session_config, saving intervals, ...", "@asimshankar what is the difference between the contrib and the Estimator version of RunConfig ? You link to one, but give an example that is using the other. Should they be considered equivalent ?\r\n\r\n", "@cipri-tom please use tf.estimator.* version. We'll deprecate contrib version of runconfig in coming releases. ", "@asimshankar it works.\r\nthank you very much."]}, {"number": 13457, "title": "Two issues on tf.nn.ctc_loss", "body": "Environments: tf version 1.3, CPU version; python 3.5/3.6; Win10/Ubuntu 16.04.\r\n\r\nTo begin with, we start from code:\r\n\r\nimport tensorflow as tf\r\nnum_classes, batch_size, seq_len = 3, 1, 2\r\nlabels = tf.SparseTensor(indices=[[0,0]], values=[0], dense_shape=[1,1])\r\ninputs = tf.zeros([seq_len, batch_size, num_classes])\r\nloss = tf.nn.ctc_loss(labels, inputs, [seq_len])\r\nprint(tf.InteractiveSession().run(loss))\r\n\r\ntf.nn.ctc_loss behaves as expected, and print the correct answer: 1.09861231\r\n\r\nIssue one: How to calculate the ctc loss of a sequence with all blanks? The tf.nn.ctc_loss API requires that values < num_labels, so we have no way to achieve it? If I do change the values in the above example to num_classes - 1 (the reserved blank ID), tf.nn.ctc_loss has no complain, and returns the wrong answer: 0.81093025! The correct answer is 2*log(3). The code to reproduce issue one is as below:\r\n\r\nimport tensorflow as tf\r\nnum_classes, batch_size, seq_len = 3, 1, 2\r\nlabels = tf.SparseTensor(indices=[[0,0]], values=[2], dense_shape=[1,1])\r\ninputs = tf.zeros([seq_len, batch_size, num_classes])\r\nloss = tf.nn.ctc_loss(labels, inputs, [seq_len])\r\nprint(tf.InteractiveSession().run(loss))\r\n\r\nIssue two: Let's change the sequence length to 1 as below\r\n\r\nimport tensorflow as tf\r\nnum_classes, batch_size, seq_len = 3, 1, 1\r\nlabels = tf.SparseTensor(indices=[[0,0]], values=[2], dense_shape=[1,1])\r\ninputs = tf.zeros([seq_len, batch_size, num_classes])\r\nloss = tf.nn.ctc_loss(labels, inputs, [seq_len])\r\nprint(tf.InteractiveSession().run(loss)) \r\n\r\nand run the code again. This code gives the correct answer, log(3), in Ubuntu, but crashes in Win10 with message: Kernel died, restarting.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I asked the same questions on stackoverflow (https://stackoverflow.com/questions/46652720/how-to-calculate-ctc-loss-of-a-sequence-with-all-blanks-using-tf-nn-ctc-loss), and get no answer after one month. My conclusion is that the current tf.nn.ctc_loss API cannot handle sequences with all blanks.", "Moving discussion to #14659 ."]}, {"number": 13456, "title": "Clean up our libcuda stub when building the GPU Docker container", "body": "A sensible suggestion by @flx42.", "comments": ["Jenkins, test this please.", "Jenkins, test this please"]}, {"number": 13455, "title": "Branch 170752644", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 13454, "title": "Simplify random selection of context words for word2vec example", "body": "Use random.sample to simplify random selection of context words", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "@ZxYuan Can you recreate / modify this PR to the `master` branch rather than the `r1.3` branch? ", "@frankchn recreated PR #13531 "]}, {"number": 13453, "title": "sampled version of sparse_softmax_cross_entropy_with_logits", "body": "Following from the already implemented `sampled_softmax_loss`, this is a very similar implementation that performs sampling for `sparse_softmax_cross_entropy_with_logits`. ", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@ebrevdo WDYT? Should we do this in contrib first?", "Hi folks, thank you for looking at this request. In regards to the test failures:\r\n\r\n- **Linux CPU Tests:** `api_compatibility_test`\r\nI take it this is to be expected given that the API would change. If these changes are accepted, how do I go about updating the goldens?\r\n\r\n- **Linux CPU Tests (Python 3):** ` do_virtualenv_pip_test`\r\nNot sure about this one, but I can investigate tomorrow.", "@frankchn I am new to this. Is there something I should do to have the `stat: awaiting response` label removed? Thanks!", "Jenkins, test this please.", "@ebrevdo PTAL again. Thanks!", "(Removing API review label as this is in `contrib`)", "Thanks @TTrapper ! Let's see what @ebrevdo has to say.", "If the special case code is only used by the sampled function, perhaps it's better to keep it in that function only?", "Thanks @ebrevdo. I believe it's general enough to be useful to others. I'm also weary of having code elsewhere that assumes `out_logits == concat[true_logits, sampled_logits]`. If that ever changes, then it is nice if changes to the `out_labels` are all in the same place as well. Let me know if you agree.", "Hi @drpngx, @ebrevdo. Just checking in on my above comment. Thanks!", "@TTrapper I think the recommendation was to move to contrib. That will still be available to everyone.", "@drpngx Thanks, I think @ebrevdo 's concern was that the proposed `sampled_sparse_softmax_loss` was the only place where the special case logic for getting true indices was used. I have now removed that special case logic from `_compute_sampled_logits` and added the specific use case directly to `sampled_sparse_softmax_loss`.", "I see.\r\n\r\nJenkins, test this please."]}, {"number": 13452, "title": "Error Raised contradicting to the documentation", "body": "Hi,\r\n\r\nThis [Line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py#L35-L36) says the variable can be left empty, but when done raises this error, `TypeError: print_tensors_in_checkpoint_file() takes exactly 3 arguments (2 given)`", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\nIn particular, it will be helpful if you can provide details on what exactly you tried and what the full error trace is. From the little error text provided, it seems this is the Python interpreter saying that you did not provide all 3 arguments to the function. If you do not want to provide the `tensors` argument, indicate so by provided `None`, for example: `print_tensors_in_checkpoint_file(filename, None, False)`.\r\n\r\nHope that helps."]}, {"number": 13451, "title": "Add support for CUBLAS_TENSOR_OP_MATH in fp16 GEMM", "body": "- Applies to matrix multiplications with fp16 input/output.\r\n  Computations will fall back to pseudo-fp16 if tensor op math is\r\n  disabled or not supported.\r\n- Enabled by default, but can be disabled by setting the environment\r\n  variable TF_DISABLE_TENSOR_OP_MATH=1.", "comments": ["Can one of the admins verify this patch?", "For review by @zheng-xq. ", "Jenkins, test this please.", "Jenkins, test this please. (running again in case this was infra)", "Just pushed.", "Done.", "@zheng-xq PTAL", "Ping @zheng-xq. Are there additional changes needed?", "This looks okay for now. Let's merge it in and improve it later. Thanks for the contribution!", "LGTM", "Jenkins, test this please."]}, {"number": 13450, "title": "Updating the install sources file with a supported configs table", "body": "This is an attempt to fix b/28192343. It will also help with issues like https://github.com/tensorflow/tensorflow/issues/13377. I'm completely open to all criticism with moving it somewhere else and formatting. ", "comments": ["Jenkins, test this please."]}, {"number": 13449, "title": "[FeatureRequest] TFDBG change default limit depth for recursive checking tensors", "body": "Hi,\r\n\r\nRecently, I was debugging with TFDBG for a neural network training. It is pretty useful, however, when I want to check some ill-conditioned tensors and their parent, by default TFDBG give me its parents up to a recursive level of 20, i.e. limit depth=20 by default. I could solve the problem by putting a \"-d 1\" to check its immediate parent each time but it is quite annoying to input this every time.\r\n\r\nWould you consider adding a feature in this TFDBG allowing us to set a default value for recursive retrieval of a tensor's input nodes?\r\n\r\nMy TensorFlow version is\r\n```('v1.3.0-rc2-20-g0787eee', '1.3.0')```\r\n\r\nHere is an example of tracing a node's inputs when running the TFDBG currently \r\n\r\n\r\n``` \r\nInputs to node \"node name\" (Depth limit = 20, controlled input included )\r\n...\r\n... some detail information\r\n...\r\n```\r\nfor command  ```>tfdbg li -c -r \"node name\" ```\r\n\r\n", "comments": ["Thanks for the feature request @kcyu1993 . I'll try to get that implemented at HEAD this week."]}, {"number": 13448, "title": "Building for tensorflow.dll failed because architectures < sm35 are not supported", "body": "Hi,\r\nI am trying to compile the dynamic library tensorflow.dll with cmake for windows 10 and GPU enabled. However, the library tf_core_gpu_kernels failed because of the following error:\r\n\"Unable to compile CudaAtomicAdd for complex64  architectures < sm35 are not supported\"\r\n\r\nI am building for a device with a card graphic Gforce 1080Ti (compute 61). So my configuration in Visual Studio is:\r\nCuda C/C++-> Device -> Code generation: compute_61, sm_61. But, I am still having the error.\r\nIs it related with any other configuration?. It would be great if someone could help with this issue.\r\n\r\nThanks in advance.\r\n\r\n\r\n ", "comments": ["Although I added the codepath for other compute values over the weekend (just git pull and you'll see the new fixed code), and it should compile for you now, it appears that the check isn't quite right in windows.  @mrry any reason why [this check](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/cuda_kernel_helper.h#L440) would fail in windows?\r\n\r\nThe reason I ask, is there are other functions in there, like ldg, which should be properly calling the __ldg intrinsic for a speedup on SM35+, but if those are falling back to the SM30 versions then the windows build could be unintentionally running slower GPU kernels.", "[This line](http://google3/third_party/tensorflow/contrib/cmake/CMakeLists.txt?l=227&rcl=170746452) controls how `nvcc` builds on Windows. It looks like we build for `sm_30`, `sm_35` and `sm_52`. Is it possible that `__CUDA_ARCH__` takes the lowest of these? (I'm not particularly familiar with `nvcc` options, so you might need to find someone else to take a look....)", "Hi,\r\nThank you for your help.\r\n\r\nI have rebased (\"git pull\") and I have rebuilt again, but I am afraid I have more errors than before.\r\nAlso, this time I have the error below\r\n/MatrixBase.h(472): fatal error C1060: insufficient heap space in compiler\r\n\r\nI have faced this last error before,  when I was building the tensorflow library with GPU, however it works fine building it for CPU.\r\nI am not sure if the issue is related with cmake or GPU. But, it seems some people have been able to build the library for GPU. Do you have any suggestion. I would be appreciated any help.\r\n\r\nNote:\r\nmrry, The link you added it doesn\u00b4t worked. Could it be possible to added here ?\r\n\r\nThanks in advance.", "Hi again,\r\n\r\nI have change my cudnn for the one recommended (5.1) and swigwin-3.0.10. Also, I have generated the project again after I rebased. It is building with no errors yet. So, I will let you know if it works.\r\n\r\nThanks again.", "I build the project and I don\u00b4t have the previous error anymore. However, the tensorflow.dll wasn\u00b4t generated because of the following errors:\r\n\r\nLine 44287: 147>  C:/Demos/TF_DLL_GPU/tensorflow/tensorflow/core/kernels/topk_op_gpu.cu.cc(550): note: vea la referencia a la creaci\u00f3n de instancias de plantilla de funci\u00f3n 'cudaError tensorflow::impl::LaunchTopKKernel<const T>(const cudaStream_t &,int,const T *,int,int,int,bool,T *,int *)' que se est\u00e1 compilando\r\n\tLine 49887: 151>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward sin resolver al que se hace referencia en la funci\u00f3n \"private: bool __cdecl perftools::gputools::cuda::CudnnSupport::DoFusedConvolveImpl<signed char,float,float,5,4>(class perftools::gputools::Stream *,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::FilterDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,class perftools::gputools::dnn::ConvolutionDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,enum perftools::gputools::dnn::ActivationMode,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> *,class perftools::gputools::ScratchAllocator *,class perftools::gputools::dnn:...\r\n\tLine 49888: 152>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward sin resolver al que se hace referencia en la funci\u00f3n \"private: bool __cdecl perftools::gputools::cuda::CudnnSupport::DoFusedConvolveImpl<signed char,float,float,5,4>(class perftools::gputools::Stream *,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::FilterDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,class perftools::gputools::dnn::ConvolutionDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,enum perftools::gputools::dnn::ActivationMode,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> *,class perftools::gputools::ScratchAllocator *,class perftools::gputools::dnn:...\r\n\tLine 49889: 150>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward sin resolver al que se hace referencia en la funci\u00f3n \"private: bool __cdecl perftools::gputools::cuda::CudnnSupport::DoFusedConvolveImpl<signed char,float,float,5,4>(class perftools::gputools::Stream *,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::FilterDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,class perftools::gputools::dnn::ConvolutionDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,enum perftools::gputools::dnn::ActivationMode,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> *,class perftools::gputools::ScratchAllocator *,class perftools::gputools::dnn:...\r\n\tLine 49890: 151>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 sin resolver al que se hace referencia en la funci\u00f3n \"public: __cdecl perftools::gputools::cuda::CudnnRnnDescriptor::CudnnRnnDescriptor(class perftools::gputools::cuda::CUDAExecutor *,struct cudnnContext *,int,int,int,enum cudnnRNNInputMode_t,enum cudnnDirectionMode_t,enum cudnnRNNMode_t,enum cudnnDataType_t,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (??0CudnnRnnDescriptor@cuda@gputools@perftools@@QEAA@PEAVCUDAExecutor@123@PEAUcudnnContext@@HHHW4cudnnRNNInputMode_t@@W4cudnnDirectionMode_t@@W4cudnnRNNMode_t@@W4cudnnDataType_t@@M_KPEAVScratchAllocator@23@@Z)\r\n\tLine 49891: 152>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 sin resolver al que se hace referencia en la funci\u00f3n \"public: __cdecl perftools::gputools::cuda::CudnnRnnDescriptor::CudnnRnnDescriptor(class perftools::gputools::cuda::CUDAExecutor *,struct cudnnContext *,int,int,int,enum cudnnRNNInputMode_t,enum cudnnDirectionMode_t,enum cudnnRNNMode_t,enum cudnnDataType_t,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (??0CudnnRnnDescriptor@cuda@gputools@perftools@@QEAA@PEAVCUDAExecutor@123@PEAUcudnnContext@@HHHW4cudnnRNNInputMode_t@@W4cudnnDirectionMode_t@@W4cudnnRNNMode_t@@W4cudnnDataType_t@@M_KPEAVScratchAllocator@23@@Z)\r\n\tLine 49892: 150>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 sin resolver al que se hace referencia en la funci\u00f3n \"public: __cdecl perftools::gputools::cuda::CudnnRnnDescriptor::CudnnRnnDescriptor(class perftools::gputools::cuda::CUDAExecutor *,struct cudnnContext *,int,int,int,enum cudnnRNNInputMode_t,enum cudnnDirectionMode_t,enum cudnnRNNMode_t,enum cudnnDataType_t,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (??0CudnnRnnDescriptor@cuda@gputools@perftools@@QEAA@PEAVCUDAExecutor@123@PEAUcudnnContext@@HHHW4cudnnRNNInputMode_t@@W4cudnnDirectionMode_t@@W4cudnnRNNMode_t@@W4cudnnDataType_t@@M_KPEAVScratchAllocator@23@@Z)\r\n\tLine 49893: 150>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\summarize_graph.exe : fatal error LNK1120: 2 externos sin resolver\r\n\tLine 49894: 151>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\compare_graphs.exe : fatal error LNK1120: 2 externos sin resolver\r\n\tLine 49895: 152>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\benchmark_model.exe : fatal error LNK1120: 2 externos sin resolver\r\n\tLine 49896: 153>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward sin resolver al que se hace referencia en la funci\u00f3n \"private: bool __cdecl perftools::gputools::cuda::CudnnSupport::DoFusedConvolveImpl<signed char,float,float,5,4>(class perftools::gputools::Stream *,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::FilterDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,class perftools::gputools::dnn::ConvolutionDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,enum perftools::gputools::dnn::ActivationMode,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> *,class perftools::gputools::ScratchAllocator *,class perftools::gputools::dnn:...\r\n\tLine 49897: 153>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 sin resolver al que se hace referencia en la funci\u00f3n \"public: __cdecl perftools::gputools::cuda::CudnnRnnDescriptor::CudnnRnnDescriptor(class perftools::gputools::cuda::CUDAExecutor *,struct cudnnContext *,int,int,int,enum cudnnRNNInputMode_t,enum cudnnDirectionMode_t,enum cudnnRNNMode_t,enum cudnnDataType_t,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (??0CudnnRnnDescriptor@cuda@gputools@perftools@@QEAA@PEAVCUDAExecutor@123@PEAUcudnnContext@@HHHW4cudnnRNNInputMode_t@@W4cudnnDirectionMode_t@@W4cudnnRNNMode_t@@W4cudnnDataType_t@@M_KPEAVScratchAllocator@23@@Z)\r\n\tLine 49898: 153>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\transform_graph.exe : fatal error LNK1120: 2 externos sin resolver\r\n\tLine 49928: 156>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward sin resolver al que se hace referencia en la funci\u00f3n \"private: bool __cdecl perftools::gputools::cuda::CudnnSupport::DoFusedConvolveImpl<signed char,float,float,5,4>(class perftools::gputools::Stream *,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::FilterDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,class perftools::gputools::dnn::ConvolutionDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,enum perftools::gputools::dnn::ActivationMode,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> *,class perftools::gputools::ScratchAllocator *,class perftools::gputools::dnn:...\r\n\tLine 49929: 156>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 sin resolver al que se hace referencia en la funci\u00f3n \"public: __cdecl perftools::gputools::cuda::CudnnRnnDescriptor::CudnnRnnDescriptor(class perftools::gputools::cuda::CUDAExecutor *,struct cudnnContext *,int,int,int,enum cudnnRNNInputMode_t,enum cudnnDirectionMode_t,enum cudnnRNNMode_t,enum cudnnDataType_t,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (??0CudnnRnnDescriptor@cuda@gputools@perftools@@QEAA@PEAVCUDAExecutor@123@PEAUcudnnContext@@HHHW4cudnnRNNInputMode_t@@W4cudnnDirectionMode_t@@W4cudnnRNNMode_t@@W4cudnnDataType_t@@M_KPEAVScratchAllocator@23@@Z)\r\n\tLine 49930: 156>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\tf_label_image_example.exe : fatal error LNK1120: 2 externos sin resolver\r\n\tLine 49934: 157>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward sin resolver al que se hace referencia en la funci\u00f3n \"private: bool __cdecl perftools::gputools::cuda::CudnnSupport::DoFusedConvolveImpl<signed char,float,float,5,4>(class perftools::gputools::Stream *,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::FilterDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,class perftools::gputools::dnn::ConvolutionDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,enum perftools::gputools::dnn::ActivationMode,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> *,class perftools::gputools::ScratchAllocator *,class perftools::gputools::dnn:...\r\n\tLine 49935: 157>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 sin resolver al que se hace referencia en la funci\u00f3n \"public: __cdecl perftools::gputools::cuda::CudnnRnnDescriptor::CudnnRnnDescriptor(class perftools::gputools::cuda::CUDAExecutor *,struct cudnnContext *,int,int,int,enum cudnnRNNInputMode_t,enum cudnnDirectionMode_t,enum cudnnRNNMode_t,enum cudnnDataType_t,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (??0CudnnRnnDescriptor@cuda@gputools@perftools@@QEAA@PEAVCUDAExecutor@123@PEAUcudnnContext@@HHHW4cudnnRNNInputMode_t@@W4cudnnDirectionMode_t@@W4cudnnRNNMode_t@@W4cudnnDataType_t@@M_KPEAVScratchAllocator@23@@Z)\r\n\tLine 49936: 157>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\tf_tutorials_example_trainer.exe : fatal error LNK1120: 2 externos sin resolver\r\n\tLine 50019: 161>c_api.cc.obj : error LNK2019: s\u00edmbolo externo \"class tensorflow::Status __cdecl tensorflow::ops::BuildWhileLoop(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::function<class tensorflow::Status __cdecl(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class tensorflow::Output *)> const &,class std::function<class tensorflow::Status __cdecl(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *)> const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *,bool,class tensorflow::Output *)\" (?BuildWhileLoop@ops@t...\r\n\tLine 50020: 161>while_gradients.obj : error LNK2001: s\u00edmbolo externo \"class tensorflow::Status __cdecl tensorflow::ops::BuildWhileLoop(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::function<class tensorflow::Status __cdecl(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class tensorflow::Output *)> const &,class std::function<class tensorflow::Status __cdecl(class tensorflow::Scope const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *)> const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > *,bool,class tensorflow::Output *)\" (?BuildWhileLoo...\r\n\tLine 50021: 161>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward sin resolver al que se hace referencia en la funci\u00f3n \"private: bool __cdecl perftools::gputools::cuda::CudnnSupport::DoFusedConvolveImpl<signed char,float,float,5,4>(class perftools::gputools::Stream *,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::FilterDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,class perftools::gputools::dnn::ConvolutionDescriptor const &,class perftools::gputools::DeviceMemory<signed char> const &,float,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,enum perftools::gputools::dnn::ActivationMode,class perftools::gputools::dnn::BatchDescriptor const &,class perftools::gputools::DeviceMemory<signed char> *,class perftools::gputools::ScratchAllocator *,class perftools::gputools::dnn:...\r\n\tLine 50022: 161>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 sin resolver al que se hace referencia en la funci\u00f3n \"public: __cdecl perftools::gputools::cuda::CudnnRnnDescriptor::CudnnRnnDescriptor(class perftools::gputools::cuda::CUDAExecutor *,struct cudnnContext *,int,int,int,enum cudnnRNNInputMode_t,enum cudnnDirectionMode_t,enum cudnnRNNMode_t,enum cudnnDataType_t,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (??0CudnnRnnDescriptor@cuda@gputools@perftools@@QEAA@PEAVCUDAExecutor@123@PEAUcudnnContext@@HHHW4cudnnRNNInputMode_t@@W4cudnnDirectionMode_t@@W4cudnnRNNMode_t@@W4cudnnDataType_t@@M_KPEAVScratchAllocator@23@@Z)\r\n\tLine 50023: 161>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\tensorflow.dll : fatal error LNK1120: 3 externos sin resolver\r\n\r\n\r\nAny help with this issue would be very much appreciated.\r\n", "I have checked the linker and everything looks fine, apart from:\r\n wsock32.lib\r\nws2_32.lib\r\nshlwapi.lib\r\nadvapi32.lib\r\n\r\nI got rid of them, because they haven\u00b4t been created, and tensorflow is not complaing about them.\r\n\r\nThe issue see in:\r\n1>c_api.cc.obj : error LNK2019: tensorflow::ops::BuildWhileLoop ...\r\n1>while_gradients.obj : error LNK2001: tensorflow::ops::BuildWhileLoop ...\r\n1>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnConvolutionBiasActivationForward ...\r\n1>cuda_dnn.obj : error LNK2019: s\u00edmbolo externo cudnnSetRNNDescriptor_v6 ... \r\n1>C:\\Demos\\TF_DLL_GPU\\tensorflow\\tensorflow\\contrib\\cmake\\build\\Release\\tensorflow.dll : fatal error LNK1120: 3 externos sin resolver\r\n\r\nThe the libraries tf_c.lib, tf_cc.lib and tf_stream_executer have been created correctly, so not sure why tensorflow is complaying about c_api.cc , while_gradients and cuda_dnn.\r\n\r\nAny ideas how to fix this issue?\r\n\r\nThanks for your time. \r\n", "I think there are some object files missing from the `tensorflow_static` target in [`tf_shared_lib.cmake`](https://github.com/tensorflow/tensorflow/blob/635196732151e6d8638c189c52f4c4336ede81b6/tensorflow/contrib/cmake/tf_shared_lib.cmake). Can you try adding the following line [here](https://github.com/tensorflow/tensorflow/blob/635196732151e6d8638c189c52f4c4336ede81b6/tensorflow/contrib/cmake/tf_shared_lib.cmake#L38) and [here](https://github.com/tensorflow/tensorflow/blob/635196732151e6d8638c189c52f4c4336ede81b6/tensorflow/contrib/cmake/tf_shared_lib.cmake#L75)?\r\n\r\n```\r\n    $<TARGET_OBJECTS:tf_cc_while_loop>\r\n```\r\n\r\nThe symbols `cudnnConvolutionBiasActivationForward` and `cudnnSetRNNDescriptor_v6` seem to be part of cuDNN 6. I'm not sure where you picked up the recommendation to use cuDNN 5.1, but I think that that might be incorrect. Upgrading to cuDNN 6 should fix this part of the build.", "Hi mrry,\r\nThank you very much for your help. I have rebuilt and I have been able to generate the tensorflow.dll.\r\nI have run the model resnet vs inception (trained with my own dataset) in a machine with card graphic GForce 1080Ti. The times measured to detect objects are below:\r\n- CPU ~ 27.8 sec\r\n- GPU ~ 6.79 sec\r\n\r\nI can see with GPU enabled it works better, but still is not enough. Could you tell me please if there is any make it faster?\r\n\r\nThank you again for all help provided.\r\n", "Glad to hear that it's building now! Since the original build problem is fixed, I'm going to close this issue.\r\n\r\nIt's difficult to guess reasons for poor performance, but I'd recommend that you take a look at the [performance guide](https://www.tensorflow.org/performance/performance_guide), and open a new issue if none of these suggestions help.", "Ok. Thank you very much.\n\nOn 4 October 2017 at 19:35, Derek Murray <notifications@github.com> wrote:\n\n> Glad to hear that it's building now! Since the original build problem is\n> fixed, I'm going to close this issue.\n>\n> It's difficult to guess reasons for poor performance, but I'd recommend\n> that you take a look at the performance guide\n> <https://www.tensorflow.org/performance/performance_guide>, and open a\n> new issue if none of these suggestions help.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13448#issuecomment-334229642>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADrfhOVQIPqqoYmdzhTvNpB1xS6fJ6Uaks5so8HpgaJpZM4Pqy-9>\n> .\n>\n", "@helen-medina what are the dependencies you use to run tensorflow-GPU models in visual studio?\r\nit looks like I am missing something:\r\n```\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)\" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)\r\n1>tf_core_kernels.lib(fft_ops.obj) : error LNK2001: unresolved external symbol \"public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)\" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)\r\n1>tf_core_kernels.lib(conv_ops_3d.obj) : error LNK2001: unresolved external symbol \"public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)\" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)\r\n1>tf_core_kernels.lib(conv_ops.obj) : error LNK2001: unresolved external symbol \"public: virtual __cdecl perftools::gputools::ScratchAllocator::~ScratchAllocator(void)\" (??1ScratchAllocator@gputools@perftools@@UEAA@XZ)\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnForward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> *,bool,class perftools::gputools::ScratchAllocator *,class perftools::gputools::ScratchAllocator *)\" (?ThenRnnForward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@M@23@AEBVRnnStateTensorDescriptor@523@23221PEAV723@3434_NPEAVScratchAllocator@23@6@Z)\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnForward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> *,bool,class perftools::gputools::ScratchAllocator *,class perftools::gputools::ScratchAllocator *)\" (?ThenRnnForward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@N@23@AEBVRnnStateTensorDescriptor@523@23221PEAV723@3434_NPEAVScratchAllocator@23@6@Z)\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnBackward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> const &,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<float> *,class perftools::gputools::DeviceMemory<unsigned char> *,class perftools::gputools::ScratchAllocator *)\" (?ThenRnnBackward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@M@23@AEBVRnnStateTensorDescriptor@523@2322123232222PEAV723@444PEAV?$DeviceMemory@E@23@PEAVScratchAllocator@23@@Z)\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: class perftools::gputools::Stream & __cdecl perftools::gputools::Stream::ThenRnnBackward(class perftools::gputools::dnn::RnnDescriptor const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnSequenceTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::dnn::RnnStateTensorDescriptor const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> const &,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<double> *,class perftools::gputools::DeviceMemory<unsigned char> *,class perftools::gputools::ScratchAllocator *)\" (?ThenRnnBackward@Stream@gputools@perftools@@QEAAAEAV123@AEBVRnnDescriptor@dnn@23@AEBVRnnSequenceTensorDescriptor@523@AEBV?$DeviceMemory@N@23@AEBVRnnStateTensorDescriptor@523@2322123232222PEAV723@444PEAV?$DeviceMemory@E@23@PEAVScratchAllocator@23@@Z)\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: class perftools::gputools::port::StatusOr<class std::unique_ptr<class perftools::gputools::dnn::RnnDescriptor,struct std::default_delete<class perftools::gputools::dnn::RnnDescriptor> > > __cdecl perftools::gputools::StreamExecutor::createRnnDescriptor(int,int,int,enum perftools::gputools::dnn::RnnInputMode,enum perftools::gputools::dnn::RnnDirectionMode,enum perftools::gputools::dnn::RnnMode,enum perftools::gputools::dnn::DataType,float,unsigned __int64,class perftools::gputools::ScratchAllocator *)\" (?createRnnDescriptor@StreamExecutor@gputools@perftools@@QEAA?AV?$StatusOr@V?$unique_ptr@VRnnDescriptor@dnn@gputools@perftools@@U?$default_delete@VRnnDescriptor@dnn@gputools@perftools@@@std@@@std@@@port@23@HHHW4RnnInputMode@dnn@23@W4RnnDirectionMode@723@W4RnnMode@723@W4DataType@723@M_KPEAVScratchAllocator@23@@Z)\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: class perftools::gputools::port::StatusOr<class std::unique_ptr<class perftools::gputools::dnn::RnnSequenceTensorDescriptor,struct std::default_delete<class perftools::gputools::dnn::RnnSequenceTensorDescriptor> > > __cdecl perftools::gputools::StreamExecutor::createRnnSequenceTensorDescriptor(int,int,int,enum perftools::gputools::dnn::DataType)\" (?createRnnSequenceTensorDescriptor@StreamExecutor@gputools@perftools@@QEAA?AV?$StatusOr@V?$unique_ptr@VRnnSequenceTensorDescriptor@dnn@gputools@perftools@@U?$default_delete@VRnnSequenceTensorDescriptor@dnn@gputools@perftools@@@std@@@std@@@port@23@HHHW4DataType@dnn@23@@Z)\r\n1>tf_core_kernels.lib(cudnn_rnn_ops.cc.obj) : error LNK2001: unresolved external symbol \"public: class perftools::gputools::port::StatusOr<class std::unique_ptr<class perftools::gputools::dnn::RnnStateTensorDescriptor,struct std::default_delete<class perftools::gputools::dnn::RnnStateTensorDescriptor> > > __cdecl perftools::gputools::StreamExecutor::createRnnStateTensorDescriptor(int,int,int,enum perftools::gputools::dnn::DataType)\" (?createRnnStateTensorDescriptor@StreamExecutor@gputools@perftools@@QEAA?AV?$StatusOr@V?$unique_ptr@VRnnStateTensorDescriptor@dnn@gputools@perftools@@U?$default_delete@VRnnStateTensorDescriptor@dnn@gputools@perftools@@@std@@@std@@@port@23@HHHW4DataType@dnn@23@@Z)\r\n1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_avgpooling_op_gpu.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync\r\n1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_argmax_op_gpu.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync\r\n1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync\r\n1>tf_core_gpu_kernels.lib(tf_core_gpu_kernels_generated_concat_lib_gpu_impl.cu.cc.obj) : error LNK2001: unresolved external symbol cudaMemcpyAsync\r\n```\r\n"]}, {"number": 13447, "title": "Fix up compatibility issues with bazel >= 0.6.0", "body": "In this pull request, set() is simply replaced with depset() since set() is obsolete in bazel >= 0.6.0.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Sorry. This pull request is duplicated with (#13443). Please close it soon. Thanks.", "Thanks for checking @jerry73204 The other PR passes the CLA checks, so do you mind if I close your pull request and merge the other one?", "That's fine."]}, {"number": 13446, "title": "Dataset: \"Shuffle\" doesn't work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:  source\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: python 3.5\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.5.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Aug 25 10:00:00 2017 (1503655200)\r\nBuild timestamp: 1503655200\r\nBuild timestamp as int: 1503655200\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n== tensorflow import ============================================      \r\ntf.VERSION = 1.3.0                                                                    \r\ntf.GIT_VERSION = b'v1.3.0-rc1-2408-ge9d5ee1'                             \r\ntf.COMPILER_VERSION = b'v1.3.0-rc1-2408-ge9d5ee1'   \r\n\r\n\r\n### Describe the problem\r\n\"Shuffle\" from Dataset doesn't work. \r\n\r\n### Source code / logs\r\nThe following files can be used to reproduce problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.set_random_seed(123)\r\n\r\ndef input_pipeline(filenames, batch_size):\r\n    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\r\n    dataset = (tf.contrib.data.TextLineDataset(filenames)\r\n               .map(lambda line: tf.decode_csv(\r\n                    line, record_defaults=[['1'], ['1'], ['1']], field_delim='-'))\r\n               .shuffle(buffer_size=10)  # Equivalent to min_after_dequeue=10.\r\n               .batch(batch_size))\r\n\r\n    # Return an *initializable* iterator over the dataset, which will allow us to\r\n    # re-initialize it at the beginning of each epoch.\r\n    return dataset.make_initializable_iterator()\r\n\r\nfilenames=['1.txt']\r\nbatch_size = 3\r\nnum_epochs = 3\r\niterator = input_pipeline(filenames, batch_size)\r\n\r\n# `a1`, `a2`, and `a3` represent the next element to be retrieved from the iterator.\r\na1, a2, a3 = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    for epoch in range(num_epochs):\r\n        # Resets the iterator at the beginning of an epoch.\r\n        sess.run(iterator.initializer)\r\n        print('epoch:%d' % (epoch))\r\n        try:\r\n            while True:\r\n                a, b, c = sess.run([a1, a2, a3])\r\n                print(a, b, c)\r\n        except tf.errors.OutOfRangeError:\r\n            # This will be raised when you reach the end of an epoch (i.e. the\r\n            # iterator has no more elements).\r\n            pass\r\n```\r\n\r\nThe corresponding file: \"1.txt\"\r\n\r\n```\r\n1,2-3,4-A\r\n7,8-9,10-B\r\n12,13-14,15-C\r\n17,18-19,20-D\r\n22,23-24,25-E\r\n27,28-29,30-F\r\n32,33-34,35-G\r\n37,38-39,40-H\r\n```\r\n\r\nThe output:\r\n```\r\n2017-10-02 15:06:43.523320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-02 15:06:43.523788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.44GiB\r\n2017-10-02 15:06:43.523800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nepoch:0\r\n[b'27,28' b'17,18' b'7,8'] [b'29,30' b'19,20' b'9,10'] [b'F' b'D' b'B']\r\n[b'32,33' b'22,23' b'12,13'] [b'34,35' b'24,25' b'14,15'] [b'G' b'E' b'C']\r\n[b'1,2' b'37,38'] [b'3,4' b'39,40'] [b'A' b'H']\r\nepoch:1\r\n[b'27,28' b'17,18' b'7,8'] [b'29,30' b'19,20' b'9,10'] [b'F' b'D' b'B']\r\n[b'32,33' b'22,23' b'12,13'] [b'34,35' b'24,25' b'14,15'] [b'G' b'E' b'C']\r\n[b'1,2' b'37,38'] [b'3,4' b'39,40'] [b'A' b'H']\r\nepoch:2\r\n[b'27,28' b'17,18' b'7,8'] [b'29,30' b'19,20' b'9,10'] [b'F' b'D' b'B']\r\n[b'32,33' b'22,23' b'12,13'] [b'34,35' b'24,25' b'14,15'] [b'G' b'E' b'C']\r\n[b'1,2' b'37,38'] [b'3,4' b'39,40'] [b'A' b'H']\r\n```\r\n", "comments": ["Can you describe the problem? From the shuffled output, it does seem to be working as intended.\r\n\r\n(Perhaps you need to pass a different seed for each epoch? When you call  `tf.set_random_seed(123)` at the beginning of your program, that will make the output of the shuffle deterministic.)", "thanks a lot for your instant response!\r\nI expect that the order of the shuffled output in each epoch would be different. When a seed is set by ```tf.se_random_seed```, the training process is supposed to be reproducible. \r\nBut the current shuffle function always produces the same order of the shuffled outputs for each epoch. ", "Right, it is trying very hard to be reproducible, but there's no indication in the code of when it should reshuffle. (Consider that if it did reshuffle each iteration, you wouldn't be able to reproduce the same sequence within the same session.) If you're using `tf.set_random_seed()`, you need to do a little more work to get different orders on each epoch. The easiest workaround would be to specify an additional seed as a placeholder, which you then feed with a different value on each :\r\n\r\n```python\r\ndef input_pipeline(filenames, batch_size, seed=None):\r\n    # Define a `tf.contrib.data.Dataset` for iterating over one epoch of the data.\r\n    dataset = (tf.contrib.data.TextLineDataset(filenames)\r\n               .map(lambda line: tf.decode_csv(\r\n                    line, record_defaults=[['1'], ['1'], ['1']], field_delim='-'))\r\n               .shuffle(buffer_size=10, seed=seed)  # Equivalent to min_after_dequeue=10.\r\n               .batch(batch_size))\r\n\r\n    # Return an *initializable* iterator over the dataset, which will allow us to\r\n    # re-initialize it at the beginning of each epoch.\r\n    return dataset.make_initializable_iterator()\r\n\r\nfilenames=['1.txt']\r\nbatch_size = 3\r\nnum_epochs = 3\r\nseed = tf.placeholder(tf.int64, shape=())\r\niterator = input_pipeline(filenames, batch_size, seed)\r\n\r\n# `a1`, `a2`, and `a3` represent the next element to be retrieved from the iterator.\r\na1, a2, a3 = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    for epoch in range(num_epochs):\r\n        # Resets the iterator at the beginning of an epoch.\r\n        sess.run(iterator.initializer, feed_dict={seed: epoch})\r\n        print('epoch:%d' % (epoch))\r\n        try:\r\n            while True:\r\n                a, b, c = sess.run([a1, a2, a3])\r\n                print(a, b, c)\r\n        except tf.errors.OutOfRangeError:\r\n            # This will be raised when you reach the end of an epoch (i.e. the\r\n            # iterator has no more elements).\r\n            pass\r\n```", "I appreciate the solution you suggested. Do you think, is there any more elegant way? ", "I think the workaround is about as elegant as it will get with the current API. (One could imagine adding something like per-`run()` seeds and threading those through the random operations, but that would be a big change!)"]}, {"number": 13445, "title": "Update seq2seq.py: To avoid error thread.lock", "body": "Issue: Can't pickle thread.lock object \r\nhttps://github.com/tensorflow/tensorflow/issues/11157\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "cla signed", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "@mandar1010 any update?", "Closing as stalled. Please reopen with a test."]}, {"number": 13444, "title": "Feature request: Allow for custom hooks in Slim's `evaluate_once` to support TFDBG", "body": "### System information\r\n- **TensorFlow version**: `v1.3.0-rc2-20-g0787eee`\r\n\r\n### Describe the problem\r\nThe evaluation functions provided by `contrib.slim.python.slim` are wrappers around methods from `contrib.training.python.training`. `contrib.slim.python.slim.evaluation_loop` provides argument `hooks` which attaches custom hooks to the evaluation loop. This can be used to hook the TFDBG debugger into evaluation. `contrib.slim.python.slim.evaluation_once` however does not provide the argument, even though the underlying `contrib.training.python.training.evaluate_once` does support the argument. The code to extend the hooks with custom hooks is already there in `contrib.slim.python.slim.evaluation_loop` but that fix somehow was not applied to `contrib.slim.python.slim.evaluation_once`.\r\n\r\nThe request is to implement the addition of custom hooks to `contrib.slim.python.slim.evaluation_once` so that TFDBG can be used with this method. This is a really easy fix that adds a lot of functionality.\r\n", "comments": ["@caisq Sorry to bother you, but could you give an indication of when you could get to this? It would really help me to have this in TensorFlow. :)", "@rjbruin Sorry about the delay.\r\n\r\n@sguada @rjbruin's FR makes sense to me. The change should be small and straightforward. I'll send you a change list soon."]}, {"number": 13443, "title": "Rename set to depset", "body": "`set` is deprecated in Bazel starting from 0.6.0.\r\n\r\nFixes #13377 \r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13442, "title": "Tensorflow-GPU installation Error ", "body": "GPU:  1050Ti\r\nOS: windows 10\r\n--------------------------\r\nCuda tool kit: version 9.0\r\ncuDNN: version 8.0\r\ntensorflow-gpu: verision 1.3\r\n--------------------------\r\nafter installing CUDA tool kit v.90 checked 'bandwith test'\r\nC:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe \r\n-------------------------\r\n%Path%\r\nPath variables \r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp\r\nC:\\Program Files\\cuda\\bin\r\n--------------------------\r\n\r\ncreated a virtual environment 'tensorflow-gpu' \r\nconda install tensorflow-gpu \r\n\r\nto check the tensorflow installation\r\nimport tensorflow as tf\r\n\r\nto check the devices available \r\n>>from tensorflow.python.client import device_lib\r\n>> device_lib.list_local_devices()\r\n\r\nit show only CPU, not the GPU stack \r\n\r\n------------------------------\r\nplease help me out where i am getting wrong.\r\n\r\nin the virtual environment 'tensorflow-gpu' some times when i load \r\n>> import tensorflow as tf\r\nit thoughs dll missing error\r\nPlease help me out almost spent 3-4 days in it. \r\n\r\nC:\\WINDOWS\\system32>python\r\nPython 3.6.1 |Anaconda custom (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Anaconda 3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n", "comments": ["It seems that you're using CUDA 9.0, however TensorFlow 1.3 requires CUDA 8.0 (see https://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support).\r\n\r\nCould you try with CUDA 8?", "I have installed CUDA 8.0 \r\nthe thing is, I want to run the code on GPU version of tensorflow  \r\n\r\ninstall GPU version of tensorflow & then run this code on GPU\r\npip install tensorflow-gpu \r\n\r\nCreate a Graph using GPU\r\nwith tf.device('/gpu:0'):\r\n    x = tf.constant([10.0, 20.0, 30.0, 40.0, 50.0, 60.0], shape=[2, 3], name ='a')\r\n    y = tf.constant([10.0, 20.0, 30.0, 40.0, 50.0, 60.0], shape=[3, 2], name ='b')\r\n    \r\nd = tf.matmul(a,b)\r\n\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\nprint(sess.run(d))\r\n\r\nThrough exception,\r\n\r\nthen I realize let me look for GPU version if listed. \r\n\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndef get_available_gpus():\r\n    local_device_protos = device_lib.list_local_devices()\r\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\r\nget_available_gpus()\r\n\r\n\r\n![tensorflow-cpu](https://user-images.githubusercontent.com/9411418/31124608-d0967188-a862-11e7-9cbf-588a49a01e11.PNG)\r\n", "Even after installing tensorflow-gpu verision on an conda env its not listing me GPU Device.\r\n\r\nis that an issue with path settings for CUDA or cuDNN?", "`# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Mon Oct  2 00:46:38 2017\r\n\r\n@author: ayubq\r\n\"\"\"\r\n\r\n\r\nimport ctypes\r\nimport imp\r\nimport sys\r\n\r\ndef main():\r\n  try:\r\n    import tensorflow as tf\r\n    print(\"TensorFlow successfully installed.\")\r\n    if tf.test.is_built_with_cuda():\r\n      print(\"The installed version of TensorFlow includes GPU support.\")\r\n    else:\r\n      print(\"The installed version of TensorFlow does not include GPU support.\")\r\n    sys.exit(0)\r\n  except ImportError:\r\n    print(\"ERROR: Failed to import the TensorFlow module.\")\r\n\r\n  candidate_explanation = False\r\n\r\n  python_version = sys.version_info.major, sys.version_info.minor\r\n  print(\"\\n- Python version is %d.%d.\" % python_version)\r\n  if not (python_version == (3, 5) or python_version == (3, 6)):\r\n    candidate_explanation = True\r\n    print(\"- The official distribution of TensorFlow for Windows requires \"\r\n          \"Python version 3.5 or 3.6.\")\r\n  \r\n  try:\r\n    _, pathname, _ = imp.find_module(\"tensorflow\")\r\n    print(\"\\n- TensorFlow is installed at: %s\" % pathname)\r\n  except ImportError:\r\n    candidate_explanation = False\r\n    print(\"\"\"\r\n- No module named TensorFlow is installed in this Python environment. You may\r\n  install it using the command `pip install tensorflow`.\"\"\")\r\n\r\n  try:\r\n    msvcp140 = ctypes.WinDLL(\"msvcp140.dll\")\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'msvcp140.dll'. TensorFlow requires that this DLL be\r\n  installed in a directory that is named in your %PATH% environment\r\n  variable. You may install this DLL by downloading Microsoft Visual\r\n  C++ 2015 Redistributable Update 3 from this URL:\r\n  https://www.microsoft.com/en-us/download/details.aspx?id=53587\"\"\")\r\n\r\n  try:\r\n    cudart64_80 = ctypes.WinDLL(\"cudart64_80.dll\")\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Download and install CUDA 8.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\"\"\")\r\n\r\n  try:\r\n    nvcuda = ctypes.WinDLL(\"nvcuda.dll\")\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that\r\n  this DLL be installed in a directory that is named in your %PATH%\r\n  environment variable. Typically it is installed in 'C:\\Windows\\System32'.\r\n  If it is not present, ensure that you have a CUDA-capable GPU with the\r\n  correct driver installed.\"\"\")\r\n\r\n  cudnn5_found = False\r\n  try:\r\n    cudnn5 = ctypes.WinDLL(\"cudnn64_5.dll\")\r\n    cudnn5_found = True\r\n  except OSError:\r\n    candidate_explanation = True\r\n    print(\"\"\"\r\n- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Note that installing cuDNN is a\r\n  separate step from installing CUDA, and it is often found in a\r\n  different directory from the CUDA DLLs. You may install the\r\n  necessary DLL by downloading cuDNN 5.1 from this URL:\r\n  https://developer.nvidia.com/cudnn\"\"\")\r\n\r\n  cudnn6_found = False\r\n  try:\r\n    cudnn = ctypes.WinDLL(\"cudnn64_6.dll\")\r\n    cudnn6_found = True\r\n  except OSError:\r\n    candidate_explanation = True\r\n\r\n  if not cudnn5_found or not cudnn6_found:\r\n    print()\r\n    if not cudnn5_found and not cudnn6_found:\r\n      print(\"- Could not find cuDNN.\")\r\n    elif not cudnn5_found:\r\n      print(\"- Could not find cuDNN 5.1.\")\r\n    else:\r\n      print(\"- Could not find cuDNN 6.\")\r\n      print(\"\"\"\r\n  The GPU version of TensorFlow requires that the correct cuDNN DLL be installed\r\n  in a directory that is named in your %PATH% environment variable. Note that\r\n  installing cuDNN is a separate step from installing CUDA, and it is often\r\n  found in a different directory from the CUDA DLLs. The correct version of\r\n  cuDNN depends on your version of TensorFlow:\r\n  \r\n  * TensorFlow 1.2.1 or earlier requires cuDNN 5.1. ('cudnn64_5.dll')\r\n  * TensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')\r\n    \r\n  You may install the necessary DLL by downloading cuDNN from this URL:\r\n  https://developer.nvidia.com/cudnn\"\"\")\r\n    \r\n  if not candidate_explanation:\r\n    print(\"\"\"\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\"\"\")\r\n\r\n  sys.exit(-1)\r\n\r\nif __name__ == \"__main__\":\r\n  main()`\r\n\r\n\r\n\r\n--------\r\n\r\nTensorFlow successfully installed.\r\nThe installed version of TensorFlow does not include GPU support.\r\nAn exception has occurred, use %tb to see the full traceback.", "@AyubQuadri based on your script, it seems that your `import tensorflow as tf` isn't picking up the GPU version that you installed.\r\n\r\nI'm not a python expert, but to figure out what file you're importing, you can do something like this:\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.__file__\r\n\r\nI'm expecting you'll be picking up some other non-gpu install.", "I am closing this issue, as i have configured the Tensorflow-GPU version of it.", "none of this makes any sense", "@AyubQuadri  Hi could some one help me out , i have installed cuda 9.1 with cuddn 7.1 and all the dlls are present in the folder yet when i try to use tensorflow gpu it give me \"ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\" but when i download the cuda 9.0 it again gives the same error saying cudart64_80.dll is not available . \r\n\r\nI have tired installing all the version of the tensorflow and cuda yet i get error i have checked the path variable to not sure why i am getting this error on windows 10", "Could not find 'nvcuda.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\\Windows\\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.\r\n\r\n\r\nhow can i solve that????", "I have a same error @kaushikrudraksh, can you find a solution?"]}]