[{"number": 26413, "title": "AttributeError: module 'tensorflow._api.v1.lite' has no attribute 'Optimize' version 1.13", "body": "following the Post-training quantization guide,\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nAttributeError: module 'tensorflow._api.v1.lite' has no attribute 'Optimize'\r\n```\r\ni want to convert the mnist model, full code here\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ntf.__version__\r\n\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_labels = train_labels[:1000]\r\ntest_labels = test_labels[:1000]\r\n\r\ntrain_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\r\ntest_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\r\n\r\n# Returns a short sequential model\r\ndef create_model():\r\n  model = tf.keras.models.Sequential([\r\n    keras.layers.Dense(512, activation=tf.keras.activations.relu, input_shape=(784,)),\r\n    keras.layers.Dropout(0.2),\r\n    keras.layers.Dense(10, activation=tf.keras.activations.softmax)\r\n  ])\r\n  \r\n  model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                metrics=['accuracy'])\r\n  \r\n  return model\r\n\r\n\r\n# Create a basic model instance\r\nmodel = create_model()\r\nmodel.fit(train_images, train_labels, epochs=5)\r\nmodel.summary()\r\nmodel.save('my_mnist_v2.h5')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\"my_mnist_v2.h5\")\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\n```\r\ntf version 1.13", "comments": ["Hello @jyang356 ,\r\nIam facing the same error message. Were you able to resolve it? kindly let me know the solution.", "Same problem, following\r\n", "same problem, following\r\n", "I wasnt able to solve the above problem, but proceeded with following code to save quantized model:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\ntflite_model = converter.convert()\r\nopen(\"model.tflite\", \"wb\").write(tflite_model)\r\n#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\nconverter.post_training_quantize=True\r\ntflite_quantized_model=converter.convert()\r\nopen(\"quantized_model.tflite\", \"wb\").write(tflite_quantized_model)\r\n```\r\n", "thank you, your solution is valid also with  : \r\n`converter = tf.lite.TFLiteConverter.from_keras_model_file(saved_model_dir)` instead of `converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)`", "@sunejas I did that but now I get this error:\r\n\r\n`\"invalid shape '{1}'.\".format(_tensor_name(tensor), shape_list))\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.`", "Thank you @sunejas, I refered to the code you post. Just add one line and make it.\r\n\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\n   \r\n    # added code to convert model to a quantized tflite format.\r\n    converter.post_training_quantize = True\r\n\r\n    tflite_model = converter.convert()\r\n    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n    print(\"convert model to tflite format done.\")\r\n```", "I tried this method of calling post_training_quantize=True.\r\nBut when i call interpreter, the inference code of Tensorflow and Tensorflow lite does not match.\r\nIs there any setting to be done in Interepreter ?\r\n\r\n```\r\ngraph_def_file = \"C:/Users/GF63/Downloads/tf2_tflite_issues/mobilenet_v1_1.0_224/frozen_graph.pb\"\r\ninput_arrays = [\"input\"]\r\noutput_arrays = [\"MobilenetV1/Predictions/Softmax\"]\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\n\r\nconverter.post_training_quantize=True\r\n\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\ninterpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninput_shape = input_details[0]['shape']\r\n\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\nThis output_data compared against Tensorflow inference are different.\r\nAny ideas ?\r\n", "> I tried this method of calling post_training_quantize=True.\r\n> But when i call interpreter, the inference code of Tensorflow and Tensorflow lite does not match.\r\n> Is there any setting to be done in Interepreter ?\r\n> \r\n> ```\r\n> graph_def_file = \"C:/Users/GF63/Downloads/tf2_tflite_issues/mobilenet_v1_1.0_224/frozen_graph.pb\"\r\n> input_arrays = [\"input\"]\r\n> output_arrays = [\"MobilenetV1/Predictions/Softmax\"]\r\n> converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\n> \r\n> converter.post_training_quantize=True\r\n> \r\n> tflite_model = converter.convert()\r\n> open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n> interpreter = tf.lite.Interpreter(model_path=\"converted_model.tflite\")\r\n> interpreter.allocate_tensors()\r\n> input_details = interpreter.get_input_details()\r\n> output_details = interpreter.get_output_details()\r\n> input_shape = input_details[0]['shape']\r\n> \r\n> input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n> interpreter.set_tensor(input_details[0]['index'], input_data)\r\n> interpreter.invoke()\r\n> output_data = interpreter.get_tensor(output_details[0]['index'])\r\n> print(output_data)\r\n> ```\r\n> \r\n> This output_data compared against Tensorflow inference are different.\r\n> Any ideas ?\r\n\r\nI found you use a random sample to forward, maybe you need feed  the same data to both format of models.", "Yes, I am feeding same data to both. I believe random does not matter , as long as i am feeding same data to tflite and tensorflow.\r\n", "@jyang356 's original error was because the Optimizers attribute was added in 1.13.1, which I think was available at the time of OP's post.", "Yes, i checked the older version and found one that did the same thing but with a different name/function. \r\n\r\nSorry, i was very busy at that time and completely forgot it. i can't find my source code now, probably deleted, because it's just the \"hello world\" code from git.", "So is this bug from 1.13 ?\r\nor is there any correction/workaround possibility to code ?\r\n", "> So is this bug from 1.13 ?\r\n> or is there any correction/workaround possibility to code ?\r\n\r\nHow different are the outputs? I'm getting the same difference between pb and quantized in both 1.13.0rc2 and 1.14.0, so it doesn't seem to be 1.13-specific. Some difference is to be expected. I think the 'scale' attribute in the UINT8 tensors (the convolution weights) more or less give an idea about the magnitude of the error you can expect."]}, {"number": 26412, "title": "Segmentation fault when loading SSD-MobileNetV1 into TF Lite calibration tool", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nArch Linux (4.20.1-arch1-1-ARCH SMP PREEMPT)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\nTensorFlow Git commit ID: 12606ff846566dd842444f27f7fed4f911a58580\r\nTensorFlow Models Git commit ID: [`338088d`](https://github.com/tensorflow/models/commit/338088df1782c0bf2dc105170730f2230edf2df7)\r\n- Python version:\r\n3.7.2\r\n- Bazel version (if compiling from source):\r\n0.21.0-1\r\n- GCC/Compiler version (if compiling from source):\r\n8.2.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nIn Python, calling `tensorflow.lite.python.optimize.calibrator.Calibrator(float32_tflite_ssd_mobilenetv1_model_contents)` leads to a segmentation-fault.\r\n\r\n**Describe the expected behavior**\r\nNo segmentation-fault when instantiating `Calibrator` with the floating-point SSD-MobileNetV1 TF Lite model.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```bash\r\n$ wget  http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz\r\n$ tar vxf ssd_mobilenet_v1_coco_2018_01_28.tar.gz\r\n$ cd ssd_mobilenet_v1_coco_2018_01_28\r\n<Configure paths in pipeline.config to point to COCO dataset and label-map (see tensorflow/models/research/object_detection/data/mscoco_label_map.pbtxt)>\r\n$ python <Path to workspace>/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py --pipeline_config_path=pipeline.config --trained_checkpoint_prefix=model.ckpt --output_directory=. --add_postprocessing\r\n$ toco --graph_def_file=tflite_graph.pb --output_file=float32.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=FLOAT --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false --allow_custom_ops\r\n```\r\n```python\r\n# load_model_into_calibrator.py\r\nfrom tensorflow.lite.python.optimize import calibrator\r\n\r\nwith open('float32.tflite', 'rb') as f:\r\n  quantizer = calibrator.Calibrator(f.read())\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`gdb` full backtrace attached.\r\n[gdb-backtrace.txt](https://github.com/tensorflow/tensorflow/files/2938277/gdb-backtrace.txt)\r\n\r\n", "comments": ["The custom post-processor appended to the said TF Lite model's outputs is causing the segmentation fault. Excluding the detector's post-processor as demonstrated below works. Note that the post-processor operator is associated with the [`detection_postprocess` TF Lite kernel](https://github.com/tensorflow/tensorflow/blob/12606ff846566dd842444f27f7fed4f911a58580/tensorflow/lite/kernels/detection_postprocess.cc).\r\n\r\n```bash\r\n$ toco --graph_def_file=tflite_graph.pb --output_file=float32.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='raw_outputs/box_encodings','raw_outputs/class_predictions','anchors' --inference_type=FLOAT --mean_values=128 --std_dev_values=128 --change_concat_input_ranges=false\r\n$ python load_model_into_calibrator.py\r\n```", "@ajarthurs: Right now the calibrator does not support custom ops, which detection_postprocess is. However it should not crash, but rather give an error message.\r\n@jianlijianli : This is related to custom op support, for this bug we at least need to report an error.", "> @ajarthurs: Right now the calibrator does not support custom ops, which detection_postprocess is. However it should not crash, but rather give an error message.\r\n> @jianlijianli : This is related to custom op support, for this bug we at least need to report an error.\r\n\r\nI can confirm the crash. We are running into the same on DeepSpeech model, and the crash happens at https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc#L30\r\n\r\nThis is because https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/lite/tools/optimize/calibration/logging_op_resolver.cc#L27-L28 `base_resolver` cannot find the op because `base_resolver.FindOp(op_and_version.first, op_and_version.second);` is indeed `TfLiteRegistration* BuiltinOpResolver::FindOp(tflite::BuiltinOperator op, int version)`.\r\n\r\nA quick fix is checking for `base_registration` against `nullptr` and continue: it will `RuntimeError` at least.", "Indeed, custom ops are taken care of since r1.15, so maybe this should just be closed: that ship has sailed ... https://github.com/tensorflow/tensorflow/commit/c8fd0ce78e236355f435dec1e9fc6cf11cdeed36", "Awhile ago, I settled for removing `detection_postprocess` from the rest of the graph. Wasn't sure if you guys were resolving the seg-fault with an error message, but it sounds like this issue has gone stale. So, I agree; let's close this."]}, {"number": 26411, "title": "GPU placement of tf.nn.conv2d during tf.data.Dataset.map call causes UnimplementedError (NHWC)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux 4.20.13-arch1-1-ARCH #1 SMP PREEMPT Wed Feb 27 19:10:28 UTC 2019 x86_64 GNU/Linux`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): `community python-tensorflow-opt-cuda`\r\n- TensorFlow version (use command below): `1.13.1`\r\n- Python version: `3.7.2`\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: `10.1 / 7.5`\r\n- GPU model and memory: `Geforce GTX 1080 Ti 11GB`\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nEvaluation of `tf.nn.conv2d` in a `tf.data.Dataset.map` call fails during `Session.run()` call of `tf.data.iterator.get_next()` with error:\r\n```python\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Generic conv implementation only supports NHWC tensor format for now.\r\n```\r\nSee the full error log and the attached code below for the full example.\r\n\r\nIt seems that the graph is successfully created, but the evaluation of the `tf.nn.conv2d` call (which is implicitly placed on the GPU) is not possible.\r\nThis is somehow related to the fact that this `tf.nn.conv2d` call is wrapped in a `tf.data.Dataset.map` call.\r\nSetting `data_format=\"NHWC\"` or `use_cudnn_on_gpu=False` procudes the same error.\r\n\r\n**Describe the expected behavior**\r\n`tf.nn.conv2d` should be evaluated, which is done iff the convolution is explicitly placed on the CPU (e.g. `with tf.device('/cpu:0'):` or `CUDA_VISIBLE_DEVICES=\"\"`).\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass IteratorInitializerHook(tf.train.SessionRunHook):\r\n    \"\"\"Hook to initialise data iterator after Session is created.\"\"\"\r\n\r\n    def __init__(self, func=None):\r\n        super(IteratorInitializerHook, self).__init__()\r\n        self.iterator_initializer_func = func\r\n\r\n    def after_create_session(self, session, coord):\r\n        \"\"\"Initialise the iterator after the session has been created.\"\"\"\r\n        self.iterator_initializer_func(session)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    def apply_kernel(tensor, kernel=tf.random_normal([3, 3])):\r\n        t = tf.expand_dims(tensor, 0)\r\n        t = tf.expand_dims(t, -1)\r\n        k = tf.expand_dims(kernel, -1)\r\n        k = tf.expand_dims(k, -1)\r\n\r\n        # TODO the following line fails during Session.run\r\n        #  call of tf.data.Iterator.get_next() (last line in this file)\r\n        tf_conv = tf.nn.conv2d(t, k, [1, 1, 1, 1], \"SAME\")\r\n        return tf.squeeze(tf_conv)\r\n\r\n    def do_some_things(x, y):\r\n        x = apply_kernel(x)\r\n        return x, y\r\n\r\n    n, image_shape = 100, [256, 256]\r\n\r\n    ds = tf.data.Dataset.from_tensor_slices((\r\n        tf.random_uniform([n] + image_shape), tf.random_uniform([n])\r\n    ))\r\n    ds = ds.map(do_some_things)\r\n    iterator = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)\r\n    data = iterator.get_next()\r\n    ds_init_op = iterator.make_initializer(ds)\r\n\r\n    with tf.train.SingularMonitoredSession(\r\n            hooks=[IteratorInitializerHook(lambda s: s.run(ds_init_op))],\r\n            config=tf.ConfigProto(log_device_placement=True)\r\n    ) as sess:\r\n        _ = sess.run(data)\r\n```\r\n\r\n**Other info / logs**\r\nHere is the full error log, including device placements.\r\n[tf-conv-NHWC-issue.log](https://github.com/tensorflow/tensorflow/files/2938294/tf-conv-NHWC-issue.log)\r\n\r\nHere is a zipped version of the python example\r\n[tf-conv-NHWC-issue.py.zip](https://github.com/tensorflow/tensorflow/files/2938289/tf-conv-NHWC-issue.py.zip)\r\n\r\n\r\nNot sure if this is important, but it seems that all tensorflow ops from the function which is called by `tf.data.Dataset.map` are not showing up in the device placement logs.", "comments": ["I think the problem here is a bug in an optimization pass that's causing the `Dataset.map()`, which should be placed on **CPU**, to be rewritten to use a version of `tf.nn.conv2d()` that is only available on GPU.\r\n\r\nCan you try disabling the layout optimizer as follows and let us know if this works?\r\n\r\n```python\r\n# ...\r\nconfig = tf.ConfigProto(log_device_placement=True)\r\nconfig.graph_options.rewrite_options.layout_optimizer = 2  # RewriterConfig.OFF\r\n\r\n    with tf.train.SingularMonitoredSession(\r\n            hooks=[IteratorInitializerHook(lambda s: s.run(ds_init_op))],\r\n            config=config\r\n    ) as sess:\r\n        _ = sess.run(data)\r\n```", "Thanks for the fast reply @mrry!\r\n\r\nIndeed, the snippet you posted works fine, without any errors!", "Added the original logfile and example script in the top post\r\n\r\n>Here is the full error log, including device placements.\r\n> [tf-conv-NHWC-issue.log](https://github.com/tensorflow/tensorflow/files/2938294/tf-conv-NHWC-issue.log)\r\n>\r\n> Here is a zipped version of the python example\r\n> [tf-conv-NHWC-issue.py.zip](https://github.com/tensorflow/tensorflow/files/2938289/tf-conv-NHWC-issue.py.zip)\r\n", "Thanks for confirming!\r\n\r\n@ezhulenev It looks like one of the function optimization passes in Grappler is applying the layout optimizer under the assumption that all devices will be available inside the function. Is there a way to inhibit this optimization, e.g. when the function is named in an attr that's passed to a CPU-only op? (At least for tf.data, there's another chance to run optimizers in the TFDataMetaOptimizer, /cc @rachellim.)", "I think it's the problem in layout optimizer, function body node does not have assigned devices, but layout optimizer \"approximates\" the placer with https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/layout_optimizer.cc#L574 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/virtual_placer.cc#L82.\r\n\r\nCurrent plan is to remove function optimization from Grappler in it's current form, this problem should go away after that. But that's at few weeks away.\r\n\r\n/cc @rmlarsen @andyly ", "FYI internal issue for that is b/126811947", "Is there anything I can do to help resolve this issue?", "Can you try to explicitly set device to CPU inside function body. I think that might work as well.", "Yes, when the device is explicitly set to `tf.device('/cpu:0')` everything works as expected.", "@mrry : is there any plan to have GPU support for `tf.data.Dataset.map`?\r\n\r\nAn example use case would be to extract frozen features from a pretrained network while loading images, then caching the dataset to avoid computing again those features.\r\n\r\nI've asked the same question [here](https://github.com/tensorflow/hub/issues/140#issuecomment-524255004), when I got this error trying to run a TF Hub module in `tf.data.Dataset.map`:\r\n>`NotFoundError: No registered 'MapDataset' OpKernel for GPU devices compatible with node {{node MapDataset}}`", "> Can you try to explicitly set device to CPU inside function body. I think that might work as well.\r\n\r\nThat's a quite precise solution which handles my problem right away. I have used tf.device(\"/cpu:0\") in the outer portions of the body but the layout optimization error was still given. But when placed in the body itself, the problem has got away.\r\n\r\nThanks...", "Setting to `tf.device('/cpu:0')` worked for me as well on 1.15.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "I think the issue is solved in 2.x, maybe @ezhulenev can confirm that internal issue b/126811947 is closed?", "I don't use this function in my tensorflow environment. You can confirm it as resolved.", "@sbrodehl , Yes the internal issue is closed, you can go ahead and close this issue, if you don't have any further questions. Thanks!", "Thank you very much for the confirmation!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26411\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26411\">No</a>\n"]}, {"number": 26410, "title": "Add Minor Versions for cuDNN & CUDA in Tested build configurations tables", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/install/source#tested_build_configurations\r\n\r\n\r\n**Describe the documentation issue**\r\nThe tested build configurations table does not supply adequate information for the user since it does not provide minor versions for the cuDNN and CUDA tested in the build.  Please add this information.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n", "comments": ["Tested config for TF 1.12 is `CUDA 9.0` and `cuDNN 7.2.1`\r\nhttps://github.com/tensorflow/tensorflow/blob/5b900cfe4b3b848f577315a0dde09a729f770e95/tensorflow/tools/docker/Dockerfile.devel-gpu#L16\r\nTF 1.12 and below versions are obsolete. `CUDA` and `cuDNN` minor versions are provided for TF 1.13.1 and above versions.\r\n\r\n"]}, {"number": 26405, "title": "[TF 2.0] summary API migration paths", "body": "This feature request tracks adding and improving migration paths in tf_upgrade_v2 to handle the various summary APIs.\r\n\r\nThe goal state is:\r\n1) 1.x tf.summary migrated to tf.compat.v1.summary by default (done)\r\n2) 1.x tf.summary migrated to tf.compat.v2.summary where possible to do correctly (not done, may not be feasible)\r\n3) 1.x tf.contrib.summary migrated to tf.compat.v2.summary with best-effort correctness (partly done)\r\n\r\nFor (2) it's not possible to do this correctly in all cases because the summary APIs depend on non-local context in different ways (e.g. file writer creation works differently) and return different results (1.x returns a Summary protocol buffer, 2.x a boolean). It might be worth seeing if we can add an option to the upgrade script to indicate a preference for the conservative approach (stick with tf.compat.v1) or the more aggressive upgrade (migrate to tf.compat.v2, but more likely to be broken).\r\n\r\nFor (3) we can't migrate to compat.v1 because contrib is not exposed in v1, so migrating to V2 is the only path forwards. Migration rules here are only partially implemented (for `scalar`, `audio`, `image`, and `histogram`). We still need rules for migrating the actual writer and recording control symbols.\r\n\r\nTracked internally at b/124529441.", "comments": ["@nfelt Can you please help me understanding this issue a little better. So, basically the main thing we need to do here is changing all occurrences of tf.summary with tf.compat.v2.summary and tf.contrib.summary migrated with tf.compat.v2.summary. Did I understanding this correctly?", "So this issue is for tracking making the migration tooling and docs better, not for actually documenting the migration paths in the issue itself.\r\n\r\nBut the short version is that if you're using tf.contrib.summary, the new TF 2.0 API is pretty similar except that recording is on by default so you don't need always_record_summaries() any more.  If you're using tf.summary, the calls to `scalar()`, `histogram()`, etc. look the same, but the file writer works quite differently.  There's a brief overview of the new API at the bottom of this page on effective TF 2.0: https://www.tensorflow.org/alpha/guide/effective_tf2#use_tfmetrics_to_aggregate_data_and_tfsummary_to_log_it\r\n\r\nAs I make the migration tooling better and add documentation, I'll update this issue.", "@nfelt Is there something that I can help with?", "This is essentially done aside from getting better docs, which https://github.com/tensorflow/tensorboard/pull/2514 should address.", "@nfelt Could you please let us know if we can close the issue with the merged [PR](https://github.com/tensorflow/tensorboard/pull/2514) ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "We can consider this closed."]}, {"number": 26404, "title": "Returned None Values when computing Gradients", "body": "**Describe the current behavior**\r\nI am using the **Resnet50** code maintained under **_Eager Execution_** as is. When computing gradients \r\n*grads = tape.gradient(loss, model.variables)\r\nI get NONE values return in grads. I traced down the issue and realized that those none values correspond to the BatchNormalization layers. The layers are set to trainable=True, so I am not quite sure why they do not train or why they return None values. \r\n\r\n******Obviously, model.variables != tape.variables**** \r\n\r\n**Code to reproduce the issue****\r\n\r\n_With tf.GradientTape() as tape:\r\n      prediction = model(x, training=True)\r\n      var_list = tape.watched.variables()\r\nloss = compute_loss(prediction, label, operation = 'L2')\r\ngrads = tape.gradient(loss, model.variables)\r\noptimizer.apply_gradients(zip(grads, model.variables), global_step = step))_\r\n\r\n\r\n**Could you please explain why am I getting those None values?**\r\n\r\n", "comments": ["You should use `model.trainable_variables` instead of `model.variables`.", "Thank you @ppwwyyxx , That did get rid of the NONEs. Does this mean BatchNormalization layers hold non-trainable variables? ", "> Does this mean BatchNormalization layers hold non-trainable variables?\r\n\r\nYes."]}, {"number": 26403, "title": "build error with OpenMPI", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux xxxx 3.10.0-862.6.3.el7.x86_64 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 0.13 (downloaded 6/Mar/2019)\r\n- Python version: 3.7.2\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source): 0.22.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5 \r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: NVIDIA V100, P100 both 16GB\r\n\r\n\r\n\r\n**Describe the problem**  Build fails with MPI support enabled\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n$ bazel build --config=opt --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n.....\r\n3 errors detected in the compilation of \"/tmp/tmpxft_000633d8_00000000-7_ring.cu.compute_60.cpp1.ii\".\r\nERROR: /N/dc2/projects/osg-storage/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: output 'tensorflow/contrib/mpi_collectives/_objs/python/ops/_mpi_ops_gpu/ring.cu.pic.o' was not created\r\nERROR: /N/dc2/projects/osg-storage/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nIt seems a patch for this was described here: https://github.com/tensorflow/tensorflow/issues/17437 about a year ago, it would be very useful if this could be included in the current release.", "comments": ["As in TF 2.0, all contrib projects including the MPI support will be removed from the TF core, which means this patch will not be merged here.\r\n\r\nMPI and MPI collectives will be migrated to [tensorflow/networking](https://github.com/tensorflow/networking). Although the migration process is still undergoing, we do not have sufficient support from the community to the MPI collectives. You are encouraged to help :)", "Thanks for the info, I found the \"enable MPI support\" configuration option suggestive. I'll remove it from my configuration files.", "@steige2 can you explain more detail for how you fix this error?", "I accepted the default to this configuration question:\r\nDo you wish to build TensorFlow with MPI support? [y/N]\r\nthat is, solved the problem by NOT building with MPI support."]}, {"number": 26402, "title": "Check instruction IsFusible() in gpu_fusible.cc", "body": "", "comments": ["Closing as changes included in PR 26277"]}, {"number": 26401, "title": "Tensor::FromProto should check the type of tensor", "body": "When deserializing a tensor from pb, there is a Tensor::FromProto function for us to use. However, this function do not do any checks on the type of the proto. This unexpected behaviour will cause core dump when the type of the proto and that of the tensor mismatch. Here is a toy example which will cause core dump when deserializing tensor proto.\r\n\r\n```\r\n// prepare tensor proto which have a string tensor\r\nTensorProto proto;\r\nTensor tensor(DT_STRING, {1});\r\ntensor.scalar<string>()() = \"foo\";\r\ntensor.AsProtoField(&tensorProto);\r\n// deserialze the tensor proto to a variant\r\nTensor newTensor(DT_VARIANT, {1});\r\nif (!newTensor.FromProto(tensorProto)) {\r\n  // FromProto is expected to failed\r\n  return false;\r\n}\r\n// the software will core on getting the FooVariant\r\nauto fooVariant = newTensor.scalar<Variant>()().get<FooVariant>();\r\n```\r\n\r\nThe code of  Tensor::FromProto\r\n```\r\nbool Tensor::FromProto(Allocator* a, const TensorProto& proto) {\r\n  CHECK_NOTNULL(a);\r\n  TensorBuffer* p = nullptr;\r\n  if (!TensorShape::IsValid(proto.tensor_shape())) return false;\r\n  if (proto.dtype() == DT_INVALID) return false;\r\n  TensorShape shape(proto.tensor_shape());\r\n  const int64 N = shape.num_elements();\r\n  if (N > 0 && proto.dtype()) {\r\n    bool dtype_error = false;\r\n    if (!proto.tensor_content().empty()) {\r\n      const auto& content = proto.tensor_content();\r\n      CASES_WITH_DEFAULT(proto.dtype(), p = Helper<T>::Decode(a, content, N),\r\n                         dtype_error = true, dtype_error = true);\r\n    } else {\r\n      CASES_WITH_DEFAULT(proto.dtype(), p = FromProtoField<T>(a, proto, N),\r\n                         dtype_error = true, dtype_error = true);\r\n    }\r\n    if (dtype_error || p == nullptr) return false;\r\n  }\r\n  shape_ = shape;\r\n  set_dtype(proto.dtype());\r\n  UnrefIfNonNull(buf_);\r\n  buf_ = p;\r\n  // TODO(misard) add tracking of which kernels and steps are calling\r\n  // FromProto.\r\n  if (buf_ != nullptr && buf_->data() != nullptr && LogMemory::IsEnabled()) {\r\n    LogMemory::RecordTensorAllocation(\"Unknown (from Proto)\",\r\n                                      LogMemory::UNKNOWN_STEP_ID, *this);\r\n  }\r\n  return true;\r\n}\r\n```", "comments": ["@cjqw ,\r\nHi, can you please check if the issue exists in latest version of the TF? Thanks!", "@cjqw,\r\n Can you please respond to the above comment? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26401\">No</a>\n"]}, {"number": 26400, "title": "fix tflite android demo: fix normalization for mobile-net", "body": "", "comments": []}, {"number": 26399, "title": "[tensorflow.org] Devsite Code Container not working properly", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 2.0\r\n- Doc Link: https://www.tensorflow.org/install\r\n\r\n\r\n**Describe the documentation issue**\r\n[Link](https://drive.google.com/file/d/10G-wS5A_QQ1ma7YRLdCpUVmist-J3t-N/view?usp=sharing)\r\nAs seen in the .gif file in the link uploaded, the code container for \"Download a Package\" section isn't working correctly. On decreasing window size, the code container initially gets cut into half without a scroll feature and then gets completely deleted from the webpage.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes", "comments": ["This appears to be fixed. Thanks"]}, {"number": 26398, "title": "Converting tensor2tensor transformer to tf-lite graph", "body": "I am trying to convert the frozen transformer model from tensor2tensor to a tf-lite graph:\r\n\r\n```\r\n$ tflite_convert --output_file=/tmp/tf-lite/mdl --graph_def_file=frozen.pb --input_arrays=wave_input --output_arrays=outputs\r\n```\r\n\r\nI am getting the following error \r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sfalk/miniconda3/envs/t2t/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n```\r\n\r\nI would seem that there are a lot of unsupported operations - does this cause the error? \r\n\r\nThe last line from the tflite_convert output says \r\n\r\n>  `F tensorflow/contrib/lite/toco/tooling_util.cc:968] Check failed: array->has_shape()`\r\n\r\nI don't know if anybody can make sense out of that ...\r\n\r\nIs it possible to fix this or is there simply no way to convert this graph at this point in time?\r\n\r\n<details><summary><b>Click to show full tflite_convert output</b></summary>\r\n<p>\r\n\r\n```\r\n2019-03-06 13:53:51.592675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: RandomStandardNormal\r\n2019-03-06 13:53:51.592791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SplitV\r\n2019-03-06 13:53:51.594222: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Cos\r\n2019-03-06 13:53:51.594254: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: RFFT\r\n2019-03-06 13:53:51.594260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: stft/rfft\r\n2019-03-06 13:53:51.594263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ComplexAbs\r\n2019-03-06 13:53:51.594283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LinSpace\r\n2019-03-06 13:53:51.594290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\r\n2019-03-06 13:53:51.594346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LinSpace\r\n2019-03-06 13:53:51.594352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\r\n2019-03-06 13:53:51.594399: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: SplitV\r\n2019-03-06 13:53:51.594531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\r\n2019-03-06 13:53:51.594564: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.596289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2019-03-06 13:53:51.596411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Cos\r\n2019-03-06 13:53:51.596466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Where\r\n2019-03-06 13:53:51.596473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: transformer_ext/body/parallel_0/body/encoder/pad_reduce/get_ids/Where\r\n2019-03-06 13:53:51.596591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.596714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.596835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.597110: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.597220: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.597444: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.597699: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.597780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd\r\n2019-03-06 13:53:51.597900: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.598021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.598154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.598540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.598656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.599083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.599551: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.599635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd\r\n2019-03-06 13:53:51.599755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.599878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.600066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.600402: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.600518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.600968: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.601443: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.601549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd\r\n2019-03-06 13:53:51.601745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.601951: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.602150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.602506: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.602647: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.603160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.603627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.603779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd\r\n2019-03-06 13:53:51.603972: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.604173: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.604374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.604724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.604852: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.605260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.605712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.605818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ScatterNd\r\n2019-03-06 13:53:51.605950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Cos\r\n2019-03-06 13:53:51.606029: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: MatrixBandPart\r\n2019-03-06 13:53:51.606257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.606486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.606714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.606942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.607165: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.607388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.607628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607659: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607666: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607672: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607707: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607747: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607766: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607802: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.607870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LoopCond\r\n2019-03-06 13:53:51.607876: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: transformer_ext/while/LoopCond\r\n2019-03-06 13:53:51.608650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608682: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608688: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608694: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608714: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608733: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608910: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.608919: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.609056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.609098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.609238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.609276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.609419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.609460: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.609746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.609797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.609872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.609893: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.610025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.610086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.610326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.610390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.610453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.610462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.610832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.610871: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.610896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.611271: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.611311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.611333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.611369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.611379: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.611499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.611536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.611675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.611712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.611849: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.611886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.612169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.612207: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.612257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.612266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.612391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.612526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.612746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.612783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.612833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.612842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.613232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.613295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.613330: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.613729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.613768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.613793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.613841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.613862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.613986: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.614023: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.614161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.614213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.614362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.614411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.614710: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.614774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.614847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.614868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.615001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.615051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.615280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.615343: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.615419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.615440: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.615813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.615863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.615899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616299: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ListDiff\r\n2019-03-06 13:53:51.616350: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616605: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616611: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616662: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.616682: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-03-06 13:53:51.617107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: IsFinite\r\n2019-03-06 13:53:51.617115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: transformer_ext/while/ReduceLogSumExp/IsFinite\r\n2019-03-06 13:53:51.617189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617227: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617267: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617323: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617329: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617363: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617369: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617374: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617446: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: GatherNd\r\n2019-03-06 13:53:51.617488: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\r\n2019-03-06 13:53:51.617494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\r\n2019-03-06 13:53:51.617499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\r\n2019-03-06 13:53:51.956887: F tensorflow/contrib/lite/toco/tooling_util.cc:968] Check failed: array->has_shape() \r\nAborted (core dumped)\r\n```\r\n\r\n</p>\r\n</details><br/>\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.6\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 4x GeForce 1080 GTX\r\n\r\n```\r\n$ uname -spomvi\r\nLinux #48~16.04.1-Ubuntu SMP Tue Jan 29 18:03:48 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n$ pip freeze | grep tensor\r\nmesh-tensorflow==0.0.5\r\ntensor2tensor==1.12.0\r\ntensorboard==1.12.0\r\ntensorflow-gpu==1.12.0\r\ntensorflow-metadata==0.9.0\r\ntensorflow-probability==0.5.0\r\n```\r\n\r\n", "comments": ["@stefan-falk Could you check whether the error persists with latest TF2.0?  Also, provide details on the [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). thanks!", "@jvishnuvardhan I've installed `tensorflow==2.0.0-alpha0-gpu` and executed the same command as above. It seems it is not supported and I do not find any information about `tf.lite.TFLiteConverter.from_concrete_function()`.\r\n\r\n```\r\n$ tflite_convert --output_file=/tmp/tf-lite/mdl --graph_def_file=frozen.pb --input_arrays=wave_input --output_arrays=outputs\r\nTraceback (most recent call last):\r\n  File \"/home/sfalk/miniconda3/envs/tf2/bin/tflite_convert\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/sfalk/miniconda3/envs/tf2/lib/python3.5/site-packages/tensorflow/lite/python/tflite_convert.py\", line 448, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/sfalk/miniconda3/envs/tf2/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/sfalk/miniconda3/envs/tf2/lib/python3.5/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/sfalk/miniconda3/envs/tf2/lib/python3.5/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/sfalk/miniconda3/envs/tf2/lib/python3.5/site-packages/tensorflow/lite/python/tflite_convert.py\", line 263, in run_main\r\n    raise ValueError(\"tflite_convert is currently unsupported in 2.0. \"\r\nValueError: tflite_convert is currently unsupported in 2.0. Please use the Python API tf.lite.TFLiteConverter.from_concrete_function().\r\n```", "Would be nice to get this fixed. I'm hitting the same error.\r\n`Converting multiple functions is under development.`", "Can't wait for improvements of lite.TFLiteConverter. I'm not able to convert resnet_v1 to `.tflite`\r\nGetting this logs:\r\n```I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayScatterV3\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayV3\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: LoopCond\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayReadV3\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Substr\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DecodeJpeg\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DecodePng\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DecodeGif\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Substr\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: DecodeBmp\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 2\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Enter\r\nI tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayWriteV3\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Exit\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3\r\nI tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\r\nF tensorflow/lite/toco/tooling_util.cc:1041] Check failed: array->has_shape() \r\n```\r\nIf you know a way to convert that kind of model - I would appreciate any tips", "> I am trying to convert the frozen transformer model from tensor2tensor to a tf-lite graph:\r\n> \r\n> ```\r\n> $ tflite_convert --output_file=/tmp/tf-lite/mdl --graph_def_file=frozen.pb --input_arrays=wave_input --output_arrays=outputs\r\n> ```\r\n> \r\n> I am getting the following error\r\n> \r\n> ```\r\n> Traceback (most recent call last):\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/bin/tflite_convert\", line 11, in <module>\r\n>     sys.exit(main())\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 412, in main\r\n>     app.run(main=run_main, argv=sys.argv[:1])\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n>     _sys.exit(main(argv))\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 408, in run_main\r\n>     _convert_model(tflite_flags)\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/tflite_convert.py\", line 162, in _convert_model\r\n>     output_data = converter.convert()\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/lite.py\", line 453, in convert\r\n>     **converter_kwargs)\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 342, in toco_convert_impl\r\n>     input_data.SerializeToString())\r\n>   File \"/home/sfalk/miniconda3/envs/t2t/lib/python3.5/site-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n>     (stdout, stderr))\r\n> RuntimeError: TOCO failed see console for info.\r\n> ```\r\n> \r\n> I would seem that there are a lot of unsupported operations - does this cause the error?\r\n> \r\n> The last line from the tflite_convert output says\r\n> \r\n> > `F tensorflow/contrib/lite/toco/tooling_util.cc:968] Check failed: array->has_shape()`\r\n> \r\n> I don't know if anybody can make sense out of that ...\r\n> \r\n> Is it possible to fix this or is there simply no way to convert this graph at this point in time?\r\n> \r\n> Click to show full tflite_convert output\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n> * TensorFlow installed from (source or binary): pip\r\n> * TensorFlow version (use command below): 1.12.0\r\n> * Python version: 3.5.6\r\n> * CUDA/cuDNN version: 9.0\r\n> * GPU model and memory: 4x GeForce 1080 GTX\r\n> \r\n> ```\r\n> $ uname -spomvi\r\n> Linux #48~16.04.1-Ubuntu SMP Tue Jan 29 18:03:48 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n> \r\n> $ pip freeze | grep tensor\r\n> mesh-tensorflow==0.0.5\r\n> tensor2tensor==1.12.0\r\n> tensorboard==1.12.0\r\n> tensorflow-gpu==1.12.0\r\n> tensorflow-metadata==0.9.0\r\n> tensorflow-probability==0.5.0\r\n> ```\r\n\r\nHi Stefan, I am also facing the same problems. A lot of operations are not yet supported by tf-lite. I was trying to convert a saved model to a tf-lite file. BTW how did you manage to obtain frozen graph for the transformer mode.\r\nI am using https://github.com/tensorflow/models/tree/master/official/transformer/ to create my model.\r\n\r\nI have been able to generate saved_model.pb with estimator's export_saved_model fn, but facing issues while obtaining frozen graph. Could you help with this?", "@abhishekm-in , I hope this will help with frozen graph issue\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\n# start a session using a temporary fresh Graph\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n    output_graph = \"name_of_the_frozen_graph.pb\"\r\n\r\n    # clear devices to allow TensorFlow to control on which device it will load operations\r\n    clear_devices = True\r\n\r\n    # import the meta graph in the current default Graph\r\n    saver = tf.train.import_meta_graph(\"path_to_saved_model .meta\", \r\n                                        clear_devices=clear_devices)\r\n\r\n    # restore the weights\r\n    saver.restore(sess, \"path_to_last_checkpoint .ckpt\")\r\n\r\n    # use a built-in TF helper to export variables to constants\r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n                        sess, # session is used to retrieve the weights\r\n                        tf.get_default_graph().as_graph_def(), # graph_def is used to retrieve the nodes \r\n                        [\"name_of_the_last_layer\"]) \r\n\r\n    # serialize and dump the output graph to the filesystem\r\n    with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n        \r\n    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n```", "The conversion fails because there are (among other unsupported ops) control flow ops (like Enter, Exit, LoopCond) in your model. tflite_convert can't handle those control flow ops at the moment. And we are working on a new converter that can handle the control flow ops.", "@haozha111 That's great news. Any idea when this new converter might be available?", "Hi, the new converter is being developed and hopefully it will release in a few months (I don't have a timeline though, sorry about that).", "> @abhishekm-in , I hope this will help with frozen graph issue\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> \r\n> \r\n> # start a session using a temporary fresh Graph\r\n> with tf.Session(graph=tf.Graph()) as sess:\r\n>     output_graph = \"name_of_the_frozen_graph.pb\"\r\n> \r\n>     # clear devices to allow TensorFlow to control on which device it will load operations\r\n>     clear_devices = True\r\n> \r\n>     # import the meta graph in the current default Graph\r\n>     saver = tf.train.import_meta_graph(\"path_to_saved_model .meta\", \r\n>                                         clear_devices=clear_devices)\r\n> \r\n>     # restore the weights\r\n>     saver.restore(sess, \"path_to_last_checkpoint .ckpt\")\r\n> \r\n>     # use a built-in TF helper to export variables to constants\r\n>     output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n>                         sess, # session is used to retrieve the weights\r\n>                         tf.get_default_graph().as_graph_def(), # graph_def is used to retrieve the nodes \r\n>                         [\"name_of_the_last_layer\"]) \r\n> \r\n>     # serialize and dump the output graph to the filesystem\r\n>     with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n>         f.write(output_graph_def.SerializeToString())\r\n>         \r\n>     print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n> ```\r\n\r\n@Vearol, Thanks for your reply! I am aware of this method. The problem is that the transformer model I cited in my reply uses estimator. So I converted to saved_model.pb but I am not able to convert it to frozen graph pb. Do you know how to go about it?\r\n\r\n@stefan-falk could you let me know how you managed to obtain frozen graph from transformer estimator.", "Hi there, I'm also experiencing this issue in TF 2.0.  I use `save_model` in TF 2.0 and get `saved_mode.pb` as well as `variables` folder.  When I try to convert this to tflite file I get following error:\r\n```\r\nConverting unsupported operation: AddV2 (I get many of these lines)\r\n\r\nCurrent thread 0x00007f6e8b969780 (most recent call first):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"<path>/dist-packages/absl/app.py\", line 250 in _run_main\r\n  File \"<path>/dist-packages/absl/app.py\", line 299 in run\r\n  File \"<path>/dist-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"<path>/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \".../local/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n```\r\nWould be great to find solution...", "@stefan-falk , others: Do you have a code snippet on how you froze the transformer network?\r\nI am using the TF 2.0 implementation that's from the tensorflow repository.\r\nThanks in advance.", "@EphChem Can you please provide simple standalone code to reproduce the issue? Can you try to use new experimental converter by adding this line `converter.experimental_new_converter = True` and see whether the error persists. Thanks!", "@EphChem Could you please respond to the above comment.\r\n@stefan-falk please confirm if the issue still persist.\r\n", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26398\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26398\">No</a>\n"]}, {"number": 26397, "title": "Fix deprecation warnings in beam search decoder", "body": "This one is for the master branch", "comments": ["Please send this PR to [tensorflow/addons](https://github.com/tensorflow/addons) instead; contrib is deprecated and stated to be removed so we're not making new changes to it.", "tf/contrib/seq2seq still looks active to me\r\nhttps://github.com/tensorflow/tensorflow/commit/87d558f2a993f595ffe9b0c58b4a5672be26954c\r\n\r\nThere is an open PR to migrate seq2seq to addons, so I cannot send my PR there yet\r\nhttps://github.com/tensorflow/addons/pull/72", "Let's fix this one too for tf1.14.\n\nOn Sat, Mar 9, 2019, 6:13 AM George Sterpu <notifications@github.com> wrote:\n\n> tf/contrib/seq2seq still looks active to me\n> 87d558f\n> <https://github.com/tensorflow/tensorflow/commit/87d558f2a993f595ffe9b0c58b4a5672be26954c>\n>\n> There is an open PR to migrate seq2seq to addons, so I cannot send my PR\n> there yet\n> tensorflow/addons#72 <https://github.com/tensorflow/addons/pull/72>\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/26397#issuecomment-471181165>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyoSwhPWQBWZq-rvgfeselOZO2Xwks5vU8GagaJpZM4bgzlj>\n> .\n>\n", "@georgesterpu can you please resolve conflicts", "This PR has been made redundant in the meantime after https://github.com/tensorflow/tensorflow/commit/df3a3375941b9e920667acfe72fb4c33a8f45503, will close it. Apologies.", "But wait, there is one left :shipit: \r\nthis [math_ops.div](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L1264) ---> math_ops.divide"]}, {"number": 26396, "title": "Fix deprecation warnings in beam search decoder", "body": "", "comments": []}, {"number": 26395, "title": "build_info is missing CUDA build information", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA 9.0 cuDNN 7.0.5\r\n- GPU model and memory: NVIDIA Titan V\r\n\r\n**Describe the current behavior**\r\n\r\nThe internal `build_info` module can be used to query information about the build, importing it like:\r\n\r\n```py\r\nfrom tensorflow.python.platform import build_info\r\n```\r\n\r\n However, while in previous versions you could get for things like `build_info.cuda_version_number` or `build_info.cudnn_version_number`, now the only available relevant attribute is `build_info.is_cuda_build` (and in TensorFlow 1.13.1 also `build_info.msvcp_dll_name`).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe `build_info` module should also contain information about CUDA version among other things.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```py\r\nfrom tensorflow.python.platform import build_info\r\nprint(build_info.cuda_version_number)\r\n# AttributeError: module 'tensorflow.python.platform.build_info' has no attribute 'cuda_version_number'\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThe `build_info` module is generated during the build process by [`gen_build_info.py`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tools/build_info/gen_build_info.py). Looking through the source, it seems the now deprecated CMake build system would fill this information in [`tensorflow/contrib/cmake/CMakeLists.txt`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/contrib/cmake/CMakeLists.txt#L557-L563), that would be used by the command defined in [`tensorflow/contrib/cmake/tf_python.cmake`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/contrib/cmake/tf_python.cmake#L239-L240), which actually called [`gen_build_info.py`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tools/build_info/gen_build_info.py). Now however the corresponding Bazel build rule in [`tensorflow/tensorflow.bzl`](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tensorflow.bzl#L2011-L2019) only provides the build type (`\"cuda\"` or `\"cpu\"`) and recently also `msvcp_dll_name` on Windows.\r\n\r\nEven though this is not a public API, it seems like a step backwards to miss this information with respect to the CMake build system.", "comments": ["@meteorcloudy This regressed when we switched to using Bazel for the build on Windows. Is there a way to pass the CUDA and cuDNN version information to the genrule [here](https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/tensorflow.bzl#L2011-L2019) so that we get the better error messages again?", "OK, I will take a look!", "So what is the resolution to this problem?\r\n\r\ngetting\r\n> AttributeError: module 'tensorflow.python.platform.build_info' has no attribute 'cuda_version_number'\r\n\r\nusing tensorflow, tensorflow-gpu `2.4.0-rc1`"]}, {"number": 26394, "title": "Allow building TF + nvidia GPU targeting < sm35 if XLA is not enabled", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: pip in venv\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GTX 650 Ti\r\n\r\n\r\n\r\nI was able to successfully build TF from source with XLA enabled and compute capability 3.0. \r\nHowever, when a session is created the python interpreter exits (complaining about insufficient compute capability):\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.Session()\r\n2019-03-06 12:49:41.776396: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500130000 Hz\r\n2019-03-06 12:49:41.776727: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556553ef5ee0 executing computations on platform Host. Devices:\r\n2019-03-06 12:49:41.776741: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-03-06 12:49:41.809556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-03-06 12:49:41.810593: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n2019-03-06 12:49:41.810666: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA\r\n\r\n```\r\n\r\nif I reconfigure TF by disabling XLA, and rebuild (again with compute capability 3.0), than TF works fine.\r\n\r\nSo I guess, a simple check if compute capability >= 3.5 when XLA is enabled, could at least prevent building non-functional TF.", "comments": ["Hey, a similar issue was filed a while ago, and a fix has been deployed [here](https://github.com/tensorflow/tensorflow/pull/25767). Would be available in the master branch/the next release.", "I actually found similar issues, but non of them was XLA related. And indeed as you pointed out (#25767) the config script does no longer allow building TF with compute capability bellow 3.5. Even it looks like if XLA is disabled TF could still be built and function properly with capability 3.0.\r\n\r\nalso just for referece - #24126", "> as you pointed out (#25767) the config script does no longer allow building TF with compute capability bellow 3.5. \r\n\r\nIf this is the case, I'm not sure what is the bug here?", "The \"bug\" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.\r\nWhat leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.\r\n", "> The \"bug\" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.\r\n> What leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.\r\n\r\nSorry, I'm still confused.\r\n\r\nIn the second sentence, I'm understanding that TF built without XLA and run on sm30 *does* work?  But in the first sentence I'm understanding that you *can't* build TF with XLA disabled targeting sm30?  Those seem at odds.", "Well, the first sentence is - you can build it if you patch configure.py to accept compute capability 3.0 (with disabled XLA). And in sentence two, there are two statements - a) XLA and 3.0 = broken-TF, and b) disabled XLA and 3.0 = working-TF.", "....I mean, I had to patch config.py in order to build an TF version for my old GPU (compute capability 3.0).\r\n", "OK, so what you'd like is for the TF build team to change config.py so that it doesn't require sm35, if XLA is disabled?", "Yes, this is what the issue was about. (edit: at least after I learned there was a  >= 3.5 check on the main branch already, preventing TF from being built with 3.0)", "It may be the case that TF build team has decided that they do not support < sm35 at all, even though you observed that it happened to work in your particular case.  Over to them, this is not really an XLA question.", "Yes, sorry for the confusion, it was really about, a better config sanity check.", "@chsigg may be able to confirm or deny, but here is what I can say:\r\n\r\nBelow compute capability 3.0 TF will NOT work. XLA, or no xla, TF needs features in compute capability 3.0\r\nBelow 3.5 is best effort, we do not test it, or even build it.  It may work, may not work, you are free to use at your own risk.\r\nAbove 3.5 will work.\r\n6.0 is the most heavily tested one for us, and any issues we see for 6.0 or newer is promptly triaged.", "We can probably change #25767 to issue an error for compute capability < 3.0 (down from currently < 3.5). \r\n\r\nFor compute capability < 3.5, we can issue a warning that XLA is not supported. XLA generates CUDA code for whatever GPU is present at runtime, so anything more than a warning during configuration seems prohibitive. ", "I can submit another PR if it is required, but I would appreciate it if you could specify the exact information that should be conveyed in the warning. The original PR did issue a warning and not an error. It was changed because it seemed like TensorFlow would surely fail to work below 3.5 (which is not really the case maybe?).", "The compute capability check is now a warning for < 3.5, and and error for < 3.0. Closing."]}, {"number": 26393, "title": "Kubernetes GPU Docs & k8s-compatible Docker Images", "body": "- TensorFlow version: 1.13.1\r\n- Doc Link: https://www.tensorflow.org/install/gpu\r\n\r\nThe existing documentation on how to setup TensorFlow with docker&GPU is great. Unfortunately it doesn't work with Kubernetes. This [Kubernetes Engine GPUs](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus) guide suggest using \"nvidia/cuda:10.0-runtime-ubuntu18.04\" docker image. TensorFlow docs suggest \"tensorflow/tensorflow:latest-gpu\". \r\n\r\nIdeally, we could use them both as base images, but that's impossible. \r\n\r\nAnd of course this depends also on which VM image you're using for the k8s nodes.\r\n\r\nIt would be great if the docs provide some guidance on this setup. \r\n\r\nIt would be really awesome if a docker image compatible with one of k8s VM images was available containing both the necessary NVidia support for a k8s node and complete tensorflow-gpu package environment.", "comments": ["Thanks for your suggestions. As far as I know, we don't have the resources to focus on Kubernetes-specific Dockerfiles right now (@lamberta or @dynamicwebpaige may be able to comment on whether or not there are plans for docs). I'd welcome a contribution to our dockerfiles repository, though.", "@angersson, could I have a try for this?", "@angersson Thanks for your reply. I did try really hard to construct a working image for more than a week, but it seems it requires some more expertise (or it's just me ;) ). I've tried both - starting from the \"tensorflow/tensorflow:latest-gpu\" and trying to include things missing from k8s point of view or starting from the k8s NVidia image and installing tensorflow and dependencies.", "@a6802739 Sure, I'd be happy to review a PR.\r\n\r\n@bartoszkp No worries -- Dockerfiles are notoriously hard to get working correctly.", "@bartoszkp You can see an example of how to build under kubernetes https://github.com/puzl-ee/tensorflow", "Hi @bartoszkp ! Have you checked these threads on deploying Tensorflow model through serving on kubernetes? R [1](https://www.tensorflow.org/tfx/serving/serving_kubernetes) ,[2](https://aws.amazon.com/blogs/compute/tensorflow-serving-on-kubernetes-spot-instances/) .", "@mohantym Hello! Thanks for the comment. I've completely lost context on this one and do not have time to pursue it again. But hopefully this thread will be helpful for other visitors! Cheers! :-)", "Ok @bartoszkp ! Thanks for replying . Closing this issue for now. Please post on [TF serving](https://github.com/tensorflow/serving) repo  if you need further assistance.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26393\">No</a>\n"]}, {"number": 26392, "title": "train with MirroredStrategy with multiple gpus has no speed up", "body": "**System information**\r\nOS Platform and Distribution: CentOS Linux release 7.3.1611\r\nTensorFlow installed from: (pip install tensorflow-gpu)\r\nTensorFlow version: Tensorflow 1.12\r\nBazel version: N/A\r\nGPU model and memory: Tesla P40 24G\r\nExact command to reproduce: N/A\r\nMobile device: N/A\r\nCUDA/cuDNN version: cuda 9.0 with cudnn7.1.4\r\n\r\n**Describe the current behavior**\r\nI trained with tensorflow for multi-gpu with MirroredStrategy and estimator. My model is a simple dnn. The layer number is 256->128->64.  when I set bigger batch_size (such as 1024), the training speed using multiple gpus  has no improvement compared with single gpu. As following picture shows:\r\n\r\n![image](https://user-images.githubusercontent.com/11607954/53925412-da2d3000-40ba-11e9-994f-1130348d7b67.png)\r\n\r\n\r\nWhat I want to ask is that the acceleration performance of MirroredStrategy has associated with the type of model? Is that the complex model like cnn just can have obvious acceleration performance when using multiple gpus?\r\n\r\n\r\n\r\n", "comments": ["You should see performance of 1 GPU * number of GPUs  * 90%-ish percent scaling.  Please check your measurement logic, because it's common to only count the single GPU number.  Please re-open if you think it's still an issue, and if so, please provide more details about your setup.  One other thing you could do is to look at `$ nvidia-smi` during your training to observe the utilization of your GPUs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26392\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26392\">No</a>\n"]}, {"number": 26391, "title": "str_cat.h(267): error :expression must have a constant value", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):gentoo \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No, intel PC\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:1.12.0 and 1.13.0-r2\r\n- Python version:3.4.3\r\n- Installed using virtualenv? pip? conda?:no \r\n- Bazel version (if compiling from source):0.20.0\r\n- GCC/Compiler version (if compiling from source): 4.9.3\r\n- CUDA/cuDNN version:8.0/6.0\r\n- GPU model and memory:Geferfoce 1050Ti\r\n\r\nexternal/com_google_absl/absl/strings/string_view.h(496): warning: expression has no effect\r\nexternal/com_google_absl/absl/strings/str_cat.h(267): error: expression must have a constant value\r\nexternal/com_google_absl/absl/strings/str_cat.h(267): error: expression must have a constant value\r\n\r\n", "comments": ["@wuyankun Could you provide more details on the issue and its context? Were you able to successfully install TF? Could you provide a code to reproduce the bug? Please provide more details to find root-cause of the issue. Thanks!", "Package:    sci-libs/tensorflow-1.12.0\r\nRepository: gentoo\r\nMaintainer: perfinion@gentoo.org\r\nUSE:        abi_x86_64 amd64 cpu_flags_x86_sse cpu_flags_x86_sse2 cuda elibc_glibc kernel_linux python_targets_python2_7 python_targets_python3_4 userland_GNU\r\nFEATURES:   nostrip preserve-libs sandbox splitdebug userpriv usersandbox\r\nChecking for at least 5 GiB RAM ...\r\nChecking for at least 5 GiB disk space at \"/var/tmp/portage/sci-libs/tensorflow-1.12.0/temp\" ...\r\n\r\n>>> Unpacking source...\r\n>>> Unpacking tensorflow-1.12.0.tar.gz to /var/tmp/portage/sci-libs/tensorflow-1.12.0/work\r\n>>> Unpacking tensorflow-patches-1.12.0.tar.bz2 to /var/tmp/portage/sci-libs/tensorflow-1.12.0/work\r\nCopying fft.tgz to bazel distdir as oourafft-20061228.tgz\r\nCopying fd6845384b86.tar.gz to bazel distdir as eigen-fd6845384b86.tar.gz\r\nCopying 48cd2c3f351ff188bc85684b84a91b6e6d17d896.tar.gz to bazel distdir as abseil-cpp-48cd2c3f351ff188bc85684b84a91b6e6d17d896.tar.gz\r\nCopying dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz to bazel distdir as bazelbuild-rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz\r\nCopying 3992066a95b823efc8ccc1baf82a1cfc73f6e9b8.zip to bazel distdir as double-conversion-3992066a95b823efc8ccc1baf82a1cfc73f6e9b8.zip\r\nCopying 816a4ae622e964763ca0862d9dbd19324a1eaf45.tar.gz to bazel distdir as farmhash-816a4ae622e964763ca0862d9dbd19324a1eaf45.tar.gz\r\nCopying 38ebac7b059e84692f53e5938f97a9943c120d98.zip to bazel distdir as gemmlowp-38ebac7b059e84692f53e5938f97a9943c120d98.zip\r\nCopying fd3d9af80465e4383162e4a7c5e2f406e82dd968.tar.gz to bazel distdir as highwayhash-fd3d9af80465e4383162e4a7c5e2f406e82dd968.tar.gz\r\nCopying 03d856977ecbaac87e598c0c4bafca96761b9ac7.tar.gz to bazel distdir as nvidia-nccl-03d856977ecbaac87e598c0c4bafca96761b9ac7.tar.gz\r\nCopying 1.8.0.zip to bazel distdir as cub-1.8.0.zip\r\nCopying backports.weakref-1.0rc1.tar.gz to bazel distdir\r\n>>> Source unpacked in /var/tmp/portage/sci-libs/tensorflow-1.12.0/work\r\n>>> Preparing source in /var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0 ...\r\nApplying 0001-Make-AWS-GCP-HDFS-Kafka-and-Ignite-default-on-but-in.patch ...\r\nApplying 0002-systemlibs-unbundle-icu.patch ...\r\nApplying 0003-systemlibs-Unbundle-protobuf.patch ...\r\nApplying 0004-no_support-flags-were-negated-so-did-not-work.patch ...\r\nApplying 0005-Use-Starlark-version-of-http_archive.patch ...\r\nApplying 0006-Add-k8-to-toolchains.patch ...\r\n>>> Source prepared.\r\n>>> Configuring source in /var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0 ...\r\nSetting CUDA version: 8.0\r\nSetting CUDNN version: 6.0\r\nExtracting Bazel installation...\r\nWARNING: ignoring LD_PRELOAD in environment.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 07e4ebc7-e002-4c6b-ae9d-b5f2b3e40245\r\nYou have bazel 0.20.0- (@non-git) installed.\r\nNVIDIA: failed to execute `/usr/bin/nvidia-modprobe -u`: Permission denied.\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5]: \r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apacha Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\nConfiguration finished\r\n>>> Source configured.\r\n>>> Compiling source in /var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0 ...\r\nbazel --bazelrc=/var/tmp/portage/sci-libs/tensorflow-1.12.0/temp/bazelrc --output_base=/var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0-bazel-base build --nobuild //tensorflow:libtensorflow_framework.so //tensorflow:libtensorflow.so //tensorflow:libtensorflow_cc.so\r\nWARNING: ignoring LD_PRELOAD in environment.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO:Invocation ID: 62d945b0-af42-4b34-a9a8-fc9ba6a0858b\r\nLoading:\r\n32mLoading:0 packages loaded\r\n    currently loading: tensorflow\r\n    Fetching @local_config_cuda; fetching\r\n    Fetching @local_config_rocm; Restarting.\r\n\r\nbazel --bazelrc=/var/tmp/portage/sci-libs/tensorflow-1.12.0/temp/bazelrc --output_base=/var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0-bazel-base build //tensorflow:libtensorflow_framework.so //tensorflow:libtensorflow.so\r\nWARNING: ignoring LD_PRELOAD in environment.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: dd7e0bfa-b412-4a1a-8d17-a8780e43d75d\r\n\r\n\r\nIn file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,\r\n                 from ./tensorflow/core/framework/resource_mgr.h:25,\r\n                 from ./tensorflow/core/common_runtime/device.h:43,\r\n                 from ./tensorflow/core/common_runtime/device_set.h:23,\r\n                 from ./tensorflow/core/grappler/clusters/cluster.h:24,\r\n                 from ./tensorflow/core/grappler/costs/graph_properties.h:22,\r\n                 from tensorflow/core/grappler/costs/graph_properties.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':\r\n./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {\r\n                                             ^\r\nIn file included from tensorflow/core/grappler/costs/graph_properties.cc:31:0:\r\n./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':\r\n./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       std::distance(input_it, input_name.end()) < node_name.size()) {\r\n                                                 ^\r\ntensorflow/core/grappler/costs/graph_properties.cc: In member function 'tensorflow::Status tensorflow::grappler::SymbolicShapeRefiner::UpdateFunction(const tensorflow::NodeDef*)':\r\ntensorflow/core/grappler/costs/graph_properties.cc:501:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < grappler_function_item.inputs().size(); ++i) {\r\n                       ^\r\ntensorflow/core/grappler/costs/graph_properties.cc:556:55: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       } else if (ic->input_tensors_as_shapes().size() > i &&\r\n                                                       ^\r\ntensorflow/core/grappler/costs/graph_properties.cc:599:19: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       if (port_id >= output_properties.size()) {\r\n                   ^\r\ntensorflow/core/grappler/costs/graph_properties.cc: In member function 'tensorflow::Status tensorflow::grappler::SymbolicShapeRefiner::UpdateNode(const tensorflow::NodeDef*, bool*)':\r\ntensorflow/core/grappler/costs/graph_properties.cc:697:48: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         if (c->output_tensors_as_shapes.size() > src_output) {\r\n                                                ^\r\ntensorflow/core/grappler/costs/graph_properties.cc: In member function 'bool tensorflow::grappler::SymbolicShapeRefiner::EquivalentShapesAndTypes(const std::vector<tensorflow::shape_inference::ShapeAndType>&, const std::vector<tensorflow::shape_inference::ShapeAndType>&) const':\r\ntensorflow/core/grappler/costs/graph_properties.cc:869:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int i = 0; i < st1.size(); ++i) {\r\n                       ^\r\ntensorflow/core/grappler/costs/graph_properties.cc: In member function 'tensorflow::Status tensorflow::grappler::SymbolicShapeRefiner::AddFunction(const tensorflow::NodeDef*)':\r\ntensorflow/core/grappler/costs/graph_properties.cc:896:48: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (grappler_function_item.inputs().size() > function_node->input_size()) {\r\n                                                ^\r\ntensorflow/core/grappler/costs/graph_properties.cc: In static member function 'static tensorflow::Status tensorflow::grappler::GraphProperties::UpdateEnqueue(const tensorflow::NodeDef*, const std::unordered_map<const tensorflow::NodeDef*, const tensorflow::NodeDef*>&, tensorflow::grappler::SymbolicShapeRefiner*, bool*)':\r\ntensorflow/core/grappler/costs/graph_properties.cc:1501:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 1; i < ctx->input_types.size(); ++i) {\r\n                     ^\r\ntensorflow/core/grappler/costs/graph_properties.cc: In member function 'tensorflow::Status tensorflow::grappler::GraphProperties::InferStatically(bool)':\r\ntensorflow/core/grappler/costs/graph_properties.cc:1688:57: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         } else if (ic->input_tensors_as_shapes().size() > i &&\r\n                                                         ^\r\ntensorflow/core/grappler/costs/graph_properties.cc:1715:57: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n         } else if (ctx->output_tensors_as_shapes.size() > i &&\r\n                                                         ^\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:25,\r\n                 from ./tensorflow/core/lib/core/errors.h:21,\r\n                 from ./tensorflow/core/framework/tensor_shape.h:23,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:22,\r\n                 from ./tensorflow/core/framework/shape_inference.h:20,\r\n                 from ./tensorflow/core/grappler/costs/graph_properties.h:21,\r\n                 from tensorflow/core/grappler/costs/graph_properties.cc:16:\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:452:47:   required from here\r\n./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\n./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\n./tensorflow/core/util/tensor_format.h:461:54:   required from here\r\n./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\n./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\n./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\n\r\nIn file included from tensorflow/core/grappler/utils.cc:16:0:\r\n./tensorflow/core/grappler/utils.h: In function 'int tensorflow::grappler::NodePositionIfSameNode(const string&, const string&)':\r\n./tensorflow/core/grappler/utils.h:117:49: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n       std::distance(input_it, input_name.end()) < node_name.size()) {\r\n                                                 ^\r\ntensorflow/core/grappler/utils.cc: In function 'void tensorflow::grappler::PermuteNodesInPlace(tensorflow::GraphDef*, std::vector<int>*, bool)':\r\ntensorflow/core/grappler/utils.cc:327:14: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     while (n != (*permutation)[n]) {\r\n              ^\r\ntensorflow/core/grappler/utils.cc: In member function 'std::string tensorflow::grappler::SimpleGraphView::PrintToString() const':\r\ntensorflow/core/grappler/utils.cc:500:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     for (int j = 0; j < outputs(i).size(); ++j) {\r\n                       ^\r\n    Compiling .../contrib/cloud/kernels/bigquery_table_accessor.cc; 10s local\r\n    Compiling tensorflow/core/kernels/sequence_ops.cc; 8s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 2s local\r\n    [-----] Linking tensorflow/core/kernels/libcolorspace_op_gpu.pic.lo\r\n\r\nINFO: From Compiling tensorflow/contrib/cloud/kernels/bigquery_table_accessor.cc:\r\n[984 / 1,066]8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 141s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 134s local\r\n    Compiling .../core/kernels/data/group_by_window_dataset_op.cc; 21s local\r\n    Compiling tensorflow/core/kernels/data/iterator_ops.cc; 21s local\r\n    Compiling .../contrib/cloud/kernels/bigquery_table_accessor.cc; 11s local\r\n    Compiling tensorflow/core/kernels/sequence_ops.cc; 8s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 2s local\r\n    Linking tensorflow/core/kernels/libcolorspace_op_gpu.pic.lo; 0s local\r\n\r\nKtensorflow/contrib/cloud/kernels/bigquery_table_accessor.cc: In function 'tensorflow::Status tensorflow::{anonymous}::ParseJson(tensorflow::StringPiece, Json::Value*)':\r\ntensorflow/contrib/cloud/kernels/bigquery_table_accessor.cc:35:16: warning: 'Reader' is deprecated (declared at /usr/include/jsoncpp/json/reader.h:35): Use CharReader and CharReaderBuilder instead [-Wdeprecated-declarations]\r\n   Json::Reader reader;\r\n                ^\r\n[984 / 1,066] 8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 141s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 134s local\r\n    Compiling .../core/kernels/data/group_by_window_dataset_op.cc; 21s local\r\n    Compiling tensorflow/core/kernels/data/iterator_ops.cc; 21s local\r\n    Compiling .../contrib/cloud/kernels/bigquery_table_accessor.cc; 11s local\r\n    Compiling tensorflow/core/kernels/sequence_ops.cc; 8s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 2s local\r\n    Linking tensorflow/core/kernels/libcolorspace_op_gpu.pic.lo; 0s local\r\n\r\n[988 / 1,071] 8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 141s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 134s local\r\n    Compiling .../core/kernels/data/group_by_window_dataset_op.cc; 21s local\r\n    Compiling tensorflow/core/kernels/data/iterator_ops.cc; 21s local\r\n    Compiling tensorflow/core/kernels/sequence_ops.cc; 8s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 3s local\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 0s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 0s local\r\n\r\n[989 / 1,071] 8 actions, 7 running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 141s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 134s local\r\n    Compiling .../core/kernels/data/group_by_window_dataset_op.cc; 22s local\r\n    Compiling tensorflow/core/kernels/data/iterator_ops.cc; 21s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 3s local\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 0s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 0s local\r\n    [-----] Linking tensorflow/core/kernels/libsequence_ops.pic.lo\r\n\r\n[990 / 1,072] 8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 142s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 135s local\r\n    Compiling .../core/kernels/data/group_by_window_dataset_op.cc; 22s local\r\n    Compiling tensorflow/core/kernels/data/iterator_ops.cc; 22s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 3s local\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 1s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 0s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_double.cc; 0s local\r\n\r\nINFO: From Compiling tensorflow/core/kernels/data/group_by_window_dataset_op.cc:\r\n[990 / 1,072] 8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 142s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 135s local\r\n    Compiling .../core/kernels/data/group_by_window_dataset_op.cc; 22s local\r\n    Compiling tensorflow/core/kernels/data/iterator_ops.cc; 22s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 4s local\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 1s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 1s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_double.cc; 0s local\r\n\r\n[KIn file included from ./tensorflow/core/framework/dataset.h:26:0,\r\n                 from ./tensorflow/core/kernels/data/dataset.h:18,\r\n                 from ./tensorflow/core/kernels/data/captured_function.h:23,\r\n                 from tensorflow/core/kernels/data/group_by_window_dataset_op.cc:20:\r\n./tensorflow/core/framework/model.h: In constructor 'tensorflow::data::model::SharedState::SharedState(tensorflow::int64, std::shared_ptr<tensorflow::mutex>, std::shared_ptr<tensorflow::condition_variable>)':\r\n./tensorflow/core/framework/model.h:46:9: warning: 'tensorflow::data::model::SharedState::value' will be initialized after [-Wreorder]\r\n   int64 value;\r\n         ^\r\n./tensorflow/core/framework/model.h:44:26: warning:   'std::shared_ptr<tensorflow::mutex> tensorflow::data::model::SharedState::mu' [-Wreorder]\r\n   std::shared_ptr<mutex> mu;\r\n                          ^\r\n./tensorflow/core/framework/model.h:40:12: warning:   when initialized here [-Wreorder]\r\n   explicit SharedState(int64 value, std::shared_ptr<mutex> mu,\r\n            ^\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 6s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_double.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex128.cc; 5s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_float.cc; 0s local\r\n\r\n\r\n[Kexternal/com_google_absl/absl/strings/string_view.h(496): warning: expression has no effect\r\n\r\nexternal/com_google_absl/absl/strings/str_cat.h(259): error: expression must have a constant value\r\n\r\nexternal/com_google_absl/absl/strings/str_cat.h(259): error: expression must have a constant value\r\n\r\n2 errors detected in the compilation of \"/var/tmp/portage/sci-libs/tensorflow-1.12.0/temp/tmpxft_00003ad6_00000000-7_searchsorted_op_gpu.cu.cpp1.ii\".\r\n[994 / 1,074] 8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 148s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 141s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 9s local\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 6s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_double.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex128.cc; 5s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_float.cc; 0s local\r\n\r\n\r\nERROR: /var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0/tensorflow/core/kernels/BUILD:891:1: output 'tensorflow/core/kernels/_objs/searchsorted_op_gpu/searchsorted_op_gpu.cu.pic.o' was not created\r\n[994 / 1,074] 8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 148s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 141s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 9s local\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 6s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_double.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex128.cc; 5s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_float.cc; 0s local\r\n\r\n\r\nERROR: /var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0/tensorflow/core/kernels/BUILD:891:1: not all outputs were created or valid\r\n[994 / 1,074] 8 actions running\r\n    Compiling tensorflow/core/kernels/svd_op_complex64.cc; 148s local\r\n    Compiling tensorflow/core/kernels/svd_op_complex128.cc; 141s local\r\n    Compiling tensorflow/core/kernels/searchsorted_op_gpu.cu.cc; 9s local\r\n    Compiling .../distributed_runtime/device_resolver_distributed.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex64.cc; 6s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_double.cc; 6s local\r\n    Compiling .../core/kernels/matrix_solve_ls_op_complex128.cc; 5s local\r\n    Compiling tensorflow/core/kernels/matrix_solve_ls_op_float.cc; 0s local\r\n\r\n\r\nElapsed time: 580.201s, Critical Path: 162.87s\r\n[1,002 / 1,074] no action\r\n\r\n702 processes: 702 local.\r\n[1,002 / 1,074] no action\r\n\r\nFAILED: Build did NOT complete successfully\r\n\r\n Build did NOT complete successfully\r\n ERROR: sci-libs/tensorflow-1.12.0::gentoo failed (compile phase):\r\n ebazel failed\r\n Call stack:\r\n ebuild.sh, line  133:  Called src_compile\r\n environment, line 4755:  Called ebazel 'build' '//tensorflow:libtensorflow_framework.so' '//tensorflow:libtensorflow.so'\r\n environment, line 2018:  Called die\r\n The specific snippet of code:\r\n \"${@}\" || die \"ebazel failed\"\r\n If you need support, post the output of `emerge --info '=sci-libs/tensorflow-1.12.0::gentoo'`,\r\n the complete build log and the output of `emerge -pqv '=sci-libs/tensorflow-1.12.0::gentoo'`.\r\n The complete build log is located at '/var/tmp/portage/sci-libs/tensorflow-1.12.0/temp/build.log'.\r\n The ebuild environment file is located at '/var/tmp/portage/sci-libs/tensorflow-1.12.0/temp/environment'.\r\n \u001b[31;01m*\u001b[0m Working directory: '/var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0'\r\n \u001b[31;01m*\u001b[0m S: '/var/tmp/portage/sci-libs/tensorflow-1.12.0/work/tensorflow-1.12.0'", "Portage 2.2.28 (python 3.4.3-final-0, default/linux/amd64/13.0/desktop/kde, gcc-4.9.3, glibc-2.22-r4, 4.4.111-gentoo+ x86_64)\r\n=================================================================\r\n                         System Settings\r\n=================================================================\r\nSystem uname: Linux-4.4.111-gentoo+-x86_64-Intel-R-_Core-TM-_i7-4710MQ_CPU_@_2.50GHz-with-gentoo-2.2\r\nKiB Mem:    16374512 total,  13126992 free\r\nKiB Swap:          0 total,         0 free\r\nTimestamp of repository gentoo: Tue, 26 Jul 2016 00:45:01 +0000\r\nsh bash 4.3_p42-r1\r\nld GNU ld (Gentoo 2.25.1 p1.1) 2.25.1\r\napp-shells/bash:          4.3_p42-r1::gentoo\r\ndev-java/java-config:     2.2.0-r3::gentoo\r\ndev-lang/perl:            5.20.2::gentoo\r\ndev-lang/python:          2.7.10-r1::gentoo, 3.4.3-r1::gentoo, 3.5.2::gentoo\r\ndev-util/cmake:           3.3.1-r1::gentoo\r\ndev-util/pkgconfig:       0.28-r2::gentoo\r\nsys-apps/baselayout:      2.2::gentoo\r\nsys-apps/openrc:          0.19.1::gentoo\r\nsys-apps/sandbox:         2.10-r1::gentoo\r\nsys-devel/autoconf:       2.13::gentoo, 2.69::gentoo\r\nsys-devel/automake:       1.11.6-r1::gentoo, 1.14.1::gentoo, 1.15::gentoo\r\nsys-devel/binutils:       2.25.1-r1::gentoo\r\nsys-devel/gcc:            4.9.3::gentoo\r\nsys-devel/gcc-config:     1.7.3::gentoo\r\nsys-devel/libtool:        2.4.6::gentoo\r\nsys-devel/make:           4.1-r1::gentoo\r\nsys-kernel/linux-headers: 4.3::gentoo (virtual/os-headers)\r\nsys-libs/glibc:           2.22-r4::gentoo\r\nRepositories:\r\n\r\ngentoo\r\n    location: /usr/portage\r\n    sync-type: rsync\r\n    sync-uri: rsync://rsync.gentoo.org/gentoo-portage\r\n    priority: -1000\r\n\r\nACCEPT_KEYWORDS=\"amd64\"\r\nACCEPT_LICENSE=\"* -@EULA\"\r\nCBUILD=\"x86_64-pc-linux-gnu\"\r\nCFLAGS=\"-ggdb -O2 -pipe\"\r\nCHOST=\"x86_64-pc-linux-gnu\"\r\nCONFIG_PROTECT=\"/etc /usr/share/config /usr/share/gnupg/qualified.txt\"\r\nCONFIG_PROTECT_MASK=\"/etc/ca-certificates.conf /etc/dconf /etc/env.d /etc/fonts/fonts.conf /etc/gconf /etc/gentoo-release /etc/sandbox.d /etc/splash /etc/terminfo\"\r\nCXXFLAGS=\"-ggdb -O2 -pipe\"\r\nDISTDIR=\"/usr/portage/distfiles\"\r\nFCFLAGS=\"-O2 -pipe\"\r\nFEATURES=\"assume-digests binpkg-logs config-protect-if-modified distlocks ebuild-locks fixlafiles merge-sync news nostrip parallel-fetch preserve-libs protect-owned sandbox sfperms splitdebug strict unknown-features-warn unmerge-logs unmerge-orphans userfetch userpriv usersandbox usersync xattr\"\r\nFFLAGS=\"-O2 -pipe\"\r\nGENTOO_MIRRORS=\"http://mirrors.163.com/gentoo/\"\r\nLANG=\"en_US.utf8\"\r\nLDFLAGS=\"-Wl,-O1 -Wl,--as-needed\"\r\nPKGDIR=\"/usr/portage/packages\"\r\nPORTAGE_CONFIGROOT=\"/\"\r\nPORTAGE_RSYNC_OPTS=\"--recursive --links --safe-links --perms --times --omit-dir-times --compress --force --whole-file --delete --stats --human-readable --timeout=180 --exclude=/distfiles --exclude=/local --exclude=/packages --exclude=/.git\"\r\nPORTAGE_TMPDIR=\"/var/tmp\"\r\nUSE=\"X a52 aac acc acl acpi alsa amd64 ap berkdb bindist bluetooth branding bzip2 cairo cdda cdr cli consolekit cracklib crypt cups cxx dbus debug declarative dri dri3 dts dvd dvdr eap-sim emboss encode exif fam fasteap firefox flac fortran gdbm gif glamor gpm gtk hs2-0 iconv ipv6 jpeg kde kipi lcms ldap libnotify mad mmx mmxext mng modules mp3 mp4 mpeg multilib ncurses nls nptl ogg opengl openmp p2p pam pango pcre pdf phonon plasma png policykit ppds qt3support qt5 readline sdl seccomp semantic-desktop session sna spell sse sse2 startup-notification svg tcpd tdls tiff truetype udev udisks uncommon-eap-types unicode upower usb uxa vorbis wps wxwidgets x264 xattr xcb xcomposite xinerama xml xscreensaver xv xvid xvmc zlib\" ABI_X86=\"64\" ALSA_CARDS=\"ali5451 als4000 atiixp atiixp-modem bt87x ca0106 cmipci emu10k1x ens1370 ens1371 es1938 es1968 fm801 hda-intel intel8x0 intel8x0m maestro3 trident usb-audio via82xx via82xx-modem ymfpci\" APACHE2_MODULES=\"authn_core authz_core socache_shmcb unixd actions alias auth_basic authn_alias authn_anon authn_dbm authn_default authn_file authz_dbm authz_default authz_groupfile authz_host authz_owner authz_user autoindex cache cgi cgid dav dav_fs dav_lock deflate dir disk_cache env expires ext_filter file_cache filter headers include info log_config logio mem_cache mime mime_magic negotiation rewrite setenvif speling status unique_id userdir usertrack vhost_alias\" CALLIGRA_FEATURES=\"kexi words flow plan sheets stage tables krita karbon braindump author\" CAMERAS=\"ptp2\" COLLECTD_PLUGINS=\"df interface irq load memory rrdtool swap syslog\" CPU_FLAGS_X86=\"mmx sse sse2 mmxext\" ELIBC=\"glibc\" GPSD_PROTOCOLS=\"ashtech aivdm earthmate evermore fv18 garmin garmintxt gpsclock itrax mtk3301 nmea ntrip navcom oceanserver oldstyle oncore rtcm104v2 rtcm104v3 sirf superstar2 timing tsip tripmate tnt ublox ubx\" INPUT_DEVICES=\"keyboard mouse evdev\" KERNEL=\"linux\" LCD_DEVICES=\"bayrad cfontz cfontz633 glk hd44780 lb216 lcdm001 mtxorb ncurses text\" LIBREOFFICE_EXTENSIONS=\"presenter-console presenter-minimizer\" OFFICE_IMPLEMENTATION=\"libreoffice\" PHP_TARGETS=\"php5-5\" PYTHON_SINGLE_TARGET=\"python2_7\" PYTHON_TARGETS=\"python2_7 python3_4\" RUBY_TARGETS=\"ruby20 ruby21\" USERLAND=\"GNU\" VIDEO_CARDS=\"intel i915\" XTABLES_ADDONS=\"quota2 psd pknock lscan length2 ipv4options ipset ipp2p iface geoip fuzzy condition tee tarpit sysrq steal rawnat logmark ipmark dhcpmac delude chaos account\"\r\nUnset:  CC, CPPFLAGS, CTARGET, CXX, EMERGE_DEFAULT_OPTS, INSTALL_MASK, LC_ALL, MAKEOPTS, PORTAGE_BUNZIP2_COMMAND, PORTAGE_COMPRESS, PORTAGE_COMPRESS_FLAGS, PORTAGE_RSYNC_EXTRA_OPTS, USE_PYTHON\r\n", "@jvishnuvardhan , thanks for your help .maybe you can see the issue  #23127, there are some more info.\r\n", "I just switch to cuda 9.0 , the issues is gone, I don't know why, If I can use cuda 8.0 , will be more better", "@wuyankun Please check tested build configurations [here](https://www.tensorflow.org/install/source_windows#gpu).\r\nFor TF1.12, it was built with cuda9.0 which means some of the dependencies will be looking for paths referencing cuda9.0.\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-1.12.0 | 3.5-3.6 | MSVC 2015 update 3 | Bazel 0.15.0 | 7 | 9\r\n\r\nIf you want to use cuda8.0, then you need to downgrade to TF1.4.0 which is not recommended as there were lot of improvements over the time. Please close the issue if everything is working fine. Thanks!\r\n "]}, {"number": 26390, "title": "DLL load failed with error code -1073741795, Win 7", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): installation from https://www.tensorflow.org/install/pip\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: both pip, virtualenv\r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: tried with all CUDA8, CUDA7.5,CUDA9\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried different version of python and tensorflow and check the script for selecting CUDA version,  it has the same error. Also I installed airflow but desn't work!!!!!!\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", l\r\nine 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__\r\n.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_t\r\nensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\azak\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I tried all of the tensorflow versions only 1.5 works and I need tensorflow 1.12 version.\r\nregards,\r\nAzadeh.", "I have the same problem.  Clean pip install of tensorflow on python 3.7\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): installation from https://www.tensorflow.org/install/pip\r\nTensorFlow version: 1.13\r\nPython version: 3.7.2\r\nInstalled using virtualenv? pip? conda?: pip\r\nBazel version (if compiling from source):no\r\nGCC/Compiler version (if compiling from source): no\r\nCUDA/cuDNN version: no CUDA\r\nGPU model and memory:\r\nDescribe the problem\r\n\r\nPS C:\\Users\\i102604> python\r\nPython 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\r\n\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\nternal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\nternal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <\r\nmodule>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 4\r\n9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\r\n\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\r\n\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\nternal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\nternal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> quit()", "I got the same error even with CPU version tensorflow-1.9.0-cp36-cp36m-win_amd64.whl in windows 10.\r\nSo frustrating\r\n\r\ntensorflow 1.5 works fine with keras 2.1.6 and python 3.6.8", "> I have the same problem. Clean pip install of tensorflow on python 3.7\r\n> System information\r\n> \r\n> OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 7\r\n> Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n> TensorFlow installed from (source or binary): installation from https://www.tensorflow.org/install/pip\r\n> TensorFlow version: 1.13\r\n> Python version: 3.7.2\r\n> Installed using virtualenv? pip? conda?: pip\r\n> Bazel version (if compiling from source):no\r\n> GCC/Compiler version (if compiling from source): no\r\n> CUDA/cuDNN version: no CUDA\r\n> GPU model and memory:\r\n> Describe the problem\r\n> \r\n> PS C:\\Users\\i102604> python\r\n> Python 3.7.2 (tags/v3.7.2:9a3ffc0492, Dec 23 2018, 23:09:28) [MSC v.1916 64 bit (AMD64)] on win32\r\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> \r\n> > > > import tensorflow as tf\r\n> > > > Traceback (most recent call last):\r\n> > > > File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\r\n> > > > \", line 58, in \r\n> > > > from tensorflow.python.pywrap_tensorflow_internal import *\r\n> > > > File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\n> > > > ternal.py\", line 28, in \r\n> > > > _pywrap_tensorflow_internal = swig_import_helper()\r\n> > > > File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\n> > > > ternal.py\", line 24, in swig_import_helper\r\n> > > > _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> > > > File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n> > > > return load_dynamic(name, filename, file)\r\n> > > > File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n> > > > return _load(spec)\r\n> > > > ImportError: DLL load failed with error code -1073741795\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n> File \"\", line 1, in \r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow__init__.py\", line 24, in <\r\n> module>\r\n> from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python__init__.py\", line 4\r\n> 9, in \r\n> from tensorflow.python import pywrap_tensorflow\r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\r\n> \", line 74, in \r\n> raise ImportError(msg)\r\n> ImportError: Traceback (most recent call last):\r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\r\n> \", line 58, in \r\n> from tensorflow.python.pywrap_tensorflow_internal import *\r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\n> ternal.py\", line 28, in \r\n> _pywrap_tensorflow_internal = swig_import_helper()\r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_in\r\n> ternal.py\", line 24, in swig_import_helper\r\n> _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n> return load_dynamic(name, filename, file)\r\n> File \"C:\\Users\\i102604\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n> return _load(spec)\r\n> ImportError: DLL load failed with error code -1073741795\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/errors\r\n> \r\n> for some common reasons and solutions. Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> > > > quit()\r\n\r\nin advance thanks for your help. but how come to install tensorflow on python 3.7 while it is mentioned on its official website compatible with python 3.6?\r\nI have problem with cpu version!", "Could you follow the instructions listed [here](https://github.com/tensorflow/tensorflow/issues/26451#issuecomment-471079686). @nouranik In future, please don't create duplicate issues #26451. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26390\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26390\">No</a>\n", "I have the same Issue.  \r\n\r\n****\r\nImportError: DLL load failed with error code -1073741795\r\nfrom tensorflow.python import pywrap_tensorflow\r\n****\r\n\r\nI have spend 60hours trying to import. But unfortunately no success.\r\nI tried with python3.8 and python3.7 with tensorflow 1.5- 2.2 on pycharm and jupyter note book.. But no success.. \r\n\r\nCan someone  please help me ", "Performed the steps 5 times again and again....But still same error message 1073741795\r\n\r\nOS: win 7. \r\n64bit\r\n1) Uninstalled tensorflow, python, anaconda, pycharm.\r\n2) Reinstall python 3.7 and the anaconda and pycharm and tensorflow1.15.0 (tried all other versions too).\r\n3) Install  Microsoft Visual C++ Redistributable (All Versions)\r\n3) Then created new virtual environment and then activated. \r\n4) After that able to install tensorflow but \r\n5) Tried to import tensorflow. But unable to import. Gives same error 1073741795. \r\nSame error message in jupyter notebook, pycharm and in CLI.\r\n\r\nCan somebody please give me solution to this", "@azhar2ds Please create a new issue with the details of your system (fill the template). Thanks!", "I solved it with the help of this repo https://github.com/fo40225/tensorflow-windows-wheel \r\n\r\nSeems like processors that don't support AVX instructions (old processors) won't work with official build of tensorflow."]}, {"number": 26389, "title": "TF Lite toco/graph_transformations warning fix", "body": "toco/graph_transformations module warning fix", "comments": ["@shahzadlone \r\n\r\nFixed all review comments and please go through one more time. Thanks in advance.", "@Dayananda-V can you please resolve conflicts ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 26388, "title": "Bazel incompatible change: Get all tests passing with --incompatible_disable_legacy_cc_provider for Bazel 0.25.0", "body": "It seems that bazel-watcher will start breaking with the next Bazel release 0.25 because of an incompatible change (https://github.com/bazelbuild/bazel/issues/7036)\r\n\r\nIt looks like Tensorflow is still using the legacy 'cc' provider for some Starlark rules. I explained in the issue about the steps you can take to migrate to using CcInfo. The same functionality is available with that new provider.\r\n\r\nHere's the error:\r\n```\r\nERROR: tensorflow/tensorflow/lite/python/optimize/BUILD:28:1: in deps attribute of _py_wrap_cc rule //tensorflow/lite/python/optimize:tensorflow_lite_wrap_calibration_wrapper_py_wrap: '//tensorflow/lite/python/optimize:calibration_wrapper_lib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'tf_py_wrap_cc', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1743:16\r\nAnalyzing [0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\\\r\n loaded, 15887 targets configured)\r\n\r\n\r\nERROR: tensorflow/tensorflow/lite/python/optimize/BUILD:28:1: in deps attribute of _py_wrap_cc rule //tensorflow/lite/python/optimize:tensorflow_lite_wrap_calibration_wrapper_py_wrap: '@local_config_python//:python_headers' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'tf_py_wrap_cc', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1743:16\r\nAnalyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\\\r\n loaded, 15894 targets configured)\r\n\r\n\r\nERROR: tensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:lib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54\r\nAnalyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\\\r\n loaded, 18659 targets configured)\r\n\r\n\r\nERROR: tensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:lib_internal' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54\r\nAnalyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\\\r\n loaded, 18662 targets configured)\r\n\r\n\r\nERROR: tensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:version_lib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54\r\nAnalyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\\\r\n loaded, 18665 targets configured)\r\n\r\nERROR: [0mtensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core:framework_bounds_check' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54\r\nAnalyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\\\r\n loaded, 18666 targets configured)\r\n\r\nERROR: [0mtensorflow/tensorflow/core/BUILD:2731:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:framework_internal_headers_lib_gather: '//tensorflow/core/platform/default/build_config:platformlib' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54\r\nAnalyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\\\r\n loaded, 18669 targets configured)\r\n\r\nERROR: [0mtensorflow/tensorflow/core/BUILD:2842:1: in deps attribute of _transitive_hdrs rule //tensorflow/core:stream_executor_headers_lib_gather: '//tensorflow/core:stream_executor' does not have mandatory providers: 'cc'. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in tensorflow/tensorflow/tensorflow.bzl:1505:54\r\nAnalyzing:[0m target //tensorflow/tools/pip_package:build_pip_package (0 packages\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26388\">No</a>\n"]}, {"number": 26387, "title": "TF Lite operator_test non-trivial initializers not support fix", "body": "Bazel test case run operator_test non-trivial designated initializers not supported fix", "comments": []}, {"number": 26386, "title": "building error on win7 with VS2015: ADD_LIBRARY for library tf_contrib_tpu_ops without any source files", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win7 X64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version:1.13.1\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):cmake\r\n- GCC/Compiler version (if compiling from source):vs 2015\r\n- CUDA/cuDNN version:no cuda\r\n- GPU model and memory:no gpu\r\n\r\n\r\n\r\n**Describe the problem**\r\nwhen I use cmake GUI config TF, it show me the folowing error message:\r\n\"You have called ADD_LIBRARY for library tf_contrib_tpu_ops without any source files. This typically indicates a problem with your CMakeLists.txt file\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/tpu/ops\"\r\nIt looks no \"ops\" under tensorflow/contrib/tpu, anyone have idea how to find the relate source files or how to block the tpu module in config file?\r\nmany thanks~\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["We highly encourage you to use bazel to build TF, unfortunately we do not support cmake builds for TF. Thanks! ", "Thanks ymodak! \r\nDo you have a introduction of how to build TF from bazel? Could you give me a link?\r\nThanks!", "You are welcome. \r\nHere are the links you asked for,\r\nBuild from source - https://www.tensorflow.org/install/source_windows\r\nBuild from pip - https://www.tensorflow.org/install/pip  (select windows in step 1 and 2)", "Thanks ymodak,\r\nI have a try~", "Hi ymokak,\r\nI have looked the link you provided, it looks to show how to build a .whl package. What I want is build a .sln C++ project for vs2015. Since there is no folder  \"ops\" under the tensorflow/contrib/tpu, I got failure when I configure. Do you have idea on that?\r\nThanks~", "@cj741 Were you able to solve this issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi ymodak,\r\nI still have the following problem, when I generate the sln file. I think the problem is all the source files were moved to \"tensorflow/core/tpu\", do you have idea how to change the path from \"tensorflow/contrib/tpu/ops\" to \"tensorflow/core/tpu\"? \r\n\r\n\"You have called ADD_LIBRARY for library tf_contrib_tpu_ops without any source files. This typically indicates a problem with your CMakeLists.txt file\r\nCMake Error at tf_python.cmake:217 (message):\r\n  Python module not found: tensorflow/contrib/tpu/ops\"\r\n\r\nthanks,\r\nJin", "> Hi ymodak,\r\n> I still have the following problem, when I generate the sln file. I think the problem is all the source files were moved to \"tensorflow/core/tpu\", do you have idea how to change the path from \"tensorflow/contrib/tpu/ops\" to \"tensorflow/core/tpu\"?\r\n> \r\n> \"You have called ADD_LIBRARY for library tf_contrib_tpu_ops without any source files. This typically indicates a problem with your CMakeLists.txt file\r\n> CMake Error at tf_python.cmake:217 (message):\r\n> Python module not found: tensorflow/contrib/tpu/ops\"\r\n> \r\n> thanks,\r\n> Jin\r\n\r\nModify **tf_core_kernels.cmake** with all wrong path dependencies. Also **tf_core_ops.cmak**e have same path to change. \r\nI'm using Windows 10, tensorflow r1.14, Visual Studio 2015, no python bindings, no GPU,  only c++ interface.", "> @cj741 Were you able to solve this issue?\r\n\r\n[https://github.com/tensorflow/tensorflow/commit/a45d4bd80c3d93466a8daf641fe699c6d00096f2](url)"]}, {"number": 26385, "title": "TensorFlow Lite Android nightly runs inference x2 faster than latest version 1.13.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OnePlus 6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `org.tensorflow:tensorflow-lite:0.0.0-nightly` / `org.tensorflow:tensorflow-lite:1.13.1`\r\n\r\n**Describe the current behavior**\r\nI am a contributor of [this library](https://github.com/the-super-toys/glimpse-android) where we use TensorFlow lite to run a custom model.\r\nWhen using the `org.tensorflow:tensorflow-lite:0.0.0-nightly` verstion of tensorflow lite we achieve an inference time of 22ms (the model setup takes about 5ms).\r\nIf we just change the version to `org.tensorflow:tensorflow-lite:1.13.1` without changing anything else in our code, the inference time goes up to 40ms (the model setup now takes less than 1ms).\r\n\r\nWhy could this be happening? Is this a bug or the nightly version will always provide a faster inference?\r\n", "comments": ["A couple more details:\r\nIn order to measure the inference latency, we use the following snippet:\r\n```kotlin\r\nval timings = TimingLogger(\"GlimpseDebug\", \"predict\")\r\nval intpr = Interpreter(rawModel, Interpreter.Options().apply { setNumThreads(1) })\r\ntimings.addSplit(\"init interpreter\")\r\nintpr.run(inputBuffer, output)\r\ntimings.addSplit(\"inference\")\r\ntimings.dumpToLog()\r\n```\r\nThis is the output using `org.tensorflow:tensorflow-lite:0.0.0-nightly`\r\n<img width=\"212\" alt=\"captura de pantalla 2019-03-06 a las 9 30 14\" src=\"https://user-images.githubusercontent.com/4237014/53866745-8fe07c00-3ff2-11e9-8453-1721c99068e1.png\">\r\n\r\n\r\nWhile this is the output using `org.tensorflow:tensorflow-lite:1.13.1` (nothing else changes)\r\n<img width=\"205\" alt=\"captura de pantalla 2019-03-06 a las 9 32 35\" src=\"https://user-images.githubusercontent.com/4237014/53866852-d2a25400-3ff2-11e9-98cc-9339f48ac295.png\">\r\n", "I found that(on Android) differen't tensorflow-lite version would lead to different inference time, and tflight-nightly updates every day, so if I rebuild the project, I'll get totally different inference time, sometimes even the results are different. ", "@pabloppp This is a stale issue. Many bugs have been fixed in the latest TF version. Could you please execute your code using Latest Version 2.6 (and tf-nightly) and let us know if the issue still persists? \r\n\r\nAlso, please note that we are not supporting TF1.x. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26385\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26385\">No</a>\n"]}, {"number": 26384, "title": "Remove unnecessary cast", "body": "The cast to int32 is not needed here.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26384) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F26384) for more info**.\n\n<!-- ok -->"]}, {"number": 26383, "title": "tf2.0, tf.keras cannot be automatically completed in pycharm", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["Also have this issue", "Same here", "i also have this issue", "i found the solution in https://intellij-support.jetbrains.com/hc/en-us/community/posts/360002486739-PyCharm-cannot-import-tensorflow-keras.The method is use `from tensorflow.python import keras`,and it worked in my sitution", "> i found the solution in https://intellij-support.jetbrains.com/hc/en-us/community/posts/360002486739-PyCharm-cannot-import-tensorflow-keras.The method is use `from tensorflow.python import keras`,and it worked in my sitution\r\n\r\nJust a quick word of warning for anyone who is doing this workaround: This is not officially supported by TF and can break functionality with newer TF2 versions (>=2.0.0rc0). See this issue: https://github.com/tensorflow/tensorflow/issues/32646\r\n\r\nAs per a TF dev, one should never import tensorflow.python: https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546\r\n\r\nSo for anyone who suddenly experiences Keras optimizers crashing after upgrading their TF2 version, consider undoing this workaround.\r\n\r\nFor me, the code completion is fixed in PyCharm 2019.3.", "tf.keras also can't be automatically completed in VS Code. Bazel test also can't find the tf.keras. Only the tf.python.keras exists."]}, {"number": 26382, "title": "Tensorflow GPU Installation Requirement Issues", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tensorflow-gpu 1.12.0\r\n- Python version: 3.5.2\r\n- Installed using virtualenv? pip? conda?: virtualenv pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version :CUDA Version 10.0.130/ cuDNN 7.4.1\r\n- GPU model and memory: GeForce GTX 108 \r\n\r\n\r\n\r\nI am trying to install TensorFlow using \r\n`pip install --upgrade tensorflow-gpu`\r\n\r\nI end up with the following messages:\r\n\r\n`tensorflow 1.12.0 has requirement tensorboard<1.13.0,>=1.12.0, but you'll have tensorboard 1.13.1` `which is incompatible.`\r\n`symfit 0.4.6 has requirement scipy>=1.0, but you'll have scipy 0.17.0 which is incompatible.`\r\n`scikit-image 0.14.1 has requirement pillow>=4.3.0, but you'll have pillow 3.1.2 which is incompatible.`\r\n\r\n`Cannot uninstall 'enum34'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.`\r\n\r\nI am not sure how to reinstall the individual components with their correct version (tensorboard, symfit, scikit-image). \r\n", "comments": ["You can just uninstall any package by running \r\n```pip uninstall pillow``` and then you can reinstall package with required version like \r\n```pip install pillow==4.3.0```", "ok thank you!"]}, {"number": 26381, "title": "Added Average Filtering 1D", "body": "Code to test the newly added Average filtering feature. This is working fine at my end.\r\n\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.util.tf_export import tf_export\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import image_ops_impl\r\nfrom tensorflow.python.ops import script_ops\r\n\r\nfname = 'add noise  image.png'\r\nimg = matplotlib.pyplot.imread(fname)\r\nimport numpy as np\r\n\r\ntf_img = tf.convert_to_tensor(img)\r\n\r\n@tf_export('image.average_filter_1D')\r\ndef average_filter_1D(input,filter_shape=3):\r\n    \"\"\"  This methods takes 3D Tensor Images.\r\n         Other than Tensor it takes optional parameter filter_Size\r\n         Default Filter Shape = 3\r\n         This Median Filtering is done by using 1D filters of user's choice\r\n         Filter_size should be odd\r\n         This method takes both kind of images where pixel values lie between 0 to 255 and where it lies between 0.0 and 1.0\r\n    \"\"\"\r\n\r\n    input = image_ops_impl._Assert3DImage(input)\r\n    m,no,ch = int(input.shape[0]),int(input.shape[1]),int(input.shape[2])\r\n    filter_shapex = filter_shape\r\n    if m * no < filter_shapex :\r\n        raise ValueError(\"No of Pixels in each dimension of the image should be more than the filter size. Got filter_shape \"\r\n                         \"(%s)\"% filter_shape+\" Image Shape (%s)\"% input.shape)\r\n    if filter_shapex % 2 == 0 :\r\n        raise ValueError(\"Filter size should be odd. Got filter_shape (%s)\" % filter_shape )\r\n    input = math_ops.cast(input,dtypes.float64)\r\n    def my_func (input2):\r\n        tf_i = input2.reshape(m*no*ch)\r\n        maxi = max(tf_i)\r\n        if maxi == 1:\r\n            input2 /= maxi\r\n        else :\r\n            input2 /= 255\r\n        #k and l is the Zero-padding size\r\n        res = np.empty((m,no,ch))\r\n        for a in range(ch):\r\n            img = input2[:,:,a:a+1]\r\n            img = img.reshape(m * no)\r\n            k = filter_shapex - 1\r\n            img  = np.pad(img,((k / 2, k / 2)),'constant', constant_values=(0))\r\n            res1 = np.empty((m*no))\r\n            for i in range(img.shape[0] - k):\r\n                li = []\r\n                for b in range(i, i + filter_shapex):\r\n                    li.append(img[b])\r\n                res1[i] = sum(li) / filter_shapex\r\n            res1 = res1.reshape(m,no,1)\r\n            res[:,:,a:a+1] = res1\r\n        res *= 255\r\n        res = res.astype('int64')\r\n        return res\r\n\r\n    y = script_ops.py_func(my_func, [input], dtypes.int64)\r\n    return y\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nmimage = average_filter_1D(tf_img,5)\r\n\r\nfig = plt.figure()\r\nfig.add_subplot()\r\nplt.imshow(img,cmap='gray')\r\nplt.show()\r\nmimage = mimage.eval()\r\nfig.add_subplot()\r\nif mimage.shape[2] == 1:\r\n    mimage = mimage.reshape(mimage.shape[0],mimage.shape[1])\r\nplt.imshow(mimage,cmap = 'gray')\r\nplt.show()", "comments": ["Nagging Reviewer @annarev: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied.", "Isn't this equivalent to conv2d with a kernel which is just 1/n?"]}, {"number": 26380, "title": "Make weighted_cross_entropy_with_logits consistent with sigmoid_cross_entropy_with_logits", "body": "This fix tries to address the issue raised in #26337 where weighted_cross_entropy_with_logits and sigmoid_cross_entropy_with_logits are not consistent with funtion definitions.\r\n\r\nThis fix changes the definitions in v2 so that they are consistent, but keep the old defs in v1.\r\n\r\nNote this PR also fixes sparse_softmax_cross_entropy_with_logits consistency issue, as was specified in #26337.\r\n\r\nThis fix fixes #26337.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}]