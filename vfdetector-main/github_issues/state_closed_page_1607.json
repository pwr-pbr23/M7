[{"number": 4701, "title": "Unable to run \"bazel build tensorflow/examples/image_retraining:retrain\"", "body": "Hi! I was going through the re-trainer tutorial but i keep encountering the following error.\n\n<code>Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nerror: Could not expand include path '~/.gitcinclude'\nfatal: bad config file line 49 in /usr/local/git/etc/gitconfig\nTraceback (most recent call last):\n  File \"tensorflow/tools/git/gen_git_source.py\", line 254, in <module>\n    generate(args.generate)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 207, in generate\n    write_version_info(dest_file, git_version)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 160, in write_version_info\n    if b\"\\\"\" in git_version or b\"\\\" in git_version:\nTypeError: 'in <string>' requires string as left operand, not bytes\nTarget //tensorflow/examples/image_retraining:retrain failed to build\nUse --verbose_failures to see the command lines of failed build steps. </code>\n\nAm i doing something incorrectly?\n", "comments": ["This happens when using Python 3, as that code in gen_git_source.py ('b\"\\\"\" in ...' etc.) works only for Python 2.\n\nAs a workaround, I've patched my gen_git_source.py locally by removing these 'b' characters (thus passing a string into the \"in\" operation instead of a byte). However, this is probably just a workaround, as this change may make it incompatible with Python 2 and only work with Python 3.\n\nAnother workaround is to use Python 2 instead of 3.\n", "Hi @olivermg Could you advise how do you run tensorflow using Python 2? After installing with tensorflow using conda (anaconda2-4.1.1) , I proceed to clone the tensorflow repo, run /.configure before attempting to execute \"bazel build tensorflow/examples/image_retraining:retrain\". However I still get the following issues:\n\n<code>\nERROR: ../tensorflow/tensorflow/core/BUILD:1030:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nerror: Could not expand include path '~/.gitcinclude'\nfatal: bad config file line 49 in /usr/local/git/etc/gitconfig\nTraceback (most recent call last):\n  File \"tensorflow/tools/git/gen_git_source.py\", line 254, in <module>\n    generate(args.generate)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 207, in generate\n    write_version_info(dest_file, git_version)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 160, in write_version_info\n    if b\"\\\"\" in git_version or b\"\\\" in git_version:\nTypeError: 'in <string>' requires string as left operand, not bytes\nTarget //tensorflow/examples/image_retraining:retrain failed to build\n</code>\n\nThanks!\n", "I followed your post here ( https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/lOEFA0PHguA/46Za6_ouAwAJ ) and tried to set up Python 2 using conda. But i encounter similar issues.\n\n<code>\ntensorflow/tensorflow/core/BUILD:1030:1: Executing genrule //tensorflow/core:version_info_gen failed: bash failed: error executing command\n  (cd /private/var/tmp/_bazel_AMOS/ad612b9cca884ed29e9c8b7eb90ae243/execroot/tensorflow && \\\n  exec env - \\\n    PATH='/usr/local/sbin:/Users/AMOS/.rbenv/shims:/Users/AMOS/.jenv/shims:/Users/AMOS/.jenv/bin:/usr/local/Cellar/pyenv-virtualenv/1.0.0/shims:/Users/AMOS/.pyenv/shims:/Users/AMOS/.nvm/versions/node/v6.2.2/bin:/Users/AMOS/.node/bin:export /usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/share/dotnet:/Library/TeX/texbin' \\\n    TMPDIR=/var/folders/d6/7pkzyf4j25b21nqs_05n_ch80000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; tensorflow/tools/git/gen_git_source.py --generate tensorflow/tools/git/gen/spec.json tensorflow/tools/git/gen/head tensorflow/tools/git/gen/branch_ref \"bazel-out/local-opt/genfiles/tensorflow/core/util/version_info.cc\"'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nerror: Could not expand include path '~/.gitcinclude'\nfatal: bad config file line 49 in /usr/local/git/etc/gitconfig\nTraceback (most recent call last):\n  File \"tensorflow/tools/git/gen_git_source.py\", line 254, in <module>\n    generate(args.generate)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 207, in generate\n    write_version_info(dest_file, git_version)\n  File \"tensorflow/tools/git/gen_git_source.py\", line 160, in write_version_info\n    if b\"\\\"\" in git_version or b\"\\\" in git_version:\nTypeError: 'in <string>' requires string as left operand, not bytes\nTarget //tensorflow/examples/image_retraining:retrain failed to build\n</code>\n", "As far as I understand the whole process, the necessary steps to have bazel run with Python2, you have to follow these steps (assuming Anaconda):\n1. Create the Anaconda virtual env via `conda create -n tensorflow python=2.7` . A thing to note here: Don't run this command from within your cloned working copy of Tensorflow Serving, as the conda command seems to get confused when a directory \"tensorflow\" exists in the current working directory. I think it'll try to install the virtual env into that folder then, which is not what you want. Ergo: Run that conda command from within a directory in which NO directory named \"tensorflow\" (or whatever name you choose) exists.\n2. Make sure that no bazel processes exist from previous calls to it, as that may result in bazel \"remembering\" parts of the environment you had previously. So to be on the safe side, kill all potentially running bazel processes. I was actually wondering why bazel wasn't using Python2 although I was in a corresponding Anaconda environment. As I've figured out after a while, it was because bazel was obviously picking up a bazel process created earlier when I was still trying Python3.\n3. Activate the Anaconda environment you created in step 1, via `source activate tensorflow`.\n4. You should now have Python2 being the default. You can check via e.g. `pip --version` or `python --version`.\n5. (Make sure that you install the necessary pip dependencies into this virtual env, like numpy & mock, via `pip install numpy mock`. You'll probably also have to re-run the `./configure` step in the tensorflow subdir.)\n\nThis should allow you to run bazel using Python2 as far as my brief knowledge is concerned ;) .\n", "gen_git_source.py should work in Python 3 and Python 2. It used to not have the b\"  prefixes on the string literals, but @mrry added them for Windows compatibility. Looking, but the easiest work around is to just edit gen_git_source.py to remove the b\" in the string literals.\n", "Automatically closing due to lack of recent activity. Please file a new issue referring to this issue, if the issue is still relevant and important. If this is documenting some kind of bug, be sure to use the latest version of TensorFlow. Thank you so much."]}, {"number": 4700, "title": "Branch 134847453", "body": "", "comments": ["@jhseu, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @dsmilkov and @tensorflower-gardener to be potential reviewers\n"]}, {"number": 4699, "title": "Fail configure script immediately on error.", "body": "For lines such as `default_python_bin_path=$(which python)`, add a `|| true`\nexpression so that if the executable does not exist, the command will not exit\nwith a non-zero exit code, and the variable will be set to an empty string.\n\nAlso, use `yes` to automate the `./configure` script when running CI tests\nrather than piping in an empty file, which causes subsequent `read` commands\nto fail.\n\nThis change also augments the code in the configure script for resolving the\ncuDNN library to search both the case where libcudnn is installed to a lib/lib64\ndirectory (such as /usr/local/cuda-7.5/lib64) and when it is not (such as\n/usr/lib/x86_64-linux-gnu), as well as when the platform is Darwin.\n\nFixes #4662\n", "comments": ["@davidzchen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers\n", "Interesting, it looks like it is failing at the following line:\n\n``` sh\n    read -p \"Please specify which gcc should be used by nvcc as the host compiler. [Default is $default_gcc_host_compiler_path]: \" GCC_HOST_COMPILER_PATH\n```\n\nwhich is exiting with exit code `1`.\n\nIt seems that the current approach of piping in an empty file to the invocation of the configure script makes subsequent `read` commands to exit with an error code.\n\nI have added a `-d` flag to have the `configure` script use defaults when finding libraries and toolchains on the system and updating the `configured` script to invoke `configure` with that flag.\n\nAnother command that causes `configure` to fail is when the script tries to resolve `libcudnn`. I found that the script only covers the case where `libcudnn` is installed to a `lib`/`lib64` directory, such as `/usr/local/cuda-7.5/lib64`, but not case where it isn't, such as `/usr/lib/x86_64-linux-gnu`, which is, in fact, the cuDNN install dir on the GPU docker image.\n", "The tests are now passing. PTAL.\n", "The failing test in the Linux CPU Tests (Python 3) run, `//tensorflow/contrib/learn:graph_io_test`, does not appear to be related to this change.\n", "Weird, I've seen that before where graph_io_test \"fails\", but all the tests in it passed. Feel free to ignore that test and merge @yifeif.\n", "I have seen similar failure sporadically in nightly py3 builds as well. Will investigate separately. Merging. Thanks @davidzchen and @jhseu!\n"]}, {"number": 4698, "title": "freeze graph from .ckpt to .pd file fail", "body": "Now i am using tensor flow to train inception v1 (in slim model folder) on my data set. i am using the latest nightly build 0.10.\n i want to convert the big .ckpt to .pd file. i try \"freeze graph\" in the tool box but fail.\n\nthe command is: (the graph format is default pbtxt)\n## bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/home/DeepLearning/tensorflow/v1/graph.pbtxt --input_checkpoint=/home/DeepLearning/tensorflow/v1/model.ckpt-1315 --output_graph=/home/DeepLearning/tensorflow/v1/test.pb --output_node_names=softmax\n\nthe error message is:\n\nTraceback (most recent call last):\n  File \"/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 134, in <module>\n    tf.app.run()\n  File \"/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\n  File \"/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 131, in main\n    FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\n  File \"/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/tools/freeze_graph.py\", line 103, in freeze_graph\n    _ = tf.import_graph_def(input_graph_def, name=\"\")\n  File \"/home/DeepLearning/tensorflow/bazel-bin/tensorflow/python/tools/freeze_graph.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 252, in import_graph_def\n    op_def = op_dict[node.op]\n**KeyError: u'RestoreV2'**\n\n---\n\nI am not clear about **KeyError: u'RestoreV2**. is there anyone meet the same issue?\n\nthanks!\n", "comments": ["I pull the lastest code of tensorflow and rebuild by running: \n**bazel build tensorflow/python/tools:freeze_graph** \nthe issue is gone.\n"]}, {"number": 4697, "title": "Can't import reader.py correctly for rnn/ptb/", "body": "I cloned the code and downloaded the dataset. However when I run \n`python ptb_word_lm.py --data_path=simple-examples/data/ --model small`\n\nit gives the following error message:\n\nTraceback (most recent call last):\n  File \"ptb_word_lm.py\", line 369, in <module>\n    tf.app.run()\n  File \"/Users/fanyang1/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"ptb_word_lm.py\", line 329, in main\n    train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n  File \"ptb_word_lm.py\", line 93, in **init**\n    self.input_data, self.targets = reader.ptb_producer(\n**AttributeError: 'module' object has no attribute 'ptb_producer'**\n\nIt seems that for some unknown reason, the reader.py file is not imported correctly. \n\nIs this a python problem or more TensorFlow related problem? \n\nThanks!\n", "comments": ["Hi, I bet you have installed the Tensor with the version 0.10.0rc0 (with this url $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py2-none-any.whl); however you are running the latest version of source code 0.11.0rc0 which has conflict with the compiled tensorflow library version 0.10.0rc0.\n\nSolution, checkout the 0.10.0rc0 version of the source code; or compile the tensorflow package again with the latest source code.\n\nHope this help.\n", "@fanyangxyz did the comment from @kinni help?  I suspect the same problem.\n", "@kinni thanks for the comment. However, changing the source code version did not help with the import problem. \n", "@fanyangxyz, open ptb_word_lm.py and modify the header changing `from tensorflow.models.rnn.ptb import reader` to just `import reader`, there maybe some issue in your environment settings.\n\nI got the same problem, but fix it using the trick above. Hope this help.\n", "@kinni This change solved the import problem. Thanks! \nCould you talk more about the environment settings? I've encountered quite a few import problems before. It would be great if I could pinpoint the cause. \n", "There seems to be some recent changes in the PTB model which are not part of 0.11.0rc0. This include, among other things, interface changes in `tensorflow.models.rnn.ptb.reader`, in particular the `ptb_producer` function, that is not present in 0.11.0rc0. When you do `import tensorflow.models.rnn.ptb.reader` you are importing the PTB `reader` module from your installation, which is not compatible with the current PTB model in `master`; so changing the line to the relative import `import reader` works. Another option would be to checkout the tag 0.11.0rc0 in your cloned git repo.\n", " @kinni, I import reader, but it says no module named reader ", "Same problem here YananLiu. Someone pls help !!", "change ptb_producer to pt_iterator"]}, {"number": 4696, "title": "Enable to pass feed_fn to fit method", "body": "There was no way to use `fit` method to pass `feed_fn`.\n\nI tried to use [fit](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L314) method of `learn.estimator.Estimator` (actually the method is defined in its parent `BaseEstimator`).\n\nAlso I tried to use [DataFeeder](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py#L217) to feed data. I can use `input_builder` and `get_feed_dict_fn()` of `DataFeeder` instance to extract data. But when I tried to feed data to `fit` method, I couldn't pass `feed_fn`, extracted from `DataFeeder`.\n\nThis change add `feed_fn` keyword argument to `fit` method, to handle above situation.\n", "comments": ["@tyfkda, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @tensorflower-gardener and @ilblackdragon to be potential reviewers\n", "Can one of the admins verify this patch?\n", "@terrytangyuan, would you please take a look? Thanks!\n", "I think you also need to change it in Trainable with docstring. @martinwicke should be able to help more.\n", "@tyfkda  - Is there a need to support feed_fn because as @martinwicke pointed out [here](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/ZEzEa1TyYuE), `x` and `y` already support iterators by using `StreamingDataFeeder` internally. I wonder what would be use case for needing feed_fn explicitly?\n", "@abhitopia \nThank you for your information, I am going to read the thread.\n\nI am wondering that I can replace `tensorflow.contrib.learn.TensorFlowEstimator` to `Estimator`, suggested by @terrytangyuan in [#4684](https://github.com/tensorflow/tensorflow/pull/4684#issuecomment-250772081).\n`TensorFlowEstimator#fit` uses `feed_fn` ([ref.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/base.py#L255)) But I cannot do the same thing using `Estimator` because it doesn't accept `feed_fn` for `fit` method.\n", "Y\u200bou should be able to use the x and y arguments.\n", "@martinwicke - I thought about it a bit more. Although `fit()` support `x` and `y` to be iterables, it constraints them to be tensors(Dask or Numpy). On the other hand, with `input_fn`, you can pass arbitrary dictionary. Many NLP problems require `x` to be more than just one tensor. For instance, we might need to pass the length of variable length batch. How can work around this without having to use some sort of `Reader`?\n", "You can use a `py_func` inside your input_fn. There's lots of downsides to using a `py_func`, but in this case you probably don't care much. Would that work for you?\n\nI don't think we want to add more arguments to the signature of fit. \n", "I can use `Estimator#fit` with passing `x` and `y` arguments,\nso solved the problem.\n\nI am going to close this pull request.\n", "@martinwicke - Thanks. Would you be kind enough to suggest a little blueprint on writing your proposed `input_fn` using `py_func`, please? For context you can assume, there is some function `get_data()` which fetches one sample of data point from database on every call and returns `None` when one epoch is completed.\n", "That is a question best asked on StackOverflow, where others will be able\nto find it more easily.\n\nOn Thu, Oct 13, 2016 at 11:13 AM, Abhi Agg notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke - Thanks. Would you be kind\n> enough to suggest a little blueprint on writing your proposed input_fn\n> using py_func, please? For context you can assume, there is some function\n> get_data() which fetches one sample of data point from database on every\n> call and returns None when one epoch is completed.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4696#issuecomment-253593204,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_eITT_VAx8ILHAz6y-P4umDidvYAks5qznTTgaJpZM4KLgqm\n> .\n", "@martinwicke - I have posted the question here-http://stackoverflow.com/questions/40033667/how-to-pass-data-from-arbitrary-sources-not-files-though-tensorflow-contrib-le\n"]}, {"number": 4695, "title": "Refactored CMake rules to reduce redundancy.", "body": "A step towards concentrating the platform- and compiler-specific code in one place.\n", "comments": ["@mrry, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ageron, @ebrevdo and @tensorflower-gardener to be potential reviewers\n"]}, {"number": 4694, "title": "Merge release branch back into master.", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "All commits already in repo. CLA checks done on the commits.\n"]}, {"number": 4693, "title": "http://commondatastorage.googleapis.com/books1000/ no longer available", "body": "This is not tensorflow issue but an issue with the first tutorial in the udacity program https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb\n\nThe url is not found and is not accessible via a browser, tried Chrome 53 on Mac OS X 10.11\n", "comments": ["Found the right url after googling it http://yaroslavvb.com/upload/notMNIST/\n\nTutorial just needs an update\n", "That's the URL hosted on my personal server I changed it to GCS-hosted version under /books1000/ because my ISP blocked yaroslavvb.com due to too many accesses.  Someone should rehost it in a place that can take load or fix the commondatastorage address (I'm guessing it's an internal billing issue) @vincentvanhoucke \n", "I can access both:\nhttp://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz\nhttp://commondatastorage.googleapis.com/books1000/notMNIST_small.tar.gz\nWhat's not working for you? Were you trying to get to:\nhttp://commondatastorage.googleapis.com/books1000/ ? That won't work.\n", "FYI: (1) The last post on https://discussions.udacity.com/t/assignment-1-code-cell-2-error/197910 now  points to @yaroslavvb 's personal server. (2) notMNIST_large.tar.gz on books1000 is now about 30 MB smaller than the original. Did you remove broken images?\n", "I haven't touched the version on my server\n\nOn Mon, Nov 14, 2016 at 8:09 AM, Joachim Wagner notifications@github.com\nwrote:\n\n> FYI: (1) The last post on https://discussions.udacity.\n> com/t/assignment-1-code-cell-2-error/197910 now points to @yaroslavvb\n> https://github.com/yaroslavvb 's personal server. (2)\n> notMNIST_large.tar.gz on books1000 is now about 30 MB smaller than the\n> original. Did you remove broken images?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4693#issuecomment-260378912,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHCDIMX4TspHqpnztFVosfpsp0uJ0ks5q-IfSgaJpZM4KLWU0\n> .\n", "Nothing has changed in a while, I certainly haven't touched the dataset since Yaroslav put it up.\n", "Hi,\r\nI just downloaded from http://commondatastorage.googleapis.com/books1000/notMNIST_large.tar.gz\r\nand check the file size. It is 52723712. \r\nChanged that number on train_filename = maybe_download('notMNIST_large.tar.gz', 52723712) and worked.\r\n\r\nThanks\r\n", "Hi,\r\nThis is issue is because in assignment its **https** while this should be **http**.\r\nHope you got that right.\r\nThanks", "(1) Both http and https work right now. \r\n(2) I don't remember but if an https connection had been refused, I sure would have tried http as well. \r\n(3) File sizes changed again. Creating issue #12208"]}, {"number": 4692, "title": "Small fix about cuDNN version in release.md.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @tensorflower-gardener and @ilblackdragon to be potential reviewers\n"]}, {"number": 4691, "title": "Merge r0.11 branch to get any fixes in the branch.", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @rohan100jain and @keveman to be potential reviewers\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Dropping in favor of #4694 \n"]}, {"number": 4690, "title": "Small fixes to RELEASE.md", "body": "", "comments": ["@gunan, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @ilblackdragon and @tensorflower-gardener to be potential reviewers\n", "Only changing release.md, no need for running tests.\n"]}, {"number": 4689, "title": "Fixed documentation for batch_normalization", "body": "Fixed batch_normalization part of the nn python Documentation.\n\nFixed syntax errors restricting the latex code to load in the markdown documentation file for \"tf.nn.batch_normalization(...)\".\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please\n", "No problem! :) \n"]}, {"number": 4688, "title": "Unable to evaluate dense tensor obtained from sparse complex tensor", "body": "I am unable to evaluate/print/run  a dense tensor which is obtained from a complex sparse tensor.\n\na = tf.SparseTensor(indices=[[0, 0, 0], [1, 2, 1]], values=[1.0+2j, 2.0], shape=[3, 4, 2])\n\nb = tf.sparse_tensor_to_dense(a, default_value=0.0)\n\nsess  = tf.Session()\n\nsess.run(b)\n\nreturns the following error:\n\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'SparseToDense' with these attrs.  Registered kernels:\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\n  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\n  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\n\n```\n [[Node: SparseToDense_3 = SparseToDense[T=DT_COMPLEX128, Tindices=DT_INT64, validate_indices=true](SparseTensor_2/indices, SparseTensor_2/shape, SparseTensor_2/values, SparseToDense_3/default_value)]]\n```\n\nI also tried \n\nbr = tf.real(b)\n\nsess.run(br)\n\nHowever, this gives the same error as above.\n\nNote, however, that following works fine:\n\nx = tf.ones((3,3), dtype=tf.complex128)\n\nsess.run(x)\n\nI am currently using tensorflow v 0.10.0 on macosx (cpu only).\n\nI am not sure why I am unable to evaluate complex tensor obtained from sparse_tensor_to_dense operator operation. \n", "comments": ["@amrit-poudel Thanks for the clear description of the problem!\n\nThe error message indicates that `sparse_tensor_to_dense` doesn't support complex values yet; we simply haven't implemented it.  Calling sess.run on a complex dense tensor works, since it doesn't involve `sparse_tensor_to_dense`.\n\n@concretevitamin might know whether we have any plans to add support for complex values to `sparse_tensor_to_dense`.  \n", "I am not aware of any plan at the moment.  CC-ing @ebrevdo.  Contributions welcome though!\n", "We'll soon have a scatter_nd op which should subsume this behavior and work with the data type. At that point we'll probably point sparse to dense to use that kernel.\n", "Many thanks!\n", "Cool, I'll assign myself and update this issue when `scatter_nd` is in.\n", "Has there been any progress on this yet? How long do you think this might take? Thanks for the effort.\n", "I don't see a `ScatterNd` op yet.  Re-assigning to @ebrevdo to update this when available.\n", "Is there a quick fix to this without using ScatterNd? For instance, if the sparse tensor is complex, one could split into real and imaginary parts (not sure if this is available for a sparse tensor) and then call sparse_tensor_to_dense? \n", "Yes; that will work.  You will have to create two SparseTensors: one taking\nthe real values and one taking the imaginary values (the indices and shape\ntensors can be the same).\n\nOn Fri, Oct 21, 2016 at 4:21 PM, QED notifications@github.com wrote:\n\n> Is there a quick fix to this without using ScatterNd? For instance, if the\n> sparse tensor is complex, one could split into real and imaginary parts\n> (not sure if this is available for a sparse tensor) and then call\n> sparse_tensor_to_dense?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4688#issuecomment-255487589,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim1X7bQ7uvlk5xBytq6ia-R7CORWEks5q2Uj3gaJpZM4KLRmy\n> .\n", "Actually, I meant taking a real and imaginary parts of a complex sparse tensor and then performing sparse_tensor_to_dense operation on real and imaginary part separately.  It appears to me that taking real and imaginary parts of a complex sparse tensor is not possible. \n", "Are there any plans to add this op?"]}, {"number": 4687, "title": "Fixed documentation for batch_normalization", "body": "Fixed batch_normalization part of the nn python Documentation.\n\nFixed syntax errors restricting the latex code to load in the markdown documentation file for \"tf.nn.batch_normalization(...)\".\n", "comments": ["Can one of the admins verify this patch?\n", "Moved: https://github.com/tensorflow/tensorflow/pull/4689\n"]}, {"number": 4686, "title": "dynamic_rnn_decoder initial commit", "body": "In the [raw_rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1032) @ebrevdo [mentions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1043) that `raw_rnn` would be suitable to build a dynamic decoder with.\n\nThis is a dynamic decoder based on the `raw_rnn` as specified above.\n\nDiscussion points:\n- (Critical) The PR makes use of functools, is this allowed according to your standards?\n- (Critical) The PR makes use of contrib.layers, is this allowed according to the ops folder standards?\n- (Matter of taste) The PR could make use of [_extract_argmax_and_embed](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1043), but the `decoder_fn` makes it more customizable.\n- (Matter of taste) The PR requires decoder inputs while validating to access the start-of-sequence symbol (could be an issue for another PR later).\n", "comments": ["Can one of the admins verify this patch?\n", "Please note that this is related to the `seq2seq_loss` optimization from [this](https://github.com/tensorflow/tensorflow/pull/4382) PR.\n", "Hi alrojo. The dynamic decoder looks great, thanks for making it! It would be great to have at least some unit tests for it, could you add some to seq2seq_test in kernel_tests? Thanks!\n", "@lukaszkaiser Thanks for your fast response, I can look into the testing part, sure.\n\nBy `kernel_test` do you mean [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/seq2seq_test.py)? And do you have any specific guidelines or standards that I should follow, or should I just try and make something similar to what is already in the `seq2seq_test.py` file?\n", "Yes, that's exactly the test I meant. But after Eugene's comment on the other issue, I think it might be even better if you moved the code to a new tf.contrib.seq2seq module and made a test for it there. Would that be ok with you? If not, we can put it in seq2seq and then we'll move it. But it seems to make sense to have a new module where the input is always a tensor, never a list of tensors, so we can build a consistent API. Does it sound good to you?\n", "Hi alrojo. How is this PR now that the skeleton for seq2seq is there? Are you ready to rebase in contrib so I can review and we can merge this? Thanks again!\n", "Hi lukasz,\nI will update the PR with test cases for the sequence loss and rnn decoder in the new contrib folder this Saturday/Sunday.\n", "Hi alrojo,\n\nThanks for contributing! I'm starting to try your code on some real translation data (so there will be more comments).\n\nFor now, the first thing I noticed is perhaps to set the default value of encoder_projection to None & have check like:\n    if encoder_projection:\n      state = encoder_projection(initial_state, cell.output_size)\n    else:\n      state = initial_state\n(Maybe you can throw some error if the size doesn't match).\n\n-Thang\n", "Thanks for your feedback @ebrevdo, I will look into it.\n", "Lukasz and I discussed the existing API.  Here's how we think it could be improved.  First, remove the embedding stuff and make `inputs` optional.  `sequence_length` may be provided iff `inputs` is.  If `inputs` is provided, decoding is allowed to run up to the number of frames in `inputs` and no longer than that (regarldess of what `decoder_fn` says).  If `inputs` is not provided, `decoder_fn` must say when to stop for each minibatch entry.\n\nThere should be separate `decoder_fn` impls for training and for inference; see below for details on that:\n1. change the decoder_fn to have the following signature (up to you what you call the arguments and the outputs, of course; these are just placeholders)\n\n```\ndecoder_fn(time : scalar int,\n  cur_state : tensor_tuple [batch_size, ...],\n  cur_output : tensor_tuple [batch_size, ...],\n  cur_input tensor_tuple [batch_size, ...],\n  decoder_state : tensor_tuple [anything])\n-> \n(done : bool vec [batch_size],\n next_state : tensor_tuple [batch_size, ...],\n next_input : tensor_tuple [batch_size, ...],\n next_decoder_state : tensor_tuple[anything])\n```\n\nhere tensor_tuple means a (possibly nested) tuple of tensors or single tensor; and, e.g., `next_state` must have the same nested structure and shape as `cur_state` (same for `next_decoder_state`); and `next_input` should have consistent structure and shape across all calls.\n\nhere is how decoder_fn gets called:\n\n```\ntraining time (inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **Not None** (first input)\n  decoder_state: None\ntraining time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: not None (first input)\n  decoder_state: previous emitted decoder state\n\ninference time (**NO** inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **None**\n  decoder_state: None\ninference time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: **None**\n  decoder_state: previous emitted decoder state\n```\n\nhere's are the two simplest decoder_fns (one for training, one for inference):\n\n```\nsimplest case, training:\n  1. if cur_state: None, cur_output: None, cur_input: not None, decoder_state: None\n   return\n     next_state: your initial state\n     next_input: cur_input\n     done = [False] * batch_size\n     next_decoder_state: None\n  2. if cur_state: not None, cur_output: not None, cur_input: not None, decoder_state: None\n    return\n      next_state = cur_state\n      next_input: cur_input\n      done: [False] * batch_size\n      next_decoder_state: None\n```\n\n```\nsimplest case, inference:\n  1. if cur_state: None, cur_output: None, cur_input: None, decoder_state: None\n    return\n       next_state: your initial state\n       next input: your go symbol input\n       done: [False] * batch_size\n       next_decoder_state: None\n   2. if cur_state: not None, cur_output: not None, cur_input: None, decoder_state: None\n     return\n       next_state: cur_state\n       next_input: your choice, e.g. embed of argmax of cur_output\n       done: if argmax of cur output is done symbol\n       next_decoder_state: None\n```\n\nunit tests should contain these two decoder_fns.\n", "I have started writing up a simple `decoder_fn` to better understand how to support it.  And I have a question @ebrevdo @lukaszkaiser :\n\n```\ndef decoder_fn(...)\nArgs:\n`time` is current timestep, denoted `t`\n`cur_state` is the previous decoder state denoted `s_{t-1}`\n`cur_output` is ?? (same as `cur_state` or `y_{t-1}`?)\n`cur_input` is the input `x_t`\n`decoder_state` is a, possibly tuple, structure to store information between\n  timesteps (usable for beam search, a simple decoder_fn will not use it).\n\nReturns:\n`done` is a boolean vector used by raw_rnn on when to halt computation and\n  copy states\n`next_state` is the next decoder state denoted `s_t`\n`next_input` is the next input denoted `x_{t+1}`\n`next_decoder_state` is a, possibly tuple, structure to store information\n  between timesteps (usable for beam search, a simple decoder_fn will not use it).\n```\n\nWhat is the purpose of `cur_output`? Especially as we do not return a `next_output`?\nThanks\n", "Also, regarding `cur_input` as an argument to the `decoder_fn`. The raw_rnn api does not provide a `cur_input` as an argument to the [loop_fn function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1106). But I don't think we need it anyway in the decoder_fn. I will try and code something up.\n", "I think of it as follows: assume in step {t-1} we got (_, old_state, cur_input, context) = decoder_fn(...). So we have our old_state and cur_input and we run the cell, so: cur_output, cur_state = RNNCell(cur_input, old_state). Now we again run decoder_fn on (time, cur_state, cur_output, cur_input, context).\n\nNote that cur_state is actually after the RNNCell -- I think that's the only reasonable way (if you want the previous one, you can transport it in context). So decoder_fn gets access to everything the Cell produces, so it doesn't need to run it. Is that reasonable?\n", "Yes and cur_input you provide by reading from the input TensorArray.\n\nOn Oct 19, 2016 4:46 PM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> I think of it as follows: assume in step {t-1} we got (_, old_state,\n> cur_input, context) = decoder_fn(...). So we have our old_state and\n> cur_input and we run the cell, so: cur_output, cur_state =\n> RNNCell(cur_input, old_state). Now we again run decoder_fn on (time,\n> cur_state, cur_output, cur_input, context).\n> \n> Note that cur_state is actually after the RNNCell -- I think that's the\n> only reasonable way (if you want the previous one, you can transport it in\n> context). So decoder_fn gets access to everything the Cell produces, so it\n> doesn't need to run it. Is that reasonable?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-254972317,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim4d4jJzR63M16YLixpk_ojPBNZfYks5q1qvPgaJpZM4KLMiD\n> .\n", "I have tried coding the `decoder_fn` and avoiding users handling tensor_arrays. However, I kept coming back to the `decoder_fn` essentially being equal to the `loop_fn` and the `rnn_decoder` just being a wrapper for `loop_fn`.\n\nInstead (or perhaps as an extension to the `decoder_fn`), I made `rnn_decoder` into a class and modularized all tasks in the `loop_fn` call so the user can provide custom functions for `new_state`, `new_input`, `new_output`, `new_loop_state`, all with access to `self` allowing calls to input_ta and similar if needed. This will allow the user to only have to change the \"modules\" they need when injecting new code. E.g. I only had to change the `input_fn` (a few lines of code) when going from training to evaluating. And without having tested it, I also think that you only need to change `input_fn` for attention support.\n\nMy hopes is that this will achieve the equivalent of what our `decoder_fn` is described to do, but with even less code.\n\nAs a result, the decoder_train and decoder_eval are now separated (you call them with different `input_fn`) and I made an `input_fn` with the eos and embedding loop_up for early stopping.\n\nI just finished running the code with no bugs \u00bd hour ago, so I won't be able to submit a clean commit tonight, but I will be at Google tomorrow by 10 to code with @lmthang. If you want more descriptions, see the code or want to discuss feel free to come by.\n\nEDIT: here is the `loop_fn` from the \"new\" `rnn_decoder`:\n\n``` python\n    def loop_fn(time, cell_output, cell_state, loop_state):\n      elements_finished = (time >= self.sequence_length) #TODO handle seq_len=None\n      # get s_t, y_t\n      emit_output = cell_output\n      if cell_output is None:\n        next_cell_state = self.state\n      else:\n        next_cell_state = self.state_fn(self, cell_state)\n      # get x_{t+1}\n      next_input, elements_finished = self.inputs_fn(self, time, next_cell_state, elements_finished)\n      # get loop_state\n      next_loop_state = self.loop_fn(self, loop_state)\n      return (elements_finished, next_input, next_cell_state,\n              emit_output, next_loop_state)\n```\n", "Dear Alex -- if decoder_rnn becomes a simple function, that's good, right? That's the best about good abstractions, if they solve the problem and are simple, even better! Let's never add complexity for complexity's sake.\n\nI'm not convinced about your grouping of functions. For example, you enforce that state_fn only takes state. But for beam search, it'll have to have access to the context of the beam to re-group the states.\n\nHow about doing the \"easy\" version in this PR and putting it in. We can see then if it's too little and re-do if needed. I'd really prefer such incremental approach to getting in a larger new thing and only later seeing the problems.\n", "FYI, I worked with @alrojo last Friday & was able to run his rnn code on real translation data (which produces similar results as the dynamic_rnn). I have also proposed several simplifications to the code. So, looking forward to a new and cleaner version from @alrojo soon!\n", "Yes please revert to the changes we suggested. We prefer a functional style\nover complicated class heirarchies.\n\nOn Oct 21, 2016 12:16 AM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> Dear Alex -- if decoder_rnn becomes a simple function, that's good, right?\n> That's the best about good abstractions, if they solve the problem and are\n> simple, even better! Let's never add complexity for complexity's sake.\n> \n> I'm not convinced about your grouping of functions. For example, you\n> enforce that state_fn only takes state. But for beam search, it'll have to\n> have access to the context of the beam to re-group the states.\n> \n> How about doing the \"easy\" version in this PR and putting it in. We can\n> see then if it's too little and re-do if needed. I'd really prefer such\n> incremental approach to getting in a larger new thing and only later seeing\n> the problems.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-255311260,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim6dRCeBiRwcdVUK5hK5eNVceAoBEks5q2GbCgaJpZM4KLMiD\n> .\n", "Hi Alexander,\n\nThanks again for working on this! Looking forward to the new API we\ndiscussed yesterday (I can quickly test it today if you have some initial\nversion ready).\n\n-Thang\n\nOn Wed, Oct 19, 2016 at 5:44 PM ebrevdo notifications@github.com wrote:\n\nYes and cur_input you provide by reading from the input TensorArray.\n\nOn Oct 19, 2016 4:46 PM, \"Lukasz Kaiser\" notifications@github.com wrote:\n\n> I think of it as follows: assume in step {t-1} we got (_, old_state,\n> cur_input, context) = decoder_fn(...). So we have our old_state and\n> cur_input and we run the cell, so: cur_output, cur_state =\n> RNNCell(cur_input, old_state). Now we again run decoder_fn on (time,\n> cur_state, cur_output, cur_input, context).\n> \n> Note that cur_state is actually after the RNNCell -- I think that's the\n> only reasonable way (if you want the previous one, you can transport it in\n> context). So decoder_fn gets access to everything the Cell produces, so it\n> doesn't need to run it. Is that reasonable?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-254972317\n> ,\n> or mute the thread\n> <\n> https://github.com/notifications/unsubscribe-auth/ABtim4d4jJzR63M16YLixpk_ojPBNZfYks5q1qvPgaJpZM4KLMiD\n> \n> .\n\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorflow/tensorflow/pull/4686#issuecomment-254980175,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAYNRXgI4YJ7yz3sP8vekzzSijsr5mdqks5q1rlbgaJpZM4KLMiD\n.\n", "I will make a push (without attention, this might take until the weekend as I am currently traveling) that should work, including all of our wish-list changes in a few hours. It will be without official TestCase style kernel_tests however.\n\nThree comments:\n@ebrevdo : We have tried to make `rnn_decoder` a class instance in order to maximize user-friendliness (went through this with @lmthang this Friday). The reason is we need to access class variables when writing user-friendly `decoder_fn`'s.\n\n@lmthang : I don't think we can have `sequence_length` be `None`, this could cause the decoder to get struck and eventually run OOM. Instead I have made it possible to just supply an integer value instead, representing the \"maximal_allowed\" amount of steps to decoder.\n\nDo you have any specific wishes for the `kernel_tests`?\n", "It was super nice meeting the team, thanks for having me!\n\n@lmthang you can try to run the code form the new commit, should support `inputs=None`. Made an extensive docstring for the `rnn_decoder` on how to use it.\n", "> we had discussed a clean API on friday; but this seems like a big diversion from that.\n\nand\n\n> why does decoder_fn need self?\n\nFrom previous answer:\n\n> I have tried coding the `decoder_fn` and avoiding users handling tensor_arrays. However, I kept coming back to the `decoder_fn` essentially being equal to the `loop_fn` and the `rnn_decoder` just being a wrapper for `loop_fn`.\n> \n> Instead (or perhaps as an extension to the `decoder_fn`), I made `rnn_decoder` into a class and modularized all tasks in the `loop_fn` call so the user can provide custom functions for `new_state`, `new_input`, `new_output`, `new_loop_state`, all with access to `self` allowing calls to input_ta and similar if needed. This will allow the user to only have to change the \"modules\" they need when injecting new code. E.g. I only had to change the `input_fn` (a few lines of code) when going from training to evaluating. And without having tested it, I also think that you only need to change `input_fn` for attention support.\n> \n> My hopes is that this will achieve the equivalent of what our `decoder_fn` is described to do, but with even less code.\n> \n> As a result, the decoder_train and decoder_eval are now separated (you call them with different `input_fn`) and I made an `input_fn` with the eos and embedding loop_up for early stopping.\n> \n> I just finished running the code with no bugs \u00bd hour ago, so I won't be able to submit a clean commit tonight, but I will be at Google tomorrow by 10 to code with @lmthang. If you want more descriptions, see the code or want to discuss feel free to come by.\n> \n> EDIT: here is the `loop_fn` from the \"new\" `rnn_decoder`:\n> \n> ``` python\n>     def loop_fn(time, cell_output, cell_state, loop_state):\n>       elements_finished = (time >= self.sequence_length) #TODO handle seq_len=None\n>       # get s_t, y_t\n>       emit_output = cell_output\n>       if cell_output is None:\n>         next_cell_state = self.state\n>       else:\n>         next_cell_state = self.state_fn(self, cell_state)\n>       # get x_{t+1}\n>       next_input, elements_finished = self.inputs_fn(self, time, next_cell_state, elements_finished)\n>       # get loop_state\n>       next_loop_state = self.loop_fn(self, loop_state)\n>       return (elements_finished, next_input, next_cell_state,\n>               emit_output, next_loop_state)\n> ```\n\nThen given Lukasz Answer:\n\n> I'm not convinced about your grouping of functions. For example, you enforce that state_fn only takes state. But for beam search, it'll have to have access to the context of the beam to re-group the states.\n\nI changed the interface to take the entire `loop_fn` instead (named  `decoder_fn`) with access to `self` instead of the four functions used above to allow maximal customization. Leaving the `rnn_decoder` a wrapper for `raw_rnn` to handle things like [this](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/seq2seq.py#L258).\n\nThe reason for access to self is to get access to some of the processed variables in `rnn_decoder`, such as: `inputs_ta`, `sequence_length`, `input_depth`, `batch_size`, `dtype`, `state`, `time_major` (will be used for attention) and future variables.\n\n> decoder_fn should not know anything about inputs_ta\n\nOften the main difference between training, evaluating, attention is to switch the input part. For this we need access to `inputs_ta`, [see here](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/decoder_fn.py#L52).\n\n> variables are accessible via vs.get_variable(); there's no need to store them anywhere\n\nand\n\n> batch_size can be inferred from the input shapes.\n\nAlternatively to using classes and `self` to store information, I guess we could either allow `rnn_decoder` to pass a `**kwargs` argument or store all of the above in `tf.Variable`'s and then use `vs.get_variable();`? (I might need to read a bit more about using `vs.get_variable` for the latter).\n\nPlease let me know what you think on the current architecture of having `rnn_decoder` as a wrapper for `raw_rnn` with `decoder_fn` supplying the `loop_fn` and if yes, if we should use a `**kwargs` architecture or `tf.Variable` architecture to send variables in-between the `rnn_decoder` and `decoder_fn` instead of using classes and `self`.\n", "@ebrevdo: maybe you can help @alrojo  (and perhaps me) here. If it seems too complicated to talk through github, I'll meet you offline.\n\nMaybe, let's look at the two functions `loop_fn_train` (line 39) and `loop_fn` (for eval, line 87) in this [file](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/decoder_fn.py). Are these good starting points to talk?\n", "> @ebrevdo: maybe you can help @alrojo (and perhaps me) here. If it seems too complicated to talk through github, I'll meet you offline.\n> \n> Maybe, let's look at the two functions loop_fn_train (line 39) and loop_fn (for eval, line 87) in this file. Are these good starting points to talk?\n\nI will be on standby with further development of the `kernel_tests` and the `decoder_fn_attention` until we have agreed on an interface between the `rnn_decoder` and `decoder_fn` then.\n\nI am also available on skype/hangout/appear.in today and from Friday onwards in the morning hours (as I will be gmt +1 from Friday).\n", "@lmthang and i will meet tomorrow and discuss the changes.\n", "[dynamic_decoder.txt](https://github.com/tensorflow/tensorflow/files/557353/dynamic_decoder.txt)\n\nHi @alrojo,\n\nI worked with @lukaszkaiser and @ebrevdo. It's a bit tricky to get things right in terms of the API we discussed last time. But I finally have some concrete realization of the API for you to look at (see the attached file, or [here](https://codeshare.io/XBTzz) for some syntax highlighting). This is only for training, see decoder_fn_train at the end. Writing decoder_fn_eval will be more interesting, which the attached code does give some hints. Maybe you can take a look to see if the code make sense and if you can help write decoder_fn_eval?\n\nThanks!\n-Thang\n", "Nice, great work! I like that you are moving the `encoder_state` to the `decoder_fn` as future seq2seq models might have different initialization approaches.\n\nYou might want to consider having the `decoder_fn` as classes to allow user-specific parameters to be passed.\nE.g. the code provided in [codeshare link](https://codeshare.io/XBTzz) would not work if you had not defined `encoder_state` somewhere above.\nBelow is `decoder_fn_train` and `decoder_fn_eval`, we should consider also providing an abstract class like the [RNNCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L87).\n\n``` python\nclass decoder_fn_train():\n\n  def __init__(self, encoder_state):\n    self.encoder_state = encoder_state\n\n  def __call__(self, cell_state, cell_output, context_state):\n    if cell_state == None: # first call, return encoder_state\n      return (None, self.encoder_state, None, cell_output, context_state)\n    else:\n      return (None, cell_state, None, cell_output, context_state)\n\nclass decoder_fn_eval():\n\n  def __init__(self, encoder_state, embeddings, sos_id, eos_id,\n               output_fn=None, dtype=tf.int32):\n    self.encoder_state = encoder_state\n    self.embeddings = embeddings\n    self.dtype = dtype\n    self.output_fn = output_fn\n    self.sos_id = tf.Variable(sos_id, dtype=self.dtype)\n    self.eos_id = tf.Variable(eos_id, dtype=self.dtype)\n\n    self.batch_size = array_ops.shape(self.encoder_state)[0]\n    # If you have an output projection on your cell, set output_fn to None\n    if self.output_fn is None:\n      self.output_fn = lambda x: x\n\n  def __call__(self, cell_state, cell_output, context_state):\n\n    if cell_output == None:\n      # invariant that this is time == 0\n      next_input_id = tf.ones([self.batch_size], dtype=self.dtype) * self.sos_id\n      done = tf.zeros([self.batch_size,], dtype=tf.bool)\n      cell_state = self.encoder_state\n    else:\n      next_input_id = tf.argmax(self.output_fn(cell_output), 1)\n      # set to None as I provide sequence_length when debugging\n      done = None#tf.equal(next_input_id, self.eos_id)\n    next_input = tf.gather(self.embeddings, next_input_id)\n\n    return (done, cell_state, next_input, cell_output, context_state)\n```\n\nOne note:\nAs pointed in earlier post regarding allowing `sequence_lengths=None`:\n\n> I don't think we can have sequence_length be None, this could cause the decoder to get stuck and eventually run OOM. Instead I have made it possible to just supply an integer value, representing the \"maximal_allowed\" amount of steps to decoder.\n\nBy allowing `sequence_lengths = None` we rely on the `decoder_fn_eval` to eventually return `True` for all sequences.\nImagine validating with an untrained network on a word-to-word model with 80k classes.\nThen you would have a 1 in 80k chance of \"randomly\" reaching the `eos` symbol.\n\nEDIT:\nThe alternative \"max_int\" value I refered in previous post to is used [here](https://github.com/tensorflow/tensorflow/pull/4686/commits/15eba5e1160ab34d2940a916bc9de8f8502317a6#diff-193a5806e6330e98e72ce0499036b9a4R261)\n\nFrom here you can provide a logical \"or\" on the time>=max_seq_len to allow earlier stopping.\n\nEDIT:\nIs the `context_state` tuple only used for infering `batch_size` or will it be needed for later for beam search?\n", "Regarding attention, we would need to \"concat\" something extra to the input.\n\nWe can do this by either adding the input to the `decoder_fn` or by reformulating the following:\n\n``` python\n    # call decoder function\n    if inputs is not None: # training\n      # get next_cell_input\n      if cell_state == None:\n        next_cell_input = inputs_ta.read(0)\n      else:\n        batch_size = array_ops.shape(done)[0]\n        next_cell_input = control_flow_ops.cond(\n            tf.equal(time, max_time),\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n            lambda: inputs_ta.read(time))\n      (next_done, next_cell_state, _, emit_output, next_context_state) = (\n          decoder_fn(cell_state, cell_output, context_state))\n```\n\ninto the following\n\n``` python\n    # call decoder function\n    if inputs is not None: # training\n      # get next_cell_input\n      if cell_state == None:\n        next_cell_input = inputs_ta.read(0)\n      else:\n        batch_size = array_ops.shape(done)[0]\n        next_cell_input = control_flow_ops.cond(\n            tf.equal(time, max_time),\n            lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n            lambda: inputs_ta.read(time))\n      (next_done, next_cell_state, additional_input, emit_output, next_context_state) = (\n          decoder_fn(cell_state, cell_output, context_state))\n      if additional_input is not None:\n          # concat along feature dimension\n          next_cell_input = tf.concat(concat_dim=-1, [next_cell_input, additional_input])\n```\n", "You don't need a class, just a function that accepts the encoder_state and\ncreates an appropriate decoder_fn with the encoder state available via\nclosure. That creator function can return the new decoder_fn to the user.\n\nOn Oct 29, 2016 11:04 AM, \"Alexander Rosenberg Johansen\" <\nnotifications@github.com> wrote:\n\n> Regarding attention, we would need to \"concat\" something extra to the\n> input.\n> \n> We can do this by either adding the input to the decoder_fn or by\n> reformulating the following:\n> \n> ```\n> # call decoder function\n> if inputs is not None: # training\n>   # get next_cell_input\n>   if cell_state == None:\n>     next_cell_input = inputs_ta.read(0)\n>   else:\n>     batch_size = array_ops.shape(done)[0]\n>     next_cell_input = control_flow_ops.cond(\n>         tf.equal(time, max_time),\n>         lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n>         lambda: inputs_ta.read(time))\n>   (next_done, next_cell_state, _, emit_output, next_context_state) = decoder_fn(cell_state, cell_output, context_state)\n> ```\n> \n> into\n> \n> ```\n> # call decoder function\n> if inputs is not None: # training\n>   # get next_cell_input\n>   if cell_state == None:\n>     next_cell_input = inputs_ta.read(0)\n>   else:\n>     batch_size = array_ops.shape(done)[0]\n>     next_cell_input = control_flow_ops.cond(\n>         tf.equal(time, max_time),\n>         lambda: array_ops.zeros([batch_size, input_depth], dtype=dtype),\n>         lambda: inputs_ta.read(time))\n>   (next_done, next_cell_state, additional_input, emit_output, next_context_state) = decoder_fn(cell_state, cell_output, context_state)\n>   if additional_input is not None:\n>       # concat along feature dimension\n>       tf.concat(concat_dim=-1, [next_cell_input, additional_input])\n> ```\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-257106172,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim5DkMDEtNGiCBWvmGMSXTDEYByb0ks5q44mrgaJpZM4KLMiD\n> .\n", "Hi @alrojo,\n\nThanks for the update! Let's use functions instead of classes as @ebrevdo suggested (see below - you did something similar before).\n\n``` python\ndef decoder_fn_train(encoder_state):\n  def decoder_fn(cell_state, cell_output, context_state):\n    if cell_state == None: # first call, return encoder_state\n      return (None, encoder_state, None, cell_output, context_state)\n    else:\n      return (None, cell_state, None, cell_output, `context_state)\n  return decoder_fn  \n```\n\nI agree about the max_len at inference, which I think you can pass to the wrapper as below:\n\n``` python\ndef decoder_fn_eval(encoder_state, embeddings, sos_id, eos_id, max_len):\n  def decoder_fn(time, cell_state, cell_output, context_state):\n\n  return decoder_fn  \n```\n\nAbove I added back the `time` variable to decoder_fn (so that you can compare with max_len), which means we need to update the arguments for decoder_fn_train & dynamic_decoder as well. \n\nPerhaps to keep things simple for now, let's not use output_fn & worry about attention later.\n\nIf these sound good, maybe you can update your commits so that @ebrevdo can check and I can test on real data.\n\n-Thang\n", "@lmthang @ebrevdo I have updated the code for the PR accordingly to your comments (with no docstrings and kernel_tests for now, will come when we settle on the code).\n\nGiven we handle the `done` vector in the `decoder_fn` for evaluation, we might want to consider doing the same for training, so supplying `sequence_lengths` in `decoder_fn_train` instead of [this](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/seq2seq.py#L89).\n\nAlso, which name do you prefer for the decoder? `dynamic_decoder`, `rnn_decoder` or something else?\n", "I agree it seems cleaner to provide sequence_lengths to the decoder_fn instead of having dynamic_decoder do it.  However, it also makes the dynamic_decoder that much more complicated (and it's already quite complicated).  What do you think?  I'm open on this one; happy to either have the dynamic_decoder take care of it when `inputs` is not `None`, but also happy to let the `decoder_fn` take care of it.\n", "Is this ready to review?  (Let us know when it is so we know when to re-review)\n", "@vrv: will let you know soon!\n\n@alrojo @ebrevdo: I'd propose to keep sequence_lengths in dynamic_decoder so users don't have to bother handling sequence_lengths in decoder_fn_train. I find it natural (and logically clear) to have (inputs, decoder_lengths) go in pair: they both have to be not None at training time & both need to be None at inference time. We should make sure we enforce those constraints, Alexander.\n\nAs Alexander asked, shall we standardize names as below?\ndynamic_rnn_decoder\ndecoder_fn_train\ndecoder_fn_inference\n(Of course, feel free to prepend the names of decoder_fn like you did Alexander like simple_decoder_fn_train, simple_decoder_fn_inference)\n\n-T\n", "I like the name of the decoder.  I'm wondering how everyone feels about\nhaving decoder_fn_{train,inference} be wrapped in a single function that\nreturns the correct one depending on a python bool is_training?\n\nOn Mon, Oct 31, 2016 at 12:57 PM, Thang Luong notifications@github.com\nwrote:\n\n> @vrv https://github.com/vrv: will let you know soon!\n> \n> @alrojo https://github.com/alrojo @ebrevdo https://github.com/ebrevdo:\n> I'd propose to keep sequence_lengths in dynamic_decoder so users don't have\n> to bother handling sequence_lengths in decoder_fn_train. I find it natural\n> (and logically clear) to have (inputs, decoder_lengths) go in pair: they\n> both have to be not None at training time & both need to be None at\n> inference time. We should make sure we enforce those constraints, Alexander.\n> \n> As Alexander asked, shall we standardize names as below?\n> dynamic_rnn_decoder\n> decoder_fn_train\n> decoder_fn_inference\n> (Of course, feel free to prepend the names of decoder_fn like you did\n> Alexander like simple_decoder_fn_train, simple_decoder_fn_inference)\n> \n> -T\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-257402462,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimznOy42cXfEK6FrAZt25NK0mJ7lkks5q5kgRgaJpZM4KLMiD\n> .\n", "I'm very happy with the recent changes, but I'd prefer not to wrap too much the different decoder_fn's at this point. They are only in tests for now, right? Let's make this PR as simple as possible and get it in.\n", "Yes, I think it's better to keep decode_fn_{train,inference} as separated functions for now since they take different arguments.\n", "SGTM for keeping them separate functions.  still not sure i understand why\nthe train function doesn't accept an embeddings dict but the inference one\ndoes?\n\nOn Mon, Oct 31, 2016 at 1:17 PM, Thang Luong notifications@github.com\nwrote:\n\n> Yes, I think it's better to keep decode_fn_{train,inference} as separated\n> functions for now since they take different arguments.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-257408136,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimwEdbK7cbw4Uda-WYdLRvytv7rqFks5q5kzxgaJpZM4KLMiD\n> .\n", "@ebrevdo: good question! It might be worth thinking (maybe later).\n\nRight now, at training time (when `inputs` is not None), users can pass whatever inputs they want (embedding lookup vectors or any vectors, etc). Our `simple_decoder_fn_inference` happens to be a specific decoder_fn that uses argmax for cell_output to get discrete words & then do embedding lookup from the argmax results (so we made an implicit assumption that at training time, `inputs` passed to the `dynamic_rnn_decoder` was looked up from the same `embeddings` matrix). \n\nFrom that point of view, I think our current `dynamic_rnn_decoder` seems fine. Those decoder_fn_{train, inference} are just examples of a specific usage.\n", "Okay, I assume we are all good on the code now and I will start writing docstrings followed by `kernel_tests`.\n", "@alrojo: One thing about the `maximum_length`, I was wondering we should remove this line `maximum_length = array_ops.constant(maximum_length, dtype=dtype)` in the `decoder_fn_interference`? The reason is users might want to have the maximum_length determined at run time, e.g., twice as long as the maximum encoder lengths `tf.round(2 * tf.reduce_max(encoder_lengths))`.\n", "While writing the tests I found that our current code does not support explicit `batch_size` because the `batch_size` is inferred [dynamically](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/seq2seq.py#L89). Thus the input loses shape during training.\n\nPlease see [commit](https://github.com/tensorflow/tensorflow/pull/4686/commits/be166a7df2023fe5600731dbf199839953694a46) if you agree with fix-up @ebrevdo .\n", "I have commited an initial draft of the seq2seq_test, please have a look @ebrevdo . Thanks.\n", "I want to confirm that I've tested the argmax decoder on a real dataset and it produces sensible results. Hopefully this PR gets done soon!\n", "I will make another big commit for Thursday. Thanks for your patience :)\n\nThe current architecture is dependent on the output projection wrapper. If we remove it, we need to reinstate the output_fn.\n\nThe value comparison for the test case is a little complicated as we don't know the embeddings chosen by the random initialized decoder under inference. But maybe if we switch the order and do inference first. I will try it out.\n", "For the training decoder, you don't need output_fn do you?  You just run\nthe training decoder to get some values, and then matmul those to get your\nfinal output (when training).  No?  Am I wrong?\n\nOn Tue, Nov 8, 2016 at 3:16 PM, Alexander Rosenberg Johansen <\nnotifications@github.com> wrote:\n\n> I will make another big commit for Thursday. Thanks for your patience :)\n> \n> The current architecture is dependent on the output projection wrapper. If\n> we remove it, we need to reinstate the output_fn.\n> \n> The value comparison for the test case is a little complicated as we don't\n> know the embeddings chosen by the random initialized decoder under\n> inference. But maybe if we switch the order and do inference first. I will\n> try it out.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-259290141,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim8kR6Tcudw0XDngT6JvD9zOmB4W6ks5q8QLmgaJpZM4KLMiD\n> .\n", "i.e., you do:\n\noutputs, ... = dynamic_rnn_decoder(cell, training_fn, inputs)\nprojected_outputs = tf.matmul(outputs, embedding_matrix)\n\nOn Tue, Nov 8, 2016 at 3:23 PM, Eugene Brevdo ebrevdo@gmail.com wrote:\n\n> For the training decoder, you don't need output_fn do you?  You just run\n> the training decoder to get some values, and then matmul those to get your\n> final output (when training).  No?  Am I wrong?\n> \n> On Tue, Nov 8, 2016 at 3:16 PM, Alexander Rosenberg Johansen <\n> notifications@github.com> wrote:\n> \n> > I will make another big commit for Thursday. Thanks for your patience :)\n> > \n> > The current architecture is dependent on the output projection wrapper.\n> > If we remove it, we need to reinstate the output_fn.\n> > \n> > The value comparison for the test case is a little complicated as we\n> > don't know the embeddings chosen by the random initialized decoder under\n> > inference. But maybe if we switch the order and do inference first. I will\n> > try it out.\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-259290141,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/ABtim8kR6Tcudw0XDngT6JvD9zOmB4W6ks5q8QLmgaJpZM4KLMiD\n> > .\n", "Yes, it is only needed for inference.\n", "For inference you can just put the projection into the decoder_fn right?\n\nOn Tue, Nov 8, 2016 at 4:28 PM, Alexander Rosenberg Johansen <\nnotifications@github.com> wrote:\n\n> Yes, it is only needed for inference.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-259302889,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimwElH6qsmj2B7T7CxSHPxEQYqMpsks5q8ROvgaJpZM4KLMiD\n> .\n", "Hi @alrojo,\n\nTo elaborate @ebrevdo's point, here's a concrete piece of code that works:\n\n``` python\nlogits = layers.linear(cell_output, num_decoder_symbols, scope=scope)\nnext_input_id = math_ops.cast(math_ops.argmax(logits, 1), dtype=dtype)\n```\n\nto replace the following line in `simple_decoder_fn_inference`:\n\n``` python\nnext_input_id = math_ops.cast(math_ops.argmax(cell_output, 1), dtype=dtype)\n```\n\nNote that `layers` is from contrib.layers.python.layers.\n", "Hi @lmthang and @ebrevdo \n\nI have updated the `seq2seq_test` using `output_fn` as a projection instead of the cell wrapper.\nPlease note I also made some minor changes to the `simple_decoder_fn_inference` function in two separate commits.\n\n> To elaborate, here's a concrete piece of code that works:\n> \n> logits = layers.linear(cell_output, num_decoder_symbols, scope=scope)\n> next_input_id = math_ops.cast(math_ops.argmax(logits, 1), dtype=dtype)\n> to replace the following line in simple_decoder_fn_inference:\n> \n> next_input_id = math_ops.cast(math_ops.argmax(cell_output, 1), dtype=dtype)\n> Note that layers is from contrib.layers.python.layers\n\nSure, I can put `layers.linear(cell_output, num_decoder_symbols, scope=scope)` in, instead of the `output_fn(cell_output)`.\n\nQuestion:\n\nHow would I train the projection? I assume it should be accessible both while training and validating. Perhaps take example in the newly updated [`seq2seq_test`](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/kernel_tests/seq2seq_test.py)?\n\nAlso, I am available on Skype at github_alrojo if you are interested in doing some pair programming! :)\n", "Hi @alrojo,\n\nAt training time, you can also do something similar (replacing lines 78-83 in your seq2seq_test):\n\n``` python\noutputs = layers.linear(outputs, num_decoder_symbols, scope=decoder_scope)\n```\n\nMake sure you pass in the same scope for both training and testing. \n\nDoes that help?\n", "@lmthang \nSo instead of passing the projection function explicitly, we use the scopes to share weights with a projection matrix?\n\nI can try and make that work, yes.\n\nEdit: Removed comment on `layers.linear`\n", "Hi @alrojo,\n\nOn the question of whether we use function or scope to share the projection matrix, perhaps @ebrevdo or @lukaszkaiser can better answer? I'm using scope in my own testing code right now.\n\nOn your comments about layers.linear, sorry, I'm not sure if I follow. I thought the code does what we want: It first reshape the 3D tensor to [-1, last_dimension] [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1314), do the transformation, and put back into the original 3D shape [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L1335).\n\n-Thang\n", "> On your comments about layers.linear, sorry, I'm not sure if I follow. I thought the code does what we want: It first reshape the 3D tensor to [-1, last_dimension] here, do the transformation, and put back into the original 3D shape here.\n\n@lmthang You are completely right, my apologies!\n", "As for scopes vs function, I'm fine with both, I think scope is slightly easier but do just any way that works for you.\n", "Hi @alrojo,\n\nContinuing from my earlier code, one more thing you should add to make the `simple_decoder_fn_inference` correct:\n\n``` python\nif cell_output == None:\n  ...\n  cell_output = array_ops.zeros(num_decoder_symbols, dtype=dtypes.float32)\nelse:\n  ...\n  cell_output = logits\n```\n\nThis part `array_ops.zeros(num_decoder_symbols, dtype=dtypes.float32)` is a temporary hack so that `raw_rnn` can get the right output size. Also, we ideally want to return the word indices (beside the logits), but this is slightly tricky to do as we need to update `raw_rnn`. We (=TF team) will address these issues later once this PR is approved.\n\n-Thang\n", "I added the `next_input`.\n\n> Continuing from my earlier code\n\nOkay, I will implement that tomorrow.\n", "Hi @lmthang \n\nThe `layers.linear` is now used [instead](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/kernel_tests/seq2seq_test.py#L58).\n\nThe `cell_output` now uses the [logits projected](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/decoder_fn.py#L82).\n\nNote: I had to add an extra argument, `num_decoder_symbols`, to the `simple_decoder_fn_inference`.\n\nPlease let me know if you have more code blocks that you would like to change.\n\nFurthermore, I assume the only major challenge left (before writing docstrings and building) is the equality test mentioned by @ebrevdo ? (please see latest \"requested changes\").\n", "@alrojo, the changes look great to me! thanks for your help!\n\n-Thang\n", "@alrojo: one more thing, the way we obtain the `batch_size` from `encoder_state` in `simple_decoder_fn_inference` is not correct when encoder_state is a tuple, e.g., when using BasicLSTMCell. Perhaps, we can pass `batch_size` as another argument to that `simple_decoder_fn_inference`?\n", "You can also flatten the state using nest.flatten and get the batch size\nfrom the first entry.\n\nOn Nov 11, 2016 10:30 PM, \"Thang Luong\" notifications@github.com wrote:\n\n@alrojo https://github.com/alrojo: one more thing, the way we obtain the\nbatch_size from encoder_state in simple_decoder_fn_inference is not correct\nwhen encoder_state is a tuple, e.g., when using BasicLSTMCell. Perhaps, we\ncan pass batch_size as another argument to that simple_decoder_fn_inference?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tensorflow/tensorflow/pull/4686#issuecomment-260105241,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtimzx49HzZ9xnSrVu03FFgFLKxhj5wks5q9Vz8gaJpZM4KLMiD\n.\n", "@lmthang @ebrevdo we now have support for `BasicLSTMCell`. However, to make it work I had to include this [hack](https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/decoder_fn.py#L36) to the init of the decoder functions.\n", "That's fine.  We'll clean this up after you push.\n\nOn Mon, Nov 14, 2016 at 7:51 AM, Alexander Rosenberg Johansen <\nnotifications@github.com> wrote:\n\n> @lmthang https://github.com/lmthang @ebrevdo\n> https://github.com/ebrevdo we now have support for BasicLSTMCell.\n> However, to make it work I had to include this hack\n> https://github.com/alrojo/tensorflow/blob/dynamic_decoder/tensorflow/contrib/seq2seq/python/ops/decoder_fn.py#L36\n> to the init of the decoder functions.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-260372976,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimwFQiYxEPZT98UUeZnFljT9Wjri8ks5q-IN0gaJpZM4KLMiD\n> .\n", "Let's keep it as is for now.\n\nOn Mon, Nov 14, 2016 at 9:06 AM, Alexander Rosenberg Johansen <\nnotifications@github.com> wrote:\n\n> ## _@alrojo_ commented on this pull request.\n> \n> In tensorflow/contrib/seq2seq/python/kernel_tests/seq2seq_test.py\n> https://github.com/tensorflow/tensorflow/pull/4686:\n> \n> > -            [decoder_outputs_train, decoder_state_train])\n> > -        decoder_outputs_inference_res, decoder_state_inference_res = sess.run(\n> > -            [decoder_outputs_inference, decoder_state_inference])\n> >   +\n> > -        # Assert outputs\n> > -        self.assertEqual((decoder_sequence_length, batch_size, output_size),\n> > -                         decoder_outputs_train_res.shape)\n> > -        self.assertEqual((batch_size, decoder_size),\n> > -                         decoder_outputs_inference_res.shape[0:2])\n> > -        self.assertEqual((batch_size, decoder_size),\n> > -                         decoder_state_train_res.shape)\n> > -        self.assertEqual((batch_size, decoder_size),\n> > -                         decoder_state_inference_res.shape)\n> > -        # The dynamic decoder might end earlier than `maximal_length`\n> > -        # under inference\n> > -        true_value = (decoder_sequence_length>=\n> \n> @ebrevdo https://github.com/ebrevdo would you like to keep kernel_tests\n> as is, or modify dynamic_rnn_decoder to also return the sequence_lengths\n> in order to support value assertion? (as elaborated above)\n> Thanks\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686, or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim2d7TV0hovsRCRLQkSftxFydiKl6ks5q-JUCgaJpZM4KLMiD\n> .\n", "@alrojo Can you rebase, please?\n", "@rmlarsen done.\n\nI assume the code is finalized now. Do you want me to continue with making docstrings? In such case I plan on having it ready by the mid/end of the week.\n", "Hi @lmthang @ebrevdo @lukaszkaiser \n\nI have pushed the docstrings.\n", "Hello, I have tested the latest commit of this pull request, and find that an error occurs when using with `MultiRNNCell` of LSTMCell.\nA single-layer LSTMCell seems to work fine (I've tested the LSTMCell with state_is_tuple option), however when I wrap the cell with MultiRNNCell, the following exception is raised.\n\n```\nValueError: The two structures don't have the same number of elements. \nFirst structure: Tensor(\"Train/Model/simple_decoder_fn_train/packed:0\", shape=(4, 2, ?, 1024), dtype=float32),\nsecond structure: (LSTMStateTuple(c=1024, h=1024), LSTMStateTuple(c=1024, h=1024), LSTMStateTuple(c=1024, h=1024), LSTMStateTuple(c=1024, h=1024)).\n```\n\nEDIT:\nI found that the problem is due to the following lines.\n\n``` python\n    with tf.name_scope(name=name, default_name='simple_decoder_fn_train',\n                       values=[encoder_state]):\n        if type(encoder_state) is not tf.nn.rnn_cell.LSTMStateTuple:\n            encoder_state = tf.convert_to_tensor(encoder_state)\n```\n\nWhen we use MultiRNNCell of LSTMCell, the type of its state is not LSTMStateTuple but tuple, and consequently, it passes the if condition and the state is packed into a [num_layers, 2, batch_size, hidden_dim] tensor.\nThus, I think we need to detect whether the cell is LSTMCell (with state_is_tuple option) using a smarter way.\n\nBelow are my clumsy workaround codes.\n\n``` python\n    with tf.name_scope(name=name, default_name='simple_decoder_fn_train',\n                       values=[encoder_state]):\n        if (not isinstance(encoder_state, tf.nn.rnn_cell.LSTMStateTuple)\n                and not isinstance(encoder_state, tuple)):\n            encoder_state = tf.convert_to_tensor(encoder_state)\n```\n", "Hi @jihunchoi , thanks for pointing this out!\n\nYes, it is because `tf.convert_to_tensor` changes the RNNCell to a tensor. I assume the reason why we use `tf.convert_to_tensor` was because the user might supply a numpy matrix instead.\n\nI thought about your solution as well, but I found it overengineering the data structures from the `RNNCell`. Instead I removed the `tf.convert_to_tensor`. Your code should run just fine now.\n", "Let's get this change in and we can fix it afterwards.\n\nOn Fri, Nov 18, 2016 at 1:14 PM, Alexander Rosenberg Johansen <\nnotifications@github.com> wrote:\n\n> Hi @jihunchoi https://github.com/jihunchoi , thanks for pointing this\n> out!\n> \n> Yes, it's because that tf.convert_to_tensor changes the RNNCell to a\n> tensor. I assume the reason why we use tf.convert_to_tensor was because\n> the user might supply a numpy matrix instead? (@ebrevdo\n> https://github.com/ebrevdo?)\n> \n> I thought about your solution as well, but I found it overengineering the\n> data structures from the RNNCell. Instead I removed the\n> tf.convert_to_tensor. Your code should run just fine now.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-261643391,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtimzc-t1kjhoSHC_J1-05YloDrLDmdks5q_hUsgaJpZM4KLMiD\n> .\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Hi @lukaszkaiser @ebrevdo @lmthang ,\r\n\r\nUnless you have more change request, the code is ready to be tested and submitted.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "@tensorflow-jenkins test this please.", "@ebrevdo could you perhaps help me dissect the Windows Cmake Tests failures from @tensorflow-jenkins ? Thanks", "It looks like the errors are not associated with the code in this PR? @ebrevdo ", "Yeah you can ignore those errors.\n\nOn Thu, Nov 24, 2016 at 9:53 AM, Alexander Rosenberg Johansen <\nnotifications@github.com> wrote:\n\n> It looks like the errors are not associated with the code in this PR?\n> @ebrevdo <https://github.com/ebrevdo>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-262825378>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim00416XQKoTFpKMG9m8ISKpk-LVvks5rBc8xgaJpZM4KLMiD>\n> .\n>\n", "Failures are unrelated.  OK to merge.", "@tensorflow-jenkins test this please.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "@ebrevdo I have redesigned and tested with the Linux CPU build, it seems to work now.\r\n\r\nOn specifics of implementation:\r\n\r\n>this leads to an error:\r\n>\r\n>name, \"\".join(traceback.format_list(tb))))\r\nValueError: Variable root/rnn/gru_cell/gates/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n>\r\n>due to a change i recently pushed. this is because you're using the \"rnn\" scope twice. what you should do is:\r\n>\r\n>with tf.variable_scope(\"rnn\") as scope:\r\ntf.nn.dynamic_rnn(..., scope=scope)\r\n>\r\n>scope.set_reuse()\r\n... = tf.contrib.seq2seq.dynamic_rnn_decoder(..., scope=scope)\r\n...\r\n... = tf.contrib.seq2seq.dynamic_rnn_decoder(..., scope=scope)\r\n\r\nI did the following instead:\r\n\r\n```\r\nwith tf.variable_scope(\"rnn\") as scope:\r\n  tf.nn.dynamic_rnn(..., scope=scope)\r\n\r\nwith tf.variable_scope(\"decoder\") as scope:\r\n  ... = tf.contrib.seq2seq.dynamic_rnn_decoder(..., scope=scope)\r\n  ...\r\n  scope.reuse_variables()\r\n  ...\r\n  ... = tf.contrib.seq2seq.dynamic_rnn_decoder(..., scope=scope)\r\n```\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "@tensorflow-jenkins Test this please", "Awesome ..! Thanks for a superb collaboration @lukaszkaiser @ebrevdo @lmthang :)", "Thanks for your help @alrojo! Much appreciated. Stay tune for more changes that will really make training seq2seq easier."]}, {"number": 4685, "title": "ResourceExhaustedError when formating ImageNet data to TFRecord", "body": "Hi All Tensorflow users,\n\nI followed the steps in https://github.com/tensorflow/models/tree/master/inception to build the imagenet to TFRecord format. But when I execute the command \n\"bazel-bin/inception/download_and_preprocess_imagenet \"${DATA_DIR}\"\", \nI have the following errors:\n\nException in thread Thread-6:\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/threading.py\", line 811, in **bootstrap_inner\n  File \"/usr/lib64/python2.7/threading.py\", line 764, in run\n  File \"/home/rengan/.cache/bazel/_bazel_rengan/2c9ddf610cf302ceda3f2b852669382c/execroot/inception/bazel-out/local-fastbuild/bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py\", line 388, in _process_image_files_batch\n    image_buffer, height, width = _process_image(filename, coder)\n  File \"/home/rengan/.cache/bazel/_bazel_rengan/2c9ddf610cf302ceda3f2b852669382c/execroot/inception/bazel-out/local-fastbuild/bin/inception/build_imagenet_data.runfiles/inception/inception/data/build_imagenet_data.py\", line 316, in _process_image\n    image_data = tf.gfile.FastGFile(filename, 'r').read()\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 102, in read\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 72, in _preread_check\n  File \"/usr/lib64/python2.7/contextlib.py\", line 24, in __exit**\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\nResourceExhaustedError: /cm/shared/scratch/rengan/Data/imagenet-data/raw-data/train//n03124170/n03124170_675.JPEG\n\nThere are similar errors in other threads. It successfully generated 128 validation data but less than 100 training data (should be 1024).\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?: no related issues were found.\n### Environment info\n\nOperating System: Redhat 7.2\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): f98c5ded31d7da0c2d127c28b2c16f0307a368f0\n2. The output of `bazel version`: \nWARNING: Output base '/home/rengan/.cache/bazel/_bazel_rengan/2c9ddf610cf302ceda3f2b852669382c' is on NFS. This may lead to surprising failures and undetermined behavior.\n.\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Sep 29 22:19:27 2016 (1475187567)\nBuild timestamp: 1475187567\nBuild timestamp as int: 1475187567\n\nThanks in advance.\n", "comments": ["Hi,\n\nI have a same kind of issue when trying to download and preprocess the MSCOCO dataset used in im2txt model  (https://github.com/tensorflow/models/tree/master/im2txt). It seems that the script `bazel-bin/im2txt/download_and_preprocess_mscoco \"${MSCOCO_DIR}\"` is able to process up to 1024 images but after that all the threads fail:\n\n```\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/threading.py\", line 811, in __bootstrap_inner\n  File \"/usr/lib64/python2.7/threading.py\", line 764, in run\n  File \"bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py\", line 278, in _process_image_files\n    sequence_example = _to_sequence_example(image, decoder, vocab)\n  File \"bazel-bin/im2txt/download_and_preprocess_mscoco.runfiles/im2txt/im2txt/data/build_mscoco_data.py\", line 214, in _to_sequence_example\n    encoded_image = f.read()\n  File \"<PYTHONDIR>/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 99, in read\n  File \"<PYTHONDIR>/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 72, in _preread_check\n  File \"/usr/lib64/python2.7/contextlib.py\", line 24, in __exit__\n  File \"<PYTHONDIR>/lib/python2.7/site-packages/tensorflow/python/framework/errors.py\", line 463, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\nResourceExhaustedError: <DATA_DIR>/models/im2txt/im2txt/data/mscoco/raw-data/train2014/COCO_train2014_000000069569.jpg\n```\n\nApparently the script does not close the file handles it uses properly which leads to this kind of unfortunate situation.\n\nOperating System: CentOS Linux release 7.2.1511 (Core)\nCUDA 7.5\ncuDNN 5.0\nTensorFlow master branch\n", "Ah, looks like there is a missing call to writer.close() in models/im2txt/im2txt/data/build_mscoco_data.py\nI'll send out a fix shortly.\n", "Thanks for pointing out this issue. I added the writer.close() in models/inception/inception/data/build_imagenet_data.py and the problem was solved. \n", "@cshallue can you tell us how to patch things while you work on a fix?\n", "I have pushed the fix to this branch:\nhttps://github.com/cshallue/models/tree/writer\n\nI will merge into tensorflow_models/master soon.\n", "@cshallue if all that is different is adding writer.close() on line 289 of build_mscoco_data.py, then this is not solving it for me. I am having the same exact problem as @huttunensami.\n", "Have you tried reducing the number of threads?\n", "@cshallue I am running with 1 thread by changing line 133 in build_mscoco_data.py\n\ntf.flags.DEFINE_integer(\"num_threads\", 1,\n                        \"Number of threads to preprocess the images.\") \n", "I've pushed the patch to call writer.close(), which fixes the issue for @hfutxrg\n\nI'm not sure yet why the issue still exists for @siavashk\n", "@cshallue I will continue digging for the next hour to see if I can get it to work, and if I do, I will reply on the same thread. In case it helps, I am building this on a cloud instance with a K2. The compute capability of these cards is 3.0, which is what I configured Tensorflow with.\n", "Looking more closely at the stack trace above, it's clear that the error is thrown while reading an image from disk:\n  with tf.gfile.FastGFile(image.filename, \"r\") as f:\n    encoded_image = f.read()\n\nIt sounds like you've run out of memory. Is there any way you can increase that on your cloud instance?\n", "I have a GPU instance from Softlayer, the available memory is 64GBs. Even though it is highly unlikely, I checked this before posting on the thread using top.\n", "I wonder if you get still get an error if you use the native Python open() method instead of tf.gfile.FastGFile()?\n", "Good point. I tried it, processing is slow but seems to be working so far with 8 thread. Previously it would fail immediately. Did we just find a bug in Tensorflow IO?\n\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K2, pci bus id: 0000:83:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GRID K2, pci bus id: 0000:84:00.0)\nLaunching 8 threads for spacings: [[0, 73296], [73296, 146592], [146592, 219888], [219888, 293184], [293184, 366480], [366480, 439776], [439776, 513072], [513072, 586368]]\n2016-09-30 17:54:34.244437 [thread 0]: Processed 1000 of 73296 items in thread batch.\n2016-09-30 17:54:56.926579 [thread 4]: Processed 1000 of 73296 items in thread batch.\n2016-09-30 17:55:03.010200 [thread 1]: Processed 1000 of 73296 items in thread batch.\n2016-09-30 17:55:03.078273 [thread 6]: Processed 1000 of 73296 items in thread batch.\n2016-09-30 17:55:06.125806 [thread 5]: Processed 1000 of 73296 items in thread batch.\n2016-09-30 17:55:06.261466 [thread 3]: Processed 1000 of 73296 items in thread batch.\n2016-09-30 17:55:06.557369 [thread 2]: Processed 1000 of 73296 items in thread batch.\n2016-09-30 17:55:06.630173 [thread 7]: Processed 1000 of 73296 items in thread batch.\n", "This might indeed be a bug in tf.gfile.FastGFile().\nPlease let me know if it finishes successfully for you now (the preprocessing usually takes 1+ hours).\n", "@cshallue It worked!\n", "Great! I will push a patch shortly to unblock everyone, and I'll close this issue.\nI'll follow up next week about tf.gfile.FastGFile(), and work on that in a separate issue.\n"]}, {"number": 4684, "title": "Fix skflow resnet example to work on v0.10.0", "body": "I tried to run `resnet.py` skflow example on TensorFlow v0.10.0,\nbut an error occurred.\n\nI have fixed the code to run on v0.10.0:\n- `tensorflow.contrib.learn.ops.conv2d` was deprecated and has been removed in #4373 .\n  So changed to use `contrib.layers.convolution2d` .\n- `classifier` wasn't created if no previous model existed in `models/resnet/graph.pbtxt`,\n  but it was failed to run at first time.\n  `TensorFlowEstimator` creation was removed in #2863 , so revert the code removal.\n", "comments": ["Can one of the admins verify this patch?\n", "@tyfkda, thanks for your PR! By analyzing the annotation information on this pull request, we identified @gideonite, @vrv and @martinwicke to be potential reviewers\n", "Could you change to use Estimator and make sure the changes work with the version on master branch?\n", "@terrytangyuan ,\nCan I make your suggestion in other pull request to keep this change simple?\n\nAccording to your suggestion, I am trying to modify the code to use `Estimator` instead of `TesorFlowEstimator`, but it looks the class lacks functionality. Especially I cannot pass `feed_fn` to it, which is used in [TensorFlowEstimator#fit](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/base.py#L254).\n\nI have sent a pull request to add the functionality (#4696). When it is accepted, I will proceed to modify the code to use `Estimator`.\n", "Sure, perhaps could you follow other examples that are being tested (you can take a look at the shell script in examples folder) so this example is being tested going forward? You might need to make sure it doesn't take forever to run, using the smaller dataset provided in TF.Learn.datasets. \n", "@terrytangyuan \nI've added a commit which adds resnet.py to the shell script.\nI use `100` for test count, I am not sure it is good for CI.\n", "Test failed, I should add the target into `BUILD`.\nI've fixed the commit.\n", "@tensorflow-jenkins test this please\n", "@terrytangyuan Would you please take a look?\nTest failed because the code used `tensorflow.examples.tutorials` package.\n\nI have changed the code to use `learn.datasets.load_dataset`,\nreferred mnist.py example.\n", "Looks like there are some conflicts. @tyfkda could you resolve them? Thanks!\n", "@yifeif Thank you for the notice, I have fixed the conflict, would you please take a look?\n\nIt seems examples have moved from tensorflow/examples/skflow to tensorflow/examples/learn in d74cc130dd0ce5ecd1421ef126e4446f337a856c (#3364),\nso I moved resnet.py example accordingly.\n", "@martinwicke Would you please take a look? Thanks!\n", "@martinwicke Would you please take a look? Thanks!\n", "Jenkins, test this please!\n", "Looks great, thanks for doing this!\n"]}, {"number": 4683, "title": "sigmoid_cross_entropy shape compatible check bug (setting n_classes=2 in DNNClassifier)", "body": "### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root    558720 Sep 15 07:02 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root        16 Sep 15 07:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root        19 Sep 15 07:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rw-r--r-- 1 root root    415432 Sep 15 07:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root    775162 Sep 15 07:02 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 xj   users       13 Jul 27 13:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 xj   users       17 Jul 27 13:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 xj   xj    78065952 Apr 23 03:17 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rwxrwxr-x 1 xj   users 79337624 Jul 27 13:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-rw-r-- 1 xj   users 69756172 Jul 27 13:53 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   bad7c50b9dc9789ad7dd0a62daca40b7269841ed\n2. The output of `bazel version`\n   .\n   Build label: 0.3.1\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Fri Jul 29 09:09:52 2016 (1469783392)\n   Build timestamp: 1469783392\n   Build timestamp as int: 1469783392\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nfrom **future** import absolute_import\nfrom **future** import division\nfrom **future** import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib as contrib\nimport numpy as np\n# Data sets\n\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\n# Load datasets.\n\ntraining_set = contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TRAINING,\n                                                                target_dtype=np.int,\n                                                                features_dtype=np.float32)\ntest_set = contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TEST,\n                                                            target_dtype=np.int,\n                                                            features_dtype=np.float32)\n# Specify that all features have real-value data\n\nfeature_columns = [contrib.layers.real_valued_column(\"\", dimension=4)]\n# Build 3 layer DNN with 10, 20, 10 units respectively.\n\nclassifier = contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=2,\n                                            model_dir=\"/tmp/iris_model\")\n# Fit model.\n\nclassifier.fit(x=training_set.data,\n               y=training_set.target.astype(np.int),\n               steps=200)\n", "comments": ["This seems to be a duplicate of #4715 \n", "Indeed this looks like a duplicate of #4715 - closing this out so that we can focus on the fix over there.\n"]}, {"number": 4682, "title": "Running tensorboard with vz-projector demo in development mode", "body": "Is it possible to actually test the `vz-projector` element in the tensorboard components ? \nI know it's a bleeding edge feature but I'd like to give it a try to see how it works but I get the following error while running `gulp`\n\n```\nTypeScript error: components/vz-projector/data.ts(170,24): Error TS2345: Argument of type '{ metadata: { [key: string]: number | string; }; index: number; vector: number[]; projectedPoint:...' is not assignable to parameter of type 'DataPoint[]'.\n  Type '{ metadata: { [key: string]: number | string; }; index: number; vector: number[]; projectedPoint:...' is not assignable to type 'DataPoint'.\n    Types of property 'projections' are incompatible.\n      Type '{}' is not assignable to type '{ [key: string]: number; }'.\n        Index signature is missing in type '{}'.\nTypeScript error: components/vz-projector/vz-projector-bookmark-panel.ts(74,5): Error TS2346: Supplied parameters do not match any signature of call target.\nTypeScript error: components/vz-projector/vz-projector-bookmark-panel.ts(88,7): Error TS2339: Property 'download' does not exist on type 'HTMLAnchorElement'.\n```\n", "comments": ["@dsmilkov: Daniel, could you take a look?  Thanks.\n", "I manage to make the tsne demo working with the following correction in the `data.ts` file\n\n```\ngetSubset(subset?: number[]): DataSet {\n    let pointsSubset = subset ? subset.map(i => this.points[i]) : this.points;\n    let points = pointsSubset.map(dp => {\n      return {\n        metadata: dp.metadata,\n        index: dp.index,\n        vector: dp.vector.slice(),\n        projectedPoint: [0, 0, 0] as [number, number, number],\n        projections: {} as {[key: string]: number;}\n      };\n    });\n```\n\nThe next problem is that I get `numeric is not defined` when running PCA.\n", "Hi,\n\nI appreciate your interest, however the `vz-projector` is not yet released, so it is not possible to test it yet. We plan to release it soon (no promises as to exactly when). When we release it, we will add a documentation in the \"How to\" section next to the other TensorBoard pages.\n\nAs for the broken compile, we are fixing it internally today - the update should show up in 1-2 days on GitHub. Thanks for bringing the issue to our attention.\n", "Hi @dsmilkov. I'm testing the latest verison of the embedding project which looks nice. I'm wondering how can I create a demo tensors in .bytes format ?"]}, {"number": 4681, "title": "restore previous checkpoint and continue training", "body": "i am beginner in tensorflow and i have a problem of restoring my previous checkpoint, after adding a new output for my network. So what i want to do after restoring, add a new label. what is the process to do that ? any idea about that \n", "comments": ["This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorflow` tag.\n"]}, {"number": 4680, "title": "./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nI'm trying to build the pi-examples by building the tensorflow via makefile on Linux(Ubuntu 14.04).\n\nSo 1st I've done \"Building on Linux\" successfully from here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#building-on-linux\n\nnow at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples I'm trying to build camera example, but when I've run `make -f tensorflow/contrib/pi_examples/camera/Makefile` command, the result came out as follows on terminal:\n\n`gcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/downloads -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/downloads/eigen-latest/ -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/gen/proto/ -I/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/camera/camera.cc -o /home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/gen/obj/tensorflow/contrib/pi_examples/camera/camera.o\nIn file included from ./tensorflow/core/framework/tensor.h:19:0,\n                 from tensorflow/contrib/pi_examples/camera/camera.cc:33:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"unsupported/Eigen/CXX11/Tensor\"\n                                          ^\ncompilation terminated.\nmake: *** [/home/tushar/Desktop/tushar/tensorflow/tensorflow/contrib/pi_examples/camera/gen/obj/tensorflow/contrib/pi_examples/camera/camera.o] Error 1\n`\n\nWhy? could anyone confirm me that third-party library specially (eigen3) is complete or something missing.\n\nI've also tried with replacing it's third-party eigen3 library by separately downloaded library, but that create new error, but similar to this (i.e. XYZ...   No such file or directory)\n", "comments": ["I have had the same issue today as well.  \n", "+1\n", "@maciekcc: could you take a look at this?  Thanks.\n", "Yes,My Pi also encountered this problem,and I found in the ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor ,just one line in this file:\n#include \"unsupported/Eigen/CXX11/Tensor\",Is that due to some UNcompleted compile?\n", "Hi, I had the same problem for my Mac with ios_example, when I launched it from xcode, I got error:\n#include \"unsupported/Eigen/CXX11/Tensor\"                 it said that it can not find the Tensor file.\nSo I just followed here, to add this line #include \"unsupported/Eigen/CXX11/Tensor\" inside  the Tensor file(which is located in: tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor), but when I run it again in xcode, it did not work, I got the same error.\n\nHere is the content for the Tensor file:\n\n// This file is part of Eigen, a lightweight C++ template library\n// for linear algebra.\n//\n// Copyright (C) 2014 Benoit Steiner benoit.steiner.goog@gmail.com\n// Copyright (C) 2013 Christian Seiler christian@iwakd.de\n//\n// This Source Code Form is subject to the terms of the Mozilla\n// Public License v. 2.0. If a copy of the MPL was not distributed\n// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n//#ifndef EIGEN_CXX11_TENSOR_MODULE\n//#define EIGEN_CXX11_TENSOR_MODULE\n\n#include \"unsupported/Eigen/CXX11/Tensor\"\n#include \"../../../Eigen/Core\"\n\n#include <Eigen/src/Core/util/DisableStupidWarnings.h>\n\n#include \"../SpecialFunctions\"\n#include \"src/util/CXX11Meta.h\"\n#include \"src/util/MaxSizeVector.h\"\n\n/*\\* \\defgroup CXX11_Tensor_Module Tensor Module\n  *\n- This module provides a Tensor class for storing arbitrarily indexed\n- objects.\n  *\n- \\code\n- #include <Eigen/CXX11/Tensor>\n- \\endcode\n  */\n\n#include <cmath>\n#include <cstddef>\n#include <cstring>\n\n#ifdef _WIN32\ntypedef __int32 int32_t;\ntypedef unsigned __int32 uint32_t;\ntypedef __int64 int64_t;\ntypedef unsigned __int64 uint64_t;\n#else\n#include <stdint.h>\n#endif\n\n#if __cplusplus > 199711 || EIGEN_COMP_MSVC >= 1900\n#include <random>\n#endif\n\n#ifdef _WIN32\n#include <windows.h>\n#elif defined(__APPLE__)\n#include <mach/mach_time.h>\n#else\n#include <time.h>\n#endif\n\n#ifdef EIGEN_USE_THREADS\n#include \"ThreadPool\"\n#endif\n\n#ifdef EIGEN_USE_GPU\n#include <iostream>\n#include <cuda_runtime.h>\n#if defined(**CUDACC**)\n#include <curand_kernel.h>\n#endif\n#endif\n\n#include \"src/Tensor/TensorMacros.h\"\n#include \"src/Tensor/TensorForwardDeclarations.h\"\n#include \"src/Tensor/TensorMeta.h\"\n#include \"src/Tensor/TensorFunctors.h\"\n#include \"src/Tensor/TensorCostModel.h\"\n#include \"src/Tensor/TensorDeviceDefault.h\"\n#include \"src/Tensor/TensorDeviceThreadPool.h\"\n#include \"src/Tensor/TensorDeviceCuda.h\"\n#include \"src/Tensor/TensorIndexList.h\"\n#include \"src/Tensor/TensorDimensionList.h\"\n#include \"src/Tensor/TensorDimensions.h\"\n#include \"src/Tensor/TensorInitializer.h\"\n#include \"src/Tensor/TensorTraits.h\"\n#include \"src/Tensor/TensorUInt128.h\"\n#include \"src/Tensor/TensorIntDiv.h\"\n#include \"src/Tensor/TensorGlobalFunctions.h\"\n\n#include \"src/Tensor/TensorBase.h\"\n\n#include \"src/Tensor/TensorEvaluator.h\"\n#include \"src/Tensor/TensorExpr.h\"\n#include \"src/Tensor/TensorReduction.h\"\n#include \"src/Tensor/TensorReductionCuda.h\"\n#include \"src/Tensor/TensorArgMax.h\"\n#include \"src/Tensor/TensorConcatenation.h\"\n#include \"src/Tensor/TensorContractionMapper.h\"\n#include \"src/Tensor/TensorContractionBlocking.h\"\n#include \"src/Tensor/TensorContraction.h\"\n#include \"src/Tensor/TensorContractionThreadPool.h\"\n#include \"src/Tensor/TensorContractionCuda.h\"\n#include \"src/Tensor/TensorConversion.h\"\n#include \"src/Tensor/TensorConvolution.h\"\n#include \"src/Tensor/TensorFFT.h\"\n#include \"src/Tensor/TensorPatch.h\"\n#include \"src/Tensor/TensorImagePatch.h\"\n#include \"src/Tensor/TensorVolumePatch.h\"\n#include \"src/Tensor/TensorBroadcasting.h\"\n#include \"src/Tensor/TensorChipping.h\"\n#include \"src/Tensor/TensorInflation.h\"\n#include \"src/Tensor/TensorLayoutSwap.h\"\n#include \"src/Tensor/TensorMorphing.h\"\n#include \"src/Tensor/TensorPadding.h\"\n#include \"src/Tensor/TensorReverse.h\"\n#include \"src/Tensor/TensorShuffling.h\"\n#include \"src/Tensor/TensorStriding.h\"\n#include \"src/Tensor/TensorCustomOp.h\"\n#include \"src/Tensor/TensorEvalTo.h\"\n#include \"src/Tensor/TensorForcedEval.h\"\n#include \"src/Tensor/TensorGenerator.h\"\n#include \"src/Tensor/TensorAssign.h\"\n#include \"src/Tensor/TensorScan.h\"\n\n#include \"src/Tensor/TensorExecutor.h\"\n#include \"src/Tensor/TensorDevice.h\"\n\n#include \"src/Tensor/TensorStorage.h\"\n#include \"src/Tensor/Tensor.h\"\n#include \"src/Tensor/TensorFixedSize.h\"\n#include \"src/Tensor/TensorMap.h\"\n#include \"src/Tensor/TensorRef.h\"\n\n#include \"src/Tensor/TensorIO.h\"\n\n#include <Eigen/src/Core/util/ReenableStupidWarnings.h>\n\n//#endif // EIGEN_CXX11_TENSOR_MODULE\n\nAny help, thanks in advance\n", "@maciekcc any update please?\n", "https://github.com/tensorflow/tensorflow/compare/master...magicwifi:patch-1\n\n@tusharsoni08 \n", "I've also just encountered this on a Raspberry Pi 3. \n\nAfter success with https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile hit the issue as above when trying to make as specified in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/pi_examples\n", "Ok I've just tried the makefile fix @magicwifi [suggested](https://github.com/tensorflow/tensorflow/compare/master...magicwifi:patch-1) above (but in pi_examples/_label_image rather than pi_examples/camera), i.e. the reference to \"eigen-latest/\" becomes \"eigen/\".  \n\nI now have a working binary in tensorflow/contrib/pi_examples/label_image/gen/bin/label_image :)\n", "@magicwifi  After tried your's suggestion I've `undefined reference to google::protobuf::` related lots of errors.\n", "@tusharsoni08\n I followed the Raspberry Pi section of the instructions at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#Raspberry Pi\n In RPI3 and worked well;\nAnd I suggested you follow\n https://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md \n", "@magicwifi What actually i'm doing is I'm using PI examples to build that on linux (Ubuntu 14.04) for testing by using the https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#building-on-linux rather https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#raspberry-pi . \n", "@magicwifi \nThank you for finding the makefile issue with the pi examples.  I started with a clean instance of the latest raspbian, followed the Pi instructions , edited the example makefiles and everything worked fine.\n", "@j0el  cool\uff01\n", "@tusharsoni08  a little different when compling some package between in ubuntu and in raspbian\n", "changing the reference to \"eigen-latest/\" -> \"eigen/\" in the makefile totally worked! thanks @magicwifi \n", "I chane just like @magicwifi  said and there is a another error\n\n```\npi@raspberrypi:~/tensorflow/tensorflow $ make -f tensorflow/contrib/pi_examples/label_image/Makefile\ngcc --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads  -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o\ntensorflow/contrib/pi_examples/label_image/label_image.cc:31:48: fatal error: tensorflow/core/framework/graph.pb.h: No such file or directory\n #include \"tensorflow/core/framework/graph.pb.h\"\n                                                ^\ncompilation terminated.\ntensorflow/contrib/pi_examples/label_image/Makefile:80: recipe for target '/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o' failed\nmake: *** [/home/pi/tensorflow/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o] Error 1\n```\n\nI can't find graph.pb.h\n", "Hi, did you resolve this problem with :#include \"unsupported/Eigen/CXX11/Tensor\"\n\nThanks\n", "Hi, just change the \"eigen-latest/\" -> \"eigen/\" in the makefile\n\nbut now I facing with the error  \"#include \"tensorflow/core/framework/graph.pb.h\" with pi zero\n", "I followed the instructions on a clean Raspberry Pi build and ran into this issue, which I fixed using the patch to the Makefile, but now am getting this when trying to compile the label_image program as per the instructions:\n\n``` bash\n$ make -f tensorflow/contrib/pi_examples/label_image/Makefile  CXX=g++-4.8\ng++-4.8 --std=c++11 -O0 -I/usr/local/include -I. -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/downloads/eigen/ -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto/ -I/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/../../makefile/gen/proto_text/ -c tensorflow/contrib/pi_examples/label_image/label_image.cc -o /home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o\nIn file included from tensorflow/contrib/pi_examples/label_image/label_image.cc:26:0:\n/usr/include/jpeglib.h:774:3: error: \u2018size_t\u2019 does not name a type\n   size_t free_in_buffer; /* # of byte spaces remaining in buffer */\n   ^\n/usr/include/jpeglib.h:786:3: error: \u2018size_t\u2019 does not name a type\n   size_t bytes_in_buffer; /* # of bytes remaining in buffer */\n   ^\nIn file included from /usr/include/jpeglib.h:27:0,\n                 from tensorflow/contrib/pi_examples/label_image/label_image.cc:26:\n/usr/include/jpeglib.h:817:3: error: \u2018size_t\u2019 has not been declared\n   JMETHOD(void *, alloc_small, (j_common_ptr cinfo, int pool_id,\n   ^\n/usr/include/jpeglib.h:819:3: error: \u2018size_t\u2019 has not been declared\n   JMETHOD(void FAR *, alloc_large, (j_common_ptr cinfo, int pool_id,\n   ^\nIn file included from tensorflow/contrib/pi_examples/label_image/label_image.cc:26:0:\n/usr/include/jpeglib.h:964:34: error: \u2018size_t\u2019 has not been declared\n EXTERN(void) jpeg_CreateCompress JPP((j_compress_ptr cinfo,\n                                  ^\n/usr/include/jpeglib.h:966:36: error: \u2018size_t\u2019 has not been declared\n EXTERN(void) jpeg_CreateDecompress JPP((j_decompress_ptr cinfo,\n                                    ^\n/usr/include/jpeglib.h:974:30: error: \u2018FILE\u2019 has not been declared\n EXTERN(void) jpeg_stdio_dest JPP((j_compress_ptr cinfo, FILE * outfile));\n                              ^\n/usr/include/jpeglib.h:975:29: error: \u2018FILE\u2019 has not been declared\n EXTERN(void) jpeg_stdio_src JPP((j_decompress_ptr cinfo, FILE * infile));\n                             ^\ntensorflow/contrib/pi_examples/label_image/label_image.cc: In function \u2018tensorflow::Status LoadJpegFile(std::string, std::vector<unsigned char>*, int*, int*, int*)\u2019:\ntensorflow/contrib/pi_examples/label_image/label_image.cc:108:32: error: cannot convert \u2018FILE* {aka _IO_FILE*}\u2019 to \u2018int*\u2019 for argument \u20182\u2019 to \u2018void jpeg_stdio_src(j_decompress_ptr, int*)\u2019\n   jpeg_stdio_src(&cinfo, infile);\n                                ^\ntensorflow/contrib/pi_examples/label_image/label_image.cc: In function \u2018int main(int, char**)\u2019:\ntensorflow/contrib/pi_examples/label_image/label_image.cc:318:3: error: \u2018vector\u2019 was not declared in this scope\n   vector tensorflow::Flag > flag_list = {\n   ^\ntensorflow/contrib/pi_examples/label_image/label_image.cc:318:3: note: suggested alternatives:\nIn file included from /usr/include/c++/4.8/vector:64:0,\n                 from tensorflow/contrib/pi_examples/label_image/label_image.cc:29:\n/usr/include/c++/4.8/bits/stl_vector.h:210:11: note:   \u2018std::vector\u2019\n     class vector : protected _Vector_base<_Tp, _Alloc>\n           ^\n/usr/include/c++/4.8/bits/stl_vector.h:210:11: note:   \u2018std::vector\u2019\ntensorflow/contrib/pi_examples/label_image/label_image.cc:318:10: error: expected \u2018;\u2019 before \u2018tensorflow\u2019\n   vector tensorflow::Flag > flag_list = {\n          ^\ntensorflow/contrib/pi_examples/label_image/label_image.cc:333:52: error: \u2018flag_list\u2019 was not declared in this scope\n   string usage = tensorflow::Flags::Usage(argv[0], flag_list);\n                                                    ^\ntensorflow/contrib/pi_examples/label_image/label_image.cc:335:8: error: in argument to unary !\n   if (!parse_result) {\n        ^\ntensorflow/contrib/pi_examples/label_image/label_image.cc:341:49: error: cannot convert \u2018std::string {aka std::basic_string<char>}\u2019 to \u2018const char*\u2019 for argument \u20181\u2019 to \u2018void tensorflow::port::InitMain(const char*, int*, char***)\u2019\n   tensorflow::port::InitMain(usage, &argc, &argv);\n                                                 ^\ntensorflow/contrib/pi_examples/label_image/Makefile:79: recipe for target '/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o' failed\nmake: *** [/home/pi/tensorflow/tensorflow/contrib/pi_examples/label_image/gen/obj/tensorflow/contrib/pi_examples/label_image/label_image.o] Error 1\n```\n\nAnyone have any ideas? Master is at c6af51e2cee6ecbc1b4bb30de129d2b07a381120\n", "I have the same question today @marktheunissen \n", "Same thing as @marktheunissen. Could it be something with g++ version?\n", "I tried different versions but nothin - I'm still stuck unfortunately. Should we open a separate issue?\n", "@marktheunissen - I agree, we could definitely open a separate issue. Also, thing came to my mind, could problem be with version of libjpeg-dev ?\n", "Possibly libjpeg, I did try the different versions but no dice in my testing\n", "@marktheunissen - do you mind opening new issue and notifying me here about it? I'll be home in 3-4 hours so I can do it if you're not able to.\n", "I won't have time until tomorrow morning (14 hours from now for me) - go ahead and if not I'll do it then, thanks!\n", "@shaolinkhoa - did you manage to find a solution to graph.pb.h not found?\nI applied the @magicwifi fix and then had the same problem. On Pi3.\n", "Hi @johnnymakk , \nno, i still waiting the solution in this post  #3251 \n\nyou should leave a comment in that post, so that drpngx could know and fix it.\n", "This issue is quite old and hasn't had recent activity. If it is still not working in the latest version of TensorFlow, and please create a new bug. Thank you.", "@anbai106  Hey anbai106,   did you solve the problem \"unsupported/Eigen/CXX11/Tensor\" file not found on Mac? Recently I got the same error when building tf_camera_example, if you had fixed it, please let me know, my email is liyangsh48@gmail.com.", "I have this problem as well on Mac using TensorFlow 1.2 (can't test on 1.3 due to #12018)", "Hi, is this issue resolved?\r\n\r\nI want to include tensorflow in my c++ app and including file `#include \"tensorflow/core/public/session.h\"` causes the error\r\n`tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory`\r\n\r\nI've built tensorflow with `bazel build -c opt //tensorflow:libtensorflow_cc.so` and I added tensorflow repo directory, as well as `bazel-genfiles` to include path", "Download eigen (from eigen.tuxfamily.org) and add it in the path, then 'unsupported' directory containing the Eigen/CX11/Tensor will be in the search path. This 'Tensor' file is different one. \r\nBut there are a bunch of other files missing, like .pb.h files. ", "Has anyone been able to resolve the issue ?\r\n\r\nI have included \"tensorflow/third_party/eigen3\" but I get below error\r\n\r\n> tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: error: #include nested too deeply\r\n>  #include \"unsupported/Eigen/CXX11/Tensor\"", "You need to include the Eigen3 (from eigen.tuxfamily.org) library in the -I for your target. From CMake it is something like:\r\n```\r\nfind_package(Eigen3)\r\nadd_executable(...)\r\ntarget_include_directories(<target> INTERFACE ${EIGEN3_INCLUDE_DIR})\r\n```", "the solution on Pi didn't work on iOS ? anyone got the right answer?", "Just check your eigen's version, make sure it is 3.3 or newer. I replaced the 3.2 version with 3.3.4, and fixed this problem. Maybe this can help you too.", "I built tensorflow from source including creation and installation of the python installer.  For the tensorflow directory:\r\n\r\n$ find tensorflow -type f -name 'Tensor' -print\r\ntensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor\r\n\r\nFor my python dist-packages directory:\r\n\r\n$ find dist-packages -type f -name 'Tensor' -print\r\ndist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor\r\ndist-packages/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/Tensor\r\n \r\nThe first Tensor file is the one that is indirectly included by session.h.  The 2nd Tensor file is the one that the first Tensor file needs to include.  Since the 2nd file is not present in the tensorflow tree (I don't know what magic puts it into the dist-packages tree), I don't see how a bazel build can work unless you arrange to get the 2nd file by some means (via network?).  If you try to #include the 2nd file in a bazel c_opts block you get an error for the file not being in the workspace tree.\r\n\r\nTherefore I think I am on the road to success with my first tensorflow project by not using bazel and getting my include files from the dist-packages.  I am using makefile and correct use of -I in my compiler commands has fixed this problem.\r\n\r\nRoger\r\n", "the problem was actually in the relative path of the header file taken in the Tensor file.\r\n\r\ninstalled path for Tensor is /usr/include/eigen3/unsupported/Eigen/CXX11/Tensor\r\n\r\nbut mentioned in the Tensor file is \"unsupported/Eigen/CXX11/Tensor\"\r\n\r\nSo there should be an entry upto /usr/include/eigen3/ in the project path to run this correctly so that it can be used.", "Pawan apparently refers to a case in which the user has separately installed Eigen so that its include files are in /usr/include/eigen3.  The case I refer to, and I think is what most people have done, is to build Tensorflow from source, which includes some subset of Eigen under third_party.  In this case, users are advised to build applications in the Tensorflow tree using bazel but the Tensorflow tree only includes the short Tensor include file and not the longer one that it itself attempts to include.", "Yes, it works. I followed the answer of @leftofcenter .\r\nIn the path of python which you installed on you target machine, and find the directory of \"dist-packages\" or \"site-packages\":\r\n\r\n> find -type f -name 'Tensor' -print\r\nmaybe the output:\r\n/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor\r\n\r\nSo, you can copy the directory \"unsupported/\" to where you place the reference header files. For example, \r\n\r\n> sudo cp -r ***/site-packages/tensorflow/include/unsupported/ /usr/local/include/google/tensorflow/\r\n\r\nAnd, you must also copy another folder of \"Eigen\" in \"***/site-packages/tensorflow/include/\"\r\n\r\n> sudo cp -r ***/site-packages/tensorflow/include/Eigen/ /usr/local/include/google/tensorflow/\r\n", "In my case, if I copy the whole directory(`***/site-packages/tensorflow/include/third_party/eigen3/`) to the include_path, compiler shows me an error saying \"too much include\". So, I found the path `**\\site-packages\\tensorflow\\include\\external\\eigen_archive` contains same structure and file and add this path to the include_path since some errors occur when I directly add this path `***/site-packages/tensorflow/include/third_party/eigen3/` to the include_path.\r\n\r\n It works for me, but I am not sure this is the right answer.\r\n\r\nIn addition, my code could not find the absl, so, I also add the `site-packages\\tensorflow\\include\\tensorflow\\include\\external\\com_google_absl` to the include_path", "it skips to other error", "In my case, I can download the whole eigen [repo](https://gitlab.com/libeigen/eigen).\r\nCopy the directory \"eigen/unsupported/Eigen/\" to replace \"tensorflow/third_party/eigen3/unsupported/Eigen\"", "For the poor future engineer that stumbles upon this thread because they're trying to build a lib/app using another build system using tensorflow...\r\n\r\nI was able to build against the latest (2.1) Tensorflow and get around this issue by adding the following lines in cmake (which directly translate to include directives):\r\n```\r\ninclude_directories(<path to bazel-genfiles>/bazel-genfiles/tensorflow/include)\r\ninclude_directories(<path to bazel-genfiles>/bazel-genfiles/tensorflow/include/src)\r\n```\r\n\r\nBUT there's a CRUCIAL step. There's a naming issue. You'll need BOTH `third_party` AND `third-party` directories. That is, copy the `third_party` directory in bazel-genfiles/tensorflow/include to `third-party`. This also works if you are using the headers from extracting the python wheel (you'll notice in that case that the headers are split between two folders in the wheel). ", "https://www.youtube.com/watch?v=3i6o3TeQn7s", "> \r\nhi, this answer helps  [Issues including Eigen for simple C++ TensorFlow Lite test program](https://stackoverflow.com/questions/54779775/issues-including-eigen-for-simple-c-tensorflow-lite-test-program)\r\n\r\n`find . -name \"eigen*\" -type d `   \r\n\r\nthen edit your CMakeList.txt  :\r\n` include_directories(\r\n        ${CMAKE_CURRENT_SOURCE_DIR}/tf/eigen\r\n)\r\n`\r\n\r\ntry replace eigen path listed one by one."]}, {"number": 4679, "title": "Issue #4244", "body": "Feature request: Docker image build (parameterized_docker_build.sh) lacks option for AVX2 optimization #4244\n", "comments": ["@hholst80, thanks for your PR! By analyzing the annotation information on this pull request, we identified @caisq and @tensorflower-gardener to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@hholst80 thank you for doing this. Please sign the CLA.\n\n@tensorflow-jenkins test this please.\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 4678, "title": "dynamic_raw_rnn", "body": "Working example of the recent [raw_rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1032) by @ebrevdo .\n\nThis working example implements the [dynamic_rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L685) without tuple support (yet) using the `raw_rnn` interface.\n\nThe working example builds on this [snippet](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1076), but is fully functional and provides the same interface as `dynamic_rnn` using the raw_rnn implementation.\n\nThe purpose is not to replace `dynamic_rnn` (at least not yet?), but to give a code base with commentaries for implementing custom RNNs using the `raw_rnn` interface.\n", "comments": ["Can one of the admins verify this patch?\n", "Can you talk about the reasoning behind this PR?  It seems to duplicate a subset dynamic_rnn's functionality.  We're not going to replace dynamic_rnn with raw_rnn wrappers because raw_rnn doesn't have access to the input size; thereby disallowing certain optimizations that dynamic_rnn can make now and in the future.\n", "@ebrevdo Perhaps this is more suitable as an \"introduction to raw_rnn\" in the commentary like your [snippet](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L1076). If that is of interest I will re-PR it as such?\n"]}, {"number": 4677, "title": "strided_slice unexpected behaviour", "body": "On commit `931ff427f3a55a9c6c734a4c325d6af1b53665c3`, the following happens when trying to slice a tensor with an invalid slice.\n\n``` python\nt = tf.ones([2, 3])\nt[4, 0]\n# <tf.Tensor 'strided_slice_10:0' shape=() dtype=float32>\nt[4, 0].eval()\n#0.0\n\n# Also when calling the function directly:\ntf.strided_slice(t, [4, 0], [5, 1], [1, 1], shrink_axis_mask=0b11).eval()\n#0.0\n\n# Note this is different to the numpy behaviour\na = np.ones([2, 3])\na[4, 0]\n# IndexError: index 4 is out of bounds for axis 0 with size 2\n```\n\nAlso, calling `tf.strided_slice` directly sometimes results in core dumps when using it with nonsensical parameters:\n\n``` python\nt = tf.ones([2, 3])\ntf.strided_slice(t, [0, 0], [100, 100], [1, 1], shrink_axis_mask=0b11)\n# <tf.Tensor 'StridedSlice_1:0' shape=() dtype=float32>\ntf.strided_slice(t, [0, 0], [100, 100], [1, 1], shrink_axis_mask=0b11).eval()\n# F tensorflow/core/kernels/strided_slice_op.cc:307] Check failed: tmp.CopyFrom(input, final_shape)\n# Aborted (core dumped)\n```\n### Environment info\n\nOperating System:\nUbuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 May 23 17:04 /usr/local/cuda-7.5/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 May 23 17:03 /usr/local/cuda-7.5/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5\n-rwxr-xr-x 1 root root 59909104 May 23 17:02 /usr/local/cuda-7.5/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 58775484 May 23 17:02 /usr/local/cuda-7.5/lib64/libcudnn_static.a\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   `931ff427f3a55a9c6c734a4c325d6af1b53665c3`\n2. The output of `bazel version`\n\n```\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n```\n", "comments": ["Closing this as both issues have been fixed in version 0.11 (although a ValueError is given instead of an IndexError)\n\n``` python\nt = tf.ones([2, 3])\nt[4, 0]\n# ValueError: slice index 4 of dimension 0 out of bounds.\n```\n\nIt was probably `caceb02f75ff80a8e48440720cec3d7d6fa3297e` that fixed it.\n"]}, {"number": 4676, "title": "Generate additional cudart linkopts in cuda_configure.", "body": "Currently, the target `@local_config_cuda//cuda:cudart_static` contains a\n`select()` condition that references the `@//tensorflow:darwin` config\ncondition.\n\nWhile this is fine when building out of the `tensorflow` workspace, this breaks\nthe build for serving and other projects that vendor TF since those workspaces\ndo not have a `//tensorflow:darwin` target (under serving, that label would be\n`//tensorflow/tensorflow:darwin` under the current setup).\n\nThis change removes the `select()` and moves the logic for generating additional\nplatform-specific linkopts for cudart_static into cuda_configure, making the\n`@local_config_cuda` workspace more self-contained.\n", "comments": ["@davidzchen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @nsuke and @meteorcloudy to be potential reviewers\n", "I have updated the commit message based on the discussion.\n\nPTAL.\n", "Looks like Jenkins is having problems:\n\n```\njavax.servlet.ServletException: org.apache.commons.jelly.JellyTagException: jar:file:/var/cache/jenkins/war/WEB-INF/lib/jenkins-core-2.24.jar!/hudson/model/AbstractBuild/index.jelly:110:84: <st:include> org.apache.commons.jelly.JellyTagException: jar:file:/var/cache/jenkins/war/WEB-INF/lib/jenkins-core-2.24.jar!/lib/hudson/summary.jelly:105:31: <d:invokeBody> Cannot get property 'fullName' on null object\n...\n```\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4675, "title": "LinearClassifier feature_columns overwritten in LinearClassifier.fit", "body": "`tensorflow.contrib.learn.python.learn.estimators.linear.LinearClassifier.fit`\neffectively returns\n`tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator.fit`\n`Estimator.fit` calls:\n`tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._train_model`\nwhich has these lines:\n\n```\n      features, targets = input_fn()\n      self._check_inputs(features, targets)\n      train_op, loss_op = self._get_train_ops(features, targets)\n```\n\n`tensorflow.contrib.learn.python.learn.estimators.estimator.Estimator._check_inputs`\n\n```\n    if self._features_info is not None:\n      ...\n    else:\n      self._features_info = tensor_signature.create_signatures(features)\n```\n\nSo we get to a point where features, as derived from the input_fn, is treated as our feature columns set.\n\nIn pseudo code:\n\n```\ndef input_function()\n    return [foo, bar, baz], quux\n\nlc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be\nlc.fit(input_function) # run fit on out input function\nlc._feature_columns # repr _feature_columns on the instantiated classifier\n    [foo]\nlc.estimator._features_info # _features_info on the instantiated classifiers instantiated estimator\n    [foo, bar, baz]\n```\n### The issue is:\n\nAlthough this line appears to indicate that we will be making an estimation based on the feature columns supplied:\n`lc=LinearClassifier(feature_columns=[foo]) # We pass in what we want our feature column to be`\nWhat happens is that the passed in `feature_columns` is unused and instead the return from the input_function supplied to fit are used.\n\nAm I correct in thinking that if the `feature_columns` arg is supplied that only those columns should be used by the classifiers estimator?\nThat when we instantiate the classifier we are setting the feature_columns we expect to be used?\n\nThe work around for this is simply to only return the columns you need from your input function however I found this misleading.\n\nPoint in the tutorial:\nEither the code or the tutorial need to be changed.\nhttps://www.tensorflow.org/versions/r0.10/tutorials/wide/index.html#defining-the-logistic-regression-model\n\nError raised:\n`WARNING:tensorflow:Setting feature info to`\nas per tensorflow/contrib/learn/python/learn/estimators/estimator.py:613\n", "comments": ["@hengtze: is this something that needs to be changed in the wide tutorial?  If not, is this something the tf.learn side should look into?\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 4674, "title": "GPU pip wheel cannot be used on a system without the GPU libraries installed", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI found some issues like #2653 but it wasn't really helpful.\n### Environment info\n\nOperating System: Ubuntu 16.04.1\n\nInstalled version of CUDA and cuDNN: CUDA 8.0 / cuDNN 5.1.5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   558720 Sep 30 15:48 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 30 15:48 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 30 15:48 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rwxr-xr-x 1 root root   415432 Sep 30 15:48 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root   775162 Sep 30 15:48 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Sep 30 15:50 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n\nI've built TF from the source (latest r0.10 branch 931ff427f with cherry-picked 6d57860e3 (for avx2 issue)) with CUDA enabled.\n\n```\n$ bazel version\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n`python3 -c \"import tensorflow; print(tensorflow.__version__)\"` would normally work with GPU.\n\n```\n$ python3 -c \"import tensorflow as tf; print(tf.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\n0.10.0\n$ env | grep cuda\nCUDA_HOME=/usr/local/cuda\nLD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\n```\n\nHowever, if I unset CUDA-related variables and SET `CUDA_VISIBLE_DEVICES=-1`(for using CPU Only), TF won't work.\n\n```\n$ unset LD_LIBRARY_PATH             \n$ unset CUDA_HOME           \n$ CUDA_VISIBLE_DEVICES=-1 python3 -c \"import tensorflow as tf; print(tf.__version__)\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/leebc/.local/lib/python3.5/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/home/leebc/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 48, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/home/leebc/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/home/leebc/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\n```\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Can you try `CUDA_VISIBLE_DEVICES=none` or `CUDA_VISIBLE_DEVICES=''`?\n", "Unfortunately, either `CUDA_VISIBLE_DEVICES=none` or `CUDA_VISIBLE_DEVICES=''` won't work. Do I need to build and test other branches?\n", "I think the in-preparation 0.11 release will fix this, but not at the 0.10 branch you're using.  Can you try it [here](https://github.com/tensorflow/tensorflow/tree/r0.11)?\n", "Does the latest build at MASTER have this fix or no?\n", "Well, neither [r0.11](https://github.com/tensorflow/tensorflow/tree/r0.11) and [master](https://github.com/tensorflow/tensorflow/tree/master) branches won't work. IIRC, this `CUDA_VISIBLE_DEVICES` variable works in r0.8 with some old CUDA and cuDNN. Do I need to bisect commits?\n", "in my case of r0.10, using `export CUDA_VISIBLE_DEVICES=\"2\"`,  along with\n\n```\n# Create a session for running Ops on the Graph.\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config) \n```\n\nit works ok.\n", "@zszhong I think the complaint is that the gpu pip wheel does not work if you don't have the GPU libraries installed.  CUDA_VISIBLE_DEVICES is not interpreted at all by TensorFlow code, it only affects how the driver makes the devices available to the process.\n\nI don't remember it ever being the case that you could run the GPU pip build on a system without the GPU libraries installed.  It must be something related to the way our swig wrapper has a static linkage to the cuda libraries, even though the C++ code dynamically loads the library. @keveman or @zheng-xq, let me know if you have any ideas.\n", "@vrv, sorry that I'm not familiar with the internal tensorflow code, so I can't find the reasons from the in-depth viewpoint. I didn't use the official wheel files, and I always build and install tensorflow from source. In the branch of `r0.10`,  it works OK. The environment is `RedHat 64bit, GCC 4.8.4, Python 2.7, CUDA 7.0 with CuDNN 4.0`. Yes, It is probably related to the `CUDA 8.0`, I didn't use the `CUDA 8.0`.\n\nI suggest @bc-lee to build tensorflow from source, because the official wheel may have any other dependencies that your machine doesn't have (although it might be built with static linkage). There are many experiences on how to build from source and it doesn't cost much time.  In addition, if the system didn't have GPU driver, why do you use the GPU wheel? I think you should use the CPU version. And if you have installed the driver, why do you use the CPU version?\n", "Indeed, our current answer is: only install the GPU wheel on a machine that has the GPU libraries installed, otherwise install the CPU wheel.\n\nHowever, it might be nice sometimes for people to be able to install one wheel that worked everywhere (even if it was more bloated) -- this issue is just tracking that feature request.  It's not something we're highly likely to work on any time soon given the hundreds of other issues we currently have, but if someone from the community is interested in digging in to figure it out, it would be appreciated :).\n", "@zszhong I use TF by building it from source, not from official wheel(which is linked against CUDA 7.5 / cuDNN 5).\n\n@vrv Understood. Sometimes I'd like to run TF without any GPU use along with GPU-using TF running. Installing two versions of TF (using virtualenv) might help, but I was curious if I can disable GPU feature completely using `CUDA_VISIBLE_DEVICES`. This issue isn't crutial issues than other Tons of issues, but It would be awesome if single wheel file works both GPU and CPU version. I'll remain this issue be open.\n", "@bc-lee If you have a system that has a GPU, you can install the GPU tensorflow version, but only run your code on CPU. GPU pip wheel file will still allow you to run your graph on CPU.\nYou can then run another process, start another process, which runs your code on GPU. But to be able to do that, you have to have a machine with GPU, with CUDA and CuDNN installed.\nI do not see the direct relation of the use case you described to being able to install a TF GPU pip package on a machine with no GPU. \nIf you were asking a TF package that came with all GPU drivers, CUDA and CuDNN, that is not possible due to different constraints in the licenses of all these software.\n", "@bc-lee you can disable GPU completely by using `CUDA_VISIBLE_DEVICES`\n\nAt Google the preference was to use \"fat binary\" approach -- single binary that bundles GPU drivers and works on both GPU and non-GPU machines. The annoyance of open-source world is that GPU drivers have to be installed separately\u00a0and can be missing on machines without a physical GPU. A way to simplify things is to install GPU drivers on all machines, whether or not physical GPU is present, and use a GPU-binary.\n", "As a workaround, You can install nvidia drivers and cuda on your system even if you do not have a NVIDIA GPU and install the GPU version. Or if you have a GPU and install the GPU version, you can use `CUDA_VISIBLE_DEVICES` environment variable to run TensorFlow only on CPU.\r\n\r\nI will close this issue as there has not been any updates for a long time, but we have a few initiatives which can help with this issue in the long term.", "@gunan, sorry to revive this thread (I am happy to create a separate issue if you prefer), but you mentioned:\r\n\r\n> You can install nvidia drivers and cuda on your system even if you do not have a NVIDIA GPU and install the GPU version\r\n\r\nI did just that (on a machine without a GPU but with nvidia drivers installed and `CUDA_VERSION` and `NVIDIA_VISIBLE_DEVICES` to ``) and it works but I still get these annoying log messages the first time I create a session:\r\n\r\n```\r\n> python -c 'import tensorflow as tf; tf.Session()'\r\n2018-09-12 00:56:38.003223: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)\r\n2018-09-12 00:56:38.003303: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel driver does not appear to be running on \r\nthis host (8c18459333df): /proc/driver/nvidia/version does not exist\r\n```\r\n\r\nIs there another environment variable I can set to prevent the GPU build from trying to initialize cuda when running without a physical GPU?\r\n\r\nThank you", "You may be able to set TF_CPP_MIN_LOG_LEVEL, but that may also block any errors you may have during graph creation and execution.", "Thank you @gunan but like you mentioned, hiding errors isn't ideal for our users.\r\n\r\nYou mentioned above that \"we have a few initiatives which can help with this issue in the long term\", do you have any issues tracking these?", "None on public side, but we are hoping we can merge some of these soon."]}, {"number": 4673, "title": "InvalidArgument error for space_to_batch function using half-float (fp16)", "body": "I am trying to execute atrous_conv2d operation using half-float (fp16) as input data type (float32 dataType works fine) but I get the following InvalidArgumentError:\n\n**tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'Tower_0/conv2/atrous_conv2d/SpaceToBatch': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available**\n\nHere is the software configuration:\nOS: SLES 12\nTensor Flow: 0.10 (compiled from sources using Cuda 8.0.27 and cuDNN 5.1.5)\nPython: 3.5.2\nBazel : 0.3.1\n\nIs this a known issue? If so, in which Tensor Flow version will it be fixed?\n", "comments": ["@benoitsteiner: the SpaceToBatch GPU kernel lacks half support.  Is this something already on your radar?  If not, we can just mark this as contributions welcome, of course.\n"]}, {"number": 4672, "title": "Can't visualize plots from multiple log directories in Tensorboard after refreshing browser. Must close reopen tab.", "body": "### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n1. Run tensorboard for one log directory such as:\n   tensorboard --logdir=some/directory1\n2. View it on a browser\n3. Run tensorboard for two log directories such as:\n   tensorboard --logdir=somelabel1:some/directory1,somelabel2:some/directory2\n4. Refresh the browser tab\n\nExpected: two plots\nActual: only one plot\n1. If you close the browser tab and then navigate to tensorboard again the two plots will be visible.\n### What other attempted solutions have you tried?\n\nClose reopen browser tab.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["@danmane: could you take a look?\n", "I'm not sure that I can reproduce this. When I write `tensorboard --logdir x:/tmp/data,y:/tmp/moredata`, I can view data from both logdirs, and this still holds on refresh.\r\n\r\nIf you can provide a consistent repro case, please feel free to reopen this issue in our new repository at https://github.com/tensorflow/tensorboard/issues. Thanks!"]}]