[{"number": 21394, "title": "tflite: \"Unimplemented: this graph contains an operator of type TransposeConv for which the quantized form is not yet implemented\". So how long will that be supported?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@suharshs, do we have any plans to implement this soon?\r\n@merryImage , could you provide your model code which is causing this to be invoked. It may be avoidable.\r\n", "Quantized TransposeConv hasn't been prioritized yet, contributions are welcome, but we are also tracking these operations requests here: https://github.com/tensorflow/tensorflow/issues/21526 to help with prioritizing. For that reason, we will close this issue, please reopen if you have more info on the specific model you are trying to convert. Thanks!"]}, {"number": 21393, "title": "Fix lite Makefile; CCX, CC and AR flags", "body": "White spaces in declaration of CCX, CC and AR flags caused `make: arm-linux-gnueabihf-: Command not found`. Where `arm-linux-gnueabihf-` was given as CC_PREFIX.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi @sbalk this is a duplicate of #21399. I'm closing this in favor of the other one.", "Dear drpngx,\n\nThanks for mentioning. Could you please tell me why you accepted the other\npull request although I made mine one day earlier?\nI presume it's because of my formatting. if that's the case, how could I\nimprove my pull requests so that this doesn't happen again?\n\nThanks!\nStijn\n\nOn Fri, 10 Aug 2018 at 20:49, drpngx <notifications@github.com> wrote:\n\n> Hi @sbalk <https://github.com/sbalk> this is a duplicate of #21399\n> <https://github.com/tensorflow/tensorflow/pull/21399>. I'm closing this\n> in favor of the other one.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21393#issuecomment-412171214>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGbPk0Po0bLsCKpSdTZrd6WIW-910B0vks5uPdW3gaJpZM4VvhGw>\n> .\n>\n-- \nStijn Balk\n", "Hi @sbalk it just so happened that I got to the other one before and it was in a more advanced stage. Your PR was just fine. I liked the title which describes the problem well. Please keep contributing!", "Cool thanks, will do!\n\nOn Fri, 10 Aug 2018 at 22:48, drpngx <notifications@github.com> wrote:\n\n> Hi @sbalk <https://github.com/sbalk> it just so happened that I got to\n> the other one before and it was in a more advanced stage. Your PR was just\n> fine. I liked the title which describes the problem well. Please keep\n> contributing!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/21393#issuecomment-412200959>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AGbPk2jhBJxFa1VlUZv01fj3mQD61amPks5uPfGNgaJpZM4VvhGw>\n> .\n>\n-- \nStijn Balk\n"]}, {"number": 21392, "title": "Add correlation cost layer from FlowNet", "body": "This adds a new operation to compute the correlation layer used in\r\nFlowNet: Learning Optical Flow with Convolutional Networks by\r\nDosovitskiy et al.\r\n\r\nIt is a refactored version of the open-source implementation\r\n\"https://github.com/NVIDIA/flownet2-pytorch\"\r\nresp. [original implementation](\r\nhttps://github.com/lmb-freiburg/flownet2/blob/master/src/caffe/layers/correlation_layer.cu)\r\nadapted to use CUB and support both data_formats NCHW, NHWC.", "comments": ["@derekjchow could you take a look at this?", "@martinwicke what's our policy on contributing to contrib right now?", "For the convenience of the reviewer, I squashed all commits into a single commit (rebased to my local copy of the master HEAD), added more comments and better tests.\r\nI think I got everything in place and tested everything I can think of (usage of shm, different number of threads/block).\r\n\r\nThe remaining three open questions from my side are:\r\n- Should I include my [benchmark script](https://github.com/PatWie/tensorflow/blob/correlation_layer_shm/tensorflow/contrib/correlation_cost/python/kernel_tests/correlation_cost_op_benchmark.py) as well?\r\n- Most `.Doc(R\"doc` are [empty or omitted](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc). Should I copy the [python documentation](https://github.com/tensorflow/tensorflow/blob/91b163b9bd8dd0f8c2631b4245a67dfd387536a6/tensorflow/contrib/correlation_cost/python/ops/correlation_cost_op.py#L40-L79) to [REGISTER_OP](https://github.com/tensorflow/tensorflow/blob/91b163b9bd8dd0f8c2631b4245a67dfd387536a6/tensorflow/contrib/correlation_cost/ops/correlation_cost_op.cc#L89) as well?\r\n- Is it required to register the gradient [on the C++ side as well](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc/gradients)? If so, where to place this in contrib?", "Hi @PatWie it's a large contribution to review, and to maintain. Since `tf.contrib` is going to be retired, there is limited value to adding significant new contributions, unless they are so significant that they will be moved into core quickly.\r\n\r\nWith straight python code, the best route is probably for one to host in a separate repo. Having gone through the exercise myself, I can attest that it is not completely trivial to have `C++` ops in an external project at this point.", "I just followed your response which indicated [contributions welcome](https://github.com/tensorflow/tensorflow/issues/11956#issuecomment-362761880).\r\n\r\nDoes \"large\" means, it can take several weeks or does \"retired\" means I should close this PR right now? I am ok with both as long as it is clearly communicated and this PR does not disappear in the flood of PRs of small fixes without any decision.\r\n\r\nFor the definition of significance: almost all recent state-of-the-art optical-flow estimation approaches rely on this operation, and there is no way to code this as a one-liner python code.\r\n\r\n>  I can attest that it is not completely trivial to have C++ ops in an external project at this point.\r\n\r\nThat's not true. I did multiple times [here](https://github.com/PatWie/tensorflow-cmake) and [here](https://github.com/PatWie/tensorflow-recipes/tree/master/OpticalFlow). I am happy to keep it there. Without wanting to sound rude, I guess there might be some broader interest making it acceptable to ship such an operation directly by TensorFlow.\r\n\r\nI will certainly not rebase again the master HEAD after a month or so. So we should let this PR die right now, if there is no interest.\r\n", "I have just announced this: we will deprecate contrib. I have no problem accepting this PR before then -- there's a lot of content in contrib that will need to find a new home, and adding one more op only makes this marginally worse. \r\n\r\nWhat I would like is if all the ops that end up in contrib.nn are actually sourced from contrib/nn, i.e., have ops\u00a0and kernels folders similar to, say, contrib/kafka. I now that's not the case today with a bunch of other ops too, which is sad. Since this is already not the case, I'm ok accepting this as is (I have not reviewed the code, assuming the code is ok). \r\n\r\nGoing a little meta: realistically, the TensorFlow team cannot be gatekeepers for all of contrib. The long waits for PR reviews are a symptom of that. \r\n\r\nIdeally, somebody in the community wants to maintain a repo containing, say contrib.nn or contrib.ops or contrib.keras. We'd be happy to provide help setting this up, test tooling and a place on tensorflow.org. Ideally this would be organized as a [SIG](https://www.tensorflow.org/community/contributing).", "friendly ping to @vikaskyadav (assigned reviewer).\r\nThis PR seems to die, or is it still alive?", "@PatWie Just a heads up, we plan to review this as part of tensorflow/addons following our move out of tf.contrib. \r\n\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/addons/xS4-QKgP4WI", "Let me know if and how I can assist you with moving this PR to tensorflow/addons.", "Will do, probably will ask for a new PR once the repo is setup in the coming weeks. Thanks.", "hi @PatWie sorry for nagging but is there any updates on when or if the commit will be merged ?", "As far as I know, this commit will not be merged here into TensorFlow., while I just followed the \"[contribution welcome](https://github.com/tensorflow/tensorflow/issues/11956#issuecomment-362761880)\".\r\n\r\nIf you need this layer, please have a look at my project here (FlowNet2 + PWC) which uses that operation:\r\nhttps://github.com/PatWie/tensorflow-recipes/tree/master/OpticalFlow\r\n\r\nYou get this layer+implementation there and simply compile this operation via \"cmake . && make\". You do not even need the TensorFlow git-repo.\r\nBefore asking: I did not receive any update about `tensorflow/addons` as well.\r\n\r\n", "Still very much planning to include this in [tensorflow/addons](https://github.com/tensorflow/addons). We're hoping to get our GPU CI testing setup this month and will follow up when ready. Apologies for delay", "Nagging Reviewer @ebrevdo: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 60 days with no activity and the `awaiting review` label has been applied.", "@PatWie Would you be interested in starting a PR for Addons? We have yet to support GPU ops, but it's one of the next things on the agenda. See https://github.com/tensorflow/addons/issues/118\r\n\r\nJust a note: we'll probably ask that you be the maintainer for the layer as it's a large chunk of (well-written) code.", "Any news on this?\r\nI would really like to see this code in Addons... It's used in multiple places now and I would like to do so too.", "@PatWie Any update please ?", "see https://github.com/tensorflow/addons/pull/207 for WIP. But I need to test it locally.\r\n\r\nIn the meantime I suggest to use my implementation with CMake from here:\r\nhttps://github.com/PatWie/tensorflow-recipes/tree/master/OpticalFlow"]}, {"number": 21391, "title": "tf.linspace doesn't except integers for start/stop and floats for num", "body": "tf.linspace(start=1.0, stop=3.0, num=3) - works fine.\r\ntf.linspace(start=1, stop=3, num=3) yields an error (because start and stop are integers)\r\ntf.linspace(start=1.0, stop=3.0, num=3.0) yields an error (because num is float)\r\nIn both cases the error is: \"Could not find valid device for node name: \"LinSpace\" \"\r\n\r\nI would expect the following behavior:\r\n* start and stop can be integers (the op can convert it to float if fractions are produced; but see below)\r\n* num can be float, if its modulo with 1 is zero (the op can convert it to int; but see below)\r\n* the error message should reflect the problem precisely\r\n* the comments and reference page should reflect the expected dtype more clearly (at the top, not just in args)\r\n\r\nThis behavior is how numpy linspace function works (including stating all optional dtype in the autocomplete while typing).\r\n\r\ndtype conversion within a TF op might not follow TF design rules as it might create dtype changes without the user awareness. Yet, I would still be happy to have an error message that explicitly states the problem with the dtypes.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: See above", "comments": ["@RanFeldesh On Linux the error is quite descriptive:\r\n```\r\n# python3\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.linspace(start=1, stop=3, num=3)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 3908, in lin_space\r\n    \"LinSpace\", start=start, stop=stop, num=num, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'start' has DataType int32 not in list of allowed values: bfloat16, float32, float64\r\n>>> tf.linspace(start=1.0, stop=3.0, num=3.0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 3908, in lin_space\r\n    \"LinSpace\", start=start, stop=stop, num=num, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'num' has DataType float32 not in list of allowed values: int32, int64\r\n>>> tf.__version__\r\n'1.9.0'\r\n>>> \r\n\r\n```", "Not so sure about the automatic conversion of dtype, but would be happy to add support if this could be confirmed by tf API team.", "Dear @yongtang , thank you so much for the quick reply and verification on your machine.\r\nThe first post's error message was with eager mode. With the graph mode, I get a similar error message  to your Linux example. It didn't occur to me that the vague error message might be due to eager mode. I usually debug in eager, then execute in graph. It might be possible to add the explicit error message in eager mode as well. I wonder how simple it is adding it, and if it falls within the TF API design rules.\r\n\r\nRunning on same config as in the first post:\r\n* Here is the complete code for eager mode:\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntf.linspace(1,2,1)\r\n```\r\n* And the associated error message (eager mode; it doesn't state the error type explicitly):\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Ran/Documents/TensorFlow Models/NeuronalMemory/linspace.py\", line 3, in <module>\r\n    tf.linspace(1,2,1)\r\n  File \"C:\\Users\\Ran\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 4205, in lin_space\r\n    start, stop, num, name=name, ctx=_ctx)\r\n  File \"C:\\Users\\Ran\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 4225, in lin_space_eager_fallback\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"C:\\Users\\Ran\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node name: \"LinSpace\"\r\nop: \"LinSpace\"\r\ninput: \"dummy_input\"\r\ninput: \"dummy_input\"\r\ninput: \"dummy_input\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\nattr {\r\n  key: \"Tidx\"\r\n  value {\r\n    type: DT_INT32\r\n  }\r\n}\r\n [Op:LinSpace]\r\n\r\nProcess finished with exit code 1\r\n```\r\n* The same in a graph-mode code:\r\n```\r\nimport tensorflow as tf\r\ntf.linspace(1,2,1)\r\n```\r\n* the error message in graph mode (which states error explicitly - which is great!):\r\n```\r\nFile \"C:/Users/Ran/Documents/TensorFlow Models/NeuronalMemory/linspace.py\", line 2, in <module>\r\n    tf.linspace(1,2,1)\r\n  File \"C:\\Users\\Ran\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 4188, in lin_space\r\n    \"LinSpace\", start=start, stop=stop, num=num, name=name)\r\n  File \"C:\\Users\\Ran\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"C:\\Users\\Ran\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'start' has DataType int32 not in list of allowed values: bfloat16, float32, float64\r\n```", "Thanks, @RanFeldesh and @yongtang . @asimshankar -- can you comment on whether this would be a desired feature, and, while you're here, whether the error message can be improved for eager?", "Will look into it. \r\nSome possibly useful background information can be found in https://stackoverflow.com/questions/51698096/from-design-perspective-why-arent-the-error-messages-in-eager-and-in-graph-mod/51735894#51735894"]}, {"number": 21390, "title": "Add support for unknown batch sizes in group_norm.", "body": "Previously, the `group_norm` layer did not work when the batch size was not known (as is the case when using e.g. `tf.data.Dataset.batch`). \r\n\r\nThe change directly addresses this case by replacing a call to `Tensor.shape` with a call to `array_ops.shape`, as well as replacing `None` with `-1` when calling `array_ops.reshape`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it...", "CLAs look good, thanks!\n\n<!-- ok -->", "@shlens", "Thank you for doing this. Please add a unit test to guarantee behavior is correct.", "Hey @shlens, I\u2019ve now added the unit test, and actually changed the code to support an arbitrary number of unknown dimensions. Indexing into `TensorShape` is a bit ugly without slices, so I tried my best, but there might be better ways to do it.", "any news on this issue? Or did anyone find a way to circumvent this issue by transforming the tensor before applying group norm? I tried `tf.map_fn(tensor, lambda x: group_norm(x, ...))` but without success.", "@patzm: If you use [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for your input pipeline, you can work around this issue by using either [`tf.contrib.data.batch_and_drop_remainder(batch_size)`](https://www.tensorflow.org/api_docs/python/tf/contrib/data/batch_and_drop_remainder), or [`tf.data.Dateset.batch(batch_size, drop_remainder=True)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), depending on your TensorFlow version. They both yield static batch sizes, but the last bit of your data will be dropped if necessary.", "This changes contrib, which will disappear shortly. I'd prefer not to take this PR, and reopen a PR against github.com/tensorflow/addons, where these extensions should be hosted in future."]}, {"number": 21389, "title": "DistributionStrategy in tf.keras doesn't support multi input models", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian and macOS 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0.dev20180805\r\n- **Python version**: 3.6 and 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: See below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n@anj-s Huge thanks for adding support for distribution strategies to keras in 92279f8bfa6ce2124439aabfa6db84d722dc2b66 \ud83c\udf89 \r\n\r\nWhile using it together with single input models works great, it fails with an uncaught exception when using it with multiple inputs:\r\n\r\n### Source code / logs\r\nConsider the following unit tests:\r\n```python\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.distribute.python import mirrored_strategy\r\nfrom tensorflow.python import keras\r\nfrom tensorflow.python.data.ops import dataset_ops\r\nfrom tensorflow.python.platform import test\r\nfrom tensorflow.python.training import gradient_descent\r\n\r\nclass TrainingTest(test.TestCase):\r\n  def test_dataset_input_tuples(self):\r\n    with self.test_session():\r\n      a = keras.layers.Input(shape=(3,), name='input_a')\r\n      b = keras.layers.Input(shape=(4,), name='input_b')\r\n      x = keras.layers.concatenate([a, b])\r\n      y = keras.layers.Dense(5, name='dense')(x)\r\n\r\n      model = keras.Model(inputs=[a, b], outputs=[y])\r\n      model.compile(loss='mse', metrics=['mae'], optimizer='rmsprop')\r\n\r\n      inputs_a = np.zeros((10, 3))\r\n      inputs_b = np.zeros((10, 4))\r\n      targets = np.zeros((10, 5))\r\n      dataset = dataset_ops.Dataset.from_tensor_slices(((inputs_a,\r\n                                                         inputs_b),\r\n                                                        targets))\r\n      dataset = dataset.repeat(100)\r\n      dataset = dataset.batch(10)\r\n\r\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n\r\n  def test_distributed_dataset_input_tuples(self):\r\n    with self.test_session():\r\n      a = keras.layers.Input(shape=(3,), name='input_a')\r\n      b = keras.layers.Input(shape=(4,), name='input_b')\r\n      x = keras.layers.concatenate([a, b])\r\n      y = keras.layers.Dense(5, name='dense')(x)\r\n      model = keras.Model(inputs=[a, b], outputs=[y])\r\n\r\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\r\n      strategy = mirrored_strategy.MirroredStrategy(['/device:GPU:1',\r\n                                                     '/device:CPU:0'])\r\n\r\n      model.compile(loss='mse',\r\n                    metrics=['mae'],\r\n                    optimizer=optimizer,\r\n                    distribute=strategy)\r\n\r\n      inputs_a = np.zeros((10, 3))\r\n      inputs_b = np.zeros((10, 4))\r\n      targets = np.zeros((10, 5))\r\n      dataset = dataset_ops.Dataset.from_tensor_slices(((inputs_a,\r\n                                                         inputs_b),\r\n                                                        targets))\r\n      dataset = dataset.repeat(100)\r\n      dataset = dataset.batch(10)\r\n\r\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n\r\nif __name__ == '__main__':\r\n  test.main()\r\n```\r\n\r\nWhile the first test works well, the second will fail with the following error:\r\n```python-traceback\r\nEpoch 1/1\r\n2/2 [==============================] - 0s 55ms/step - loss: 0.0000e+00 - mean_absolute_error: 0.0000e+00\r\n.WARNING:tensorflow:You are accessing attribute optimizerof theDistributedCallbackModel that may not have been setcorrectly.\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\r\n  return _inspect.getargspec(target)\r\nE.\r\n======================================================================\r\nERROR: test_distributed_dataset_input_tuples (__main__.TrainingTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"tf_test.py\", line 57, in test_distributed_dataset_input_tuples\r\n    model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1366, in fit\r\n    validation_split=validation_split)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 794, in _standardize_user_data\r\n    validation_split=validation_split)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 713, in _distribution_standardize_user_data\r\n    validate_distributed_dataset_inputs(self._distribution_strategy, x, y)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/distributed_training_utils.py\", line 212, in validate_distributed_dataset_inputs\r\n    ' are of type {}'.format(type(x)))\r\nValueError: Dataset input to the model should be tensors instead they are of type <class 'tuple'>\r\n\r\n----------------------------------------------------------------------\r\nRan 3 tests in 0.914s\r\n\r\nFAILED (errors=1)\r\n```\r\n\r\nThe same is true when using a dictionary as an input.\r\nSince I'm new to using distribution strategies, I'm unsure if this is just a problem with too strict input validation (as in #20753) or if this needs more work to support this use case.\r\n\r\nThanks for the great work!", "comments": ["Thank you lgeiger@ for filing the issue! This error was raised due to support not being added for multiple inputs and targets. I am working on a fix and will update this issue as soon as the fix is in.\r\n\r\nYou mentioned running into this issue when using dictionary as input. Currently DistributionStrategy requires a Dataset object as input to fit, evaluate and predict. Can you clarify how you were passing a dictionary? Thanks!", "@anj-s Awesome to hear that you're working on this.\r\n\r\nSorry for the confusion. I was using a Dataset with a dict instead of a tuple for the first input.\r\nHere is a updated test case, that covers the behavior:\r\n```python\r\nfrom absl.testing import parameterized\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.distribute.python import mirrored_strategy\r\nfrom tensorflow.python import keras\r\nfrom tensorflow.python.data.ops import dataset_ops\r\nfrom tensorflow.python.platform import test\r\nfrom tensorflow.python.training import gradient_descent\r\n\r\n\r\nclass TrainingTest(test.TestCase, parameterized.TestCase):\r\n\r\n  @parameterized.named_parameters(\r\n      ('tuple', lambda *x: x, None),\r\n      ('dict', lambda x, y: {'input_a': x, 'input_b': y}, None),\r\n      ('tuple_distribute',\r\n       lambda *x: x,\r\n       mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])),\r\n      ('dict_distribute',\r\n       lambda x, y: {'input_a': x, 'input_b': y},\r\n       mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])))\r\n  def test_multi_input_model(self, input_fn, distribute):\r\n    with self.test_session():\r\n      a = keras.layers.Input(shape=(3,), name='input_a')\r\n      b = keras.layers.Input(shape=(4,), name='input_b')\r\n      x = keras.layers.concatenate([a, b])\r\n      y = keras.layers.Dense(5, name='dense')(x)\r\n\r\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\r\n\r\n      model = keras.Model(inputs=[a, b], outputs=[y])\r\n      model.compile(loss='mse', metrics=['mae'], optimizer=optimizer, distribute=distribute)\r\n\r\n      input_a = np.zeros((10, 3))\r\n      input_b = np.zeros((10, 4))\r\n      targets = np.zeros((10, 5))\r\n      dataset = dataset_ops.Dataset.from_tensor_slices((input_fn(input_a, input_b), targets))\r\n      dataset = dataset.repeat(100)\r\n      dataset = dataset.batch(10)\r\n\r\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n\r\nif __name__ == '__main__':\r\n  test.main()\r\n```", "Thank you for the example! This was very helpful to reproduce. I have a solution and will let you know as soon as the fix is in.", "> Thank you for the example! This was very helpful to reproduce. I have a solution and will let you know as soon as the fix is in.\r\n\r\nSure, I'm happy to help. Thanks for working on this, I'm looking forward to the feature.", "The fix has been checked in. lgeiger@ Can you try running your code again and see if you run into issues?\r\nThanks!", "@anj-s Thanks for pushing a fix. A quick test revealed a sneaky issue, propably related to [incorrect flattening](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/distributed_training_utils.py#L238).\r\n\r\nWhile it works when the input names are alphabetically sorted, it fails when they're not. Checkout the following code:\r\n```python\r\nimport numpy as np\r\n\r\nfrom tensorflow.contrib.distribute.python import mirrored_strategy\r\nfrom tensorflow.python import keras\r\nfrom tensorflow.python.data.ops import dataset_ops\r\nfrom tensorflow.python.platform import test\r\nfrom tensorflow.python.training import gradient_descent\r\n\r\n\r\nclass TrainingTest(test.TestCase):\r\n  def test_multi_input_model(self):\r\n    with self.test_session():\r\n      a = keras.layers.Input(shape=(3,), name='aa_input_a')\r\n      b = keras.layers.Input(shape=(4,), name='zz_input_b')\r\n      x = keras.layers.concatenate([a, b])\r\n      y = keras.layers.Dense(5, name='dense')(x)\r\n\r\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\r\n      distribute = mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])\r\n\r\n      model = keras.Model(inputs=[a, b], outputs=[y])\r\n      model.compile(loss='mse', metrics=['mae'], optimizer=optimizer, distribute=distribute)\r\n\r\n      input_a = np.zeros((10, 3))\r\n      input_b = np.zeros((10, 4))\r\n      targets = np.zeros((10, 5))\r\n      dataset = dataset_ops.Dataset.from_tensor_slices(({'aa_input_a': input_a, 'zz_input_b': input_b}, targets))\r\n      dataset = dataset.repeat(100)\r\n      dataset = dataset.batch(10)\r\n\r\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n\r\n  def test_multi_input_model_non_alphabetic(self):\r\n    with self.test_session():\r\n      a = keras.layers.Input(shape=(3,), name='zz_input_a')\r\n      b = keras.layers.Input(shape=(4,), name='aa_input_b')\r\n      x = keras.layers.concatenate([a, b])\r\n      y = keras.layers.Dense(5, name='dense')(x)\r\n\r\n      optimizer = gradient_descent.GradientDescentOptimizer(0.001)\r\n      distribute = mirrored_strategy.MirroredStrategy(['/device:GPU:1', '/device:CPU:0'])\r\n\r\n      model = keras.Model(inputs=[a, b], outputs=[y])\r\n      model.compile(loss='mse', metrics=['mae'], optimizer=optimizer, distribute=distribute)\r\n\r\n      input_a = np.zeros((10, 3))\r\n      input_b = np.zeros((10, 4))\r\n      targets = np.zeros((10, 5))\r\n      dataset = dataset_ops.Dataset.from_tensor_slices(({'zz_input_a': input_a, 'aa_input_b': input_b}, targets))\r\n      dataset = dataset.repeat(100)\r\n      dataset = dataset.batch(10)\r\n\r\n      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\r\n\r\nif __name__ == '__main__':\r\n  test.main()\r\n```", "I don't think this is an issue in TF 2 anymore."]}, {"number": 21388, "title": "[deleted]", "body": "[deleted]", "comments": []}, {"number": 21387, "title": "Fix issue with tf.nn.relu6 grad in eager", "body": "This fix tries to address the issue raised in #21380\r\nwhere the gradient of tf.nn.relu6 throws out an error.\r\nThe issue seems to be that relu6 was not placed\r\nin OpDoesntRequireInput like tf.nn.relu.\r\n\r\nThis fix fixes #21380.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 21386, "title": "Fix the name property for the _UnreadVariable class in eager mode", "body": "Fixes #21384 \r\nThis works, but I'm not entirely sure this solution is the right approach, your comments are most welcome.", "comments": ["Can you add a test?\r\n\r\nAlso, I think returning self in eager mode is perfectly fine and simpler than this.", "Thanks for merging, I'll make sure to add tests next time.  Cheers!"]}, {"number": 21385, "title": "Support empty inputs in some maxpool kernels. (#21338)", "body": "This only partially remedies #21338. A more serious issue is that such bugs cannot be detected by the unit test framework and the cudaError will leak into other ops.\r\n\r\nSimilar issues probably exist in many other ops as well.", "comments": ["Guessing by your descriptions that you are unable to write tests for this? Is this correct?\r\n\r\nAlso, the github issue you tried to link seems to be a deleted/closed issue. Is there more context for this. Thanks! ", "I linked to the wrong issue, sorry. The issue is: https://github.com/tensorflow/tensorflow/issues/21338\r\n\r\n> Guessing by your descriptions that you are unable to write tests for this? Is this correct?\r\n\r\nThere is already a test for this but the test is not functioning. See the issue threads for details.", "Nagging Reviewer @zheng-xq: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 97 days with no activity and the `awaiting review` label has been applied."]}, {"number": 21384, "title": "_UnreadVariable name property fails in TF 1.10-rc1", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.0-rc0-18-ge5e9a8f4e9 1.10.0-rc1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `tfe.Variable(1).assign(2).name` in eager mode.\r\n\r\n### Describe the problem\r\nIn eager mode, the result of an assignment is an instance of the class `tensorflow.python.ops.resource_variable_ops._UnreadVariable`. When you call its `name` property, it tries to access `self._parent_op.name`, but its `_parent_op` attribute is `None`, so there is an exception. This is a regression in TF 1.10-rc1, since it worked fine in TF 1.9.0.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\ntf.enable_eager_execution()\r\n\r\ntfe.Variable(1).assign(2).name\r\n```\r\n\r\nOutputs the following exception:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1082, in name\r\n    return self._parent_op.name\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```\r\n\r\nSince `str()` and `repr()` use the `name` property, they fail as well. This makes it impossible to call `assign()` in a Python shell, since the shell automatically tries to display the output:\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> import tensorflow.contrib.eager as tfe\r\n>>> tf.enable_eager_execution()\r\n>>> tfe.Variable(1).assign(2)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 264, in __repr__\r\n    self.name, self.get_shape(), self.dtype.name,\r\n  File \"/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1082, in name\r\n    return self._parent_op.name\r\nAttributeError: 'NoneType' object has no attribute 'name'\r\n```", "comments": ["I looked a bit into this issue:\r\n* the `_UnreadVariable` is a subclass of `ResourceVariable`\r\n* The `ResourceVariable` class has a `name` property which returns `self._handle_name`.\r\n* In TF 1.9.0, the `_UnreadVariable` just inherited from the `name` property of the `ResourceVariable`class. This worked.\r\n* In TF 1.10.0-rc1, the `_UnreadVariable` class now overrides the `name` property and returns `self._parent_op.name` instead. This fails.\r\n\r\nA quick & dirty workaround is to just remove the override:\r\n\r\n```pycon\r\n>>> import tensorflow as tf\r\n>>> import tensorflow.contrib.eager as tfe\r\n>>> tf.enable_eager_execution()\r\n>>> from tensorflow.python.ops.resource_variable_ops import _UnreadVariable\r\n>>> del _UnreadVariable.name\r\n>>> print(tfe.Variable(1).assign(2))\r\n<tf.Variable '' shape=() dtype=int32, numpy=2>\r\n```\r\n\r\nNo exception. However, it displays an empty name even when I set the variable's name:\r\n\r\n```pycon\r\n>>> print(tfe.Variable(1, name=\"test\").assign(2))\r\n<tf.Variable '' shape=() dtype=int32, numpy=2>\r\n```\r\n\r\nI guess that's what the overridden `name` property was supposed to fix.", "After further investigation, it seems that the issue comes from the fact that the `assign()` method calls `gen_resource_variable_ops.assign_variable_op()` which returns an assignment operation in graph mode, but it returns `None` in eager mode. Then the `assign()` method returns `self._lazy_read(assign_op)`, which in eager mode is equivalent to `self._lazy_read(op=None)`.  Here is the `_lazy_read()` method:\r\n\r\n```python\r\n  def _lazy_read(self, op):\r\n    if self.trainable:\r\n      tape.watch_variable(self)\r\n    return _UnreadVariable(\r\n        self._handle, self.dtype, self._shape, self._in_graph_mode,\r\n        self._handle_deleter if not self._in_graph_mode else None, op,\r\n        self._unique_id)\r\n```\r\n\r\nThis method creates an `_UnreadVariable` with its `parent_op=op`, which in this case is `None`. So of course `self.parent_op.name` will fail.", "I added a `parent_name` argument to the `_UnreadVariable` class, and instead of setting `_handle_name=\"\"` when in eager mode, I set it to this `parent_name`.  Then I tweaked the `_lazy_read()` method to make it set `parent_name=self._handle_name` and I removed the `name` property from the `_UnreadVariable` class. Now everything works fine. I'll submit a PR. Please tell me what you think.\r\n\r\nBTW, I'm not sure why `assign()` doesn't return `self` in eager mode? I'm sure there's a good reason for having an `_UnreadTensor` class in eager mode, but what is it?", "@ageron do you have any idea the purpose of `_UnreadVariable`?", "@chunyang-wen `_UnreadVariable` exists to take some constructs which you are allowed to do with ref variables and implement them for resource variables, such as `tf.assign_add(tf.assign_add(x, y), z)`. I don't find these particularly idiomatic but we cannot break backwards compatibility.", "> @chunyang-wen `_UnreadVariable` exists to take some constructs which you are allowed to do with ref variables and implement them for resource variables, such as `tf.assign_add(tf.assign_add(x, y), z)`. I don't find these particularly idiomatic but we cannot break backwards compatibility.\r\n\r\nBut it seems that it breaks control dependency.\r\n\r\n```python\r\na = (Code which generate `_UnreadVariable`)\r\nb = array_ops.gather(a, indices)\r\n\" b is used elsewhere\r\n```\r\n\r\nIt seems that if I place no control dependency for `a -> b`, the program does not get correct result. But if I add following code, it works properly.\r\n\r\n```python\r\nwith tf.control_dependency([a]):\r\n    b = array_ops.gather(a, indices)\r\n```\r\n\r\nTensorflow version = 1.8, minimal reproduce case is a little hard to extract from our code. Do you have any insights about this problem? Maybe we are mixing the use of `Variable` and `ResourceVariable`", "Does the problem still exist in TF nightly? There were some bugs in\n_UnreadVariable which have been fixed since then.\n\nOn Tue, Apr 23, 2019 at 7:11 PM Chunyang Wen <notifications@github.com>\nwrote:\n\n> @chunyang-wen <https://github.com/chunyang-wen> _UnreadVariable exists to\n> take some constructs which you are allowed to do with ref variables and\n> implement them for resource variables, such as tf.assign_add(tf.assign_add(x,\n> y), z). I don't find these particularly idiomatic but we cannot break\n> backwards compatibility.\n>\n> But it seems that it breaks control dependency.\n>\n> a = (Code which generate `_UnreadVariable`)\n> b = array_ops.gather(a, indices)\" b is used elsewhere\n>\n> It seems that if I place no control dependency for a -> b, the program\n> does not get correct result. But if I add following code, it works properly.\n>\n> with tf.control_dependency([a]):\n>     b = array_ops.gather(a, indices)\n>\n> Tensorflow version = 1.8, minimal reproduce case is a little to extract\n> from our code. Do you have any insights about this problem?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21384#issuecomment-486038437>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLRCRKSSBVKGFKLFV3PR66UDANCNFSM4FN4HFUA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp It is an internal framework based on tensorflow and we can not directly test it on newest tensorflow because we have implemented couple of OPs and we have to do some dirty work to fully upgrade to newest tensorflow. How can I search those bug fixes? Maybe after investigating the issues and fixes, I can find out what happens with my program and _UnreadVariable.", "We do not backport fixes to older versions. Sorry, but on this you're on\nyour own. You can try to cherry-pick the changes since then which have\nincluded the tensorflow/python/ops/resource_variable_ops.py but this might\nbreak other things.\n\nOn Wed, Apr 24, 2019 at 7:07 PM Chunyang Wen <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> It is an internal framework based on\n> tensorflow and we can not directly test it on newest tensorflow because we\n> have implemented couple of OPs and we have to do some dirty work to fully\n> upgrade to newest tensorflow. How can I search those bug fixes? Maybe after\n> investigating the issues and fixes, I can find out what happens with my\n> program and _UnreadVariable.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21384#issuecomment-486490979>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJ6K73GZOCSHEUDRYDPSEG77ANCNFSM4FN4HFUA>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 21383, "title": "Using tfrecord + dataset API  + estimator, how to add dynamic input to randomly augment data like in old version", "body": "I am now using tensorflow 1.9 and I want to use tfrecord + dataset API  + estimator.\r\nHowever, I find a question.\r\nFor example, I use tfrecord to record the input image and label. However, in each training step, I want to generate random indices to delete the value of some pixels. The indices is generated in each step.\r\n\r\nBefore I used old version tensorflow, I can do the following\r\nplaceholder 1 : image from dataset\r\nplaceholder 2: label from dataset\r\nplaceholder 3: indices tf.random\r\nAnd I can use tf.gather_nd(image, indice=inidices), so I can randomly augment the input image.\r\n\r\nHowever, I do not know how to feed the random numpy array in tensorflow 1.9\r\n", "comments": [" I use tf_record to record my data and label. However, I also want to use a random input which cannot be recorded by tf_record. Usually, in old tensorflow version, we can use the following code:\r\n\r\nimage = tf.placeholder(tf.float32, shape=(None, height,width,3))\r\nindices = tf.placeholder(tf.float32, shape=(None, height,width))\r\nimage_filter = tf.gather_nd(image, indices)\r\nAnd here, the indices will be fed a value of\r\n\r\nindice = np.random.rand()\r\nwhich is changeable and cannot be recorded before training. If using `\r\n\r\nsess.run(......., feed_dict = {.......,'indices':indice})\r\nthis can be very easy to feed the indices.\r\n\r\nHowever, I do not know how to implement this while using estimator.train as I do not know where to add the definition of indices and where to feed it into the network.", "That is not a place for usage questions.\r\nPlease do it for example on stackoverflow.com", "Nagging Assignee @tatianashp: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@LZDSJTU So sorry this fell through the cracks. If you still have this question please ask it on StackOverflow. [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) "]}, {"number": 21382, "title": "ImportError: No module named '_pywrap_tensorflow'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Microsoft Windows [Version 6.1.7601] \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pip command\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: Python 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI have researched most of the issue but it seems none of them resolves mine. \r\n\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nMicrosoft Windows [Version 6.1.7601]\r\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\r\n\r\nC:\\Users\\Anmol Anand>python\r\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)]\r\n on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(_\r\n_file__)])\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.p\r\ny\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(_\r\n_file__)])\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.p\r\ny\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\Anmol Anand\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-\r\npackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_st\r\narted/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>", "comments": ["I suspect this is a duplicate of https://github.com/tensorflow/tensorflow/issues/19584\r\nWhat is your CPU make and model?", "Nagging Assignee @gunan: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 21381, "title": "Is it possible to pass a PyObject into tf.py_func?", "body": "Some times we need to use some information inside tf.pyfunc, and its unnecessary to convert these information into basic types that can feed into tensorflow.\r\n\r\nFor example, I don't know how to pass a pyObject into  tf.py_func, so I must write like this:\r\n```python\r\nimport pickle\r\nfrom typing import List\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata: List[int] = [1, 2, 3]\r\nextra_info: List[object] = [{1: \"Some python object that cannot be easily converted into basic types\"},\r\n                            {2: \"Some python object that cannot be easily converted into basic types\"},\r\n                            {3: \"Some python object that cannot be easily converted into basic types\"}\r\n                            ]\r\n\r\n\r\ndef some_func(one_data: int, one_info_serialized: str):\r\n    one_extra_info = pickle.loads(one_info_serialized)\r\n    # just imagine I will use the object\r\n    print(one_extra_info)\r\n    return np.int32(0)\r\n\r\n\r\ndata_pl = tf.placeholder(dtype=tf.int32, shape=())\r\ninfo_pl = tf.placeholder(dtype=tf.string)\r\nop = tf.py_func(some_func, [data_pl, info_pl], tf.int32)\r\nsession = tf.Session()\r\n\r\nfor one_data, one_info in zip(data, extra_info):\r\n    session.run(op, feed_dict={data_pl: one_data,\r\n                               # How can I pass this PyObject directly?\r\n                               info_pl: pickle.dumps(one_info)})\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@draplater If you want to pass non-tensor arguments to a `py_func` you can do so quite easily by wrapping your `py_func` definition in a wrapper where the desired variable is within scope.\r\n\r\nFor example something like this will work:\r\n\r\n```\r\ndef mk_some_func(extra_info):\r\n    def some_func(tensor0, tensor1, ...):\r\n        # Use `extra_info` as a py object here\r\n        return np.int32(0)\r\n    return some_func\r\n```\r\n\r\nThen when you want to create your py_func:\r\n```\r\nop = tf.py_func(mk_some_func(extra_info), [data_pl, info_pl], tf.int32)\r\n```\r\n\r\nRemember that if `extra_info` is a graph-time tensor, this method will not work. Also if the info you wish to pass is redefined with each session, you will need to get cute with how you modify the `extra_info` dictionary (etc). However if the data will not change for the given session, the above method should work out.", "@sabhiram Thanks for the help!\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there (and it also supports helpful members of the community a bit better than we can here).\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21380, "title": "tf.GradientTape.gradient raise error with tf.nn.relu6", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.10.0-rc1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\ntf.GradientTape.gradient raise error with tf.nn.relu6\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\nw = tf.contrib.eager.Variable([[1.0]])\r\nwith tf.GradientTape() as tape:\r\n    loss = tf.nn.relu6(w * w)\r\ngrad = tape.gradient(loss, w)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 858, in gradient\r\n    output_gradients=output_gradients)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 63, in imperative_grad\r\n    tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 116, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\", line 387, in _Relu6Grad\r\n    return gen_nn_ops.relu6_grad(grad, op.outputs[0])\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n", "comments": ["Added a PR #21387 as an attempt for the fix.", "Thanks for the fix @yongtang ! Marking as \"Contributions Welcome\" since your PR is a welcome contribution :)", "I should mention that since the fix won't be in 1.10, the following workaround can be used till then:\r\n\r\n```python\r\n@tf.custom_gradient\r\ndef my_relu6(x):\r\n    out = tf.nn.relu6(x)\r\n    from tensorflow.python.ops import nn_ops\r\n    def grad(dy):\r\n        return nn_ops.relu6_grad(dy, out)\r\n    return out, grad\r\n```\r\n\r\nAnd then use `my_relu6` instead of `tf.nn.relu6`. For example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\nw = tf.contrib.eager.Variable([[1.0]])\r\nwith tf.GradientTape() as tape:\r\n    loss = my_relu6(w * w)\r\ngrad = tape.gradient(loss, w)\r\n```"]}, {"number": 21379, "title": "Need solution to install TensorFlow on computer without using an Internet Connection.", "body": "I want to install tensorflow on my pc. but due to internet connectivity issues my pc is not having an internet connection. So please tell me the solution to install into my pc. i am very new in the world of programming. So, please provide me the simple procedure of it. ", "comments": ["@ankitjhagithub  Youcan download .whl files and install it in your pc. [More info on `whl` please refer here](https://wheel.readthedocs.io/en/stable/)\r\n\r\n\r\n-\r\n[.whl files download link](https://pypi.org/project/tensorflow-gpu/#files)", "I have downloaded it but not able to do any further. Explain solution please\n\nOn Sat, Aug 4, 2018, 21:34 Madhan Varadhodiyil <notifications@github.com>\nwrote:\n\n> @ankitjhagithub <https://github.com/ankitjhagithub> Youcan download .whl\n> files and install it in your pc. More info on whl please refer here\n> <https://wheel.readthedocs.io/en/stable/>\n> .whl files download link <https://pypi.org/project/tensorflow-gpu/#files>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21379#issuecomment-410459192>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZkoy1p6RlhmmNB1j0yh-_ZCVlSfX3hCks5uNcYegaJpZM4Vu7Li>\n> .\n>\n", "pip install downloaded.whl", "Hi @ankitjhagithub just like @varadhodiyil mentioned you can download it first and then install it.\r\n\r\nDownload the whl from another machine [here](https://files.pythonhosted.org/packages/37/ff/97d4542f805ae25bf4b65b6263515584c78bd9a6111ed78ea971eff2946a/tensorflow-1.9.0-cp27-cp27mu-manylinux1_x86_64.whl) and then run the following command on the copied whl file on your machine.\r\n\r\nExample (Python2 Linux):\r\n```\r\npip install tensorflow-1.9.0-cp27-cp27mu-manylinux1_x86_64.whl\r\n```", "thank you.\n\nOn Tue, Aug 7, 2018 at 4:53 AM, Amit Patankar <notifications@github.com>\nwrote:\n\n> Hi @ankitjhagithub <https://github.com/ankitjhagithub> just like\n> @varadhodiyil <https://github.com/varadhodiyil> mentioned you can\n> download it locally first and then install it.\n>\n> Download the whl from another machine here\n> <https://files.pythonhosted.org/packages/37/ff/97d4542f805ae25bf4b65b6263515584c78bd9a6111ed78ea971eff2946a/tensorflow-1.9.0-cp27-cp27mu-manylinux1_x86_64.whl>\n> and then run the following command on the copied whl file on your machine.\n>\n> Example (Python2 Linux):\n>\n> pip install tensorflow-1.9.0-cp27-cp27mu-manylinux1_x86_64.whl\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/21379#issuecomment-410883142>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZkoy-Cd9HcoCOqaOsxIsmi_FDnEif5Wks5uONANgaJpZM4Vu7Li>\n> .\n>\n\n\n\n-- \nAnkit Jha\nIndian Institute of Information technology, Sri City, Chittoor, Andhra\nPradesh-517646\nMob: +91-9764194921\n"]}, {"number": 21378, "title": "lib / pip_package: only include licenses if option is enabled", "body": "The license files were included unconditionally so the whole dependency\r\nneeds to be downloaded even if disabled and not used. This puts them\r\nbehind in a select() so they are only included when the feature is\r\nenabled for the build and significantly reduces the dependencies when\r\nconfigured without all the cloud options.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["I fixed the buildifier format change from https://source.cloud.google.com/results/invocations/d989dfc3-e01c-434a-8bec-4220f8fbbfc7/log\r\n"]}, {"number": 21377, "title": "Tflite does not seem to support LSTM.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux release 7.3.1611\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: cuda=8.0.61 cudnn=7.0.5\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Source code / logs\r\nSee below.\r\n\r\n### Describe the problem\r\nI'm trying to convert a `.pb` graph into tflite format, which containes a bi-directional LSTM. However, whenever I run the command:\r\n```\r\ntoco --graph_def_file=opt_frozen.pb \\\r\n--output_file= output.tflite \\\r\n--output_format=TFLITE \\\r\n--inference_type=FLOAT \\\r\n--inference_input_type=FLOAT \\\r\n--input_arrays=[input_tensor_names] \\\r\n--output_arrays=[output_tensor_names]\r\n```\r\nIt failed with following error messages:\r\n```\r\n2018-08-03 14:35:41.259412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.259490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.271357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.271401: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.271439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.271454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.279573: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.279626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.279989: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280043: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280087: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2018-08-03 14:35:41.280107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2018-08-03 14:35:41.280123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280141: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LogicalAnd\r\n2018-08-03 14:35:41.280175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond\r\n2018-08-03 14:35:41.280224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280303: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2018-08-03 14:35:41.280317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2018-08-03 14:35:41.280340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2018-08-03 14:35:41.280362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ReverseV2\r\n2018-08-03 14:35:41.280385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280433: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280469: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2018-08-03 14:35:41.280487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayV3\r\n2018-08-03 14:35:41.280502: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LogicalAnd\r\n2018-08-03 14:35:41.280546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LoopCond\r\n2018-08-03 14:35:41.280588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Exit\r\n2018-08-03 14:35:41.280671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArraySizeV3\r\n2018-08-03 14:35:41.280693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayGatherV3\r\n2018-08-03 14:35:41.280965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\r\n2018-08-03 14:35:41.280982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.280997: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\r\n2018-08-03 14:35:41.281074: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n2018-08-03 14:35:41.281102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ReverseV2\r\n2018-08-03 14:35:41.281123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayScatterV3\r\n2018-08-03 14:35:41.281138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Enter\r\n2018-08-03 14:35:41.281157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayReadV3\r\n2018-08-03 14:35:41.281228: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: TensorArrayWriteV3\r\n```\r\nAnd I assume that LSTM is not currently supported in tflite.\r\nSo is that the case? And if it's true, is there any way that I can work around this problem?\r\n\r\n", "comments": ["You can generate a LSTM cell indidivually and manually loop back state (i.e. don't use static_rnn or dynamic_rnn). It will convert the lstm as individual operators. For more help, please provide a simple example that you are trying to run.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@aselle  I have a simple example to volunteer :) I wish you would take mine as an example! Full source is https://colab.research.google.com/drive/1sB8vv3TM5Sp45TicJ8PxEvpUAX42XqPY\r\n and I can gladly provide sample data as well.\r\n\r\nHere's the part specific to consrtructing the model:\r\n\r\n```python\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, LSTM, Dropout\r\n\r\nregressor = Sequential()\r\nregressor.add(LSTM(units=nh, return_sequences=True, \r\n                   input_shape=(timesteps, dim)))\r\n\r\nlayers = (len(endog) + 1) if len(endog) > 1 else 2 # 4\r\nfor i in range(1, layers):\r\n  if i < layers - 1:\r\n    cell = LSTM(units=nh, return_sequences=True)\r\n  else:\r\n    cell = LSTM(units=nh)\r\n\r\n  regressor.add(cell)\r\n  \r\nregressor.add(Dropout(dropout)) # 0.2\r\nregressor.add(Dense(units=dim))\r\n              \r\nregressor.compile(optimizer='adam', loss='mean_squared_error')\r\n```\r\n\r\nthis is a pretty small/simple LSTM  for use with categorical and continuous time series data, with, for my data and prediction range, about 4 layers of about 12 nodes + dropout and output."]}, {"number": 21376, "title": "Tensorflow Lite Don't supoort squared_difference?", "body": "I have made a model trained using tensorflow. \r\nWhen I transfer it to tensorflow lite, it has not supported squared difference operation!\r\nHow can I solve this problem?\r\nThanks very much!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "When I convert Tensorflow object detection api model to tensroflow lite, I have get this problem.\r\nI tensorflow/contrib/lite/toco/import_tensorflow.cc:1053] Converting unsupported operation: SquaredDifference", "What model are you trying to convert? Are you sure its an eval model? Often squared difference comes in the loss function of a training model and right now you should only be trying to convert a frozen eval graph. Or does your eval graph have a squared difference operation?", "Will track this in the above tracking issue, please feel free to add more info about what model you are trying convert and if the graph is training or eval, in this issue. Thanks!", "I got a bug when using toco to transform model\r\n\r\nValueError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node batch_normalization_55/cond/batchnorm/add/y}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1.1e-05>](batch_normalization_55/cond/Switch)\r\n"]}, {"number": 21375, "title": "Raspberry Pi install command not properly formatted.", "body": "Added proper formatting to the install commands.\r\n\r\nPlease refer to this documentation website page: https://www.tensorflow.org/install/install_raspbian\r\n\r\n![image](https://user-images.githubusercontent.com/1383831/43673722-69c097a4-977c-11e8-9489-4766aa4d8b8c.png)\r\n\r\nI have fixed this improper formatting so that the shell commands are properly rendered.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Seems to have been fixed by https://github.com/tensorflow/tensorflow/pull/21424 already.\r\n\r\nThanks for the PR though!", "Ah. Even though I PR'd a fix first haha. \ud83d\ude1e "]}, {"number": 21374, "title": "Move bazel.rc to workspace root to support bazel-0.18.0", "body": "Bazel 0.17.0 will contain a change for which rc files it accepts.\r\nhttps://github.com/bazelbuild/bazel/commit/ec83598cb6ee4136166bb562a24dc5dfa58921db\r\nhttps://github.com/bazelbuild/bazel/issues/4502\r\n\r\nOld bazel used to read %workspace%/tools/bazel.rc. New bazel will not\r\nread that and instead will only read %workspace%/.bazelrc.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nAlso updates bazel rules_closure to the latest and changes ./configure to echo a workspace relative path instead of full path in case the .bazelrc accidentally gets committed.\r\n", "comments": ["There is a fatal error if the .bazelrc imports a .tf_configure.bazelrc that does not exist. I will open a bazel issue to make it nonfatal.\r\n\r\nChanging the import to workspace-relative should make things a little better, but care will still need to be taken to ensure that the import is not accidentally committed to the file anyway.\r\n\r\n```\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n[bazel INFO src/main/cpp/option_processor.cc:203] Looked for a system bazelrc at path '/etc/bazel.bazelrc', but none was found.\r\n[bazel INFO src/main/cpp/option_processor.cc:259] Looking for master bazelrcs in the following three paths: /home/jason/code/venv/tf/tensorflow/tools/bazel.rc, , \r\n[bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /home/jason/code/venv/tf/tensorflow/.bazelrc\r\n[bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /home/jason/code/venv/tf/tensorflow/.tf_configure.bazelrc\r\n[bazel FATAL src/main/cpp/blaze.cc:1271] Unexpected error reading .blazerc file '/home/jason/code/venv/tf/tensorflow/.tf_configure.bazelrc'\r\n```\r\n", "Filed: https://github.com/bazelbuild/bazel/issues/5765", "Thanks for the PR! One question.\r\n\r\nWhy do we have to update io_bazel_rules_closure in WORKSPACE file for this change?", "@case540 oh, i guess we dont. I just saw it was out of date and updated it. Do you want me to remove that commit and make a separate PR for that?", "Yeah, please remove that change from this PR. Only create a separate PR if there is a very specific reason you need to update the dependency. Thanks again", "@case540 done. but maybe we want to wait till bazel decides on how they want to name things before merging this?", "Sounds good to me.", "Nagging Assignee @case540: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Rebased to quiet the bot. The rc file change was rolled back for bazel-0.17 and will be in 0.18 instead so gotta wait a little more so we get the new syntax right.", "Any progress on this? We also need this change to fix TensorFlow failure with Bazel@HEAD on our CI.\r\nPlease see https://github.com/bazelbuild/bazel/issues/5945", "@meteorcloudy when will try-import hit a released version of Bazel? Once I update ./configure to use try-import it'll also need to have the minimum required version set higher too so I was thinking to do this only once bazel is released? Having people use bazel from git is a bunch more complicated.", "Or should I just do this change first using import then we change to try-import a bit later? (but not too long)", "Hi @perfinion , thanks for working on this.\r\n`try-import` should be available from `0.18.0`, since `0.17.0` is still not out yet, we might have to wait a few more weeks.\r\nI think we can do this change first if you think it's ok.", "@meteorcloudy my main concern is if people accidentally commit anything weird in the /.bazelrc file. Everyone will just have to be careful to make sure the import line does not get committed to the file until we switch to try-import.\r\n\r\n@martinwicke @yifeif Can we make one of the test bots double check the .bazelrc line to confirm no PR accidentally commits the \"import %workspace%/.bazelrc\" line? Or if the sanity stuff is in the tree somewhere i'll add a commit to update that together.", "Is the test failure related to this change?", "@meteorcloudy I dont think so, //tensorflow/compiler/xla/tests:exhaustive_f32_elementwise_op_test_cpu has been failing a lot the last couple weeks. The MacOS and android failures I dont recall seeing last time I ran the tests maybe something broke in the mean time. ", "rebased and updated commit messages to 0.18.0 from 0.17.0", "@case540 @meteorcloudy looks like the auto-import failed, you'll have to do it manually i think", "@case540 Can you help importing this change?", "huh, no idea why import/copybara isnt running properly. Yeah, I can handle importing this.", "Didnt forget about this. Will hopefully have this PR pulled internally over the weekend. Just needs some internal update due to the way our migration scripts work (this PR removes an opensource-only file).", "@case540 thanks for the help! is there also something you can add (to the sanity bot maybe?) to flag if a PR accidentally commits the \"import %workspace%/.bazelrc\" line until we get the try-import stuff?\r\nOtherwise it shouldn't be too long before 0.18.0 is out so we can just keep an eye on it carefully.", "@case540 Do you want me to rebase and fix the conflicts? or is it already in the queue?", "Yes, could you fix the conflicts please. I tried to merge this in this morning and hit a bunch of random errors.", "@case540 done :)"]}, {"number": 21373, "title": "nsync dep updates", "body": "A new version of nsync is released, this updates the workspace.bzl and adds nsync to the libs that can be unbundled. I've packaged nsync on Gentoo Linux and tested the full unbundled build.\r\n\r\nThe CMake build is also updated to the latest version, the patch is no longer required and is removed. I have tested the cmake build on my machine but the CI needs to test windows since I can't.\r\n\r\nThe last commits update ./configure to handle adding TF_SYSTEM_LIBS and an option to disable stripping.\r\n\r\n@m3bm3b, can you review too?", "comments": ["The mirroring will need to be updated by whoever has the perms for that, I already added the URL:\r\n\"https://mirror.bazel.build/github.com/google/nsync/archive/1.20.1.tar.gz\"\r\nshould point to: \"https://github.com/google/nsync/archive/1.20.1.tar.gz\",\r\n\r\n@gunan can you trigger the tests so the windows CMake build runs, thats the most important bit :)", "Mirroring of the URL is complete.\r\nHowever, as soon as we started windows bazel tests, cmake build went broken.\r\nThey may be already failing.", "Looks like the windows compile and install succeed in https://source.cloud.google.com/results/invocations/85798d52-c506-4199-b58a-b95d5e54b2cc/log Line 3787. After that the tests fail horribly but that looks like a PYTHON_PATH issue maybe and unrelated to this.", "I managed to build (thought didn't run tests of) tensorflow using \r\ncmake on Linux, MacOS, and Windows using this pull request.\r\n\r\n", "Great, thanks for testing.\r\nThen I will proceed with the merge.", "@samikama has some reservations about this on #20003.", "@drpngx I dropped the BAZEL_STRIP commit and rebased everything. @samikama can remove it completely in #20003 instead.", "Do you want me to rebase and fix the configure.py conflict or are you already doing the merge?", "Sorry for the delay in response.\r\nCould you rebase and fix the conflicts, then I can re-test and merge the PR.", "@gunan no worries, rebased :)"]}, {"number": 21372, "title": "Tensorflow r1.9 build failed to compile sparse_matmul_op.cc on gcc4.8.1 ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 6.9\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: tag r1.9\r\n- **Python version**: Python 2.7.12 :: Anaconda 2.4.1 (64-bit)\r\n- **Bazel version (if compiling from source)**: Build label: 0.16.0- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**: gcc 4.8.1 / binutils 2.28\r\n- **CUDA/cuDNN version**: cuda90/9.0.176, cudnn/7.1.3\r\n- **GPU model and memory**: K80\r\n- **Exact command to reproduce**: bazel build --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" --config=cuda //tensorflow/tools/pip_package:build_pip_package  --verbose_failures\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI cannot build tensorflow from sources.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nThe exact command I can reproduce this bug is below:\r\n\r\n\r\n> external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/sparse_matmul_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/sparse_matmul_op.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote bazel-out/host/bin/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote bazel-out/host/bin/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem bazel-out/host/bin/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -c tensorflow/core/kernels/sparse_matmul_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/sparse_matmul_op/sparse_matmul_op.pic.o >& console.txt\r\n\r\nThe console outputs are attached. \r\n[console.txt](https://github.com/tensorflow/tensorflow/files/2259203/console.txt)\r\n", "comments": ["Maybe this is a gcc 4.8.1 specific bug.\r\nI switched to gcc 5.4.0 and I can build tensorflow now."]}, {"number": 21371, "title": "fix var type issue which breaks crf_decode", "body": "CRF decode can fail when default type of \"0\" (as viewed by math_ops.maximum) does not match the type of sequence_length. \r\n\r\nThis change is parallel in motivation and solution to the fix in https://github.com/tensorflow/tensorflow/commit/c0f1080188c5c6955cfa3b3c086ac262b1e5ec02, for crf_log_norm()=>_multi_seq_fn().", "comments": ["Just modified one of the a couple of the CRF tests to presumably(?) provide better coverage of different var types for sequence_length settings."]}, {"number": 21370, "title": "[Intel MKL] Updating artifact references in Readme.md.", "body": "Hi @gunan. This will update the README.md to show the status of the Intel public CI builds as well as the status of the Jenkins job to build the 1.9 release whls.", "comments": ["@gunan Done", "Hi @gunan, Can you merge this one?\r\n\r\nThanks!"]}, {"number": 21369, "title": "Fix the pip install TLS issue for pip test builds.", "body": "PiperOrigin-RevId: 207319608", "comments": []}, {"number": 21368, "title": "Not able to port a 6-layered mobilenet tflite model to mobile", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nPixel 2\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.8\r\n- **Python version**:\r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n1. Set the mobileNet model endpoint to conv6-depthwise\r\n2. Re-train the model from scratch with using cifar10 dataset\r\n3. Freeze the graph with the checkpoints  \r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import graph_util\r\nimport os,sys\r\n\r\noutput_node_names = \"MobilenetV1/Predictions/Reshape\"\r\nsaver = tf.train.import_meta_graph('/home/users/saman/yitao/tensorflow_android/models/research/slim/batch_32/model.ckpt-156300.meta', clear_devices=True)\r\ngraph = tf.get_default_graph()\r\ninput_graph_def = graph.as_graph_def()\r\nsess = tf.Session()\r\nsaver.restore(sess, \"/home/users/saman/yitao/tensorflow_android/models/research/slim/batch_32/model.ckpt-156300\")\r\noutput_graph_def = graph_util.convert_variables_to_constants(\r\n            sess, # The session is used to retrieve the weights\r\n            input_graph_def, # The graph_def is used to retrieve the nodes\r\n            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\r\n)\r\noutput_graph=\"frozen-model-conv6-bat-32.pb\"\r\nwith tf.gfile.GFile(output_graph, \"wb\") as f:\r\n    f.write(output_graph_def.SerializeToString())\r\nsess.close()\r\n\r\n(4) Optimize the model \r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32.pb \\\r\n--out_graph=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32-optimized.pb \\\r\n--inputs='input' \\\r\n--outputs='MobilenetV1/Predictions/Reshape' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,32,32,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n\r\n(5) Convert the model to tflite\r\n bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32-optimized.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=/home/yitao/TF_1.8/tensorflow/my_frozen_pb/frozen-model-conv6-bat-32-optimized.tflite --inference_type=FLOAT \\\r\n  --input_type=FLOAT --input_arrays=input \\\r\n  --seed2 \\\r\n  --output_arrays=MobilenetV1/Predictions/Reshape --input_shapes=1,32,32,3 \\\r\n  --allow_custom_ops\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nNot able to train a model from scratch and port it Android to utilize the Android Nerual Network api through TFLite. After training a model and following the steps to convert the graph to tflite model, there are still some ops that are not supported by the TFLite runtime in my graph. What should I do? \r\nAny help is apreciated!\r\n\r\nLogcat is throwing the following errors. It seems that those ops are not stripped from the model during the optimization step. \r\n### Source code / logs\r\n \r\n08-03 15:14:52.183 10271-10271/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: android.example.com.tflitecamerademo, PID: 10271\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find custom op for name 'RandomUniform' with version 1\r\n    Didn't find custom op for name 'FLOOR' with version 1\r\n    Didn't find custom op for name 'RSQRT' with version 1\r\n    Didn't find custom op for name 'FIFOQueueV2' with version 1\r\n    Didn't find custom op for name 'QueueDequeueV2' with version 1\r\n    Didn't find custom op for name 'SquaredDifference' with version 1\r\n    Registration failed.\r\n\r\n\r\n", "comments": ["Following is the log of the TOCO converter. \r\n\r\ntensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 224 operators, 311 arrays (0 quantized)\r\n2018-08-03 14:39:47.544660: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 224 operators, 311 arrays (0 quantized)\r\n2018-08-03 14:39:47.544705: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"MobilenetV1/Logits/Dropout_1b/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2018-08-03 14:39:47.547793: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 179 operators, 277 arrays (0 quantized)\r\n2018-08-03 14:39:47.548771: W tensorflow/contrib/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:85] RandomUniform op outputting \"MobilenetV1/Logits/Dropout_1b/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2018-08-03 14:39:47.550304: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 179 operators, 277 arrays (0 quantized)\r\n2018-08-03 14:39:47.552372: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 131072 bytes, theoretical optimal value: 131072 bytes.\r\n\r\nI noticed that the RandomUniform is complaining about the seed. Is there a way I can set the seed for the /dev/random from the system? \r\nThanks. \r\n", "Correct me if I'm wrong, but those ops seem to be part of your model.  If it's part of the model, it won't get stripped and you'll need custom ops for them.  I'm curious about your model can you post it?", "Hi gragundier, \r\n\r\nThanks for your reply. \r\nIt is a simplified version of mobilenet v1. I changed the end point of the mobilenet model. \r\nYou can find the end point in models/research/slim/nets/mobilenet_v1.py\r\nI changed that to Conv2d_6_pointwise layer. \r\n```\r\ndef mobilenet_v1_base(inputs,\r\n                      final_endpoint='Conv2d_6_pointwise',\r\n                       min_depth=8,\r\n                       depth_multiplier=1.0,\r\n                       conv_defs=None,\r\n                       output_stride=None,\r\n                       use_explicit_padding=False,\r\n                       scope=None):\r\n```", "It could be that you are now feeding in a placeholder input. You seem to have a queue and some loss functions. Could you provide the frozen graphdef (or even a screenshot of the graph) so that we can see what else it could be. @gargn, could you comment.", "Hi, \r\n\r\nAttached is the frozen model. \r\n[frozen-model-conv6-bat-32.zip](https://github.com/tensorflow/tensorflow/files/2276239/frozen-model-conv6-bat-32.zip)", "I am still not able to understand this fully, although I was able to find a way to work around this problem. \r\nI need to provide an eval model to the freeze_graph script instead of using the one I save while training my model. \r\nIf I train a model and save the pbtxt file, which contains the graph, should TensoFlow freeze_graph be able to remove all the ops that are not related to inference? Because we only freeze a model when we do not want to update the weights anymore. \r\nI found this quite inconvenient to write up another graph only for inference. \r\nPlease correct me if I have any misunderstanding on this, thanks!", "I ran the following command on the TensorFlow nightly build (installed using the command `pip install tf-nightly`). The command resulted in the error `ValueError: Invalid tensors 'input' were found.`:\r\n\r\n```\r\ntflite_convert \\\r\n--graph_def_file=$TENSORFLOW_FILE \\\r\n--output_file=$TFLITE_FILE \\\r\n--input_arrays=input \\\r\n--output_arrays=MobilenetV1/Predictions/Reshape \\\r\n--input_shapes=1,32,32,3\r\n--allow_custom_ops\r\n```\r\n\r\nI add this point because this error seems different than the one that you noted. I looked into the model using TensorBoard and it appears that your model is a MobileNet training graph containing the ops FIFOQueueV2, QueueDequeueV2, SquaredDifference. TensorFlow Lite only works with eval graphs, not training graphs.\r\n\r\nIn order to create a MobileNet eval graph:\r\n1. Create a separate eval graph. You can use this MobileNet eval script to generate it from the checkpoints: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1_eval.py\r\n2. Pass the correct input and output nodes to the `freeze_graph.py` script. I suggest using the command line tool.\r\n\r\nAfter you do this, try using the `tflite_convert` command above from the tf-nightly build if possible. You might need different inputs and outputs. If you run into any issues during this process, please provide any intermediate files including the meta graph and checkpoint files as well as any updated commands.", "@gargn hi, \r\n\r\nThanks for your reply!\r\nMy workaround is below, which I believe is similar to what you pointed out. \r\nIt seems that the construct of the eval graph is based on the slim and arg_scope.\r\nBut what should I do if I am trying to train/deploy a custom model which does not slim in the model definition? I do not want to rewrite my model using slim.....\r\nCan you tell me what exactly does the eval model contain? \r\n\r\n```\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\nfrom nets import mobilenet_v1\r\n\r\nNUM_CLASSES = 10\r\n\r\ndef export_eval_pbtxt():\r\n  \"\"\"Export eval.pbtxt.\"\"\"\r\n  with tf.Graph().as_default() as g:\r\n    images = tf.placeholder(dtype=tf.float32,shape=[None,32,32,3])\r\n    # using one of the following methods to create graph, depends on you\r\n    #_, _ = mobilenet_v1.mobilenet_v1(inputs=images,num_classes=NUM_CLASSES, is_training=False)\r\n    with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=False,regularize_depthwise=True)):\r\n      _, _ = mobilenet_v1.mobilenet_v1(inputs=images, is_training=False, depth_multiplier=1.0, num_classes=NUM_CLASSES)\r\n    eval_graph_file = '/home/users/saman/yitao/tensorflow_android/models/research/slim/mobilenet_v1_eval.pbtxt'\r\n    with tf.Session() as sess:\r\n        with open(eval_graph_file, 'w') as f:\r\n            f.write(str(g.as_graph_def()))\r\n\r\ndef main():\r\n    print(\"python main function\")\r\n    export_eval_pbtxt()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "I trained a straightforward model which contains only two convolutional layers. The freeze and tflite conversion when smoothly, but when I deploy to mobile, the application through a segmentation fault. \r\nThanks. ", "Since you are able to convert and the segfault is a new issue, can you please provide the resulting segfault stack trace/core dump? ", "Hi following is the error message. Is that the stack trace you referred to? \r\n\r\n08-19 13:39:32.244 1583-1663/? A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x0 in tid 1663 (CameraBackgroun), pid 1583 (flitecamerademo)\r\n08-19 13:39:32.244 1134-1161/? I/ActivityManager: Displayed android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity: +401ms\r\n08-19 13:39:32.272 1724-1724/? I/crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone\r\n08-19 13:39:32.272 873-873/? I//system/bin/tombstoned: received crash request for pid 1583\r\n08-19 13:39:32.272 1724-1724/? I/crash_dump64: performing dump of process 1583 (target tid = 1663)\r\n08-19 13:39:32.273 1724-1724/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n    Build fingerprint: 'google/walleye/walleye:8.1.0/OPM2.171019.029/4657601:user/release-keys'\r\n    Revision: 'MP1'\r\n    ABI: 'arm64'\r\n    pid: 1583, tid: 1663, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<\r\n    signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n    Cause: null pointer dereference\r\n        x0   00000072d8403000  x1   0000000000000000  x2   0000000000060000  x3   00000072d8403000\r\n        x4   0000000000060000  x5   00000072d8463000  x6   0000000000000000  x7   0000000000430001\r\n        x8   00000072ea9538a0  x9   0000000000000000  x10  00000072ea953800  x11  0000000000000050\r\n        x12  0000007378abe250  x13  4d4630dd011c21f3  x14  000000737a2d0000  x15  ffffffffffffffff\r\n        x16  00000072dea53008  x17  00000073792b02f0  x18  000000000000000a  x19  00000072f5842900\r\n        x20  000000000000002a  x21  00000072ea9d6500  x22  0000000000000050  x23  0000000000000000\r\n        x24  00000072ea9d6500  x25  00000072ea953800  x26  00000072ea9d6538  x27  00000072f5842920\r\n        x28  0000000000000000  x29  00000072d82f8110  x30  00000072de995a94\r\n        sp   00000072d82f8110  pc   00000073792b03d8  pstate 0000000020000000\r\n08-19 13:39:32.279 1724-1724/? A/DEBUG: backtrace:\r\n        #00 pc 000000000001c3d8  /system/lib64/libc.so (memcpy+232)\r\n        #01 pc 00000000000bba90  /data/app/android.example.com.tflitecamerademo-KoNQ6lWiyX75U9zVntbapA==/lib/arm64/libtensorflowlite_jni.so\r\n        #02 pc 00000000000d9aa8  /data/app/android.example.com.tflitecamerademo-KoNQ6lWiyX75U9zVntbapA==/lib/arm64/libtensorflowlite_jni.so\r\n        #03 pc 00000000000122e4  /data/app/android.example.com.tflitecamerademo-KoNQ6lWiyX75U9zVntbapA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)\r\n        #04 pc 0000000000553bf0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)\r\n        #05 pc 000000000054ae4c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n        #06 pc 00000000000dc5d0  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+264)\r\n        #07 pc 000000000029b49c  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n        #08 pc 0000000000295a90  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+700)\r\n        #09 pc 0000000000533f50  /system/lib64/libart.so (MterpInvokeStatic+264)\r\n        #10 pc 000000000053ca94  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n        #11 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)\r\n        #12 pc 000000000027b7cc  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame*, art::JValue*)+216)\r\n        #13 pc 0000000000295a70  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+668)\r\n        #14 pc 0000000000532ad8  /system/lib64/libart.so (MterpInvokeVirtual+652)\r\n        #15 pc 000000000053c914  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n        #16 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)\r\n        #17 pc 000000000027b7cc  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame*, art::JValue*)+216)\r\n        #18 pc 0000000000295a70  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+668)\r\n        #19 pc 0000000000532ad8  /system/lib64/libart.so (MterpInvokeVirtual+652)\r\n        #20 pc 000000000053c914  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n        #21 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)\r\n        #22 pc 000000000027b7cc  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame*, art::JValue*)+216)\r\n        #23 pc 0000000000295a70  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+668)\r\n        #24 pc 0000000000532ad8  /system/lib64/libart.so (MterpInvokeVirtual+652)\r\n        #25 pc 000000000053c914  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n        #26 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)\r\n        #27 pc 000000000027b7cc  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame*, art::JValue*)+216)\r\n        #28 pc 0000000000295a70  /system/lib64/libart.so (_ZN3art11interpreter6DoCallILb0ELb0EEEbPNS_9ArtMethodEPNS_6ThreadERNS_11ShadowFrameEPKNS_11InstructionEtPNS_6JValueE+668)\r\n        #29 pc 0000000000532ad8  /system/lib64/libart.so (MterpInvokeVirtual+652)\r\n        #30 pc 000000000053c914  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n        #31 pc 0000000000275c00  /system/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::DexFile::CodeItem const*, art::ShadowFrame&, art::JValue, bool)+444)\r\n        #32 pc 0000000000525450  /system/lib64/libart.so (artQuickToInterpreterBridge+1052)\r\n        #33 pc 0000000000553d0c  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n08-19 13:39:32.280 1724-1724/? A/DEBUG:     #34 pc 00000000000070f8  /dev/ashmem/dalvik-jit-code-cache (deleted)\r\n08-19 13:39:32.558 764-1668/? I/EaselControlClient: easelConnThread: Opening easel_conn", "Nagging Assignee @gargn: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I was able to get the following code working with last night's tf-nightly. It is based off of the Python code that you provided. The main difference is that it freezes the graph and converts the Flatbuffer to a TFLite model within the Python code itself. Can you clarify if this is what you are looking for:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nslim = tf.contrib.slim\r\nfrom nets import mobilenet_v1\r\n\r\nNUM_CLASSES = 10\r\nMOBILENET_FILENAME = 'PATH-TO-DATA/mobilenet_v1_eval.pbtxt'\r\n\r\nINPUT_ARRAYS = ['input']\r\nOUTPUT_ARRAYS = ['MobilenetV1/Predictions/Reshape']\r\n\r\ndef export_eval_pbtxt():\r\n  \"\"\"Export eval.pbtxt.\"\"\"\r\n  with tf.Graph().as_default() as g:\r\n    # Need to provide the name in order to have the name of the input arrays for conversion.\r\n    images = tf.placeholder(dtype=tf.float32,shape=[None,32,32,3], name=INPUT_ARRAYS[0])\r\n    # using one of the following methods to create graph, depends on you\r\n    # _, _ = mobilenet_v1.mobilenet_v1(inputs=images,num_classes=NUM_CLASSES, is_training=False)\r\n    with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=False,regularize_depthwise=True)):\r\n     _, _ = mobilenet_v1.mobilenet_v1(inputs=images, is_training=False, depth_multiplier=1.0, num_classes=NUM_CLASSES)\r\n\r\n    with tf.Session().as_default() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        # Freeze the graph so that you can convert to TFLite it later.\r\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n            sess, sess.graph_def, OUTPUT_ARRAYS)\r\n        with open(MOBILENET_FILENAME, 'w') as f:\r\n            f.write(str(frozen_graph))\r\n\r\ndef main():\r\n    print(\"python main function\")\r\n    export_eval_pbtxt()\r\n\r\n    # Convert the graph.\r\n    converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\r\n            MOBILENET_FILENAME, INPUT_ARRAYS, OUTPUT_ARRAYS)\r\n    tflite_model = converter.convert()\r\n\r\n    # Load TFLite model and allocate tensors.\r\n    interpreter = tf.contrib.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke()\r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n    print(output_data)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nIn order to get the `from models import mobilenet_v1` working, I had to download the `models` repository and update the PYTHONPATH via the command: `export PYTHONPATH=PATH-TO-MODELS/models/research/slim:$PYTHONPATH`", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21367, "title": "[tf 1.8.0] tf.nn.swish would have issue when restoring using tf.saved_model.loader.load", "body": "Hi,\r\n\r\nI found an error for tf.nn.swish for tensorflow version **1.8.0**\r\nFirst, I save the model using tf.saved_model API, then I restore the model using the same API. \r\n\r\nIt turns out to have the following error:\r\n\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-3-a18708547b00> in <module>()\r\n      1 sess = tf.Session(graph=tf.Graph())\r\n----> 2 zzz = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], 'model_saved')\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/loader_impl.pyc in load(sess, tags, export_dir, **saver_kwargs)\r\n    217 \r\n    218     # Build a saver by importing the meta graph def to load.\r\n--> 219     saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs)\r\n    220 \r\n    221     if saver:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\r\n   1953       clear_devices=clear_devices,\r\n   1954       import_scope=import_scope,\r\n-> 1955       **kwargs)\r\n   1956 \r\n   1957   if meta_graph_def.HasField(\"saver_def\"):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\r\n    741         name=(import_scope or scope_to_prepend_to_names),\r\n    742         input_map=input_map,\r\n--> 743         producer_op_list=producer_op_list)\r\n    744 \r\n    745     # Restores all the other collections.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.pyc in new_func(*args, **kwargs)\r\n    430                 'in a future version' if date is None else ('after %s' % date),\r\n    431                 instructions)\r\n--> 432       return func(*args, **kwargs)\r\n    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    434                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.pyc in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    487         try:\r\n    488           results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 489               graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    490           results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n    491         except errors.InvalidArgumentError as e:\r\n\r\nFailedPreconditionError: Input 0 ('swish_f32') for 'MatMul_1' was not previously added to ShapeRefiner.\r\n\r\n# my solution\r\n\r\ndef myswish_beta(x):\r\n    \"\"\"\r\n    Swish with beta not-traininable!\r\n    \"\"\"\r\n    beta=tf.Variable(initial_value=1.0,trainable=False,name='swish_beta')\r\n    return x*tf.nn.sigmoid(beta*x)\r\nactivation_fun = myswish_beta", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@pswpswpsw Is this still an issue?"]}, {"number": 21365, "title": "Fix nadam optimizer", "body": "Resolves issues #15035 and #13980.", "comments": ["Thanks for the PR! Could you add a test that would have failed without this change. ", "Thanks for the review! I updated the test. \r\n\r\nThe test wasn't failing before because the \"sparse\" test was creating a sparse vector that was actually dense (i.e. it's of size 2 and values are being specified for both indices). FWIW it looks like a number of other optimizers (at least Adam, LazyAdam) have the same shortcoming in their tests.", "Alex, it looks like you reviewed the original nadam PR https://github.com/tensorflow/tensorflow/pull/9889. I'm not familiar with the nadam algorithm.", "Hey Emma!", "Hey Alex, thanks for the review!", "oops sorry didn't realize this didn't pass lint checks until now -- should be fixed.", "@strubell could you pull rebase and push again?", "Sorry I've been travelling -- will do this soon. Thanks for taking a look! ", "@strubell could you pull rebase and push again?", "Please send this PR to tensorflow/addons instead; contrib is deprecated and stated to be removed so we're not making new changes to it."]}, {"number": 21364, "title": "@tf.custom_gradient fails in eager execution mode", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX High Sierra 10.13.2\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Running on CPU\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n@tf.custom_gradient does not work in eager execution mode although the documentation states it does.\r\n\r\n### Source code / logs\r\nProviding a minimal example to reproduce:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntfe = tf.contrib.eager\r\ntf.enable_eager_execution()\r\n\r\n@tf.custom_gradient\r\ndef log1pexp(x):\r\n    e = tf.exp(x)\r\n\r\n    def grad(dy):\r\n        return dy * (1 - 1 / (1 + e))\r\n    return tf.log(1 + e), grad\r\n\r\nwith tf.GradientTape() as tape:\r\n    x = tf.constant(100.)\r\n    y = log1pexp(x)\r\n\r\nprint(tape.gradient(y, [x]))\r\n# prints None as default derivative is undefined\r\n# not working properly - it did not use the custom gradient func (and print 1.0)\r\n\r\n# Graph mode is working OK!\r\n# x = tf.constant(100.)\r\n# y = log1pexp(x)\r\n# dy = tf.gradients(y, x)\r\n#\r\n# with tf.Session() as sess:\r\n#     print(sess.run(dy))\r\n# # prints 1.0 - the custom derivative\r\n```", "comments": ["Short answer: Adding a [`tape.watch(x)`](https://www.tensorflow.org/api_docs/python/tf/GradientTape#watch) will make this work, so:\r\n\r\n```python\r\nwith tf.GradientTape() as tape:\r\n    x = tf.constant(100.)\r\n    tape.watch(x)\r\n    y = log1pexp(x)\r\n\r\nprint(tape.gradient(y, [x]))\r\n```\r\n\r\nAs per the [documentation for `GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape), trainable `Variable` objects are watched automatically, but tensors aren't.\r\n\r\n@RamNathaniel : While we figure out how to make it harder to trip over this quirk, I hope you can continue to make progress (and it's fine to call `tape.watch(x)` in graph as well, so the same code will work).", "Thank you for the very quick response - much appreciated!", "Ok, I'll get to my question directly: So, If I have a model with a gradient reversal layer, i.e. a custom layer with no trainable variables, that calculates tf.identity(x) in the forward pass, but should return tf.negative(grad) in the backward pass, what should I monitor in this case? the layer itself?\r\n"]}]