[{"number": 8512, "title": "Fixed pooling layer example.", "body": "And improved consistency by keeping same name over whole example.", "comments": ["Can one of the admins verify this patch?", "Ping @sguada, should be a really easy change.", "Jenkins, test this please."]}, {"number": 8511, "title": "Documentation for serving canned estimators", "body": "Hey, \r\n\r\nthe documentation on how to serve canned estimators with tensorflow serving is somewhat confusing. The docstring for export_savedmodel says to provide a function returning an InputFnOps which isn't documented anywhere. In the source it says it moved to estimator/export.py and InputFnOps was renamed to ServingInputReceiver. ~~However, this does not exist at all.~~ [I might have been confused by multiple checked out versions here, I suppose]\r\n\r\nDigging around in the tests for estimator it all is becoming a bit clearer but it would be great if there were more documentation on this topic (also from the serving side as I now have my model exported but cannot get any predictions out of it :( )\r\n\r\nThanks!\r\n\r\nChristian\r\n\r\nMentions @martinwicke and @ispirmustafa", "comments": ["Hey,\r\n\r\nI'd really appreciate an answer here, as the integration between tensorflow and tensorflow serving is not really working for me. I have installed the latest python distribution with pip which says it's version 1.0.1. This version does not contain estimator/export.py so I will not be confused again by what is there and what isn't.\r\n\r\nI've created my model and exported it with export_savedmodel and this seems to be served just fine (except of course, there's no way to see what serving serves).\r\n\r\nI used the following code to create the input function\r\n\r\n```\r\nfrom tensorflow.contrib.layers.python.layers import feature_column as feature_column_lib\r\n\r\nfeature_columns = [\r\n      feature_column_lib.real_valued_column(\r\n          'pred_me', dimension=1)\r\n  ]\r\nfeature_spec = feature_column_lib.create_feature_spec_for_parsing(\r\n      feature_columns)\r\n\r\nserving_input_fn = tf.contrib.learn.utils.build_parsing_serving_input_fn(feature_spec)\r\n```\r\n\r\nTo call this the documentation says something about tf.Examples which are nowhere defined (except inside the proto files) but I suppose this was not the right function to use. There's also build_default_serving_input_fn which would seem to be the right function to use for me but I cannot get this to work either as it is not too clear what the input should be.\r\n\r\nI did some digging around and came up with this code\r\n\r\n```from tensorflow.python.framework import constant_op\r\n\r\nfeature_spec = {'pred_me': constant_op.constant([2.0])}\r\nserving_input_fn = tf.contrib.learn.utils.input_fn_utils.build_default_serving_input_fn(feature_spec)```\r\n\r\nwhich comes from the documentation (well test) of a newer code base and actually uses the renamed function build_raw_serving_input_receiver_fn. But this comes back with an exception: **ValueError: 'Const_19:0' is not a valid scope name** which kind of makes sense because within build_raw_serving_input_receiver_fn there's code that transforms the tensor's name like this \r\n\r\n```# Reuse the feature tensor name for the placeholder, excluding the index\r\n      placeholder_name = t.name.split(':')[0]```\r\n\r\nwhich is missing in the originial tf.contrib.learn.utils.input_fn_utils.build_default_serving_input_fn\r\n\r\n```features_placeholders[name] = array_ops.placeholder(dtype=t.dtype,\r\n                                                          shape=shape,\r\n                                                          name=t.name)\r\n```\r\n\r\nSo is this just a bug in the old library still distributed with pip or how is this supposed to work.\r\n\r\nI really do appreciate what you are doing here but this really needs some clarification.\r\n\r\nchris ", "@davidsoergel could you please take a look?", "Any progress on this? I am having the same issue.\r\n@dale-cooper @davidsoergel ", "There is a related thread https://github.com/tensorflow/serving/issues/228#issuecomment-300146926 where I provided a solution.\r\nI think I figured it out, I'm not sure how orthodox my solution is, but at least it works for me.\r\n\r\nA lot of people try to use canned estimators right now (especially wide and deep one), but because there is no documentation majority is confused.\r\n\r\nI'd be willing to contribute if help is welcome.", "@MtDersvan: pending review by @dr4b @wolffg we'll be very excited to have the your tutorial and publish it on tensorflow.org.", "I am also fighting with the API to be able to save a model + weights from an `Estimator` and then do prediction elsewhere (without necessarily using TF Serving). See here for details: http://stackoverflow.com/questions/43966073/prediction-from-model-saved-with-tf-estimator-estimator-in-tensorflow\r\n\r\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "Additional information: we're moving canned estimators to core and\ndocumentation will be part of that.\n", "Still no good documentation ons serving and monitoring. I think this bug need to be reopened.", "Have you seen https://www.tensorflow.org/serving/ ?", "I'm trying to write a C++ application that loads a SavedModel and feeds it tensors in the same process. I'm using Tensorflow 1.4 and it seems to me that there is still too little documentation on how to do this. The documentation at https://www.tensorflow.org/programmers_guide/saved_model comes close, but should have an example using build_raw_serving_input_receiver_fn() in Python and then show how to feed the data in C++.", "The source for that guide is in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/saved_model.md\r\n\r\nIf you send a PR, we can add such an example. I think it would be helpful.", "I hope the tutorial of serving an estimator can have a complete example for new users to understand and try. At the end, it says to refer to more examples but the git repository does not have any examples of how to serve an estimator (especially the pre-made estimators).  ", "NEED MORE TUTORIALS OF SERVING"]}, {"number": 8510, "title": "Error running TF just after install", "body": "Hello, guys.\r\nI got an error just after install Python 3.5.3 and Tensorflow in a Windows 10 system and it fails in the very first test, the one suggested in Tensorflow install instructions.\r\nThe messages I got are here:\r\n\r\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello=tf.constant('Hello')\r\n>>> sess=tf.Session()\r\n>>> print(sess.run(hello))\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'Hello'\r\n>>>\r\n\r\nDo you know how to fix this issue.\r\n\r\nThanks in advance,\r\n\r\nDaniel Vermes\r\n", "comments": ["Duplicate of #7859", "Working now. Thanks a million.", "Great to know! Thanks for posting the issue.", "Humm build 114-windows-GPU does not work for me... Any ideas ?\r\nWait for 115-windows-GPU ?\r\n\r\nEdit:\r\nMy bad, a clean remove an wheel 114 reinstall do the trick.", "This issue is fixed at `HEAD` it's been a while so you could actually ignore it but if you want you could try another wheel like 112. The fix for this wasn't cherry picked to the current release but it will make it to the 1.1r.\r\nJust making sure, you uninstalled TF before installing the nightly, correct?"]}, {"number": 8509, "title": "Hello, guys.", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Stupid entry. Sorry for that."]}, {"number": 8508, "title": "Expose feature importances in TensorForestEstimator", "body": "TensorForestEstimator can log/print feature importances but they are not accessible.\r\n\r\nCould you expose a feature_importances property in TensorForestEstimator or expose the graph builder object instantiated by it so we can access the feature_importances() method?", "comments": ["@gilberthendry could you take a look? ", "feature importances are currently calculated with a TF graph, so can't really expose as a property or a function in the estimator, it only makes sense in the context of a tf.Session (which only exists during training or eval, which is controlled under the hood of estimators).  One option is to include it as an eval metric, would that be ok?", "Sounds good to me", "I've added support to get feature importances during inference/eval time.  It should make its way into the nightly build shortly, and an unknown amount of time into an official TF release.   You can just pass report_feature_importances to the estimator=True, and add an eval metric such as:\r\ntf.contrib.learn.metric_spec.MetricSpec(\r\n          lambda x: x,\r\n          prediction_key=eval_metrics.FEATURE_IMPORTANCE_NAME)", "From the comments, it sounds like this is done.  Please close issues once they are done.", "> I've added support to get feature importances during inference/eval time. It should make its way into the nightly build shortly, and an unknown amount of time into an official TF release. You can just pass report_feature_importances to the estimator=True, and add an eval metric such as:\r\n> tf.contrib.learn.metric_spec.MetricSpec(\r\n> lambda x: x,\r\n> prediction_key=eval_metrics.FEATURE_IMPORTANCE_NAME)\r\n\r\nI was looking at the comments in this issue to determine if this functionality was available in the version of TensorFlow that I am using (r1.12).  I notice that the string constant `FEATURE_IMPORTANCE_NAME` is defined in `tensorflow/contrib/tensor_forest/client/eval_metrics.py`, but the code that uses this constant in `tensorflow/contrib/tensor_forest/client/random_forest.py` is not.  Is this a case where the commit in 5c21d55000c03a90281880f0eb55b12fcaa528fe was only partially merged into master?"]}, {"number": 8507, "title": "LocalCLIDebugWrapperSession hides exception messages", "body": "### Environment info\r\nOperating System: osx\r\n\r\nInstalled version of CUDA and cuDNN: no\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: nightly, python3, today\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:1.0.1\r\n\r\n```python\r\nimport tensorflow as tf\r\nwith tf.device('/gpu:0'):\r\n    x = tf.get_variable('W', shape=[10])\r\n\r\nsess = tf.Session()\r\nfrom tensorflow.python import debug as tf_debug\r\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\r\nsess.run(tf.global_variables_initializer())\r\n```\r\n\r\nMy machine doesn't have GPU, and the above code throws:\r\n```\r\nOSError: Dump root directory /var/folders/3t/1kq225bs04j3k_2rbdcn2g34g86_w8/T/tfdbg_fw0uylcr does not exist\r\n```\r\nBut it should throw the device error, like when debug session is not used:\r\n```\r\nCannot assign a device to node 'W': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\r\n```", "comments": ["@ppwwyyxx thank you very much for reporting this issue. I'll take a look at it soon.", "Update: the underlying cause of this bug is that `LocalCLIDebugWrapper` attempts to intercept `OpError`s that happen during the underlying `tf.Session.run()` calls. It works if the `OpError` happens in the middle of a graph execution, which will generate the debug dump directory. But in this case, the `OpError` (due to invalid device name) happens before the graph execution even happens, so there is no debug dump directory and `LocalCLIDebugWrapper` generates an error message that masks the original exception. I will push a fix to this issue soon. "]}, {"number": 8506, "title": "A Strange Behavior in TensorFlow about Conv2d Creation and Weight Optimization", "body": "To simplify the tensorflow syntax, I created utility functions that would generate a conv2d layer more easily for me. However, when I use it in an architecture, the results are bizzare and different from the vanilla coding version. Even though both methods are returning the same tensor shape, tensorflow fails to optimize the weights in the second approach and I always get extremely low accuracies, vs the first version returns a perfect accuracy.\r\n\r\nIt seems like every time Tensorflow initializes the weights randomly instead of optimizing it. Is this a known bug?\r\n\r\n\r\nVersion 1\r\n---\r\n`sigma = 0.1`\r\n`x = tf.placeholder(tf.float32, (None, 32, 32, 3))`\r\n\r\n`conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))`\r\n`conv1_b = tf.Variable(tf.zeros(6))`\r\n`conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b`\r\n\r\n\r\nVersion 2\r\n---\r\n\r\n`def weights(dims, mu = 0, sigma = 0.1):`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`w = tf.Variable(tf.truncated_normal(shape = dims, mean = mu, stddev = sigma))`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`return w`\r\n\r\n`def bias(dims):`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`b = tf.Variable(tf.zeros(dims))`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`return b`\r\n\r\n`def conv(layer, dims, stride = [1, 1, 1, 1], pad = 'VALID'):`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`w = weights(dims)`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`b = bias(dims[-1])`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`conv2 = tf.nn.conv2d(layer, w, strides = stride, padding = pad) + b`\r\n&nbsp;&nbsp;&nbsp;&nbsp;`return conv2`\r\nthis line is the only line that I put in my network architecture: so I created my conv2d layers like this:\r\n\r\n`conv2 = conv(x, [5, 5, 3, 6]) `", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "I don't see why it's not a bug! otherwise I would've posted in on stackoverflow. Anyways, thanks!"]}, {"number": 8505, "title": "tutorials/rnn/translate/translate.py Errors", "body": "Hello,\r\n\r\nI am trying to run the \"translate.py\" example from the sequence to sequence tutorial but it fails.\r\n\r\nThe tensorflow version is today nightly built (tensorflow-1.0.1-cp35-cp35m-linux_x86_64.whl). The model git containing the tutorial  is at the latest commit on the master branch.\r\n\r\nI am using a linux OS.\r\n\r\nThe error message is:\r\n\r\nValueError: Shape must be rank 2 but is rank 1 for 'model_with_buckets/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/MatMul_1' (op: 'MatMul') with input shapes: [?], [?,1024]\r\n\r\nMany thanks!\r\n\r\nAlexandre Nanchen\r\n", "comments": ["I can't reproduce on my end.  Can you double-check the commit hash of the `translate.py` you're using?", "The model git is at HEAD: e8464d33d363c6f8e90a29c08e5af11ef01be7a1 (Update the DRAGNN) of 17 of March 2017\r\n\r\nThe translate.py  is: c902a86785052291083682c4b833747e7ea52597 (Ability to train the translation model ...) of January 3rd 2017 from Viacheslav Kovalevskyi.\r\n\r\nI also tried to do again a \"git clone https://github.com/tensorflow/models.git\"  and then run \"python tutorials/rnn/translate/translate.py\".\r\n\r\nAm I using the correct python file?\r\n\r\nHere is the trace file: [debug.txt](https://github.com/tensorflow/tensorflow/files/852246/debug.txt)\r\n\r\nMany thanks for this quick help!!\r\n\r\nAlexandre Nanchen", "I see at least two potentially related fixes after Jan 3, 2017: https://github.com/tensorflow/models/commits/master/tutorials/rnn/translate/seq2seq_model.py.\r\n\r\nCould you retry after definitely pulling in those changes?  Feel free to reopen if this doesn't work for you.", "Not sure to understand.\r\n\r\ngit log -n 1 tutorials/rnn/translate/seq2seq_model.py yield commit\r\n\r\n0d8916f4b84e8687eb46696883b50b84ac6aa420\r\n\r\nIt always has been to this commit which is from March 2017 and I am on the master branch.\r\n\r\nCan you please tell me which commit of the \"model git\" you are working with?\r\n\r\nMany thanks.\r\n\r\nAlex.andre Nanchen\r\n\r\n", "Hmm, previously you stated\r\n\r\n`The translate.py is: c902a86785052291083682c4b833747e7ea52597 (Ability to train the translation model ...) of January 3rd 2017 from Viacheslav Kovalevskyi.`\r\n\r\nwhich is _before_ 0d8916, one of the fixes that I supposed would fix your problems.  Now that you've confirmed you're on 0d8916, could you retry the whole thing and see if the issue persisted?\r\n\r\nIf it did, let us know and feel free to reopen (/cc @nealwu).", "Ah, some confusion. It was the \"translate.py\" file that had this date not the \"seq2seq_model.py\".\r\n\r\nI did a git log -n 1 tutorials/rnn/translate/translate.py that indicated that the file \"translate.py\" file did not change from January 2017.\r\n\r\nSee https://github.com/tensorflow/models/commits/master/tutorials/rnn/translate/translate.py\r\n\r\nIn the debug.txt file attached in previous comment there is indeed the \"legacy_seq2seq\" directory that was I think, modified in commit #0d8916f4b84e8687eb46696883b50b84ac6aa420 from March 2017.\r\n\r\nDoes this make sense?\r\n\r\nSorry for the confusion.\r\n\r\nAlex.\r\n\r\n\r\n\r\n\r\n", "Hello,\r\n\r\nBased on github rules I cannot reopen an issue when closed by a collaborator.\r\n\r\nAlex.\r\n", "The issue still persist. \r\n\r\nThe only way I got it working is installing tensorflow 0.12 and doing a git checkout of the model directory at the last commit of **translate.py**.\r\n\r\nAlex.\r\n", "Re-opening this to see if @nealwu has some insights ", "I believe you have the same issue as this: https://github.com/tensorflow/models/issues/1221. It looks like the documentation on our site and the actual code are not in sync, which has caused some confusion [here](https://github.com/tensorflow/models/pull/982) and [here](https://github.com/tensorflow/models/issues/836). I'll check on what the correct argument order should be.", "I believe this should be fixed now via https://github.com/tensorflow/models/pull/1226. If you run into further issues let me know.", "There is another error related to post tensorflow/tensorflow#8191\r\n\r\nValueError: Attempt to reuse RNNCell <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.GRUCell object at 0x7f6f517906a0> with a different variable scope than its first use.  First use of cell was with scope\r\n\r\nAlready tried to fix it without success. Can you please help.\r\n\r\nThanks.\r\n\r\nAlex.", "I believe that is related to the discussion here: https://github.com/tensorflow/models/pull/1131. If you are running the master version of TensorFlow, you may want to reinstall with TensorFlow 1.0 instead.", "Yes, that solved it!\r\n\r\nAs a summary:\r\n- use \"model\" git commit of 20 March 2017 f6e23e5618ef18625966c6668f1f90dca25dbc56\r\n- use \"official\" release of tensorflow 1.0.1, i.e. pip install tensorflow (recently changed)\r\n\r\nMany thanks!\r\n\r\nAlex.\r\n"]}, {"number": 8504, "title": "Cannot reuse variables by tf.layers.conv2d", "body": "I am trying to make two conv layers share the same weights, however, it seems the API does not work. \r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.random_normal(shape=[10, 32, 32, 3])\r\n\r\nconv1 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=None, name='conv')\r\nprint(conv1.name)\r\nconv2 = tf.layers.conv2d(x, 3, [2, 2], padding='SAME', reuse=True, name='conv')\r\nprint(conv2.name)\r\n```\r\n\r\ngives\r\n\r\n```\r\nconv/BiasAdd:0\r\nconv_2/BiasAdd:0\r\n```\r\n\r\n--------------------------------Update-------------------------------\r\nAccording to a [post](https://stackoverflow.com/questions/42862300/tensorflow-reuse-variable-with-tf-layers-conv2d), the weights are already sharing, with different layer names, since the computation not sharing. However, is it feasible to consider a better naming strategy so that it is easier to see from names that different layers are sharing the same weights ?", "comments": ["@ebrevdo Thanks for your PR. I have a short question \r\n\r\n```\r\nwith tf.variable_scope('new_scope'):\r\n    ker = tf.get_variable('kernel', [5, 6])\r\n    fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=None)\r\n    fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=True)\r\n```\r\n\r\nWith the new behavior, will `fc2` reuse the weights from `fc1` or from `ker` ?\r\nBecause previously the weight sharing should be \r\n```\r\nwith tf.variable_scope('new_scope'):\r\n    fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=None)\r\n    fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, reuse=True)\r\n```\r\n", "I accidentally introduced a bug.  It *should* reuse the weights from fc1,\nbut i accidentally made it reuse the weights from ker.  i'm sending an\nupdate to reverse this behavior.\n\nOn Wed, Mar 22, 2017 at 12:52 AM, Xingdong Zuo <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> Thanks for your PR. I have a short\n> question\n>\n> with tf.variable_scope('new_scope'):\n>     ker = tf.get_variable('kernel', [5, 6])\n>     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)\n>     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)\n>\n> With the new behavior, will fc2 reuse the weights from fc1 or from ker ?\n> Because previously the weight sharing should be\n>\n> with tf.variable_scope('new_scope'):\n>     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)\n>     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8504#issuecomment-288323108>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9CC9jizTqiQkcxXS_47Cao8h9IIks5roNNGgaJpZM4MgzyA>\n> .\n>\n", "Fix should be in.\n\nOn Wed, Mar 22, 2017 at 9:44 AM, Eugene Brevdo <ebrevdo@gmail.com> wrote:\n\n> I accidentally introduced a bug.  It *should* reuse the weights from fc1,\n> but i accidentally made it reuse the weights from ker.  i'm sending an\n> update to reverse this behavior.\n>\n> On Wed, Mar 22, 2017 at 12:52 AM, Xingdong Zuo <notifications@github.com>\n> wrote:\n>\n>> @ebrevdo <https://github.com/ebrevdo> Thanks for your PR. I have a short\n>> question\n>>\n>> with tf.variable_scope('new_scope'):\n>>     ker = tf.get_variable('kernel', [5, 6])\n>>     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)\n>>     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)\n>>\n>> With the new behavior, will fc2 reuse the weights from fc1 or from ker ?\n>> Because previously the weight sharing should be\n>>\n>> with tf.variable_scope('new_scope'):\n>>     fc1 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=None**)\n>>     fc2 = tf.layers.dense(inputs=tf.constant(np.random.randn(3, 4, 5)), units=3, **reuse=True**)\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/8504#issuecomment-288323108>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim9CC9jizTqiQkcxXS_47Cao8h9IIks5roNNGgaJpZM4MgzyA>\n>> .\n>>\n>\n>\n"]}, {"number": 8503, "title": "Can aggregate all gradients from local wokers on local GPUs before push to parameter server \uff1f", "body": "Can we use Graph mechnism to build the situation that all workers will aggregate all gradients from their replicated sub-graph\uff0c before pushing gradients to all pservers\uff1f \r\n\r\nIf OK\uff0c can show us some examples\uff1f \r\n\r\nBTW\uff1a \r\n\r\n* with r1.0.1 version, inception training with distributed nodes will crash for several old APIs\uff0cand some more unpredictable reason. \r\n  > TypeError: __init__() got an unexpected keyword argument 'replica_id' . \r\n\r\n* some ps error comes if we use non-default DNS IP when multiple NICS exist.  \r\n\r\n", "comments": ["You can although it'll take some coding on the user end, someone on stackoverflow may be more likely to answer. The comment about crashing could be formed as another issue, if you could follow the template with reproducible example"]}, {"number": 8502, "title": "iOS - No OpKernel was registered to support Op 'ExtractImagePatches' with these attrs", "body": "Hi everyone,\r\n\r\nI am trying to load a graph inside iOS that was generated by another software into a .pb file.  It was tested on a Ubuntu machine, so I know the model works correctly.\r\nWhen trying to use it with iOS, the project builds correctly and then I have the following error:\r\n\r\n```\r\nCould not create TensorFlow Graph: Invalid argument: No OpKernel was registered to support Op 'ExtractImagePatches' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: ExtractImagePatches = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 2, 2, 1], padding=\"VALID\", rates=[1, 1, 1, 1], strides=[1, 2, 2, 1]](concat)]]\r\n```\r\n\r\nI know other issues have been posted related to this; however I was unable to locate a solution.\r\n\r\nI am using TensorFlow 1.0.1 and Python 3.6.0\r\nI can run the iOS examples with no problem.\r\n\r\niOS seems not to be able to find `ExtractImagePatches` Op.\r\n\r\nIf anyone could help with this issue, it would be greatly appreciated.\r\n\r\n\r\n", "comments": ["By default, the iOS build target doesn't link in `core/kernels:extract_image_patches_op`.  Can you try to modify your `BUILD` file and explicitly link this in?  ", "@concretevitamin Which `BUILD` file should I modify?  The `BUILD` file in `tensorflow/tensorflow` or `tensorflow/tensorflow/core`?\r\n\r\nAfter I modify the correct file, how shall I rebuild?  Will I need to run `build_all_ios.sh` script?\r\n\r\nThanks for your help.", "I'm adding one of our iOS experts @satok16 to further advise.  (Thanks Satoshi!)", "To build binaries for iOS, you need to use \"build_all_ios.sh\" which builds targets by make instead of bazel.  As this process doesn't use BUILD file, you should add kernel c++ files into \"[tf_op_files.txt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_op_files.txt)\".\r\n", "@satok16 \r\nThanks.  After adding the kernel for extract_image_patches to `tf_op_files.txt`, I was able to compile successfully; however I am still running into problems with the iOS demo.\r\n\r\nI am trying to use the camera demo provided and modify it for object detection instead of object classification.  Is there a tutorial for this anywhere online that I could follow or that you could point me to?\r\n\r\nThanks.", "I'm not pretty sure that there is any tutrial for them other than https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\r\n\r\nAndrew, do you have any idea about this?", "@petewarden has a nice iOS walkthrough here: https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/amp/", "As we have Pete's great post, let's close this issue for now.  Feel free to create a new ticket if you have any other issues.\r\n\r\nThanks!"]}, {"number": 8501, "title": "seq2seq.py: encoder_inputs from embedding_attention_seq2seq to static_rnn wrong?", "body": "The encoder_inputs to the embedding_attention_seq2seq function is stated as \"A list of 1D int32 Tensors of shape **[batch_size]**.\"\r\n\r\nWhich is a shape of **[input_size, batch_size]**\r\n\r\nThis same input is then passed to **core_rnn.static_rnn** directly where **static_rnn** expects an input shape of **[batch_size, input_size]** instead.\r\n\r\n```\r\nwith variable_scope.variable_scope(\r\n      scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\r\n    dtype = scope.dtype\r\n    # Encoder.\r\n    encoder_cell = core_rnn_cell.EmbeddingWrapper(\r\n        cell,\r\n        embedding_classes=num_encoder_symbols,\r\n        embedding_size=embedding_size)\r\n    encoder_outputs, encoder_state = core_rnn.static_rnn(\r\n        encoder_cell, encoder_inputs, dtype=dtype)\r\n```\r\n\r\nThis seems like an error or am I unclear?\r\n\r\nThanks", "comments": ["@xiejw would you mind taking a look at this?", "@ebrevdo could help here. ", "A list of 1D int32 Tensors of shape [batch_size] isn't of shape [input_size, batch_size] until you concat every element in the list. It's a **list** of element of shape [batch_size]\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Looks like @louishenrifranc answered this!  Thanks!"]}, {"number": 8500, "title": "TensorFlow 1.0 on Windows: OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits", "body": "Just installed TensorFlow 1.0 on Windows 10 (x64) on a Surface Pro 4 following the instructions from here: https://www.tensorflow.org/install/install_windows (e.g. pip3 install --upgrade tensorflow).\r\n\r\nI'm unable to validate my installation, though - here's what I'm running into:\r\n\r\n```~\\Projects> python\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf;\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()\r\n>>> print(sess.run(hello))\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'Hello, TensorFlow!'\r\n>>>\r\n```\r\n\r\nThere seem to be several similar reports that indicate this is fixed, but as far as I can tell, this is still effectively broken when you install a clean version of TensorFlow on Windows. Is installing a nightly build the only way to get past this?", "comments": ["Duplicate of #7859.", "Thanks @Carmezim.  Closing this for now, please re-open if using the nightly doesn't work for you.", "Are there instructions on how to install a nightly build? It's not clear how to get that set up.", "@rringham You download the wheel and install it with `pip` like the build on the website, but you will be doing that locally. \r\n\r\nFor instance, [select the most recent working build](http://ci.tensorflow.org/view/Nightly/job/nightly-win/) (blue) from the list on the left, choose the version you want, CPU or GPU, and download the wheel. \r\nOn your terminal `cd` to the directory you downloaded the build on and install it with `pip install <name-of-the-wheel>`\r\n", "Thank you @Carmezim!\r\n\r\nUpdate - nightly build works great, thanks again!", "Installing the latest nightly \"tensorflow_gpu-1.1.0rc2-cp35-cp35m-win_amd64.whl\" seems to fixe those errors but running the same code as @rringham I get:\r\n\r\n\r\n```\r\n2017-04-20 23:40:48.766978: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-20 23:40:48.766978: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-20 23:40:48.766978: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-20 23:40:48.766978: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-20 23:40:48.767978: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-20 23:40:48.767978: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n```", "@anselal please see those\r\nhttp://stackoverflow.com/questions/43134753/tensorflow-wasnt-compiled-to-use-sse-etc-instructions-but-these-are-availab\r\n\r\nhttp://stackoverflow.com/questions/43335531/how-to-use-sse4-1-instructions-without-install-tensorflow-from-source", "@Carmezim thank you for the hint !! :+1: "]}, {"number": 8499, "title": "tensorboard README: fix grammar mistake", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 8498, "title": "Upgrade Protobuf", "body": "We are building TensorFlow master using bazel on 64 bit platform.\r\n\r\nWe need a fix for 64 bit platform as mentioned in #1044 in `tensorflow/third_party/protobuf/3.0.0/src/google/protobuf/stubs/atomicops_internals_generic_gcc.h`. \r\n\r\nThis fix is available in protobuf with commit id [c59473d ](https://github.com/google/protobuf/commit/c59473d53eafadd126502657e5c5c33e952b67ed)or higher.\r\nWill it be possible to pick this or higher commit of protobuf in TensorFlow?\r\n\r\n", "comments": ["Reference: [#4380](https://github.com/tensorflow/tensorflow/issues/4380)", "Your original reference \"#1044\" should point to the one in [protobuf](https://github.com/google/protobuf/pull/1044). And I am confused of the \"64 bit platform\" you were referring to, until I notice it is probably `mips64`. It would be better if you could make that clear, as current master has no problem building on `x86-64`.", "We were getting this issue for s390x platform.\r\nHowever, I have cloned the code again and now, I could build TensorFlow without any error.\r\nClosing this issue.\r\n\r\n\r\n"]}, {"number": 8497, "title": "Tensorflow Retrain Model performance", "body": "I am using [Tensorflow for poet](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0) guide for train own model. I have create retrained_graph.pb and retrained_labels.txt. While I use it in application then I get error that \r\n\r\n\r\n`Caused by: java.lang.UnsupportedOperationException: Op BatchNormWithGlobalNormalization is not available in GraphDef version 21. It has been removed in version 9. Use tf.nn.batch_normalization(). at org.tensorflow.Graph.importGraphDef(Native Method) at org.tensorflow.Graph.importGraphDef(Graph.java:118) `\r\n\r\nAfter That  further train model for application use [Tensorflow for mobile ](https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/)  blog and create **optimized_graph.pb, rounded_graph.pb, mmapped_graph.pb** files.\r\n\r\noptimized_graph.pb and rounded_graph.pb file work in android application without any error.\r\nWhile use mmapped_graph.pb I get error that **Failed to initialize: java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef**\r\n\r\nPerformance of application is not good while use optimized_graph.pb and rounded_graph.pb file.While application camera screen not contain any flower photos otherwise random flower name show with high confidence rate. Any way to detect only flower and remain blank when not flowers.\r\n\r\n![screenshot](https://cloud.githubusercontent.com/assets/25680329/24044730/8edab30c-0b42-11e7-8209-a5fab382b81c.png)\r\n", "comments": ["Closing as this seems to be a duplicate of #8396.\r\n\r\nRegarding the model performance: this is expected because the SoftMax layer at the top of Inception will always output values summing to 1.0. To work around this you would need to use a different type of layer for classification or add a default class to your training data (training with a variety of random images may work, but that's a discussion for elsewhere).", "Hi @kinDSa! I'm doing the same tutorial on Tensorflow for poets, but getting some errors on retraining model. May be it's because of tensorflow version. Tutorial was written on 0.9.0 version. And I've tried get.io/tensorflow/tensorflow:latest-devel. May I ask which version did you use? :)", "@ainurb, I am using 1.0.1 version. Still I getting this error but after follow tensorflow for mobile blog then I successfully use model."]}, {"number": 8496, "title": "[Feature request] np.random.choice analogue", "body": "TF has no analogue to `np.random.choice` function that chooses random element from tensor (optionally according to provided probabilities). It's especially useful in RL problems when you use epsilon-greedy exploration strategy.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n[These](http://stackoverflow.com/questions/41123879/numpy-random-choice-in-tensorflow) [two](http://stackoverflow.com/questions/37757986/weighted-random-tensor-select-in-tensorflow) solve the problem using `tf.multinomial`, but maybe it will be convenient to include `tf.random_choice` in TF API?\r\n\r\nIt could be implemented through `tf.multinomial` , for example.", "comments": ["I'm not aware of anyone working on this.  We'd happily take a contribution!  (It likely should go into `contrib` as the first cut.)", "@concretevitamin I will prepare pull request. Is it okay to implement this using `tf.multinomial` or should I implement new op for this?\r\n\r\nWhere should I place the implementation? `tf.contrib.distribution` has a little different semantic (contains `Distribution` subclasses).", "I think implementing in terms of existing ops is fine.\r\n\r\n@aselle - do you have a suggestion on where to put such numpy counterparts in contrib?  If not, I'm tempted to say it's okay to go into `random_ops.py`. ", "(Also /cc @ebrevdo to see if `tf.contrib.distribution` has any files that could host this.)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I want to know if the tf.random_choice was done? But I can't find the python files in contrib/distributions/python/ops/.  And I really want it....", "Here's an implementation for uniform sampling with replacement along a specific axis (untested).\r\n\r\n```\r\ndef random_choice(a, axis, samples_shape=None):\r\n    \"\"\"\r\n\r\n    :param a: tf.Tensor\r\n    :param axis: int axis to sample along\r\n    :param samples_shape: (optional) shape of samples to produce. if not provided, will sample once.\r\n    :returns: tf.Tensor of shape a.shape[:axis] + samples_shape + a.shape[axis + 1:]\r\n    :rtype: \r\n\r\n    Examples:\r\n    >>> a = tf.placeholder(shape=(10, 20, 30), dtype=tf.float32)\r\n    >>> random_choice(a, axis=0)\r\n    <tf.Tensor 'GatherV2:0' shape=(1, 20, 30) dtype=float32>\r\n    >>> random_choice(a, axis=1)\r\n    <tf.Tensor 'GatherV2_1:0' shape=(10, 1, 30) dtype=float32>\r\n    >>> random_choice(a, axis=1, samples_shape=(2, 3))\r\n    <tf.Tensor 'GatherV2_2:0' shape=(10, 2, 3, 30) dtype=float32\r\n    >>> random_choice(a, axis=0, samples_shape=(100,))\r\n    <tf.Tensor 'GatherV2_3:0' shape=(100, 20, 30) dtype=float32>\r\n    \"\"\"\r\n    \r\n    if samples_shape is None:\r\n        samples_shape = (1,)\r\n    shape = tuple(a.get_shape().as_list())\r\n    dim = shape[axis]\r\n    choice_indices = tf.random_uniform(samples_shape, minval=0, maxval=dim, dtype=tf.int32)\r\n    samples = tf.gather(a, choice_indices, axis=axis)\r\n    return samples\r\n```", "This worked for me. It returns both the sampled values and their indices in the original tensor:\r\n\r\n```\r\ndef random_choice(x, size, axis=0, unique=True):\r\n    dim_x = tf.cast(tf.shape(x)[axis], tf.int64)\r\n    indices = tf.range(0, dim_x, dtype=tf.int64)\r\n    sample_index = tf.random.shuffle(indices)[:size]\r\n    sample = tf.gather(x, sample_index, axis=axis)\r\n\r\n    return sample, sample_index\r\n\r\n```", "```\r\n    def random_choice(a, size):\r\n        \"\"\"Random choice from 'a' based on size without duplicates\r\n        Args:\r\n            a: Tensor\r\n            size: int or shape as tuple of ints e.g., (m, n, k).\r\n        Returns: Tensor of the shape specified with 'size' arg.\r\n\r\n        Examples:\r\n            X = tf.constant([[1,2,3],[4,5,6]])\r\n            random_choice(X, (2,1,2)).numpy()\r\n            -----\r\n            [\r\n              [\r\n                [5 4]\r\n              ],\r\n              [\r\n                [1 2]\r\n              ]\r\n            ]\r\n        \"\"\"\r\n        if isinstance(size, int) or np.issubdtype(type(a), np.integer) or (tf.is_tensor(a) and a.shape == () and a.dtype.is_integer):\r\n            shape = (size,)\r\n        elif isinstance(size, tuple) and len(size) > 0:\r\n            shape = size\r\n        else:\r\n            raise AssertionError(f\"Unexpected size arg {size}\")\r\n\r\n        sample_size = tf.math.reduce_prod(size, axis=None)\r\n        assert sample_size > 0\r\n\r\n        # --------------------------------------------------------------------------------\r\n        # Select elements from a flat array\r\n        # --------------------------------------------------------------------------------\r\n        a = tf.reshape(a, (-1))\r\n        length = tf.size(a)\r\n        assert sample_size <= length\r\n\r\n        # --------------------------------------------------------------------------------\r\n        # Shuffle a sequential numbers (0, ..., length-1) and take size.\r\n        # To select 'sample_size' elements from a 1D array of shape (length,),\r\n        # TF Indices needs to have the shape (sample_size,1) where each index\r\n        # has shape (1,),\r\n        # --------------------------------------------------------------------------------\r\n        indices = tf.reshape(\r\n            tensor=tf.random.shuffle(tf.range(0, length, dtype=tf.int32))[:sample_size],\r\n            shape=(-1, 1)   # Convert to the shape:(sample_size,1)\r\n        )\r\n        return tf.reshape(tensor=tf.gather_nd(a, indices), shape=shape)\r\n```\r\n```\r\nX = tf.constant([[1,2,3,4,5,6]])\r\nprint(random_choice(X, 2).numpy())\r\n-----\r\n[2 5]\r\n\r\nX = tf.constant([[1,2,3],[4,5,6]])\r\nprint(random_choice(X, (2,2,1)).numpy())\r\n-----\r\n[[[5]\r\n  [4]]\r\n\r\n [[1]\r\n  [2]]]\r\n```"]}, {"number": 8495, "title": "\"grep: AVX: No such file or directory\" -> importing tensorflow libraries in python, on mac ", "body": "When I try to import tensorflow in python, under python2.7 or 3 I get the following error output, it loads fine, but the errors are annoying.. (Also running in bash terminal if that matters...)\r\n\r\ngrep: AVX: No such file or directory\r\ngrep: 10.9.: No such file or directory\r\n\r\nI thought it might be an issue with the specific version of grep on the mac os so I updated to the latest grep using brew, and now I get this error...\r\n\r\ngrep: warning: GREP_OPTIONS is deprecated; please use an alias or script\r\ngrep: AVX: No such file or directory\r\ngrep: warning: GREP_OPTIONS is deprecated; please use an alias or script\r\ngrep: 10.9.: No such file or directory\r\n\r\nwhich is going in the opposite direction I was hoping to. Is anyone else getting this? Is there a fix? Much appreciated any help on this, have no clue if this is just something I need to live with on a mac, or if I can find a way to fix it.\r\n\r\nShould also mention i am using version 1.0.1 of tensorflow which I got running this command from above... python -c \"import tensorflow; print(tensorflow.version)\"\r\n\r\nInstalled tensorflow using\r\npip3 install --upgrade tensorflow\r\nfor python 3 for instance.", "comments": ["\r\nFigured it out, something I installed had added this to my .bash_profile\r\nexport GREP_OPTIONS=\u2019\u2013color=auto\u2019 export CLICOLOR=1;\r\n\r\nin hindsight kinda obvious...   "]}, {"number": 8494, "title": "Segfault: \"Check failed: num_elements_ <= kMaxElements\"", "body": "I'm running the current nightly build as of today (2017-03-17). I'm working with very large, sparse matrices, and my program seems to have hit a variable overflow. It crashed with the following log entry:\r\n\r\n    2017-03-17 10:48:34.554715: F tensorflow/core/framework/tensor_shape.cc:212] Check failed: num_elements_ <= kMaxElements (1141286132736 vs. 1099511627776)\r\n    Aborted (core dumped)\r\n\r\nI'm guessing this is because my sparse matrix is too large to handle for tensorflow. It would be great if this could somehow be fixed. Please refer to #7783 for more details about my code (that was another overflow issue I encountered).\r\n\r\n\r\n\r\n\r\n", "comments": ["Looks like we can just bump that limit up, I'll work on getting that checked in.", "@girving beat me to it. There's a patch in review, one of us will update this issue once it's checked in."]}, {"number": 8493, "title": "Implementation of LSTMCell inconsistent with the refered paper.", "body": "This is an extended issue of the existing issue #8469 .\r\n\r\nIn the comments, it said that the LSTMCell is implemented based on this [paper](https://research.google.com/pubs/archive/43905.pdf). \r\nIn this paper, the several gates are computed like `sum_i(args[i] * W[i])`. Here, the args are list of input and m_state. More details can be found in the mentioned paper.\r\nHowever, in the code the gates are computed as:\r\n```\r\n      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n      lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True)\r\n      i, j, f, o = array_ops.split(\r\n          value=lstm_matrix, num_or_size_splits=4, axis=1)\r\n```\r\nDiving into the `_linear` function, showing that:\r\n```\r\n      res = math_ops.matmul(array_ops.concat(args, 1), weights)\r\n```\r\nThe `args `are firstly concatenated, and then `matmul `with the weights. In this implementation, the parameter number of the weights is much larger than what said in the paper. This is totally different from what said in the paper.\r\n\r\n\r\n", "comments": ["Sorry, I just found that they are in fact the same.\r\nSorry for proposing a wrong issue."]}, {"number": 8492, "title": "Gradient by a function issue", "body": "I find out an inconsistency in tf.gradients function.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.get_variable(\"x\", shape=(1,))\r\nx2 = x * 2\r\nx4_issue = (x * 2) * 4\r\nx4 = x2 * 4\r\n\r\nissue_grads = tf.gradients(x4_issue, x2)[0]\r\ngrads = tf.gradients(x4, x2)[0]\r\n\r\nprint issue_grads\r\nprint grads\r\n```\r\n\r\n\r\n I would expect the two prints have the same output because the expressions are equivalent.\r\nHowever, output that I'm getting is following:\r\n\r\n```\r\nNone\r\nTensor(\"gradients_1/mul_3_grad/Reshape:0\", shape=(1,), dtype=float32)\r\n```\r\n\r\n", "comments": ["This is expected behavior.  On `x2 = x * 2`, TensorFlow inserted a `Mul` node into the current graph.  On the subsequent `... = (x * 2) ...`, TF inserted yet another new `Mul` node.  In other words, tensor `x4_issue` does not depend on `x2` in anyway. "]}, {"number": 8491, "title": "Convolution and CuDNN version TF v1.0", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nDid not find much.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nUbuntu 16.04.1 LTS (GNU/Linux 4.4.0-64-generic x86_64)\r\n\r\nInstalled version of CUDA and CuDNN: \r\n\r\n```\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/stubs/libcuda.so\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n/usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/lib64/libcudart.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-8.0/extras/Debugger/include/libcudacore.h\r\n/usr/local/cuda-8.0/extras/Debugger/lib64/libcudacore.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcuda.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcuda.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cudnn-5.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cudnn-5.0/lib64/libcudnn.so\r\n/usr/local/cudnn-5.0/lib64/libcudnn.so.5\r\n/usr/local/cudnn-5.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/lib64/stubs/libcuda.so\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5\r\n/usr/local/cuda-7.5/lib64/libcudadevrt.a\r\n/usr/local/cuda-7.5/lib64/libcudart.so\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/extras/Debugger/include/libcudacore.h\r\n/usr/local/cuda-7.5/extras/Debugger/lib64/libcudacore.a\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcuda.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcuda.so.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5\r\n/usr/local/cuda-7.5/lib/libcudadevrt.a\r\n/usr/local/cuda-7.5/lib/libcudart.so\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/lib/i386-linux-gnu/libcuda.so.370.28\r\n/usr/lib/i386-linux-gnu/libcuda.so\r\n/usr/lib/i386-linux-gnu/libcuda.so.1\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.370.28\r\n/usr/lib/x86_64-linux-gnu/libcudart_static.a\r\n/usr/lib/x86_64-linux-gnu/stubs/libcuda.so\r\n/usr/lib/x86_64-linux-gnu/libcuda.so\r\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5\r\n/usr/lib/x86_64-linux-gnu/libcudadevrt.a\r\n/usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n/usr/lib/x86_64-linux-gnu/libcudart.so\r\n/usr/lib/x86_64-linux-gnu/libcudart.so.7.5.18\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n```\r\npip install tensor-gpu --upgrade\r\n```\r\nUsing TF v1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nModel compiles and runs when avoiding function call:\r\n\r\n```\r\ntf.nn.conv2d()\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nUsing tensorflow 0.12rc.However, this changes a bunch of our implementations due to API changes. Have not tried downgrading to CuDNN v5.0.\r\n\r\n### Error Log:\r\n\r\n```\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:390] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5110 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAborted (core dumped)\r\n```\r\n\r\n\r\nThanks! Really enjoying TF v1.0.1\r\n\r\n", "comments": ["You should upgrade your cudnn (from 5005 to 5110)"]}, {"number": 8490, "title": "Tools for generating the website is not available in this public github repo ", "body": "Could the website generator tools for tensorflow.org be changed to  open source tools here. It will be good and appreciated!\r\nMany people want to get the offline version of tensorflow documentation, not only the API documentation, I think they can use the  tools to generate the website from markdown files, and  to offline html files. It will be helpful.\r\nAnd I think if the tool become a open source tool. It will be more useful and powerful.  thanks:)", "comments": ["@martinwicke ", "Everything up to conversion to markdown is already open-source, you can easily generate html from that using hoedown or other md interpreters (tools/docs).\r\n\r\nFor the \"last mile\" we use an internal system, and we will not try to replace it with an open-source version."]}, {"number": 8489, "title": "I cant download it !", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": []}, {"number": 8488, "title": "Checkpoints default values for RunConfig", "body": "With save_checkpoints_secs having a default value (of 600), the call to RunConfig fails if only save_checkpoints_steps is passed. To address this, the value is defaulted only if both the parameters are None. In this new way, the call to RunConfig will not fail if only save_checkpoints_steps is passed ", "comments": ["Can one of the admins verify this patch?", "@ispirmustafa  - Thanks for the review", "Jenkins, test this please.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Please fix the test failures. `_USE_DEFAULT` must be qualified (as `RunConfig._USE_DEFAULT`).", "@martinwicke Thanks. Will do ", "Jenkins, test this please."]}, {"number": 8487, "title": "init_fn Error of tf.train.Scaffold", "body": "In my experiment the **init_fn** of the **tf.train.Scaffold** can not  work correctly. It seems that this is caused by the **lambda** usage in the **__init__** of **tf.train.Scaffold**\r\n\r\n\r\n\r\n### Environment info\r\nOperating System: ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n-rw-r--r-- 1 root root    556000 Jan 26 23:48 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root        16 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root        19 Jan 26 23:51 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root    415432 Jan 26 23:48 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root    775162 Jan 26 23:48 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 1000 users       13 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 1000 users       18 Nov  7 07:00 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 1000 users 84163560 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 1000 users 70364814 Nov  7 06:47 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n\r\nAll the experiments run in a **nvidia-docker container** run with **tensorflow/tensorflow:devel-latest-gpu** image\r\n\r\n1. A link to the pip package you installed: tensorflow/tensorflow:devel-latest-gpu\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```python\r\nwith tf.Graph().as_default():\r\n    images = tf.placeholder(shape=[32, 224, 224, 3], dtype=tf.float32)`\r\n    logits = tf.layers.conv2d(images, 3, 1)\r\n    all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\r\n    my_init_fn = slim.assign_from_checkpoint_fn('', all_vars)\r\n    scaffold = tf.train.Scaffold(init_fn=my_init_fn)\r\n    with tf.train.MonitoredTrainingSession(\r\n        checkpoint_dir='',\r\n        scaffold=scaffold,\r\n        ) as session:\r\n        pass\r\n```\r\n### What other attempted solutions have you tried?\r\nIt seems that the problem may be solved by replacing the \r\n```python\r\n scaffold = tf.train.Scaffold(init_fn=my_init_fn)\r\n```\r\nwith \r\n```python\r\nscaffold = tf.train.Scaffold()\r\nscaffold._init_fn = my_init_fn\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n```\r\nTraceback (most recent call last):\r\n  File \"start_job.py\", line 219, in <module>\r\n    train_param=get_train_param(train_args)\r\n  File \"/root/py_libs/slim_toolbox/training/train_core.py\", line 150, in train_network\r\n    config=get_default_sess_config()) as session:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 315, in MonitoredTrainingSession\r\n    return MonitoredSession(session_creator=session_creator, hooks=all_hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 601, in __init__\r\n    session_creator, hooks, should_recover=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 434, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 767, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 772, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 494, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 375, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 264, in prepare_session\r\n    init_fn(sess)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 128, in <lambda>\r\n    self._init_fn = lambda sess: init_fn(self, sess)\r\nTypeError: callback() takes exactly 1 argument (2 given)\r\n```\r\n", "comments": ["Hmm, the docstring of `assign_from_checkpoint_fn()` states\r\n```\r\n  Returns:\r\n    A function that takes a single argument, a `tf.Session`, that applies the\r\n    assignment operation. If no matching variables were found in the checkpoint\r\n    then `None` is returned.\r\n```\r\nwhereas the `Scaffold()` expects `init_fn` to be\r\n```\r\nOptional function to use to initialize the model after running\r\n    the init_op.  Will be called as `init_fn(scaffold, session)`.\r\n```\r\n\r\nGiven this, both usages look invalid to me, and I'm not sure why the second one succeeded.  ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I had the same issue. Do you plan to fix it?", "I ended up using an intermediate function as a workaround:\r\n```\r\ndef init_fn_for_scaffold(scaffold_self, session):\r\n    return init_fn(session)\r\n```\r\n\r\nI just want to know where the bug is, in `monitored_session.py` file or in `tf.contrib.framework.assign_from_checkpoint_fn` function ? Can `self` parameter be used inside init_fn for something ?", "I delete the `self` parameter. It works."]}, {"number": 8486, "title": "Invalid argument: No OpKernel was registered to support Op 'Add' with these attrs.", "body": "### Environment info\r\nOperating System:\r\n\r\nMac \r\n\r\n I hava look Mobile and Embedded TensorFlow (TensorFlow Dev Summit 2017) video on yotube.\r\n\r\nIn the video , I hava learn some function to reduce tensorflow so file size on Android.\r\n\r\nI do here\r\n```\r\nPrints a header file to be used with SELECTIVE_REGISTRATION.\r\n\r\nExample usage:\r\nprint_selective_registration_header \\\r\n--graphs=path/to/graph.pb > ops_to_register.h\r\n\r\nThen when compiling tensorflow, include ops_to_register.h in the     include\r\n search path and pass -DSELECTIVE_REGISTRATION  - see\r\n core/framework/selective_registration.h for more details.\r\n```\r\n\r\nthe *.pb file is myself , then I get the ops_to_register.h file here\r\n\r\n```c++\r\n  #ifndef OPS_TO_REGISTER\r\n  #define OPS_TO_REGISTER\r\n  constexpr inline bool ShouldRegisterOp(const char op[]) {\r\n      return false\r\n     || (strcmp(op, \"Add\") == 0)\r\n     || (strcmp(op, \"Const\") == 0)\r\n     || (strcmp(op, \"Conv2D\") == 0)\r\n     || (strcmp(op, \"Exp\") == 0)\r\n     || (strcmp(op, \"Identity\") == 0)\r\n     || (strcmp(op, \"Max\") == 0)\r\n     || (strcmp(op, \"MaxPool\") == 0)\r\n     || (strcmp(op, \"NoOp\") == 0)\r\n     || (strcmp(op, \"Placeholder\") == 0)\r\n     || (strcmp(op, \"RealDiv\") == 0)\r\n     || (strcmp(op, \"Relu\") == 0)\r\n     || (strcmp(op, \"Reshape\") == 0)\r\n     || (strcmp(op, \"Sub\") == 0)\r\n     || (strcmp(op, \"Sum\") == 0)\r\n     || (strcmp(op, \"_Recv\") == 0)\r\n     || (strcmp(op, \"_Send\") == 0)\r\n     ;\r\n    }\r\n  #define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n  const char kNecessaryOpKernelClasses[] = \",\"\r\n\"BinaryOp< CPUDevice, functor::add<float>>,\"\r\n\"ConstantOp,\"\r\n\"Conv2DOp<CPUDevice, float>,\"\r\n \"UnaryOp< CPUDevice, functor::exp<float>>,\"\r\n  \"IdentityOp,\"\r\n \"ReductionOp<CPUDevice, float, Eigen::internal::MaxReducer<float>>,\"\r\n\"MaxPoolingOp<CPUDevice, float>,\"\r\n \"NoOp,\"\r\n  \"PlaceholderOp,\"\r\n \"BinaryOp< CPUDevice, functor::div<float>>,\"\r\n  \"ReluOp<CPUDevice, float>,\"\r\n\"ReshapeOp,\"\r\n\"BinaryOp< CPUDevice, functor::sub<float>>,\"\r\n \"ReductionOp<CPUDevice, float, Eigen::internal::SumReducer<float>>,\"\r\n \"RecvOp,\"\r\n\"SendOp,\"\r\n ;\r\n#define SHOULD_REGISTER_OP_KERNEL(clz)            \r\n   (strstr(kNecessaryOpKernelClasses, \",\" clz \",\") != nullptr)\r\n\r\n #define SHOULD_REGISTER_OP_GRADIENT false\r\n #endif\r\n```\r\n\r\nI put ops_to_register.h in tensorflow/tensorflow/core/framework dir.\r\n\r\nthen I do:   \r\n\r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a    --copt=\"-DSELECTIVE_REGISTRATION\"`\r\n\r\n\r\nIn my android studio project , initializeTensorFlow() My slef .pb file ,but I got the error:\r\n\r\n\r\n```\r\ntensorflow_inference_jni.cc:145 Could not create TensorFlow graph: Invalid argument: No OpKernel was registered to support Op 'Add' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n            <no registered kernels>\r\n\r\n          \t [[Node: add_1 = Add[T=DT_FLOAT](Conv2D, Reshape)]]\r\n```\r\n\r\n\r\n\r\n\r\n", "comments": ["can someone help me solve the problem ?\r\n\r\nMy android phone is meizu MX6.", "@cwhipkey can you take a look at this?", "One cause of this we've seen before is when the host compiler, that made ops_to_register.h, differs from the compiler used for the device.  The difference we saw was in handling of whitespace in kernel names.\r\n\r\nIn your case, you could try changing the ops_to_register.h to remove the space before CPUDevice where it occurs, e.g.:\r\n`  BinaryOp< CPUDevice, functor::add<float>>`\r\nbecomes\r\n`  BinaryOp<CPUDevice, functor::add<float>>`\r\n\r\nand see if that helps.\r\n\r\nThis problem is fixed in the master branch - the generated ops_to_register.h uses functions that compare the kernel names ignoring whitespace differences.", "thanks your suggetion , I got the  right reesult. @cwhipkey "]}, {"number": 8485, "title": "AOT compilation doesn't work on MacOS", "body": "I was trying to reproduce this [example](https://www.tensorflow.org/versions/master/experimental/xla/tfcompile) on how to use AOT compilation. My graph is made by calling [make_test_graphs.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tests/make_test_graphs.py) and I used [test_graph_tfmatmul.config.pbtxt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tests/test_graph_tfmatmul.config.pbtxt) as my config file. I failed at '**Step 2: Use tf_library build macro to compile the subgraph**'. \r\nHere is the error message:\r\nerror:/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool: file: bazel-out/local-opt/genfiles/test_aot/test_graph_tfmatmul.o is not an object file (not allowed in a library)\r\n\r\nHowever, if I `bazel build` the same `tf_library` on a GCE instance, it compiles successfully. The only difference is that the Tensorflow on my GCE instance is cuda enabled. I think there may be a bug in AOT compilation in CPU-only mode.\r\n\r\n### Environment info\r\nOperating System: macOS 10.12\r\nProcessor: intel 'haswell'.\r\nTensorflow version: 1.0.1 stable release\r\nbazel version: 0.4.4\r\n", "comments": ["@xuehanxiongsc Instead of writing your own bazel file, perhaps you can try using one of the existing tests.  E.g.:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tests/BUILD#L118\r\n\r\nSo try something like this:\r\n\r\n$ bazel test tensorflow/compiler/aot/tests:tfcompile_test\r\n\r\nCan you report back what results you get from this?  If there is a problem, I'm suspecting it's more likely to be a Mac vs. linux issue, rather than a CPU vs. GPU issue.", "@tatatodd Here is the log:\r\n\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\n\\-----------------------------------------------------------------------------\r\nRunning main() from test_main.cc\r\n[==========] Running 8 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 8 tests from TFCompileTest\r\n[ RUN      ] TFCompileTest.Add\r\ndyld: lazy symbol binding failed: Symbol not found: ___tensorflow_compiler_aot_tests__test_graph_tfadd\r\n  Referenced from: /private/var/tmp/_bazel_xuehan.xiong/4f4d45e6f5aaa2c03c85228dc2ff01af/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/tfcompile_test.runfiles/org_tensorflow/tensorflow/compiler/aot/tests/tfcompile_test\r\n  Expected in: flat namespace\r\n\r\ndyld: Symbol not found: ___tensorflow_compiler_aot_tests__test_graph_tfadd\r\n  Referenced from: /private/var/tmp/_bazel_xuehan.xiong/4f4d45e6f5aaa2c03c85228dc2ff01af/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/tfcompile_test.runfiles/org_tensorflow/tensorflow/compiler/aot/tests/tfcompile_test\r\n  Expected in: flat namespace\r\n\r\nexternal/bazel_tools/tools/test/test-setup.sh: line 123: 58107 Abort trap: 6           \"${TEST_PATH}\" \"$@\"\r\n", "@xuehanxiongsc I'm assuming that's when run on your Mac?\r\n\r\nNow that I think about it, AOT won't work at all.  At a minimum we'll need to add the appropriate LLVM target triple for mac; the default is `x86_64-pc-linux` which won't work:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/882e75db49f09afac6aee5cd38798a82f80f5419/tensorflow/compiler/aot/tfcompile.bzl#L277\r\n\r\nI seem to recall a few other problems wrt Mac builds too.  I don't actually have a Mac, but contributions to get this working are definitely welcome!", "@tatatodd You are right. It is related to macOS. I just compiled successfully on Linux without cuda support. ", "XLA isn't yet supported beyond Linux.  Hopefully this will work eventually, but closing to reduce bug tracker clutter."]}, {"number": 8484, "title": "sparse_softmax_cross_entropy_with_logits returns NaN instead of raising an error when the class int is not in the range [0, logit_size) when run on GPU", "body": "If you provide to tf.nn.tf.nn.sparse_softmax_cross_entropy_with_logits() labels that are not inside the number of classes  (that is the length of the logits, or the length of the second dimension of the logits if using the first dimension for batching), you get a loss of NaN and then everything in the model becomes NaN. I spent quite some time debugging this in my model and I believe that this should not be silent, there should be at least a warning at execution time.\r\n\r\nMore detail I just discovered: if you run it on CPU an InvalidAgrumentError is raised, if you run it on GPU, you get NaN, so you may want to fix only the GPU implementation to behave like the CPU one.\r\n\r\nEnvironment info\r\n\r\nOperating System: Ubuntu 16.04 64bit\r\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1\r\nTensorflow version: 1.0.0\r\n\r\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nRun it with CUDA_VISIBLE_DEVICE=\"\" to see the difference on CPU and GPU.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclasses = 5\r\nnum_datapoints = 100\r\n\r\nxs = np.random.rand(num_datapoints,4)\r\nys = np.random.randint(classes + 1, size=num_datapoints)\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x = tf.placeholder(tf.float32, shape=[None, 4])\r\n    y = tf.placeholder(tf.int32, shape=[None, ])\r\n\r\n    w = tf.Variable(tf.random_normal([4, classes]))\r\n    b = tf.Variable(tf.random_normal([classes]))\r\n\r\n    logits = tf.matmul(x, w) + b\r\n\r\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\r\n    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\n\r\nwith tf.Session(graph=graph) as session:\r\n    tf.global_variables_initializer().run()\r\n    for step in range(201):\r\n        _, loss_val = session.run(\r\n            [optimizer, loss],\r\n            feed_dict={x: xs, y: ys})\r\n        print(\"step {step} - loss: {loss_val:.6f}\".format(step=step, loss_val=loss_val))\r\n```\r\n", "comments": ["@zheng-xq, is this intended behavior? Is there any way to make this more friendly?", "@ebrevdo, who adds the fused implementation of that kernel", "I'm afraid it's not possible to efficiently send back error messages on\ninvalid inputs with GPU.  Other ops behave similarly, returning Nan when an\nerror condition occurs on GPU.\n\nOn Mar 24, 2017 10:42 AM, \"zheng-xq\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo>, who adds the fused implementation\n> of that kernel\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8484#issuecomment-289093189>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9OlP2f4EBMvVRvWgpGPD06nMesgks5rpAB2gaJpZM4MgDu4>\n> .\n>\n", "embedding_lookup has a similar behavior: returns 0 for indices outside the range on GPU while it complains about those indices on CPU. What I personally would like to see as a user is an homogeneous behavior. Something like returning 0 or null for all problems for which the CPU implementation returns an error or a warning and the GPU implementation can't do checks.\r\nAn additional idea / suggestion: maybe a \"debug mode\" flag could help, of by default, that you can turn on while developing and gives you more warning. Or, in alternative, write somewhere \"we suggest to run everything on CPU while in development in order to get significant errors and warnings and to run models on GPU only after having debugged them on the CPU\". I guess transparency and explanations about why something like this happens is the best approach.", "Closing for now since this needs the same unimplemented mechanism for GPU error reporting that https://github.com/tensorflow/tensorflow/issues/2594 and https://github.com/tensorflow/tensorflow/issues/3638 require.", "I spent quite some time debugging the `NaN` issue in my system and finally realized this is the issue. For ex: If you have 101 classes and you encode labels from 1-101, your model will always have a `NaN`! Because 101 is greater than the size of the logits!\r\n\r\nCan we have at least a Warning popping up, instead of simply showing that the loss diverged with a NaN? That can bring down the debugging time to a great extent for many users.", "> I spent quite some time debugging the `NaN` issue in my system and finally realized this is the issue. For ex: If you have 101 classes and you encode labels from 1-101, your model will always have a `NaN`! Because 101 is greater than the size of the logits!\r\n> \r\n> Can we have at least a Warning popping up, instead of simply showing that the loss diverged with a NaN? That can bring down the debugging time to a great extent for many users.\r\n\r\nFinally, the error makes sense. Spent a day printing tensors and trying to debug.", "This is reasonable to add but nontrivial.  Rasmus what's a good base\nexample for returning errors from the GPU?\n\nOn Fri, Oct 19, 2018, 6:29 PM Sayan Paul <notifications@github.com wrote:\n\n> I spent quite some time debugging the NaN issue in my system and finally\n> realized this is the issue. For ex: If you have 101 classes and you encode\n> labels from 1-101, your model will always have a NaN! Because 101 is\n> greater than the size of the logits!\n>\n> Can we have at least a Warning popping up, instead of simply showing that\n> the loss diverged with a NaN? That can bring down the debugging time to a\n> great extent for many users.\n>\n> Finally, the error makes sense. Spent a day printing tensors and trying to\n> debug.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8484#issuecomment-431537740>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0C4V0BCH2SYpjlQpmhbekDtVElDks5umnyDgaJpZM4MgDu4>\n> .\n>\n", "@karthik-hegde you saved me. The problem I had is related to what you mentioned. I have a few differences between the number of categories I assigned and the actual number.", "> I spent quite some time debugging the `NaN` issue in my system and finally realized this is the issue. For ex: If you have 101 classes and you encode labels from 1-101, your model will always have a `NaN`! Because 101 is greater than the size of the logits!\r\n> \r\n> Can we have at least a Warning popping up, instead of simply showing that the loss diverged with a NaN? That can bring down the debugging time to a great extent for many users.\r\n\r\nYou just save my day. Thanks a lot. My target vector is a pad with zeros and I forget to consider the 0 class in the final layer", "I am really disappointed with TF, I lost a lot of time just debugging the nan in my loss, thinking of all the possibilities, and then it turns out it is just a wrong error message! Thanks, everyone for opening this issue, you saved me!"]}, {"number": 8483, "title": "Errors polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED", "body": "I am running Tensor-flow on Google compute engine instances and I met with this problem when the training period is finished (after step 9990).\r\n```\r\n2017-03-17 00:24:23.021298: step 9980, loss = 0.72 (242.4 examples/sec; 0.264 sec/batch)\r\n2017-03-17 00:24:34.693612: step 9990, loss = 0.79 (208.4 examples/sec; 0.307 sec/batch)\r\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED\r\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\r\nAborted (core dumped)\r\n```\r\nCan anyone identify this problem and let me know how to fix it?", "comments": ["Marking as duplicate of #8517 "]}]