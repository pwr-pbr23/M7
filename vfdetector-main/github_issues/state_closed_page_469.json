[{"number": 39739, "title": "tf.keras.layers.Embedding convert error", "body": "**System information**\r\n- Linux Ubuntu 16.04):\r\n- tf-nightly\r\n\r\nHello, \r\nI have problem when converting \"tf.keras.layers.Embedding\".\r\n\"from_keras_model\" works well but   \"from_saved_model\" doesn't. \r\n\r\n**Code**\r\n```\r\nembed_model = tf.keras.models.Sequential([ \r\n    tf.keras.layers.Embedding(10, 10)\r\n])\r\ndinput1 = np.random.randint([1],10)\r\nout=embed_model(dinput1)\r\n\r\nMODEL_DIR = \"embed_model\"\r\nembed_model.save(MODEL_DIR, save_format=\"tf\" )\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR) \r\n#converter = tf.lite.TFLiteConverter.from_keras_model(embed_model)\r\n\r\nconverter.experimental_new_converter = True\r\ntflite_embed_model = converter.convert()\r\n\r\nopen(\"dec_model.tflite\", \"wb\").write(tflite_embed_model)\r\n```\r\n\r\n**Error Log**\r\nTraceback (most recent call last):\r\n  File \"/local1/sshwang/virtual_envs/pytorch_1/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 180, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"/local1/sshwang/virtual_envs/pytorch_1/lib/python3.5/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: Failed to find function '__inference__wrapped_model_251'. The imported TensorFlow GraphDef is ill-formed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"embedding_model.py\", line 21, in <module>\r\n    tflite_embed_model = converter.convert()\r\n  File \"/local1/sshwang/virtual_envs/pytorch_1/lib/python3.5/site-packages/tensorflow/lite/python/lite.py\", line 920, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/local1/sshwang/virtual_envs/pytorch_1/lib/python3.5/site-packages/tensorflow/lite/python/lite.py\", line 752, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/local1/sshwang/virtual_envs/pytorch_1/lib/python3.5/site-packages/tensorflow/lite/python/lite.py\", line 498, in convert\r\n    **converter_kwargs)\r\n  File \"/local1/sshwang/virtual_envs/pytorch_1/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 555, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/local1/sshwang/virtual_envs/pytorch_1/lib/python3.5/site-packages/tensorflow/lite/python/convert.py\", line 183, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: Failed to find function '__inference__wrapped_model_251'. The imported TensorFlow GraphDef is ill-formed.\r\n\r\n ", "comments": ["@adonisues,\r\nI am facing a different error stating `ValueError: None is only supported in the 1st dimension. Tensor 'embedding_1_input' has invalid shape '[None, None]'.` Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/68f00eb3ba79b4a412d66794777ca9b0/39739.ipynb#scrollTo=43DsjLFZ5y2Y).\r\n\r\nCould you please provide the complete code to reproduce the issue reported here. Thanks!", "@adonisues As you are using `model.save`, you need to use `from_keras_model`. Once you update that part of code, then everything works as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/958ea1a4e069c7221b1c4e6da1a33ffe/39739.ipynb). \r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39738, "title": "Subclassed layers not automatically saved to checkpoint in TF2.2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 18.04`\r\n- TensorFlow installed from (source or binary): `binary`\r\n- TensorFlow version (use command below): `2.1 and 2.2`\r\n- Python version: `3.6 and 3.8`\r\n- CUDA/cuDNN version: `10.2 / 7.6.5`\r\n- GPU model and memory: `TITAN X (Pascal) 12 GB`\r\n\r\n\r\n**Describe the current behavior**\r\nWith TF 2.1, I was able to subclass the `tf.keras.Sequential` class as part of a model and checkpoints would save fine as far as I knew. \r\nWith TF 2.2, I saw that `tf.keras.Sequential` layers are not tracked in the checkpoint unless the layers are also set as attribute of the subclass.\r\n\r\n**Describe the expected behavior**\r\nI'm curious to know if the behavior in TF 2.2 is the intended behavior. \r\nThere are workarounds like manually setting the attributes or instantiating a `tf.keras.Sequential`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom pprint import pprint\r\n\r\nimport tensorflow as tf\r\n\r\nclass CustomSequential(tf.keras.Sequential):\r\n    def __init__(self, set_attributes=False, name='CustomSequential', **kwargs):\r\n        super().__init__([\r\n            tf.keras.layers.Conv2D(2, 3, 2, name='conv2'),\r\n            tf.keras.layers.Conv2D(4, 3, 2, name='conv3'),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(10)\r\n        ], name=name, **kwargs)\r\n\r\n        if set_attributes:\r\n            self.conv1 = self.layers[0]\r\n            self.conv2 = self.layers[1]\r\n            self.dense = self.layers[3]\r\n\r\n\r\nclass CustomModel(tf.keras.Model):\r\n    def __init__(self, set_attributes, name='CustomModel', **kwargs):\r\n        super().__init__(name=name, **kwargs)\r\n        self.convolution = tf.keras.layers.Conv2D(8, 3, name='conv1')\r\n        self.sequential = CustomSequential(set_attributes, name='MySequential')\r\n\r\n    def call(self, inputs):\r\n        net = self.convolution(inputs)\r\n\r\n        return self.sequential(net)\r\n\r\n\r\nprint('TensorFlow version')\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\nfor set_attributes in [True, False]:\r\n    print(f'Set sequential layer attributes: {set_attributes}')\r\n    inputs = tf.random.uniform([5, 16, 16, 3])\r\n    model = CustomModel(set_attributes)\r\n    outputs = model(inputs)\r\n\r\n    checkpoint = tf.train.Checkpoint(step=tf.Variable(1), model=model)\r\n    manager = tf.train.CheckpointManager(checkpoint, f'./custom_model_{set_attributes}', max_to_keep=3)\r\n    manager.save()\r\n\r\n    pprint(tf.train.list_variables(manager.latest_checkpoint))\r\n    print('------------------')\r\n```\r\n\r\n***TF 2.1 Result***\r\n```\r\nTensorFlow version\r\nv2.1.0-rc2-17-ge5bf8de 2.1.0\r\nSet sequential layer attributes: True\r\n[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\r\n ('model/convolution/bias/.ATTRIBUTES/VARIABLE_VALUE', [8]),\r\n ('model/convolution/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 3, 8]),\r\n ('model/sequential/layer-0/bias/.ATTRIBUTES/VARIABLE_VALUE', [2]),\r\n ('model/sequential/layer-0/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 8, 2]),\r\n ('model/sequential/layer-1/bias/.ATTRIBUTES/VARIABLE_VALUE', [4]),\r\n ('model/sequential/layer-1/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 2, 4]),\r\n ('model/sequential/layer-3/bias/.ATTRIBUTES/VARIABLE_VALUE', [10]),\r\n ('model/sequential/layer-3/kernel/.ATTRIBUTES/VARIABLE_VALUE', [16, 10]),\r\n ('save_counter/.ATTRIBUTES/VARIABLE_VALUE', []),\r\n ('step/.ATTRIBUTES/VARIABLE_VALUE', [])]\r\n------------------\r\nSet sequential layer attributes: False\r\n[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\r\n ('model/convolution/bias/.ATTRIBUTES/VARIABLE_VALUE', [8]),\r\n ('model/convolution/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 3, 8]),\r\n ('model/sequential/layer-0/bias/.ATTRIBUTES/VARIABLE_VALUE', [2]),\r\n ('model/sequential/layer-0/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 8, 2]),\r\n ('model/sequential/layer-1/bias/.ATTRIBUTES/VARIABLE_VALUE', [4]),\r\n ('model/sequential/layer-1/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 2, 4]),\r\n ('model/sequential/layer-3/bias/.ATTRIBUTES/VARIABLE_VALUE', [10]),\r\n ('model/sequential/layer-3/kernel/.ATTRIBUTES/VARIABLE_VALUE', [16, 10]),\r\n ('save_counter/.ATTRIBUTES/VARIABLE_VALUE', []),\r\n ('step/.ATTRIBUTES/VARIABLE_VALUE', [])]\r\n------------------\r\n```\r\n\r\n***TF 2.2 Result***\r\n```\r\nTensorFlow version\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\nSet sequential layer attributes: True\r\n[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\r\n ('model/convolution/bias/.ATTRIBUTES/VARIABLE_VALUE', [8]),\r\n ('model/convolution/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 3, 8]),\r\n ('model/sequential/conv1/bias/.ATTRIBUTES/VARIABLE_VALUE', [2]),\r\n ('model/sequential/conv1/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 8, 2]),\r\n ('model/sequential/conv2/bias/.ATTRIBUTES/VARIABLE_VALUE', [4]),\r\n ('model/sequential/conv2/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 2, 4]),\r\n ('model/sequential/dense/bias/.ATTRIBUTES/VARIABLE_VALUE', [10]),\r\n ('model/sequential/dense/kernel/.ATTRIBUTES/VARIABLE_VALUE', [16, 10]),\r\n ('save_counter/.ATTRIBUTES/VARIABLE_VALUE', []),\r\n ('step/.ATTRIBUTES/VARIABLE_VALUE', [])]\r\n------------------\r\nSet sequential layer attributes: False\r\n[('_CHECKPOINTABLE_OBJECT_GRAPH', []),\r\n ('model/convolution/bias/.ATTRIBUTES/VARIABLE_VALUE', [8]),\r\n ('model/convolution/kernel/.ATTRIBUTES/VARIABLE_VALUE', [3, 3, 3, 8]),\r\n ('save_counter/.ATTRIBUTES/VARIABLE_VALUE', []),\r\n ('step/.ATTRIBUTES/VARIABLE_VALUE', [])]\r\n------------------\r\n```\r\n\r\nThis may be related to #37839.", "comments": ["I have tried in colab with TF-GPU version 2.1.0 , 2.2.0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/85788d8136638f667bccc372d5739162/untitled919.ipynb).Thanks!", "I am not seeing any issue with TF nightly version(`2.3.0-dev20200520`).Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/b164bd6f380c1111e6a240de97d5ab2b/untitled919.ipynb).You could use tf-nightly for now and in the next couple of months new stable version will be released .Please, close this thread if this solves your question. Thanks!", "Good to know it should no longer be an issue. Thanks for the quick response.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39738\">No</a>\n"]}, {"number": 39737, "title": "Provide way to exclude CoreML delegate from bazel build", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently when you build for iOS (specifically Swift in our case) you end up with the `coreml_delegate` target whether or not you consume it. As described in this comment, this may be something we want to support optionally excluding:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d53c999feb2751b6a3ec9bace5be6f0b4620e4b4/tensorflow/lite/experimental/swift/BUILD.apple#L13-L24\r\n\r\n**Will this change the current api? How?**\r\n\r\nI think we could maintain the current behavior, and have a [`config_setting`](https://docs.bazel.build/versions/master/be/general.html#config_setting) that could be used with a `--define` such as `--define=EXCLUDE_COREML=1` that we could [`select`](https://docs.bazel.build/versions/master/be/functions.html#select) on to opt these out. This is done extensively in the [envoy build system](https://github.com/envoyproxy/envoy/blob/2b0633a09ce2ebf7ff4ba19ac470c013b1f2d35b/bazel/BUILD#L114) for similar reasons.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who doesn't need the CoreML specific logic and wants to potentially reduce their app size (in some cases this may have very little app size effect because of dead stripping)\r\n\r\n**Any Other info.**\r\n\r\nMore discussion on https://github.com/tensorflow/tensorflow/pull/38956", "comments": ["Thank you for opening the issue! I think using `config_setting` would be a reasonable approach to take. A few things to note that could make things a bit more complex:\r\n\r\n1. CoreML and Metal are the two delegates that are currently offered, but there may be more in the future.\r\n2. Some developers may want no delegation deps, others may want 1 or more.\r\n3. If adding configs to exclude deps, then would need to consider that developers would need to update the exclusions each time a new delegate is added. Perhaps \"opting-in\" might be a simpler approach, i.e. devs explicitly list the delegates they want to include?\r\n\r\n@teijeong what are your thoughts?", "Thanks for tracking this! As delegates will add up significant sizes, yes that's what we really want. I really need inputs from someone who has more experience on these kind of issues, just like you two :) \r\n\r\nBesides modifying the BUILD rule, I don't know what's the good approach on this. Config looks straightforward, but I don't know how flags should be configured in XCode, when using bazel to build the app. \r\n\r\nWhen I tested, including CoreMLDelegate.swift file (whether or not the app is actually using it) increased the app's size, so this is definitely needed.", "I've submitted a PoC / RFC of opting in to these https://github.com/tensorflow/tensorflow/pull/39765\r\n\r\nI've tested it in all configurations and it seems to work right now. We probably want to update CI to exercise them all"]}, {"number": 39736, "title": "libhexagon_interface.so for non Android - Openwrt platform", "body": "@tensorflow/tflite\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Openwrt 15.05\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: ipq6018\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r2.2\r\n- GCC/Compiler version (if compiling from source): musl arch64 gcc 5.3+ (https://musl.libc.org/)\r\n\r\n**Describe the problem**\r\nI'm trying to get hexagon delegate to work with QC's IPQ6018 SoC which contains a hexagon DSP. The currently available libhexagon_interface.so doesn't work as it has Android Dependencies. Is it possible to get a libhexagon_interface.so cross-compiled for openwrt without Android dependencies? \r\n\r\nArch: armv8-64\r\nOne can build musl arch64 compiler using this project https://github.com/richfelker/musl-cross-make\r\nAfter this step, using the binary generated you can cross-compile libhexagon_interface.so\r\n\r\nI can help with testing if there is no access to Hardware. Thanks!\r\n", "comments": ["Hi,\r\nSorry for the delay.\r\nWe currently have also arm linux version which we can share.\r\nIs it hard requirement to use musl ?", "I can give the arm linux version a try first, can you send it to me? Also, was it compiled for Armv8 instruction set?", "Can you reach out directly, so i can share it with you to try.\r\n\r\nThanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Shared with Hardik. Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39736\">No</a>\n"]}, {"number": 39735, "title": "ValueError when attempting to use adapt method from TextVectorization layer ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- TensorFlow version (use command below): v1.12.1-32169-gf7d038cc3b 2.3.0-dev20200519\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nI've tried running the following two text classification guides:\r\n- https://keras.io/examples/nlp/text_classification_from_scratch/\r\n- https://keras.io/examples/nlp/pretrained_word_embeddings/\r\n\r\nIn each case, when I reach the call to the `adapt`  method, e.g. `vectorizer.adapt(text_ds)`, the following error is raised:\r\n\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py in set_vocabulary(self, vocab)\r\n    282           \"Attempted to set a vocabulary larger than the maximum vocab size. \"\r\n    283           \"Passed vocab size is %s, max vocab size is %s.\" %\r\n--> 284           (total_vocab_size, self.max_tokens))\r\n    285 \r\n    286     start_index = num_special_tokens\r\n>\r\n> ValueError: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 20001, max vocab size is 20000\r\n\r\n**Describe the expected behavior**\r\nThe error should not be raised.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\nimport numpy as np\r\n\r\n#define a set of docs as per https://machinelearningmastery.com/ example\r\ndocs = np.array(['Well done!',\r\n\t\t'Good work',\r\n\t\t'Great effort',\r\n\t\t'nice work',\r\n\t\t'Excellent!',\r\n\t\t'Weak',\r\n\t\t'Poor effort!',\r\n\t\t'not good',\r\n\t\t'poor work',\r\n\t\t'Could have done better.'])\r\n\r\nvectorizer = TextVectorization(max_tokens=5, output_sequence_length=4)\r\ntext_ds = tf.data.Dataset.from_tensor_slices(docs)\r\nvectorizer.adapt(text_ds)\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-71-6ce2d6cd5e1a> in <module>()\r\n      3 vectorizer = TextVectorization(max_tokens=5, output_sequence_length=4)\r\n      4 text_ds = tf.data.Dataset.from_tensor_slices(docs)\r\n----> 5 vectorizer.adapt(text_ds)\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in adapt(self, data, reset_state)\r\n    408           \"adapt() requires a Dataset or an array as input, got {}\".format(\r\n    409               type(data)))\r\n--> 410     super(TextVectorization, self).adapt(preprocessed_inputs, reset_state)\r\n    411 \r\n    412   def get_vocabulary(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py in adapt(self, data, reset_state)\r\n    203 \r\n    204     updates = self._combiner.extract(accumulator)\r\n--> 205     self._set_state_variables(updates)\r\n    206 \r\n    207   def _set_state_variables(self, updates):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in _set_state_variables(self, updates)\r\n    521           updates[_OOV_IDF_NAME])\r\n    522     else:\r\n--> 523       self.set_vocabulary(updates[_VOCAB_NAME])\r\n    524 \r\n    525   def _preprocess(self, inputs):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py in set_vocabulary(self, vocab, df_data, oov_df_value)\r\n    471                           \"called.\").format(mode=self._output_mode))\r\n    472 \r\n--> 473     self._index_lookup_layer.set_vocabulary(vocab)\r\n    474 \r\n    475     # When doing raw or integer output, we don't have a Vectorize layer to\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/index_lookup.py in set_vocabulary(self, vocab)\r\n    282           \"Attempted to set a vocabulary larger than the maximum vocab size. \"\r\n    283           \"Passed vocab size is %s, max vocab size is %s.\" %\r\n--> 284           (total_vocab_size, self.max_tokens))\r\n    285 \r\n    286     start_index = num_special_tokens\r\n\r\nValueError: Attempted to set a vocabulary larger than the maximum vocab size. Passed vocab size is 6, max vocab size is 5.\r\n", "comments": ["@gtjemwa \r\nI think  tokens in docs is 17 in your example.Hence max tokens should be considered greater than 17 then you will not have this issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/0b698009a433c05664652475c7da156e/untitled922.ipynb).Thanks!", "@ravikyram \r\nThanks for the response. Correct, setting max tokens to None or greater than number of unique words in the docs, everything does work. However, my understanding is that is I set a value for max_tokens, and the vocabulary is then determined by word frequency, with less frequently occurring tokens considered OOV.  This is the same approach in the demo example here https://keras.io/examples/nlp/text_classification_from_scratch, where the max_tokens is set to 20000, but we still get the same ValueError.", "@gtjemwa I cannot reproduce the error. Am i missing something? Can you please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/3cc2e418a749f163298e95d13777ee7f/untitled922.ipynb). Thanks!", "@jvishnuvardhan \r\nLooks like the error doesn't occur in 2.3.0-dev20200522 (it was on 2.3.0-dev20200519 that I was having the issue.\r\nThanks. I'll close this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39735\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39735\">No</a>\n"]}, {"number": 39734, "title": "[XLA] Extra VLOG for PTXAS calls ", "body": "This PR does a few changes related to PTXAS:\r\n- print the PTXAS command line used via VLOG\r\n- It will also output the verbose output of PTXAS as already asked on line 210, but not displayed.\r\n- Add the TF_EXTRA_PTXAS_OPTIONS environment variable that allows to pass extra parameter to PTXAS.\r\n\r\nI tag this XLA as to my knowledge this code is only used by XLA.", "comments": ["@nouiz  Can you please check @timshen91's comments and keep us posted. Thanks!", "Will do. But it could take a few days.", "Updated per instruction. I tested manually with run_hlo_modules and it behave as expected.\r\n\r\nNote, I tested with replay_computation and now the XLA_FLAGS aren't passed correctly for it. So it doesn't behave correctly. But this isn't related to this PR.", "run_hlo_module seems to replace replay_computation. As replay_computation is partially broken, why not just remove it and point to the new tools?", "Thanks for the work!"]}, {"number": 39733, "title": "Documentation of accepted datatypes for `validation_data` in keras.Model.fit is incorrect. ", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe description of the keras `Model.fit` function is either ambiguous, or incorrect, regarding the accepted datatypes for the parameter `validation_data`. Specifically, some data types *are* accepted, even when the documentation states that they are not. For example, the datatype `keras.utils.Sequence` *is* accepted as a possible datatype, and (as far as I can tell) behaves as one would expect. \r\n\r\nAs far as I can tell, this is primarily true when the user passes a generator/Sequence as `x`. In this case, the function `Model.fit` dispatches to the (deprecated) function `Model.fit_generator`, which *does* accept a generator or Sequence for the `validation_data` parameter.\r\n\r\nThis documentation should be corrected to unambiguously state one of the following: \r\n\r\n- *exactly* the list of datatypes that are accepted (e.g. numpy arrays, lists, pandas dataframes, etc). Note, this may require more work in order to fully test this set of datatypes. \r\n- *an approximation* of the list of datatypes that are accepted, with a cavaeat that some may be untested/only sometimes valid\r\n\r\nIf the types accepted are dependent on the type of `x`, then this should also be documented. \r\n\r\n### Submit a pull request?\r\n\r\nIf necessary, I am happy to open a PR, however I think that given this is clear user-facing code, and the primary interface for most tensorflow users it woudl be best to have this fix spearheaded by an internal developer.", "comments": ["The documentation for `model.fit` arguments  are now updated with latest [tf-nightly api docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#fit). Thanks!"]}, {"number": 39732, "title": "ImportError: cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model' (F:\\New folder\\New folder\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\__init__.py)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@shreedevibhat \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "ImportError: cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model' (C:\\Users\\Microsoft\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\__init__.py)\r\n\r\ngetting same error while \r\nimport keras.layers \r\n\r\n`---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-9329815dec1e> in <module>\r\n----> 1 import keras.layers\r\n      2 from keras.models import Model\r\n      3 from keras.applications.vgg16 import VGG16\r\n      4 from keras.applications.vgg16 import preprocess_input\r\n      5 from keras.preprocessing import image\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 from . import losses_utils\r\n      8 from . import metrics_utils\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py in <module>\r\n----> 1 from .load_backend import epsilon\r\n      2 from .load_backend import set_epsilon\r\n      3 from .load_backend import floatx\r\n      4 from .load_backend import set_floatx\r\n      5 from .load_backend import cast_to_floatx\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py in <module>\r\n     88 elif _BACKEND == 'tensorflow':\r\n     89     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 90     from .tensorflow_backend import *\r\n     91 else:\r\n     92     # Try and load external backend.\r\n\r\n~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.eager import context\r\n      7 from tensorflow.python.framework import device as tfdev\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     26 \r\n     27 # pylint: disable=g-bad-import-order\r\n---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     29 from tensorflow.python.tools import module_util as _module_util\r\n     30 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     81 from tensorflow.python import data\r\n     82 from tensorflow.python import distribute\r\n---> 83 from tensorflow.python import keras\r\n     84 from tensorflow.python.feature_column import feature_column_lib as feature_column\r\n     85 from tensorflow.python.layers import layers\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py in <module>\r\n     25 \r\n     26 from tensorflow.python.keras import activations\r\n---> 27 from tensorflow.python.keras import applications\r\n     28 from tensorflow.python.keras import backend\r\n     29 from tensorflow.python.keras import callbacks\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\__init__.py in <module>\r\n     24 from tensorflow.python.keras import backend\r\n     25 from tensorflow.python.keras import engine\r\n---> 26 from tensorflow.python.keras import layers\r\n     27 from tensorflow.python.keras import models\r\n     28 from tensorflow.python.keras import utils\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py in <module>\r\n     27 \r\n     28 # Advanced activations.\r\n---> 29 from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\r\n     30 from tensorflow.python.keras.layers.advanced_activations import PReLU\r\n     31 from tensorflow.python.keras.layers.advanced_activations import ELU\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\advanced_activations.py in <module>\r\n     25 from tensorflow.python.keras.engine.base_layer import Layer\r\n     26 from tensorflow.python.keras.engine.input_spec import InputSpec\r\n---> 27 from tensorflow.python.keras.utils import tf_utils\r\n     28 from tensorflow.python.ops import math_ops\r\n     29 from tensorflow.python.util.tf_export import keras_export\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\__init__.py in <module>\r\n     37 from tensorflow.python.keras.utils.layer_utils import print_summary\r\n     38 from tensorflow.python.keras.utils.losses_utils import squeeze_or_expand_dimensions\r\n---> 39 from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\n     40 from tensorflow.python.keras.utils.np_utils import normalize\r\n     41 from tensorflow.python.keras.utils.np_utils import to_categorical\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\multi_gpu_utils.py in <module>\r\n     20 from tensorflow.python.framework import ops\r\n     21 from tensorflow.python.keras import backend as K\r\n---> 22 from tensorflow.python.keras.engine.training import Model\r\n     23 from tensorflow.python.ops import array_ops\r\n     24 from tensorflow.python.util.tf_export import keras_export\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in <module>\r\n     38 from tensorflow.python.keras import optimizers\r\n     39 from tensorflow.python.keras.distribute import distributed_training_utils\r\n---> 40 from tensorflow.python.keras.engine import network\r\n     41 from tensorflow.python.keras.engine import training_arrays\r\n     42 from tensorflow.python.keras.engine import training_distributed\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py in <module>\r\n     37 from tensorflow.python.keras import backend\r\n     38 from tensorflow.python.keras import callbacks\r\n---> 39 from tensorflow.python.keras import saving\r\n     40 from tensorflow.python.keras.engine import base_layer\r\n     41 from tensorflow.python.keras.engine import base_layer_utils\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\__init__.py in <module>\r\n     31 from tensorflow.python.keras.saving.save import load_model\r\n     32 from tensorflow.python.keras.saving.save import save_model\r\n---> 33 from tensorflow.python.keras.saving.saved_model import export_saved_model\r\n     34 from tensorflow.python.keras.saving.saved_model import load_from_saved_model\r\n     35 from tensorflow.python.keras.saving.saving_utils import trace_model_call\r\n\r\nImportError: cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model' (C:\\Users\\Microsoft\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\__init__.py)`\r\n\r\ntensorflow-gpu                     2.2.0\r\ntensorflow                         1.14.0\r\nKeras                              2.3.1\r\nKeras-Applications                 1.0.8\r\nKeras-Preprocessing                1.1.2\r\n\r\nI installed keras gpu version using  conda install -c anaconda keras-gpu\r\n\r\n\r\n", "@shreedevibhat\r\nplease update as per above comment", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39732\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39732\">No</a>\n"]}, {"number": 39731, "title": "Fix deprecation message from GatherV2Grad", "body": "This PR tries to fix the issue in #39701 where there are deprecation message from GatherV2Grad:\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.identity instead.\r\n```\r\n\r\nThis PR fixes the message and use array_ops.identity instead (as was suggested by message).\r\n\r\nThis PR fixes #39701.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @jaingaurav for the review. The PR has been updated with cpu constraint added. Please take a look.", "Because if you don't have the CPU placement there's a risk we'll bounce the\nint64 shape tensor to a GPU and back, which is pretty slow.\n\nOn Sun, May 24, 2020 at 11:39 PM Gaurav Jain <notifications@github.com>\nwrote:\n\n> *@jaingaurav* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/array_grad.py\n> <https://github.com/tensorflow/tensorflow/pull/39731#discussion_r429755494>\n> :\n>\n> > @@ -641,7 +641,7 @@ def _GatherV2Grad(op, grad):\n>    # For axis 0 gathers, build an appropriately shaped IndexedSlices.\n>    if axis_static == 0:\n>      if context.executing_eagerly():\n> -      params_tail_shape = params_shape.cpu()[1:]\n> +      params_tail_shape = array_ops.identity(params_shape)[1:]\n>\n> @alextp <https://github.com/alextp>: Is this CPU placement necessary? I\n> looked at the history of this code and couldn't quite figure out why the\n> CPU placement is needed.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/39731#discussion_r429755494>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNVYZVQQ76KPIWFOYDRTIHCPANCNFSM4NGGYUUA>\n> .\n>\n\n\n-- \n - Alex\n", "@yongtang Can you please check @alextp's comments and keep us posted. Thanks!", "ping @alextp @jaingaurav to see if there are any thing else that needs to address?"]}, {"number": 39730, "title": "Keras Tensorboard Callback conflicts with Lambda Callback writing summaries", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but will test stock examples to verify issue\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Docker tensorflow/tensorflow:2.2.0-gpu\r\n- TensorFlow version (use command below):  Docker tensorflow/tensorflow:2.2.0-gpu\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.4 (what'ts in the docker container)\r\n- GPU model and memory: RTX 2080, 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\n\r\nSpecify callbacks for Keras:\r\n- TensorboardCallback, with a non-default batch interval\r\n- LambdaCallback, which uses a created summary writer to write images a >1 batch interval which differs from the above\r\n\r\nIn the below script, the following settings produce expected results only when the `update_freq` is set to 1. \r\n\r\n**Describe the expected behavior**\r\n\r\nImage writer should log and the status should be 1 (true)\r\n\r\nBoth summaries are written correctly at the correct batch intervals\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n''' \r\nModified from tensorboard tutorial\r\nhttps://www.tensorflow.org/tensorboard/image_summaries\r\n'''\r\nimport io\r\nimport itertools\r\nfrom datetime import datetime\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\n# Define callback batch intervals\r\ntb_callback_interval = 2\r\nlambda_callback_interval = 10\r\n\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = \\\r\n    fashion_mnist.load_data()\r\n\r\nlogdir = \"runs/bug_test/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = keras.callbacks.TensorBoard(\r\n    log_dir=logdir, update_freq=tb_callback_interval)\r\nfile_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\r\n\r\n\r\ndef log_image(batch, logs):\r\n    tf.print(\"Batch: \", batch)\r\n    if batch % lambda_callback_interval == 0:\r\n        with file_writer_cm.as_default():\r\n            status = tf.summary.image(\r\n                \"Confusion Matrix\", tf.expand_dims(test_images[0:5, :, :], -1), step=batch)\r\n            tf.print(\"File write status (should be 1): \", status)\r\n\r\n\r\ncm_callback = keras.callbacks.LambdaCallback(on_batch_end=log_image)\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Flatten(input_shape=(28, 28)),\r\n    keras.layers.Dense(32, activation='relu'),\r\n    keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss='sparse_categorical_crossentropy',\r\n    metrics=['accuracy']\r\n)\r\n\r\n# Train the classifier.\r\nmodel.fit(\r\n    train_images,\r\n    train_labels,\r\n    epochs=5,\r\n    verbose=1,  # Suppress chatty output\r\n    callbacks=[tensorboard_callback, cm_callback],\r\n    validation_data=(test_images, test_labels),\r\n)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.1, 2.2 and nightly version and was able to reproduce the issue.However I have noticed in expected results status shown as 1 for few results and as 0 for few results.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/789ff5fb7f91f709140582f7dbc89d98/untitled923.ipynb).Thanks!\r\n", "Note that ordering of the callbacks, i.e.:\r\n```\r\ncallbacks = [custom_callback, tensorboard_callback]\r\n```\r\nvs\r\n```\r\ncallbacks = [tensorboard_callback, custom_callback]\r\n```\r\nalso changes the behavior. \r\n\r\nCurrently, as a work around I have the \"update_freq\" for both my custom writer and the Keras writer set to the same number. If my custom callback with custom writer appears first in the list of callbacks, then it will work as expected. However changing the ordering or if the log frequencies are not the same, then the result is not as expected.", "Any progress for this bug? I face the same issue. The ordering of the callbacks affects the `tf.summary.*` functions' behavior. `tf.sumaary.image` returns `False` when the callback order is `[tensorboard_callback, custom_callback]`.", "Was able to replicate the issue in TF v2.5,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/7df41bf394227bfe712290dfd64b785f/untitled128.ipynb)..Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/7df41bf394227bfe712290dfd64b785f/untitled128.ipynb#scrollTo=0_JfNugFpPUp) ..Thanks!", "Inherit from tf.keras.callbacks.TensorBoard and rewrite ```on_train_batch_begin/end``` function, and use ```summary_op_v2``` directly(like that in TensorBoard source code),  code likes below:\r\n```python\r\nclass Custom(tf.keras.callbacks.TensorBoard)\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n    def on_train_batch_end(self, , batch, logs=None):\r\n        super().on_train_batch_end(batch, logs)\r\n        with self._train_writer.as_default():\r\n           summary_ops_v2.image(\"image\", image, step=batch)\r\n        self._train_writer.flush()\r\n\r\n```", "Hi @christopherbate! \r\nWe are checking to see whether you still need help in this issue .Have you checked above [suggestion](https://github.com/tensorflow/tensorflow/issues/39730#issuecomment-912710417) yet? Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39730\">No</a>\n"]}, {"number": 39728, "title": "Fix #39462", "body": "When using `get_value` to retrieve the value for LR, we receive a np.float32. However when we cast this to a python float, there are unwanted trailing significant digits added to our value that causes old_lr to never be exactly equal to our supplied min_lr, despite it being equal internally.\r\n\r\nBy leaving old_lr as it's original np.float32, and by casting our supplied min_lr to np.float32, we are now able to properly compare the values and the block of code should not be executed again after lr has been set to min_lr.", "comments": []}, {"number": 39727, "title": "Minor change to fix #39462", "body": "#39462\r\n\r\nCasting to float here adds significant digits to the end of the value so that when compared to our supplied min_lr, the two will never be equal. Internally the LR exists in it's proper form without the trailing sig digits, only after casting is the value altered. Removing this casting here ensures this does not happen and now we can properly see if min_lr has been reached.", "comments": []}, {"number": 39726, "title": "Fix building issues with CUDA 10.2", "body": "Hello there,\r\n\r\nFound SpMM APIs not defined for Win32 platforms in CUDA 10.2. Other than that, CUDA 10.2 comes with CUBLAS 10.2.2 which causes a version mismatch in find_cuda_config.py.\r\n\r\nAfter fixed these two issues, TF is ok to build on Windows with Visual Studio 2019 on my machine.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39726) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@alecyu2018 Thank you for your contribution. Can you please sign CLA? Thanks!", "The CLA page showed I signed it in the morning.\n\nOn Wed, May 20, 2020 at 20:32 gbaned <notifications@github.com> wrote:\n\n> @alecyu2018 <https://github.com/alecyu2018> Thank you for your\n> contribution. Can you please sign CLA? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/39726#issuecomment-631858266>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AK4VZ3XHQIFWFRZBPUE3763RSSOEPANCNFSM4NGFE7QQ>\n> .\n>\n", "@googlebot I signed it!\r\n\r\nIn case that my signed CLA is ignored for I left github username blank (since I thought I did not change it), I updated my github username in it, and so sending the \"I signed it!\" comment again.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39726) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.\r\n\r\nYeah, found the e-mail in my commit not the main one.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39726) for more info**.\n\n<!-- ok -->", "Hi @chsigg , any problem found?", "@chsigg Can you please review this PR ? Thanks!", "Another week passed. Everybody stay safe.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@alecyu2018 Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 39725, "title": "Fix sha256sum issue of mkl_dnn/oneDNN", "body": "This PR tries to address the issue raised in #39696 where\r\nthe sha256 of the exiting workspace.bzl archive for mkl_dnn\r\ndoes not match between mirrored one and the github one.\r\n\r\nThe issue comes from the fact that mkl_dnn has been renamed\r\nto oneDNN repo, and github repackaged the archive.\r\nThe original mkl_dnn now alias to oneDNN (with re-archive).\r\n\r\nThis PR adjust the download link to go directly to\r\noneDNN so that the sha256 matches.\r\n\r\nThis PR fixes #39696.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mihaimaruseac Tthere was a missing typo (`,`) in my PR. I had updated the PR, can you review again? Really sorry for the inconvenience.", "I should have noticed that.", "@mihaimaruseac Thanks a lot for the help!"]}, {"number": 39724, "title": "Optimize recall and accuracy for a sub-class in multi-class classification", "body": "Hi everyone,\r\n\r\nSorry about asking this question but I am a beginner with Tensorflow and Keras.\r\nHere is my question :\r\n- I have annotated 2.000 images of living cells, 8.000 images of dead cells and 8.000 images with nothing. They are labeleled respectively as \"Living\", \"Death\" and \"Empty\". (Images are 299x299x1)\r\n- I have run some models, which basically look like this:\r\n\r\n```\r\nmodel = Sequential([\r\n                    Conv2D(16, (5,5), activation='relu', input_shape=(299,299,1)),\r\n                    MaxPooling2D(pool_size=(2,2)),\r\n                    Conv2D(32, (5,5), activation='relu'),\r\n                    Flatten(),\r\n                    Dense(200),\r\n                    Activation('relu'),\r\n                    Dropout(0.3),\r\n                    Dense(3),\r\n                    Activation('softmax')\r\n])\r\n\r\nopt = SGD(lr=0.001)\r\nmodel.compile(loss = \"categorical_crossentropy\", \r\n              optimizer = opt,\r\n              metrics=['accuracy'])\r\n```\r\n- At the end of training, validation accuracy is around 90%. Nevertheless, when I check with confusion matrix, it appears that classification is very good to detect \"Empty\" images (accuracy almost 99%), but it is less good for separation of \"Living\" from \"Death\" images (circa 80%).\r\n- The main purpose for me is to optimize recall and accuracy for classification of \"Living\" cells. \r\n\r\n=> That's why I am wondering if there is a way to track recall and accuracy for a specific sub-class ('Living\" here) like in early-stop.\r\n\r\nThank you for your help !\r\n\r\nKindly !\r\n\r\n\r\n\r\n", "comments": ["You can always write your own metrics.\r\n\r\nFor your code:\r\n\r\n```\r\nmodel.compile(loss = \"categorical_crossentropy\", \r\n              optimizer = opt,\r\n              metrics=['accuracy'])\r\n```\r\n\r\nSimply define a metric function and calculate the accuracy yourself.\r\n\r\n```\r\ndef some_metric(y_true, y_pred):\r\n    return ...\r\n\r\nmodel.compile(loss = \"categorical_crossentropy\", \r\n              optimizer = opt,\r\n              metrics=[[some_metric]])\r\n```\r\n\r\nwhere y_true being the label, (BATCH, 3) as you have three classes, and y_pred being the predicted value (BATCH, 3)\r\n\r\nnote, however, that the metric function needs to return accuracy for each batch (BATCH,), not all accuracies averaged to one value.", "Thank you @wysohn !"]}, {"number": 39723, "title": "TFLu: Update CMSIS-NN glue interface in conv.cc", "body": "The CMSIS-NN glue for TFLu convolution now adopts a wrapper function\r\n(arm_convolve_wrapper_s8) to simplify the integration of future optimizations\r\navailable in CMSIS-NN. The wrapper function is responsible\r\nto dispatch the most optimized kernel accordingly with the parameters passed", "comments": []}, {"number": 39722, "title": "tf.experimental.CsvDataset breaking down if select_cols is provided", "body": "tensorflow 2.2.0\r\nubuntu 20.04\r\n```\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train_ = pd.DataFrame(x_train.reshape(60000,-1),columns = ['col_'+str(i) for i in range(28*28)])\r\nx_train_['col_cat1'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]\r\nx_train_['col_cat2'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]\r\npd.DataFrame(x_train_).to_csv('x_train_.csv',index=False)\r\n\r\ncdtypes = pd.read_csv('x_train_.csv',nrows=1).dtypes\r\n\r\ncdtypes = cdtypes.sort_index()\r\n\r\nx_train_ = tf.data.experimental.CsvDataset(\r\n    'data/x_train_.csv', [np.nan if i == (float or int) else '__missing__' for i in cdtypes],\r\n    select_cols=cdtypes.index, field_delim=',', use_quote_delim=True, na_value='',header=True,\r\n)\r\n```\r\noutput:\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-47-8d8f52fc868d> in <module>\r\n     12 x_train_ = tf.data.experimental.CsvDataset(\r\n     13     'data/x_train_.csv', [np.nan if i == (float or int) else '__missing__' for i in cdtypes],\r\n---> 14     select_cols=cdtypes.index, field_delim=',', use_quote_delim=True, na_value=''\r\n     15 )\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py in __init__(self, filenames, record_defaults, compression_type, buffer_size, header, field_delim, use_quote_delim, na_value, select_cols)\r\n    703         select_cols,\r\n    704         argument_default=[],\r\n--> 705         argument_dtype=dtypes.int64,\r\n    706     )\r\n    707     self._element_spec = tuple(\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/util/convert.py in optional_param_to_tensor(argument_name, argument_value, argument_default, argument_dtype)\r\n     30   if argument_value is not None:\r\n     31     return ops.convert_to_tensor(\r\n---> 32         argument_value, dtype=argument_dtype, name=argument_name)\r\n     33   else:\r\n     34     return constant_op.constant(\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1339 \r\n   1340     if ret is None:\r\n-> 1341       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1342 \r\n   1343     if ret is NotImplemented:\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    319                                          as_ref=False):\r\n    320   _ = as_ref\r\n--> 321   return constant(v, dtype=dtype, name=name)\r\n    322 \r\n    323 \r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    260   \"\"\"\r\n    261   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 262                         allow_broadcast=True)\r\n    263 \r\n    264 \r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    268   ctx = context.context()\r\n    269   if ctx.executing_eagerly():\r\n--> 270     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    271     if shape is None:\r\n    272       return t\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n     94       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n     95   ctx.ensure_initialized()\r\n---> 96   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n     97 \r\n     98 \r\n\r\nValueError: invalid literal for int() with base 10: 'col_0'\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39722\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39722\">No</a>\n"]}, {"number": 39721, "title": "tf.distribute.MirroredStrategy() ERROR:root:'NoneType' object has no attribute 'write'", "body": "**System information**\r\n- using Amazon Sagemaker\r\n- instance type = 'ml.p3.8xlarge',\r\n\r\n**Describe the current behavior**\r\nwith running model.fit(), prior to progress bar, following message is displayed.\r\n`ERROR:root:'NoneType' object has no attribute 'write'`\r\n\r\n**Describe the expected behavior**\r\nMessage to not be present given it's not present in documentation examples.\r\nhttps://www.tensorflow.org/tutorials/distribute/keras\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n# Resources:\r\n# https://huggingface.co/transformers/model_doc/roberta.html#tfrobertamodel\r\n# https://www.tensorflow.org/tutorials/distribute/keras\r\n# https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta\r\n\r\n###################################################################\r\n# IMPORTS Pt.1\r\n###################################################################\r\nprint(\"running imports, pt.1...\")\r\n\r\nimport os\r\nprint(os.getcwd())\r\nprint(os.listdir(os.getcwd()))\r\n\r\n###################################################################\r\n# INSTALLS\r\n###################################################################\r\nprint(\"running installs...\")\r\n\r\nos.system('pip install transformers')\r\n\r\n###################################################################\r\n# IMPORTS Pt.2\r\n###################################################################\r\nprint(\"running imports...\")\r\n\r\nimport time\r\n\r\nimport numpy as np # linear algebra\r\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n\r\n# TF Imports\r\nimport tensorflow as tf\r\nprint(\"tf version: \", tf.__version__)\r\n\r\n# Keras Imports\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.layers import Dense, Dropout, Input\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\r\nfrom tensorflow.keras.callbacks import CSVLogger\r\n\r\n# Sklearn\r\nfrom sklearn import metrics\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n# Garbage Collector\r\nimport gc\r\nimport sys\r\n\r\nimport transformers\r\n# from transformers import TFAutoModel, AutoTokenizer\r\n# from transformers import RobertaTokenizer, TFRobertaModel\r\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\r\nfrom tqdm.notebook import tqdm\r\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\r\n\r\n# Boto is the Amazon Web Services (AWS) SDK for Python, which allows Python developers to write software that makes\r\n# use of Amazon services like S3 and EC2. Boto provides an easy to use, object-oriented API as well as low-level direct\r\n# service access.\r\nimport boto3\r\n#import s3fs\r\n# args\r\nimport argparse\r\n\r\n###################################################################\r\n# HELPER FUNCTIONS\r\n###################################################################\r\nprint(\"defining helper functions...\")\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser()\r\n    # data directories\r\n    parser.add_argument('--data', type=str, default=os.environ.get('SM_CHANNEL_DATA'))\r\n    return parser.parse_known_args()\r\n\r\n\r\ndef regular_encode(texts, tokenizer, maxlen=512):\r\n    enc_di = tokenizer.batch_encode_plus(\r\n        texts, \r\n        return_attention_masks=False, \r\n        return_token_type_ids=False,\r\n        pad_to_max_length=True,\r\n        max_length=maxlen\r\n    )\r\n    return np.array(enc_di['input_ids'])\r\n\r\n\r\ndef build_model(transformer, max_len=512):\r\n    \"\"\"\r\n    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\r\n    \"\"\"\r\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\r\n    sequence_output = transformer(input_word_ids)[0]\r\n    cls_token = sequence_output[:, 0, :]\r\n    out = Dense(1, activation='sigmoid')(cls_token)\r\n    \r\n    model = Model(inputs=input_word_ids, outputs=out)\r\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\r\n    \r\n    return model\r\n\r\n\r\ndef get_train_data(data_dir):\r\n    train = pd.read_csv(os.path.join(data_dir, \"jigsaw_mjy_train_val_openaug_523200.csv\"))\r\n    test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\r\n    sub = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\r\n    return train, test, sub\r\n\r\nif __name__ == \"__main__\":\r\n    \r\n    ###################################################################\r\n    # SETTINGS\r\n    ###################################################################\r\n    # Detect hardware, return appropriate distribution strategy\r\n    try:\r\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\r\n        # set: this is always the case on Kaggle.\r\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n        print('Running on TPU ', tpu.master())\r\n    except ValueError:\r\n        tpu = None\r\n    \r\n    if tpu:\r\n        tf.config.experimental_connect_to_cluster(tpu)\r\n        tf.tpu.experimental.initialize_tpu_system(tpu)\r\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n    else:\r\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\r\n        strategy = tf.distribute.MirroredStrategy()\r\n        \r\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\r\n    \r\n    ###################################################################\r\n    # CONSTANTS\r\n    ###################################################################\r\n    print(\"setting constants...\")\r\n    \r\n    # Configuration\r\n    MAX_LEN = 192\r\n    #MODEL = 'jplu/tf-xlm-roberta-large'\r\n    #MODEL = 'roberta-base'\r\n    MODEL = 'distilbert-base-multilingual-cased'\r\n    myOutput = '/opt/ml/model/'\r\n    expCounter = 1\r\n    AUTO = tf.data.experimental.AUTOTUNE\r\n    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\r\n    EPOCHS = 2\r\n    \r\n    ###################################################################\r\n    # TOKENIZER\r\n    ###################################################################\r\n    print(\"loading tozenizer...\")\r\n    \r\n    # First load the real tokenizer\r\n    #tokenizer = AutoTokenizer.from_pretrained(MODEL)\r\n    #tokenizer = RobertaTokenizer.from_pretrained(MODEL)\r\n    tokenizer = DistilBertTokenizer.from_pretrained(MODEL)\r\n    \r\n    ###################################################################\r\n    # DATA\r\n    ###################################################################\r\n    print(\"loading training data...\")\r\n\r\n    args, _ = parse_args()\r\n    \r\n    train, test, sub = get_train_data(args.data)\r\n    x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\r\n    x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\r\n    y_train = train.toxic.values\r\n    \r\n    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.33, random_state=1331)\r\n    \r\n    train_dataset = (\r\n        tf.data.Dataset\r\n        .from_tensor_slices((x_train, y_train))\r\n        .repeat()\r\n        .shuffle(2048)\r\n        .batch(BATCH_SIZE)\r\n        .prefetch(AUTO)\r\n    )\r\n    \r\n    valid_dataset = (\r\n        tf.data.Dataset\r\n        .from_tensor_slices((x_valid, y_valid))\r\n        .batch(BATCH_SIZE)\r\n        .cache()\r\n        .prefetch(AUTO)\r\n    )\r\n    \r\n    test_dataset = (\r\n        tf.data.Dataset\r\n        .from_tensor_slices(x_test)\r\n        .batch(BATCH_SIZE)\r\n    )\r\n\r\n    ###################################################################\r\n    # LOAD MODEL\r\n    ###################################################################\r\n    print(\"loading model ...\")\r\n    \r\n    with strategy.scope():\r\n        #transformer_layer = TFAutoModel.from_pretrained(MODEL)\r\n        transformer_layer = TFDistilBertModel.from_pretrained(MODEL)\r\n        model = build_model(transformer_layer, max_len=MAX_LEN)\r\n    model.summary()\r\n    \r\n    ###################################################################\r\n    # TRAINING\r\n    ###################################################################\r\n    print(\"run training ...\")\r\n    \r\n    n_steps = x_train.shape[0] // BATCH_SIZE\r\n    \r\n    train_history = model.fit(\r\n        train_dataset,\r\n        steps_per_epoch=n_steps,\r\n        validation_data=valid_dataset,\r\n        epochs=EPOCHS\r\n    )\r\n    \r\n    ###################################################################\r\n    # SCORE\r\n    ###################################################################\r\n    print(\"score test data ...\")\r\n    sub['toxic'] = model.predict(test_dataset, verbose=1)\r\n    sub.to_csv('submission.csv', index=False)\r\n    \r\n    ###################################################################\r\n    # COMPLETE\r\n    ###################################################################\r\n    \r\n    print(\"Program complete!\")\r\n    \r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nip-10-0-221-83:51:274 [1] NCCL INFO comm 0x7f5f70006a40 rank 1 nranks 4 cudaDev 1 nvmlDev 1 - Init COMPLETE\r\nip-10-0-221-83:51:275 [2] NCCL INFO comm 0x7f5f68006a40 rank 2 nranks 4 cudaDev 2 nvmlDev 2 - Init COMPLETE\r\nip-10-0-221-83:51:276 [3] NCCL INFO comm 0x7f5f6c007790 rank 3 nranks 4 cudaDev 3 nvmlDev 3 - Init COMPLETE\r\nip-10-0-221-83:51:268 [0] NCCL INFO Launch mode Group/CGMD\r\nERROR:root:'NoneType' object has no attribute 'write'\r\n#015   1/5561 [..............................] - ETA: 61:32:20 - loss: 0.7133 - accuracy: 0.531\r\n```\r\n\r\nFull work can be found here:\r\nhttps://github.com/yeamusic21/DistilBert-TF2-Keras-Multi-GPU-Sagemaker-Training\r\nData can be found here:\r\nwww.kaggle.com/yeayates21/jigsaw-multilingual-aug-mix\r\nhttps://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data\r\n", "comments": ["@yeamusic21,\r\nI was able to run the given code on TF v2.2 without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/28c33edf8f510973b80a538727daec4f/39721.ipynb). Thanks! ", "Ah, so looks like 2.2.0 is not supported on Sagemaker yet.\r\n\r\nhttps://github.com/aws/sagemaker-python-sdk#tensorflow-sagemaker-estimators\r\n\r\nGiven your Colab notebook, I trust this is just a version issue.\r\n\r\nSo what does the error message mean?  Do you know?  Looks like my code still runs with the error message, do you think it's still running on all GPUs?\r\n\r\nThanks for all your help @amahendrakar !! \ud83d\udc4d \ud83d\udcaf :-D", "FYI  - I was able to test this using Azure and 2.2.0 gives no error message.  Thanks again!", "The resolver is only supported in Google Cloud according to the documentation. Is there a workaround to give the sagemaker details on the cluster runtime specs to the resolver so that we can run the code similarly ? @amahendrakar ", "@t-T-s,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 39720, "title": "TFLite, Model with Conv2DTranspose fails to convert, fully quantization, int8", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): tf-nightly\r\n- TensorFlow version (or github SHA if from source): tf-nightly\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThis issue is very similar to the [issue](https://github.com/tensorflow/tensorflow/issues/39718), but \r\nthe problematic layer is Conv2DTranspose, so it is different model here.\r\nI tested models with other layers and all are fine, except this one and the issue logged above, separately.\r\n\r\nhttps://colab.research.google.com/drive/1g8wjs5D3N9blNpWYMIQ8R_AipZASUKH8?usp=sharing\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput_size = [5, 5, 2]\r\nkernel_size = [3, 3, 6]\r\nstride = [2, 2]\r\n\r\ninput_0 = tf.keras.layers.Input(shape=input_size)\r\nlayer_0 = tf.keras.layers.Conv2DTranspose(\r\n            filters=kernel_size[-1],\r\n            kernel_size=kernel_size[0:2],\r\n            strides=stride,\r\n            activation=None,\r\n            use_bias=False,\r\n            name = \"transpose_conv\"\r\n        )(input_0)\r\nmodel = tf.keras.models.Model(inputs=[input_0], outputs=[layer_0])\r\nmodel.summary()\r\n\r\nkeras_layer = [\r\n  layer for layer in model.layers if layer.name == \"transpose_conv\"\r\n][0]\r\nkeras_layer.set_weights(\r\n            [\r\n                np.random.rand(\r\n                    kernel_size[0],\r\n                    kernel_size[1],\r\n                    kernel_size[2],\r\n                    input_size[2],\r\n                ).astype(np.float32)\r\n            ]\r\n        )\r\n\r\nnum_calib = 1000\r\ndef _get_calib_data_func():\r\n  def representative_data_gen():\r\n    for _ in range(num_calib):\r\n      yield [\r\n        np.random.rand(\r\n          1, input_size[0], input_size[1], input_size[2],\r\n        ).astype(np.float32)\r\n      ]\r\n\r\n  return representative_data_gen\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.representative_dataset = _get_calib_data_func()\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\ntflite_model_INT8 = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n<ipython-input-18-717edec90ae0> in <module>()\r\n      3 \r\n      4 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n----> 5 tflite_model_INT8 = converter.convert()\r\n\r\n3 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, resize_input)\r\n     91     return self._calibrator.QuantizeModel(\r\n     92         np.dtype(input_type.as_numpy_dtype()).num,\r\n---> 93         np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n     94 \r\n     95   def calibrate_and_quantize_single(self,\r\n\r\nRuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor functional_5/transpose_conv/Shape\r\nEmpty min/max for tensor functional_5/transpose_conv/Shape\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/84df698f70c2446228fb50f1914c5a38/untitled194.ipynb) . Thanks!", "I am also experiencing a similar problem with a similar model. \r\n", "The issue seems to be resolved with the latest tf-nightly ('2.3.0-dev20200610') and TF 2.2.0. Feel free to re-open it if you still face this issue. \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39720\">No</a>\n"]}, {"number": 39719, "title": "TPU PyFunction results in UnavailableError: failed to connect to all addresses", "body": "Similar to #38762, and unfixed in tf-nightly. See gist here https://colab.research.google.com/gist/jonashaag/128c2b9c51cb0df5d00ef072928631e3/38762.ipynb\r\n\r\n```\r\nds = tf.data.Dataset.from_generator(f, output_types='int32').map(g)\r\n(_ for _ in ds)\r\n```\r\n\r\n```\r\nUnavailableError: failed to connect to all addresses\r\n```", "comments": ["Note: This is partly fixed in 20200611 in that creating the `Dataset` instance works, and also turning it into a generator. However if you replace the last line in the Gist with\r\n\r\n```\r\nnext(_ for _ in ds)\r\n```\r\n\r\nit is still broken in the same way.", "I tried to run the code  in colab with TF v2.5 & didn't face any issue,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/8ffd408110456c34b3f40e894a74fbfb/38762.ipynb#scrollTo=esVOgxbb2rx9)..Thanks !", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39719\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39719\">No</a>\n"]}, {"number": 39718, "title": "TF Lite nightly: Model with Fully Connected layer can't be converted, fully quantization, int8", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): tf-nightly\r\n- TensorFlow version (or github SHA if from source): tf-nightly\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\ntrain_data, test_data = mnist.load_data()\r\n\r\npre_process = lambda x: x / 255.0\r\nnum_calib = 1000\r\ncalib_data = pre_process(\r\n            train_data[0][0 : num_calib].astype(np.float32)\r\n        )\r\n\r\nmodel = tf.keras.Sequential(\r\n            [\r\n                tf.keras.layers.InputLayer(input_shape=(28, 28)),\r\n                tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n                tf.keras.layers.Conv2D(\r\n                    filters=12, kernel_size=(3, 3), activation=tf.nn.relu\r\n                ),\r\n                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n                tf.keras.layers.Flatten(),\r\n                tf.keras.layers.Dense(10, activation=tf.nn.softmax),\r\n            ]\r\n        )\r\nmodel.summary()\r\n\r\ntrain_images = pre_process(train_data[0])\r\ntrain_labels = train_data[1]\r\ntest_images = pre_process(test_data[0])\r\ntest_labels = test_data[1]\r\n# Train the digit classification model\r\nmodel.compile(\r\n  optimizer=\"adam\",\r\n  loss=\"sparse_categorical_crossentropy\",\r\n  metrics=[\"accuracy\"],\r\n)\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=1,\r\n  validation_data=(test_images, test_labels),\r\n)\r\n\r\ndef _get_calib_data_func():\r\n  def representative_data_gen():\r\n    for input_value in calib_data:\r\n      input_value = np.expand_dims(input_value, axis=0).astype(np.float32)\r\n      yield [input_value]\r\n\r\n  return representative_data_gen\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.representative_dataset = _get_calib_data_func()\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\ntflite_model_INT8 = converter.convert()\r\n```\r\n\r\n\r\n\r\n```\r\n**RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor sequential_2/reshape_2/Shape\r\nEmpty min/max for tensor sequential_2/reshape_2/Shape**\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@wwwind,\r\nI was able to reproduce the issue with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/4a4ea0178b34597d6035794ba3eb3e36/39718-nightly.ipynb). However, the code works with the stable version [TF v2.2](https://colab.research.google.com/gist/amahendrakar/54d9272a3c32841d1532a994cc5820bb/39718-2-0.ipynb). Please check the linked gist.\r\n\r\nCould you please try running the code with TF v2.2 and let us know if it works. Thanks!", "@amahendrakar Yes, I confirm - it works in tensorflow 2.2.0", "@wwwind,\r\nThank you for the update. Please feel free to close the issue if resolved. Thanks!", "I get the same error when dealing with tf-nightly. However, I can't go back to 2.2 because a new error suddenly shows up: Tensor 'input_1' has invalid shape '[None, None, None, 3]'. I've seen in other thread it's recommended to use tf-nightly to fix the last error.\r\n\r\nSo I can see here kind of deadlock between versions.", "> @amahendrakar Yes, I confirm - it works in tensorflow 2.2.0\r\n\r\nClosing this issue as resolved.", "@sramirez,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template along with the complete code, so that we can track the issue there. Thanks!", "@amahendrakar Could this bug be fixed in tf-nightly ? \r\nI am working with tf-nightly and I really appreciate if it will be fixed in the current code.", "Thanks, I will take a look into this.", "@wwwind I have updated your code and [here](https://colab.research.google.com/gist/jvishnuvardhan/05d9d1bcfe32caead45728ac250e8dbf/39718-nightly.ipynb) is the working version of your code with `tf-nightly`. Thanks!", "At a high level there are two ways to fix this issue (one of which @jvishnuvardhan pointed out).\r\n\r\n1. Try setting it in keras and keeping conversion the same way:\r\n```\r\ninput = tf.keras.layers.Input(shape=(225, 225, 3), batch_size=1)\r\n```\r\n\r\n2. Use `set_shape` on the model input and fix the batch size to `1` (example here: https://www.tensorflow.org/lite/convert/python_api#examples_).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39718\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39718\">No</a>\n"]}, {"number": 39717, "title": "Update tpu_strategy.py; replacing np.rank with np.ndim", "body": "np.rank[DEP] -> np.ndim\r\n**Ref**: https://numpy.org/doc/stable/release.html#rank-function\r\n\r\n**Fixing this error:**\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:174 run  **\r\n        return self.extended.tpu_run(fn, args, kwargs, options)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:867 tpu_run\r\n        return func(args, kwargs)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:915 tpu_function\r\n        rank = np.rank(input_tensor)\r\n\r\nAttributeError: module 'numpy' has no attribute 'rank'", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39717) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39717) for more info**.\n\n<!-- ok -->"]}, {"number": 39716, "title": "TF 2.2.0 ptxas issue", "body": "Running in linux some code I get the following warning:\r\n\r\n`You are using ptxas 8.x, but TF requires ptxas 9.x (and strongly prefers >= 9.2.88).  Compilation of XLA kernels below will likely fail.`\r\n\r\nHowever when I run with the flag `TF_CPP_VMODULE=asm_compiler=2` I get this output:\r\n\r\n`2020-05-20 14:57:36.735438: I tensorflow/stream_executor/gpu/asm_compiler.cc:169] Looking for ptxas at /usr/local/cuda-10.1/bin/bin/ptxas`\r\n`2020-05-20 14:57:36.735500: I tensorflow/stream_executor/gpu/asm_compiler.cc:169] Looking for ptxas at /usr/local/cuda/bin/ptxas`\r\n`2020-05-20 14:57:36.735521: I tensorflow/stream_executor/gpu/asm_compiler.cc:178] Using ptxas at /usr/local/cuda/bin/ptxas`\r\n\r\nSo it seems that when it's looking in cuda-10.1 it's going in the wrong dir (there is no cuda-10.1/bin/bin, the ptxas binary is in cuda-10.1/bin). Does anyone know how to fix this?\r\n\r\nThe situation is somewhat similar to #33375 ", "comments": ["@FMalerba Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Managed to find a workaround with\r\n`export XLA_FLAGS=--xla_gpu_cuda_data_dir=path_to_cuda`", "@FMalerba \r\nplease let us know if we can move this issue to closed status.", "sure", "Moving to closed status with confirmation", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39716\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39716\">No</a>\n"]}, {"number": 39715, "title": "tfa ReduceLROnPlateau callback from Tf keras is not recognizing cohen kappa metrics direction in 'Auto' mode", "body": "Hi all,\r\nthe 'auto' mode in ReduceLROnPlateau  and ModelCheckpoint  are looking for specific string 'acc' in the name of the metrics to be monitor. this actually leads to unlickly scenarious of not working properly even while using metrics that are defined in tfa and hoping tf will be aware of the direction . this can be added in the doc to make the developers understand how to name their metrics or to set min max mode on their own. \r\nthanks\r\n\r\nhttps://github.com/tensorflow/addons/issues/1865\r\n", "comments": ["@yuvaramsingh94 \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Request you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest yo to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram This Is as DOC improvment issue but he hasn't selected the doc template opening this.", "I used colab. this is a simple one line code and its a doc issue . i do remember opening this issue under `Documentation Issue` .\r\nat https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau\r\n\r\nunder Arguments -> mode we can find this line `in auto mode, the direction is automatically inferred from the name of the monitored quantity.`\r\n\r\nthis statement is misleading as it says auto mode can detect direction on its own . but, it does not work if the naming of the metrics does not fall within the criteria set in tf callback code as explained above ('acc' string is imp to determine the direction). \r\n\r\nso requesting to modify this line to talk about the naming part of the metrics too. this is same for `ModelCheckpoint ` and few others.\r\n\r\nthanks\r\n ", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}, {"number": 39714, "title": "Two weird things when I use custom model `train_step` and loss", "body": "**System information**\r\n- colab reproduce url: https://colab.research.google.com/drive/15hJPKHgn8aPi1mVi3XFOFvOgj85QcL-Z?usp=sharing\r\n- windows 10\r\n- TensorFlow version 2.2\r\n- Python version:3.7\r\n- CUDA/cuDNN version:10.1\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom tensorflow.python.eager import backprop\r\nfrom tensorflow.python.keras.engine import data_adapter\r\n\r\nclass CustomModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(CustomModel, self).__init__()\r\n        self.flat = tf.keras.layers.Flatten(input_shape = (28, 28))\r\n        \r\n    def call(self, inputs, training=False, **kwargs):\r\n        x = self.flat(inputs)\r\n        out = (x, x, x)\r\n        return out\r\n\r\n    def train_step(self, data):\r\n        data = data_adapter.expand_1d(data)\r\n        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\r\n\r\n        with backprop.GradientTape() as tape:\r\n            y_pred = self(x, training = True)\r\n            loss0 = tf.reduce_sum(self.losses)\r\n            loss1, loss2, loss3 = self.loss(y, y_pred)\r\n            total_loss = tf.reduce_sum([loss0, loss1, loss2, loss3])\r\n        grads = tape.gradient(total_loss, self.trainable_variables)\r\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n        self.compiled_metrics.update_state(y, y_pred)\r\n        return {m.name: m.result() for m in self.metrics}\r\n\r\nclass MultiTaskLoss(tf.keras.losses.Loss):\r\n    def __init__(self):\r\n        super(MultiTaskLoss, self).__init__(reduction = tf.keras.losses.Reduction.NONE)\r\n\r\n    def call(self, y_true, y_pred):\r\n        tf.print(y_pred[0].shape, y_pred[1].shape, y_pred[2].shape)\r\n        loss1 = tf.reduce_sum(y_pred[0])\r\n        loss2 = tf.reduce_sum(y_pred[1])\r\n        loss3 = tf.reduce_sum(y_pred[2])\r\n        return tf.cast(loss1, tf.float32), tf.cast(loss2, tf.float32), tf.cast(loss3, tf.float32)\r\n\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\n\r\ntfds.list_builders()\r\ndataset = tfds.load('mnist', split='train')\r\ndataset = dataset.map(lambda exa: (exa['image'], exa['label']))\r\ndataset = dataset.batch(8)\r\nmodel = CustomModel()\r\nloss = MultiTaskLoss()\r\nmodel.compile(loss = loss, optimizer = 'Adam')\r\nmodel.fit(dataset, epochs=1)\r\n```\r\n\r\n**current behavior**\r\n1 can't recieve tuple y_pred in custom loss when using self.compiled_loss(y, y_pred)\r\n2 `iterating over `tf.Tensor` is not allowed` exception raised when using autograph\r\n\r\n**Describe the expected behavior**\r\nhope everything is ok.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/15hJPKHgn8aPi1mVi3XFOFvOgj85QcL-Z?usp=sharing\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/bd69b7ad6c3d9fce4938ff3a7f29ff00/39714.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/f9ab32fa296f574d41e7fb9cecfb5048/39714.ipynb). Please find the attached gist. Thanks!", "@BSlience Can yo please make a simple standalone code to reproduce the issue? The current code runs for a long time. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39714\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39714\">No</a>\n"]}, {"number": 39713, "title": "Does tensorflow 1.15.0 support int8 tflite convertion? Wrong accurary.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.15\r\n- TensorFlow version (or github SHA if from source): 1.15.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n\r\nconverter=tf.lite.TFLiteConverter.from_frozen_graph(pb_path,input_arrays=input_tensor_name\r\n                                                   ,output_arrays=class_tensor_name\r\n                                                   ,input_shapes=input_tensor_shape)\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.representative_dataset=representative_data_gen\r\n\r\ntflite_model=converter.convert()\r\n \r\n \r\nwith open('owntempuint.tflite','wb') as f:\r\n    f.write(tflite_model)\r\n\r\n```\r\n\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n\r\nThe int8 model produced successfully, however, the accuracy is very low, while from the same .pb model whose accuracy is about 0.51, float tflite model achieve 0.47 accuracy, the int8 tflite model only has 0.04 with the same input.\r\n\r\n", "comments": ["@dreamPoet  \r\nI ran the code shared and face a different error, please provide with complete code such that we can replicate the issue.\r\nPlease find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/25d7a8784b7190e680df4f5b2272de10/untitled195.ipynb) of error faced.", "I think the error you showed was because you use tf2.0 which discards the .pb model. \r\n\r\nI can provide some codes producing tflite from .h5 model, which also have problem when producing int8 model.  \r\n[https://github.com/dreamPoet/tflearning/](https://github.com/dreamPoet/tflearning/)\r\n\r\nThese codes are written based on the official tutorial, as the validation set was produced randomly, every time the print information will be different, sample print info was like:\r\n\r\n> Raw model accuracy: 85.938% \r\n>Quant TF Lite accuracy: 84.375%\r\n>INT8 Quant TF Lite accuracy: 18.750%\r\n\r\nAs u can see, the INT8 input and output version tflite has very low accuracy.", "The gist error can be fixed by changing the `tf.lite.TFLiteConverter.from_frozen_graph` to `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph` \r\n\r\nCould you also share the code where you compute the inference? The one caveat with models that have `inference_input_type` and `inference_output_type` set to `int8` or `uint8` is this -- you need to manually map (aka quantize) the float inputs to integer inputs for inference on your device. To understand how this can be done -- refer to the equation provided [here](https://www.tensorflow.org/lite/performance/quantization_spec), and it's equivalent code in python [here](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/core/task/model_util_test.py#L89-L113) which compares a keras model's output with it's integer-only TFLite model's output . (I hope this makes sense -- if not I can provide more details)", "Closing issue due to user inactivity. Feel free to re-open it if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39713\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39713\">No</a>\n", "@MeghnaNatraj, Thanks for the suggestion, but I don't think `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph` is included in Tensorflow 1.15.0. Would you check? ", "@cloudlakecho It looks like it is included in TF 1.15.0. You can try the code here:\r\n\r\n1. Open https://colab.sandbox.google.com/ and start a new notebook.\r\n2. Run `pip install tensorflow==1.15.0`\r\n3. Restart the notebook.\r\n5. Run the following code \r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\ntf.compat.v1.lite.TFLiteConverter.from_frozen_graph\r\n```\r\n\r\nThe output is:\r\n```\r\n1.15.0\r\n<bound method TFLiteConverter.from_frozen_graph of <class 'tensorflow.lite.python.lite.TFLiteConverter'>>\r\n```\r\n\r\nThis implies that it was able to load the API. ", "@MeghnaNatraj Thanks for instruction, I tried and it is my output:\r\n```\r\n>>> import tensorflow as tf\r\n>>> print (tf.__version__)\r\n1.15.0\r\n>>> tf.compat.v1.lite.TFLitedConverter.from_frozen_graph\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/media/.venv/lib/python3.6/site-packages/tensorflow_core/python/util/module_wrapper.py\", line 193, in __getattr__\r\n    attr = getattr(self._tfmw_wrapped_module, name)\r\nAttributeError: module 'tensorflow._api.v1.compat.v1.lite' has no attribute 'TFLitedConverter'\r\n```\r\n\r\nIs there any discrepancy between you and me? I think I installed using `pip` and tested in Ubuntu 16.04. \r\n\r\nI also tried at Colab Sandbox and this is output:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-7541c3c1ebc5> in <module>()\r\n      3 print(tf.__version__)\r\n      4 \r\n----> 5 tf.compat.v1.lite.TFLiteConvert.from_frozen_graph\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py in __getattr__(self, name)\r\n    191   def __getattr__(self, name):\r\n    192     try:\r\n--> 193       attr = getattr(self._tfmw_wrapped_module, name)\r\n    194     except AttributeError:\r\n    195       if not self._tfmw_public_apis:\r\n\r\nAttributeError: module 'tensorflow._api.v1.compat.v1.lite' has no attribute 'TFLiteConvert'\r\n```\r\n\r\nOnly missing step is 3rd one, Restart the notebook.\r\n\r\nOh, I found the difference. It should be `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph`, `TFLiteConverter` not `TFLiteConvert`. It was my typo. \r\n\r\nThanks for the suggestion. ", "There is a typo in your command\r\n\r\n\u274c It is *not* - tf.compat.v1.lite.**TFLited**Converter.from_frozen_graph\r\n\u2705 Instead it is - tf.compat.v1.lite.**TFLite**Converter.from_frozen_graph"]}, {"number": 39712, "title": "Non-OK-status: tensorflow::Env::Default()->DeleteFile(ptx_path) status: Not found: ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentos7.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip install officail\r\n- TensorFlow version (use command below):\r\n1.15.2 with gpu\r\n- Python version:\r\n3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10.0/ 7.6\r\n- GPU model and memory:\r\nv100 32G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nCan't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version.\r\n2020-04-07 16:28:20.691087: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Searched for CUDA in the following directories:\r\n2020-04-07 16:28:20.691210: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   ./cuda_sdk_lib\r\n2020-04-07 16:28:20.691315: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   /usr/local/cuda\r\n2020-04-07 16:28:20.691454: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   .\r\n2020-04-07 16:28:20.691571: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:75] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\r\n2020-04-07 16:28:20.725596: F tensorflow/stream_executor/cuda/ptxas_utils.cc:181] Non-OK-status: tensorflow::Env::Default()->DeleteFile(ptx_path) status: Not found: /tmp/tempfile-72d9c7c8-2841-4447-bd7d-3947098f8e24-6a7fc700-2624-5a2af2a888f80; No such file or directory\r\nFatal Python error: Aborted\r\n\r\n**Describe the expected behavior**\r\n1. the code in follows log the mislead warning? \r\nhttps://github.com/tensorflow/tensorflow/blob/4386a6640c9fb65503750c37714971031f3dc1fd/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc#L405\r\n```\r\n       bool log_warning = true;\r\n          if (maybe_cubin.status().code() ==\r\n              tensorflow::error::Code::NOT_FOUND) {\r\n            // Missing ptxas is expected in some environments where CUDA SDK\r\n            // binaries are not available. We don't want to spam logs with\r\n            // identical warnings in this case.\r\n\r\n            // TODO(jlebar): we should implement a LOG_FIRST_N and LOG_EVERY_N\r\n            // for more general usage.\r\n            static std::atomic<bool> warning_done(false);\r\n            log_warning = !warning_done.exchange(true);\r\n          }\r\n          if (log_warning) {\r\n            PrintCantFindCudaMessage(\r\n```\r\n2. if some exception that the ptx_path did not create, there will rasie a error that : `2020-04-07 16:28:20.725596: F tensorflow/stream_executor/cuda/ptxas_utils.cc:181] Non-OK-status: tensorflow::Env::Default()->DeleteFile(ptx_path) status: Not found:`\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4386a6640c9fb65503750c37714971031f3dc1fd/tensorflow/stream_executor/cuda/ptxas_utils.cc#L184\r\n```\r\n\r\n  // Write ptx into a temporary file.\r\n  string ptx_path;\r\n  if (!env->LocalTempFilename(&ptx_path)) {\r\n    return port::InternalError(\"couldn't get temp PTX file name\");\r\n  }\r\n  auto ptx_cleaner = tensorflow::gtl::MakeCleanup([&ptx_path] {\r\n    TF_CHECK_OK(tensorflow::Env::Default()->DeleteFile(ptx_path));\r\n  });\r\n\r\n  TF_RETURN_IF_ERROR(\r\n      tensorflow::WriteStringToFile(env, ptx_path, ptx_contents));\r\n  VLOG(2) << \"ptx written to: \" << ptx_path;\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ymodak  could you look at this problem?", "Does this reproduce on tensorflow-nightly?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Does this reproduce on tensorflow-nightly?\r\n\r\ni didnot try , because we are using tf v1.15", "Most likely it has been solved in nightly. If it has, we can attempt a backport when doing a patch release on 1.15.\r\n\r\nIn the end, we are doing development only on master branch and the releases are snapshots of the code at that time, with some cherry-picks. So you will have to move to newer versions at one point", "Has it been solved? I have a same problem using TF2.0.0. Please tell me the solution, thank you !", "@GuoYL36 can you try reproducer with `tf-nightly`?", "> @GuoYL36 can you try reproducer with `tf-nightly`?\r\n\r\nNo, I did not try. Now I can run code normally without doing anything, and I just want to say it is amazing.<^_^>   ", "So it seems it has been solved. I think we can close the issue and reopen if there is a resurgence", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39712\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39712\">No</a>\n"]}, {"number": 39711, "title": "Add test for tangent batches", "body": "- Use tf.vectorized_map to run _jvp over a batch of tangents \r\n- Test correctness with a function that right-multiplies tangents to jacobian matrix\r\n\r\n@allenlavoie ", "comments": ["@abhichou4 This PR is in draft, any update on this? Please. Thanks!", "> @abhichou4 This PR is in draft, any update on this? Please. Thanks!\r\n\r\nSorry for the delay. I wanted to use this PR to figure out related tests that can be added. I can add them here or make other smaller PRs. @allenlavoie let me know."]}, {"number": 39710, "title": "SciSharp.TensorFlow.Redist-Windows-GPU missing for tf.net 0.20.0-alpha", "body": "**Problem**\r\n\r\nIs there a SciSharp.TensorFlow.Redist-Windows-GPU (tensorflow.dll) distribution available for Tensorflow.NET 0.20.0-alpha?\r\n\r\nThe most resent NuGet package available seems to be v. 1.15.1, which does not seem to be compatible with Tensorflow.NET 0.20.0-alpha.\r\n\r\nWhen could a working redistributable be expected?\r\nIs it possible to download it or build it manually? (please provide steps)\r\n\r\n**Any other info / logs**\r\nThe following exception is thrown when trying to use SciSharp.TensorFlow.Redist-Windows-GPU version v. 1.15.1 with Tensorflow.NET 0.20.0-alpha:\r\n\r\nSystem.TypeInitializationException: The type initializer for 'Tensorflow.Binding' threw an exception. ---> System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation. ---> System.EntryPointNotFoundException: Unable to find an entry point named 'VSpace_Handle' in DLL 'tensorflow'.\r\n    at Tensorflow.c_api.VSpace_Handle(VSpace_callback_Ones ones, VSpace_callback_AggregateGrads aggregate_grads)\r\n   at Tensorflow.tensorflow.InitGradientEnvironment()\r\n--- End of inner exception stack trace ---\r\n    at System.RuntimeTypeHandle.CreateInstance(RuntimeType type, Boolean publicOnly, Boolean noCheck, Boolean& canBeCached, RuntimeMethodHandleInternal& ctor, Boolean& bNeedSecurityCheck)\r\n   at System.RuntimeType.CreateInstanceSlow(Boolean publicOnly, Boolean skipCheckThis, Boolean fillCache, StackCrawlMark& stackMark)\r\n   at System.Activator.CreateInstance[T]()\r\n   at Tensorflow.Binding.New[T]()\r\n   at Tensorflow.Binding..cctor()\r\n--- End of inner exception stack trace ---\r\n    at Tensorflow.Binding.get_tf()\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39710\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39710\">No</a>\n", "@SG-AI Latest [tensorflow.dll ](https://github.com/tensorflow/tensorflow/issues/35405#issuecomment-653621878)for c# api "]}, {"number": 39709, "title": "builtins.TypeError: Expected binary or unicode string, got 0", "body": "I am quite new to Tensorflow, so in summary I converted the pcap file into csv file and load it in the python, I have made a column named detectionnumber such that when the Info column contains the word \"SYN\" and \"ACK\",  or \"RST\", it will append the value into the column I have made. Then I managed to add the column named detection I have made into the csv file. But the problem lies when I try to get the csv file into the dataset, it keeps showing the error, what should I do to solve these issues?`\r\nHere is the file: [data.zip](https://github.com/tensorflow/tensorflow/files/4656000/data.zip)\r\n\r\n`import datetime\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nimport pandas as pd\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nimport csv\r\n\r\nfile_path = '/home/root2001/Documents/PythonForMachineLearning/L05-Network-Packets'\r\nfile_training_full = pd.read_csv(file_path, sep='delimiter', header=None)\r\n\r\nf = open(file_path)\r\ncsvf = csv.reader(f)\r\n\r\n#for row in csvf:\r\n    #print(row)\r\n\r\n#Adds a column to each row, a row contain SYN ACK append 1, RST append 2, or else append 0\r\nprotocol1 = \"SYN\"\r\nprotocol2 = \"ACK\"\r\nprotocol3 = \"RST\"\r\n\r\n\r\ndetectionnumber = []\r\nfor row1 in csvf:\r\n   if (protocol1 in row1[6] and protocol2 in row1[6] ):\r\n      value = 1\r\n      detectionnumber.append(value)\r\n   elif (protocol3 in row1[6]):\r\n      value = 2\r\n      detectionnumber.append(value)\r\n   else:\r\n      value = 0\r\n      detectionnumber.append(value)\r\n   \r\nprint(detectionnumber)\r\ntest = len(detectionnumber)\r\nprint(test)\r\n\r\nfile_training_full[\"detection\"] = detectionnumber\r\nprint(file_training_full)\r\n\r\nLABEL_COLUMN = 'detection'\r\n\r\ndef get_dataset(file_path, **kwargs):\r\n\r\n   dataset = tf.data.experimental.make_csv_dataset(\r\n      file_path,\r\n      batch_size=100,\r\n      label_name=LABEL_COLUMN,\r\n      num_epochs=1, **kwargs) \r\n   return dataset\r\n\r\nget_dataset(file_training_full)`", "comments": ["@Sustrommings\r\n Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "System Information: Linux Ubuntu 18.04 64bit, Tensorflow version 2.0.0, Tensorflow is compiled from source (gcc 7.5.0), python version 3.6.8\r\n\r\nCSV file - [data.zip](https://github.com/tensorflow/tensorflow/files/4656656/data.zip)\r\n\r\n\r\n\r\nHere is the complete code\r\n\r\nimport datetime\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow import feature_column\r\nfrom tensorflow.keras import layers\r\nimport pandas as pd\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nimport csv\r\n\r\nfile_path = '/home/root2001/Documents/PythonForMachineLearning/L05-Network-Packets'\r\nfile_training_full = pd.read_csv(file_path, sep='delimiter', header=None)\r\n\r\nf = open(file_path)\r\ncsvf = csv.reader(f)\r\n\r\n#for row in csvf:\r\n    #print(row)\r\n\r\n#Adds a column to each row, a row contain SYN ACK append 1, RST append 2, or else append 0\r\nprotocol1 = \"SYN\"\r\nprotocol2 = \"ACK\"\r\nprotocol3 = \"RST\"\r\n\r\n\r\ndetectionnumber = []\r\nfor row1 in csvf:\r\n   if (protocol1 in row1[6] and protocol2 in row1[6] ):\r\n      value = 1\r\n      detectionnumber.append(value)\r\n   elif (protocol3 in row1[6]):\r\n      value = 2\r\n      detectionnumber.append(value)\r\n   else:\r\n      value = 0\r\n      detectionnumber.append(value)\r\n   \r\nprint(detectionnumber)\r\ntest = len(detectionnumber)\r\nprint(test)\r\n\r\nfile_training_full[\"detection\"] = detectionnumber\r\nprint(file_training_full)\r\n\r\n\r\nLABEL_COLUMN = 'detection'\r\n\r\n\r\ndef get_dataset(file_path, **kwargs):\r\n\r\n   dataset = tf.data.experimental.make_csv_dataset(\r\n      file_path,\r\n      batch_size=100,\r\n      label_name=LABEL_COLUMN,\r\n      num_epochs=1, **kwargs) \r\n   return dataset\r\n\r\nget_dataset(file_training_full)\r\n\r\n\r\n#model = tf.keras.models.Sequential([\r\n   #tf.keras.layers.Dense(128, activation='relu'),\r\n   #tf.keras.layers.Dropout(0,2),\r\n   #tf.keras.layers.Dense(10)\r\n   #])\r\n", "@\r\nI ran the code shared by you and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/a238f8e61c367d9416b883c84edfa31a/untitled190.ipynb)", "Ok I have now fixed the indentation error, now the problem is the file_path where the retrieval of the csv file from my ubuntu directory, but the error has shown that there are no such file or directory, how do I specify the file path in the gist\r\n\r\n[Updated Gist](https://gist.github.com/Sustrommings/04ba51877a3067cbb712056ac2ab6051)", "I have already solved the problem, thanks for helping."]}]