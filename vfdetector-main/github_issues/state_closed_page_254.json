[{"number": 46846, "title": "Creating custom model for object detection", "body": "How can I create a custom model from a directory containing some images????????????I dont want to download a pre trained model.I want to make my own model for object detection.How can I do that??????", "comments": ["@Barshan-Mandal \r\nPlease follow these links: [link](https://keras.io/examples/vision/retinanet/) [link1](https://github.com/jaspereb/Retinanet-Tutorial),[link2](https://d2l.ai/).\r\nAS this is not a bug or feature request, please move this issue to closed status and open a [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for any further queries.", "I asked about training using custom images....How can I do that??\r\n", "I dont want to use pre trained model.I asked about training using custom images from a folder....How can I do that??", "@Barshan-Mandal\r\nA directory containing just images is not enough for object detection, you will have to annotate your images, you can create your own model from scratch the recommended way is to apply transfer learning on pre-trained model that would save a lot of time and computation, yielding good accuracy.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46845, "title": "[docker]  ERROR executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc", "body": "I have configured the `GPU` build from sources via `docker` as described in the docs:\r\n```\r\ndocker pull tensorflow/tensorflow:devel-gpu\r\ndocker run --gpus all -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" tensorflow/tensorflow:devel-gpu bash\r\n```\r\nThe `configure` selected options were:\r\n\r\n```\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1 \r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: core2\r\n```\r\n\r\nPlease note that I have selected `core2` since my aim is to build a older arch cpu with GPUs support build. Then I run the build as \r\n\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nUsing the build option `--verbose_failures` I get the following error logs:\r\n\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/external/com_google_protobuf/BUILD:110:11: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs \\\r\n    PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.d '-frandom-seed=bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.o' -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -w core2 -g0 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -c external/com_google_protobuf/src/google/protobuf/stubs/statusor.cc -o bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nx86_64-linux-gnu-gcc-7: error: core2: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.453s, Critical Path: 0.14s\r\nINFO: 17 processes: 14 internal, 3 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nThe whole log is available [here](https://gist.github.com/loretoparisi/d8d7b29010d72bedb37cc61f4f48b1d2).", "comments": ["@loretoparisi \r\n\r\nCan you please fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "@amahendrakar Yes, here are you system info (we are in `docker`)\r\n```\r\nroot@55166535ad1d:/tensorflow_src# bash <(curl -s https://raw.githubusercontent.com/tensorflow/tensorflow/master/tools/tf_env_collect.sh) && cat tf_env.txt\r\nCollecting system information...\r\nTraceback (most recent call last):\r\n  File \"/tmp/check_tf.py\", line 1, in <module>\r\n    import tensorflow as tf;\r\nModuleNotFoundError: No module named 'tensorflow'\r\nWARNING: Package(s) not found: tensorflow\r\nWrote environment to tf_env.txt. You can review the contents of that file.\r\nand use it to populate the fields in the github issue template.\r\n\r\ncat tf_env.txt\r\n\r\n\r\n== check python ===================================================\r\npython version: 3.6.9\r\npython branch: \r\npython build version: ('default', 'Oct  8 2020 12:12:24')\r\npython compiler version: GCC 8.4.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020\r\nos release version: 5.4.0-42-generic\r\nos platform: Linux-5.4.0-42-generic-x86_64-with-Ubuntu-18.04-bionic\r\nlinux distribution: ('Ubuntu', '18.04', 'bionic')\r\nlinux os distribution: ('Ubuntu', '18.04', 'bionic')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='55166535ad1d', release='5.4.0-42-generic', version='#46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy               1.18.5\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n(...)\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Feb  2 08:23:11 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\r\n| 45%   23C    P0    N/A /  75W |    312MiB /  4033MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0.221\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 9, 'final', 0)\r\n\r\n== bazel version  ====https://gist.github.com/loretoparisi/4808e2b53d7babdc33108b3615505d99https://gist.github.com/loretoparisi/4808e2b53d7babdc33108b3615505d99===========================================\r\nBuild label: 3.7.2\r\nBuild time: Thu Dec 17 16:57:23 2020 (1608224243)\r\nBuild timestamp: 1608224243\r\nBuild timestamp as int: 1608224243\r\n```\r\n\r\nThe whole output is available [here](https://gist.github.com/loretoparisi/4808e2b53d7babdc33108b3615505d99).\r\n\r\nThank you.", "@loretoparisi,\r\nCan you please let us know which version of Tensorflow are you trying to install? Thanks! ", "@rmothukuru Hello, I'm using the latest development docker file of tensorflow available on the docker hub `docker pull tensorflow/tensorflow:devel-gpu`\r\nAs result, the built docker image was\r\n\r\n```\r\nREPOSITORY              TAG                             IMAGE ID       CREATED         SIZE\r\ntensorflow/tensorflow   devel-gpu                       011678955031   3 days ago      7GB\r\n```\r\n\r\nSo I assume this image is using the `Dockerfile` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile) in the tensorflow repo, so it uses the master if I'm not wrong looking at the contents of the Docker file [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile#L87):\r\n\r\n```\r\n# Check out TensorFlow source code if --build-arg CHECKOUT_TF_SRC=1\r\nARG CHECKOUT_TF_SRC=0\r\nRUN test \"${CHECKOUT_TF_SRC}\" -eq 1 && git clone https://github.com/tensorflow/tensorflow.git /tensorflow_src || true\r\n```", "@rmothukuru @ravikyram thanks for your support. Do you need any other info from me to address this issue?\r\nIf I look at the error line it seems to me that the option for the architecture is not parsed correctly:\r\n\r\n```\r\nx86_64-linux-gnu-gcc-7: error: core2: No such file or directory\r\n```\r\n\r\nwhere `core2` has been passed as option to the config wizard here:\r\n\r\n```\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: core2\r\n```\r\n\r\n\r\n\r\nThank you.", "Any update on this issue? I'm willing to help if needed.", "@loretoparisi Sorry for the delay. Since `core2` is a platform target, it would be specified like this: `-march=core2` (more [details here](https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/i386-and-x86-64-Options.html)).", "@angerson hi Austin, thank you for your support. Yes I assumed that the bazel config would do this automatically when using the bazel wizard.\nIf you look here it seems there is an error in parsing config options for _march_\nvalue, for which I should have passed \"core2\", or at least I though so.\n\nThank you!\n\n> @rmothukuru @ravikyram thanks for your support. Do you need any other info from me to address this issue?\n> \n> If I look at the error line it seems to me that the option for the architecture is not parsed correctly:\n> \n> \n> \n> ```\n> \n> x86_64-linux-gnu-gcc-7: error: core2: No such file or directory\n> \n> ```\n> \n> \n> \n> where `core2` has been passed as option to the config wizard here:\n> \n> \n> \n> ```\n> \n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: core2\n> \n> ```\n> \n> \n> \n> \n> \n> \n> \n> Thank you.\n\n", "Sorry, I don't quite understand the quote there. When configuring TensorFlow's build, you should specify the whole `-march=core2` flag.", "@angerson okay thank you let me check the whole bazel command again", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46845\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46845\">No</a>\n"]}, {"number": 46844, "title": "gpt2 int8 quantization:  op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 15 (DEQUANTIZE) failed to prepare ", "body": "TF: 2.4.1 \r\nHuggingface/transformers: 4.2.2\r\nPython: 3.8\r\n\r\n**Describe the current behavior**\r\nThe int8 quantization during the exporting has no error or problem. But it would throw an error when it is invoked. \r\n```\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:61 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 15 (DEQUANTIZE) failed to prepare.\r\n```\r\nNote if we only use `last_hidden_state` for tflite, this would have no problem. It seems like `tf.matmul` is a problem.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport random\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom transformers import *\r\n\r\nrng = random.Random()\r\n\r\ngpt2_model = TFGPT2Model.from_pretrained('distilgpt2')\r\n\r\n\r\ndef get_tf_lm_head_tensor():\r\n    gpt2_lm_pt_model = GPT2LMHeadModel.from_pretrained('distilgpt2')\r\n    np_tensor = gpt2_lm_pt_model.lm_head.weight.detach().numpy()\r\n    np_tensor = np.transpose(np_tensor)\r\n    tf_lm_head_tensor = tf.convert_to_tensor(np_tensor)\r\n    return tf_lm_head_tensor\r\n\r\n\r\ntf_lm_head = get_tf_lm_head_tensor()\r\n\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(1, None), dtype=tf.int32, name=\"input_ids\")])\r\ndef serving_func(input_ids):\r\n    outputs = gpt2_model(input_ids, training=False)\r\n    last_hidden_state = outputs[0][0][-1:, :]\r\n    next_token_logits = tf.matmul(last_hidden_state, tf_lm_head)\r\n    next_token = tf.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)\r\n    log_probs = tf.math.reduce_max(tf.nn.log_softmax(next_token_logits))\r\n    return {\"decoded_ids\": next_token, \"log_probs\": log_probs}\r\n\r\n\r\ntensors = []\r\nfor example in range(100):\r\n    values = [[rng.randint(0, 30000) for _ in range(8)]]\r\n    tensors.append(np.array(values, dtype=np.int32))\r\n\r\n\r\ndef representative_dataset_gen():\r\n    for ts in tensors:\r\n        yield [ts]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([serving_func.get_concrete_function(\r\n    tf.TensorSpec(shape=(1, None), dtype=tf.int32, name='input_ids'))])\r\n# converter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS,\r\n                                       tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\ntflite_quant_model = converter.convert()\r\nwith open(\"/tmp/model.tflite\", 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n\r\n# invoke the model\r\nimport tensorflow as tf\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"/tmp/model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nimport numpy as np\r\n# Test the TensorFlow Lite model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.int32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Here are some similar PRs. https://github.com/tensorflow/tensorflow/issues/31053", "Is it possible to share your converted TFLite file to us?", "> Is it possible to share your converted TFLite file to us?\r\n\r\nHi @abattery, here is the code snippet to generate the tflite model. \r\n```\r\nimport random\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom transformers import *\r\n\r\nrng = random.Random()\r\n\r\ngpt2_model = TFGPT2Model.from_pretrained('distilgpt2')\r\n\r\n\r\ndef get_tf_lm_head_tensor():\r\n    gpt2_lm_pt_model = GPT2LMHeadModel.from_pretrained('distilgpt2')\r\n    np_tensor = gpt2_lm_pt_model.lm_head.weight.detach().numpy()\r\n    np_tensor = np.transpose(np_tensor)\r\n    tf_lm_head_tensor = tf.convert_to_tensor(np_tensor)\r\n    return tf_lm_head_tensor\r\n\r\n\r\ntf_lm_head = get_tf_lm_head_tensor()\r\n\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(1, None), dtype=tf.int32, name=\"input_ids\")])\r\ndef serving_func(input_ids):\r\n    outputs = gpt2_model(input_ids, training=False)\r\n    last_hidden_state = outputs[0][0][-1:, :]\r\n    next_token_logits = tf.matmul(last_hidden_state, tf_lm_head)\r\n    next_token = tf.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)\r\n    log_probs = tf.math.reduce_max(tf.nn.log_softmax(next_token_logits))\r\n    return {\"decoded_ids\": next_token, \"log_probs\": log_probs}\r\n\r\n\r\ntensors = []\r\nfor example in range(100):\r\n    values = [[rng.randint(0, 30000) for _ in range(8)]]\r\n    tensors.append(np.array(values, dtype=np.int32))\r\n\r\n\r\ndef representative_dataset_gen():\r\n    for ts in tensors:\r\n        yield [ts]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([serving_func.get_concrete_function(\r\n    tf.TensorSpec(shape=(1, None), dtype=tf.int32, name='input_ids'))])\r\n# converter.experimental_new_converter = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS,\r\n                                       tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\ntflite_quant_model = converter.convert()\r\nwith open(\"/tmp/model.tflite\", 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n```\r\n\r\nTo install packages, I have used \r\n```\r\npip install tensorflow==2.4.1\r\npip install transformers==4.2.2\r\n```\r\ncolab. https://colab.research.google.com/drive/1lbhB7Zl5rHUXKRncE8ZH2J5izhknyQeb?usp=sharing", "FYI, I ran your colab with `!pip install tf-nightly transformers==4.2.2`. It worked without any errors.", "@gyin-ai,\r\nI was able to reproduce the error with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/dc8a0d5f02b4e0af412afa0a38f55097/46844.ipynb). \r\n\r\nHowever as mentioned by @abattery, the issue seems to be resolved with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/06ec1c74ae271756db28bb2a17e37128/46844-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "@abattery @amahendrakar thanks a lot for your help. After installing the new tf-nightly, it did work. However I came into the following problems when I tried to do the tf profiling. \r\n\r\n1. Use macOS to run the code snippet. I will get the following info. I don't see this message in CoLab. \r\n```\r\n2021-02-02 21:50:33.249191: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-02-02 21:50:35.650041: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1768] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\r\nFlex ops: FlexErf\r\nDetails:\r\n        tf.Erf {device = \"\"}\r\n```\r\n\r\n2. When I tried to run the model with tf profile, https://www.tensorflow.org/tfx/serving/tensorboard. It doesn't show the tensorflow stats as before but only Erf\r\n<img width=\"840\" alt=\"image\" src=\"https://user-images.githubusercontent.com/67664443/106704766-832f7e80-65a1-11eb-9ded-d0142f55ae32.png\">\r\n\r\nassume `/tmp/distilgpt2_tflite/1/model.tflite` is where the model is. \r\n```\r\nmkdir -p /Users/<user_name>/logs/inference_demo/plugins/profile/\r\ndocker run --name tftest -it --entrypoint /bin/bash --mount type=bind,source=/tmp/distilgpt2_tflite/,target=/models/gpt2/ --mount type=bind,source=/Users/<user_name>/logs/inference_demo/plugins/profile/,target=/Users/<user_name>/logs/inference_demo/plugins/profile/ -p 8501:8501 -p 8500:8500 tensorflow/serving:nightly\r\ntensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=my_model --model_base_path=/models/gpt2/1/ --prefer_tflite_model=true\r\n\r\n# outside the container\r\ncurl -d '{\"inputs\": [[3, 288, 831]]}' -X POST http://localhost:8501/v1/models/my_model:predict\r\n\r\n# then the tensorboard profile would be able to capture the information\r\n```\r\n\r\nA normal tf profile without any quantization would be like this.\r\n<img width=\"1026\" alt=\"image\" src=\"https://user-images.githubusercontent.com/67664443/106706031-da365300-65a3-11eb-9c80-2e49ae9f6e0a.png\">\r\n\r\n\r\n", "As far as I know, the TensorFlow Lite does not support the performance profiling integration with TensorBoard yet. The only Erf operator, covered by the TF kernels not TFLite kernels, is shown. Looks like it is an intended behavior. CCing @yyoon ", "Unassigning model-optimization team and assigning @yyoon temporarily.\r\n\r\n@gyin-ai, can you close this if @abattery's answer is sufficient?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46844\">No</a>\n", "@teijeong @abattery tflite seems not to convert `log_softmax` correctly. It will output a constant value for any input. `log_probs = tf.math.reduce_max(tf.nn.log_softmax(next_token_logits))`\r\n\r\nUsing `2.5.0rc2` for tensorflow and `4.2.2` for transformers", "@gyin-ai  Could you file a separate issue to keep each issue focused?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46844\">No</a>\n"]}, {"number": 46843, "title": "Distributed training using Parameterstrategy ", "body": "Hi All,\r\n\r\nI am new to Tensorflow and trying to implement distributed Tensorflow using ParameterStrategy based on the [Documentation][1]. So far I have the code below\r\n\r\n  [1]: https://www.tensorflow.org/tutorials/distribute/parameter_server_training\r\n```\r\nimport multiprocessing\r\nimport os\r\nimport portpicker\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers.experimental.preprocessing as kpl\r\nimport tensorflow_hub as hub\r\nimport numpy as np\r\nprint(tf.__version__)\r\n\r\ndef create_in_process_cluster(num_workers, num_ps):\r\n  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\r\n  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\r\n  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\r\n\r\n  cluster_dict = {}\r\n  cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\r\n  if num_ps > 0:\r\n    cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\r\n\r\n  cluster_spec = tf.train.ClusterSpec(cluster_dict)\r\n\r\n  # Workers need some inter_ops threads to work properly.\r\n  worker_config = tf.compat.v1.ConfigProto()\r\n  if multiprocessing.cpu_count() < num_workers + 1:\r\n    worker_config.inter_op_parallelism_threads = num_workers + 1\r\n\r\n  for i in range(num_workers):\r\n    tf.distribute.Server(\r\n        cluster_spec, job_name=\"worker\", task_index=i, config=worker_config,\r\n        protocol=\"grpc\")\r\n\r\n  for i in range(num_ps):\r\n    tf.distribute.Server(\r\n        cluster_spec, job_name=\"ps\", task_index=i, protocol=\"grpc\")\r\n\r\n  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\r\n      cluster_spec, task_id=0, task_type=\"worker\",rpc_layer=\"grpc\")\r\n  return cluster_resolver\r\n\r\n# Set the environment variable to allow reporting worker and ps failure to the\r\n# coordinator. This is a workaround and won't be necessary in the future.\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n\r\nNUM_WORKERS = 3\r\nNUM_PS = 2\r\ncluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\r\n\r\nvariable_partitioner = (\r\n    tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n        num_shards=NUM_PS))\r\n\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n\r\nword = \"Elephant\"\r\nsentence = \"I am a sentence for which I would like to get its embedding.\"\r\nparagraph = (\r\n    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\r\n    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\r\n    \"the more 'diluted' the embedding will be.\")\r\nmessages = [word, sentence, paragraph]\r\nlabels=[\"1\",\"2\",\"3\"]\r\nreviews = [[1,0,0],[0,1,0],[0,0,1]]\r\n\r\nencoder=hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\r\n\r\nX_train=encoder(messages)\r\n\r\nwith strategy.scope():\r\n  feature_lookup_layer = kpl.StringLookup(vocabulary=messages)\r\n\r\n  label_lookup_layer = kpl.StringLookup(vocabulary=labels,\r\n                                        num_oov_indices=0,\r\n                                        mask_token=None)\r\n\r\n  raw_feature_input = keras.layers.Input(\r\n      shape=(1,), dtype=tf.string, name=\"feature\")\r\n  feature_id_input = feature_lookup_layer(raw_feature_input)\r\n  feature_preprocess_stage = keras.Model(\r\n      {\"features\": raw_feature_input}, feature_id_input)\r\n\r\n  raw_label_input = keras.layers.Input(\r\n      shape=(3,), dtype=tf.string, name=\"label\")\r\n  label_id_input = label_lookup_layer(raw_label_input)\r\n  label_preprocess_stage = keras.Model({\"label\": raw_label_input}, label_id_input)\r\n\r\nexamples = {\"features\": [word,sentence,paragraph], \"label\": [[\"1\",\"0\",\"0\"],[\"0\",\"1\",\"0\"],[\"0\",\"0\",\"1\"]]}\r\nprint(examples)\r\ndef dataset_fn(_):\r\n  raw_dataset = tf.data.Dataset.from_tensor_slices(examples)\r\n\r\n  train_dataset = raw_dataset.map(\r\n      lambda x: (\r\n          {\"features\": feature_preprocess_stage(x[\"features\"])},\r\n          label_preprocess_stage(x[\"label\"])\r\n      )).shuffle(200).batch(32).repeat()\r\n  return train_dataset\r\n\r\n\r\n# These variables created under the `strategy.scope` will be placed on parameter\r\n# servers in a round-robin fashion.\r\nwith strategy.scope():\r\n  # Create the model. The input needs to be compatible with KPLs.\r\n  model_input = keras.layers.Input(\r\n      shape=(3,), dtype=tf.int64, name=\"model_input\")\r\n\r\n  emb_layer = keras.layers.Embedding(\r\n      input_dim=len(feature_lookup_layer.get_vocabulary()), output_dim=20)\r\n  emb_output = tf.reduce_mean(emb_layer(model_input), axis=1)\r\n  dense_output = keras.layers.Dense(units=1, activation=\"sigmoid\")(emb_output)\r\n  model = keras.Model({\"features\": model_input}, dense_output)\r\n\r\n  optimizer = keras.optimizers.RMSprop(learning_rate=0.1)\r\n  accuracy = keras.metrics.Accuracy()\r\n\r\n\r\n@tf.function\r\ndef step_fn(iterator):\r\n\r\n  def replica_fn(batch_data, labels):\r\n    with tf.GradientTape() as tape:\r\n      pred = model(batch_data, training=True)\r\n      per_example_loss = keras.losses.CategoricalCrossentropy(\r\n              reduction=tf.keras.losses.Reduction.NONE)(labels, pred)\r\n      loss = tf.nn.compute_average_loss(per_example_loss)\r\n      gradients = tape.gradient(loss, model.trainable_variables)\r\n\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\r\n    accuracy.update_state(labels, actual_pred)\r\n    return loss\r\n\r\n  batch_data, labels = next(iterator)\r\n  losses = strategy.run(replica_fn, args=(batch_data, labels))\r\n  return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\r\n\r\n\r\ncoordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)\r\n@tf.function\r\ndef per_worker_dataset_fn():\r\n  return strategy.distribute_datasets_from_function(dataset_fn)\r\n\r\n\r\nper_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\r\nper_worker_iterator = iter(per_worker_dataset)\r\n\r\nnum_epoches = 2\r\nsteps_per_epoch = 1\r\nfor i in range(num_epoches):\r\n  accuracy.reset_states()\r\n  for _ in range(steps_per_epoch):\r\n    coordinator.schedule(step_fn, args=(per_worker_iterator,))\r\n  # Wait at epoch boundaries.\r\n  coordinator.join()\r\n  print (\"Finished epoch %d, accuracy is %f.\" % (i, accuracy.result().numpy()))\r\n```\r\n\r\nIn this example, I'm trying to convert the document example from binary classification to Categorical classification. But I'm getting the following error.\r\n\r\n> ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: (None,)\r\n \r\nI have Tensotflow `2.4.1` version  ", "comments": ["@cmuniyappa \r\nCan you please refer to these links and let us know if it helps, [link](https://stackoverflow.com/questions/55389913/valueerror-input-0-of-layer-dense-is-incompatible-with-the-layer-expected-mi), [link1](https://datascience.stackexchange.com/questions/54491/error-input-0-is-incompatible-with-layer-flatten-1-expected-min-ndim-3-found)", "@Saduf2019 Thanks it helped!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46843\">No</a>\n"]}, {"number": 46842, "title": "Use vectorized load/store in GatherOpKernel", "body": "- Vectorized load/store achieves higher memory throughput.\r\n- This commit chooses the optimal vector size that is compatible with the data shape and pointer alignment.\r\n- Note that the `ldg` function would require more changes to support the vectorized types, but it is not actually needed: the use of `const __restrict__` means the kernel still generates `LDG` instructions without it.\r\n- Some quick benchmarking showed the following speedups with this commit when gathering vectors of length 256 on an NVIDIA TITAN V GPU:\r\n    float64: 1.02x\r\n    float32: 1.07x\r\n    float16: 1.35x\r\n    bool:    1.67x\r\n\r\ncc @nluehr ", "comments": ["I've just pushed a couple of fixes for issues I noticed since submitting, and also a small refactor to make it cleaner. The commit messages have the details.", "Is something blocking this from being merged?", "> Is something blocking this from being merged?\r\n\r\nSuper annoying reason: the \"Crask OK\" annotations you added are not on the right lines.  For now I'll manually land the PR after changing the CHECKs to DCHECKs, I hope that's OK.", "Sounds good, thanks.", "This has been rolled back as it uses C++17 features. TF only supports C+14 at the moment.", "What C++17 features? It successfully built with C++14 locally and in the github CI.", "Oh I think I found it: clang complains about `template <int vec_size> typename Functor` instead of `template <int vec_size> class Functor`. (gcc does not complain).\r\nThis is a trivial fix. What's the best way to make it?", "@benbarsdell I rolled this forward with a fix."]}, {"number": 46841, "title": "[TFLM] CEVA-DSP platform - Update makefiles for CEVA-ToolBox 18.05", "body": "Dealt with the issues regarding the makesfiles from PR: https://github.com/tensorflow/tensorflow/pull/46500\r\nAnd updated to latest CEVA-Toolbox, moving from clang 7.0.1 to clang 9.0.1 which solved some building issues for us.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46839, "title": "Update tf.keras.layers.convolutional.py with extended docstring information.", "body": "Update tf.keras.layers.convolutional.py docstrings with better information about kernel/bias initializers and consistency throughout each of the different operations. ", "comments": []}, {"number": 46838, "title": "Update nn_ops.py", "body": "Fix [#46834](https://github.com/tensorflow/tensorflow/issues/46834) and [#46994](https://github.com/tensorflow/tensorflow/issues/46994)\r\nOn having kernel size as 0 , the code crashes on execution. This pull request will raise a **FloatingPointError** upon getting a value of 0 for the `ksize` argument in `nn_ops`.", "comments": ["Probably better to check this in the kernel itself, since that catches more uses.", "I mean checking in the C++ tensorflow::OpKernel itself rather than the\nPython binding, returning a bad status instead of throwing an FPE. That\nwill cover tf.raw_ops.* usage from Python, and C++ usage, Java usage, etc.\nand make sure people's SavedModels throw a sensible error.\n\nOn Tue, Feb 2, 2021 at 11:55 AM Hiran Sarkar <notifications@github.com>\nwrote:\n\n> @allenlavoie <https://github.com/allenlavoie> The added code itself\n> checks the kernel and would return a FloatingPointError on getting its\n> value as 0, if this is what you meant. This would add the functionality to\n> all the nn functions/ops which gets the ksize argument as 0 and is not\n> restricted to a particular operation/function.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/46838#issuecomment-771933779>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AA4O4UIOWQUWUIUQ4FRLWW3S5BKCFANCNFSM4W5OEE6Q>\n> .\n>\n", "> I mean checking in the C++ tensorflow::OpKernel itself rather than the Python binding, returning a bad status instead of throwing an FPE. That will cover tf.raw_ops.* usage from Python, and C++ usage, Java usage, etc. and make sure people's SavedModels throw a sensible error.\r\n> [\u2026](#)\r\n\r\nThis makes sense. @around-star , for example, here's the OpFunction for XLA, where you should implement the fix. :)\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/compiler/tf2xla/kernels/pooling_ops.cc#L77-L99\r\n\r\nSince different device has different kernels, there are multiple other kernel files too for the same Op, where you should check."]}, {"number": 46837, "title": "FATAL ERROR: tensorflow/core/framework/types.pb.h: No such file or directory", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. Standard C++ program used to previously compile and run with TF 1.1 (CUDA - 9.2 and CUDNN 7.2 )\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: CUDA - 10.2/ CUDNN - 7.6\r\n- GPU model and memory: Quadro P5200, 32 Gb RAM\r\n\r\n\r\n**Describe the current behavior**\r\nTF 2.3.0 successfully installs with bazel. However, while compiling a C++ program using gcc, I get the following error\r\n_**/usr/local/include/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory**_\r\nI tried to locate the types.pb.h file and found the file in _/home/%username%/.cache/bazel/_bazel_%username%/3fa00a5b455754a3e6fd353fefb67596/execroot/org_tensorflow/bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/core/framework/types.pb.h_\r\nCopying or including this folder/file did not help as additional dependencies were missing. \r\n\r\n**Standalone code to reproduce the issue**\r\nClone tensorflow from github and checkout v2.3.0. Run ./configure with following options (refer attachment)\r\nBazel build with the following options:\r\n_bazel build -c opt \\\r\n            --copt=-mavx \\\r\n            --copt=-mavx2 \\\r\n            --copt=-mfma \\\r\n            --copt=-mfpmath=both \\\r\n            --copt=-msse4.2 \\\r\n            --config=cuda //tensorflow:libtensorflow_cc.so_\r\n[TF_Config.txt](https://github.com/tensorflow/tensorflow/files/5906638/TF_Config.txt)\r\n\r\n", "comments": ["@shashank2710,\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "@amahendrakar, \r\n**Issue still persists in v2.4.1.** \r\n\r\nError 1:\r\n**_/usr/local/include/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory_**\r\n\r\nResolution:\r\nResolved by adding  /usr/local/include/tensorflow/bazel-bin to the c++ include path. \r\n/usr/local/include/tensorflow/bazel-bin has a symlink to /home/%username%/.cache/bazel/_bazel_%username%/3fa00a5b455754a3e6fd353fefb67596/execroot/org_tensorflow/bazel-out/k8-opt/bin\r\n\r\nError 2:\r\nAfter resolving Error 1 I encounter the following error. This is similar to #22007\r\n**_include/tensorflow/core/lib/core/stringpiece.h:34:38: fatal error: absl/strings/string_view.h: No such file or directory_**\r\n\r\nResolution \r\n$ git clone https://github.com/abseil/abseil-cpp.git\r\n$ ln -s abseil-cpp/absl ./absl\r\n\r\nError 3\r\nAfter resolving Error 2 I encounter the following error.\r\n**_/usr/local/include/tensorflow/bazel-bin/tensorflow/core/framework/types.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory_**\r\nResolution: \r\nI had to include the following in my c++ program\r\n/home/%username%/.cache/bazel/_bazel_%username%/3fa00a5b455754a3e6fd353fefb67596/external/com_google_protobuf/src\r\n\r\nNote: \r\nI have installed Bazel 3.1.0 from source, not the standard apt-repository\r\n\r\n", "@shashank2710,\r\nTensorFlow v2.4 is built and tested against **CUDA 11.0** and **cuDNN 8**. Could you please update the CUDA and cuDNN packages on your machine and check if it helps.\r\n\r\nAlso, please run the `bazel clean --expunge` command before building and let us know if you are facing the same error. Thanks!", "@amahendrakar \r\nI re-installed TF v2.4.0 with **CUDA 11.0 and cuDNN 8.0** as requested. All 3 errors mentioned above still exist. \r\n\r\nAfter resolving the errors manually, I tried to compile the project using g++ and received the following errors\r\n\r\n> TensorFlowUtils.cpp:(.text+0x753): undefined reference to `tensorflow::Env::GetFileSize(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, unsigned long long*)'\r\nmakefile:50: recipe for target failed\r\nTensorFlowUtils.cpp:(.text+0x7b4): undefined reference to `tensorflow::Env::NewRandomAccessFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<tensorflow::RandomAccessFile, std::default_delete<tensorflow::RandomAccessFile> >*)'\r\n./src/TensorFlowUtils.o: In function `TensorFlowUtils::getTopLabels(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, int, tensorflow::Tensor*, tensorflow::Tensor*)':\r\nTensorFlowUtils.cpp:(.text+0x3367): undefined reference to `tensorflow::Scope::WithOpNameImpl(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\n./src/TensorFlowUtils.o: In function `TensorFlowUtils::loadGraph(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<tensorflow::Session, std::default_delete<tensorflow::Session> >*)':\r\nTensorFlowUtils.cpp:(.text+0x3d6f): undefined reference to `tensorflow::ReadBinaryProto(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, google::protobuf::MessageLite*)'\r\n./src/TensorFlowUtils.o: In function `TensorFlowUtils::readTensorFromImageFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, int, float, float, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)':\r\nTensorFlowUtils.cpp:(.text+0x54fd): undefined reference to `tensorflow::Scope::WithOpNameImpl(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\n./src/TensorFlowUtils.o: In function `TensorFlowUtils::convertOpenCvImageToTfImageFileIo(cv::Mat, int, int, float, float)':\r\nTensorFlowUtils.cpp:(.text+0x67fb): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\n./src/TensorFlowUtils.o: In function `tensorflow::Scope tensorflow::Scope::WithOpName<char const*>(char const*) const':\r\nTensorFlowUtils.cpp:(.text._ZNK10tensorflow5Scope10WithOpNameIJPKcEEES0_DpT_[_ZNK10tensorflow5Scope10WithOpNameIJPKcEEES0_DpT_]+0x57): undefined reference to `tensorflow::Scope::WithOpNameImpl(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\n./src/TensorFlowUtils.o: In function `tensorflow::Status tensorflow::errors::NotFound<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*)':\r\nTensorFlowUtils.cpp:(.text._ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8NotFoundIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_EEENS_6StatusEDpT_]+0x97): undefined reference to `tensorflow::strings::StrCat[abi:cxx11](tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&, tensorflow::strings::AlphaNum const&)'\r\n./src/TensorFlowUtils.o: In function `tensorflow::Status tensorflow::errors::DataLoss<char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long long, char const*, unsigned long>(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, char const*, unsigned long long, char const*, unsigned long)':\r\nTensorFlowUtils.cpp:(.text._ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_yS3_mEEENS_6StatusEDpT_[_ZN10tensorflow6errors8DataLossIJPKcNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEES3_yS3_mEEENS_6StatusEDpT_]+0x196): undefined reference to `tensorflow::strings::internal::CatPieces[abi:cxx11](std::initializer_list<absl::lts_2020_02_25::string_view>)'\r\n./src/DetectChars.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)':\r\nDetectChars.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringImmEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringImmEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x51): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n./src/DetectChars.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\nDetectChars.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x50): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n./src/DetectChars.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)':\r\nDetectChars.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIxxEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIxxEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x51): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n./src/Classification.o: In function `Classification::init()':\r\nClassification.cpp:(.text+0x461): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\nClassification.cpp:(.text+0x517): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\n./src/Classification.o: In function `Classification::classify(cv::Mat const&)':\r\nClassification.cpp:(.text+0x11ca): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\n./src/SubClass.o: In function `SubClass::init()':\r\nSubClass.cpp:(.text+0xa81): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\nSubClass.cpp:(.text+0xaa3): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\n./src/SubClass.o:SubClass.cpp:(.text+0xb5b): more undefined references to `tensorflow::Status::empty_string[abi:cxx11]()' follow\r\n\r\nI am currently referring #43307 and #40538\r\nI compiled with \r\n\r\n> bazel build  --config=v2 --config=opt --copt=-O3 --copt=-m64 --copt=-march=native --verbose_failures //tensorflow:install_headers //tensorflow:tensorflow //tensorflow:tensorflow_cc //tensorflow:tensorflow_framework //tensorflow/tools/lib_package:libtensorflow\r\n\r\nThis was able to resolve 12 of 18 g++ tensorflow linker errors. \r\nI still have the have the following errors during c++ compilation. \r\n> Classification.cpp:(.text+0x461): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\nClassification.cpp:(.text+0x517): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\n./src/Classification.o: In function `Classification::classify(cv::Mat const&)':\r\nClassification.cpp:(.text+0x11ca): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\n./src/SubClass.o: In function `SubClass::init()':\r\nSubClass.cpp:(.text+0xa81): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\nSubClass.cpp:(.text+0xaa3): undefined reference to `tensorflow::Status::empty_string[abi:cxx11]()'\r\n./src/SubClass.o:SubClass.cpp:(.text+0xb5b): more undefined references to `tensorflow::Status::empty_string[abi:cxx11]()' follow\r\ncollect2: error: ld returned 1 exit status\r\n\r\n", "@amahendrakar \r\nWeekend update. Referred #43307 and #40538. \r\nI compiled TF v2.4 with CUDA 11 and CuDNN 8.0 with the following bazel command options\r\n\r\n>bazel build -c opt --copt=-O3 --copt=-m64 --copt=-march=native --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --verbose_failures //tensorflow:install_headers //tensorflow:tensorflow //tensorflow:tensorflow_cc //tensorflow:tensorflow_framework //tensorflow/tools/lib_package:libtensorflow\r\n\r\nThis resolved Error 2 and all g++ compiler/linker errors. **Errors 1 and 2 still exist.** \r\n ", "You should not need to manually download other repos and symlink.\r\n\r\nMost likely your bazel cache is corrupted.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@amahendrakar , \r\nA fresh ubuntu install and updated bazel flags helped resolve this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46837\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46837\">No</a>\n", "May I ask what do you means about ubuntu fresh install and update bazel?\r\n\r\nCan you break them into the detail, thanks "]}, {"number": 46836, "title": "Lower ResizeNearestNeighbor when half_pixel_centers=true", "body": "Lower ResizeNearestNeighbor when `half_pixel_centers=true`. This PR makes the lowering pattern match the use in TF python library as combination of `align_corners=false` and `half_pixel_centers=true` is the only code path in `tf.image.resize`. Test thoroughly in https://colab.research.google.com/drive/1YCaLPOaagFzuDRg4M67bTUz1boatCvhf?usp=sharing.\r\n\r\nAlso remove some unnecessary reshape\r\n- Before AddV2: x_indices can be broadcasted\r\n- Before and after GatherV2: indices can be 2D without back and forth reshape", "comments": ["@WindQAQ  Can you please resolve conflicts? Thanks!", "@WindQAQ Can you please resolve conflicts? Thanks!", "@WindQAQ Any update on this PR? Please. Thanks!", "@WindQAQ  Can you please check @tenjua's comments and keep us posted ? Thanks!", "@WindQAQ  Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 46835, "title": "Bug or me? Using a generator with a dataset, impossible to correctly specify shapes", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10/Ubuntu 18.04/Macos\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below):v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: 1080TI 8gigs\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCreating a functional model with multiple outputs seems to work fine if all of the data is in memory or being sourced from files, but using a generator it seems to be impossible to properly define the shapes.  I've tried both the deprecated `output_*` options and the current `output_signature` options to no avail.  It is entirely possible it is just me, but if so, it is possible the the documentation might be a tad incorrect. :)\r\n\r\n**Describe the expected behavior**\r\nI would expect that the shape of the data can be properly defined and used for fitting!\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThis code will compile fine but fail to fit with an error concerning the data shape.  I've redefined the data to be any of a variety of shapes... As soon as I switch to multiple outputs, this fails.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers, models\r\n\r\ndef generate_sample():\r\n    x = list(\"123456789\")\r\n    y = list(\"2345\")\r\n    while 1:\r\n        yield np.array(x).astype(np.float32),[np.array(y).astype(np.float32),np.array(y).astype(np.float32)]\r\n\r\ndataset = tf.data.Dataset.from_generator(generate_sample,\r\n            output_signature=(\r\n                 tf.TensorSpec(shape=(9,), dtype=tf.float32),\r\n                 tf.TensorSpec(shape=(2,4), dtype=tf.float32)\r\n\r\n            ))\r\n\r\ndataset = dataset.batch(batch_size=32)\r\n\r\ninputs = keras.Input(shape=(next(generate_sample())[0].shape))\r\nx = layers.Dense(512, activation = \"relu\")(inputs)\r\nx_outputs = layers.Dense(4, activation=\"relu\", name=\"output\")(x)\r\ny_outputs = layers.Dense(4, activation=\"relu\", name=\"output2\")(x)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=[x_outputs,y_outputs])\r\nmodel.compile(loss=\"mse\", optimizer = \"adam\", metrics=['accuracy'])\r\nhistory = model.fit(dataset, epochs=1, steps_per_epoch=10, validation_data=dataset, validation_steps=5)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-102-3dad39c1e2c1> in <module>\r\n----> 1 history = model.fit(dataset, epochs=1, steps_per_epoch=10, validation_data=dataset, validation_steps=5)\r\n\r\n~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1098                 _r=1):\r\n   1099               callbacks.on_train_batch_begin(step)\r\n-> 1100               tmp_logs = self.train_function(iterator)\r\n   1101               if data_handler.should_sync:\r\n   1102                 context.async_wait()\r\n\r\n~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    886         # Lifting succeeded, so variables are initialized and we can run the\r\n    887         # stateless function.\r\n--> 888         return self._stateless_fn(*args, **kwds)\r\n    889     else:\r\n    890       _, _, _, filtered_flat_args = \\\r\n\r\n~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2940       (graph_function,\r\n   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n-> 2942     return graph_function._call_flat(\r\n   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   2944 \r\n\r\n~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1916         and executing_eagerly):\r\n   1917       # No tape is watching; skip to running the function.\r\n-> 1918       return self._build_call_outputs(self._inference_function.call(\r\n   1919           ctx, args, cancellation_manager=cancellation_manager))\r\n   1920     forward_backward = self._select_forward_and_backward_functions(\r\n\r\n~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    553       with _InterpolateFunctionError(self):\r\n    554         if cancellation_manager is None:\r\n--> 555           outputs = execute.execute(\r\n    556               str(self.signature.name),\r\n    557               num_outputs=self._num_outputs,\r\n\r\n~\\anaconda3\\envs\\SEC595\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError:  Incompatible shapes: [32,2,4] vs. [32,4]\r\n\t [[node mean_squared_error/SquaredDifference (defined at <ipython-input-102-3dad39c1e2c1>:1) ]] [Op:__inference_train_function_12605]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "comments": ["I just tried this yielding a nested tuple and it may have just been me.  It could be that the docs could benefit from an example along these lines because it is not intuitive how to do this properly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46835\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46835\">No</a>\n"]}, {"number": 46834, "title": "floating point exception in tf.nn.avg_pool3d and tf.nn.max_pool3dwhen ksize=0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\nfloating point exception in `tf.nn.avg_pool3d` and `tf.nn.max_pool3d` when `ksize`=0\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nexpect no crash\r\n**Standalone code to reproduce the issue**\r\n\r\nThe code crashes in nightly version too. Check out the [gist](https://colab.research.google.com/drive/10O4Qn2S4uW-40jLDHvlUBN_vlMsQkUo-?usp=sharing)\r\n~~~python\r\ntf.nn.avg_pool3d(input=tf.ones((1,1,1,1,1)), strides=1, ksize=0, padding='VALID')\r\n~~~\r\n\r\n~~~python\r\ntf.nn.max_pool3d(input=tf.ones((1,1,1,1,1)), strides=1, ksize=0, padding='VALID')\r\n~~~\r\nOutput:\r\n~~~python\r\nFloating point exception (core dumped)\r\n~~~", "comments": ["@DNXie @ravikyram I have created a pull request to solve this issue , which will help raise `FloatingPoitnError`  when `ksize = 0`", "I have tried in colab with TF version 2.4, nightly version(`2.5.0-dev20210201`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/96c0b6fc38409baaca216bdb942a228a/untitled650.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46834\">No</a>\n"]}, {"number": 46832, "title": "Error when converting a resnetv1_50 based model trained with Tensorflow.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): Tensorflow 1.15\r\n\r\n### 2. Code\r\n\r\n```\r\nfrom tensorflow import lite\r\n## Training model code is omitted here ##\r\n\r\nsaver.save(sess, path_to_save, global_step=it)\r\n        \r\n# Converting a GraphDef from session.\r\nconverter = lite.TFLiteConverter.from_session(sess, list(batch.values()), posenet.output_tensors)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\nopen(\"./converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n\r\n#### Option B: Paste your code here or provide a link to a custom end-to-end colab\r\n\r\n```\r\n[Link to my notebook in drive](https://drive.google.com/file/d/1xnmr69QOCmUByZbDQNTBLBp0RQbbKYqb/view?usp=sharing)\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\nThe conversion didn't work.\r\n\r\n### 5. (optional) Any other info / logs\r\n**These are my inputs:**\r\n[<tf.Tensor 'fifo_queue_Dequeue:0' shape=(1, 200, 200, 3) dtype=float32>,\r\n<tf.Tensor 'fifo_queue_Dequeue:1' shape=(1, 26, 26, 21) dtype=float32>,\r\n<tf.Tensor 'fifo_queue_Dequeue:2' shape=(1, 26, 26, 21) dtype=float32>,\r\n<tf.Tensor 'fifo_queue_Dequeue:3' shape=(1, 26, 26, 42) dtype=float32>,\r\n<tf.Tensor 'fifo_queue_Dequeue:4' shape=(1, 26, 26, 42) dtype=float32>]\r\n\r\n**These are my outputs:**\r\n[<tf.Tensor 'pose/part_pred/block4/BiasAdd:0' shape=(1, 26, 26, 21) dtype=float32>,\r\n<tf.Tensor 'pose/locref_pred/block4/BiasAdd:0' shape=(1, 26, 26, 42) dtype=float32>]\r\n\r\n\r\n```\r\nI'm showing the error output below, any help will be useful:\r\n`---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-10-458b03f29263> in <module>\r\n     30         converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n     31         converter.allow_custom_ops=True\r\n---> 32         tflite_model = converter.convert()\r\n     33 #         open(\"./converted_model.tflite\", \"wb\").write(tflite_model)\r\n     34 #         with open('model.tflite', 'wb') as f:\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\lite.py in convert(self)\r\n    981           input_tensors=self._input_tensors,\r\n    982           output_tensors=self._output_tensors,\r\n--> 983           **converter_kwargs)\r\n    984     else:\r\n    985       result = _toco_convert_graph_def(\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    447       input_data.SerializeToString(),\r\n    448       debug_info_str=debug_info_str,\r\n--> 449       enable_mlir_converter=enable_mlir_converter)\r\n    450   return data\r\n    451 \r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\nEl sistema no puede encontrar la ruta especificada.\r\n2021-02-01 17:29:45.263535: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n2021-02-01 17:29:45.264019: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nc:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nc:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nc:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nc:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nc:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nc:\\users\\pipita\\anaconda3\\envs\\aws_train\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2021-02-01 17:29:48.157783: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: FIFOQueueV2\r\n2021-02-01 17:29:48.158185: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2021-02-01 17:29:48.290435: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: QueueDequeueV2\r\n2021-02-01 17:29:48.322668: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 685 operators, 1055 arrays (0 quantized)\r\n2021-02-01 17:29:48.343643: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 683 operators, 1054 arrays (0 quantized)\r\n2021-02-01 17:29:48.367174: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 683 operators, 1054 arrays (0 quantized)\r\n2021-02-01 17:29:48.582688: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 92 operators, 231 arrays (0 quantized)\r\n2021-02-01 17:29:48.586252: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 92 operators, 231 arrays (0 quantized)\r\n2021-02-01 17:29:48.588560: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 92 operators, 231 arrays (0 quantized)\r\n2021-02-01 17:29:48.593331: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 10880000 bytes, theoretical optimal value: 7680000 bytes.\r\n2021-02-01 17:29:48.594149: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 10608315996 ops, equivalently 5304157998 MACs\r\n2021-02-01 17:29:48.594470: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 24644870`\r\n\r\n```\r\n\r\n\r\n\r\n", "comments": ["Could you try the conversion with the recent TensorFlow version, for example, TF 2.4.1 or tf-nightly? The TOCO converter, which is being used by TF 1.5, now is deprecated.", "You can export the saved model from TF 1.5 and you can try the conversion with the exported saved model on the latest TF version .", "@lupitia1 \r\nThere is no support for tf 1.x, could you please try with 2.x and let us know if you face any issues.", "Thanks, I will try this and also try to migrate my model training to Tensorflow 2.0", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I gave up with resnet and I'm attempting to build the model using finetuning to MobileNetV2 in TF2.0.\r\nHopefully I will be able to use it on the edgetpu.\r\nThank you all!"]}, {"number": 46831, "title": "TF 2.4.1: Mirrored Strategy not providing performance boost RTX 3090 ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0/8.0.4.30\r\n- GPU model and memory: 2x RTX 3090/24GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI have a dataset created from an image directory (tf.keras.preprocessing.image_dataset_from_directory()). I then use a mirrored strategy (tf.distribute.MirroredStrategy) to create and compile my model then I try training the model using model.fit(). The call to model.fit causes an error message to inform me that the dataset is unshardable. I am therefore not receiving a performance increase from the second RTX 3090 GPU that I have just installed in the machine. \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen I used this approach using Tensorflow 2.3.1 and 2 x RTX 2060s, I did not generate any error messages about sharding the dataset, the log printed that it was running with a mirrored strategy, and listed the GPUs being used, and there was a significant performance boost\r\n\r\n**Standalone code snippet to reproduce the issue**\r\n\r\n```\r\n\r\ndef get_compiled_model(num_classes, dim_x, dim_y, dim_z):\r\n\r\n        data_augmentation = keras.Sequential(\r\n          [\r\n                layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(dim_x, dim_y, dim_z)),\r\n                layers.experimental.preprocessing.RandomRotation(0.1),\r\n                layers.experimental.preprocessing.RandomZoom(0.1),\r\n          ]\r\n        )\r\n\r\n\r\n        model = Sequential([\r\n          data_augmentation,\r\n          layers.experimental.preprocessing.Rescaling(1./255),\r\n          layers.Conv2D(2, 3, padding='same', activation='relu'),\r\n          layers.MaxPooling2D(),\r\n          #layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n          #layers.MaxPooling2D(),\r\n          #layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n          #layers.MaxPooling2D(),\r\n          layers.Dropout(0.2),\r\n          layers.Flatten(),\r\n          layers.Dense(6, activation='relu'),\r\n          layers.Dropout(0.2),\r\n          layers.Dense(num_classes)\r\n        ])\r\n        model.compile(\r\n                #optimizer=tf.keras.optimizers.Adam(),\r\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'],\r\n        )\r\n\r\n        return model\r\n\r\n\r\n## create data set \r\n\r\n        train_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n          data_dir,\r\n          validation_split=0.2,\r\n          subset=\"training\",\r\n          seed=123,\r\n          image_size=(50, 50),\r\n          batch_size=2048,\r\n          color_mode='grayscale'\r\n        )\r\n        val_ds = tf.keras.preprocessing.image_dataset_from_directory(\r\n          data_dir,\r\n          validation_split=0.2,\r\n          subset=\"validation\",\r\n          seed=123,\r\n          image_size=(50, 50),\r\n          batch_size=2048,\r\n          color_mode='grayscale'\r\n        )\r\n\r\n\r\n## create model with mirrored strategy \r\n        mirrored_strategy = tf.distribute.MirroredStrategy()\r\n\r\n        with mirrored_strategy.scope():\r\n\r\n                model = get_compiled_model(2, 50, 50, 1):\r\n\r\n\r\n## train model \r\n\r\n        history = model.fit(\r\n          train_ds,\r\n          validation_data=val_ds,\r\n          epochs=5000,\r\n          initial_epoch=0,\r\n        )\r\n\r\n```\r\n", "comments": ["Hi @LastHorizon, can you provide screenshots showing the training logs between 2.3 and 2.4? I would like to know what the difference in training time per epoch is. \r\n\r\nAs for the error message about the dataset being unshardable, there is no autosharding when you're doing single worker training. Autosharding is only necessary with multi worker training. So with `MirroredStrategy` the autosharding options register as no-op. However, in 2.4 the code is shared with `MultiWorkerMirroredStrategy` so you see the warnings regardless. If you want to get rid of the that warning, [you can set the autoshard policy to data](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy), but in the single worker case it should not actually impact performance.\r\n\r\n```\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\ntrain_ds = train_ds.with_options(options)\r\n```\r\n", "Hi @nikitamaia \r\n\r\nUnfortunately I can't provide screenshots because I am accessing the machine remotely and the 2060s have been taken out. (They are about to installed in a new machine.) However I can remember what the stats are that you are interested in: \r\n\r\n`MirroredStrategy` with two RTX 2060s - Epoch takes **85s**\r\n`MirroredStrategy` with two RTX 3090s - Epoch takes **90s**\r\n\r\nAs you can see, the two 2060 graphics cards are out performing the much more powerful 3090 cards! ", "Can you provide fully reproducible code? It's difficult to debug much further without reproducible code or logs.\r\n\r\nAlso, did setting the autoshardpolicy make the sharding error go away?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46830, "title": "TFLM: Add new Cortex M target for running on a FVP", "body": "* Adds new target for running on a fixed virtual platform based on Arm\r\n  Corstone-300 software.\r\n* Adds test script for running with FVP.\r\n* Adds new download scripts.\r\n* Adds new CI script.\r\n* Adds readme file.\r\n\r\n\r\nThis is fixing: https://github.com/tensorflow/tensorflow/issues/46829\r\n\r\nThe CI script takes about 5 minutes to run for me.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain To speed up the time for the CI script we could just test the CMSIS-NN kernels. Then it took 1 minute and 44 seconds ", "I was able to get the time down to 3 minutes and 14 seconds by changing the FVP cpulimit to 1. With only CMSIS-NN kernels it was 74 seconds.\r\nOne option is that we only run CMSIS-NN kernels for both this target as well as stm32f4. Bluepill already runs all unit tests and we are primarily interested in the CMSIS-NN kernels.\r\nI counted the stm32f4 CI script to take 149 seconds and with only CMSIS-NN kernels and debug build it took 105 seconds.\r\nThat means the total time would increase with only 30 seconds.\r\n", "> I was able to get the time down to 3 minutes and 14 seconds by changing the FVP cpulimit to 1. With only CMSIS-NN kernels it was 74 seconds.\r\n> One option is that we only run CMSIS-NN kernels for both this target as well as stm32f4. Bluepill already runs all unit tests and we are primarily interested in the CMSIS-NN kernels.\r\n> I counted the stm32f4 CI script to take 149 seconds and with only CMSIS-NN kernels and debug build it took 105 seconds.\r\n> That means the total time would increase with only 30 seconds.\r\n\r\nGood info, thanks!\r\n\r\nFiltering for just the CMSIS kernels might be more maintenance overhead than we want to take on. We do have the ability to run more than a single CI job (which will soon become easier), but let's decide what we want to do after this PR is merged.", "didn't see earlier that the bazel build is failing (as part of the TfLite Micro CI).\r\n\r\nYou'll have to add a new target for system_setup, similar to:\r\nhttps://github.com/tensorflow/tensorflow/blob/b57b89f7f547ed09902a2946fa2f48db21b2ee02/tensorflow/lite/micro/BUILD#L121-L130\r\n\r\nand add that as a dep to the tests. There's a chance that all the tests will need this additional dependency, which would be very unfortunate. I'll take a look and get back to you.", "I have created https://github.com/tensorflow/tensorflow/pull/47077 to add InitializeTarget as a separate change (and deal with the bazel dependency issue separate from adding the Cornstone300 target)"]}, {"number": 46829, "title": "Missing CI for OPTIMIZED_KERNEL_DIR=cmsis_nn with MVEI extension", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nThe CI script tensorflow/lite/micro/tools/ci_build/test_stm32f4.sh tests OPTIMIZED_KERNEL_DIR=cmsis_nn with DSP extension.\r\nHowever there is no equivalent test for MVEI extension, i.e. Cortex-M55.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46829\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46829\">No</a>\n"]}, {"number": 46828, "title": "To save subclassed Keras model from_config method is mandatory", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/guide/keras/save_and_serialize#custom_objects\r\n\r\n## Description of issue (what needs changing):\r\nThe guide states that to save/load custom layer or a subclassed model, the get_config and optionally from_config methods should be overwritten. However, in the case of subclassed model the definition of from_config is needed and not optional.\r\n\r\n### Clear description\r\nOverwriting from_config is needed because the method of the base class Model calls Functional.from_config, which looks for the key 'layers' in the config and therefore raises an exception in case of a subclassed model. \r\n\r\n### Submit a pull request?\r\nSubmitted PR to update the guide.\r\nhttps://github.com/keras-team/keras-io/pull/363\r\n", "comments": ["Closing this issue as per the discussion in the associated PR thread tagged above. Thanks!"]}, {"number": 46827, "title": "tensorflow 2.4 ParameterServerStrategy + Estimator will be stuck", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\ntf version: 2.4\r\ncluster spec:\r\nParameterServerStrategyV2 is now connecting to cluster with cluster_spec: ClusterSpec({'chief': ['tensorflow-tess-search-rk2-mlp-339212-chief-0.mlp.svc:2222'], 'evaluator': ['tensorflow-tess-search-rk2-mlp-339212-evaluator-0.mlp.svc:2222'], 'ps': ['tensorflow-tess-search-rk2-mlp-339212-ps-0.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-1.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-2.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-3.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-4.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-5.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-6.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-7.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-8.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-9.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-10.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-11.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-12.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-13.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-14.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-ps-15.mlp.svc:2222'], 'worker': ['tensorflow-tess-search-rk2-mlp-339212-worker-0.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-1.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-2.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-3.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-4.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-5.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-6.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-7.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-8.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-9.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-10.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-11.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-12.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-13.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-14.mlp.svc:2222', 'tensorflow-tess-search-rk2-mlp-339212-worker-15.mlp.svc:2222']})\r\n1\u3001tf.compat.v1.distribute.experimental.ParameterServerStrategy + Estimator :  Error reported to Coordinator: 'NoneType' object has no attribute 'extended'\r\nexample code:\r\nstrategy = tf.compat.v1.distribute.experimental.ParameterServerStrategy()\r\nmodel_config = tf.estimator.RunConfig(\r\n        train_distribute=strategy,\r\n        eval_distribute=None,\r\n        log_step_count_steps=config[\"parameters\"][\"log_steps\"],\r\n        save_summary_steps=config[\"parameters\"][\"save_summary_steps\"],\r\n        save_checkpoints_steps=config[\"parameters\"][\"save_checkpoints_steps\"],\r\n        save_checkpoints_secs=None,\r\n        keep_checkpoint_max=config[\"parameters\"][\"keep_checkpoint_max\"],\r\n )\r\nestimator = tf.estimator.Estimator(\r\n        model_fn=model_fn, model_dir=model_dir, params=config, config=model_config\r\n)\r\n![image](https://user-images.githubusercontent.com/13100437/106465747-5ada4e00-64d5-11eb-8ddc-11fb45695a88.png)\r\n\r\n2\u3001tf.distribute.experimental.ParameterServerStrategy + Estimator: the training job will be stuck\r\nexample code:\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\ncluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n variable_partitioner = (\r\n    tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n    num_shards=ps_number))\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver=cluster_resolver,\r\n    variable_partitioner=variable_partitioner)\r\nmodel_config = tf.estimator.RunConfig(\r\n        train_distribute=strategy,\r\n        eval_distribute=None,\r\n        log_step_count_steps=config[\"parameters\"][\"log_steps\"],\r\n        save_summary_steps=config[\"parameters\"][\"save_summary_steps\"],\r\n        save_checkpoints_steps=config[\"parameters\"][\"save_checkpoints_steps\"],\r\n        save_checkpoints_secs=None,\r\n        keep_checkpoint_max=config[\"parameters\"][\"keep_checkpoint_max\"],\r\n    )\r\nestimator = tf.estimator.Estimator(\r\n        model_fn=model_fn, model_dir=model_dir, params=config, config=model_config\r\n)\r\n", "comments": ["@1148569762 \r\n\r\nCan you please share colab link or simple standalone code with proper indentation to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46827\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46827\">No</a>\n"]}, {"number": 46826, "title": "Estimator: how to write asset files into the assets directory hermetically when save model", "body": "System information\r\n\r\n- python 3.6.8\r\n- tensorflow-gpu 1.15.0\r\n\r\n**Describe the problem**\r\nI have an asset file which needs to be saved within the exported model. I use estimator to export model as follows:\r\n```\r\npredictor = tf.estimator.Estimator()\r\nassets_extra = {'index.idx': '/local/path/to/index.idx'}\r\nservable_model_path = predictor.export_savedmodel(\r\n    model_path,\r\n    export_input_fn,\r\n    assets_extra,\r\n    as_text=True)\r\n```\r\n\r\nThe parameter `assets_extra` will copy the asset file `index.idx` to the export directory. And the expected result is that the tensor in the graph which contain the directory of `index.idx` should be binded to the path of `index.idx` within the export directory. Actually,  the directory of `index.idx` in the graph dosen't change, but still `/local/path/to/index.idx`. If I move the exported model to another machine which don't contain `/local/path/to/index.idx`, the model can't find the `index.idx`.\r\n\r\nSo how can I bind the tensor value to the path of `index.idx` within the export directory?", "comments": ["I found a method `tf.saved_model.Asset` which may fix this problem, but I don't know how to use it within tf.estimator", "@lonway,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46826\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46826\">No</a>\n"]}, {"number": 46825, "title": "RaggedTensor not supported as model input in generator and sequence", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- tf-nightly 2.5.0 (2021020101)\r\n- colab\r\n\r\n**Describe the current behavior**\r\ngenerators and sequences (objects that implement the keras.utils.Sequence interface) that return a RaggedTensor as model input trigger an exception\r\n\r\n**Describe the expected behavior**\r\ngenerators and sequences should support any CompositeTensor\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1Qxov6zoWEnVcGwAH67cWiqaIjGcUTwbk?usp=sharing\r\n\r\nThis notebook creates a model that uses ragged tensors and verifies that when a tensor is passed to ```model.fit``` the behaviour is as expected. It then attempts to return the save value from a generator. The generator works if the tensor is converted from ragged to dense (although that changes the computed value). Passing a ragged tensor directly to the tf.data.Dataset API also seems to work. \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nInvalidArgumentError:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was <tf.RaggedTensor [[[1, 2], [3, 4]], [[5, 6]]]>.\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'RaggedTensor'\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\r\n    dtype=dtype.as_numpy_dtype))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\r\n    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n\r\nValueError: setting an array element with a sequence.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 625, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 921, in generator_py_func\r\n    sys.exc_info()[2])\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 702, in reraise\r\n    raise value.with_traceback(tb)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 912, in generator_py_func\r\n    dtype=dtype.as_numpy_dtype))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in _convert\r\n    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n\r\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was <tf.RaggedTensor [[[1, 2], [3, 4]], [[5, 6]]]>.\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\n\t [[IteratorGetNext]] [Op:__inference_train_function_1924]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "comments": ["@pedro-r-marques \r\nplease refer these links and let us know if it helps : [link](https://github.com/tensorflow/tensorflow/issues/27598), [link1](https://github.com/tensorflow/tensorflow/issues/35342), [link2](https://stackoverflow.com/questions/63247003/tensorflow-from-generator-giving-error-generator-yielded-an-element-that-co)", "@Saduf2019 From your links it appears that the intent is that fit must be called with a Dataset which is created with ```from_generator``` such that the ```output_signature``` parameter is set; if that is the case, at least the error message should be improved imho. And most likely the documentation for ```model.fit``` should specify that using a sequence or a generator is only supported if the returned values are dense tensors.", "@pedro-r-marques \r\nCould you please try and let us know if the issue persist.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46825\">No</a>\n", "Release notes in tensorflow v2.4.0 say tf.data now supports Ragged tensors in generators. Update to tf v2.4.\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v2.4.0"]}, {"number": 46824, "title": "On Windows, full-integer TFLite conversion is broken; `schema_py_generated.py` is empty", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (20H2, build 19042)\r\n- TensorFlow installation (pip package or built from source): `tensorflow_cpu` PyPI pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\n\r\ndef make_qat_keras_model():\r\n    model = tf.keras.models.Sequential(\r\n        [\r\n            tf.keras.layers.Input((224, 224, 3)),\r\n            tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", strides=2),\r\n        ]\r\n    )\r\n    return tfmot.quantization.keras.quantize_model(model)\r\n\r\n\r\nkeras_model = make_qat_keras_model()\r\nkeras_model.summary()\r\n\r\nwith open(\"converted_keras_model.tflite\", \"wb\") as f:\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    # This code works if the following two lines are commented out\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    f.write(converter.convert())\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nThe above code yields the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\test_scipt.py\", line 23, in <module>\r\n    f.write(converter.convert())\r\n  File \"C:\\Users\\Adam\\src\\tmp-env\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 873, in convert\r\n    return super(TFLiteKerasModelConverterV2,\r\n  File \"C:\\Users\\Adam\\src\\tmp-env\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 637, in convert\r\n    result = _modify_model_io_type(result, **flags_modify_model_io_type)\r\n  File \"C:\\Users\\Adam\\src\\tmp-env\\lib\\site-packages\\tensorflow\\lite\\python\\util.py\", line 835, in modify_model_io_type\r\n    model_object = _convert_model_from_bytearray_to_object(model)\r\n  File \"C:\\Users\\Adam\\src\\tmp-env\\lib\\site-packages\\tensorflow\\lite\\python\\util.py\", line 572, in _convert_model_from_bytearray_to_object\r\n    model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\n```\r\n\r\n### Cause\r\n\r\nThe Python schema file generated as this Bazel output is just an empty file when built on Windows: https://github.com/tensorflow/tensorflow/blob/822ec5c902a7fbbb630a7ee76437abe4d8bfe13a/tensorflow/lite/python/BUILD#L11-L14\r\n\r\nThis can be verified by running `bazel build -c opt //tensorflow/lite/python:schema_py` and checking the output: `bazel-bin/external/org_tensorflow/tensorflow/lite/python/schema_py_generated.py` is empty.\r\n\r\nRunning the same build command on Linux/MacOS yields a thousands-of-LOC Python file.\r\n\r\nThe empty file issue can be further verified by unzipping the published [TensorFlow wheel](https://files.pythonhosted.org/packages/9c/3b/fb50a6d5bdaf189c6131d8db84acba2fc35d6e7d9bfd7991dd71eb304d92/tensorflow_cpu-2.4.1-cp38-cp38-win_amd64.whl) and noting that `tensorflow/lite/python/schema_py_generated.py` is an empty file.\r\n\r\nHence the Python attribute error.\r\n\r\n### Related issues\r\n\r\n* #38285", "comments": ["cc @MeghnaNatraj do you know what could be causing this problem on Windows?", "This issue is resolved in the following versions:\r\n`pip install tensorflow==2.4.0`\r\n`pip install tf-nightly` (from today: Feb 1, 2021)\r\n\r\nThere were certain tests excluded from being run on Windows/MacOS, as a result of which this error went undetected in 2.4.1. As these tests have been enabled recently, as a result of which the last few 2.5-nightly-versions (2.5 hasn't been officially released yet) contain the schema_.. file.\r\n\r\nAs you want to use full integer quantization with QAT, you now have two options:\r\n1. Use 2.4.1, and replace the file from 2.4.0 into 2.4.1\r\n2. Use tf-nightly, starting today (Feb 1, 2021) as I have verified that this file exists in this version.\r\n\r\nLet us know if this works for you.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46824\">No</a>\n", "@MeghnaNatraj Thanks for the fast response. I just checked the [2.4.0 Python 3.8 Windows wheel](https://files.pythonhosted.org/packages/84/f3/4aa5da782d3ad2ed33026a6e42280ce5a146b6fe71191b52e2bd0be7903e/tensorflow-2.4.0-cp38-cp38-win_amd64.whl) and the included `tensorflow/lite/python/schema_py_generated.py` file is empty, so downgrading to `2.4.0` is not an option (Mac and Linux versions are fine though).\r\n\r\nI will check the nightly wheels from today once they are available.", "Hi @MeghnaNatraj, thanks for looking into this. To echo Lukas' point, I tested TF 2.4.0 (both `tensorflow` and `tensorflow-cpu` packages) and got the same AttributeError as before. Is it possible that this is an issue specific to the Python 3.8 wheel (seems unlikely)?", "@AdamHillier It might be due to the OS you're on (Windows seem to have this issue). And probably Python 3.8. I am currently testing this on a MacOS with Python 3.6 and I was able to verify that the file is present. \r\n\r\nIs it possible to try running this with the latest pip package (`tf-nightly`) and let me know if you still face this issue. I will re-open and investigate the further, if the issue persists on WindowsOS with tf-nightly.", "Thanks for your help; yes, I believe this is a Windows only issue. As for `tf-nightly`, today/yesterday's published PyPI releases do not include Windows wheels.\r\n\r\nThe most recent release with a Windows wheel is `2.5.0.dev20210131` - however, when I install this in a clean environment and try and install it, I get an unrelated error `RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd` (trying to upgrade to a more recent Numpy version does not fix this, because the older Numpy version is pinned in the TF requirements).\r\n\r\nHowever, `2.5.0.dev20210130` from the day before does install and work on Windows. Unfortunately, the problem this issue describes is still present.\r\n\r\nCan we please re-open the issue, because I don't think it is currently resolved in TF-nightly? ", "@MeghnaNatraj Possibly it's related with the similar issue we found in TFLite Support. https://github.com/tensorflow/tflite-support/commit/1e6a6e832d5afb45877a3a974bde986000ac18dc\r\n\r\nI guess our TF windows wheels are built in the same environment with TFLite Support wheels, so the same problem might be there. Probably you can verify if the schema_py_generated.py is empty in the current nightly. If it is, you can give my fix a try.\r\n\r\nThanks!", "@xunkai55 Thank you for this suggestion. It looks like the past few Windows builds seem to have this issue - so I'm working on patching in your solution. \r\n\r\nI'll update this issue once the fix is merged and a new tf-nightly is published.", "I've patched the change in and the latest `tf-nightly` package has the schema_py_generated file. Thank you for the fix @xunkai55 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46824\">No</a>\n"]}, {"number": 46823, "title": "Running TensorFlow Profiler on CPU only machine but training on GPU machine?", "body": "Can I trainng a model on GPU machine and generating the profile files, but use a CPU only machine to load the files and render the profile webpage correctly?", "comments": ["@jieheroli,\r\nI was able run [this guide](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras), i.e. train and generate the model on GPU, save the `logdir` file, load it back on CPU and run the profiler without any issues. \r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/980bc108d9178a48f3078f6d8fb8836c/tensorboard_profiling_keras.ipynb#scrollTo=75oykBMhFh4e). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46823\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46823\">No</a>\n"]}, {"number": 46820, "title": "ConvLSTM2D with CUDNN crashes", "body": "System Information: Windows 10\r\ntensorflow version: 2.5.0-dev20201217\r\npython version: 3.7.9\r\ncuda version : 10.2\r\n\r\n\r\nOn using the above model, python crashes after\r\n\r\n`model = models.Sequential(\r\n\t[\r\n\t\tlayers.Input(\r\n\t\t\tshape=(timesteps, width, height, channels)\r\n\t\t),\r\n\t\tlayers.ConvLSTM2D(\r\n\t\t\tfilters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=True, dropout=0.1, recurrent_dropout=0.1\r\n\t\t),\r\n\t\tlayers.MaxPool3D(\r\n\t\t\tpool_size=(1, 2, 2), strides=(1, 2, 2), padding=\"same\"\r\n\t\t),\r\n\t\tlayers.BatchNormalization(),\r\n\t\tlayers.ConvLSTM2D(\r\n\t\t\tfilters=16, kernel_size=(3, 3), padding=\"same\", return_sequences=True, dropout=0.1, recurrent_dropout=0.1\r\n\t\t),\r\n\t\tlayers.MaxPool3D(\r\n\t\t\tpool_size=(1, 2, 2), strides=(1, 2, 2), padding=\"same\"\r\n\t\t),\r\n\t\tlayers.BatchNormalization(),\r\n\t\tlayers.ConvLSTM2D(\r\n\t\t\tfilters=8, kernel_size=(3, 3), padding=\"same\", return_sequences=False, dropout=0.1, recurrent_dropout=0.1\r\n\t\t),\r\n\t\tlayers.MaxPool2D(\r\n\t\t\tpool_size=(2, 2), strides=(2, 2), padding=\"same\"\r\n\t\t),\r\n\t\tlayers.BatchNormalization(),\r\n\t\tlayers.Flatten(),\r\n\t\tlayers.Dense(192, activation='relu'),\r\n\t\tlayers.Dense(action_num, activation='softmax')\r\n\t]\r\n)\r\n`\r\nOn using the above model, python crashes after\r\n\r\n>2021-02-01 10:03:12.227916: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:127] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/300\r\n2021-02-01 10:03:21.950076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-01 10:03:22.694349: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 8005\r\n2021-02-01 10:03:23.281554: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n>2021-02-01 10:03:23.769410: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n>2021-02-01 10:03:23.867774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-01 10:03:24.624172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n\r\non further examining the system logs found the following\r\n\r\n>Faulting application name: python.exe, version: 3.7.9150.1013, time stamp: 0x5f3ad38e\r\nFaulting module name: _pywrap_tensorflow_internal.pyd, version: 0.0.0.0, time stamp: 0x5fdb260c\r\nException code: 0xc00000fd\r\nFault offset: 0x00000000094ae988\r\nFaulting process id: 0x35dc\r\nFaulting application start time: 0x01d6f85196812501\r\nFaulting application path: C:\\Users\\Scorp\\AppData\\Local\\Programs\\Python\\Python37\\python.exe\r\nFaulting module path: C:\\Users\\Scorp\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd\r\n\r\nkeras config\r\n\r\n{\r\n    \"floatx\": \"float32\",\r\n    \"epsilon\": 1e-07,\r\n    \"backend\": \"tensorflow\",\r\n    \"image_data_format\": \"channels_last\"\r\n}\r\n\r\nPFA the code and files\r\n\r\nhttps://github.com/sivi299/lstm", "comments": ["@sivi299,\r\nTensorFlow v2.4 and TF-nightly are compatible with **CUDA 11.0** and **cuDNN 8**. For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu).\r\n\r\nI was able to run the code on TF v2.4 without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/02f234c0dfcfbc44ff512a2d00422285/46820.ipynb). Thanks!", "@amahendrakar thanks for the quick reply, I did try with Cuda 11.1/cudnn8.0.5 I am still facing the same issue I am not sure if its something to do with the GPU model itself, I am using an Nvidia 3090 series\r\n\r\nHowever, my CNN models work just fine", "I removed tf-nightly and installed tensorflow2.4 and it works fine now thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46820\">No</a>\n"]}, {"number": 46819, "title": "problematic args description table of aixs in tf.nn.softmax", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/softmax#args\r\n\r\ndocument version: 2.4.1\r\n\r\n## Description of issue (what needs changing):\r\n\r\nIn the args description table, it says axis default to -1,\r\nwhile the code below 'view alias' uses: axis=None.\r\n\r\nand according to its equivalent code, axis deafaults to None in the function reduce_sum.\r\n\r\n### so which one is right?\r\n\r\n", "comments": ["@Yonma  The value of the param axis should be -1.", "The arg description is correct. `axis = -1` by default.\r\nhttps://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/nn_ops.py#L3732-L3733", "I got it! Thank u very much, I forgot to check such thing in the sourcecode, but its really confusing at the first sight."]}, {"number": 46818, "title": "tf.cond within vectorized_map results in unknown output shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab version\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Colab version\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f 2.4.1\r\n- Python version: Colab version\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nWhen using tf.cond from within tf.vectorized_map, the output shape is lost and the result has a None batch shape. This results in memory exploding at backpropagation time.\r\n\r\n**Describe the expected behavior**\r\nThe output shape should be retained when the shapes of the two possible cond outputs are the same. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1ydXdSe0_jJqtJLYGXgkCxywjC9gFANwv?usp=sharing\r\n", "comments": ["@AdrienCorenflos \r\n\r\nIf you comment `@tf.function` then the code works as expected. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d0c7e641c6530bbb3ffa9050d9cec977/untitled647.ipynb).Is there any particular reason that you want to use @tf.function.\r\nThanks!", "Hi,\r\nI probably should have mentioned that this only happened within a tf.function indeed.\r\n\r\nThis code is actually a (very) simplified version of my original code (akin to this [one](https://github.com/EEA-sensors/sequential-parallelization-examples/blob/main/python/temporal-parallelization-bayes-smoothers/parallel_kalman_tf.ipynb)), it needs to be compiled for speed reasons.\r\n\r\nAdrien", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/cfb521a18bea983985fc8de91eca1fdc/untitled50.ipynb)..Thanks !", "To perform the `tf.cond` operation inside graph mode you need to follow excecution of OPS in a Tensorflow graph flow. Refer [this](https://stackoverflow.com/questions/37063952/) answer for more details.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46818\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46818\">No</a>\n"]}, {"number": 46817, "title": "error trying to download tensorflow with pip", "body": "\r\n**System information**\r\n-MacOs Big sur(11.1)\r\n- TensorFlow installed from pip \r\n- Python version: Python 3.9.1\r\n- pip version: 21.01\r\n\r\n\r\n**Describe the problem**\r\nwhen i want to download Tensor-flow with pip this error occurs. it says: \r\nERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow\r\n(attachment)\r\n\r\n<img width=\"569\" alt=\"Bildschirmfoto 2021-01-31 um 17 09 14\" src=\"https://user-images.githubusercontent.com/22261997/106390097-0ce71c80-63e7-11eb-971e-5b6472516600.png\">\r\n\r\n", "comments": ["@value03,\r\nTensorFlow is compatible with upto Python v3.8. Support for Python v3.9 is already being tracked in issue [#44485](https://github.com/tensorflow/tensorflow/issues/44485). For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#cpu_2).\r\n\r\n\r\nVersion | Python version | Compiler | Build tools\r\n-- | -- | -- | --\r\ntensorflow-2.4.0 | 3.6-3.8 | Clang from xcode 10.3 | Bazel 3.1.0\r\ntensorflow-2.3.0 | 3.5-3.8 | Clang from xcode 10.1 | Bazel 3.1.0\r\ntensorflow-2.2.0 | 3.5-3.8 | Clang from xcode 10.1 | Bazel 2.0.0\r\n\r\n\r\nCould you please try installing TensorFlow with Python v3.8 and check if you are facing the same error? Thanks!", "Deduplicating to #44485", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46817\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46817\">No</a>\n"]}, {"number": 46816, "title": "The chained `==` operator throws error when converting to TFLite from `tf.function`.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 (Google Colab)**\r\n- TensorFlow installation (pip package or built from source):  **pip package (Google Colab)**\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): **2.4.1 (Google Colab)**\r\n\r\n### 2. Code\r\n\r\n#### Option B: Paste your code here:\r\nConsider the scenario, in which one would like to zero-out (or filter) a tensor, based on multiple equality checks:\r\n```python\r\nimport tensorflow as tf\r\n\r\nSHAPE = (10, )\r\n\r\n@tf.function(\r\n    input_signature=[tf.TensorSpec(shape=SHAPE, dtype=tf.int32)]\r\n)\r\ndef my_function(inputs):\r\n    \r\n    # Let's say that we want to zero-out all values BUT 1 and 2 from our input tensor.\r\n\r\n    filtered_inputs = tf.where(\r\n        (inputs == 1) | (inputs == 2),\r\n        inputs,\r\n        0\r\n    )\r\n    return filtered_inputs\r\n```\r\n\r\nLet us call this function on a mock input:\r\n\r\n```python\r\nmock_inputs = tf.random.uniform(shape=SHAPE, minval=1, maxval=5, dtype=tf.int32)\r\nmy_function(mock_inputs)\r\n```\r\nIt works (so far). Now let us convert this to Tensorflow Lite as per [official documentation](https://www.tensorflow.org/lite/convert#convert_concrete_functions_)\r\n\r\n```python\r\nconcrete_func = my_function.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()   # ERROR.\r\n```\r\nThe above will fail with rather long error message that ends with\r\n```python\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Equal {device = \"\", incompatible_shape_error = false}\r\n```\r\nNow, there is a workaround: one can use the `tf.math.equal` instead of the `==` operator:\r\n\r\n```python\r\n@tf.function(\r\n    input_signature=[tf.TensorSpec(shape=SHAPE, dtype=tf.int32)]\r\n)\r\ndef my_workaround_function(inputs):\r\n\r\n    # Use Tensorflow ops instead of Python ops\r\n\r\n    filtered_inputs = tf.where(\r\n        (tf.math.equal(inputs, 1)) | (tf.math.equal(inputs, 2)),\r\n        inputs,\r\n        0\r\n    )\r\n    return filtered_inputs\r\n```\r\nAnd the above can be safely converted to TFLite:\r\n\r\n```python\r\nconcrete_func = my_workaround_function.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()  # OK. \r\n```\r\n\r\nI am opening the issue as I am unsure if this is a intended or known behaviour. For example, some people might be stuck on this situation, without figuring out the workaround. Also: as per the [Tensorflow documentation](https://www.tensorflow.org/guide/function#autograph_transformations), the `tf.function` is intended to work with simple Python ops.\r\n\r\nIf this is somewhat intended, let me know and I will close the issue (though I would greatly appreciate some explanation if possible).\r\n  \r\nBest regards,\r\nSebastian\r\n\r\n### 5. (optional) Any other info / logs\r\n\r\nHere is a full error message:\r\n```python\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Equal {device = \"\", incompatible_shape_error = false}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    214       return model_str\r\n    215     except Exception as e:\r\n--> 216       raise ConverterError(str(e))\r\n    217 \r\n    218   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1750:0: error: 'tf.Equal' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201:0: note: called from\r\n<ipython-input-54-3ccb02792638>:14:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:973:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:990:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3206:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:3361:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py:2969:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:726:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.Equal {device = \"\", incompatible_shape_error = false}\r\n```", "comments": ["@sebastian-sz You can enable tf.Equal operator by using select TF option. https://www.tensorflow.org/lite/guide/ops_select", "Could you try the above two options with tf-nightly? I confirmed that both work well with tf-nightly.", "It does work both with tf-nightly and with `tf.lite.OpsSet.SELECT_TF_OPS`. Thank you! I'm closing the issue. "]}, {"number": 46815, "title": "Problem with jit compiler:", "body": "When trying your example programm:\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\n# Pretend to load synthetic data set.\r\nfeatures = tfp.distributions.Normal(loc=0., scale=1.).sample(int(100e3))\r\nlabels = tfp.distributions.Bernoulli(logits=1.618 * features).sample()\r\n\r\n# Specify model.\r\nmodel = tfp.glm.Bernoulli()\r\n\r\n# Fit model given data.\r\ncoeffs, linear_response, is_converged, num_iter = tfp.glm.fit(\r\n    model_matrix=features[:, tf.newaxis],\r\n    response=tf.cast(labels, dtype=tf.float32),\r\n    model=model)\r\n# ==> coeffs is approximately [1.618] (We're golden!)\r\n\r\nI get the following log message:\r\n2021-01-31 13:51:01.420629: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nTraceback (most recent call last):\r\n  File \"D:/PyProjects/PILCO-master/other.py\", line 2, in <module>\r\n    import tensorflow_probability as tfp\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\__init__.py\", line 20, in <module>\r\n    from tensorflow_probability import substrates\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\substrates\\__init__.py\", line 21, in <module>\r\n    from tensorflow_probability.python.internal import all_util\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py\", line 142, in <module>\r\n    dir(globals()[pkg_name])  # Forces loading the package from its lazy loader.\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py\", line 61, in __dir__\r\n    module = self._load()\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\__init__.py\", line 35, in <module>\r\n    from tensorflow_probability.python.experimental import bijectors\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\experimental\\bijectors\\__init__.py\", line 17, in <module>\r\n    from tensorflow_probability.python.bijectors.ldj_ratio import inverse_log_det_jacobian_ratio\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\__init__.py\", line 23, in <module>\r\n    from tensorflow_probability.python.bijectors.absolute_value import AbsoluteValue\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\absolute_value.py\", line 23, in <module>\r\n    from tensorflow_probability.python.bijectors import bijector\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py\", line 35, in <module>\r\n    from tensorflow_probability.python.math import gradient\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\math\\__init__.py\", line 30, in <module>\r\n    from tensorflow_probability.python.math.generic import log1mexp\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\math\\generic.py\", line 151, in <module>\r\n    _kahan_reduction, _kahan_reduce_bwd, _kahan_reduce_tangents)\r\n  File \"C:\\Users\\Martin\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\internal\\variadic_reduce.py\", line 122, in make_variadic_reduce\r\n    @tf.function(jit_compile=True)\r\nTypeError: function() got an unexpected keyword argument 'jit_compile'\r\n", "comments": ["I have tried in colab with TF version 2.4 ,nightly version(`2.5.0-dev20210131`) and I am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/3639430154b53d3fc76528f7612f5ba1/untitled648.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46815\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46815\">No</a>\n"]}, {"number": 46814, "title": "updated punctuations", "body": "Punctuations are corrected in the readme file.", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac"]}, {"number": 46813, "title": "suffering with the problem : tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open: HelmetDetection/frozen_inference_graph.pb : The system cannot find the path specified. ; No such process", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n![issue1](https://user-images.githubusercontent.com/75311266/106377536-a8669600-63c3-11eb-865a-9fdfbb9b10f7.png)\r\n![issue1](https://user-images.githubusercontent.com/75311266/106377540-b1effe00-63c3-11eb-9385-ec6b9997a32e.png)\r\n\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@1advait,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46813\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46813\">No</a>\n"]}]