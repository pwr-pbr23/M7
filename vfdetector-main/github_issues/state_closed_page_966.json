[{"number": 24446, "title": "TOCO failed.Covert saved_model.pb to tflite", "body": "### Describe the problem\r\nI trained a [classification ](https://codelabs.developers.google.com/codelabs/mlimmersion-image-flowerstxf/index.html?index=..%2F..%2Fcloud#0)  model using ML engine.I got the model folder with saved model.pb and variables folder. Now I want to convert the saved_model.pb to tflite.I used the following code.\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/home/ubuntu/*****/model')\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)`\r\n```\r\n\r\nIt gives the following error \r\n\"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed.\"\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n2018-12-19 10:10:33.807941: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-12-19 10:10:33.812470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400070000 Hz\r\n2018-12-19 10:10:33.812612: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x3a4e660 executing computations on platform Host. Devices:\r\n2018-12-19 10:10:33.812633: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:61: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nWARNING:tensorflow:The saved meta_graph is possibly from an older release:\r\n'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\r\nWARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nWARNING:tensorflow:The saved meta_graph is possibly from an older release:\r\n'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\r\nWARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert_saved_model.py:275: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.convert_variables_to_constants\r\nWARNING:tensorflow:From /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.compat.v1.graph_util.extract_sub_graph\r\nTraceback (most recent call last):\r\n  File \"export_from_saved.py\", line 4, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2018-12-19 10:10:40.009636: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2018-12-19 10:10:40.020516: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2018-12-19 10:10:40.020613: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3\r\n2018-12-19 10:10:40.020649: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2018-12-19 10:10:40.020667: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2018-12-19 10:10:40.020678: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2018-12-19 10:10:40.020689: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2018-12-19 10:10:40.020710: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2018-12-19 10:10:40.020728: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LoopCond\r\n2018-12-19 10:10:40.020748: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2018-12-19 10:10:40.020763: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2018-12-19 10:10:40.020772: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2018-12-19 10:10:40.020784: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3\r\n2018-12-19 10:10:40.020801: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: DecodeJpeg\r\n2018-12-19 10:10:40.020834: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2018-12-19 10:10:40.020851: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2018-12-19 10:10:40.020865: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2018-12-19 10:10:40.020885: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2018-12-19 10:10:40.020902: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2018-12-19 10:10:40.020926: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2018-12-19 10:10:40.112748: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1302 operators, 1816 arrays (0 quantized)\r\n2018-12-19 10:10:40.176386: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1302 operators, 1816 arrays (0 quantized)\r\n2018-12-19 10:10:40.332638: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 159 operators, 371 arrays (0 quantized)\r\n2018-12-19 10:10:40.337273: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 156 operators, 365 arrays (0 quantized)\r\n2018-12-19 10:10:40.342033: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 156 operators, 365 arrays (0 quantized)\r\n2018-12-19 10:10:40.351531: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2018-12-19 10:10:40.352994: E tensorflow/lite/toco/toco_tooling.cc:421] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, LESS, LOGISTIC, MAX_POOL_2D, RANGE, RESHAPE, RESIZE_BILINEAR, SQUEEZE. Here is a list of operators for which you will need custom implementations: DecodeJpeg, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, LESS, LOGISTIC, MAX_POOL_2D, RANGE, RESHAPE, RESIZE_BILINEAR, SQUEEZE. Here is a list of operators for which you will need custom implementations: DecodeJpeg, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.", "comments": ["Any updates\r\n", "Hi, i have problem with toco\r\n\"RuntimeError: TOCO failed see console for info.\r\n2019-01-09 08:05:08.698392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.698452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/TensorArray\r\n2019-01-09 08:05:08.698462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.698469: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/TensorArray_1\r\n2019-01-09 08:05:08.698620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n(stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\n2019-01-09 08:05:08.698392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.698452: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/TensorArray\r\n2019-01-09 08:05:08.698462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.698469: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/TensorArray_1\r\n2019-01-09 08:05:08.698499: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-09 08:05:08.698508: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\r\n2019-01-09 08:05:08.698523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698534: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698542: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698576: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LoopCond\r\n2019-01-09 08:05:08.698592: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/while/LoopCond\r\n2019-01-09 08:05:08.698620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-01-09 08:05:08.698634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.698642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-09 08:05:08.700531: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.700558: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.700583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.700596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.700618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.700627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-01-09 08:05:08.700634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-09 08:05:08.700641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/while/TensorArrayWrite/TensorArrayWriteV3\r\n2019-01-09 08:05:08.700656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\r\n2019-01-09 08:05:08.700665: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-09 08:05:08.700671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: question_encoding/rnn/TensorArrayStack/TensorArraySizeV3\r\n2019-01-09 08:05:08.700687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-09 08:05:08.702429: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.702442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/fw/fw/TensorArray\r\n2019-01-09 08:05:08.702450: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.702456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/fw/fw/TensorArray_1\r\n2019-01-09 08:05:08.702482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-09 08:05:08.702490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/fw/fw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\r\n2019-01-09 08:05:08.702502: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702558: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LoopCond\r\n2019-01-09 08:05:08.702563: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/fw/fw/while/LoopCond\r\n2019-01-09 08:05:08.702588: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702595: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-01-09 08:05:08.702600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.702608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-09 08:05:08.704475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-01-09 08:05:08.704562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-09 08:05:08.704569: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3\r\n2019-01-09 08:05:08.704583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\r\n2019-01-09 08:05:08.704591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-09 08:05:08.704597: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArraySizeV3\r\n2019-01-09 08:05:08.704612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-09 08:05:08.704641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ReverseV2\r\n2019-01-09 08:05:08.704698: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.704707: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/bw/bw/TensorArray\r\n2019-01-09 08:05:08.704715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayV3\r\n2019-01-09 08:05:08.704721: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/bw/bw/TensorArray_1\r\n2019-01-09 08:05:08.704748: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-09 08:05:08.704756: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/bw/bw/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\r\n2019-01-09 08:05:08.704768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704777: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704784: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704806: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704815: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704825: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: LoopCond\r\n2019-01-09 08:05:08.704829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/bw/bw/while/LoopCond\r\n2019-01-09 08:05:08.704853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704859: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-01-09 08:05:08.704865: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.704872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-09 08:05:08.706701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.706720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.706742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.706752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.706771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Enter\r\n2019-01-09 08:05:08.706778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 20\r\n2019-01-09 08:05:08.706784: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-09 08:05:08.706791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/bw/bw/while/TensorArrayWrite/TensorArrayWriteV3\r\n2019-01-09 08:05:08.706804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Exit\r\n2019-01-09 08:05:08.706812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-09 08:05:08.706817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1127] Op node missing output type attribute: input_fusion/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArraySizeV3\r\n2019-01-09 08:05:08.706832: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-09 08:05:08.706860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: ReverseV2\r\n2019-01-09 08:05:08.706936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2019-01-09 08:05:08.706945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2019-01-09 08:05:08.716294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2019-01-09 08:05:08.716339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2019-01-09 08:05:08.725842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2019-01-09 08:05:08.725899: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1080] Converting unsupported operation: Abs\r\n2019-01-09 08:05:09.935688: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 10153 operators, 14372 arrays (0 quantized)\r\n2019-01-09 08:05:11.575490: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 10153 operators, 14372 arrays (0 quantized)\r\n2019-01-09 08:05:19.104584: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 7633 operators, 12950 arrays (0 quantized)\r\n2019-01-09 08:05:21.192942: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 7630 operators, 12947 arrays (0 quantized)\r\n2019-01-09 08:05:23.251206: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 7627 operators, 12941 arrays (0 quantized)\r\n2019-01-09 08:05:25.322947: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 7627 operators, 12941 arrays (0 quantized)\r\n2019-01-09 08:05:27.454312: F tensorflow/contrib/lite/toco/tooling_util.cc:618] Check faile\ud83d\ude1b dim >= 1 (0 vs. 1)\r\nAborted (core dumped)\"\r\n\r\nHelp me.", "@LeManhTruong What model are you using ? Please give more details\r\n", "> @LeManhTruong What model are you using ? Please give more details\r\n\r\nI use model for VQA, link source <https://github.com/DeepRNN/visual_question_answering>. When I converting model into \"tflite\", I have problem with TOCO, error \"unsupported operation\".", "> > @LeManhTruong What model are you using ? Please give more details\r\n> \r\n> I use model for VQA, link source https://github.com/DeepRNN/visual_question_answering. When I converting model into \"tflite\", I have problem with TOCO, error \"unsupported operation\".\r\n\r\ndid you try with  Tensorflow 1.12 ?", "> > > @LeManhTruong What model are you using ? Please give more details\r\n> > \r\n> > \r\n> > I use model for VQA, link source https://github.com/DeepRNN/visual_question_answering. When I converting model into \"tflite\", I have problem with TOCO, error \"unsupported operation\".\r\n> \r\n> did you try with Tensorflow 1.12 ?\r\n\r\nyes, I try with Tensorflow 1.12", "@LeManhTruong Please file a separate GitHub issue with all of the relevant information.\r\n\r\n@jennings1716 It seems your model has ops that are not available as TFLite built-in ops. Can you try using our [experimental feature](https://www.tensorflow.org/lite/guide/ops_select) to try converting with a broader set of ops. There is some additional information about handling unsupported ops in our [FAQ](https://www.tensorflow.org/lite/guide/faq#why_are_some_operations_not_implemented_in_tensorflow_lite).", "I just raised an issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/27278\r\n\r\nI tried to convert the sample graphdef to .tflite. It converted successfully. I don't the there is any error in tensorflow version. \r\nbut when I tried to convert another graphdef to .tflite, it is giving the same error as above.\r\nplease check the link above for details.\r\n\r\ncan anyone help me. \r\n\r\nthanks", "Tensorflow till now didnt provide tflite function for all operations and hence you get this error. We have to write custom functions for unsupported operation"]}, {"number": 24444, "title": "TypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.", "body": "Hello,\r\nI am using TF1.4,\r\nUbuntu16.04\r\nI am implementing https://github.com/zhenkaiwang/im2txt_attention\r\nI encountered during the training phase:\r\nTraceback (most recent call last):\r\n\u00a0\u00a0File \"/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/train.py\", line 114, in <module>\r\n\u00a0\u00a0\u00a0\u00a0Tf.app.run()\r\n\u00a0\u00a0File \"/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n\u00a0\u00a0\u00a0\u00a0_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\u00a0\u00a0File \"/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/train.py\", line 65, in main\r\n\u00a0\u00a0\u00a0\u00a0Model.build()\r\n\u00a0\u00a0File \"/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/show_and_tell_model.py\", line 550, in build\r\n\u00a0\u00a0\u00a0\u00a0Self.build_model()\r\n\u00a0\u00a0File \"/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/show_and_tell_model.py\", line 392, in build_model\r\n\u00a0\u00a0\u00a0\u00a0_, initial_state = lstm_cell(self.image_embeddings, zero_state)\r\n\u00a0\u00a0File \"/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/ops/rnn_cell_ops.py\", line 805, in __call__\r\n\u00a0\u00a0\u00a0\u00a0Output, new_state, _,_,_ = self._cell(inputs, state, scope)\r\n\u00a0\u00a0File \"/home/hsu/im2txt_attention-master/bazel-bin/im2txt/train.runfiles/im2txt/im2txt/ops/rnn_cell_ops.py\", line 435, in __call__\r\n\u00a0\u00a0\u00a0\u00a0i, j, f, o = array_ops.split(1, 4, concat)\r\n\u00a0\u00a0File \"/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1265, in split\r\n\u00a0\u00a0\u00a0\u00a0Split_dim=axis, num_split=num_or_size_splits, value=value, name=name)\r\n\u00a0\u00a0File \"/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5094, in _split\r\n\u00a0\u00a0\u00a0\u00a0Name=name)\r\n\u00a0\u00a0File \"/home/hsu/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 533, in _apply_op_helper\r\n\u00a0\u00a0\u00a0\u00a0(prefix, dtypes.as_dtype(input_arg.type).name))\r\nTypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.\r\nThen I found some solutions as follows\r\nOriginal code\r\nState_tuple = tf.split(split_dim=1, num_split=2, value=state_feed)\r\nAfter modification\r\nState_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1\r\n\r\nError code still appears\uff1a\r\nTypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.\r\n\r\n\r\nCan someone help me?\r\n", "comments": ["This question is better asked on [zhenkaiwang/im2txt_attention repo](https://github.com/zhenkaiwang/im2txt_attention/issues) since it is not a bug or feature request. If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as a repro example for your case. Thanks!\r\n\r\n"]}, {"number": 24443, "title": "How to export eager model as graph, and how to restore object-based checkpoint which generate by eager mode from estimator", "body": "How to export eager model as graph, and how to restore object-based checkpoint which generate by eager mode from estimator", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 24442, "title": "tf.contrib.copy_graph.copy_variable_to_graph does not set the shape of copied variable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: cuda 9.0, cudnn 7.1\r\n- GPU model and memory: GeForce GTX 1060\r\n\r\n**Describe the current behavior**\r\nVariables copied using tf.contrib.copy_graph.copy_variable_to_graph have unknown shape. This make this function unusable and useless. \r\n\r\n**Describe the expected behavior**\r\nVariables copied using tf.contrib.copy_graph.copy_variable_to_graph should have the same shape of original variables in order to make possible restoring from a checkpoint.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable((), name=\"x\")\r\n\r\ng = tf.get_default_graph()\r\n\r\ng2 = tf.Graph()\r\n\r\nwith g2.as_default():\r\n    x2 = tf.contrib.copy_graph.copy_variable_to_graph(x, g2)\r\n\r\nprint(x.get_shape()) # (0,)\r\nprint(x2.get_shape()) # <unknown>\r\n```\r\n\r\n**Fixing**\r\nThis issue is simply solved by changing lines: \r\nhttps://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/copy_graph/python/util/copy_elements.py#L89-L96\r\n\r\nto:\r\n```python\r\n  #Initialize the new variable\r\n  with to_graph.as_default():\r\n    new_var = Variable(\r\n        init_value,\r\n        trainable,\r\n        name=new_name,\r\n        collections=collections,\r\n        validate_shape=True)\r\n```\r\n\r\nIn my opinion there's no reason to set the field `validate_shape` to False, since in order to create the new variable we need to evaluate the old variable to get the init_value, so we have the actual shape of variables.\r\n", "comments": ["Can you send a pull request?", "Yes, I'll send a pull request in the next days.", "Hi, Is this issue fixed in any recent releases? \r\nCurrently, I am doing a workaround by applying `tf.reshape` to the new variable with that of the old one.", "Contrib has been depreciated here in Tensorflow repo and moved to tensorflow/addons , please reopen the issue in https://github.com/tensorflow/addons/issues if this exists in latest version. Thank you"]}, {"number": 24441, "title": "AttributeError: module 'tensorflow.contrib.saved_model' has no attribute 'saved_keras_model'", "body": "**System information**\r\n- TensorFlow version:1.9.0\r\n- Doc Link: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/save_and_restore_models.ipynb\r\n\r\n**Describe the documentation issue**\r\nI followed the notebook and all okies until this line of code:\r\nsaved_model_path = tf.contrib.saved_model.save_keras_model(model, \"./saved_models\")\r\nThe error is: \r\nAttributeError: module 'tensorflow.contrib.saved_model' has no attribute 'saved_keras_model'", "comments": ["You can try running the tutorial on google colab. For executing the script locally I would recommend upgrading your TensorFlow version.", "Thanks @ymodak, I made it."]}, {"number": 24440, "title": "1.13.0-rc0 cherry-pick request: add armebi toolchain", "body": "PiperOrigin-RevId: 226115035", "comments": []}, {"number": 24438, "title": "1.13.0-rc0 cherry-pick request: Add k8 to toolchains", "body": "PiperOrigin-RevId: 226087365", "comments": []}, {"number": 24437, "title": "Fix some typos and add built-tags output support", "body": "This change makes some minor changes:\n\n- Fix some typos\n- Add a \"test your changes\" example to the README\n- Add a --nocache flag to ignore the Docker build cache\n- Add a --write_tags_to flag to save a clean list of\n  built tags to a file. Can be used to build tags, then\n  run tests in parallel by combining xargs with --only_tags_matching.", "comments": ["While I was testing this, I realized that making new files (and folders) leads to root-owned files because of how Docker works. I replaced the output flag with a written log to stdout (the rest of the logs go to stderr) so that the list of tags is redirectable to a normal file with `>`.\r\n\r\nThis means directories would have to be created in advance, but in the case of a planned output folder, I think that's an okay tradeoff for not having to deal with root-owned files."]}, {"number": 24436, "title": "Update toolchain for arm.", "body": "PiperOrigin-RevId: 226075926", "comments": []}, {"number": 24435, "title": "Develop C++ infrastructure for testing tf.data kernel implementations in C++", "body": "# Motivation   (from @jsimsa)\r\n\r\nThe goal is to develop C++ infrastructure for testing tf.data kernel implementations in C++. Currently, the C++ tf.data kernels are only tested through Python bindings which are not fine-grained enough and some public C++ APIs are not always tested.\r\n\r\nThe flow of testing the C++ API would be:\r\n* create an instance of the dataset op \r\n* invoke the `Compute` method which takes input arguments and produces `DatasetBase` object wrapped in a `variant`\r\n* test the public API of the `DatasetBase` object, including the `MakeIterator` method which produces `IteratorBase` object\r\n* test the public API of the `IteratorBase` object\r\n\r\n# Progress\r\n\r\n- [x] Developed an example for testing some public APIs for `RangeDataset`\r\n- [ ] Add the tests for all the public APIs\r\n- [ ] Design `DatasetOpsTestBase` as the test base class\r\n- [ ] Add tests for other Dataset Ops\r\n", "comments": ["@feihugis let me know if there is anything you would like me to do w.r.t. to your code ... on a related note, I am working on a CL that provides a standalone C++ API for tf.data and the implementation details might be helpful for your work ... I will let you know when it is submitted", "@jsimsa I move this PR to [my personal repo](https://github.com/feihugis/tensorflow/pull/3) instead of the official tensorflow github repo. Do you think which one is a better place for the initials PRs? \r\n\r\nThe PR is [here](https://github.com/feihugis/tensorflow/pull/3). The first step is an example of testing `RangeDataset`. Please feel free to comment/revise/suggest the code!\r\n", "@feihugis thanks ... your personal repo works better ... I will comment there", "Thank you, @jsimsa!", "@jsimsa In case you miss the message, just let you know that the [PR](https://github.com/feihugis/tensorflow/pull/3#issuecomment-448787359) has been revised according to your comments. Thanks! ", "@jsimsa Happy new year!\r\n\r\nWhen implementing `DatasetBase::Save()`, one problem I met is that some classes (e.g., [VariantTensorDataWriter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/iterator_ops.cc#L348)) cannot be accessed by the test file as they are defined in `iterator_ops.cc` file. Do you have any suggestions? More details can be found [here](https://github.com/feihugis/tensorflow/pull/3#issuecomment-451218905)."]}, {"number": 24434, "title": "Build from source issue - Bezel test fails", "body": "### System information\r\n- **OS Platform and Distribution: Linux Ubuntu 18.04:\r\n- **TensorFlow version (git cloned from https://github.com/tensorflow/tensorflow (master):\r\n- **Python version 3.6+ (virtual environment created with anaconda 5.3.1:\r\n- **Bazel version 0.20.0:\r\n- **GCC/Compiler version gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- built-in intelGPU and memory: 8Gb:\r\n- bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...\r\n\r\n information using environment capture script:\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2692606/tf_env.txt)\r\n\r\n### Description of the problem\r\nWhen I try to compile TF from source as described in the documentation, bazel test fails.\r\n[tf_build_errors.txt](https://github.com/tensorflow/tensorflow/files/2692548/tf_build_errors.txt)\r\n\r\n### Source code / logs\r\nI face the problem just following the documentation step by step up to:\r\nbazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...\r\n", "comments": ["Hello, This is an issue with building TensorFlow from source using bazel 0.20.0 ; Please use bazel 0.18.0 with TensorFlow current code distribution.", "Tried bazel 0.18.0 as well but still failing. I had to go through several changes, mainly downgrade to bazel 0.16 and using cmake rather than gcc. Build succeeded now, at least for r1.8.", "downgrading to bazel 0.16.0 should work for an earlier release like r1.8, on Ubuntu 18.04\r\nWill check compatibility for the more recent versions of bazel and tensorflow releases with gcc.\r\nThanks.", "Can you try the following combination and please let me know:\r\n    _r1.12 , Python 3.6 , gcc 4.8 , bazel 0.15.0_\r\nThe problem it seems is with the bazel releases that are behind the curve.\r\n\r\n\r\n\r\n\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This issue is considered closed, since it is obvious that the bazel version is the issue, and the author has not reverted back for the grace period."]}, {"number": 24433, "title": "Add a warning message which happens when contrib is imported.", "body": "PiperOrigin-RevId: 225933307", "comments": []}, {"number": 24432, "title": "Fixing tf nightly build.", "body": "PiperOrigin-RevId: 225548043", "comments": []}, {"number": 24431, "title": "No TensorFlow-GPU install from pip (seems to be only CPU version)", "body": "Hello,\r\nI am coming back to you for help. I am installing a DevBox under Ubuntu 18.04.1 LTS with 4 RTX 2080Ti Nvidia cards.\r\nI have performed multiple Linux/Drivers installation without success. \r\nThere is no GPU use by Keras and fore sure no multi_GPU_model calculation. It seems that despite of the pip install TensorFlow-gpu install, no gpu install is performed. \r\nAt this time, the Nvidia drivers are 410.78 with CUDA 10.0.\r\nThe Nvidia-smi function shows me the good data : all the 4 GPUs, Drivers version, Cuda Revision\r\nI am using Anaconda Python 3.6.5.\r\nTensorFlow : 1.12.0 with GPU version\r\nKeras : TensorFlow Backend 2.2.4\r\n\r\nAdditionally, is there anybody who has experienced starting issue with Ubuntu 18.04.1 LTS and Nvidia driver such as no starting ?\r\nThank you for your help\r\nBest Regards\r\nEdouard\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": ["For information, i also tried conda install TensorFlow-gpu. 1,62Go have been downloaded (Cuda 9 included). GPU worked but I finally had a calculation error (memory error) while I know the model and data work.\r\nThanks in advance for your help ", "If you want to install Tensorflow-GPU via PIP, please use CUDA9.0.", "Hello,\r\nas I would like to use Cuda 10.0, I have tried to build Tensorflow following some tutorials. \r\nBut I am stuck on the following issue : tensorflow-1.12.0-cp27-cp27mu-linux_x86_64.whl is not a supported wheel on this platform\r\nI don't know how to solve that and/or how to create a cp36 version.\r\nCould you help me ?\r\nThanks in advance", "Hello, \r\nI precise that my pip version is up-to-date : \r\npip 18.1 from /home/edouard/anaconda3/lib/python3.6/site-packages/pip (python 3.6)\r\nBest Regards", "@EdouardDKP Is this still an issue for you? Did you try installing TF with cuda 9.0?", "Hello,\r\nsorry for the late answer. First, I had boot issue. I was not able to boot on linux ...\r\nI identified the root cause : one of my 4 cards is malfunctioning. So I remove the power of this one.\r\nThanks to that, I was able to build a specific tensorflow version. I build a 1.12 tensorflow version with bazel 0.17.2, CUDA 10.0, Cudnn 7.4, Nccl 2.3, TensorRT 5.0.2.6 with the following command line : bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nUnfortunately, CPU is still used instead of GPUs.\r\nDo you have any idea ?\r\nThanks in advance\r\n", "What is the output of ```import tensorflow as tf tf.Session(config=tf.ConfigProto(log_device_placement=True))```\r\n in your console?\r\nNote that TF 1.13.0rc0 has released and comes with prebuilt cuda 10 libraries. You might want to give it a try.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24430, "title": "tf.nn.depthwise_conv2d issue with float16 => nan output", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary (using docker image tensorflow/tensorflow:1.12.0-gpu)\r\n- TensorFlow version (use command below):  ('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n- Python version: 2.7.12\r\n- CUDA/cuDNN version: cuda 9.0, cudnn 7.2.1\r\n- GPU model and memory: nvidia Tesla V100-DGXS-16GB\r\n\r\n**Describe the current behavior**\r\ntf.nn.depthwise_conv2d_native returns nan when used with float16. Multiple call to same sess.run(op) return different results.\r\n\r\n**Describe the expected behavior**\r\nNo nan in the output. Similar value when running same op multiple times.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nwith tf.Session() as sess:\r\n    tf_input1 = tf.ones(shape=[1, 22, 28, 28], dtype=tf.float16) / 5.0\r\n    tf_input2 = tf.ones(shape=[7, 7, 22, 1], dtype=tf.float16) / 10.0\r\n\r\n    mat = tf.random.uniform( [4096,4096], minval=0, maxval=None, dtype=tf.float16, seed=0, name=None)\r\n    res_mat = tf.matmul(mat, mat)\r\n    #res_mat = tf.no_op()  # un-comment this line and the issue dissapears\r\n\r\n    sess.run(res_mat)\r\n    res = tf.nn.depthwise_conv2d_native(tf_input1, tf_input2, strides=[1, 1, 1, 1], padding=\"SAME\", data_format='NCHW')\r\n    v1 = sess.run(res)\r\n    sess.run(res_mat)\r\n\r\n    v2 = sess.run(res)\r\n    if np.isnan(v2).any():\r\n       print(\"issue: nan detected\")\r\n    np.testing.assert_equal(v1, v2)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nIssue occurs only on nvidia hardward that support float16 operation (on cpu no issue)\r\nreplacing res_mat with tf.no_op() => v1 and v2 are equal without nan\r\nUsing float32 => no nan\r\n\r\nIn my understanding the float16 version of the op does not reinitialized some buffer content.\r\n\r\n", "comments": ["@jvishnuvardhan, I'm not familiar with the inner workings of this function. Can you find someone more knowledgeable to check it out, please?", "similar, I cannot really comment on this. Please reroute.", "Assigning to @chsigg, as I am not familiar with the depthwise conv code.", "@lgeo3 Your reproducer seems to work fine with 1.14 so I suspect this has already been fixed.  I'll speculatively close this issue for now, but please reopen if there is something more to do here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24430\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24430\">No</a>\n"]}, {"number": 24429, "title": "Forced to compile Keras model for inference", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.1\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 14:01:38)\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n\r\n**Describe the current behavior**\r\nCalling `model.predict()` with a `tf.data.Dataset` as input is only possible when compiling the model first and also providing labels. The `predict` method is supposed to not require compiling before use, but the method `_standardize_user_data` ( called at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L1113) raises the following exception if model is not compiled, and a (x,y) tuple is returned from the dataset:\r\n\r\n> RuntimeError: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.\r\n\r\nIf the datset only returns the input tensor(s), x, another exception is raised:\r\n\r\n> ValueError: Please provide model inputs as a list or tuple of 2  or 3 elements: (input, target) or (input, target, sample_weights) Received <next_element>\r\n\r\n**Describe the expected behavior**\r\nI expect behaviour not to differ from when providing arrays or lists of arrays as input. I should be able to provide a `tf.data.Dataset` as input, that provides a single tensor or a list of tensors.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow.keras as keras\r\nimport tensorflow.data as data\r\n\r\n\r\ndataset_values = [1, 2, 3, 4, 5]\r\n\r\n\r\ndef single_element_input_fn():\r\n    return data.Dataset.from_tensor_slices(dataset_values)\r\n\r\n\r\ndef tuple_input_fn():\r\n    dataset = data.Dataset.from_tensor_slices(dataset_values)\r\n    # Spoof some labels in order to satisfy shape requirement\r\n    return dataset.map(lambda x: (x, 0))\r\n\r\n\r\nmodel_input = keras.layers.Input(shape=(1, ))\r\nmodel_output = keras.layers.Activation('tanh')(model_input)\r\nmodel = keras.models.Model(inputs=model_input, outputs=model_output)\r\n\r\n# passing python list of values works\r\nprint(model.predict(dataset_values))\r\ntry:\r\n    # Raises ValueError\r\n    print(model.predict(single_element_input_fn(), steps=5))\r\nexcept ValueError as e:\r\n    print(e)\r\ntry:\r\n    # Raises RuntimeError\r\n    print(model.predict(tuple_input_fn(), steps=5))\r\nexcept RuntimeError as e:\r\n    print(e)\r\n```\r\n\r\n**Other info / logs**\r\nSee code above.\r\n", "comments": ["Hi @harahu, please call `.batch(<batch_size>)` on your Dataset. When I do that with your example, everything works fine\r\n\r\nFor example:\r\n\r\n```python\r\ndef single_element_input_fn():\r\n    return data.Dataset.from_tensor_slices(dataset_values).batch(1)\r\n```\r\n", "@omalleyt12 I originally reported this issue using TF 1.12. I rechecked with that version and the issue still remains, also when using batch as you suggest. You are correct in that it now works in version 1.13.1, when using batch. I guess it might not be worth backporting the fixes. (I, at least have moved on to 1.13.1)\r\n\r\nI am, however, unsure if this really is a satisfying solution, because it means that the dimensionality requirements for when you use numpy arrays as input and when you use TF datasets are different. In the example above, if you use:\r\n```python\r\ndataset_values = np.array([1, 2, 3, 4, 5])\r\n```\r\nyou get the desired predictions, but you need to use the `batch` method in the dataset. Now, if you rather use:\r\n```python\r\ndataset_values = np.array([[1], [2], [3], [4], [5]])\r\n```\r\nyou _still_ get the correct results using the array directly, but you _don't_ need to use batching in the dataset.\r\n\r\nIn my mind it would make sense that there is a stricter one-to-one mapping between arrays and datasets, particularly when they can be used interchangeably, as in this case. This essentially means either raising exceptions when using the flattened array directly, as in the first example, or somehow inferring that an unbatched dataset should be interpreted as a _single_ batch. My reasoning for this is that I think it is confusing to deal with two slightly different requirements for input shape depending on how I choose to represent it.\r\n\r\nDo you think this is a valid point? If so, I think this probably warrants a new issue.", "Hi @harahu, yes you're correct we are only forward-fixing. Backporting changes like this requires quite a lot of work. \r\n\r\nI see your point about the difference b/t how np arrays and Datasets are handled. Technically the second example you provided is the correct one, it's quite ambiguous in the first example whether Keras should expand to:\r\n\r\n(A)\r\n```python\r\ndataset_values = np.array([[1], [2], [3], [4], [5]])\r\n```\r\n\r\nor (B):\r\n\r\n```python\r\ndataset_values = np.array([[1, 2, 3, 4, 5]])\r\n```\r\n\r\nFor historical reasons, Keras expands NP arrays to (A) but I'm not sure we want to auto-batch people's Datasets bc of this ambiguity, and bc Datasets can be used for some advanced use cases that this might mess up", "@omalleyt12 Also, strictly speaking, the original issue isn't entirely resolved. Yes, I am no longer forced to also provide labels in order to satisfy the shape requirements when using a dataset, but if I choose to, for reasons unknown, I still get the error:\r\n``You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.``\r\n\r\nSo I guess what has changed is the criticality of the issue, not the issue it self. I guess that is something to think about before closing this issue.\r\n\r\nA related issue is that if you provide an inappropriate `steps`, value (6, in this example) to `model.predict` when using a dataset as input, you trigger a warning that looks really out of place (note the reference to a training context):\r\n```\r\nWARNING:tensorflow:Your dataset iterator ran out of data; interrupting training.\r\nMake sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 6 batches).\r\nYou may need touse the repeat() function when building your dataset.\r\n```\r\nBoth of these issues are obviously the result of reusing code between implementations of `model.predict` and `model.fit`, which I see the appeal in.", "@omalleyt12 I agree that it might not be desirable to imitate the same leniency when using datasets as well. Assuming this is a quirk of the Keras specification, it is probably also not realistic to tighten up the requirements for array input to match that of dataset. But at a minimum I think it is an idea to document the differences somewhere, so that people like me can avoid confusion and head scratching."]}, {"number": 24428, "title": "Ability to Profile Estimators", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThere seems to be no straight-forward way to profile an `Estimator`. As an ML practitioner, being able to check the speed of predictions on a given architecture is *vital*.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes. \r\n\r\nI'd imagine the `estimator.RunConfig` object would take an optional reference to a `tensorflow.RunMetadata` object? I'm not an expert on the way profiling works in tf, so any guidance would be appreciated \r\n\r\n**Who will benefit with this feature?**\r\n\r\nAny ML practitioner (which is who the Estimator API was written for, as I understand) will benefit by being able to profile the speed and memory requirements of Estimators on different hardware. This will aid them selecting given architecture over another and determining whether use of a particular model is feasible for throughput requirements. \r\n\r\n**Any Other info.**\r\n", "comments": ["I use the ProfilerHook in the following way:\r\n```\r\nhooks = [tf.train.ProfilerHook(save_steps=1, output_dir=model_dir)]\r\nestimator.train(..., hooks=hooks)\r\n```\r\nCan also use with evaluate and predict.\r\nThis generates some json files you can visualize with Chrome on chrome://tracing/", "Thanks so much!", "Hi ,\r\nI tried the method given by @jnd77  with  predict ,but it give the same error as [https://stackoverflow.com/questions/53828373/profiling-tensorflow-estimators](url)\r\nIs there any solution?\r\n", "> I use the ProfilerHook in the following way:\r\n> \r\n> ```\r\n> hooks = [tf.train.ProfilerHook(save_steps=1, output_dir=model_dir)]\r\n> estimator.train(..., hooks=hooks)\r\n> ```\r\n> \r\n> Can also use with evaluate and predict.\r\n> This generates some json files you can visualize with Chrome on chrome://tracing/\r\n\r\nHey, I am able to generate the traces like this, but the program terminates with a ValueError saying that the given tag was already used for this event type (the trace is already written before this exception is raised, so I am able to view the trace). This happens only when I pass the hook in estimator.eval(), works fine with train. \r\n\r\nWhat I really wanted was to generate tensorboard visualisation and the traces with the same code, but if I add the hook, then the tensorboard visualisation is not generated due to the above error.\r\n\r\nFinally (not related to the above problem), I am not sure if this is even possible, but can we see these times inside of the tensorboard visualisation? If I add this hook in training, I see the time, but that is somehow a cumulative wall-time stamp, and not a per node run-time measure.", "> > I use the ProfilerHook in the following way:\r\n> > ```\r\n> > hooks = [tf.train.ProfilerHook(save_steps=1, output_dir=model_dir)]\r\n> > estimator.train(..., hooks=hooks)\r\n> > ```\r\n> > \r\n> > \r\n> > Can also use with evaluate and predict.\r\n> > This generates some json files you can visualize with Chrome on chrome://tracing/\r\n> \r\n> Hey, I am able to generate the traces like this, but the program terminates with a ValueError saying that the given tag was already used for this event type (the trace is already written before this exception is raised, so I am able to view the trace). This happens only when I pass the hook in estimator.eval(), works fine with train.\r\n> \r\n> What I really wanted was to generate tensorboard visualisation and the traces with the same code, but if I add the hook, then the tensorboard visualisation is not generated due to the above error.\r\n> \r\n> Finally (not related to the above problem), I am not sure if this is even possible, but can we see these times inside of the tensorboard visualisation? If I add this hook in training, I see the time, but that is somehow a cumulative wall-time stamp, and not a per node run-time measure.\r\n\r\nIt worked user with train.But when i use with evaluate and predict,like this:\r\n```\r\nhooks = [tf.train.ProfilerHook(save_steps=1, output_dir=model_dir)] \r\nestimator.predict(..., hooks=hooks)\r\nestimator.evaluate(..., hooks=hooks)\r\n```\r\n\r\nraise ValueError\r\nValueError: The provided tag was already used for this event type\r\n\r\n", "@buracagyang , I got no help on this, so I decided to use a hook I found somewhere on SO (it has been a while, I don't remember who posted it originally). The error seems to be valid, there is indeed some issue with updating the run tag in eval mode. Anyway, the custom hook worked and produced the traces during training and eval both. Here is the code for the hook:\r\n\r\n```python\r\n 817 class MetadataHook(SessionRunHook):\r\n 818     def __init__(self, save_steps=None, save_secs=None, output_dir=\"\"):\r\n 819         self._output_tag = \"blah-{}\"\r\n 820         self._output_dir = output_dir\r\n 821         self._timer = SecondOrStepTimer(every_secs=save_secs,\r\n 822                                         every_steps=save_steps)\r\n 823         self._atomic_counter = 0\r\n 824 \r\n 825     def begin(self):\r\n 826         self._next_step = None\r\n 827         self._global_step_tensor = training_util.get_global_step()\r\n 828         self._writer = tf.summary.FileWriter(self._output_dir,\r\n 829                                              tf.get_default_graph())\r\n 830 \r\n 831         if self._global_step_tensor is None:\r\n 832             raise RuntimeError(\r\n 833                 \"Global step should be created to use ProfilerHook.\")\r\n 834 \r\n 835     def before_run(self, run_context):\r\n 836         self._request_summary = (self._next_step is None\r\n 837                                  or self._timer.should_trigger_for_step(\r\n 838                                      self._next_step))\r\n 839         requests = {}#{\"global_step\": self._global_step_tensor}\r\n 840         opts = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n 841         return SessionRunArgs(requests, options=opts)\r\n 842 \r\n 843     def after_run(self, run_context, run_values):\r\n 844         global_step = self._atomic_counter + 1\r\n 845         self._atomic_counter = self._atomic_counter + 1\r\n 846         if self._request_summary:\r\n 847             tf.logging.error(f'global step is {global_step}, atomic counter is {self._atomic_counter}')\r\n 848             fetched_timeline = timeline.Timeline(run_values.run_metadata.step_stats)\r\n 849             chrome_trace = fetched_timeline.generate_chrome_trace_format()\r\n 850             with open(os.path.join(self._output_dir, f'timeline_{global_step}.json'), 'w') as f:\r\n 851                 f.write(chrome_trace)\r\n 852 \r\n 853             self._writer.add_run_metadata(run_values.run_metadata,\r\n 854                                           self._output_tag.format(global_step))\r\n 855             self._writer.flush()\r\n 856         self._next_step = global_step + 1\r\n 857 \r\n 858     def end(self, session):\r\n 859         self._writer.close()\r\n 860 \r\n```\r\nThen you need to plug this into the `eval` call which is simple:\r\n```python\r\n1146             result = estimator.evaluate(input_fn=eval_input_fn,\r\n1147                                         steps=eval_steps,\r\n1148                                         hooks=[\r\n1149                                             #profile_hook\r\n1150                                             MetadataHook(\r\n1151                                                 save_steps=1,\r\n1152                                                 output_dir=FLAGS.output_dir)\r\n1153                                         ])\r\n```\r\nSorry for the line numbers, it is copy-pasted from vim. This might work with only specific TF versions. I think I tried it with TF1.14 the last time.\r\n\r\nHope this helps! \r\n\r\nAlso, while we are on the topic of profiling, I have some questions about the GPU time-trace. I am not sure how to interpret the run-time measurements of the kernels. The profile under `*/GPU/compute:0` is the real kernel run-time or is it just the time taken to schedule the kernel? \r\nFurthermore, for a sequential network, this would not matter, right? I mean, you can not schedule the matmul kernels for layer `i` unless and until layer `i-1` has finished doing it's matmul. So, unless there are gaps in this timeline (which there aren't), this should be interpreted as the wall-time taken to run the data through this layer, correct?", "@smr97 \r\nHi, I've tried the method you gave in the code, and it works! Thank you very much!"]}, {"number": 24427, "title": "Can Tensorflow restore Graph with gradient flows from checkpoint files?", "body": "\r\nGenerally, checkpoint is used for model evaluation or inference, or fault tolerance. However, can checkpoint file be used to restore training processing, that  is, \r\n\r\n- Load checkpoint file, and build forward and backward graph flow based on these checkpoint files.\r\n- We assume that we have no the model graph python code, just only checkpoint files. \r\n\r\nIn general, we can restore forward flow immediately with inference engine, even with other  inference engine. But we wish to restore training graph with gradient flow. Does it  works? Is it big project or little   code hacking, or already APIs provided? \r\n\r\nThanks a lot. \r\n\r\n\r\n", "comments": ["If it's a reasonable feature request, we would like to engage this mini project. \r\n\r\n- Why we want this feature? \r\n- We want to easy user developmennt for AI model with GUI, because junior user does not want python programming.  We set several objectives for GUI design. Firstly, we want user can import python written model into graph. Secondly, user can hack imported graph with GUI or design graph from scratch with GUI. There is some research on this\uff0c https://arxiv.org/abs/1802.04626\uff0chowever\uff0cwe think\uff0cwe need take advantage of open source community, which has lots of model resource reference design, then we want GUI tool can also take advantage of community model resource, so we need import python code model into graph. To accomplish this objective, we think, checkpoint owns graph arch, and model variables, also we can edit checkpoint graph sometimes with careful designed GUI, then that's useful if tensorflow can restore training procedure from checkpoint without using native tensorflow describe tensorflow graph. Also this feature could help mirgrating training between different training framework. In addition, this can be reachable in theory, we do not know how about code engineering .. \r\n\r\nWish some comments reply, thanks. ", "Hello, you have raised an excellent feature request. Since you want to restore saved checkpoint models but also be able import/integrate arbitrary Python-based model graphs, and then be able to edit them graphically, and you cite the 'Barista' paper as a reference architecture to implement this feature/GUI tool. Please go ahead and _**submit a brief 2-page design document**_ and we shall find it necessitated and get it validated by an appropriate reviewer/owner.\r\n \r\n\r\n", "Thanks for your kind reply. \r\n\r\nWe will try to post some native design ASAP, maybe in weeks. We also want to engaged in related development to acquire more details to  implement it since we think the ability to develop tensorflow deeply will help us make the easy AI more reachable. \r\n\r\nThis feature could help to make different DL training framework to be unified with ONNX or tensorflow checkpoint graph under the easy DL way.\r\n", "The author has confirmed the response, hence this issue is closed.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24426, "title": "where  'tf.image.resize_bilinear' is defined ?", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version:v1.9\r\n- Doc Link:\r\nhttps://tensorflow.google.cn/api_docs/python/tf/image/resize_bilinear\r\n\r\n**Describe the documentation issue**\r\ni want to see the 'tf.image.resize_bilinear' source code.\r\nbut i can not find 'tensorflow/python/ops/gen_image_ops.py'.\r\n\r\n", "comments": ["`gen_image_ops.py` means the Python API has been generated from C++ code. In this case, the op and the kernel are defined respectively at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op.cc", "Closing this issue since @hsgkim has provided the solution. Feel free to reopen if have any further questions. Thanks!", "The link to `resize_bilinear_op.cc` is broken. The updated link - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/resize_bilinear_op.cc  and it's header is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/resize_bilinear_op.h"]}, {"number": 24425, "title": "tf.nn.softmax_cross_entropy_with_logits_v2 changes tensor shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 9.0 / 7.4.1\r\n- GPU model and memory: 1070 Ti / 8GB\r\n\r\nI have a simple network that outputs logits via a dense layer in the model_fn (using estimator API):\r\n```\r\nlogits = tf.layers.dense(inputs=dense1, units=2, activation=tf.nn.relu, name='logits')\r\n```\r\n\r\nI then calculate the loss for the training with:\r\n```\r\nif mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):\r\n  print(logits.shape)\r\n  print_op = tf.print(\"LABELS: \", logits, tf.shape(logits))\r\n  with tf.control_dependencies([print_op]):\r\n    logits = tf.identity(logits)\r\n  loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\r\n  tf.summary.scalar('cross_entropy', loss)\r\n```\r\n\r\nWhich results in an error, as the tensor shapes appear not to match:\r\n```bash\r\nCaused by op 'Reshape_3', defined at:\r\n  File \"train.py\", line 40, in <module>\r\n    fire.Fire(train)\r\n  File \"/usr/local/lib/python3.5/dist-packages/fire/core.py\", line 127, in Fire\r\n    component_trace = _Fire(component, args, context, name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/fire/core.py\", line 366, in _Fire\r\n    component, remaining_args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/fire/core.py\", line 542, in _CallCallable\r\n    result = fn(*varargs, **kwargs)\r\n  File \"train.py\", line 36, in train\r\n    learning.train(model, input, model_name, exp_name, config_file, exp_restore)\r\n  File \"/data/code/bp-labeling/learning/learning/train.py\", line 93, in train\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 610, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 711, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1237, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1195, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/data/code/bp-labeling/learning/learning/model.py\", line 86, in model_fn\r\n    return self.classifier_fn(features, images, labels, mode, logits, params, cls_count, global_step)\r\n  File \"/data/code/bp-labeling/learning/models/sample_net.py\", line 83, in classifier_fn\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op, training_hooks=[hook])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/model_fn.py\", line 201, in __new__\r\n    loss = array_ops.reshape(loss, [])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 6482, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 10 values, but the requested shape has 1\r\n\t [[node Reshape_3 (defined at /data/code/bp-labeling/learning/models/sample_net.py:83)  = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_with_logits/Reshape_2, Reshape_3/shape)]]\r\n\t [[{{node Reshape_3/_397}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_124_Reshape_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\nThe `print` function is only called once with the following output:\r\n```bash\r\nLABELS:  [[0.255024195 0.00465549901]\r\n [0.550343394 0]\r\n [0.0432553887 0]\r\n ...\r\n [0.0975153148 0]\r\n [0 0]\r\n [0 0]] [1 2]\r\n```\r\n(output from `print(logits.shape)` is `(?, 2)`).\r\n\r\nWhich appears strange, as the shape is given as `[1, 2]`, which is obviously not the case.\r\n\r\nIf I change the loss function to:\r\n```\r\nloss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\r\n```\r\n\r\nthe code runs fine and the output of the print command is e.g.:\r\n```bash\r\nLABELS:  [[0 0]\r\n [0 20.4618092]\r\n [0 0]\r\n ...\r\n [0 1.23361325]\r\n [0 0]\r\n [0 0]] [10 2]\r\n```\r\n\r\nAm I missing something or is this a bug?", "comments": ["Hello @felixnext , Can you please study this Stack Overflow post that exactly addresses your query:\r\nhttps://stackoverflow.com/questions/41412335/tf-nn-softmax-cross-entropy-with-logits-error-logits-and-labels-must-be-same\r\nPlease let us know if you are satisfied the various solutions provided in the post. Thanks.\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24424, "title": "tf.contrib.ffmpeg: No mention of alternatives after deprecation", "body": "**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/contrib/ffmpeg/decode_audio\r\n\r\n**Describe the documentation issue**\r\nIs there a plan to replace `tf.contrib.ffmpeg`? I currently rely on it in a tf.data pipeline and didn't think it was clear from the docs if there is or is not a plan on replacing the functionality. I would appreciate some clarification in the docs on that matter.\r\n\r\nE.g. the [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) seems to use [audio_ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/audio_ops.py) which AFAIK is completely missing from the docs. Is this perhaps meant to replace (some of) the functionality of `tf.contrib.ffmpeg`, or is it expected that all of the audio decoding functionality will be phased out from tensorflow?\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nYes, if clear update recommendations are provided.", "comments": ["@agrinh The tf.contrib.ffmpeg is getting deprecated in tensorflow. However, tensroflow' SIG IO (https://github.com/tensorflow/io) is planing adding support for import video/audio to tf's Dataset. The sig-io itself consists of a collection of extended data ops that could be used in tensorflow.\r\n\r\nThere is already a PR in sig-io's repo tensorflow/io#30 which adds a `VideoDataset`.\r\n\r\nPlease take a look if interested.", "@agrinh I will close this issue for now. Please open issues in https://github.com/tensorflow/io for additional tensorflow dataset support.", "Thanks for the update @yongtang"]}, {"number": 24423, "title": "Difference in performance of Python and cpp libraries:", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.8\r\n- Python version:3.5.0\r\n- Bazel version (if compiling from source):1.19\r\n- GCC/Compiler version (if compiling from source):msvc 2015\r\n- CUDA/cuDNN version:No\r\n- GPU model and memory:No\r\n\r\n\r\nIssue:\r\nThe issue is related to the performance of tensorflow on Cpp and python\r\nI have installed tensorflow(1.8) on both windows and python (binary)\r\nI am using tensorflow to predict the score of any image using a model as .pb files\r\nBut the time taken by tensorflow graph function in python is about 50ms and \r\nthat of cpp is about 500ms.\r\nI had assumed that cpp would be more faster but the results were opposite.\r\nWhat can i do to increase the performace of the tensorflow in winodows.\r\nI am using the pre-built binary in windows build using cmake release mode.\r\n", "comments": ["@HackersSpirit Check if your Cpp version uses the debug build configuration instead of the release.", "@rootkitchao : I have downloaded the release version the binary.\r\nPlease refer the link for source\r\nhttps://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.8.0/cpp\r\n", "Guys it was mistake from my end.\r\nActually i was so involved in the issue of building library and sample that i almost forgot \r\nto optimize somepart of the code.\r\n@asimshankar : Can this issue can be deleted as it is not issue it was issue at my end."]}, {"number": 24422, "title": "Rprop Optimizer", "body": "Rprop was originally proposed by Riedmiller and Braun in the paper [A direct adaptive method for faster backpropagation learning: the RPROP algorithm](https://doi.org/10.1109/ICNN.1993.298623)\r\n\r\nThere are two variants implemented in this contribution:\r\n\r\n1. iRprop+ which is described in the article\r\n  [Empirical evaluation of the improved Rprop learning algorithms](https://doi.org/10.1016/S0925-2312(01)00700-7)\r\n\r\n2. Rprop-  which is described in Riedmiller's article\r\n  [Advanced supervised learning in multi-layer perceptrons \u2014 From backpropagation to adaptive learning algorithms](https://doi.org/10.1016/0920-5489(94)90017-5), and is referred to as Rprop- (Rprop without weight-backtracking) in the\r\n  article [Empirical evaluation of the improved Rprop learning algorithms](https://doi.org/10.1016/S0925-2312(01)00700-7).\r\n\r\n\r\n\r\n\r\nThe TensorFlow implementation is described in the article\r\n  [Resilient Backpropagation (Rprop) for Batch-learning in TensorFlow](https://openreview.net/forum?id=r1R0o7yDz)\r\n\r\nrecreated from: https://github.com/tensorflow/tensorflow/pull/20918", "comments": ["Any updates on the integration of this optimizer?\r\n", "Please resend this as a pull request to tensorflow/addons\n\nOn Tue, Feb 26, 2019 at 9:18 AM kariya2 <notifications@github.com> wrote:\n\n> Any updates on the integration of this optimizer?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24422#issuecomment-467527726>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxeY8kNKhJagulQEWdij1gVOkp6ozks5vRWxvgaJpZM4ZYEaw>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp  apologies for the confusion, that was more addressed to @ciprianflow.  I would gladly submit a PR but am not extremely familiar with the changes. This optimizer could be very useful for my data and so I was wondering if he had any plans to submit the PR to add ons repo", "Hello @kariya2 , yes I am still planning on adding Rprop to TF, I will submit a PR soon.", "@ciprianflow -- I reckon this is not worked on anymore?"]}, {"number": 24421, "title": "Retrain Model & Serv it", "body": "Dear Forum Users,\r\n\r\ni was creating a Model with retrain.py on Ubuntu 18.04 in a virtual Machine on my Server (https://www.tensorflow.org/hub/tutorials/image_retraining)\r\nTensorflow is installed via pip installtion. I'm only using CPU.\r\n\r\nafter testing it with the python label_image.py it works everthing fine and the detection works great!\r\n\r\nso i try'd to serve my own model with the tensorflow-serving (https://github.com/tensorflow/serving.git).\r\n\r\non the github serving-example the execution of the model \"half_plus_two\" works also pretty well and i was able to connect via the RESTful API.\r\n\r\nthen i try'd to host my own model (out of the /tmp folder from the retrain.py)\r\ni copy'd/renamed the files from the /tmp folders to the structure of the \"half plus two\" model, like in the example \r\n\"/saved_model_half_plus_two_cpu/\" and \"/half_plus_two/\".\r\n\r\nwhen i try to start the serving via the following cmd:\r\ndocker run -t --rm -p 8561:8561 \\\r\n   -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" \\\r\n   -e MODEL_NAME=half_plus_two \\\r\n   tensorflow/serving &\r\n\r\nthe application says: \"No versions of servable half_plus_two found under base path /models/half_plus_two\"\r\ncould anyone help?\r\n\r\nthank you and regards\r\n\r\n", "comments": ["This issue is more suitable on TensorFlow Serving repo. Please post it on tensorflow/serving from [here](https://github.com/tensorflow/serving/issues). Thanks!"]}, {"number": 24420, "title": "TF doesn't build for Raspberry Pi", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 4.15 x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source, building ci_build.sh from TF distro\r\n- TensorFlow version: latest from master\r\n- Python version: tried both 2.7 and 3\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): tried both 0.20.0 and 0.19.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen following instructions at https://www.tensorflow.org/install/source_rpi to cross-compile TF for RPi, the build fails, see log below.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. clone TF source\r\n2. `tensorflow/tools/ci_build/ci_build.sh PI  tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`\r\n3. see the error below\r\n\r\nI found following issue but downgrading bazel to 0.19.2 didn't help: https://github.com/tensorflow/tensorflow/issues/24124\r\n\r\nAnother issue: from build log I see that armv6 was used to build BLAS, but I would expect armv7 instead for RPi 3, and tensorflow/tools/ci_build/pi/build_raspberry_pi.sh should use it. What's going on?\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n OpenBLAS build complete. (BLAS CBLAS)\r\n\r\n  OS               ... Linux             \r\n  Architecture     ... arm               \r\n  BINARY           ... 32bit                 \r\n  C compiler       ... GCC  (command line : /tmp/toolchain_install//tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc)\r\n  Library Name     ... libopenblas_armv6p-r0.3.1.dev.a (Multi threaded; Max num-threads is 4)\r\n\r\nTo install the library, you can run \"make PREFIX=/path/to/your/installation install\".\r\n\r\nmake -j 4 -f Makefile.install install\r\nmake[1]: Entering directory `/tmp/openblas_src'\r\nGenerating openblas_config.h in /tmp/openblas_install//include\r\nGenerating f77blas.h in /tmp/openblas_install//include\r\nGenerating cblas.h in /tmp/openblas_install//include\r\nCopying the static library to /tmp/openblas_install//lib\r\nCopying the shared library to /tmp/openblas_install//lib\r\nGenerating openblas.pc in /tmp/openblas_install//lib/pkgconfig\r\nGenerating OpenBLASConfig.cmake in /tmp/openblas_install//lib/cmake/openblas\r\nGenerating OpenBLASConfigVersion.cmake in /tmp/openblas_install//lib/cmake/openblas\r\nInstall OK!\r\nmake[1]: Leaving directory `/tmp/openblas_src'\r\nBuilding for the Pi Two/Three, with NEON acceleration\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: f1dbc59c-324c-431f-b0eb-408f1a81c125\r\nLoading: \r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nERROR: cc_toolchain_suite '@local_config_arm_compiler//:toolchain' does not contain a toolchain for CPU 'armeabi', you may want to add an entry for 'armeabi|compiler' into toolchains and toolchain_identifier 'arm-linux-gnueabihf' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).\r\n```\r\n\r\n```\r\n$ docker --version\r\nDocker version 18.04.0-ce, build 3d479c0af6\r\n```", "comments": ["Also I have a question: tensorflow/tools/ci_build/ci_build.sh deletes the docker container, so I have to wait each time while the CI container is built. Is there any regular way to work with TF CI container, so I could re-use it instead building each time anew?\r\n\r\nI see there're docker images in TF source tree, but it's not quite clear how to apply tensorflow/tools/ci_build/pi/build_raspberry_pi.sh there. A link to docs will do. \r\n\r\nTIA", "@mixaz, it seems master is now building fine for RPI via cross compiling. I worked for me at commit `92b598a32d`", "I got the same issue\r\n```\r\n  OS               ... Linux             \r\n  Architecture     ... arm               \r\n  BINARY           ... 32bit                 \r\n  C compiler       ... GCC  (command line : /tmp/toolchain_install//tools/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc)\r\n  Library Name     ... libopenblas_armv6p-r0.3.1.dev.a (Multi threaded; Max num-threads is 8)\r\n\r\nTo install the library, you can run \"make PREFIX=/path/to/your/installation install\".\r\n\r\nmake -j 8 -f Makefile.install install\r\nmake[1]: Entering directory `/tmp/openblas_src'\r\nGenerating openblas_config.h in /tmp/openblas_install//include\r\nGenerating f77blas.h in /tmp/openblas_install//include\r\nGenerating cblas.h in /tmp/openblas_install//include\r\nCopying the static library to /tmp/openblas_install//lib\r\nCopying the shared library to /tmp/openblas_install//lib\r\nGenerating openblas.pc in /tmp/openblas_install//lib/pkgconfig\r\nGenerating OpenBLASConfig.cmake in /tmp/openblas_install//lib/cmake/openblas\r\nGenerating OpenBLASConfigVersion.cmake in /tmp/openblas_install//lib/cmake/openblas\r\nInstall OK!\r\nmake[1]: Leaving directory `/tmp/openblas_src'\r\nBuilding for the Pi Two/Three, with NEON acceleration\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n/workspace/tools/bazel.rc\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n  Inherited 'common' options: --color=yes\r\nINFO: Reading rc options for 'build' from /etc/bazel.bazelrc:\r\n  'build' options: --verbose_failures --spawn_strategy=standalone --genrule_strategy=standalone\r\nINFO: Reading rc options for 'build' from /workspace/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages --python_path=/usr/bin/python --define with_ignite_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0\r\nERROR: Config value monolithic is not defined in any .rc file\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 98fabce6-d370-44be-870d-78da9b5c6ace\r\n```", "@anilknayak are you sure it is the same error? We have different error messages, and yours  seems to be related to #24104. Seems that the issue was caused by bazel 0.20, downgrading to 0.19 shall help\r\n\r\nAnybody can close this issue? Just I'm building with tensorflow-on-arm and testing if master head builds will take some time... I mean closed by those who tested )", "I am able to build libtensorflow.so for raspberry pi. I have summarized my approach after searching some issue in github tensorflow. Check otu the stackoverflow link and vote \r\n\r\nhttps://stackoverflow.com/questions/53874017/error-while-building-libtensorflow-so-on-raspberry-pi/54204267#54204267", "Tested - RPi libraries are built OK on the latest HEAD. Closing the ticket."]}, {"number": 24419, "title": "Import Custom C++ Op Error: dynamic module does not define module export function", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4 LTS server\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip3\r\n- GPU model and memory: GTX 1080 12G\r\n\r\n\r\n**Describe the problem**\r\n\r\nI write a custom op by C++ which can work by `tf.load_op_library`. But furthermore, I want to import the function encapsulating the op, which results in `ImportError: dynamic module does not define module export function`. So, how can I import the python function or module which depends on the custom Op?\r\nThank you very much.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nCustom Op C++ file `remap.cc`:\r\n```\r\n#include <unordered_map>\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"Remap\")\r\n.Input(\"input: int32\")\r\n.Input(\"base: int32\")\r\n.Output(\"output: int32\")\r\n.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n  ::tensorflow::shape_inference::ShapeHandle input;\r\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &input));\r\n  c->set_output(0, c->input(0));\r\n  return Status::OK();\r\n});\r\n\r\n\r\nclass RemapOp : public OpKernel {\r\npublic:\r\n  explicit RemapOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n    const Tensor& base_tensor = context->input(1);\r\n    auto base = base_tensor.flat<int32>();\r\n\r\n    // Build hash table\r\n    std::unordered_map<int32, int32> map;\r\n    const int base_N = base.size();\r\n    for (int i = 0; i < base_N; i++) {\r\n      map[base(i)] = i;\r\n    }\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n          &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n\r\n    // Set all elements of output tensor\r\n    const int N = input.size();\r\n    for (int i = 0; i < N; i++) {\r\n      output_flat(i) = map.at(input(i));\r\n    }\r\n  }\r\n};\r\n```\r\n\r\nmakefile:\r\n```\r\nTF_CFLAGS = $(shell python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))')\r\nTF_LFLAGS = $(shell python -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))')\r\n\r\nall:\r\n  g++ -std=c++11 -shared remap.cc -o remap.so -fPIC ${TF_CFLAGS} ${TF_LFLAGS} -O2\r\n```\r\n\r\nPython function encapsulating the Op `remap.py`:\r\n```\r\nimport tensorflow as tf\r\n\r\nremap_module = tf.load_op_library('./remap.so')\r\nremap = remap_module.remap\r\n\r\ndef foo(a):\r\n  ret = remap(a)\r\n  # Do something about ret\r\n  return ret\r\n```\r\n\r\nPython `main.py`\r\n```\r\nimport tensorflow as tf\r\nfrom remap import foo\r\n\r\nprint(foo([1, 2, 3], [1, 2, 3, 4, 5]))\r\n```\r\n\r\nCompile the Op by makefile and then run `python main.py`, then we have the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 2, in <module>\r\n    from remap import foo\r\nImportError: dynamic module does not define module export function (PyInit_remap)\r\n```", "comments": ["@aselle \r\nBesides, could you please tell me how to implement the function `remap(input, base)`, which replaces each element in `input` to the indices of `base` by the existing ops?\r\nFor example, `remap([[3, 2, 5], [6, 0, 2]], [0, -1, -2, -3, -4, -5, -6])` returns `[[-3, -2, -5], [-6, 0, -2]]`.\r\nThe tensor `base` could be large, so we don't want to compare all pairs of these two tensors. On the other hand, `tf.contrib.lookup.HashTable` seems improper owing to keeping the resources in memory among all sessions.", "@jinze1994 change remap.py to another name (eg. remap_wrap.py) works for me", "@chasex Exactly, thank you very much."]}, {"number": 24418, "title": "Fix-Bug: CHECK_GT(2^32, 0) return false", "body": "it seems that 'const size_t uval = (size_t)((unsigned)v1); ' make the unsigned long cast to unsigned int sometimes.\r\nit is a bug when I new a Tensor with 2^32 totalBytes and try to save/restore from ckpt.\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/core/util/tensor_bundle/tensor_bundle.cc#L200", "comments": ["Thanks!"]}, {"number": 24417, "title": "Tensorflow Lite Dockerfile Error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nTensorflow is being installed from the Dockerfile according to https://github.com/tensorflow/models/blob/master/research/object_detection/dockerfiles/android/README.md. I am using CPU - NO GPU. I use Ubuntu 16.04.\r\n\r\n**Describe the problem**\r\nWhen I run \"docker build --tag detect-tf .\", I get the below error in the terminal. How can I fix it?\r\n\r\nStep 3/21 : RUN export CLOUD_SDK_REPO=\"cloud-sdk-$(lsb_release -c -s)\" &&     echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     apt-get update -y && apt-get install google-cloud-sdk -y\r\n ---> Running in e985c9945ecd\r\ndeb http://packages.cloud.google.com/apt cloud-sdk-bionic main\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key output should not be parsed (stdout is not a terminal)\r\n100  1326  100  1326    0     0   5715      0 --:--:-- --:--:-- --:--:--  5715\r\ngpg: failed to start agent '/usr/bin/gpg-agent': No such file or directory\r\ngpg: can't connect to the agent: No such file or directory\r\ngpg: failed to start agent '/usr/bin/gpg-agent': No such file or directory\r\ngpg: can't connect to the agent: No such file or directory\r\nThe command '/bin/sh -c export CLOUD_SDK_REPO=\"cloud-sdk-$(lsb_release -c -s)\" &&     echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list &&     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     apt-get update -y && apt-get install google-cloud-sdk -y' returned a non-zero code: 2\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the steps from https://github.com/tensorflow/models/blob/master/research/object_detection/dockerfiles/android/README.md.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you able to remove this step in the Dockerfile and manage to build the Docker container?", "There are much more problems in other steps in the Dockerfile too. But do you know how I could fix this step first?", "This is an old issue but for people that are still running into this, add this before the glcoud install step:\r\n```\r\nRUN apt-get update && apt-get -y install gpg-agent\r\n\r\n```", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 24416, "title": "remove redundant entries in kernels:BUILD for no_mkldnn_contraction_k\u2026", "body": "\u2026ernel\r\n\r\nAddresses #24414 \r\n\r\nSince `tensorflow_mkldnn_contraction_kernel` is set to `0` on line 104, we end up with multiple successful matches in these `select` structures. The architecture specific entries are currently not needed. If there is a chance that `tensorflow_mkldnn_contraction_kernel` will be set to 1 in the build config setting default, then we'll need a more robust solution.\r\n\r\nThis fixes a current build break on ppc64le community builds.", "comments": ["I was hoping bazel-skylark `selects:with_or` would help here, but that still doesn't solve the problem of two of the keys resolving as true in the select statement.  \r\nPerhaps a separate rule altogether for the arch check?", "Unfortunately this is not going to work for us internally. I'm surprised that it passed our integration tests in Kokoro. What version of bazel you are using?", "Bazel 0.19.2\r\n\r\nSince it's hard to tell what will work for you internally, this PR was mostly for raising attention for this issue (https://github.com/tensorflow/tensorflow/issues/24414) and/or getting a quick fix in to get non-intel builds going again. It just partially reverts this commit (https://github.com/tensorflow/tensorflow/commit/10ef7edc881ee715eaae48656fcb431fe128441f) which includes an insufficient bazel select statement where more than one entry will wind up as true for non-intel platforms. I'm trying a few other things (like the aforementioned `selects:with_or` from bazel-skylark), but don't have a proper fix yet.\r\n\r\nAny suggestions you have would be great!", "Another option is to remove `build --define=tensorflow_mkldnn_contraction_kernel=0` from .bazelrc. That's basically our plan.", "@penpornk Thanks! I was reworking this PR, but if you have a fix I can wait for that if it's only a few days away. I'll keep this PR open until then.\r\n\r\n@ezhulenev Yeah. But you wouldn't accept a PR from me to make that change, would you? (I'd be happy to push one :). My goal is to fix the nightly builds on non-intel arch's.", "@jayfurmanek Sorry for the delay, and Happy New Year!\r\nWe tried to fix this with https://github.com/tensorflow/tensorflow/commit/20796e6bebb3a411655157f7ee7201af58a7d2fe on Dec 21 but it got rolled back. Just tried again with https://github.com/tensorflow/tensorflow/commit/7c9323bedc48c98be3c07b72ec1d6f4dccdefb35 yesterday and it seems it will stay. Please let us know if this solves your problem!\r\n\r\n\r\n\r\n\r\n", "This looks like it should work. Thanks."]}, {"number": 24415, "title": "[Intel MKL]Bug fix for redundant transpose removal", "body": "1. this bug is a last minute check-in to replace \"CHECK()\" with \"DCHECK()\", however it turns out that \"CHECK()\" and \"DCHECK()\" are not functional equivalent;\r\n2. unlike \"CHECK()\" whose expression will always be excuted, the expression in \"DCHECK()\" will not be executed in release mode. This causes dangling pointer failures.\r\n3. to fix this bug, we should check the result of the expression instead of the expression itself.", "comments": ["@penpornk No worries. I think I'm also responsible for this failure, for not run UT on this commit.\r\n\r\nAnyway, changes applied according to your requests, please check it if you have time."]}]