[{"number": 28445, "title": "Load Images", "body": "There is some problem in loading images \r\n\r\nfor n in range(3):\r\n  image_path = random.choice(all_image_paths)\r\n  display.display(display.Image(image_path))\r\n  print(caption_image(image_path))\r\n  print()\r\n\r\nError:\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-9-dd286b0cbeda> in <module>\r\n      2   image_path = random.choice(all_image_paths)\r\n      3   display.display(display.Image(image_path))\r\n----> 4   print(caption_image(image_path))\r\n      5   print()\r\n\r\n<ipython-input-8-a5b6e935786e> in caption_image(image_path)\r\n      3 def caption_image(image_path):\r\n      4     image_rel = pathlib.Path(image_path).relative_to(data_root)\r\n----> 5     return \"Image (CC BY 2.0) \" + ' - '.join(attributions[str(image_rel)].split(' - ')[:-1])\r\n      6 \r\n\r\nKeyError: 'daisy\\\\3764116502_f394428ee0_n.jpg'\r\n\r\nPlease what is the problem?\r\n\r\n\r\n<em>Thanks so much for taking the time to file a documentation issue and even\r\nmore thanks if you intend to contribute to updating it! Please do introduce\r\nyourself on our mailing list with\r\n[Google Groups](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs)\r\nor [email](mailto:docs@tensorflow.org) and let us know if you have any\r\nquestions. We also encourage you to review our\r\n[Documentation Contributor Guide](https://www.tensorflow.org/community/contribute/docs).\r\nAs a side note, per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:doc_template</em>\r\n\r\n## Existing URLs containing the issue:\r\n\r\nLink to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Correct Links\r\n\r\nIs the link pointing to the source code correct? To find the source code, use\r\n`git grep my_method` from the git command line in your locally checked out\r\nrepository.\r\n\r\n### Clear Description\r\n\r\nWhy should someone use this method? How is it useful?\r\n\r\n### Usage Example\r\n\r\nIs there a usage example?\r\n\r\n### Parameters Defined\r\n\r\nAre all arguments that can be passed in defined and formatted correctly?\r\n\r\n### Returns Defined\r\n\r\nAre return values defined?\r\n\r\n### Raises Listed and Defined\r\n\r\nAre errors defined?\r\n[Example](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises)\r\n\r\n### Request Visuals, if Applicable\r\n\r\nAre there currently visuals? If not, would they make the content clearer?\r\n\r\n### Submit PR?\r\n\r\nAre you planning to also submit a\r\n[Pull Request](https://help.github.com/en/articles/about-pull-requests) to fix\r\nthis issue? See the\r\n[Documentation Contributor Guide](https://www.tensorflow.org/community/contribute/docs)\r\nthe\r\n[Documentation Style Guide](https://www.tensorflow.org/community/contribute/docs_style).\r\n", "comments": ["@iqbalhabibie I clicked on the link provided (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod) , it says 404 error on page. ", "@iqbalhabibie Thanks for finding this link that is not working. On which page, you found the link? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 28444, "title": "tf.signal.inverse_stft AttributeError: 'int' object has no attribute 'value' in 2.0.0-alpha0", "body": "There appears to be a bug in tf.signal.inverse_stft, when testing for `real_frames.shape[-1].value is None`, where `real_frames.shape[-1]` is an integer, and does not have a `value`. \r\n\r\nReproducible code:\r\n```\r\nprint(tf.__version__)\r\nframe_length = 512\r\nframe_step = 256\r\nsignal = tf.random.uniform(shape=(1000,))\r\nx = tf.signal.stft(signal, frame_length, frame_step)\r\ny = tf.signal.inverse_stft(x, frame_length, frame_step)\r\n```\r\n\r\n```\r\n2.0.0-alpha0\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-261-d4526d62007f> in <module>\r\n----> 1 tf.signal.inverse_stft(x, frame_length, frame_step)\r\n\r\n/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/ops/signal/spectral_ops.py in inverse_stft(stfts, frame_length, frame_step, fft_length, window_fn, name)\r\n    242     if (frame_length_static is None or\r\n    243         real_frames.shape.ndims is None or\r\n--> 244         real_frames.shape[-1].value is None):\r\n    245       real_frames = real_frames[..., :frame_length]\r\n    246       real_frames_rank = array_ops.rank(real_frames)\r\n\r\nAttributeError: 'int' object has no attribute 'value'\r\n```", "comments": ["The issue is the usage of `real_frames.shape[-1].value`. In TF 1.x `real_frames.shape` returns a list of `Dimensions` which does have the value. In TF 2.0 `real_frames.shape` returns int so `value` is not applicable any more. Created a PR #28461 for the fix.", "Thanks for the report @timsainb, and thanks for the fix @yongtang. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28444\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28444\">No</a>\n"]}, {"number": 28443, "title": "2nd python can't run tensorflow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):tensorflow.org\r\n- TensorFlow version:1.13.1\r\n- Python version:3.5\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:1070\r\n\r\n**Describe the problem**\r\nWhile import tensorflow as tf in jupyter notebook. It's my second python in my system. The first one can run tf ok.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`import tensorflow as tf`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n`---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\RQPro\\rqalpha\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\RQPro\\rqalpha\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-41389fad42b5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\nC:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\nC:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nC:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\RQPro\\rqalpha\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\RQPro\\rqalpha\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\RQPro\\rqalpha\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\u200b\r\n\r\n\u200b`\r\n", "comments": ["TensorFlow supports CUDA 10.0 (TensorFlow >= 1.13.0). Please refer this [link](https://www.tensorflow.org/install/gpu#software_requirements) and let us know in case this does not help. Thanks!", "yes, I uninstalled 10.1 and installed 10.0. Then it works fine. But the odd thing is my anaconda-python 3.7.4 works with tensorflow-gpu 1.13.1 and CUDA 10.1 fine.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28443)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28443)\r\n"]}, {"number": 28442, "title": "expected conv2d_input to have 4 dimensions", "body": "Hello\r\n\r\nI'm on tensorflow and i'm trying to Machine Learning but i got this error here:\r\nValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (24946, 50, 50)\r\n\r\n```\r\nImage_Size is: 50x50\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pickle\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\r\n\r\n\r\npickle_ind = open(\"x.pickle\", \"rb\")\r\nx = pickle.load(pickle_ind)\r\nx = np.array(x, dtype=float)\r\n# x = x/255.0\r\n\r\npickle_ind = open(\"y.pickle\", \"rb\")\r\ny = pickle.load(pickle_ind)\r\n\r\nn_batch = len(x)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 1)))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D((2, 2)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x, y, epochs=20, batch_size=n_batch)\r\n```", "comments": ["i got it:\r\nhttps://stackoverflow.com/questions/56008114/tensorflow-expected-conv2d-input-to-have-4-dimensions", "@007fred50 As you got the stackoverflow link to resolve this issue. We will close this issue ."]}, {"number": 28441, "title": "Shape errors occur after compiling keras model with tf.keras.losses.CategoricalCrossentropy", "body": "**System information**\r\n- OS Platform and Distribution:  Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.3\r\n\r\n**Describe the current behavior**\r\nIf I want to specify, for example, a reduction method of the loss function, I will need to explicitly create an instance of `tf.keras.losses.CategoricalCrossentropy` and pass it to `model.compile()` instead of passing the `\"categorical_crossentropy\"` keyword. However, compiling a model with an instance of `tf.keras.losses.CategoricalCrossentropy` results in the following shape error when calling `model.fit()` afterwards:\r\n```\r\nInvalidArgumentError: logits and labels must have the same first dimension, got logits shape [1000,10] and labels shape [10000]\r\n\t [[{{node loss_18/dense_37_loss/CategoricalCrossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\r\n\r\n\r\n```\r\n**Describe the expected behavior**\r\nI was expecting similar behavior for both `\"categorical_crossentropy\"` and `tf.keras.losses.CategoricalCrossentropy`.\r\n\r\n**Code to reproduce the issue**\r\nA short MNIST example. I realized that removing the `to_categorical` transformation resolves the shape errors but the model does not learn properly anymore.\r\n```\r\nfrom tensorflow.python.keras.datasets import mnist\r\nfrom tensorflow.python.keras.utils import to_categorical\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n\r\nX_train, X_test = X_train.astype(float)/255, X_test.astype(float)/255\r\nX_train, X_test = X_train.reshape(len(X_train),28,28,1), X_test.reshape(len(X_test),28,28,1)\r\ny_train, y_test = to_categorical(y_train), to_categorical(y_test)\r\n\r\n# model definition\r\nmodel = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(16, 8,\r\n                             strides=2,\r\n                             padding='same',\r\n                             activation='relu',\r\n                             input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPool2D(2, 1),\r\n      tf.keras.layers.Conv2D(32, 4,\r\n                             strides=2,\r\n                             padding='valid',\r\n                             activation='relu'),\r\n      tf.keras.layers.MaxPool2D(2, 1),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(32, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation=\"softmax\")\r\n  ])\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\r\n\r\n# this causes errors\r\nloss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\r\n\r\n# using the keyword, everything works\r\n# loss = \"categorical_crossentropy\" \r\n\r\n# Compile model with Keras\r\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['categorical_accuracy'])\r\n\r\n# Train model with Keras\r\nmodel.fit(X_train, y_train, epochs=5, batch_size=1000,\r\n          validation_data=(X_test, y_test), verbose=2)\r\n\r\n```\r\nThank you!", "comments": ["@philgras  : I was able to replicate the output with the code snippet provided on Colab (TensorFlow version 1.13.1).", "@philgras I think this is intended behavior. `loss = \"categorical_crossentropy\"`  expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If you use `loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)` , it was expecting integer classes for targets. shape of the `Targets (y_train, y_test)` caused the error. Thanks!", "@philgras I tried running the sample code snippet in TF 1.13.1 and was not able to repro the difference in behavior between using `tf.keras.losses.CategoricalCrossentropy(from_logits=False)` and `\"categorical_crossentropy\"`. ", "@pavithrasv Here is the Github [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/4a3250db8ba13303ecc968b225344d9e/untitled155.ipynb) that works without any issue. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/86caa7fd84915e2645316c5ccab56a59/untitled155.ipynb) that throws an error. Between the two gists, there was only one line change. In the second gist, `tf.keras.losses.CategoricalCrossentropy` is expecting integer classes as input but categorical classes were given as input. Thanks!", "Thank you @jvishnuvardhan. I see the issue now. Looks like it has been fixed already as it does not repro in TF 2.0 alpha or the nightly release. \r\n", "Closing the PR now, please feel free to re-open if you see that the issue is not fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28441\">No</a>\n"]}, {"number": 28440, "title": "Improper output shape of DenseFeatures layer with numeric_column ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip I think, I don't remember and I don't know the difference\r\n- TensorFlow version: v1.12.0-10066-g5cbe8af8ed 2.0.0-dev20190313\r\n- Python version: 3.6.7\r\n\r\n**Describe the problem**\r\n\r\nI am using the `DenseFeatures` layer feeded with a feature `numeric_column` with a shape argument of `shape=(3,1)`. According to [the documentation of `numeric_column`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/numeric_column?hl=en), the output tensor has shape [batch_size] + `shape`. My batch size is 2 and instead of having shape `(2,3,1)` as indicated by the documentation, my output tensor only has output shape of `(2,3)`.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nHere is a minimal working example reproduccing the issue:\r\n\r\n```\r\nfrom tensorflow.data import Dataset\r\nfrom tensorflow.feature_column import numeric_column\r\nfrom tensorflow.keras.layers import DenseFeatures\r\n\r\nraw_dataset = {'feature1': [[3., 4., 6.], [2., 12., 7.]]}\r\ndataset = Dataset.from_tensor_slices(raw_dataset)\r\nbatch_size = 2\r\nbatched = dataset.batch(batch_size)\r\nfeature_column = numeric_column('feature1', shape=(3,1))\r\nlayer = DenseFeatures(feature_column)\r\nfor y in batched:\r\n    pass\r\nlayer(y)\r\n```\r\n\r\nthe output of which is\r\n\r\n```\r\n<tf.Tensor: id=19, shape=(2, 3), dtype=float32, numpy=\r\narray([[ 3.,  4.,  6.],\r\n       [ 2., 12.,  7.]], dtype=float32)>\r\n```", "comments": ["@durandg12 : I was able to reproduce the output with TensorFlow version 2.0.0-dev20190313 and 2.0.0-alpha0.", "The issue is still here in v1.12.1-3259-gf59745a381 2.0.0-beta0\r\n\r\n", "The issue is still here in tf2.0.0.", "The output is correct, and this is not an issue. The [doc](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/numeric_column?hl=en#args) on  `shape` arg in `numeric_column` means the shape of input tensor to the column (indexed by the `key` arg), not the output one. You may check the docstring of the args `shape` and `default_value` for more details. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28440\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28440\">No</a>\n"]}, {"number": 28439, "title": "Tensorflow 1.12.2 a typo for 1.13.2?", "body": "I note there was 1.12.0, but no 1.12.1.\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases has 1.12.2.", "comments": ["No, the vulnerability was fixed on master for 1.13 but we also made a patch release for 1.12. There is no 1.12.1 as that tag exists on master instead of on the r1.12 branch.", "> There is no 1.12.1 as that tag exists on master instead of on the r1.12 branch.\r\n\r\nGiven the v1.12.1 tag on the releases page this is really confusing.\r\n\r\nTF Bazel build is currently broken and it's really hard to find out whether there is any release containing the fix.\r\n", "Note that 1.13.2 has been released now"]}, {"number": 28437, "title": "How to enable INTEL_MKL_ML?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version: 1.10\r\n- Python version: 2.7.13\r\n- Installed using virtualenv? pip? conda?:  none\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- Intel CPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nI want build tensorflow with mkl that use the command below:\r\nbazel build tensorflow/tools/benchmark:benchmark_model --verbose_failures -c opt --copt=-march=native --config=mkl --copt=\"-mfpmath=both\" --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2\r\nbut when I gdb with file tensorflow/core/kernels/mkl_conv_ops.cc, I find INTEL_MKL_ML is disabled and use the #else branch code.\r\nyou can read the file tensorflow/core/kernels/mkl_conv_ops.cc that\r\n#ifdef INTEL_MKL_ML\r\nclass MklConv2DOp : public OpKernel {\r\n  Path1\r\n}\r\n#else \r\nclass MklConv2DOp : public OpKernel {\r\n  Path2\r\n}\r\n#endif\r\nWHY tensorflow execute Path2 not Path1? what the difference between these two paths?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Is build option \"--config=mkl\" means that tensorflow use mkl-dnn as default? So how can i build tensorflow with mkl-ml?", "if you didn't want to use mkldnn, could you try this build options \"--copt=-DINTEL_MKL_ML_ONLY\"? And see if it would use the path1 you mentioned above ", "@fengyuICT : Please have a look on @Leslie-Fang's suggestion and let us know if that helps. Thanks!", "I use tensorflow 1.10, it has macro INTEL_MKL_ML, so I add options --copt=-DINTEL_MKL_ML according to @Leslie-Fang , it works. Thanks ^_^"]}, {"number": 28436, "title": "libcudart.so.7.5: cannot open shared object file: No such file or directory", "body": "Hello,\r\n\r\nI try to install tensorflow-gpu on my ubuntu machine.\r\n\r\ni got this error here:\r\nImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory\r\n\r\nsome one can help me ?\r\n\r\ninside my ~/.bashrc\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\r\nexport CUDA_HOME=/usr/local/cuda\r\n\r\nInstalled:\r\nCuda 10.0\r\ncuDNN 7.5.0", "comments": ["@007fred50  It looks like you haven't used a template to create this issue(tensorflow gpu  version used). Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "@007fred50 Request you to please follow the [link1](https://github.com/tensorflow/models/issues/143), [link2](https://github.com/tensorflow/tensorflow/issues/1501).\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n"]}, {"number": 28435, "title": "TypeError: unhashable type: 'memmap'", "body": "When I want to use `tf.as_dtype` to convert a `numpy.memmap` data , the error was given. Is tensorflow not support this dtype? Or I shoulde use other function to convert my data to a type that tensorflow can recognize?", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 28434, "title": "NPM Install failing", "body": "I do hope this is the right place for this; there's no `issues` enabled on tensorflow/tfjs-node...\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   - Windows 10\r\n\r\n**Describe the problem**\r\n\r\n\r\ndownloading and unzipping is failing; apparently the unzip utility is running out of memory.  Should add an additional argument \r\ninstalling with node 11.9.0  x86 version.  Produces ths output at the end.\r\n\r\n```\r\nM:\\javascript\\tensorflow>npm install @tensorflow/tfjs-node-gpu\r\n\r\n> @tensorflow/tfjs-node-gpu@1.1.2 install M:\\javascript\\tensorflow\\node_modules\\@tensorflow\\tfjs-node-gpu\r\n> node scripts/install.js gpu download\r\n\r\n* Downloading libtensorflow\r\n[==============================] 21249429/bps 100% 0.0s\r\n(node:17992) UnhandledPromiseRejectionWarning: RangeError: Array buffer allocation failed\r\n    at new ArrayBuffer (<anonymous>)\r\n    at new Uint8Array (<anonymous>)\r\n    at new FastBuffer (internal/buffer.js:788:1)\r\n    at Function.alloc (buffer.js:280:10)\r\n    at decompress (M:\\javascript\\tensorflow\\node_modules\\adm-zip\\zipEntry.js:56:27)\r\n    at Object.getData (M:\\javascript\\tensorflow\\node_modules\\adm-zip\\zipEntry.js:242:12)\r\n    at M:\\javascript\\tensorflow\\node_modules\\adm-zip\\adm-zip.js:438:25\r\n    at Array.forEach (<anonymous>)\r\n    at Object.extractAllTo (M:\\javascript\\tensorflow\\node_modules\\adm-zip\\adm-zip.js:432:17)\r\n    at WriteStream.response.on.pipe.on (M:\\javascript\\tensorflow\\node_modules\\@tensorflow\\tfjs-node-gpu\\scripts\\resources.js:70:21)\r\n(node:17992) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by\r\ning a promise which was not handled with .catch(). (rejection id: 1)\r\n(node:17992) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js proc\r\nth a non-zero exit code.\r\nnpm WARN rollup-plugin-visualizer@1.1.1 requires a peer of rollup@>=0.60.0 but none is installed. You must install peer dependencies yourself.\r\nnpm WARN tensorflow@1.0.0 No description\r\nnpm WARN tensorflow@1.0.0 No repository field.\r\n\r\n+ @tensorflow/tfjs-node-gpu@1.1.2\r\nremoved 649 packages, updated 2 packages and audited 62 packages in 57.866s\r\nfound 0 vulnerabilities\r\n\r\n```\r\n\r\n\r\n\r\nAs I was writing this, decided to try x64 version; and it got past that point.\r\n(node-gyp couldn't find msbuild.exe; but that's a different issue)\r\n\r\nWhen I fix those issues... I still get\r\n\r\n```\r\n290 verbose stack Error: @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`\r\n290 verbose stack Exit status 1\r\n290 verbose stack     at EventEmitter.<anonymous> (C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\npm-lifecycle\\index.js:301:16)\r\n290 verbose stack     at EventEmitter.emit (events.js:197:13)\r\n290 verbose stack     at ChildProcess.<anonymous> (C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\npm-lifecycle\\lib\\spawn.js:55:14)\r\n290 verbose stack     at ChildProcess.emit (events.js:197:13)\r\n290 verbose stack     at maybeClose (internal/child_process.js:978:16)\r\n290 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)\r\n291 verbose pkgid @tensorflow/tfjs-node-gpu@1.1.2\r\n292 verbose cwd M:\\javascript\\tensorflow\r\n293 verbose Windows_NT 10.0.17134\r\n294 verbose argv \"h:\\\\dev2\\\\nodejs\\\\node.exe\" \"C:\\\\Users\\\\Panther\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\npm\\\\bin\\\\npm-cli.js\" \"install\" \"@tensorflow/tfjs-node-gpu\"\r\n```\r\n\r\nRedirecting output to a log seems to make the download fail too...  So THIS is what I get now; and don't understand what's missing.\r\n\r\n```\r\n\r\n* Downloading libtensorflow\r\n[==============================] 22424948/bps 100% 0.0s\r\n[==============================] 2118839/bps 100% 0.0s\r\n* Building TensorFlow Node.js bindings\r\nM:\\javascript\\tensorflow\\node_modules\\@tensorflow\\tfjs-node-gpu\\scripts\\install.js:165\r\n      throw new Error('node-gyp rebuild failed with: ' + err);\r\n      ^\r\n\r\nError: node-gyp rebuild failed with: Error: Command failed: node-gyp rebuild\r\ngyp ERR! build error\r\ngyp ERR! stack Error: `msbuild` failed with exit code: 1\r\ngyp ERR! stack     at ChildProcess.onExit (C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\lib\\build.js:262:23)\r\ngyp ERR! stack     at ChildProcess.emit (events.js:197:13)\r\ngyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:254:12)\r\ngyp ERR! System Windows_NT 10.0.17134\r\ngyp ERR! command \"h:\\\\dev2\\\\nodejs\\\\node.exe\" \"C:\\\\Users\\\\Panther\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\npm\\\\node_modules\\\\node-gyp\\\\bin\\\\node-gyp.js\" \"rebuild\"\r\ngyp ERR! cwd M:\\javascript\\tensorflow\\node_modules\\@tensorflow\\tfjs-node-gpu\r\ngyp ERR! node -v v11.9.0\r\ngyp ERR! node-gyp -v v3.8.0\r\ngyp ERR! not ok\r\n\r\n    at cp.exec (M:\\javascript\\tensorflow\\node_modules\\@tensorflow\\tfjs-node-gpu\\scripts\\install.js:165:13)\r\n    at ChildProcess.exithandler (child_process.js:304:5)\r\n    at ChildProcess.emit (events.js:197:13)\r\n    at maybeClose (internal/child_process.js:978:16)\r\n    at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)\r\nnpm WARN rollup-plugin-visualizer@1.1.1 requires a peer of rollup@>=0.60.0 but none is installed. You must install peer dependencies yourself.\r\nnpm WARN tensorflow@1.0.0 No description\r\nnpm WARN tensorflow@1.0.0 No repository field.\r\n\r\nnpm ERR! code ELIFECYCLE\r\nnpm ERR! errno 1\r\nnpm ERR! @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`\r\nnpm ERR! Exit status 1\r\nnpm ERR!\r\nnpm ERR! Failed at the @tensorflow/tfjs-node-gpu@1.1.2 install script.\r\nnpm ERR! This is probably not a problem with npm. There is likely additional logging output above.\r\n\r\nnpm ERR! A complete log of this run can be found in:\r\nnpm ERR!     C:\\Users\\Panther\\AppData\\Roaming\\npm-cache\\_logs\\2019-05-06T10_57_50_660Z-debug.log\r\n```\r\n\r\n(and the mentioned debug.log)\r\n\r\n```\r\n279 warn rollup-plugin-visualizer@1.1.1 requires a peer of rollup@>=0.60.0 but none is installed. You must install peer dependencies yourself.\r\n280 warn tensorflow@1.0.0 No description\r\n281 warn tensorflow@1.0.0 No repository field.\r\n282 verbose stack Error: @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`\r\n282 verbose stack Exit status 1\r\n282 verbose stack     at EventEmitter.<anonymous> (C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\npm-lifecycle\\index.js:301:16)\r\n282 verbose stack     at EventEmitter.emit (events.js:197:13)\r\n282 verbose stack     at ChildProcess.<anonymous> (C:\\Users\\Panther\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\npm-lifecycle\\lib\\spawn.js:55:14)\r\n282 verbose stack     at ChildProcess.emit (events.js:197:13)\r\n282 verbose stack     at maybeClose (internal/child_process.js:978:16)\r\n282 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:265:5)\r\n283 verbose pkgid @tensorflow/tfjs-node-gpu@1.1.2\r\n284 verbose cwd M:\\javascript\\tensorflow\r\n285 verbose Windows_NT 10.0.17134\r\n286 verbose argv \"h:\\\\dev2\\\\nodejs\\\\node.exe\" \"C:\\\\Users\\\\Panther\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\npm\\\\bin\\\\npm-cli.js\" \"install\" \"@tensorflow/tfjs-node-gpu\"\r\n287 verbose node v11.9.0\r\n288 verbose npm  v6.4.1\r\n289 error code ELIFECYCLE\r\n290 error errno 1\r\n291 error @tensorflow/tfjs-node-gpu@1.1.2 install: `node scripts/install.js gpu download`\r\n291 error Exit status 1\r\n292 error Failed at the @tensorflow/tfjs-node-gpu@1.1.2 install script.\r\n292 error This is probably not a problem with npm. There is likely additional logging output above.\r\n293 verbose exit [ 1, true ]\r\n```\r\n\r\n\r\n", "comments": ["I did manage to do `npm install --force @tensorflow/tfjs-node-gpu` and keep what was partially built.\r\nThis is the output from visual studio (second build; the first build had similar error, but deleted the lib/tensorflow.dll too; I had re-run `npm run install` to get the download again before... )\r\n\r\n```\r\n1>------ Build started: Project: tfjs_binding, Configuration: Debug x64 ------\r\n1>deps-stage\r\n1>(node:18672) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, rename 'M:\\javascript\\tensorflow\\node_modules\\@tensorflow\\tfjs-node-gpu\\deps\\lib\\tensorflow.dll' -> 'M:\\javascript\\tensorflow\\node_modules\\@tensorflow\\tfjs-node-gpu\\build\\Debug\\tensorflow.dll'\r\n1>(node:18672) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). (rejection id: 1)\r\n1>(node:18672) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\r\n1>generate_def\r\n1>CUSTOMBUILD : FATAL error : CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory\r\n1> 1: 00007FF61AD3D50A v8::internal::GCIdleTimeHandler::GCIdleTimeHandler+4618\r\n1> 2: 00007FF61ACE4F86 uv_loop_fork+79446\r\n1> 3: 00007FF61ACE5C21 uv_loop_fork+82673\r\n1> 4: 00007FF61B1ECAEE v8::internal::FatalProcessOutOfMemory+798\r\n1> 5: 00007FF61B1ECA27 v8::internal::FatalProcessOutOfMemory+599\r\n1> 6: 00007FF61B4C5844 v8::internal::Heap::RootIsImmortalImmovable+14788\r\n1> 7: 00007FF61B4C3608 v8::internal::Heap::RootIsImmortalImmovable+6024\r\n1> 8: 00007FF61B0840DB v8::internal::Factory::AllocateRawWithImmortalMap+59\r\n1> 9: 00007FF61B086B5D v8::internal::Factory::NewRawTwoByteString+77\r\n1>10: 00007FF61B085CAC v8::internal::Factory::NewStringFromUtf8+236\r\n1>11: 00007FF61B20C219 v8::String::NewFromUtf8+553\r\n1>12: 00007FF61AD078DB node::Buffer::New+26843\r\n1>13: 00007FF61B1D24FE v8::internal::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>+59870\r\n1>14: 00007FF61B1D3A60 v8::internal::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>+65344\r\n1>15: 00007FF61B1D29F9 v8::internal::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>+61145\r\n1>16: 00007FF61B1D28DB v8::internal::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>::ZoneVector<v8::internal::compiler::MoveOperands * __ptr64>+60859\r\n1>17: 000000A9F1650461\r\n1>Done building project \"tfjs_binding.vcxproj\" -- FAILED.\r\n========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========\r\n```\r\n\r\n---\r\nEdit\r\n\r\nediting the binding.gyp file of tfjs-node-gpu and adding a larger heap for node fixes the issue.  THe default is 1.5G; going to 2048 didn't help...\r\n\r\n```\r\n              'action': [\r\n                'cmd',\r\n                '/c node --max-old-space-size=3096 <@(_inputs) > <@(_outputs)'\r\n              ]\r\n```\r\n\r\nI see it crashes on the join of all the inputs; of which libtensor.dll is 734M.  so 2* 750 is 1500MB (once for having it in memory, and once in the join buffer) at least....", "Find CUDA SDK 10.0 (not 10.0) re-patch with CDNN, and success.\r\n\r\n - Node x64 version.\r\n  - [Cuda SDK 10.0 (windows) ](https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64)\r\n  - [CDNN](https://developer.nvidia.com/cudnn)\r\n\r\n```\r\nnpm install --force @tensorflow/tfjs-node-gpu\r\ncd node_modules/@tensorflow/tfjs-node-gpu\r\n# edit binding.gyp to fix the action (end of prior message).\r\nnpm run install\r\n```\r\n\r\n``` diff \r\n# under windows target\r\n              'action': [\r\n                'cmd',\r\n---                '/c node <@(_inputs) > <@(_outputs)'\r\n+++                '/c node --max-old-space-size=3096 <@(_inputs) > <@(_outputs)'\r\n              ]\r\n```\r\n - profit"]}, {"number": 28433, "title": "Quantization support for LeakyRealu and Relu 6 in tools", "body": "", "comments": ["@achowdhery can you please review the PR and provide the feedback.\r\n\r\nRegards \r\nAmit ", "@achowdhery Can you please review this PR ? Thanks!", "@achowdhery , can you please spend some time on the PR and provide your feedback.\r\n\r\nRegards\r\nAmit", "Can one of the admins verify this patch?", "@amitsrivastava78 Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28432, "title": "Build failure while installing TensorFlow from source following TF documentation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 \r\n- TensorFlow installed from (source or binary): Source \r\n- TensorFlow version: Tensorflow 2.0 \r\n- Python version: Python 3.7\r\n- Installed using virtualenv? pip? conda?: Created a Conda virtual environment \r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc version 6.5.0 \r\n- CUDA/cuDNN version: CUDA 9.0/ cuDNN 7.4.2\r\n- GPU model and memory: Geoforce GTX 1050ti\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands/steps that you executed before running into the problem**\r\n\r\nI followed the steps listed here for building from source (https://www.tensorflow.org/install/source). I keep getting stuck at the step wherein I invoke \"bazel build\" which is the penultimate step in the installation process. Now, these errors aren't consistent. I have run the command \"bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\" multiple times and each time the build fails with a different error. However this is the error I got for the last two times I tried the installation process\r\n\r\n\r\nERROR: /home/wannabe/tensorflow/tensorflow/contrib/resampler/BUILD:65:1: output 'tensorflow/contrib/resampler/_objs/python/ops/_resampler_ops_gpu/resampler_ops_gpu.cu.pic.o' was not created\r\nERROR: /home/wannabe/tensorflow/tensorflow/contrib/resampler/BUILD:65:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 234.229s, Critical Path: 40.57s\r\nINFO: 648 processes: 648 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nI copy pasted the entire stack of messages printed out to the console here https://pastebin.com/kJFxdHgA", "comments": ["@wannabeOG Please try this command bazel build -c opt -c cuda //tensorflow/tools/pip_package and Let us know how it progresses. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28431, "title": "sparse_image_warp don't accept the tensor with dynamic shape.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04)\r\n- TensorFlow installed from (binary)\r\n- TensorFlow version (1.13.1)\r\n- Python version\r\n\r\n**Describe the current behavior**\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 6, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/python/ops/sparse_image_warp.py\", line 179, in sparse_image_warp\r\n    grid_locations = _get_grid_locations(image_height, image_width)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/python/ops/sparse_image_warp.py\", line 34, in _get_grid_locations\r\n    y_range = np.linspace(0, image_height - 1, image_height)\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'int'\r\n\r\n**Describe the expected behavior**\r\nNo error: should return the warped image\r\n\r\n**Code to reproduce the issue**\r\nimage_placeholder = tf.placeholder(dtype=tf.float32, shape=[1, None, None, 1])\r\nsrc_pts = tf.constant([10, 10])\r\ndest_pts = tf.constant([100, 100])\r\ntf.contrib.image.sparse_image_warp(image_placeholder,\r\n                                                        source_control_point_locations = src_pts,\r\n                                                         dest_control_point_locations = dest_pts,\r\n                                                         interpolation_order = 2,\r\n                                                         regularization_weight = 0,\r\n                                                         num_boundary_points = 1\r\n                                                         ) \r\n**Other info / logs**\r\nNone.\r\n", "comments": ["After having a look at\r\nhttps://github.com/tensorflow/tensorflow/blob/6096a8f9088f4da725db4a15684a961724943eb8/tensorflow/contrib/image/python/ops/sparse_image_warp.py#L175\r\n\r\nFound that as this functionality is currently being provided using numpy which requires the static shape of the shape.", "@cahuja1992 Can you please tell us is this still an issue? Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28431\">No</a>\n", "Hi, I've run into this issue as well.  In order to use sparse_image_warp() in a custom Keras layer, it throws an error because it is expecting numbers rather than dynamic tensor shapes (i.e. \"None\").  The error I received came from a similar chain:\r\n\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/image/python/ops/sparse_image_warp.py\", line 185, in sparse_image_warp\r\n    _expand_to_minibatch(flattened_grid_locations, batch_size), image.dtype)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/image/python/ops/sparse_image_warp.py\", line 43, in _expand_to_minibatch\r\n    return np.tile(np.expand_dims(np_array, 0), tiles)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/lib/shape_base.py\", line 1236, in tile\r\n    shape_out = tuple(s*t for s, t in zip(c.shape, tup))\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/lib/shape_base.py\", line 1236, in <genexpr>\r\n    shape_out = tuple(s*t for s, t in zip(c.shape, tup))\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\r\n", "@spencerfrei , Please post a new issue and provide all the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new/choose). The reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!"]}, {"number": 28430, "title": "TF2.0 MirroredStrategy Example failed on windows if GPU > 1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, Using the sample code in tensorflow docs\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha0\r\n- TensorFlow version (use command below): 2.0.0-alpha0 \r\n- Python version: 3.7\r\n- CUDA/cuDNN version: cuda10/cudnn7.5\r\n- GPU model and memory: 2*1080ti\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\n\r\nimport os\r\ndatasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\nmnist_train, mnist_test = datasets['train'], datasets['test']\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n# You can also do info.splits.total_num_examples to get the total\r\n# number of examples in the dataset.\r\n\r\nnum_train_examples = info.splits['train'].num_examples\r\nnum_test_examples = info.splits['test'].num_examples\r\n\r\nBUFFER_SIZE = 10000\r\n\r\nBATCH_SIZE_PER_REPLICA = 64\r\nBATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ndef scale(image, label):\r\n  image = tf.cast(image, tf.float32)\r\n  image /= 255\r\n\r\n  return image, label\r\n\r\n# train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\ntrain_dataset = mnist_train.repeat(2).map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n\r\neval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\r\n\r\nwith strategy.scope():\r\n    model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n    model.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\nmodel.fit(train_dataset, epochs=10)\r\n\r\n```\r\nThe code above is what i copy from [tf docs](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/distribute/keras.ipynb)\r\nWhich will result an Error like:\r\n```\r\nInvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/Adam/NcclAllReduce}}with these attrs: [reduction=\"sum\", T=DT_FLOAT, num_devices=1, shared_name=\"c0\"]\r\nRegistered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\t [[training/Adam/NcclAllReduce]] [Op:__inference_keras_scratch_graph_1495]\r\n```\r\nAt first I thought I had a wrong build of tensorflow without GPU support. However, when i re-installed the whole env exactly as the guide in the official site. Still the same result. And I found that if I uncomment the mirror strategy part, the code is working good\r\n```\r\n# with strategy.scope():\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n  tf.keras.layers.MaxPooling2D(),\r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(64, activation='relu'),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n            optimizer=tf.keras.optimizers.Adam(),\r\n            metrics=['accuracy'])\r\nmodel.fit(train_dataset, epochs=10)\r\n```\r\nAnd another found is that I myself only have 2 gpus however, when tensorflow init, it prints \"adding devices\" 3 times which i thought should be 2:\r\n```\r\n2019-05-06 16:38:55.635182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1\r\n2019-05-06 16:39:01.691770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-06 16:39:01.694145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 \r\n2019-05-06 16:39:01.694243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N N \r\n2019-05-06 16:39:01.704826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   N N \r\n2019-05-06 16:39:01.705405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-05-06 16:39:01.706830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2019-05-06 16:39:02.008523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1\r\n2019-05-06 16:39:02.008834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-06 16:39:02.009000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 \r\n2019-05-06 16:39:02.009103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N N \r\n2019-05-06 16:39:02.009207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   N N \r\n2019-05-06 16:39:02.009398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-05-06 16:39:02.021524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2019-05-06 16:39:04.813463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1\r\n2019-05-06 16:39:04.813706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-06 16:39:04.813870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1 \r\n2019-05-06 16:39:04.813975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N N \r\n2019-05-06 16:39:04.814077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   N N \r\n2019-05-06 16:39:04.814274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8791 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-05-06 16:39:04.815618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8791 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe mirror strategy should work well on machine that have more than one GPU otherwise it is useless right? I found other related issues like: https://github.com/tensorflow/tensorflow/issues/28372 and https://github.com/tensorflow/tensorflow/issues/28334\r\nThey all reported that the scripts can run for 1 gpu and failed on gpu>=2\r\nAny thought is appreciated!\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/ziyi/EastAI2/models/test.py\", line 82, in <module>\r\n    model.fit(train_data, epochs=2, steps_per_epoch=100)\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 746, in fit\r\n    validation_freq=validation_freq)\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed.py\", line 131, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 263, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3217, in __call__\r\n    outputs = self._graph_fn(*converted_inputs)\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 558, in __call__\r\n    return self._call_flat(args)\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 627, in _call_flat\r\n    outputs = self._inference_function.call(ctx, args)\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 415, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\jerry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 66, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node training/Adam/NcclAllReduce}}with these attrs: [reduction=\"sum\", T=DT_FLOAT, num_devices=2, shared_name=\"c0\"]\r\nRegistered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[training/Adam/NcclAllReduce]] [Op:__inference_keras_scratch_graph_2200]\r\n\r\n```\r\n", "comments": ["Are you using AMD CPUs? As far as I know, NCCL sometimes has some problems on AMD CPUs.", "> Are you using AMD CPUs? As far as I know, NCCL sometimes has some problems on AMD CPUs.\r\n\r\nNo, I'm using Intel. One question here, if i need to install NCCL seperatly? I thought it was automatically set by previous installation?", "hi there, after a day-long searching, I finally kind of understand the concept. NCCL is a native library for GPUs communication which is only official supported for Linux. Thus here are two solutions:\r\n\r\n1. find windows non-official version of NCCL (not recommend)\r\n2. custom reduction OP like the [example](https://github.com/tensorflow/tensorflow/issues/21470#issuecomment-422506263)\r\n", "hit the same problem with nvidia k80\r\n"]}, {"number": 28429, "title": "Profile tab in tensorboard keeps saying \"Processing datasets\" ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-alpha0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1, 7.5\r\n- GPU model and memory: 3x GTX 1080Ti, 11Gb\r\n\r\n**Describe the current behavior**\r\nI was training a tf.keras and had a tensorboard callback, but i don't seem to get the new profiling feature to work.\r\n\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\r\n    (train_images, train_labels))\r\ntrain_dataset = train_dataset.shuffle(buffer_size=256)\r\ntrain_dataset = train_dataset.apply(tf.data.experimental.map_and_batch(map_func=load_data,\r\n                                                                       batch_size=batch_size,\r\n                                                                       num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n                                                                       drop_remainder=True))\r\ntrain_dataset = train_dataset.repeat()\r\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\nval_dataset = tf.data.Dataset.from_tensor_slices(\r\n    (val_images, val_labels))\r\nval_dataset = val_dataset.shuffle(buffer_size=256)\r\nval_dataset = val_dataset.apply(tf.data.experimental.map_and_batch(map_func=load_data,\r\n                                                                   batch_size=batch_size,\r\n                                                                   num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n                                                                   drop_remainder=True))\r\nval_dataset = train_dataset.repeat()\r\nval_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\ncallbacks = [tf.keras.callbacks.TensorBoard(update_freq='batch'),\r\n             tf.keras.callbacks.ModelCheckpoint('model/weights.h5', \r\n                                                save_best_only=True, \r\n                                                save_weights_only=True)]\r\nmodel.fit(train_dataset,\r\n          steps_per_epoch=len(train_images) // batch_size,\r\n          epochs=300,\r\n          validation_data=val_dataset,\r\n          validation_steps=len(val_images) // batch_size,\r\n          callbacks=callbacks)\r\n```\r\n**Other info / logs**\r\nThere are the logs from tensorboard\r\n```\r\nI0506 13:36:28.764028 139696445298432 _internal.py:122] ::ffff:10.10.10.14 - - [06/May/2019 13:36:28] \"GET /data/plugin/profile/tools HTTP/1.1\" 200 -\r\nI0506 13:36:28.786284 139696445298432 _internal.py:122] ::ffff:10.10.10.14 - - [06/May/2019 13:36:28] \"GET /data/plugin/profile/hosts?run=2019-05-05_18-45-33&tag=trace_viewer HTTP/1.1\" 200 -\r\nI0506 13:36:28.829900 139696445298432 _internal.py:122] ::ffff:10.10.10.14 - - [06/May/2019 13:36:28] \"GET /data/plugin/profile/hosts?run=2019-05-05_18-45-33&tag=trace_viewer HTTP/1.1\" 200 -\r\n```\r\n", "comments": ["@jvishnuvardhan ", "@srihari-humbarwadi Could you provide a standalone code to reproduce? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28429\">No</a>\n", "Hi. I am seeing this same issue. I built a tf.keras model with model subclassing on tf 2.0, and then executed the following code to log the graph and run the tensorboard profiler:\r\n\r\ntf.summary.trace_on(graph=True, profiler=True)\r\nnetwork(np.random.randn(1, 84, 84, 4))\r\nwith summary_writer.as_default():\r\n    tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=log_dir)\r\n\r\nThe graph gets logged to tensorboard correctly but when I open the \"profile\" tab, it says \"Processing Datasets\" and it does not make any progress, it has been stuck like that for hours.\r\nMaybe I should add that the context is Deep Reinforcement Learning, so the data comes from a replay buffer (implemented as a list), so the dataset is dynamic and data is added and removed continuously.", "@markelsanz14 Please open a new issue with your issue details and a standalone code to reproduce the issue. Please open the issue in `tensorboard` repo. Thanks! "]}, {"number": 28428, "title": "Reset sequence of samples generate by tf.random.uniform without restarting the session", "body": "As of the rules I have raised this issue on StackOverflow but it's dying a slow death over there, therefore I ask it here: https://stackoverflow.com/questions/55951644/reset-sequence-of-samples-generate-by-tf-random-uniform-without-restarting-the-s\r\n\r\nI can set the seed of the random.uniform operation, but this seed is only used as \"starting-point\" of the session. I cannot instruct the tensor to \"re-initialize\" from the set seed.\r\n\r\nI want to train my model on random samples generated by random.uniform and every batch I want the same samples to be used, without having to store these samples.\r\n\r\nI could create one big array to write these uniform samples into and use that to train upon, but using random.uniform tensor with a \"reset\" of the seed every batch seems more elegant and more memory friendly. In some use cases I have to create millions and millions of random uniform samples. Using tf.data with a random.uniform tensor should be way more elegant.", "comments": ["Please provide details such as platform you are using (operating system, architecture), TensorFlow version used. If possible, please provide the minimal code snippet which can depict the issue at hand. You can refer this [template](https://github.com/tensorflow/tensorflow/issues/new/choose) to fill in the required information. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I am using both Windows 7 and Ubuntu 18.04 and the issue is the same for both platforms cause I am trying to figure out how I can use the API. I am using TF 1.13."]}, {"number": 28427, "title": "FailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_93/bias", "body": "I run my code, but I don't know what is the below error. Please help me to fix this\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value dense_93/bias \t [[node dense_93/bias/read (defined at C:/Users/TRAN THI DIEM/Documents/diem/research/ReinforcementLearning/code/Depression/Code detecting depression using multi layer with survey data_ da chay duoc/depression_test_reinforcement.py:126) ]]\r\n\r\n\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfrom sklearn.utils import shuffle\r\n\r\nfrom sklearn import preprocessing\r\nfrom sklearn.model_selection import train_test_split\r\n#train set\r\ndf_train = pd.read_csv('data/train.csv')\r\n# test set\r\ndf_test = pd.read_csv('data/test.csv')\r\n\r\n#show the shape of the train dataframe\r\ndf_train.shape\r\ndef missing_values_table(df):\r\n        # Total missing values\r\n        mis_val = df.isnull().sum()  # sum the number of each colum that has no value\r\n        \r\n        # Percentage of missing values\r\n        mis_val_percent = 100 * df.isnull().sum() / len(df) #len(df); return number of row in df_train\r\n        \r\n        # Make a table with the results\r\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\r\n        \r\n        # Rename the columns\r\n        mis_val_table_ren_columns = mis_val_table.rename(\r\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\r\n        \r\n        # Sort the table by percentage of missing descending\r\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\r\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\r\n        '% of Total Values', ascending=False).round(1)\r\n        \r\n        # Print some summary information\r\n        print (\"The dataset has \" + str(df.shape[1]) + \" columns.\\n\"      \r\n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\r\n              \" columns that have missing values.\")\r\n        \r\n        # Return the dataframe with missing information\r\n        return mis_val_table_ren_columns\r\n    # Get the columns with > 50% missing\r\nmissing_df = missing_values_table(df_train);\r\nmissing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)\r\nprint('\\n','%d columns will be deleted.' % len(missing_columns))\r\n\r\n# Drop the columns with 50% missing data\r\ndf_train = df_train.drop(columns = list(missing_columns)) # delete some column have miss information more than 50%\r\n# shape for df_train now is (1143,71)compare with the first (1143,75)\r\n\r\n# Create a label (category) encoder object\r\n#ncoder = LabelEncoder()\r\nencoder = preprocessing.LabelEncoder()\r\n\r\n# fitting the encoder to the \"survey_date\" column\r\nencoder.fit(df_train['survey_date'])\r\n\r\n# Apply the fitted encoder to the \"survey_date\" to transform categories into integers\r\nencoded_train = encoder.transform(df_train['survey_date'])\r\n# encoded_test = encoder.transform(df_test['survey_date'])\r\n\r\n#assign the tranformed column back to the dataframe\r\ndf_train['survey_date'] = encoded_train\r\n# split data into train and test sets\r\nX = df_train.drop([\"depressed\"], axis=1)\r\n\r\n# fill missing values with mean column values\r\n\r\nimputer = preprocessing.Imputer()\r\ntransformed_X = imputer.fit_transform(X)\r\n\r\ny_temp = df_train.depressed\r\ny= df_train['depressed']\r\ny_matrix = y.reset_index().values\r\ny =y_matrix[:,1]\r\n\r\nseed = 5\r\ntest_size = 0.33\r\n\r\nX_train, X_test, y_train, y_test =train_test_split(transformed_X, y, test_size=test_size, random_state=seed)\r\n\r\ninput_size = [70]\r\nclass DQNetwork:\r\n    def __init__(self):\r\n#        self.state_size = state_size\r\n #       self.action_size = action_size\r\n    \r\n        self.learning_rate = tf.placeholder(tf.float32)\r\n        #self.weight_decay = tf.constant(1e-4)\r\n\r\n#        self.action = tf.placeholder(tf.uint16, [None,3])  \r\n        # We create the placeholders\r\n        # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\r\n        \r\n        # [None, 84, 84, 4]\r\n        self.inputs_ = tf.placeholder(tf.float64, [None,*input_size])\r\n        \r\n#        self.actions_ = tf.placeholder(tf.float32, [None, 3])\r\n\r\n        # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\r\n        self.label = tf.placeholder(tf.int64,[None])    \r\n#fully connected        \r\n        \r\n           \r\n\r\n       # self.flatten = tf.layers.flatten(self.conv6_Dropout)\r\n        ## --> [43,264]\r\n\r\n        self.output_layer1 = tf.layers.dense(inputs=self.inputs_,\r\n                                     #    bias_initializer=tf.zeros_initializer(), kernel_initializer=tf.zeros_initializer(),\r\n                                      #kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n                                     # kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      units=36,\r\n                                     activation=tf.nn.elu)\r\n        self.output_layer2 = tf.layers.dense(inputs=self.output_layer1,\r\n                                      #       bias_initializer=tf.zeros_initializer(), kernel_initializer=tf.zeros_initializer(),\r\n                                      #kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n                                     # kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      units=36,\r\n                                    activation=tf.nn.elu)\r\n        self.output = tf.layers.dense(inputs=self.output_layer2,\r\n                                     #bias_initializer=tf.zeros_initializer(), kernel_initializer=tf.zeros_initializer(),\r\n                                      #kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n                                     # kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4),\r\n                                      units=1,\r\n                                     activation=tf.nn.sigmoid)\r\n        \r\n        self.targets1 = tf.squeeze(tf.cast(self.label, tf.int32))   # Get predicted values by finding which logit is the greatest\r\n        self.targets = tf.cast(tf.argmax(self.targets1, 1), tf.int32)        \r\n        self.batch_predictions = tf.cast(tf.argmax(self.output, 1), tf.int32)\r\n        self.predicted_correctly = tf.equal(self.batch_predictions, self.targets)\r\n    # Average the 1's and 0's (True's and False's) across the batch size\r\n        self.accuracy = tf.reduce_mean(tf.cast(self.predicted_correctly, tf.float32))\r\n        \r\n        \r\n        \r\n        self.loss = tf.reduce_mean( tf.nn.sparse_softmax_cross_entropy_with_logits(logits= self.output, labels=self.label) )\r\n        #cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits_v2(logits = self.output,label = self.label)\r\n       # self.loss = tf.reduce_mean(cross_entropy, name='cross_entropy')\r\n        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\r\n        #self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\r\n# create model\r\n#model = Sequential()\r\n#model.add(Dense(36, input_dim=70, activation='elu'))\r\n#model.add(Dense(36, activation='elu'))\r\n#model.add(Dense(1, activation='sigmoid'))\r\n## Compile model\r\n#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n## Fit the model\r\n#model.fit(X_train, y_train, epochs=100, batch_size=10)\r\n#scores = model.evaluate(X_test, y_test)\r\n#print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\r\nlearning_rate = 0.0001\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ndqn = DQNetwork ()\r\n_,current_loss,current_accuracy = sess.run([dqn.optimizer,dqn.loss,dqn.accuracy],\r\n                                                  feed_dict={dqn.inputs_:X_train,dqn.label:y_train,dqn.learning_rate:learning_rate})", "comments": ["@diemtran1005 It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "@diemtran1005 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28426, "title": "Makefile build fails on MacOS 10.13", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacPro 5.1 XCode 9.4 OSX 10.13\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.13.1\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): XCode 9.4.1 clang\r\n- CUDA/cuDNN version: CUDA 10, cuDNN 7.4\r\n- GPU model and memory: GTX 1060 6Gb\r\n\r\n\r\n\r\n**Describe the problem**\r\nTyping make in the contrib Makefile locks up the machine and it becomes unreponsive\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ngit clone tensorflow\r\ngit checkout v1.13.1\r\ncd tensorflow/tensorflow/contrib/makefile\r\nmake\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nGets past absl compilation with 8 warnings, mouse stops moving machine is still pingable with port 22 open according to nmap but machine is not responsive.\r\n\r\nWould be happy with a CPU only build for now to statically link against.", "comments": ["Sorry -j24 doesnt agree with my machine when each clang instance occupies ~100Mb of memory the amount of memory on the host machine is 16Gb 24*100 = 24Gb it was swapping hard and non repsonsive.\r\n\r\nOnce the automatic -j value was hard coded to 8 the compile completed without error.", "Sorry there should be a warning the -j value is determined automatically."]}, {"number": 28425, "title": "An CNN model error: You must feed a value for placeholder tensor 'y' with dtype int32 and shape [?] ", "body": "import tensorflow as tf\r\nimport os\r\nfrom skimage import io, transform\r\nimport glob\r\n\r\nimport numpy as np\r\n\r\n\r\n#dataset parameter\r\nmode = 'folder'\r\ntrain_dataset_path = '/Users/resun/ML_data/train_data_png/'   #change here\r\ntest_dataset_path = '/Users/resun/ML_data/test_data_png/'\r\ntag_dataset_path = '/Users/resun/ML_data/tag_image/'\r\n\r\n\r\n'''\r\nROOT_FOLDER\r\n       |-------- SUBFOLDER (CLASS 0)\r\n       |             |\r\n       |             | ----- image1.jpg\r\n       |             | ----- image2.jpg\r\n       |             | ----- etc...\r\n       |             \r\n       |-------- SUBFOLDER (CLASS 1)\r\n       |             |\r\n       |             | ----- image1.jpg\r\n       |             | ----- image2.jpg\r\n       |             | ----- etc...\r\n'''\r\n# image parameters\r\nnum_classes = 2     #change here\r\nimage_height = 128    #change here\r\nimage_width = 128    #change here\r\nchannels = 3     #the 3 color channels, change to 1 if grayscale\r\n'''\r\n0: Use the number of channels in the PNG-encoded image.\r\n1: output a grayscale image.\r\n3: output an RGB image.\r\n4: output an RGBA image.\r\n'''\r\n\r\n\r\n#########################################\r\n########## reading the dataset ##########\r\n#########################################\r\n#mode = folder\r\n'''\r\n\r\ndef read_images(dataset_path, mode, batch_size):\r\n    imagepaths, labels = list(), list()\r\n    if mode == 'folder':\r\n        # an ID will be affected to each sub-folders by alphabetical order\r\n        label = 0\r\n        #list the directory\r\n        try:   #python2     #two paths in the dataset_path\r\n            classes = sorted(os.walk(dataset_path).next()[1])\r\n            #walk: For each directory in the directory tree rooted at top, yields a 3-tuple (dirpath, dirnames, filenames)\r\n        except Exception:  #python3\r\n            classes = sorted(os.walk(dataset_path).__next__()[1])\r\n\r\n        #list each sub-directory (the classes)\r\n        for c in classes:    #img inside the folder\r\n            c_dir = os.path.join(dataset_path, c)\r\n            try:   #python2\r\n                walk = os.walk(c_dir).next()\r\n            except Exception:   #python3\r\n                walk = os.walk(c_dir).__next__()\r\n\r\n            #add each image to the training set\r\n            for sample in walk[2]:    #walk[2] in sub-directory\r\n                #only keeps jpeg images\r\n                if sample.endswith('.png') or sample.endswith('.jpeg'):\r\n                    imagepaths.append(os.path.join(c_dir, sample))\r\n                    labels.append(label)\r\n            label += 1\r\n    else:\r\n        raise Exception(\"unknown mode\")\r\n\r\n    #img data convert to a tensor\r\n    imagepaths = tf.convert_to_tensor(imagepaths, dtype = tf.string)\r\n    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\r\n    #build a tf queue, shuffle data\r\n    image, label = tf.train.slice_input_producer([imagepaths, labels], shuffle=True)   #Produces a slice of each `Tensor` in `tensor_list\r\n    #read images from disk\r\n    image = tf.read_file(image)    #Reads and outputs the entire contents of the input filename\r\n    image = tf.image.decode_png(image, channels=channels)    #Decode a PNG-encoded image to a uint8 or uint16 tensor\r\n    #resize images from disk\r\n    image = tf.image.resize_images(image, [image_height, image_width])\r\n    #normalize\r\n    image = image * 1.0/127.5 - 1.0\r\n\r\n    #create batches\r\n    X, Y = tf.train.batch([image, label], batch_size = batch_size, capacity = batch_size*8, num_threads = 4)\r\n    return X, Y\r\n'''\r\n\r\n\r\ndef read_img(path):\r\n    cate = [train_dataset_path+x for x in os.listdir(train_dataset_path) if os.path.isdir(train_dataset_path+x)]\r\n    imgs = []\r\n    labels = []\r\n    for idx, folder in enumerate(cate):\r\n        for im in glob.glob(folder+'\\*.png'):\r\n            #print('reading the images: %s' %(im))\r\n            img = io.imread(im)\r\n            img = transform.resize(img, (image_width, image_height, channels))\r\n            imgs.append(img)\r\n            labels.append(idx)\r\n    X=np.asarray(imgs, np.float32)\r\n    Y=np.asarray(labels, np.int32)\r\n    return X, Y\r\n\r\n#######################################\r\n############# classic CNN #############\r\n#######################################\r\n\r\n#parameters\r\nlearning_rate = 0.001\r\nnum_steps = 100\r\nbatch_size = 50\r\ndisplay_steps = 50\r\ndropout = 0.75    #probability to keep units\r\n\r\nX_train, Y_train = read_img(train_dataset_path)\r\n#X_test, Y_test = read_images(test_dataset_path, mode, batch_size)\r\n#X_tag, Y_tag = read_images(tag_dataset_path, mode, batch_size)\r\nprint(X_train)\r\nprint(Y_train)\r\nprint(type(X_train))\r\nprint(type(Y_train))\r\n\r\n\r\n#create model\r\nx = tf.placeholder(tf.float32, shape=[None, image_width, image_height, channels], name = 'x')\r\ny = tf.placeholder(tf.int32, shape=[None, ], name = 'y')\r\ndef conv_net(input, n_classes, dropout, reuse, training):\r\n    #define a scope for reusing the variables\r\n    with tf.variable_scope('ConvNet', reuse=reuse):\r\n        conv1 = tf.layers.conv2d(input, 32, 5, activation=tf.nn.relu)   #convolution layer with 32 filters and a kernel size of 5\r\n        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)     #max pooling (down-sampling) with pool_size of 2 and strides(\u6b65\u957f) of 2\r\n        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)    #convolution layer with 64 filters and a kernel size of 3\r\n        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)     #max pooling (down-sampling) with pool-size of 2 and strides of 2\r\n        fc1 = tf.contrib.layers.flatten(conv2)    #flatten the data to a 1D vector for the fully connected layer\r\n        fc1 = tf.layers.dense(fc1, 1024)     #fully connected layer (in contrib folder for now)\r\n        fc1 = tf.layers.dropout(fc1, rate = dropout, training = training)    #apply dropout (if training is false, dropout is not applied)\r\n        out = tf.layers.dense(fc1, n_classes)     #output layer, class prediction\r\n        out = tf.nn.softmax(out) if not training else out\r\n    return out\r\n\r\nconstant = tf.constant(value=1, dtype=tf.float32)\r\n\r\nlogits_train = conv_net(x, num_classes, dropout, reuse=False, training=True)    #create a graph for training\r\nlogits_eval = tf.multiply(logits_train, constant, name='logits_eval')\r\n#logits_test = conv_net(X_test, num_classes, dropout, reuse=True, training=False)    #create another graph for testing that reuse the same weights\r\n#logits_tag = conv_net(X_tag, 2, dropout, reuse=True, training=False)\r\n\r\nloss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_train, labels=y))\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\ntrain_op = optimizer.minimize(loss_op)\r\n#evulate model\r\ncorrect_pred = tf.equal(tf.argmax(tf.nn.softmax(logits_train), 1), tf.cast(y, tf.int64))\r\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\ninit = tf.global_variables_initializer()\r\n#save object\r\nsaver = tf.train.Saver()   #The constructor adds ops to save and restore variables.\r\n\r\n\r\n#########################################\r\n############ start training #############\r\n#########################################\r\n\r\nwith tf.Session() as sess:\r\n\r\n    sess.run(init)\r\n\r\n\r\n    #start data queue\r\n    coord = tf.train.Coordinator()\r\n    tf.train.start_queue_runners(coord=coord)\r\n\r\n    for step in range(1, num_steps+1):\r\n        if step % display_steps == 0 or step == 1:    #run optimization and calculate batch loss and accuracy\r\n            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={x: X_train, y: Y_train})\r\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \"{:.3f}\".format(acc))\r\n        else:\r\n            sess.run(train_op)   #only run the optimization op (backprop)\r\n    print(\"Optimization Finished!\")\r\n", "comments": ["I think I have already give y placeholder a input Y_train, but still got error.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28424, "title": "Multiplication of two-dimensional and three-dimensional tensors", "body": "There is a two dimensional tensor a[m,n], and a three dimensional tensor b[k,n,h]. What API should I use to multiply two dimensional tensors by three dimensional tensors to get a three dimensional tensor c[k,m,h] ?\r\nActually I can got it by:\r\n\r\nimport tensorflow as tf\r\nimport tensorly as tl\r\nx=tf.constant([[[1,2],[3,7],[8,9]],\r\n[[4,5],[6,10],[11,12]]],tf.float32)\r\na=tf.constant([[-0.70711,0.57735],\r\n[0.0000,0.57735],\r\n[0.70711,0.57735]])\r\nreshape_A = tf.reshape(x, [2,6])\r\n\r\nre = tf.reshape( tf.matmul(a, reshape_A), [3, 3, 2])\r\n\r\nwith tf.Session() as sess:\r\nprint(sess.run(re))\r\nre=re.eval()\r\n\r\nBut is there an easier way?", "comments": ["@goljx This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\n", "@goljx Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28423, "title": "tf.contrib.summary.create_file_writer 'name' parameter is not recognized", "body": "While creating summary writer in eager execution, name parameter is not recognized properly, hence all the graphs are getting associated with single (default) name.\r\n```\r\nlogdir = 'logs'\r\nmodel_name = 'mnist-dense-128'\r\nsummary_writer = tf.contrib.summary.create_file_writer(logdir, flush_millis=10000,\r\nname=model_name)\r\nsummary_writer.set_as_default()\r\n```\r\nAttaching [screenshot](https://drive.google.com/open?id=1bwOFOA631vtBvejoXGu9Xf6oVTXtqnVI) of Tensorboard for reference\r\n\r\nTensorflow version: 1.13", "comments": ["@Kshitij09 I think it is more related to tensorboard repo. Could you post it [there](https://github.com/tensorflow/tensorboard/issues). If you think it is more related to TF Core, then please provide a standalone code to reproduce the issue. Thanks!", "@jvishnuvardhan I found that, we used to create sub-directories in order to maintain separate logs of models. Tensorboard label the respective statistics by their directory name. Is this the only way to assign names to graphs ? then what's the use of 'name' parameter in `create_file_writer()` method ?", "Closing this out since it is more related to Tensorboard repo. Thanks!"]}, {"number": 28422, "title": "Added support for Atrous Conv2D batch normalization folding", "body": "Added support for Atrous Conv2D batch normalization folding to handle the BatchToSpace and SpaceToBatch properly in tensorflow/contrib/quantize/python/fold_batch_norm.py\r\n\r\nAlso added unit tests for future development of bn folding for Atrous Conv2D in fold_batch_norm_test.py\r\nThe existing `_TestFoldAtrousConv2d` test was testing batch normalization folding for Depthwise Atrous Conv2D, hence renamed that test to a relevant name `_TestFoldDepthwiseAtrousConv2d.` All BN folding unit tests are passing locally on my machine.", "comments": ["gentle ping for request to review this. @rthadur @suharshs ", "Can one of the admins verify this patch?", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac"]}, {"number": 28420, "title": "ImportError: DLL load failed: The specified module could not be found. (Windows)", "body": "System:\r\n\r\nOS Name\tMicrosoft Windows 10 Pro\r\nVersion\t10.0.17763 Build 17763\r\n\r\nCUDA Version 10.0\r\n\r\nNOT RUNNING A CONDA ENVIRONMENT\r\n\r\npython version: 3.6.8\r\n\r\nInstalled via the documentation @ https://www.tensorflow.org/install/gpu\r\n\r\npip install tensorflow-gpu\r\n\r\nTried to test if tensorflow was loaded\r\n\r\npython\r\n\r\n>> import tensorflow as tf\r\n\r\nError (entire shell output):\r\n\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n", "comments": ["Have you installed cuDNN?", "Missing cuDNN. My bad. Closing issue."]}, {"number": 28419, "title": "Cannot import tensorflow on yarn", "body": "When I tried to upload all tensorflow and google.protocol pkgs to yarn, I come across a problem like this\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data05/yarn/nmdata/usercache/tiger/appcache/application_1557054435006_18114/container_e117_1557054435006_18114_01_000009/./mapreduce_text.py\", line 34, in <module>\r\n    from extract_features_tmp import predict\r\n  File \"./bert/bin/extract_features_tmp.py\", line 15, in <module>\r\n    import tensorflow as tf\r\n  File \"./tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"./tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"./tensorflow/core/framework/graph_pb2.py\", line 8, in <module>\r\n    from google.protobuf import reflection as _reflection\r\n  File \"/data01/yarn/nmdata/usercache/tiger/filecache/88784/google.tar.gz/protobuf/reflection.py\", line 62, in <module>\r\n    GeneratedProtocolMessageType = message_impl.GeneratedProtocolMessageType\r\nAttributeError: 'module' object has no attribute 'GeneratedProtocolMessageType'\r\n```\r\nWhy this happened? I used `tar -czvf` to compress tensorflow and protobuf, used -archives to upload these pkgs.tar.gz to yarn.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28418, "title": "[2.0 alpha] tf.keras callbacks - Invalid argument \"callback\" passed to K.function with TensorFlow backend", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nManjaro Linux\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below):\r\n\r\n- Python version:\r\n- CUDA/cuDNN version: 2.0 alpha\r\n\r\n**Describe the current behavior**\r\n\r\n```ValueError: Invalid argument \"callback\" passed to K.function with TensorFlow backend```\r\n\r\n**Describe the expected behavior**\r\n\r\n- Run as normal like in previous version's of tf/keras.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\r\nmodel = tf.keras.models.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=10))\r\nmodel.add(tf.compat.v1.keras.layers.CuDNNLSTM(50))\r\nmodel.add(tf.keras.layers.Dense(50, activation='relu'))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dropout(0.2))\r\n\r\n\r\nmodel.add(tf.keras.layers.Dense(1,activation='sigmoid'))\r\n\r\nes = tf.keras.callbacks.EarlyStopping(monitor='binary_crossentropy', patience=10)\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'],callback=[es])\r\n\r\nmodel.fit(train_padded_docs, train_labels,validation_data=(val_padded_docs,val_labels), epochs=15, verbose=1,batch_size=10000)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```python\r\nTrain on 200000 samples, validate on 20000 samples\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-14-7ae2a63ebc51> in <module>\r\n----> 1 model.fit(train_padded_docs, train_labels,validation_data=(val_padded_docs,val_labels), epochs=15, verbose=1,batch_size=10000)\r\n\r\n~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    871           validation_steps=validation_steps,\r\n    872           validation_freq=validation_freq,\r\n--> 873           steps_name='steps_per_epoch')\r\n    874 \r\n    875   def evaluate(self,\r\n\r\n~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    149 \r\n    150   # Get step function and loop type.\r\n--> 151   f = _make_execution_function(model, mode)\r\n    152   use_steps = is_dataset or steps_per_epoch is not None\r\n    153   do_validation = val_inputs is not None\r\n\r\n~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in _make_execution_function(model, mode)\r\n    519   if model._distribution_strategy:\r\n    520     return distributed_training_utils._make_execution_function(model, mode)\r\n--> 521   return model._make_execution_function(mode)\r\n    522 \r\n    523 \r\n\r\n~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _make_execution_function(self, mode)\r\n   2229   def _make_execution_function(self, mode):\r\n   2230     if mode == ModeKeys.TRAIN:\r\n-> 2231       self._make_fit_function()\r\n   2232       return self._fit_function\r\n   2233     if mode == ModeKeys.TEST:\r\n\r\n~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _make_fit_function(self)\r\n   2175     ]\r\n   2176     self._make_train_function_helper(\r\n-> 2177         '_fit_function', [self.total_loss] + metrics_tensors)\r\n   2178 \r\n   2179   def _make_test_function_helper(self, fn_name, outputs):\r\n\r\n~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _make_train_function_helper(self, fn_name, outputs)\r\n   2160             updates=updates,\r\n   2161             name='train_function',\r\n-> 2162             **self._function_kwargs)\r\n   2163         setattr(self, fn_name, fn)\r\n   2164 \r\n\r\n~/anaconda3/envs/AI/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in function(inputs, outputs, updates, name, **kwargs)\r\n   3249         msg = ('Invalid argument \"%s\" passed to K.function with TensorFlow '\r\n   3250                'backend') % key\r\n-> 3251         raise ValueError(msg)\r\n   3252   return GraphExecutionFunction(inputs, outputs, updates=updates, **kwargs)\r\n   3253 \r\n\r\nValueError: Invalid argument \"callback\" passed to K.function with TensorFlow backend\r\n\r\n```\r\n", "comments": ["@jkotra Can you please provide the whole snippet to reproduce the issue, when I ran the code provided here, it says 'vocabsize' not defined", "it's my mistake.\r\n\r\n```callbacks``` should be passed to ```fit()```.i was trying to pass it to ```compile()```.\r\n\r\nall works fine now :)", "@jkotra As this is resolved, closing the issue.", "@jkotra thanks a lot...just found I made the same mistake.", "@jkotra same mistake, thanks a lot :D."]}, {"number": 28417, "title": "Reloaded replicated model did not work  for distributed training", "body": "I am running distributed tutorial as following. I can use the unsaved 'model' in replicated mode to evaluate but the reloaded replicated_model didn't work.\r\nI am using CentOS7.5+Python3.7.1. I am not sure what's wrong with this.\r\nIn [45]: with strategy.scope():\r\n    ...:     replicated_model=tf.keras.experimental.load_from_saved_model(path)\r\n    ...:     replicated_model.compile(loss='sparse_categorical_crossentropy',\r\n    ...:             optimizer=tf.keras.optimizers.Adam(),\r\n    ...:             metrics=['accuracy'])\r\n    ...:     eval_loss,eval_acc=replicated_model.evaluate(eval_dataset)\r\n    ...:\r\n2019-05-06 00:59:35.889966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0, 1\r\n2019-05-06 00:59:35.890048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-06 00:59:35.890110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 1\r\n2019-05-06 00:59:35.890119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N Y\r\n2019-05-06 00:59:35.890126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 1:   Y N\r\n2019-05-06 00:59:35.890631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21625 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:3b:00.0, compute capability: 6.1)\r\n2019-05-06 00:59:35.890865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 21625 MB memory) -> physical GPU (device: 1, name: Tesla P40, pci bus id: 0000:d8:00.0, compute capability: 6.1)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-45-9147a4df43ec> in <module>\r\n      3     replicated_model.compile(loss='sparse_categorical_crossentropy',\r\n      4             optimizer=tf.keras.optimizers.Adam(),\r\n----> 5             metrics=['accuracy'])\r\n      6     eval_loss,eval_acc=replicated_model.evaluate(eval_dataset)\r\n      7\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    454     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    455     try:\r\n--> 456       result = method(self, *args, **kwargs)\r\n    457     finally:\r\n    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    459                 'with strategy.scope():\\n'\r\n    460                 '  model=_create_model()\\n'\r\n--> 461                 '  model.compile(...)'% (v, strategy))\r\n    462\r\n    463   @property\r\n\r\nValueError: Variable (<tf.Variable 'conv2d_1_10/kernel:0' shape=(3, 3, 1, 32) dtype=float32>) was not created in the distribution strategy scope of (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f4a5a7814a8>). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.\r\nwith strategy.scope():\r\n  model=_create_model()\r\n  model.compile(...)", "comments": ["@bluesbeyond Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28416, "title": "[LITE] uint8, int8, int16, datatype support for lite ops", "body": "", "comments": ["@alanchiao could you please help to review this changes. TIA\r\n\r\n", "Can one of the admins verify this patch?", "@siju-samuel Could you please resolve the conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 28415, "title": "Can't Reproduce Results With tf.keras.layers.Conv2D", "body": "Putting the code below in the beginning of my script, I can consistently reproduce the result 100%. However this is only true if I only use Dense layer.\r\n\r\n```\r\nimport numpy as np\r\nimport random as rn\r\nimport tensorflow as tf\r\nimport os\r\nos.environ['PYTHONHASHSEED'] = '0'\r\nnp.random.seed(1)\r\nrn.seed(2)\r\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\nfrom tensorflow.keras import backend as K\r\ntf.set_random_seed(3)\r\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\r\nK.set_session(sess)\r\n```\r\n\r\nIf I insert this one line \"model.add(Conv2D(32, 3, activation='relu'))\" before \"model.add(Flatten())\", it produces different results every time I rerun.\r\n\r\nInput> flatten > dense produces consistent result, but input > conv2d > flatten > dense produces different result every time I run the code.\r\n\r\nIt looks like a bug to me, but I apologize if isn't.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Debian GNU/Linux 9.8 (stretch) (GNU/Linux 4.9.0-8-amd64 x86_64)\r\n- TensorFlow installed from (source or binary): conda install -c anaconda tensorflow-gpu\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.1\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla P4 7611MiB\r\n", "comments": ["@jsl303  In order to expedite the trouble-shooting process, please provide whole code snippet to reproduce the issue reported here. Thanks!\r\n", "Here's the code. [reproduce.txt](https://github.com/tensorflow/tensorflow/files/3150090/reproduce.txt)\r\nThe result for 10 runs is below. What's interesting is that Run 1 and 5, Run 2 and 3, run 7 and 8, Run 6 and 10 match in the model with conv layers.\r\n\r\n```\r\nRun 1: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9909999966621399\r\nRun 2: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.991100013256073\r\nRun 3: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.991100013256073\r\nRun 4: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9918000102043152\r\nRun 5: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9909999966621399\r\nRun 6: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9907000064849854\r\nRun 7: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9912999868392944\r\nRun 8: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9912999868392944\r\nRun 9: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9904999732971191\r\nRun 10: Dense accuracy: 0.9431999921798706, Conv accuracy: 0.9907000064849854\r\n```", "@jsl303 After running the code with epoch=1 got these results.\r\nDense accuracy: 0.5679987038135529, Conv accuracy: 0.06941995525737293\r\nDense accuracy: 0.5325789436578751, Conv accuracy: 0.06686571390144527\r\nDense accuracy: 0.5519545083522797, Conv accuracy: 0.06901443190686404\r\nDense accuracy: 0.547787981891632, Conv accuracy: 0.06690254995841532\r\nDense accuracy: 0.5438883091449738, Conv accuracy: 0.09017947126738728\r\n\r\nFor epoch=12 these are the below results.\r\n\r\nDense accuracy: 0.19799944971501826, Conv accuracy: 0.6604872384548187\r\n 0.1981875723950565, Conv accuracy: 0.6766708342552185\r\n\r\nAs this is the custom code, This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "@jsl303 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28414, "title": "[C API] support for CudnnCompatibleLSTMCell in native code", "body": "\r\n**System information**\r\n- Have I written custom code : Yes\r\n- OS Platform and Distribution : Ubuntu 16.04\r\n- TensorFlow installed from : binary\r\n- TensorFlow version : 1.12\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: CUDA 9.0 /  CuDNN 7,1\r\n- GPU model and memory: NVidia p100, 16 gb\r\n\r\n**Bug description**\r\nAfter training the model in Python with CudnnCompatibleLSTMCell used and running the graph-freeze procedure I then try to load this model in my custom inference app that uses C-API. I use pre-compiled version of TF 1.12 for Linux. At the call to TF_GraphImportGraphDef(...) I get error TF_NOT_FOUND in the returned status. For more details please see code bellow. \r\n\r\n**Question**\r\nAfter short look through the code of TF 1.12 I cannot find clear evidence of 'CudnnCompatibleLSTMCell' being supported at all at native level. Could someone confirm that this cell can be used at all from C-API?\r\n\r\n**Code to reproduce the issue**\r\n```c++\r\nTF_Graph* LoadGraphDef(const char* file) {\r\n  TF_Buffer* buffer = ReadBufferFromFile(file);\r\n\r\n  TF_Graph* graph = TF_NewGraph();\r\n  TF_Status* status = TF_NewStatus();\r\n  TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();\r\n\r\n  TF_GraphImportGraphDef(graph, buffer, opts, status);\r\n  TF_DeleteImportGraphDefOptions(opts);\r\n  TF_DeleteBuffer(buffer);\r\n\r\n  printf(\"STATUS : %d\\n\", TF_GetCode(status));\r\n\r\n  TF_DeleteStatus(status);\r\n\r\n  return graph;\r\n}\r\n\r\nstatic TF_Buffer* ReadBufferFromFile(const char* file) {\r\n  const auto f = std::fopen(file, \"rb\");\r\n  if (f == nullptr) {\r\n    return nullptr;\r\n  }\r\n\r\n  std::fseek(f, 0, SEEK_END);\r\n  const auto fsize = ftell(f);\r\n  std::fseek(f, 0, SEEK_SET);\r\n\r\n  if (fsize < 1) {\r\n    std::fclose(f);\r\n    return nullptr;\r\n  }\r\n\r\n  const auto data = std::malloc(fsize);\r\n  std::fread(data, fsize, 1, f);\r\n  std::fclose(f);\r\n\r\n  TF_Buffer* buf = TF_NewBuffer();\r\n  buf->data = data;\r\n  buf->length = fsize;\r\n  buf->data_deallocator = DeallocateBuffer;\r\n\r\n  return buf;\r\n}\r\n\r\nstatic void DeallocateBuffer(void* data, size_t) {\r\n  std::free(data);\r\n}\r\n```", "comments": ["Were you using the contrib version of the cudnn compatible lstm cell?\r\n\r\nIf so, this op is not automatically loaded into the tensorflow binary and you need to dlopen the kernel file to make it work.\r\n\r\nI suggest instead you move to 1.13 which has this op in core tf, so it won't require dynamic linking.", "Hello, Alexandre.\r\nYes, I'm using the contrib version. By using 1.13 you mean creating/freezing the model using 1.13 or trying to load it using 1.13 C-library?\r\n\r\nIn first case - I can see in documentation that CuDNN-compatible LSTM cell is still in 'contrib' by looking at  https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnCompatibleLSTMCell\r\nDo you mean something different or mb some other tf module that I've missed?\r\n\r\nIn second case - I've already tried to load this model using 1.13 C-library, but with no success, still getting same error.\r\n\r\nPS\r\nAnother thing that caught my attention. CudnnLSTM cell is also in contrib, but I could train model with it and later use it with tf 1.12 c-library without any issues at all. Any comments on that?\r\n\r\n\r\n", "I meant both creating it and loading it with 1.13.\r\n\r\nCan you also post the full content of the not found status error message?", "Hi.\r\nI've tried re-training the model on machine with TF 1.13 but got same result : when trying to run the inference using C-API and TF 1.13 C-library I get error TF_NOT_FOUND. TF_Message() returns : \r\n\r\n`Op type not registered 'LSTMBlockCell' in binary running on PG-LAPTOP-W10. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`\r\n\r\nAny other advice?", "You need to dlopen the file containing the opdef, it's named _lstm_ops.so\nhttps://github.com/tensorflow/tensorflow/blob/ffa77644b2bc586d07eb0265c29c486897f9e4a0/tensorflow/contrib/rnn/python/ops/lstm_ops.py#L38\n\nOn Mon, May 20, 2019 at 11:31 AM Peter Glushkov <notifications@github.com>\nwrote:\n\n> Hi.\n> I've tried re-training the model on machine with TF 1.13 but got same\n> result : when trying to run the inference using C-API and TF 1.13 C-library\n> I get error TF_NOT_FOUND. TF_Message() returns :\n>\n> Op type not registered 'LSTMBlockCell' in binary running on PG-LAPTOP-W10.\n> Make sure the Op and Kernel are registered in the binary running in this\n> process. Note that if you are loading a saved graph which used ops from\n> tf.contrib, accessing (e.g.) tf.contrib.resampler should be done before\n> importing the graph, as contrib ops are lazily registered when the module\n> is first accessed.\n>\n> Any other advice?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28414?email_source=notifications&email_token=AAABHRPKMGIPNXCVOLO3WZDPWLVAXA5CNFSM4HK3LXAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVOXY#issuecomment-494098271>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLYPXEPUZ2ZRI6C4UDPWLVAXANCNFSM4HK3LXAA>\n> .\n>\n\n\n-- \n - Alex\n", "I've been using pre-compiled version of TF C-API lib provided by TF-community (presumably), I've downloaded it from:  https://www.tensorflow.org/install/lang_c\r\n\r\nThat bundle does not contain any other .so's except for libtensorflow_framework.so  and libtensorflow.so. Do I understand correctly that to perform actions that you advise I'll need to manually compile Tensorflow 1.13 for Linux? Or maybe there is some other place where I can download all the .so libs pre-compiled?\r\n\r\n", "If you download the full python distribution it has the .so libs from\ncontrib as well.\n\nOn Tue, May 21, 2019 at 4:38 AM Peter Glushkov <notifications@github.com>\nwrote:\n\n> I've been using pre-compiled version of TF C-API lib provide by\n> TF-community (presumably), I've downloaded it from:\n> https://www.tensorflow.org/install/lang_c\n>\n> That bundle does not contain any other .so except for\n> libtensorflow_framework.so and libtensorflow.so. Do I understand correctly\n> that to do actions that you advise - I'll need to manually compile\n> Tensorflow 1.13 for Linux? Or maybe there is some other place where I can\n> download all the .so libs?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28414?email_source=notifications&email_token=AAABHRK4S4GV2RASCM5FBETPWPNMXA5CNFSM4HK3LXAKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3TUOA#issuecomment-494352952>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJJWNPUJFT27GFDSJDPWPNMXANCNFSM4HK3LXAA>\n> .\n>\n\n\n-- \n - Alex\n", "Manually 'dl-opening' the **_lstm_ops.so** helped! Model could be loaded at last, thank you very much!!\r\nNow will need to resolve some issues with the actual synthesis :) ", "Can we close this?", "Yes, I thought it was closed, sorry for confusion.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28414\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28414\">No</a>\n"]}]