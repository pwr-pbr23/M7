[{"number": 46700, "title": "tf.math.reduce_prod aborts when keepdims contain large values", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\n`tf.math.reduce_prod` aborts when `keepdims` contain large values\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input is not expected, instead of crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.math.reduce_prod(input_tensor=1, keepdims=np.array([63600, 1], dtype=np.float16))\r\n~~~\r\n\r\n\r\nOutput:\r\n~~~python\r\n2021-01-26 17:02:24.497049: F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred()\r\nAborted (core dumped)\r\n~~~", "comments": ["@ymodak \r\nI ran the code on tf 2.4 and nightly but colab crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/0f6ab8c1a411eac72ae6e4502faed92b/untitled509.ipynb).", "Added a PR #46741 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46700\">No</a>\n"]}, {"number": 46699, "title": "tf.keras.backend.constant abortion", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.backend.constant` abortion\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input is not expected, instead of crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.keras.backend.constant(value=np.ones((0,1,1)), shape=[36,23,53,24,117,82,47,124,112,69,53,0])\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-01-26 16:30:57.093291: F tensorflow/core/framework/tensor_shape.cc:405] Check failed: 0 <= new_num_elements (0 vs. -1)\r\nAborted (core dumped)\r\n~~~", "comments": ["I have tried in colab with TF version 2.1, nightly version(`2.5.0-dev20210126`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/20eee659ad8f62f7d9af60e27df1ac65/untitled634.ipynb). Thanks!", "I think the issue will be fixed by PR #46717.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46699\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46699\">No</a>\n"]}, {"number": 46698, "title": "tf.sequence_mask abortion when lengths contains large value", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.sequence_mask` abortion when lengths contains large value\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input is not expected, instead of crash.\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.sequence_mask(lengths=np.array([3.05524638e+307], dtype=np.float64))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-01-26 16:02:42.088240: F tensorflow/core/framework/tensor_shape.cc:187] Non-OK-status: InitDims(dim_sizes) status: Internal: Expected shape dimensions to be non-negative, got -9223372036854775808\r\nAborted (core dumped)\r\n~~~\r\n\r\n", "comments": ["I ran the code on tf 2.4 and tf-nightly colab crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/37011b0996cc4cebc25af92ee4fa1839/untitled509.ipynb)", "Added a PR #46742 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46698\">No</a>\n", "Reopening as #46742 was rolled back", "Was able to reproduce in Nightly version TF 2.6 and the colab crashes. Pease find the gist [here](https://colab.research.google.com/gist/saikumarchalla/5df7d4f625bc4be94b76698dd78a1517/untitled92.ipynb#scrollTo=UnuuCWdFJcbp). Thanks1", "Hi @DNXie ! I think this bug has been addressed now. I getting value error instead of [Colab ](https://colab.sandbox.google.com/gist/mohantym/69e5e1575579a85cc94199ca9f03da35/github_46698.ipynb)getting crashed. Thanks!", "@mohantym It seems to be fixed in the nightly version also. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46698\">No</a>\n"]}, {"number": 46697, "title": "fixed the output description of bucketized column to one-hot encoded value", "body": "Bucketized Column returns one-hot encoded value.\r\nShould align to this page.\r\nhttps://www.tensorflow.org/tutorials/structured_data/feature_columns#bucketized_columns", "comments": ["@takumiohym  Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "@takumiohym Any update on this PR? Please. Thanks!\r\n", "Closing as there has not been any update after feedback."]}, {"number": 46696, "title": "tf.math.segment_min abortion when segment_ids contains large value", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.math.segment_min` abortion when `segment_ids` contains large value\r\n**Describe the expected behavior**\r\nexpect an exception message if the input is not expected, instead of crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.math.segment_min(data=np.ones((1,2,1,1,1), dtype=np.uint16), segment_ids=[5053997376933981534])\r\n~~~\r\n\r\noutput:\r\n~~~python\r\n2021-01-26 15:49:10.870684: F tensorflow/core/framework/tensor_shape.cc:405] Check failed: 0 <= new_num_elements (0 vs. -8338749319841588546)\r\nAborted (core dumped)\r\n~~~", "comments": ["I ran the code on tf 2.4 and nightly colab crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7b803fee1ccf099305e738c5cb124824/untitled505.ipynb).", "@DNXie,\r\nCan you please confirm if we can close this issue as this case has been covered in #46888? Thanks! ", "@rmothukuru Yes it should be covered by #46888", "Closing the issue as it is the duplicate of #46888. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46696\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46696\">No</a>\n"]}, {"number": 46695, "title": "fixed the output description of bucketized column to one-hot encoded value", "body": "", "comments": []}, {"number": 46694, "title": "At least read the BogoMIPS figure on ARM", "body": "Use the capitalization of BogoMIPS that is used by ARM architecture CPUs.", "comments": ["Pretty much a dup of #46643 "]}, {"number": 46693, "title": "tf.keras.backend.reshape abortion when shape contain large values", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.keras.backend.reshape` abortion when `shape` contain large values\r\n\r\n**Describe the expected behavior**\r\nexpect an exception message if the input is not expected, instead of crash. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n~~~python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntf.keras.backend.reshape(x=[1], shape=np.array([21943, 45817, 30516, 61760, 38987], dtype=np.uint16))\r\n~~~\r\n\r\nOutput:\r\n~~~python\r\n2021-01-26 15:32:50.289333: F tensorflow/core/framework/tensor_shape.cc:405] Check failed: 0 <= new_num_elements (0 vs. -1)\r\nAborted (core dumped)\r\n~~~", "comments": ["I have tried in colab with TF version 2.1, nightly version(2.5.0-dev20210126) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/a63148028cea8674427b82f612d8adeb/untitled636.ipynb). Thanks!", "Added a PR #46717 for the fix.", "@yongtang Thanks for th PR!\r\n\r\nBTW I just found similar abortion in `tf.reshape` and `tf.constant`.\r\n\r\nHere are the reproduce code:\r\n~~~python\r\ntf.reshape(tensor=[1], shape=np.array([21943, 45817, 30516, 61760, 38987], dtype=np.uint16))\r\ntf.constant(value=np.ones((0,1,1)), shape=[36,23,53,24,117,82,47,124,112,69,53,0])\r\n~~~\r\n\r\nCould you please make sure that the PR also fixes these two APIs? Thanks!\r\n", "@DNXie Yes tf.reshape and tf.keras.backend.reshape are through the same kernel so both will be fixed by  PR #46717 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46693\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46693\">No</a>\n"]}, {"number": 46692, "title": "Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 10,4]", "body": "Im useing tensorflow - object detection my questions is like this but!\r\n\r\nhttps://stackoverflow.com/questions/57393407/java-lang-illegalargumentexception-cannot-copy-between-a-tensorflowlite-tensor\r\n\r\n**java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 10,4].**\r\n\r\na guy answerd to this questions and sayed your should change in python side! but he didnt talk about which lines or which .py codes Im useing mobile_ssd_v2_float and too many people cant solve it! Is there any answers for this question?\r\n\r\nthis is pic of my .tflite file form Netron\r\n![yez6b](https://user-images.githubusercontent.com/42836468/105861333-d9317e80-6003-11eb-91a4-452a60b7e632.png)\r\n", "comments": ["@lintian06 could you take a look at this?", "@abattery @lintian06 can you help? ", "@MeysamSMH7,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "@MeysamSMH7 The model you use and the model in that example have different outputs.\r\nAs you might see here https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java, the model we use have 4 outputs, while your model has one.\r\n", "@amahendrakar hi\r\nVersion of TensorFlow is 1.2\r\nAnd I'm using this one\r\nSsd_mobilenet_v2_coco_2018\r\nPayton version is 2", "@thaink, hi\r\nIt's not possible!\r\nI've changed input to [1,1917,4]\r\nBut I've got this error:\r\nInvalid index 1", "@MeysamSMH7 The problem here is the outputs not input.\r\nPlease compare you model with https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/metadata/2?lite-format=tflite to see the differences.", "@thaink \r\nI know differences, but I can't use my own .tflite\r\nCan I add limitations for output model? Default is 1917 but maybe can change it into 10?", "@MeysamSMH7 The difference is the TFLite_Detection_PostProcess op is added at the output of our models.\r\nYou can also try to add that following: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\nPlease note the --add_postprocessing flag.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46692\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46692\">No</a>\n"]}, {"number": 46691, "title": "[ROCm] Update script install_pip_packages.sh for Python 2.7", "body": "Currently ROCm TF cotnainers use the `install_pip_packages.sh` script to install the Python pip package-manager.\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.rocm#L102\r\n* https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_pip_packages.sh#L20-L23\r\n\r\nStarting 01/23/2021, we started getting the following error while building TF containers for ROCM CI (when calling `get-pip.py` for Python2)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"get-pip.py\", line 24226, in <module>\r\n    main()\r\n  File \"get-pip.py\", line 199, in main\r\n    bootstrap(tmpdir=tmpdir)\r\n  File \"get-pip.py\", line 82, in bootstrap\r\n    from pip._internal.cli.main import main as pip_entry_point\r\n  File \"/tmp/tmpWkL0gn/pip.zip/pip/_internal/cli/main.py\", line 60\r\n    sys.stderr.write(f\"ERROR: {exc}\")\r\n```\r\n\r\nThe cause seems to be an update to the `get-pip.py` script, which now picks the version `pip-21.0` (previously it was `pip-20.3.4`).\r\n`pip-21.0` drops support for Python2.7 (as indicated by the following warning message)\r\n```\r\nDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020.\r\nPlease upgrade your Python as Python 2.7 is no longer maintained.\r\npip 21.0 will drop support for Python 2.7 in January 2021.\r\nMore details about Python 2 support in pip can be found at\r\nhttps://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\npip 21.0 will remove support for this functionality.\r\n```\r\n\r\nIt seems that there is now a Python 2.7 specific version of the `get-pip.py` script that we need to use, and that is what this commit does\r\n\r\nNote: Although I am filing this PR as a ROCm specific PR, this issue + fix is probably applicable to all users of the `install_pip_packages.sh` script.\r\n\r\n---------------------------------------------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work \r\n\r\n", "comments": ["@chsigg gentle ping....ROCm CSB is blocked by this PR.", "@chsigg gentle ping", "@chsigg gentle ping", "If Python 2.7 has reached end of life, shouldn't we just drop support for it? Admittedly, I didn't check who and how it's still being used.", "> If Python 2.7 has reached end of life, shouldn't we just drop support for it? Admittedly, I didn't check who and how it's still being used.\r\n\r\nthat would be ideal, but as you said we do not know who/what it will impact. Can we take this PR for now, and have the correct person in G look at making the bigger change to drop Python 2.x support + references in this and other scripts?", "@mihaimaruseac, do I understand correctly that we do not build TF for Python 2.7 anymore and this change is only needed for ROCm's CI?", "We dropped support for py2 last year.\r\n\r\nWe should no longer take PRs that only affect py2 support.", "@mihaimaruseac \r\n\r\ncan I then go ahead and modify this PR to instead, drop all `pip2` commands from the `install_pip_packages.sh` script?", "Sure, that is a better fix.", "> Sure, that is a better fix.\r\n\r\n@mihaimaruseac , done...please review"]}, {"number": 46690, "title": "predictions = model(inputs) outputs nan", "body": "Hi\r\n\r\nThis is a great project and is very useful, but I have encountered an error with model predicting\r\n\r\nI have used an lstm as the model input. Once I run window.plot(model=lstm_model) the output of predictions is just a 2D array with \"nan\" as each value. The shape of this array is 2, 79, 1\r\n\r\nPlease help\r\n\r\nP.S. Here's my code for reference\r\n\r\nprint(\"Initiating the time series forcasting module\")\r\n\r\nprint(\"Importing modules\")\r\nimport pandas as pd\r\nimport os\r\nimport matplotlib as mpl\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport tensorflow as tf\r\n\r\nprint(\"Collecting data\")\r\ndf = pd.read_csv(\"data.csv\")\r\ndates = pd.to_datetime((df.pop(\"Date\")), format = \"%Y/%m/%d\")\r\n\r\nprint(\"Splitting data\")\r\ncolumn_indices = {name: i for i, name in enumerate(df.columns)}\r\nn = len(df)\r\ntrain_df = df[0:int(n*0.9)]\r\ntest_df = df[int(n*0.9):]\r\nprint(str(len(train_df)) + \" plots for training\")\r\nprint(str(len(test_df)) + \" plots for testing\")\r\nnum_features = df.shape[1]\r\n\r\nprint(\"Normalizing data\")\r\ntrain_mean = train_df.mean()\r\ntrain_std = train_df.std()\r\ntrain_df = (train_df - train_mean) / train_std\r\ntest_df = (test_df - train_mean) / train_std\r\ndf_std = (df - train_mean) / train_std\r\n\r\nprint(\"Creating classes\")\r\nclass WindowGenerator():\r\n    def __init__(self, input_width, label_width, shift,\r\n                label_columns=None, train_df=train_df, test_df=test_df):\r\n        print(\"Generating windows\")\r\n        self.train_df = train_df\r\n        self.test_df = test_df\r\n        \r\n        self.label_columns = label_columns\r\n        if label_columns is not None:\r\n            self.label_column_indices = {name: i for i, name in\r\n                                         enumerate(label_columns)}\r\n        self.column_indices = {name: i for i, name in\r\n                               enumerate(train_df.columns)}\r\n        self.input_width = input_width\r\n        self.label_width = label_width\r\n        self.shift = shift\r\n        \r\n        self.total_window_size = input_width + shift\r\n        self.input_slice = slice(0, input_width)\r\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\r\n        \r\n        self.label_start = self.total_window_size - self.label_width\r\n        self.labels_slice = slice(self.label_start, None)\r\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\r\n    \r\n    def __repr__(self):\r\n        return \"\\n\".join([\"Total window size: \" + str(self.total_window_size),\r\n                          \"Input indices: \" + str(self.input_indices),\r\n                          \"Label indices: \" + str(self.label_indices),\r\n                          \"Label column names: \" + str(self.label_columns)])\r\n    \r\n    def split_window(self, features):\r\n        print(\"Splitting windows\")\r\n        inputs = features[:, self.input_slice, :]\r\n        labels = features[:, self.labels_slice, :]\r\n        if self.label_columns is not None:\r\n            labels = tf.stack(\r\n                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\r\n                axis=-1)\r\n        inputs.set_shape([None, self.input_width, None])\r\n        labels.set_shape([None, self.label_width, None])\r\n        return inputs, labels\r\n    \r\n    def plot(self, plot_col, model=None, max_subplots=3):\r\n        print(\"Plotting data\")\r\n        inputs, labels = self.example\r\n        plt.figure(figsize=(12,8))\r\n        plot_col_index = self.column_indices[plot_col]\r\n        max_n = min(max_subplots, len(inputs))\r\n        for n in range(max_n):\r\n            plt.subplot(3,1,n+1)\r\n            plt.ylabel(plot_col)\r\n            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\r\n                     label=\"Inputs\", marker=\".\", zorder=-10)\r\n            if self.label_columns:\r\n                label_col_index = self.label_column_indices.get(plot_col, None)\r\n            else:\r\n                label_col_index = plot_col_index\r\n                \r\n            if plot_col_index is None:\r\n                continue\r\n            \r\n            plt.scatter(self.label_indices, labels[n, :, label_col_index],\r\n                        edgecolors=\"k\", label=\"Labels\", c='#2ca02c', s=64)\r\n            \r\n            if model is not None:\r\n                predictions = model(inputs)\r\n                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\r\n                    marker='X', edgecolors='k', label='Predictions',\r\n                    c='#ff7f0e', s=64)\r\n            if n == 0:\r\n                plt.legend()\r\n        plt.xlabel(\"Date\")\r\n        plt.show()\r\n    \r\n    def make_ds(self,data):\r\n        print(\"Generating dataset\")\r\n        data = np.array(data, dtype=np.float32)\r\n        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n            data=data,\r\n            targets=None,\r\n            sequence_length=self.total_window_size,\r\n            sequence_stride=1,\r\n            shuffle=True,\r\n            batch_size=32,)\r\n        ds = ds.map(self.split_window)\r\n        return ds\r\n    \r\n    @property\r\n    def train(self):\r\n        return self.make_ds(self.train_df)\r\n    \r\n    @property\r\n    def test(self):\r\n        return self.make_ds(self.test_df)\r\n    \r\n    @property\r\n    def example(self):\r\n        result = getattr(self, \"_example\", None)\r\n        if result is None:\r\n            result = next(iter(self.train))\r\n            self._example = result\r\n        return result\r\n    \r\nclass Baseline(tf.keras.Model):\r\n  def __init__(self, label_index=None):\r\n    print(\"Creating baseline model\")\r\n    super().__init__()\r\n    self.label_index = label_index\r\n\r\n  def call(self, inputs):\r\n    if self.label_index is None:\r\n      return inputs\r\n    result = inputs[:, :, self.label_index]\r\n    return result[:, :, tf.newaxis]\r\n    \r\nwide_window = WindowGenerator(\r\n    input_width=24, label_width=24, shift=24,\r\n    label_columns=['ConfirmedCases'])\r\n\r\ndef compile_and_fit(model, window, patience=2, MAX_EPOCHS=20):\r\n  model.compile(loss=tf.losses.MeanSquaredError(),\r\n                optimizer=tf.optimizers.Adam(),\r\n                metrics=[tf.metrics.MeanAbsoluteError()])\r\n\r\n  history = model.fit(window.train, epochs=MAX_EPOCHS)\r\n  return history\r\n\r\nlstm_model = tf.keras.models.Sequential([\r\n    tf.keras.layers.LSTM(32, return_sequences=True),\r\n    tf.keras.layers.Dense(units=1)\r\n])\r\n\r\nhistory = compile_and_fit(lstm_model, wide_window)\r\n\r\nwide_window.plot(model=lstm_model, plot_col=\"ConfirmedCases\")\r\n\r\n", "comments": ["@deboss2020 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced if not shared]", "Hello @Saduf2019 \r\nI have had a look around for the template but I couldn't find it. I will give you the requested information here:\r\ntf version: 2.3.2\r\nsteps followed: I followed the official tensorflow tutorial for time series forcasting with an lstm on their website https://www.tensorflow.org/tutorials/structured_data/time_series", "@deboss2020 \r\nPlease share a colab gist of the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46689, "title": "Bazel cannot grab libstdc++ when paths for them are not default.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: Titan Xp\r\n\r\n\r\n\r\n**Describe the problem**\r\nDuring the installation, bazel cannot link the path for LD_LIBRARY_PATH.\r\nErrors occur when bazel compiles `tensorflow/compiler/...`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Install tensorflow from source with non-default path for LD_LIBRARY_PATH. (e.g. ~/opt/gcc/8.3.0/lib64)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI added some lines into `tensorflow/core/kernels/mlir_generated/build_defs.bzl` to fix this and it works.\r\n```\r\n+++ b/tensorflow/core/kernels/mlir_generated/build_defs.bzl\r\n@@ -53,6 +53,7 @@ def _gen_mlir_op_impl(ctx):\r\n                 ctx.outputs.out.path,\r\n             )\r\n         ),\r\n+        use_default_shell_env=True,\r\n     )\r\n \r\n _gen_mlir_op_rule = rule(\r\n@@ -114,6 +115,7 @@ def _gen_kernel_fatbin_impl(ctx):\r\n             \"--enable_ftz=%s\" % (ctx.attr.data_type == \"f32\"),\r\n         ],\r\n         mnemonic = \"compile\",\r\n+        use_default_shell_env=True,\r\n     )\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46689\">No</a>\n"]}, {"number": 46688, "title": "Fix inference equation and make it more readable", "body": "Replace the denominator in the equation with its square root.\r\nAnd make the multiplication of gamma a little more readable.\r\n\r\nSigned-off-by: Suraj Upadhyay <usuraj35@gmail.com>\r\n\r\nFIxes #46522 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46688) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Hi, I believe line 57 should also be fixed.", "> Hi, I believe line 57 should also be fixed.\r\n\r\nHi, @jiafulow thanks for pointing that out. It nearly slipped \ud83d\ude05\r\n\r\nI have force pushed a new commit fixing the equations at both the places now."]}, {"number": 46687, "title": "[TF2] Converted quantized TFLite model suffers severe precision/recall drop", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS High Sierra\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): 2.5.0-dev20201209\r\n- Trained on custom dataset using  https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_320x320_coco17_tpu-8.config \r\n- tflite_runtime version: 2.5.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/Path/To/Tensorflow/Model/saved_model')\r\ndef representative_data_gen():\r\n  image_urls = glob.glob(os.path.join(\"/Path/To/Imageset\",'*'))[:250]\r\n\r\n  for image in image_urls:\r\n    try:\r\n      img = cv2.imread(image)\r\n      img = cv2.resize(img, (300, 300))\r\n      img = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\r\n      img = img/255.0 \r\n      img = img.astype(np.float32)\r\n      image_list.append(img)\r\n    except Exception as e:\r\n        print(f'{str(e)} : {image}')\r\n        continue\r\n\r\n  image_list = np.array(image_list)\r\n  img = tf.data.Dataset.from_tensor_slices(image_list).batch(1)\r\n  for i in img.take(250):\r\n    yield [i]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT] # size and latency\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.target_spec.supported_types = [tf.int8]\r\ntflite_quant_model = converter.convert()\r\n\r\n#Save\r\ntflite_models_dir = pathlib.Path(\"/filepath/quantized_export_tflite_uint8\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\ntflite_model_quant_file = tflite_models_dir/\"quantized_model.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_quant_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Compiles tflite model successfully.\r\n```\r\n\r\n**Failure details**\r\n\r\nThe Tensorflow model used is a custom trained version of the MobileNet SSD V2 model using the Tensorflow Object Detection API (TF V2). This model is to be deployed on the Google Coral Dev board, and as such requires full integer quantisation (falling back to float32 for the final custom op) when converting to TFLite format. To export the model after training, export_tflite_graph_tf2.py is used (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) , before the quantization step is carried out. Once the post training quantization is complete, the resulting converted .tflite model runs successfully, however upon evaluation (using coco metrics), suffers a severe drop (>50% decrease) in performance compared to the original model, which achieves very high precision/recall when evaluated in the same manner. \r\n\r\nThe code used to generate inference is below:\r\n\r\n```\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nwidth = input_details[0]['shape'][2]\r\nheight = input_details[0]['shape'][1]\r\n\r\n# Load image\r\nimg = cv2.imread(image)\r\n(H, W) = img.shape[:2] # Used to multiply normalized output bounding box co-ords to return to image coords\r\nimg = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\r\nimg = cv2.resize(img, (width, height))\r\n\r\nif input_details[0][\"dtype\"] == np.uint8:\r\n        input_scale, input_zero_point = input_details[0][\"quantization\"]\r\n        img = img / input_scale + input_zero_point\r\n\r\nimg = np.expand_dims(img, axis=0)\r\nimg = np.uint8(img)\r\n\r\n# Generate inference\r\ninterpreter.set_tensor(input_details[0]['index'], img)\r\ninterpreter.invoke()\r\n\r\n# Output\r\nboxes = interpreter.get_tensor(output_details[0]['index'])[0]\r\nclasses = interpreter.get_tensor(output_details[1]['index'])[0]\r\nscores = interpreter.get_tensor(output_details[2]['index'])[0]\r\n\r\n```", "comments": ["Is it possible to share the saved model?", "Please find attached the quantized .tflite model, as well as the saved .pb model that has been run through the tflite converision compatibility script.\r\n\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5879096/model.zip)\r\n", "Hi @sseeds-reply , it might be a known problem in the quantizer. At the end of the model, values are requantized before fed into CONCAT op, and this makes precision/recall fall. \r\nWe're working on a fix in the new quantizer which will be ready in few days. When it's ready, you can try nightly release with following flag to test it again. Will get back to this thread when it's done. :)\r\n\r\n```python\r\n# while converting\r\nconverter.experimental_new_quantizer = True\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/4837376/107171354-ea1bc000-6a05-11eb-84df-b6e1005f86bb.png)\r\n", "Hi, also wanted to add, I tried to convert the saved model and the results do not seem to be good. I also ran the saved model and the results seem to be bad as well. I think maybe I am missing some pre-processing or post-processing functions.\r\n\r\nI made a gist for it, please let me know if I have anything wrong: I used an existing mobilenet_ssd quantized model and the model seems to work ok.\r\n\r\nhttps://gist.github.com/daverim/f22e75b67264c2ae7dfa4a45bbc284b4\r\n\r\nPerhaps the saved model is out of date, or untrained?\r\nThanks,\r\nDavid", "Hi,\r\n\r\nThank you for the suggestion, I shall try using the new converter.\r\nIn the mean time, we have found a workaround in the application logic instead.\r\n\r\nMany thanks for the help", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46687\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46687\">No</a>\n", "Hi! I'm trying to evaluate my tflite model. @sseeds-reply Could you share how are you evaluating your model using coco metrics please? it would really be a great help. Thanks! "]}, {"number": 46686, "title": "Model's allocation of node fails after conversion to TFLite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): tf-nightly (2.5.0)\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n  model_concrete_function = model.inference_decode.get_concrete_function()\r\n  converter = tf.lite.TFLiteConverter.from_concrete_functions(\r\n      [model_concrete_function]\r\n  )\r\n  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n  \r\n  if args.quantized:\r\n      converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      output_file_name = os.path.join(args.outdir, 'model_quant.tflite')\r\n  else:\r\n      output_file_name = os.path.join(args.outdir, 'model.tflite')\r\n  \r\n  tflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2021-01-26 09:30:17.656931: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-01-26 09:30:17.656958: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n/home/dmmatwic/anaconda3/envs/tflite_x86/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.5.0-dev20210125).\r\nTensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not.\r\nIf you encounter a bug, do not file an issue on GitHub.\r\nwarnings.warn(\r\n2021-01-26 09:30:20.220470: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2021-01-26 09:30:20.220498: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-01-26 09:30:20.220517: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dev-dsk-dmmatwic-1b-a5a0da5a.eu-west-1.amazon.com): /proc/driver/nvidia/version does not exist\r\n2021-01-26 09:30:20.220718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING:tensorflow:From /home/dmmatwic/anaconda3/envs/tflite_x86/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5039: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe validate_indices argument has no effect. Indices are always validated on CPU and never validated on GPU.\r\n2021-01-26 09:30:22,167 (deprecation:528) WARNING: From /home/dmmatwic/anaconda3/envs/tflite_x86/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:5039: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe validate_indices argument has no effect. Indices are always validated on CPU and never validated on GPU.\r\n2021-01-26 09:30:22.977251: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2021-01-26 09:30:22.977359: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-01-26 09:30:22.995718: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\r\n2021-01-26 09:30:23.040599: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:935] Optimization results for grappler item: graph_to_optimize\r\nfunction_optimizer: Graph size after: 900 nodes (122), 1564 edges (136), time = 10.878ms.\r\nfunction_optimizer: Graph size after: 900 nodes (0), 1564 edges (0), time = 9.301ms.\r\nOptimization results for grappler item: while_body_3883\r\nfunction_optimizer: function_optimizer did nothing. time = 0.006ms.\r\nfunction_optimizer: function_optimizer did nothing. time = 0.002ms.\r\nOptimization results for grappler item: while_body_4324\r\nfunction_optimizer: function_optimizer did nothing. time = 0.004ms.\r\nfunction_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nOptimization results for grappler item: while_cond_3882\r\nfunction_optimizer: function_optimizer did nothing. time = 0.004ms.\r\nfunction_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nOptimization results for grappler item: while_cond_4323\r\nfunction_optimizer: function_optimizer did nothing. time = 0.003ms.\r\nfunction_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2021-01-26 09:30:24,125 (lite:659) INFO: Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n2021-01-26 09:30:24.216881: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:332] Ignored output_format.\r\n2021-01-26 09:30:24.216915: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:335] Ignored drop_control_dependency.\r\n2021-01-26 09:30:24.305204: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var MLIR_CRASH_REPRODUCER_DIRECTORY to enable.\r\n2021-01-26 09:30:25.025058: I tensorflow/lite/tools/optimize/quantize_weights.cc:233] Skipping quantization of tensor arg5 because it has no allocated buffer.\r\n2021-01-26 09:30:25.026738: I tensorflow/lite/tools/optimize/quantize_weights.cc:233] Skipping quantization of tensor arg5 because it has no allocated buffer.\r\n2021-01-26 09:30:25,077 (convert_model:102) INFO: Model of size 10.273849 MBs saved to model_tflite/model_quant.tflite\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nNo link because it's internal company's model\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n I get\r\n```\r\nERROR: tensorflow/lite/kernels/kernel_util.cc:404 d1 == d2 || d1 == 1 || d2 == 1 was not true.\r\nERROR: Node number 3 (ADD) failed to prepare.\r\n```\r\nwhen running\r\n\r\n\r\n**TfLiteInterpreterAllocateTensors(interpreter)**\r\n\r\nin TFLite C++ API.\r\nThe conversion to tflite model runs fine.\r\n\r\nThe node looks like that in Netron\r\n\r\n![add_node](https://user-images.githubusercontent.com/32575801/105830725-268efb00-5fc6-11eb-804f-18d16651d07d.png)\r\n\r\n\r\nI'm not sure what exactly does this error mean. As far as I can see these conditions are met, with [1, 128] and [1, 128, 128] d1 is in fact equal to d2, while d1 and d2 also being 1? Unless I misunderstood how it works. This happens during tensors allocation, so I understand that this has nothing to do with input data shapes? Thanks :)", "comments": ["Is this tflite file generated by the same TF version used for the conversion?\r\n\r\nThe util checks whether the given shapes are broadcastable. [1, 128] and [1, 128, 128] are broadcastable also. This is an interesting.", "Hmm.. since the given shapes are valid, the error won't happen. I manually invoked the CalculateShapeForBroadcast method with the given shapes and it worked fine and returned the expected value, [1, 128, 128].\r\n\r\nIt is really hard to reproduce your problem at my side. Could you make sure that the tensor inputs are given with the correct tensor shapes in the model? Even though the above shapes in the netron look fine, there is possibility to override the follow-up shapes if the input shape is changed.", "FYI, you can override input tensor shapes but the model structure may not accept them if it is an invalid one even at the original TF graph.", "Hi @Jkeezuz I made a progress on better error messages on the broadcastable shape calculation errors. You can try the tomorrow version of the tf-nightly to get what the problematic shapes are. For example,\r\n\r\n\"Given shapes, [1, 2] and [1, 3], are not broadcastable.\"", "Hi, thanks alot! I'm using tf-nightly-gpu build installed with pip for conversion, and now I've downloaded tensorflow source code from **nightly** branch and built it with bazel \r\n`bazel build -c opt --cxxopt='-g\u2019 --verbose_failures //tensorflow/lite/c:tensorflowlite_c`\r\nand used the binaries in my c++ program build for running inference, but I'm still seeing the old error with d1 and d2. ", "Hmm. Could you verify whether the downloaded source code has the change in https://github.com/tensorflow/tensorflow/commit/9ee7896d229278f582a3a381c6d22ec0559d9765 ?", "Yes, sorry, it was my fault, I was using old lib file for tensorflow, I actually get the right message now :) \r\n```\r\nAllocating tensors\r\nERROR: Given shapes, [1913, 128] and [1, 128, 128], are not broadcastable.\r\nERROR: Node number 3 (ADD) failed to prepare.\r\n```\r\nIt's surprising because the model runs fine in python API inference and during training, without converting the model to tflite", "It looks like that the converted model only supports batch size, one. Could you make sure that the concreted function signature has dynamic batch dimension instead of one. What was the input shape?", "It would be great to share a minimal, reproducible step in a gist if possible since it will make us easily reproduce the problem.", "Hi, sorry for late reply, I was away for a while. It turned out that the problem was wrong input sizes specified (during conversion they were set to [1, None], but should've been [1, None, None], now everything works well and inference runs. Thanks for your help! :) ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46686\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46686\">No</a>\n"]}, {"number": 46685, "title": "get \"Tensor(\"args_0:0\", shape=(), dtype=string)\" when using tf.data.TextLineDataset.map()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux version 3.16.0-7-amd64 (debian-kernel@lists.debian.org) (gcc version 4.9.2 (Debian 4.9.2-10+deb8u1) ) #1 SMP Debian 3.16.59-1 (2018-10-03)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): unknown 1.15.3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: no gpu used\r\n- GPU model and memory: no gpu used\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am reading data from a hdfs path using TextLineDataset. I use `for` to iter over the dataset everything goes well but when it comes to `map`, all I get is `Tensor(\"args_0:0\", shape=(), dtype=string)` (I don'y know there it comes from). The issue can be reproduced with tf ver 2.4.1.\r\n\r\nHere is the code:\r\n\r\n```python\r\npath = 'hdfs://path-to-file'\r\ndataset = tf.data.Dataset.list_files(path)\r\ndataset = tf.data.TextLineDataset(dataset)\r\nfor line in dataset:\r\n    print(line)\r\n```\r\n\r\noutput using `for`:\r\n\r\n```\r\n...\r\ntf.Tensor(b'1\\t[1, 15, 1907, 190706, 19070605, 161, \"nan\", \"nan\", \"nan\", 2, 7, 37, \"nan\", \"nan\", 1, 0, \"nan\", \"nan\", 1819, 181903, 18190301, \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", 0, 1, 1, 1, 0, 0, \"201\", \"2.486379972076975\", \"1\", \"0\", \"0.0\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]', shape=(), dtype=string)\r\ntf.Tensor(b'0\\t[1, 13, 1909, 190911, 19091101, 195, \"nan\", \"nan\", \"nan\", 2, \"nan\", \"nan\", \"nan\", \"nan\", 1, 0, \"nan\", \"nan\", 1909, 190901, 19090101, \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]', shape=(), dtype=string)\r\n...\r\n```\r\n\r\n---\r\n\r\n```python\r\ndef parse_line(line):\r\n    print(line)\r\n    return line\r\n\r\npath = 'hdfs://path-to-file'\r\ndataset = tf.data.Dataset.list_files(path)\r\ndataset = tf.data.TextLineDataset(dataset).map(lambda x: parse_line(x))\r\n```\r\n\r\noutput using `map`: (this is all I get)\r\n\r\n```\r\nTensor(\"args_0:0\", shape=(), dtype=string)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nmap func should get element in the dataset rather than `Tensor(\"args_0:0\", shape=(), dtype=string)`\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nprovided above\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nno log\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46685\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46685\">No</a>\n", "@tabVersion What's the solution? Facing the same issue! Did you get past this?", "@ameyparanjape Hi, I first init a dataset and iter over it. Using `for` should reach the data and I process data every batch instead of using `map`.", "> @ameyparanjape Hi, I first init a dataset and iter over it. Using `for` should reach the data and I process data every batch instead of using `map`.\r\n\r\nThis is sadly the only way to achieve it \r\n", "> > @ameyparanjape Hi, I first init a dataset and iter over it. Using `for` should reach the data and I process data every batch instead of using `map`.\n> \n> \n> \n> This is sadly the only way to achieve it \n> \n> \n\nI've read related source code and I couldn't figure out why. ", "The same question", "Reproducible in version 2.5.0 as well", "Workaround https://stackoverflow.com/questions/58969880/how-to-use-map-with-tuples-in-a-tensorflow-2-dataset did not work.\r\n\r\nThe second workaround with its own generator works now without any map() call.", "I'm also facing the same issue, code is from Tensorflow official guideline [(pix2pix - loading data).](https://www.tensorflow.org/tutorials/generative/pix2pix#build_an_input_pipeline_with_tfdata) \r\nI'm using tf-gpu 2.6 and below code will pop up an error in map function due to it will get a Tensor 'Tensor(\"args_0:0\", shape=(), dtype=string)' instead of a Tensor contains a file path.\r\n\r\n`train_dataset = tf.data.Dataset.list_files(str(path / 'train/*.jpg'))`\r\n` train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)`\r\n\r\n", "Same here!"]}, {"number": 46684, "title": "Time issue in micro_speec", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\nWhen the _micro_speech_ loop is executed for the first time, `current_time` is 0. In this case, the Feature Provider requests `kFeatureSliceCount` audio data packages of `kFeatureSliceDurationMs` length. However, since `previous_time` is set equal to  `current_time` after calling `feature_provider->PopulateFeatureData()`. So running the next time, current time has increased by the number of ms that the audio provider has been recording but also have 990 ms of data already been used. The Feature Provider just sees the difference in the recorded time since the last call and asks for more data that should be available. But since audio provider has no data, it waits till the data has been recorded which takes exactly the time that is missing which are 990 ms.\r\n\r\nAm I the only one with this issue? Is it my mistake or common?\r\n\r\nThere are multiple ways to fix this: \r\n1) start the time of the audio provider at -990 ms\r\n2) don't let the feature provider request data if there is none recorded (needs audio provider init to be called separately I guess)\r\n3) don't set `previous_time` to `current_time` but to `how_many_new_slices * kFeatureSliceStrideMs`\r\n\r\n\r\n", "comments": ["@nixmeer Is this still an issue for you? Can you please test with recent `TF2.7` and `tf-nightly` and let us know whether the issue persists with recent TF version. Also, share a complete standalone code to reproduce the issue. Thanks!", "@nixmeer @jvishnuvardhan BTW, the examples repo for Espressif solutions is now maintained at: https://github.com/espressif/tflite-micro-esp-examples \r\n\r\nIf issue still persists for you, I would request you to try out the above repo.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46684\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46684\">No</a>\n"]}, {"number": 46683, "title": "micro_speech: ESP32 Audio Codec Chip not initialized", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): not sure\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32 LyraT v4.3\r\n\r\nThe example _micro_speech_ does configure an I2S bus to receive audio data. This seems to me as if there is an audio codec chip required as provided on the LyraT eval board. However, the audio codec chip is not being initialized so it does not know that it is supposed to send audio data with 16k sampling rate, 16 bit resolution, one channel and act as a replica.\r\n\r\n[Here](https://github.com/tensorflow/tensorflow/tree/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/lite/micro/examples/micro_speech#deploy-to-esp32) it says that two boards have been tested. The DevKit has no microphone and the ESP-EYE has a \"digital mic\". May the EYE'S mic just sends data with this spec by default; I could not find any information about it. So the code might deploy to both and run on both but I doubt it works.\r\n\r\nFor me, it fixed it to add the initialization of the ES8388. While doing so, I removed the \"manual\" I2S-handling and replaced it with an audio pipeline.\r\n\r\nMaybe it would not harm the example to run on the ESP-EYE, if the audio codec chip was being initialized by default or the board information from menu_config of the IDF would help.\r\n", "comments": ["@nixmeer This is a micro related issue,Please post this in [micro repository](https://github.com/tensorflow/tflite-micro/issues)  ..Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46683\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46683\">No</a>\n"]}, {"number": 46682, "title": "how can i use Sequential to achieve the process of Forward propagation", "body": "I really want know how to use Sequential to achieve the process of Forward propagation,like sess,run\r\n", "comments": ["i find it \uff0cto use np.expand_dims then predict"]}, {"number": 46681, "title": "Port int8 and float versions of batch_to_space to TFLM", "body": "Commit 1 copies the TFLite operator into TFLM\r\nCommit 2 implements basic float, int8 and error checking tests along with float and int8 implementations of batch_to_space_nd\r\n\r\nThis version requires that the flat size of input matches output, since TFLM does not support tensor resizing.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/45693", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@njeffrie Can you please resolve conflicts? Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46681) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46681) for more info**.\n\n<!-- need_author_consent -->", "an error with the merge caused a number of unnecessary files to show up in the PR, that has since been resolved and I have manually set the label to `cla: yes`", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F46681) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 46680, "title": "When ryzen build tf_to_gpu_binary.exe failed: error executing command error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS : Windows 10 20H2\r\n- TensorFlow version: 2.4.1\r\n- Python version: python=3.9\r\n- Building anaconda env\r\n- Bazel version : using bazelisk\r\n- CUDA/cuDNN version: cuda 11.1, cuDNN 8.0.5\r\n- GPU model and memory: RTX2080, 8G\r\n\r\n\r\n\r\nWhen i build tensorflow in 10900k build succeed. But error is occur when i try to build ryzen 5600x or ryzen3700x.\r\n\r\n\r\n> INFO: Found 1 target...\r\nINFO: Deleting stale sandbox base C:/users/user/_bazel_user/d3ty3xtx/sandbox\r\nERROR: C:/users/user/documents/cpplibrarys/tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:149:1: compile tensorflow/core/kernels/mlir_generated/abs_i64_kernel_cubin.sm_75.bin failed (Exit 1): tf_to_gpu_binary.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/d3ty3xtx/execroot/org_tensorflow\r\nbazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.exe --same_shape=0,1 --unroll_factors=4 --tile_sizes=256 --arch=sm_75 --input=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/abs_i64.mlir --output=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/abs_i64_kernel_cubin.sm_75.bin\r\nExecution platform: @local_execution_config_platform//:platform\r\n2021-01-26 17:38:55.379641: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nwarning: Linking two modules of different data layouts: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/nvvm/libdevice/libdevice.10.bc' is 'e-i64:64-v16:16-v32:32-n16:32:64' whereas 'acme' is 'e-i64:64-i128:128-v16:16-v32:32-n16:32:64'\r\n2021-01-26 17:38:55.768832: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2021-01-26 17:38:55.845907: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-01-26 17:38:55.847744: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:97] Internal: Lowering to LLVM IR failed.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/users/user/documents/cpplibrarys/tensorflow/tensorflow/python/data/experimental/service/BUILD:11:1 compile tensorflow/core/kernels/mlir_generated/tanh_f32_kernel_cubin.sm_75.bin failed (Exit 1): tf_to_gpu_binary.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/d3ty3xtx/execroot/org_tensorflow\r\nbazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.exe --same_shape=0,1 --unroll_factors=4 --tile_sizes=256 --arch=sm_75 --input=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/tanh_f32.mlir --output=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/tanh_f32_kernel_cubin.sm_75.bin\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 69.840s, Critical Path: 24.63s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully", "comments": ["@woojh3690,\r\nTensorFlow 2.4 is compatible with only upto Python v3.8. Could you please building TensorFlow with Python v3.8 and check if you are facing the same error?\r\n\r\nFor more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu). \r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.4.0 | 3.6-3.8 | MSVC 2019 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow_gpu-2.3.0 | 3.5-3.8 | MSVC 2019 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow_gpu-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0 | 7.6 | 10.1\r\n\r\nThanks!", "Ok, i'll try python=3.8, cuDnn 8.0, cuda 11.0", "> INFO: Found 1 target...\r\nERROR: C:/users/user/documents/cpplibrarys/tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:149:1: compile tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.sm_80.bin failed (Exit 1): tf_to_gpu_binary.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/d3ty3xtx/execroot/org_tensorflow\r\nbazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.exe --same_shape=0,1 --unroll_factors=4 --tile_sizes=256 --arch=sm_80 --input=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/abs_f16.mlir --output=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.sm_80.bin\r\nExecution platform: @local_execution_config_platform//:platform\r\n2021-01-28 18:26:08.630923: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nwarning: Linking two modules of different data layouts: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/nvvm/libdevice/libdevice.10.bc' is '' whereas 'acme' is 'e-i64:64-i128:128-v16:16-v32:32-n16:32:64'\r\n2021-01-28 18:26:08.753770: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2021-01-28 18:26:08.979474: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-01-28 18:26:08.982849: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:97] Internal: Lowering to LLVM IR failed.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2002.509s, Critical Path: 284.78s\r\nINFO: 4364 processes: 4364 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nYep, same error", "@woojh3690,\r\nPlease run the `bazel clean --expunge` command before building and let us know if you are facing the same issue. Thanks!", "Steel same error.\r\n\r\n>DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  C:/users/user/_bazel_user/d3ty3xtx/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  C:/users/user/_bazel_user/d3ty3xtx/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (410 packages loaded, 26423 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/user/documents/cpplibrarys/tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:149:1: compile tensorflow/core/kernels/mlir_generated/abs_i32_kernel_cubin.sm_80.bin failed (Exit 1): tf_to_gpu_binary.exe failed: error executing command\r\n  cd C:/users/user/_bazel_user/d3ty3xtx/execroot/org_tensorflow\r\nbazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.exe --same_shape=0,1 --unroll_factors=4 --tile_sizes=256 --arch=sm_80 --input=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/abs_i32.mlir --output=bazel-out/x64_windows-opt/bin/tensorflow/core/kernels/mlir_generated/abs_i32_kernel_cubin.sm_80.bin\r\nExecution platform: @local_execution_config_platform//:platform\r\n2021-02-01 11:12:03.899522: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nwarning: Linking two modules of different data layouts: 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0/nvvm/libdevice/libdevice.10.bc' is '' whereas 'acme' is 'e-i64:64-i128:128-v16:16-v32:32-n16:32:64'\r\n2021-02-01 11:12:04.042150: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2021-02-01 11:12:04.119817: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 4294967295\r\n2021-02-01 11:12:04.122477: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:97] Internal: Lowering to LLVM IR failed.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3896.167s, Critical Path: 287.98s\r\nINFO: 9422 processes: 9422 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nI don't know why tf_to_gpu_binary.exe failed.\r\nShould i provide more information about build environment?", "Error is\r\n\r\n```\r\n2021-01-26 17:38:55.847744: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:97] Internal: Lowering to LLVM IR failed.\r\n```\r\n\r\nAdding MLIR people", "This is failure in TF KernelGen component (@sherhut FYI), could you set MLIR_CRASH_REPRODUCER_DIRECTORY and attach the produced logs? I recall seeing an error like this but also saw it fixed. Would it also be possible to verify at head?", "Interesting. `tf_to_gpu_binary` was removed 2 month ago. I also think that it was before the MLIR-GPU-KERNEL build was fixed by @sherhut on Windows at all.", "This should be disabled by default. The tool that fails should only be triggered when building with `--define tensorflow_enable_mlir_generated_gpu_kernels=1`, which is true for CUDA but should be false for ROCm.\r\n\r\nCan you share your exact bazel invocation and the contents of your bazelrc file?", "bazel call is\r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\woojh\\source\\repos\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/woojh/Anaconda3/envs/tf/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\woojh\\source\\repos\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --_**define=tensorflow_enable_mlir_generated_gpu_kernels=0**_ --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\n\r\n\r\n.bazelrc file in line 191\r\n\r\n//#Enable the mlir generated GPU kernels only for cuda builds.\r\nbuild --define=tensorflow_enable_mlir_generated_gpu_kernels=0\r\n//# This is a more specific option, so it takes precedence over the line above for cuda builds.\r\nbuild:using_cuda --define=tensorflow_enable_mlir_generated_gpu_kernels=1", "This looks like the invocation for the cuda build. If I understand the original report correctly, the cuda build is working but it is failing for the rocm version. Is this correct?\r\n\r\nCan you share the rocm build invocation, as well?\r\n\r\nAlso, do you have the `TF_NEED_CUDA` environment variable set? \r\n\r\nFinally, could you try and pass the `--define=tensorflow_enable_mlir_generated_gpu_kernels=0` directly in the bazel invocation?", "I'm getting what looks to me as the same error, however, I'm building on a Ubuntu instance running on an older AMD APU, with an nVidia GPU (trying to build cuda, not rocm).\r\n\r\n`ERROR: /home/timigoe/MachineLearning/tensorflow/tensorflow/core/kernels/mlir_generated/BUILD:149:1: compile tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.sm_30.bin failed (Exit 1): tf_to_gpu_binary failed: error executing command\r\n  (cd /home/timigoe/.cache/bazel/_bazel_timigoe/4b1b04ca1aff23a878ffc3a3edf46634/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary '--same_shape=0,1' '--unroll_factors=4' '--tile_sizes=256' '--arch=sm_30' '--input=bazel-out/host/bin/tensorflow/core/kernels/mlir_generated/abs_f16.mlir' '--output=bazel-out/host/bin/tensorflow/core/kernels/mlir_generated/abs_f16_kernel_cubin.sm_30.bin')\r\nExecution platform: @local_execution_config_platform//:platform\r\n2021-02-11 16:04:53.175192: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nwarning: Linking two modules of different data layouts: '/usr/local/cuda-11.2/nvvm/libdevice/libdevice.10.bc' is 'e-i64:64-v16:16-v32:32-n16:32:64' whereas 'acme' is 'e-i64:64-i128:128-v16:16-v32:32-n16:32:64'\r\n\r\n2021-02-11 16:04:53.360741: W tensorflow/stream_executor/gpu/asm_compiler.cc:235] Your CUDA software stack is old. We fallback to the NVIDIA driver for some compilation. Update your CUDA version to get the best performance. The ptxas error was: ptxas fatal   : Value 'sm_30' is not defined for option 'gpu-name'\r\n\r\n2021-02-11 16:04:53.363705: E tensorflow/compiler/mlir/tools/kernel_gen/tf_to_gpu_binary.cc:97] Internal: Lowering to LLVM IR failed.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n`", "That error message is misleading. In fact your CUDA installation is too new for the compute version you are requesting. I believe `sm_30` was dropped in CUDA 11. The fallback to the driver is not currently implemented for build time compilation.\r\n\r\nIf your card really requires `sm_30` I would suggest downgrading to an older CUDA version that supports it. ", "That wasn't clear from nVidia's end. Just assumed it was like GPU drivers where they just seem to keep going - will look and report back if its still a problem, sorry for any interruptions! :D", "Oh @sherhut you right!\r\nBuild completed successfully when I pass the `--define=tensorflow_enable_mlir_generated_gpu_kernels=0` directly in the bazel invocation!\r\nlike\r\n`bazel build --config=opt --config=cuda --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nOh thank you! \ud83d\ude0d\ud83d\ude0d", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46680\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46680\">No</a>\n"]}, {"number": 46678, "title": "genop: backport fix for template error", "body": "Rewrite a conditional in a template to be syntactically valid.\r\n\r\nPiperOrigin-RevId: 284055201\r\nChange-Id: Ie0129e7553857076e60e4d5d85e912a7893168d9", "comments": ["We no longer patch 1.x TensorFlow."]}, {"number": 46677, "title": "Update lite/micro/tools/make/Makefile for exp.cc and exp_test.cc", "body": "PR6 for issue #45415.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46676, "title": "tf.raw_ops.PopulationCount for uint32 not supported but documented", "body": "\r\n**System information**\r\n\r\n- OS Platform and Distribution:  (Intel Linux Ubuntu 20.04):\r\n- TensorFlow installed from (source or binary): python pip\r\n- TensorFlow version: '2.4.1'\r\n- Python version: Python 3.8.6\r\n- CPU\r\n\r\n**Describe the current behavior**\r\ntf.raw_ops.PopulationCount of array with type uint32 fails.\r\n\r\n**Describe the expected behavior**\r\nIn api documentation for raw_ops.PopulationCount for Arg x: \r\nA Tensor. Must be one of the following types: int8, int16, int32, int64, uint8, uint16, uint32, uint64. \r\n\r\nSo this is either a documentation error or more likely a bug, because feature is important on uint32.\r\n\r\n**Standalone code to reproduce the issue**\r\na = numpy.array([3], dtype = numpy,uint32)\r\ntf.raw_ops.PopulationCount(x=a)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/util/tf_export.py\", line 404, in wrapper\r\n    return f(**kwargs)\r\n  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/ops/gen_bitwise_ops.py\", line 547, in population_count\r\n    return population_count_eager_fallback(\r\n  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/ops/gen_bitwise_ops.py\", line 570, in population_count_eager_fallback\r\n    _result = _execute.execute(b\"PopulationCount\", 1, inputs=_inputs_flat,\r\n  File \"/home/rst/PYTHON/ng3py/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node PopulationCount}} = PopulationCount[T=DT_UINT32]\r\nAll kernels registered for op PopulationCount:\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_UINT16]\r\n  device='GPU'; T in [DT_INT8]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_UINT16]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_UINT8]\r\n [Op:PopulationCount]\r\n", "comments": ["Added a PR #36716 for the support.", "I have tried in colab with TF version 2.4, Nightly version and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e8b0133170d8664f10faae4097a00c76/untitled639.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46676\">No</a>\n"]}, {"number": 46675, "title": "[TFTRT - Dynamic Shape Phase 3] Add Dynamic Shape Testing for ConvertAddN and DebugString templated for nested numerical vectors", "body": "@bixia1 @tfeher for review\r\n\r\nFeature Tracker: #45481", "comments": ["@bixia1 I think I covered all your reviews. Can you please check ;)", "@bixia1 I updated the PR. Can you please elaborate your comment : https://github.com/tensorflow/tensorflow/pull/46675#discussion_r565807287\r\n\r\nI didn't get what you mean here\r\n", "We are all good this time @bixia1 ;)", "@bixia1 could you let me know what is the issue with this PR ? That way we can move forward ?", "It currently stuck at the merge process, likely due to some irrelevant test failure. "]}, {"number": 46674, "title": "[Tensorflow Lite] Build static framework for iOS", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5 (nightly)\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.7\r\n- GCC/Compiler version (if compiling from source): Xcode 12.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nI am trying to cross compile Tensorflow Lite for iOS. I would like to build a **static** framework. Here is the build command I used:\r\n\r\n`bazel build --config=ios_arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteC_framework`\r\n\r\nThe `TensorFlowLiteC_framework` target is defined in `tensorflow/tensorflow/lite/ios/BUILD.apple` as \r\n\r\n```\r\ntflite_ios_static_framework(\r\n    name = \"TensorFlowLiteC_framework\",\r\n    hdrs = [\r\n        \":c_api.h\",\r\n        \":common.h\",\r\n        \":xnnpack_delegate.h\",\r\n        \"//tensorflow/lite/c:c_api_types.h\",\r\n    ],\r\n    allowlist_symbols_file = \":allowlist_TensorFlowLiteC.txt\",\r\n    bundle_name = \"TensorFlowLiteC\",\r\n    minimum_os_version = TFL_MINIMUM_OS_VERSION,\r\n    deps = [\r\n        \":tensorflow_lite_c\",\r\n    ],\r\n)\r\n```\r\n\r\nI had expected the resulting framework (please see the file attached below) to be a static framework, but it appears to be a dynamic framework instead. Inside the `TensorFlowLiteC.framework` folder, there is a binary file `TensorFlowLiteC`. If I do `file TensorFlowLiteC`, I get:\r\n\r\n```\r\nTensorFlowLiteC: Mach-O universal binary with 1 architecture: [arm64:Mach-O 64-bit object arm64]\r\nTensorFlowLiteC (for architecture arm64):\tMach-O 64-bit object arm64\r\n```\r\nThis appears to be a dynamic lib file to me. As far as I know, if this was a static archive, I should have gotten: `current ar archive`.\r\n\r\n[TensorFlowLiteC.framework.zip](https://github.com/tensorflow/tensorflow/files/5870061/TensorFlowLiteC.framework.zip)\r\n\r\nIs there a way to actually build Tensorflow Lite into an actual static framework for iOS?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel build --config=ios_arm64 -c opt //tensorflow/lite/ios:TensorFlowLiteC_framework`\r\n\r\n", "comments": ["Interesting. We've started to hide unnecessary symbols starting `v2.3.0`, in order to avoid potential symbol collision issues with other frameworks. Looking at it more carefully now, it seems to imply that the resulting framework would have an exported symbol table, and become a dynamic framework.\r\n\r\nCan you try building the same target with changing `tflite_ios_static_framework` to just `ios_static_framework` and removing the `allowlist_symbols_file` parameter? Does that produce you an actual static framework?", "@yyoon I followed your advice and it worked. I was able to generate a static framework. It feels like it was always the intention for `TensorFlowLiteC_framework` to be static, but it is now building dynamic by mistake. Should I make a merge request based on your suggestions? ", "No, that's probably not the right thing to do for now.\r\nWe started doing the symbol hiding work because there were real users affected by name collisions between TFLite framework and other dependencies. Simply reverting this back to static framework without symbol hiding would reintroduce the issue.\r\n\r\nMaybe we should instead add an additional build target for the static framework, which could be built by people who need one.", "@yyoon I understand your position. I think having an additional static framework build target would be a very good thing to have. I am trying to build a library of my own (I use TFLite inside my library) and I would like to avoid introducing external dependencies. This is why linking against a static TFLite framework is attractive to me and people with similar use case. \r\n\r\nAs for the existing framework build target, I would suggest stop using `tflite_ios_static_framework` build rule. Instead, use a different rule to make it clear that the resulting framework is dynamic.\r\n\r\nI will close the issue for now. Thank you for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46674\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46674\">No</a>\n", "Thanks for the suggestions!"]}, {"number": 46673, "title": "Tensorflow does not work with RTX 3070 on Windows", "body": "**System information**\r\n- Code attached below\r\n- OS: Windows 10\r\n- TensorFlow installed from binary (`pip3 install tensorflow`)\r\n- TensorFlow version: tried latest stable v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\n- CUDA/cuDNN version: cuda_11.2.0_460.89_win10\\cudnn-11.1-v8.0.5.39\r\n- GPU drivers: 460.89\r\n- GPU model and memory: seems to be recognized correctly by TF- GeForce RTX 3070 computeCapability: 8.6 coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nGetting error:\r\n```\r\n2021-01-25 21:36:01.042433: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/500\r\n2021-01-25 21:36:03.304809: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-25 21:36:03.880223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-25 21:36:03.911531: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-25 21:36:04.515409: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2021-01-25 21:36:04.515498: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2021-01-25 21:36:04.515607: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1514 : Unknown: Fail to find the dnn implementation.\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Workspace_GpwScan/dnn/sandbox/reproduce_issue.py\", line 110, in <module>\r\n    callbacks=[checkpoint, tensorboard])\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 2943, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:    Fail to find the dnn implementation.\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[sequential/lstm/PartitionedCall]] [Op:__inference_train_function_8782]\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\n[tf_2.4.1_issue_on_3070.txt](https://github.com/tensorflow/tensorflow/files/5869537/tf_2.4.1_issue_on_3070.txt)\r\n\r\n\r\nTried also with latest ```nightly 2.5.0.dev20210125``` ending up with error:\r\n```\r\n2021-01-25 21:31:05.429799: E tensorflow/stream_executor/dnn.cc:618] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1975): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2021-01-25 21:31:05.430291: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1926 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1, 128, 1, 128, 256, 128] \r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"C:/Workspace_GpwScan/dnn/sandbox/reproduce_issue.py\", line 108, in <module>\r\n    callbacks=[checkpoint, tensorboard])\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\keras\\engine\\training.py\", line 1134, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 818, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\def_function.py\", line 846, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 2994, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 1939, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\function.py\", line 569, in call\r\n    ctx=ctx)\r\n  File \"C:\\Workspace_GpwScan\\stubs\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InternalError:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1, 128, 1, 128, 256, 128] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[Adam/gradients/PartitionedCall_2]] [Op:__inference_train_function_8936]\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\n[tf_nightly_issue_on_3070.txt](https://github.com/tensorflow/tensorflow/files/5869438/tf_nightly_issue_on_3070.txt)\r\n\r\n\r\n**Describe the expected behavior**\r\nThe script was working on my old gtx 980 and CUDA 10.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport datetime\r\nimport os\r\n\r\nimport pandas as pd\r\nfrom numpy import reshape\r\n\r\nimport tensorflow as tf\r\n\r\nEPOCHS = 500\r\nBATCH_SIZE = 256\r\nTEST_SET_RATIO = 0.2\r\n\r\nLEARNING_RATE = 0.001\r\nDECAY = 3e-5\r\nLOSS_FUNC = 'categorical_crossentropy'\r\nDROPOUT = 0.2\r\nOUTPUT_PATH = \"e:\\\\ml\"\r\n\r\nRNN_SEQ_LEN = 128  # number of RNN/LSTM sequence features\r\nL_AMOUNT = 2  # number of labels\r\n\r\nMIN_ACC_TO_SAVE_MODEL = 0.6\r\n\r\n\r\ndef create_model():\r\n    new_model = tf.keras.models.Sequential()\r\n\r\n    # NETWORK INPUT\r\n    new_model.add(tf.keras.layers.LSTM(RNN_SEQ_LEN, input_shape=TR_FEATURES.shape[1:], return_sequences=True))\r\n    new_model.add(tf.keras.layers.Dropout(DROPOUT))\r\n    new_model.add(tf.keras.layers.BatchNormalization())\r\n\r\n    new_model.add(tf.keras.layers.LSTM(RNN_SEQ_LEN, return_sequences=True))\r\n    new_model.add(tf.keras.layers.Dropout(DROPOUT / 2))\r\n    new_model.add(tf.keras.layers.BatchNormalization())\r\n\r\n    new_model.add(tf.keras.layers.LSTM(RNN_SEQ_LEN))\r\n    new_model.add(tf.keras.layers.Dropout(DROPOUT))\r\n    new_model.add(tf.keras.layers.BatchNormalization())\r\n\r\n    # NETWORK OUTPUT\r\n    new_model.add(tf.keras.layers.Dense(L_AMOUNT, activation=tf.keras.activations.softmax))\r\n\r\n    opt = tf.keras.optimizers.Adam(LEARNING_RATE, decay=DECAY)\r\n    new_model.compile(optimizer=opt,\r\n                      loss=LOSS_FUNC,\r\n                      metrics=['accuracy'])\r\n\r\n    print(new_model.summary())\r\n    return new_model\r\n\r\n\r\nclass CustomModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\r\n    def __init__(self, fp, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch', **kwargs):\r\n        super().__init__(fp, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, **kwargs)\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        print(\"\\n-------------------------------------------------------------------------------------------------------\")\r\n        print(f\"epoch: {epoch}, training_acc: {round(float(logs['accuracy']), 4)}, validation_acc: {round(float(logs['val_accuracy']), 4)}\")\r\n        print(\"-------------------------------------------------------------------------------------------------------\\n\")\r\n\r\n        if MIN_ACC_TO_SAVE_MODEL <= logs['accuracy']:\r\n            super().on_epoch_end(epoch, logs)\r\n\r\n\r\nif __name__ == '__main__':\r\n    data_filename = 'input_data.csv'\r\n    print(\"Loading data file: %s\" % data_filename)\r\n    dataset = pd.read_csv(data_filename, delimiter=',', header=None)\r\n    dataset = dataset.drop(columns=[0, 1, 2, 3, 4, 5, 6]).values  # drop columns with additional information\r\n\r\n    test_set_size = int(len(dataset) * TEST_SET_RATIO)\r\n    print(\"Test set split at: %d\" % test_set_size)\r\n\r\n    train_data = dataset[:-test_set_size]\r\n    test_data = dataset[-test_set_size:]  # use most recent data for validation (extract before shuffle)\r\n\r\n    TR_F = train_data[:, 0:RNN_SEQ_LEN]\r\n    TS_F = test_data[:, 0:RNN_SEQ_LEN]\r\n\r\n    TR_L = train_data[:, RNN_SEQ_LEN:RNN_SEQ_LEN + L_AMOUNT]\r\n    TS_L = test_data[:, RNN_SEQ_LEN:RNN_SEQ_LEN + L_AMOUNT]\r\n\r\n    TR_FEATURES = reshape(TR_F, (len(TR_F), RNN_SEQ_LEN, 1))\r\n    TS_FEATURES = reshape(TS_F, (len(TS_F), RNN_SEQ_LEN, 1))\r\n\r\n    model = create_model()\r\n\r\n    TRAINING_TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n    model_name = \"sscce_%s\" % TRAINING_TIMESTAMP\r\n    os.mkdir(\"%s\\\\models\\\\%s\" % (OUTPUT_PATH, model_name))\r\n    filepath = \"%s\\\\models\\\\%s\\\\%s--{epoch:02d}-{val_accuracy:.3f}.model\" % (OUTPUT_PATH, model_name, model_name)\r\n    checkpoint = CustomModelCheckpoint(filepath,\r\n                                       monitor='val_accuracy',\r\n                                       verbose=1,\r\n                                       save_best_only=True,\r\n                                       mode='max')\r\n\r\n    log_dir = \"%s\\\\logs\\\\fit\\\\%s.model\" % (OUTPUT_PATH, model_name)\r\n    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\r\n\r\n    model.fit(x=TR_FEATURES,\r\n              y=TR_L,\r\n              epochs=EPOCHS,\r\n              batch_size=BATCH_SIZE,\r\n              shuffle=True,\r\n              validation_data=(TS_FEATURES, TS_L),\r\n              callbacks=[checkpoint, tensorboard])\r\n```\r\n\r\nDATA FILE SAMPLE: [input_data.zip](https://github.com/tensorflow/tensorflow/files/5869582/input_data.zip)\r\n \r\n**Other info / logs**\r\n\r\nProviding also path to CUDA 11.0 installation because without it getting errors like:\r\n\r\n```\r\n2021-01-25 21:44:15.989317: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n```\r\n\r\nFull win sys PATH:\r\n\r\n```\r\nPath=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin;C:\\cudnn-11.1-v8.0.5.39\\bin;C:\\Python36\\Scripts\\;C:\\Python36;C:\\ProgramData\\DockerDesktop\\version-bin;C:\\Program Files\\Docker\\Docker\\Resources\\bin;c:\\Java\\jdk1.8.0_144_x86;C:\\gradle-6.0.1\\bin;C:\\SVN\\bin;C:\\MinGW\\bin;C:\\WinAVR-20100110\\;c:\\avrdude\\;c:\\Android\\sdk\\platform-tools;C:\\adb\\;C:\\TortoiseGit\\bin;C:\\Git4Windows\\cmd;c:\\sqlite-tools-win32-x86-3130000\\;C:\\WINDOWS\\System32;C:\\WINDOWS;C:\\WINDOWS\\System32\\wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Bitvise SSH Client;C:\\Program Files (x86)\\Windows Live\\Shared;C:\\WINDOWS\\system32;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2020.3.0\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR\r\n```\r\n\r\n\r\n\r\nI was trying different combinations of cuda/cudnn/tensorflow just for the sake of it but actually only `cuda_11.2.0_460.89_win10` comes with win nvidia GPU divers version high enough to support RTX 30xx series.\r\nStill - there is no `cudnn` build designated particularly for `CUDA 11.2` yet... Maybe this is an issue...\r\n\r\nAny idea how to make it working all together?", "comments": ["@alewir \r\nPlease share all dependencies for us to replicate the issue reported, i ran the code and face this [error](https://colab.research.google.com/gist/Saduf2019/ba762477986241cbd6840379cee58382/untitled509.ipynb) as train fie is missing.\r\n\r\nThis issue looks similar to existing issues #45285, #45170", "> i ran the code and face this error as train fie is missing.\r\n\r\nSry for that, my bad. I've updated code to match filename from `DATA FILE SAMPLE: input_data.zip` provided in description.\r\n\r\nHowever I'll also try to share full data file in the evening (unfortunately it's too big for github) as from what I've observed data volume has a significant impact here too. Run on full data file fails in 1st epoch while on the above sample files even first couple of hundreds epochs might pass before the error occurs.\r\n\r\n> Please share all dependencies for us to replicate the issue reported\r\n\r\nIs it going to be enough to share here the output of `pip3 list`?\r\n\r\n> This issue looks similar to existing issues #45285, #45170\r\n\r\nI'll try the proposal from #45170 as indeed it seems somewhat related.", "@alewir\r\nI'll try the proposal from #45170 as indeed it seems somewhat related.\r\n\r\n>> please do try and update us.\r\n", "I've tried some variation of the trick from #45170 i.e. to use `ptxas.exe` from `CUDA 11.1` within `11.2` (which is the one I have installed), but no luck.\r\nI've also installed recently released `cuDNN v8.1.0 (January 26th, 2021), for CUDA 11.0,11.1 and 11.2 ` so now CUDA matches GPU drivers version and cuDNN version but I'm still getting the same issue on tf 2.4.1 randomly somewhere in the middle of epoch 1.\r\n\r\nHowever, when upgraded to latest nightly from ` tf_nightly-2.5.0.dev20210130-cp36-cp36m-win_amd64.whl`, it seems to work at least with the above simple model.\r\n", "@alewir \r\nCuda 11.0 has been tested n suggested, please try it with latest tf version and if you do not face any issues please move this to closed status as cuda 11.2 has not been tested or recommended for the latest tf version as well.", "I will check again tf 2.4.1 with CUDA 11.0 and appropriate cuDNN plus newest GPU drivers as well... I'll post the results here within next few days.\r\n\r\n--- UPDATE\r\n\r\nRolled back installation to combination:\r\n`cudnn-11.0-windows-x64-v8.0.2.39.zip`\r\n`cuda_11.0.2_451.48_win10.exe`\r\nlatest stable `tensorflow 2.4.1`\r\nupdated nVidia GPU drivers to `461.40` as `451.48` packaged with above CUDA installer won't work with rtx 3070\r\n\r\n```\r\n2021-02-04 19:36:52.279146: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-04 19:36:52.280858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-02-04 19:36:52.315034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:0b:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-04 19:36:52.315196: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-04 19:36:52.328392: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-04 19:36:52.328479: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-04 19:36:52.332034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-04 19:36:52.333477: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-02-04 19:36:52.341266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-04 19:36:52.344100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-02-04 19:36:52.345121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-04 19:36:52.345291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-04 19:36:52.345739: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-04 19:36:52.346696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:0b:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-02-04 19:36:52.346836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-04 19:36:52.346912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-04 19:36:52.346988: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-04 19:36:52.347069: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-04 19:36:52.347143: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-02-04 19:36:52.347216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-04 19:36:52.347298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-02-04 19:36:52.347669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-04 19:36:52.347832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-04 19:36:52.808972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-04 19:36:52.809063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-02-04 19:36:52.809110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-02-04 19:36:52.809324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6589 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3070, pci bus id: 0000:0b:00.0, compute capability: 8.6)\r\n2021-02-04 19:36:52.809872: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-04 19:36:54.279592: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nEpoch 1/50\r\n2021-02-04 19:36:58.615576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-04 19:36:59.131156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-04 19:36:59.157133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-04 19:36:59.700433: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2021-02-04 19:36:59.700523: E tensorflow/stream_executor/cuda/cuda_dnn.cc:340] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows\r\n2021-02-04 19:36:59.700630: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cudnn_rnn_ops.cc:1514 : Unknown: Fail to find the dnn implementation.\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_bundle\\pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"C:\\Users\\Aleksander\\.IntelliJIdea2018.3\\config\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"G:/Workspace_Betleyem3/dnn/multi/multi_training_combined.py\", line 182, in <module>\r\n    callbacks=[checkpoint, tensorboard])\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2943, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.UnknownError:    Fail to find the dnn implementation.\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[model/lstm/PartitionedCall]] [Op:__inference_train_function_17038]\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\n\r\nSo what works for me is the `cudnn-11.2-windows-x64-v8.1.0.77.zip` released recently together with both tf `2.4.1` and `2.5-nightly` but only together with `cuda_11.2.0_460.89_win10.exe` obviously.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46673\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46673\">No</a>\n", "I am facing the same problem.\r\nRTX 3070 working very well in other algorithms but incase of CNN it crashes saying Cuda error", "any one having a working fix/ specific versions of each that works, seems right now i am not able to make it work with latest drivers. For CNN model training."]}, {"number": 46671, "title": "Fix in resource var AssignVariableOp Compute", "body": "Add check in AssignVariableOp in resource_var_ops.cc similar to XlaAssignVariableOp::Compute in xla_device_ops.cc", "comments": ["What is this fixing? I assume XLA only has this check because it uses DT_INVALID for uninitialized variables. I don't think we want to allow variables to be initialized with different dtypes than they were declared with.", "This change does not allow variables to be initialized with different dtypes than they were declared with. This only allows for the update to happen first by the AssignVariableOp, in which case it's still uninitialized. \r\nIt is possible that this scenario never occurs in Tensorflow. However, when using TF-XLA, this scenario is possible when the execution uses the 'fallback' path (which essentially invokes Tensorflow ops via partitioned_call).", "Can you add a test? I'd also suggest checking explicitly for DT_INVALID, to avoid making this check more lax for non-XLA (in which case the correct dtype should come from the original VarHandleOp).", "Explicitly checking for DT_INVALID means that an uninitialized variable can have a type other than DT_INVALID. That sounds incorrect (adding the check might lead developers to believe the check is required)\r\nRegretfully we cannot reproduce the issue against an OOTB TF. We've made a change in the XLA compilation strategy such that the fallback path is always executed before the compiled path. This is what causes the AssignVariableOp to be executed before the XlaAssignVariableOp.", "Do you know which part of TF-XLA is creating the resource (with a `Var` and the wrong dtype) but not initializing it?", "> Do you know which part of TF-XLA is creating the resource (with a `Var` and the wrong dtype) but not initializing it?\r\n\r\nThere is no wrong type, unless you deem DT_INVALID a wrong type.", "So it is DT_INVALID? And do you know where that's set?\r\n\r\nThe code as-is does include uninitialized variables with real dtypes set (on uninitialized `variable->tensor()`s). If you have two `AssignVariableOp`s racing to initialize the same handle, one will create the resource and one will grab `variable->mu()` first, but it's not necessarily the same one, and if it isn't then the other will see the resource creator's \"real dtype but not initialized\" Var. I guess you could argue that this is bad and DT_INVALID is nicer, but they just seem like slightly different conventions to me, and setting the dtype carries a bit more information. So I don't understand the pushback on limiting this to ((DT_INVALID && uninitialized) || same_dtype) with a comment that this is XLA's convention.\r\n\r\nIf we don't want to accept that both conventions are OK, it seems like we should change one of them. So either set the variable's real dtype in XLA or always use DT_INVALID when creating the resource. Those sound OK but are probably larger changes.", "\"So it is DT_INVALID? And do you know where that's set?\"\r\nit's set in GetVariableInfosFromInputs:\r\n          // This var is uninitialized for now.\r\n          *ptr = new Var(DT_INVALID);\r\n\r\n\"The code as-is does include uninitialized variables with real dtypes set\"\r\nYes it does, but so does the code in XlaAssignVariableOp::Compute, which doesn't check for uninitialized variables with a type either. We choose for symmetry with existing code here, and rely on uninitialized variables to have DT_INVALID type.\r\n", "Let's only loosen the check here to the extent we need. It's OK if it's slightly different than XlaAssignVariableOp::Compute.", "@allenlavoie Incorporated your comment. Please take a look."]}, {"number": 46670, "title": "Extract reference for operator BATCH_MATMUL to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nPR step 2 for issue #46504", "comments": ["No micro files are modified during PR2.  This is part of our standard PR process.  All CI build tests pass.\r\n", "Thanks for your patience David, I must have got confused about that! Approving."]}, {"number": 46669, "title": "Update vexriscv + zephyr makefiles.", "body": "Tested that the following commands work:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv OPTIMIZED_KERNEL_DIR=vexriscv third_party_downloads\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=zephyr_vexriscv OPTIMIZED_KERNEL_DIR=vexriscv microlite\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}]