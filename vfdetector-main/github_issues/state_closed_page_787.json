[{"number": 29936, "title": "Unresolved reference from tensorflow.python", "body": "**System information**\r\n- Have I written custom code:No\r\n- OS Platform and Distribution:Ubuntu 18.04.2\r\n- TensorFlow installed from:binary\r\n- TensorFlow version:tf-nightly-gpu 1.14.1.dev20190617\r\n- Python version:3.7\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:rtx 2070 8gb\r\n\r\n**Describe the current behavior**\r\ncannot import anything from tensorflow.python. \r\n**Describe the expected behavior**\r\nIn tensorflow version 1.13 it works\r\n**Code to reproduce the issue**\r\nfrom tensorflow.python import tf2 and i get Unresolved reference", "comments": ["Can you please post a full code sample and a full error message you get?", "i just wrote this -> from tensorflow.python import tf2 and my IDE complains about Unresolved reference 'tf2'", "Can't reproduce:\r\n\r\n```bash\r\nmihaimaruseac@ankh:/tmp/github/2$ source bin/activate\r\n(2) mihaimaruseac@ankh:/tmp/github/2$ pip install tf-nightly-gpu\r\nCollecting tf-nightly-gpu\r\n  Downloading https://files.pythonhosted.org/packages/71/e3/f4452d768f2fdc742da9eede23ae65791ba63d52d36f7c98e90b8b8b6c4e/tf_nightly_gpu-1.14.1.dev20190619-cp37-cp37m-manylinux1_x86_64.whl (370.7MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 370.7MB 140kB/s \r\nRequirement already satisfied: wheel>=0.26 in ./lib/python3.7/site-packages (from tf-nightly-gpu) (0.33.4)\r\nCollecting termcolor>=1.1.0 (from tf-nightly-gpu)\r\nCollecting absl-py>=0.7.0 (from tf-nightly-gpu)\r\nCollecting astor>=0.6.0 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\r\nCollecting numpy<2.0,>=1.14.5 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/fc/d1/45be1144b03b6b1e24f9a924f23f66b4ad030d834ad31fb9e5581bd328af/numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl\r\nCollecting tf-estimator-nightly (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/ff/35/ca8675a841a561ce410b95d17202e7baaa38a8ae29977600c06bad1efb48/tf_estimator_nightly-1.14.0.dev2019061901-py2.py3-none-any.whl\r\nCollecting grpcio>=1.8.6 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/a2/66/a44fc7eb9bd3a9c480a70c30306f175521ca0531e2cf1ca54b74a450fdb9/grpcio-1.21.1-cp37-cp37m-manylinux1_x86_64.whl\r\nCollecting protobuf>=3.6.1 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/ce/86/9f6123c4c6f481862f286dbe13aa2e97bdedd7662f5fc3033c1a41f32f88/protobuf-3.8.0-cp37-cp37m-manylinux1_x86_64.whl\r\nCollecting wrapt>=1.11.1 (from tf-nightly-gpu)\r\nCollecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/3a/60/afa129c3621d62c885599076f8e89737d66e5dfffad1a08842b1c11b4540/tb_nightly-1.14.0a20190614-py3-none-any.whl\r\nCollecting keras-preprocessing>=1.0.5 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\r\nCollecting google-pasta>=0.1.6 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\r\nCollecting keras-applications>=1.0.8 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\r\nCollecting gast>=0.2.0 (from tf-nightly-gpu)\r\nCollecting six>=1.10.0 (from tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in ./lib/python3.7/site-packages (from protobuf>=3.6.1->tf-nightly-gpu) (41.0.1)\r\nCollecting markdown>=2.6.8 (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\r\nCollecting werkzeug>=0.11.15 (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl\r\nCollecting h5py (from keras-applications>=1.0.8->tf-nightly-gpu)\r\n  Using cached https://files.pythonhosted.org/packages/8e/fd/2ca5c4f4ed33ac4178f9c4d551e3946ab480866e3cd67a65a67a4bb35367/h5py-2.9.0-cp37-cp37m-manylinux1_x86_64.whl\r\nInstalling collected packages: termcolor, six, absl-py, astor, numpy, tf-estimator-nightly, grpcio, protobuf, wrapt, markdown, werkzeug, tb-nightly, keras-preprocessing, google-pasta, h5py, keras-applications, gast, tf-nightly-gpu\r\nSuccessfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.21.1 h5py-2.9.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 numpy-1.16.4 protobuf-3.8.0 six-1.12.0 tb-nightly-1.14.0a20190614 termcolor-1.1.0 tf-estimator-nightly-1.14.0.dev2019061901 tf-nightly-gpu-1.14.1.dev20190619 werkzeug-0.15.4 wrapt-1.11.2\r\n(2) mihaimaruseac@ankh:/tmp/github/2$ python\r\n```\r\n\r\n```python\r\nPython 3.7.3rc1 (default, Mar 13 2019, 11:01:15) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from tensorflow.python import tf2\r\n>>> import tensorflow as tf\r\n>>> tf.VERSION\r\n'1.14.1-dev20190619'\r\n```", "Also, if IDE complains and not Python interpreter, you should also specify which IDE it is. Python doesn't have `Undefined reference` errors on imports.\r\n\r\nProbably related: #25602 and follow-up in #29144", "```\r\nfrom tensorflow.python import tf2\r\n\r\nif tf2.enabled():\r\n    print(\"ok\")\r\nelse:\r\n    print(\"not enabled\")\r\n```\r\n\r\ni tried this and it works but the problem is with pycharm", "Then I guess we can close this as a duplicate of #29144. Please reopen if you think it is not a duplicate"]}, {"number": 29935, "title": "Correction in keras.layers.Reshape", "body": "Here is the GitHub [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/0609804e9d2f18453b6eae1dba1d8284/reshape_error.ipynb). I tried TF1.13.1 also and has the same output as with TF2.0-beta. Thanks", "comments": ["@pavithrasv and @gbaned Is there anything missing from my side? Thanks!"]}, {"number": 29934, "title": "Add Sergii Khomenko to contributor list", "body": "https://github.com/tensorflow/tensorflow/commit/dd551ab5acd4096db1e4037053effe578da6e89a\r\nhttps://github.com/tensorflow/tensorflow/commit/cb93088cb6a3f7522838fb74f133f5598c76897f", "comments": ["@lc0 FYI"]}, {"number": 29933, "title": "Fix links in TensorFlow Lite for Microcontrollers readme", "body": "Some of the links were broken.", "comments": []}, {"number": 29932, "title": "Continue training with pre trained model", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Windows 10 x64\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.13.1\r\n- Python version:3.6\r\n- CUDA/cuDNN version:10/7.4\r\n- GPU model and memory:Nvidia GeForce 840m / 4.00 Go\r\n\r\nI cloned a CNN facial landmarks project from Github, this repo can be used to detect facial with 68 landmarks ( 68 for x coordinates and 68 for y coordinates). I have a pre-trained model \" frozen_inference_graph.pb\"\r\nI used this project to trained my own dataset to detect iris region in the human eye, so I prepared my dataset to be used for training by CNN.\r\nMy problem is that the iris annotated with 40 landmarks ( 40 for x coordinates and 40 for y coordinates) and the pre-trained model is trained with 68 landmarks ( number of logits 136 (68+68 )).\r\n\r\n![redd](https://user-images.githubusercontent.com/19480228/59718798-3441a380-921b-11e9-91c2-a760102760cc.PNG)\r\n\r\nI tried to train my model from scratch, but I didn't get good results:\r\n![iriss](https://user-images.githubusercontent.com/19480228/59718868-5a674380-921b-11e9-8b3c-efb4766c875f.PNG)\r\n\r\nI want to continue training with this pre-trained model but I get this error:\r\n> InvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\nAssign requires shapes of both tensors to match. lhs shape= [1024,80] rhs shape= [1024,136]`\r\n\r\nSo it was very clear that my problem is in the number of points ( landmarks ) that are not compatibles ( 80 for iris landmarks and 136 for facial landmarks ) .\r\nI need to know it can be possible to modify the frozen model .pb and what about the transfer learning? and how to use it?\r\n\r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Thanks, but I really need help because I post this issue on StackOverflow and I didn't get any answer. \r\nIf you have any idea about how to solve this problem, please give me your point of view ", "@abdou31 In transfer learning you could freeze weights for all the layers except last layer, remove last layer and add a  last layer that outputs 80 instead of 136, and train using your data. It should work.\r\n\r\nThere are many landmark detentions models in GitHub. Please create a standalone code to reproduce the issue and support you accordingly. Thanks!", "Thanks.\r\nActually, I do that and I modified the last layers with units=136 instead of 80 and that causes the problem because the pre-trained model is trained with 80 points.\r\nSo I think is not possible to continue training with pre-trained with different last layer.", "@abdou31 It is possible to train pre-trained model (i.e. called as transfer learning). Please create a GitHub gist and share it. If your data is private, you could you public datasets (MNIST, CIFAR, etc). Thanks! ", "Can you give me instructions to do that? \r\nThanks.", "@abdou31 You could copy paste your code into [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb). Run the colab file and the create a gist (under dropdown  `File`). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "I trained from scratch , \r\nFor now this is not important because I have other issues ( using tflite model on Android device )"]}, {"number": 29931, "title": "Dataset.map(tf.keras.applications.vgg16.preprocess_input) -> AttributeError: 'Tensor' object has no attribute '_datatype_enum'", "body": "### System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. Small error-reproducing script provided below\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nTensorFlow installed from (source or binary): Binary (Python 3.6)\r\nTensorFlow version (use command below): 2.0, nightly installed 6/18/19\r\nBazel version (if compiling from source):\r\nCUDA/cuDNN version: CPU only\r\nGPU model and memory:\r\nExact command to reproduce: python map_bug.py, script provided below\r\n\r\n### Describe the problem\r\n\r\nJust installed nightly to make sure this hadn't been caught yet--I am trying to do some map() operations on a dataset, nothing fancy. If I build only one dataset in a script, it works fine. If I do it twice, however--for instance, make a train and hold-out set using the same operations--I get this mysterious error message. Pretty sure this cannot be intended behavior.\r\n\r\n### Source code / logs\r\n\r\nOriginally this was done in a large project. But I did a bit of work and whittled it down to the following script, which just uses MNIST to reproduce the error:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets\r\nimport numpy as np\r\nBATCH_SIZE = 128\r\n\r\n\r\ndef size_image_for_vgg(image):\r\n    return tf.image.resize(image, [224, 224])\r\n\r\n\r\nif __name__ == '__main__':\r\n    # punch up mnist\r\n    (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\r\n    train_images = train_images.reshape((60000, 28, 28, 1)).astype(np.float32) / 255.0\r\n    test_images = test_images.reshape((10000, 28, 28, 1)).astype(np.float32) / 255.0\r\n\r\n    # Now create a dataset\r\n    im_ds = tf.data.Dataset.from_tensor_slices(train_images)\r\n    label_ds = tf.data.Dataset.from_tensor_slices(train_labels)\r\n    im_ds_t = tf.data.Dataset.from_tensor_slices(test_images)\r\n    label_ds_t = tf.data.Dataset.from_tensor_slices(test_labels)\r\n\r\n    # If this block is commented out, the block below it will NOT throw any error\r\n    # do some normal Dataset operations on test and train data like we're getting ready to fit a model\r\n    im_ds = im_ds.map(size_image_for_vgg, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    im_ds = im_ds.map(tf.keras.applications.vgg16.preprocess_input, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    train_ds = tf.data.Dataset.zip((im_ds, label_ds))\r\n    train_ds = train_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=1000))\r\n    train_ds = train_ds.batch(batch_size=BATCH_SIZE)\r\n\r\n    im_ds_t = im_ds_t.map(size_image_for_vgg, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    # TODO throws error if you do it a second time?\r\n    im_ds_t = im_ds_t.map(tf.keras.applications.vgg16.preprocess_input, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    test_ds = tf.data.Dataset.zip((im_ds_t, label_ds_t))\r\n    test_ds = test_ds.batch(batch_size=BATCH_SIZE)\r\n```\r\n\r\n### Stack trace\r\n\r\nTraceback (most recent call last):\r\nFile \"map_bug.py\", line 33, in\r\nim_ds_t = im_ds_t.map(tf.keras.applications.vgg16.preprocess_input, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\nFile \"/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 1141, in map\r\nself, map_func, num_parallel_calls, preserve_cardinality=True)\r\nFile \"/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3320, in init\r\n**flat_structure(self))\r\nFile \"/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 4141, in parallel_map_dataset\r\npreserve_cardinality=preserve_cardinality, name=name, ctx=_ctx)\r\nFile \"/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 4224, in parallel_map_dataset_eager_fallback\r\n_attr_Targuments, other_arguments = _execute.convert_to_mixed_eager_tensors(other_arguments, _ctx)\r\nFile \"/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 210, in convert_to_mixed_eager_tensors\r\ntypes = [t._datatype_enum() for t in v] # pylint: disable=protected-access\r\nFile \"/home/menarcj/OtherSoftware/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 210, in\r\ntypes = [t._datatype_enum() for t in v] # pylint: disable=protected-access\r\nAttributeError: 'Tensor' object has no attribute '_datatype_enum'", "comments": ["Zoomed in a bit...the error disappears if either \r\n```\r\nim_ds = im_ds.map(tf.keras.applications.vgg16.preprocess_input, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n```\r\nis commented out. You don't have to comment out the whole block.", "So I can reproduce the error with the following even smaller script:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import datasets\r\nimport numpy as np\r\n\r\nif __name__ == '__main__':\r\n    # punch up mnist\r\n    (train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\r\n    train_images = train_images.reshape((60000, 28, 28, 1)).astype(np.float32) / 255.0\r\n    test_images = test_images.reshape((10000, 28, 28, 1)).astype(np.float32) / 255.0\r\n\r\n    # Now create a dataset\r\n    im_ds = tf.data.Dataset.from_tensor_slices(train_images)\r\n    im_ds_t = tf.data.Dataset.from_tensor_slices(test_images)\r\n\r\n    im_ds = im_ds.map(tf.keras.applications.vgg16.preprocess_input, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    # TODO throws error if you do it a second time?\r\n    im_ds_t = im_ds_t.map(tf.keras.applications.vgg16.preprocess_input, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n```", "Try this as a workaround:\r\n\r\n```\r\n# Add before any TF calls\r\n# Initialize the keras global outside of any tf.functions\r\ntemp = tf.random_uniform([4, 32, 32, 3])  # Or tf.zeros\r\ntf.keras.applications.vgg16.preprocess_input(temp)\r\n```", "tf.random_uniform doesn't exist, but if I use tf.zeros, adding those two lines seems to prevent the issue. I'm not sure why.", "Why the workaround works:\r\n\r\n(NOTE: the following involves tf.data internal implementation details) \r\n\r\nThe error occurs because keras' `vgg16.preprocess_input` uses a python global (`_IMAGENET_MEAN` [here](https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L143)). \r\n\r\nEach time you create a map dataset, it traces the user-defined function in a tf.function. When the function is traced in dataset.map for the first time, it creates the global (a tf.constant) in the first map function's scope. The second time it is traced, it uses the tf.constant created earlier, which is captured as an input to the second map function. However, this is a symbolic tensor (non-eager tensor) that belongs to a different tf.function, resulting in the error. \r\n\r\nAdding the two lines works because running the `preprocess_input` function before creating the dataset initializes the `_IMAGENET_MEAN` tf.constant outside of any traced functions.", "@CJMenart : Did you get the chance to have a look on @rachellim's response. Please let us know if that resolves the issue. Thanks!", "@achandraa It worked as soon as tchinen recommended I try it. I was able to complete the tasks I was working on.\r\n\r\nBut we're not just going to leave this behavior here, right? The fact that such an awkward workaround is required if you want to tf.map the same function twice seems like a bug to me. ", "This should be fixed now-- can you try with tf-nightly?", "The script above is still throwing the same error...I'm now on 2.0.0-dev20190709.", "@CJMenart Sorry should have been more clear on this. This is not fixed through tensorflow, it's fixed through keras applications. Can you git clone keras-applications from github and pip install it through \"pip install -e .\"?", "Ah OK. Just pulled keras-applications, and the snippet now runs without error.", "Great. Closing now.", "@tanzhenyu @CJMenart Sorry guys, I am running into the same issue when I run the following code on Google Colab:\r\n\r\n```python\r\ndef build_dataset(boxes_df, data_directory='/content'):\r\n    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\r\n    vgg.trainable = False\r\n    outputs = [vgg.get_layer(name).output for name in ['block5_pool']]\r\n    vgg = tf.keras.Model([vgg.input], outputs)\r\n\r\n    filenames_ds = tf.data.Dataset.from_tensor_slices(boxes_df['image_name'].apply(lambda path: os.path.join(data_directory, path)))\r\n    x1_ds        = tf.data.Dataset.from_tensor_slices(boxes_df['x_1'])\r\n    x2_ds        = tf.data.Dataset.from_tensor_slices(boxes_df['x_2'])\r\n    y1_ds        = tf.data.Dataset.from_tensor_slices(boxes_df['y_1'])\r\n    y2_ds        = tf.data.Dataset.from_tensor_slices(boxes_df['y_2'])\r\n    tmp_ds       = tf.data.Dataset.zip((filenames_ds, x1_ds, x2_ds, y1_ds, y2_ds))\r\n    #\"\"\"\r\n    images_ds    = tmp_ds.map(\r\n        lambda path, x1, x2, y1, y2: tf.image.resize_images(\r\n            tf.image.crop_to_bounding_box(\r\n                tf.image.decode_jpeg(tf.read_file(path)),\r\n                tf.cast(x1, tf.int32),\r\n                tf.cast(y1, tf.int32),\r\n                tf.cast(x2 - x1, tf.int32),\r\n                tf.cast(y2 - y1, tf.int32)\r\n            ),\r\n            (224, 224)\r\n        )\r\n    )\r\n    images_ds = images_ds.map(\r\n        lambda img: tf.keras.applications.vgg19.preprocess_input(img)\r\n    )\r\n    features_ds = images_ds.map(\r\n        lambda img: vgg(tf.expand_dims(img, axis=0)).reshape(7 * 7 * 512)\r\n    )\r\n    return features_ds\r\n```", "I came across this same error message when using `tf.keras.applications.densenet.preprocess_input` so this issue isn't just related to VGG model preprocessing. It was fixed by placing \r\n`temp = tf.zeros([4, 32, 32, 3])`\r\n`tf.keras.applications.densenet.preprocess_input(temp)`\r\nat the beginning of the file, as suggested by @tchinen and @rachellim \r\n"]}, {"number": 29930, "title": "Make LossFunctionWrapper importable", "body": "Useful for defining custom wrapped loss functions,", "comments": ["Thanks for your contribution , we will not be accepting PRs which are not against master, so closing this PR\r\nCC @mihaimaruseac"]}, {"number": 29929, "title": "How to connect two neural networks in tensorflow using graphs?", "body": "Im stetting up a system capable of recognizing faces using python and tensorflow.\r\n\r\nI want to recognize two things. \"Person\" for unknown people and \"Name of the person\" with known people (Previously train the tag in the model).\r\n\r\nFor that reason I need to implement 2 neural networks.\r\n\r\nThe first one is going to recognize faces among all the objects (example chair, desk, ..) and the second one is going to classify that frame with the face and check for the label taking in consideration all the known faces for the model. (previously trained)\r\n\r\nso, in order to solved something like this I knows that I need two neural networks and the output of the first one is going to be the input of the second. But I just dont known how can I connect this two neural networks using the graph files. (Created in the training process)\r\n\r\nRight now I can train my model, get the graph file and use that in the recognition process. But I cant mixed the two neural networks or how to add the output of the first model into the input of the second model\r\n\r\n```\r\nwith detection_graph.as_default():\r\n    with tf.Session(graph=detection_graph) as sess:\r\n        while True:\r\n            # Read frame from camera\r\n            ret, image_np = cap.read()\r\n            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n            image_np_expanded = np.expand_dims(image_np, axis=0)\r\n            # Extract image tensor\r\n            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n            # Extract detection boxes\r\n            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n            # Extract detection scores\r\n            scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\n            # Extract detection classes\r\n            classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n            # Extract number of detectionsd\r\n            num_detections = detection_graph.get_tensor_by_name(\r\n                'num_detections:0')\r\n            # Actual detection.\r\n            (boxes, scores, classes, num_detections) = sess.run(\r\n                [boxes, scores, classes, num_detections],\r\n                feed_dict={image_tensor: image_np_expanded})\r\n            # Visualization of the results of a detection.\r\n            vis_util.visualize_boxes_and_labels_on_image_array(\r\n                image_np,\r\n                np.squeeze(boxes),\r\n                np.squeeze(classes).astype(np.int32),\r\n                np.squeeze(scores),\r\n                category_index,\r\n                use_normalized_coordinates=True,\r\n                line_thickness=8)\r\n\r\n            # Display output\r\n            cv2.imshow('object detection', cv2.resize(image_np, (800, 600)))\r\n\r\n            if cv2.waitKey(25) & 0xFF == ord('q'):\r\n                cv2.destroyAllWindows()\r\n                break\r\n```\r\ndetection_graph is the model (reference to the graph file in memory)\r\n\r\nThe previous code works fine running one model (graph file) But I want to integrate a second neural network. (add the output of the first in the input of the second one). Any idea how can I do something like this? I dont see anything like this in the documentation\r\n\r\n", "comments": ["@JulianaCP Have a look on [image captioning](https://www.tensorflow.org/beta/tutorials/text/image_captioning) using tensorflow. This may help you. Thanks!", "@JulianaCP This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 29928, "title": "[ROCm] Adding ROCm support for segment_reduction_ops", "body": "This PR adds ROCm support for segment_reduction_ops.\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n----------------------------------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg  ", "comments": []}, {"number": 29927, "title": "link tensorflow1.13.1 static library failed  (gcc 4.9.3)", "body": "In file included from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/Tensor:120:0,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/tensorflow/core/framework/tensor.h:21,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/tensorflow/core/public/session.h:24,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/src/L2/Algorithm_ctrinterest/nn_raw.h:17,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/src/L2/Algorithm_ctrinterest/nnso_raw.h:9,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/src/L2/Algorithm_ctrinterest/model.cpp:2:\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function \u2018static void Eigen::internal::TensorBlockIO<Scalar, StorageIndex, NumDims, Layout, BlockRead>::Copy(const Block&, StorageIndex, const Dimensions&, const Dimensions&, const Scalar*, Scalar*)\u2019:\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:63: error: the value of \u2018j\u2019 is not usable in a constant expression\r\n         if (++block_iter_state[j].count < block_iter_state[j].size) {\r\n                                                               ^\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:392:16: note: \u2018int j\u2019 is not const\r\n       for (int j = 0; j < num_squeezed_dims; ++j) {\r\n                ^\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:393:35: error: parse error in template argument list\r\n         if (++block_iter_state[j].count < block_iter_state[j].size) {\r\n                                   ^\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function \u2018static void Eigen::internal::TensorBlockCwiseUnaryIO<UnaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const UnaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const InputScalar*)\u2019:\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:604:21: error: parse error in template argument list\r\n         if (++state.count < state.size) {\r\n                     ^\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h: In static member function \u2018static void Eigen::internal::TensorBlockCwiseBinaryIO<BinaryFunctor, StorageIndex, OutputScalar, NumDims, Layout>::Run(const BinaryFunctor&, const Dimensions&, const Dimensions&, OutputScalar*, Eigen::array<StorageIndex, NumDims>&, const LeftScalar*, Eigen::array<StorageIndex, NumDims>&, const RightScalar*)\u2019:\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBlock.h:758:21: error: parse error in template argument list\r\n         if (++state.count < state.size) {\r\n                     ^\r\nIn file included from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/Tensor:143:0,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/tensorflow/core/framework/tensor.h:21,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/tensorflow/core/public/session.h:24,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/src/L2/Algorithm_ctrinterest/nn_raw.h:17,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/src/L2/Algorithm_ctrinterest/nnso_raw.h:9,\r\n                 from /home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/src/L2/Algorithm_ctrinterest/model.cpp:2:\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h: In member function \u2018void Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::block(Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<NewDimensions, XprType>, Device>::OutputTensorBlock*) const\u2019:\r\n/home/cuijg/Documents/computer_so/xuhuanwen/so_cmake/src/publiclib-lite/third_party/tensorflow_1.13.1/static_lib/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:319:35: error: parse error in template argument list\r\n         if (++block_iter_state[i].count < block_iter_state[i].size) {\r\n                                   ^\r\nCMakeFiles/so_cmake.dir/build.make:153: recipe for target 'CMakeFiles/so_cmake.dir/src/src/L2/Algorithm_ctrinterest/model.cpp.o' failed\r\nmake[3]: *** [CMakeFiles/so_cmake.dir/src/src/L2/Algorithm_ctrinterest/model.cpp.o] Error 1\r\nCMakeFiles/Makefile2:72: recipe for target 'CMakeFiles/so_cmake.dir/all' failed\r\nmake[2]: *** [CMakeFiles/so_cmake.dir/all] Error 2\r\nCMakeFiles/Makefile2:84: recipe for target 'CMakeFiles/so_cmake.dir/rule' failed\r\nmake[1]: *** [CMakeFiles/so_cmake.dir/rule] Error 2\r\nMakefile:118: recipe for target 'so_cmake' failed\r\nmake: *** [so_cmake] Error 2", "comments": ["I had the same issue\r\nI resolved it by manually downloading the eigen v 3.3.7 (last stable)\r\n http://eigen.tuxfamily.org/index.php?title=Main_Page\r\n\r\nAnd replace the tensorflow/contrib/makefile/downloads/eigen folder with the content from the .zip file.", "@cSingleboy Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I had a similar issue and solved it by compiling it with gcc-7 instead of gcc-6"]}, {"number": 29926, "title": "How can I get the instance-wise gradient?", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 1.1.13\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nHi, Does anyone know whether tensorflow could provide instance-wise gradient? To be more precise, I mean say the loss is not a scalar, just the size of (N, 1). This is cross entropy loss before one use `tf.reduce_mean`. I wonder how could I get the gradient for each instance in each batch. This means eventually, the shape of the gradient is supposed to be (N, num_of_thetas). I think the currently `tf.gradient` can not do this, right?\r\n\r\n\r\n", "comments": ["@JiahaoYao - This is a support question, and would be better suited for StackOverflow.\r\n\r\nThe gradient terms for each tensor are automatically aggregated. Gradient computation operations such as `optimizer.compute_gradients` and the low-level primitive [`tf.gradients`](https://www.tensorflow.org/api_docs/python/tf/gradients) make a sum of all gradient operations, according to the default `AddN` aggregation method. This is fine for most cases of stochastic gradient descent.\r\n\r\nIn the end unfortunately, gradient computation will have to be made over a single batch. Of course, unless a custom gradient function is built, or the TensorFlow API is extended to provide gradient computation without full aggregation.  [Link to implementation of `tf.gradients`.](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/gradients_impl.py#L368)", "Thanks @dynamicwebpaige !"]}, {"number": 29925, "title": "[ROCm] Adding ROCm support for resource_variable_ops", "body": "This PR adds ROCm support for resource_variable_ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n-------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg ", "comments": ["@deven-amd could you please resolve conflicts ?", "@rthadur done.", "@rthadur \r\n\r\nthis is the 1st of 4 PRs that seem to be stuck in the merge pipeline. \r\nPlease let me know if there is anything that needs to be done on my end.\r\nthanks\r\n\r\ndeven", "@deven-amd copybara could not pull in this changes , trying again now."]}, {"number": 29924, "title": "[INTEL MKL] 1.14 cherrypick request. Moving compat date for fused_bacth_norm_v3. New!", "body": "This PR is a re-submission of  #29817  against r1.14 branch. ", "comments": []}, {"number": 29923, "title": "Code Using FeatureColumn W/ Keras Functional API works for 2.0.0-alpha0 But is broken for 2.0.0-beta1", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):2.0.0-beta1\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):N/A\r\n- GCC/Compiler version (if compiling from source):N/A\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThis is related to the closed issue https://github.com/tensorflow/tensorflow/issues/27416.\r\nThe code snippet below feeds feature columns to Keras models built from functional API. The code works fine for Tensorflow 2.0.0-alpha0 but brakes for Tensorflow 2.0.0-beta1 with the following error message:\r\n\r\n49 validation examples\r\n61 test examples\r\nTraceback (most recent call last):\r\nFile \"/tmp/zeppelin_pyspark-5282677307998123760.py\", line 326, in \r\nexec(code)\r\nFile \"\", line 44, in \r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 662, in call\r\noutputs = call_fn(inputs, *args, **kwargs)\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 166, in wrapper\r\n), args, kwargs)\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 340, in converted_call\r\nif inspect_utils.isbuiltin(f):\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 84, in isbuiltin\r\nif f in six.moves.builtins.dict.values():\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/pandas/core/generic.py\", line 1478, in nonzero\r\n.format(self.class.name))\r\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\nFile \"/tmp/zeppelin_pyspark-5282677307998123760.py\", line 333, in \r\nraise Exception(traceback.format_exc())\r\nException: Traceback (most recent call last):\r\nFile \"/tmp/zeppelin_pyspark-5282677307998123760.py\", line 326, in \r\nexec(code)\r\nFile \"\", line 44, in \r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 662, in call\r\noutputs = call_fn(inputs, *args, **kwargs)\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 166, in wrapper\r\n), args, kwargs)\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 340, in converted_call\r\nif inspect_utils.isbuiltin(f):\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 84, in isbuiltin\r\nif f in six.moves.builtins.dict.values():\r\nFile \"/usr/lib/envs/env-1923-ver-2635-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/pandas/core/generic.py\", line 1478, in nonzero\r\n.format(self.class.name))\r\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nThe code snippet is expected to work.\r\n\r\nOr please help point out what needs to be changed to make it work.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n#!pip install tensorflow==2.0.0-alpha0\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nURL = 'https://storage.googleapis.com/applied-dl/heart.csv'\r\ndataframe = pd.read_csv(URL)\r\ndataframe.head()\r\n\r\ntrain, test = train_test_split(dataframe, test_size=0.2)\r\ntrain, val = train_test_split(train, test_size=0.2)\r\nprint(len(train), 'train examples')\r\nprint(len(val), 'validation examples')\r\nprint(len(test), 'test examples')\r\n\r\n# A utility method to create a tf.data dataset from a Pandas Dataframe\r\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\r\n  dataframe = dataframe.copy()\r\n  labels = dataframe.pop('target')\r\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\r\n  if shuffle:\r\n    ds = ds.shuffle(buffer_size=len(dataframe))\r\n  ds = ds.batch(batch_size)\r\n  return ds\r\n\r\nbatch_size = 5 # A small batch sized is used for demonstration purposes\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nage = feature_column.numeric_column(\"age\")\r\n\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\n\r\n# numeric cols\r\nfor header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\r\n  feature_columns.append(feature_column.numeric_column(header))\r\n  feature_layer_inputs[header] = tf.keras.Input(shape=(1,), name=header)\r\n\r\n# bucketized cols\r\nage_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\nfeature_columns.append(age_buckets)\r\n\r\n# indicator cols\r\nthal = feature_column.categorical_column_with_vocabulary_list(\r\n      'thal', ['fixed', 'normal', 'reversible'])\r\nthal_one_hot = feature_column.indicator_column(thal)\r\nfeature_columns.append(thal_one_hot)\r\nfeature_layer_inputs['thal'] = tf.keras.Input(shape=(1,), name='thal', dtype=tf.string)\r\n\r\n# embedding cols\r\nthal_embedding = feature_column.embedding_column(thal, dimension=8)\r\nfeature_columns.append(thal_embedding)\r\n\r\n# crossed cols\r\ncrossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\r\ncrossed_feature = feature_column.indicator_column(crossed_feature)\r\nfeature_columns.append(crossed_feature)\r\n\r\nbatch_size = 32\r\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\r\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\r\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\r\n\r\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\r\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\r\n\r\nx = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\nx = layers.Dense(64, activation='relu')(x)\r\n\r\nbaggage_pred = layers.Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=baggage_pred)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(train_ds)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I can't seem to replicate this, and it's unclear what line the error message is coming from. Can you try again, and can you clarify what is triggering the error message?\r\n\r\nFor the record, output I get from the code above:\r\n\r\n```\r\n193 train examples\r\n49 validation examples\r\n61 test examples\r\n7/7 [==============================] - 1s 162ms/step - loss: 2.4949 - accuracy: 0.5337\r\n<google3.third_party.tensorflow.python.keras.callbacks.History at 0x7fb722d01ad0>\r\n```", "@karmel thanks for looking at this. That is strange.\r\nYes, I can still reproduce it from my zeppelin notebook. I traced the code a little bit to find the offending statement. Here is the minimal code snippets demonstrating the issue:\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nprint(np.__version__)\r\nprint(pd.__version__)\r\nprint(tf.__version__)\r\n\r\nfeature_columns = []\r\nfeature_layer_inputs = {}\r\n\r\nage = feature_column.numeric_column(\"age\")\r\nfeature_layer_inputs[\"age\"] = keras.Input(shape=(1,), name=\"age\")\r\nfeature_columns.append(age)\r\n\r\nfeature_layer = keras.layers.DenseFeatures(feature_columns)\r\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\r\n# x = layers.Dense(128, activation='relu')(feature_layer_outputs)\r\n# y = layers.Dense(1, activation='sigmoid')(x)\r\n\r\n# model = keras.Model(inputs=feature_layer_inputs, outputs=y)\r\n```\r\nThe version of numpy, pandas, and tensorflow are as follows:\r\n1.16.4\r\n0.24.2\r\n2.0.0-beta1\r\n\r\nThe offending statement is\r\n```\r\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\r\n```\r\nThe following is the starck trace:\r\nTraceback (most recent call last):\r\n  File \"/tmp/zeppelin_pyspark-5407917517763439165.py\", line 331, in <module>\r\n    exec(code)\r\n  File \"<stdin>\", line 7, in <module>\r\n  File \"/usr/lib/envs/env-1923-ver-2647-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 662, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/usr/lib/envs/env-1923-ver-2647-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 166, in wrapper\r\n    ), args, kwargs)\r\n  File \"/usr/lib/envs/env-1923-ver-2647-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py\", line 340, in converted_call\r\n    if inspect_utils.isbuiltin(f):\r\n  File \"/usr/lib/envs/env-1923-ver-2647-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 84, in isbuiltin\r\n    if f in six.moves.builtins.__dict__.values():\r\n  File \"/usr/lib/envs/env-1923-ver-2647-a-4.2.9-py-3.5.3/lib/python3.5/site-packages/pandas/core/generic.py\", line 1478, in __nonzero__\r\n    .format(self.__class__.__name__))\r\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\r\n", "@HubberDev The code (in the beginning of the issue) runs fine for me\r\n```7/7 [==============================] - 1s 104ms/step - loss: 3.3066 - accuracy: 0.4560```\r\nAlso, the minimal code works fine.\r\n\r\nSettings:\r\npython 3.6.4\r\ntf 2.0.0-beta1\r\nI saved your code to a file and ran it by calling python (no notebooks).", "Apparently, it worked on local machine as well with the same setting. Might be something wrong on my notebook setting.\r\n@mahzoon thanks for confirming it.\r\nClosing this ticket. Sorry for the false alarm."]}, {"number": 29922, "title": "Defun and function input_signatures don't accept variables as input", "body": "Defun and function input_signatures don't accept variables as input. To bypass this, we have to explicitly convert tf.Variables to tf.Tensors\r\n**System information**\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): 1.13.1\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n@tf.contrib.eager.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\r\ndef identity(x):\r\n  return x\r\n\r\na = tf.Variable(())\r\nprint(identity(a.read_value()))\r\nprint(identity(a))\r\n```\r\n\r\nThis fails on the second call:\r\n```\r\nValueError: When input_signature is provided, all inputs to the Python function must be Tensors.\r\n```", "comments": ["Please have a look on this [link](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) which says python function compiled with an input_signature must only accept Tensors as arguments. Let us know if that helps to resolve the issue. Thanks!", "> Please have a look on this [link](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/defun) which says python function compiled with an input_signature must only accept Tensors as arguments. Let us know if that helps to resolve the issue. Thanks!\r\n\r\nI see, that behavior is somewhat unintuitive. I would expect that all `tf.Variable`s would get automatically converted to `tf.Tensor` like we expect from all other ops.\r\n\r\nThanks for looking into it anyway!"]}, {"number": 29921, "title": "Tf.app.flags implicit parsing potentially causes crash with exception", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution:\r\nLinux Fedora 30\r\n- TensorFlow installed from:\r\nbinary\r\n- TensorFlow version:\r\n1.13.1\r\n- Python version:\r\n3.7.3\r\n- CUDA version\r\n10.0\r\n- GPU Model\r\nNVIDIA TITAN V Black\r\n\r\n**Describe the current behavior**\r\n\r\nThe -h, -help, -helpshort, and -helpfull arguments may cause exceptions to be triggered as opposed to displaying flags, default values, and programmer-provided information. This seems to occur when the `tf.app.flags` abseil wrapper attempts to use implicit parsing.\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen any value of -h, -help, -helpshort, -helpfull are provided as arguments to scripts using `tf.app.flags`, the help list should appear.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n''' mwe.py: displays tensorflow --help argument issue '''\r\nfrom tensorflow import app\r\napp.flags.DEFINE_string('myflag', 'Default', 'Help output')\r\nFLAGS = app.flags.FLAGS\r\nglobal_string = \"This string causes the abseil wrapper to begin processing the {0} flag\".format(FLAGS.myflag)\r\ndef main(_):\r\n  print(\"This string causes TF to race the abseil parsing process, which kills the help menu using string %s\" % FLAGS.myflag)\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\n**Other info / logs**\r\nThe below bash script uses the above code to display the issue. Notably, using abseil's app.flags will NOT cause the exception to occur with the same source code.\r\n```bash\r\npython mwe.py --help\r\nsed -i 's/global_string/#global_string/g' mwe.py\r\npython mwe.py --help\r\n```\r\n\r\nThis is related to issue #28581. After elaborating on the issue and not receiving a response, I have opened this new issue. If further information is required, please let me know how I may assist you.", "comments": ["Can you please let us know what are the exceptions you are facing .Thanks.", "Sure. Hopefully you can reproduce these results by running the included code above. But here's what I get:\r\n```bash\r\npython mwe.py\r\nThis string causes TF to race the abseil parsing process, which kills the help menu using string Default\r\n```\r\n```bash\r\npython mwe.py -help\r\n```\r\n```python\r\nTraceback (most recent call last):\r\n File \u201cmwe.py\u201d, line 5, in <module>\r\n   global_string = \u201cThis string causes the abseil wrapper to begin processing the {0} flag\u201d.format(FLAGS.myflag)\r\n File \u201c/home/.local/lib/python3.7/site-packages/tensorflow/python/platform/flags.py\u201d, line 84, in __getattr__\r\n   wrapped(_sys.argv)\r\n File \u201c/home/.local/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u201d, line 633, in __call__\r\n   name, value, suggestions=suggestions)\r\nabsl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag \u2018help\u2019\r\n```\r\nI should also note that if you switch to just using abseil without the tensorflow wrapper (change `from tensorflow import app` to `from absl import app`, the following exception is raised regardless of whether or not a help flag is specified:\r\n\r\n```bash\r\npython mwe_absl.py [-help]\r\n```\r\n```python\r\nTraceback (most recent call last):\r\n  File \"mwe.py\", line 5, in <module>\r\n    global_string = \"This string causes the abseil wrapper to begin processing the {0} flag\".format(FLAGS.myflag)\r\n  File \"/home/.local/lib/python3.7/site-packages/absl/flags/_flagvalues.py\", line 491, in __getattr__\r\n    raise _exceptions.UnparsedFlagAccessError(error_message)\r\nabsl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --myflag before flags were parsed.\r\n```", "I was able to reproduce the code in Colab. Thanks.", "Sorry, I just saw this. Will investigate", "It also reproduces in latest nightly.\r\n\r\nHowever, I don't think I'm a good owner for this, don't know where to fix. Unassigning myself.", "Tested with TF 1.15.0rc1 it breaks with message,\r\n```python\r\nUnrecognizedFlagError: Unknown command line flag 'f'\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "I am no longer maintaining the particular codebase, but I was able to update my minimum working example as follows for 2.x compatibility and the issue still seems to occur. That said, it may be lower priority as it is legacy behavior now.\r\n\r\nmwe_2x.py\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.compat.v1 import app, flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_string('myflag', 'Default', 'Help output')\r\nglobal_string = f\"Use abseil wrapper to access {FLAGS.myflag}\"\r\ndef main(argv):\r\n  print(f\"Race condition on {FLAGS.myflag}\")\r\nif __name__ == '__main__':\r\n  app.run(main)\r\n```\r\n\r\nTest for error and resulting exception:\r\n```\r\npython3 mwe_2x.py --help\r\n```\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File \"mwe_2x.py\", line 5, in <module>\r\n    global_string = f\"Use abseil wrapper to access {FLAGS.myflag}\"\r\n  File \"/.../python3.8/site-packages/tensorflow/python/platform/flags.py\", line 85, in __getattr__\r\n    wrapped(_sys.argv)\r\n  File \"/.../python3.8/site-packages/absl/flags/_flagvalues.py\", line 654, in __call__\r\n    raise _exceptions.UnrecognizedFlagError(\r\nabsl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'help'\r\n```\r\n\r\nTest to ensure error originates from Tensorflow, not Abseil (Abseil on its own properly creates exception):\r\n```\r\nsed \"s/^import/#import/\" mwe_2x.py > mwe_2x_test1.py;\r\nsed -i \"\" -e \"s/tensorflow.compat.v1/absl/\" mwe_2x_test1.py;\r\npython3 mwe_2x_test1.py --help\r\n```\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File \"mwe_2x_test1.py\", line 6, in <module>\r\n    global_string = f\"Use abseil wrapper to access {FLAGS.myflag}\"\r\n  File \"/.../python3.8/site-packages/absl/flags/_flagvalues.py\", line 498, in __getattr__\r\n    raise _exceptions.UnparsedFlagAccessError(error_message)\r\nabsl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --myflag before flags were parsed.\r\n```\r\nNote that Abseil properly catches the race condition even in --help is not given:\r\n```\r\npython3 mwe_test1.py\r\n```\r\nOutput:\r\n```\r\nTraceback (most recent call last):\r\n  File \"mwe_2x_test1.py\", line 5, in <module>\r\n    global_string = f\"Use abseil wrapper to access {FLAGS.myflag}\"\r\n  File \"/.../python3.8/site-packages/absl/flags/_flagvalues.py\", line 498, in __getattr__\r\n    raise _exceptions.UnparsedFlagAccessError(error_message)\r\nabsl.flags._exceptions.UnparsedFlagAccessError: Trying to access flag --myflag before flags were parsed.\r\n```\r\n\r\nTest to run as expected, but without desired behavior:\r\n```\r\nsed \"s/global_string/#global_string/\" mwe_2x.py > mwe_2x_test2.py;\r\npython3 mwe_2x_test2.py --help\r\n```\r\nOutput:\r\n```\r\n\r\n       USAGE: mwe_2x_test2.py [flags]\r\nflags:\r\n\r\nmwe_2x_test2.py:\r\n  --myflag: Help output\r\n    (default: 'Default')\r\n\r\nTry --helpfull to get a list of all flags.\r\n```", "If it only comes from `tf.compat`, then I think we can close this. Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29921\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29921\">No</a>\n"]}, {"number": 29920, "title": "[ROCm] Adding ROCm support for reduction gpu kernels ", "body": "This PR adds ROCm support for the code within the `reduction_gpu_kernels.cu.h file`\r\n\r\nThere are a few different functional changes associated with this PR, and they have been broken down into individual commits. Reviewing this PR on a per-commit basis will be easier.\r\n\r\nThe last commit in this PR is actually from another PR ( #29181 ). Though not a part of this PR, I have included it here, because it is required to make the `--config=rocm` build work after the changes contained within this PR. If PR 29181 gets merged before this PR, we can drop that commit from this PR (and vice-versa)\r\n\r\nThis is a non-trivial change, and a pre-req for upstream ROCm support for all operators that rely on the GPU reduction kernels contained within this file.\r\n\r\n-----------------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": ["@chsigg gentle ping...please review. thanks."]}, {"number": 29919, "title": "TF=GPU 2.0.0b TF Transform missing AUTOTUNE on import", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): POSIX\r\n- TensorFlow version: 2.0.0b-gpu\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: Tesla T4\r\n\r\n\r\n\r\n**Describe the problem**\r\nAttempted to use TFX, specifically TF Transform, however when importing TF Transform it throws the error that AUTOTUNE is unknown.\r\n\r\n`!pip install tensorflow_transform`\r\n`import tensorflow_transform as tft`\r\n\r\n**Any other info / logs**\r\nAttributeError: module 'tensorflow.python.data.experimental.ops.optimization' has no attribute 'AUTOTUNE'\r\nFor the full log see the attached file\r\n\r\n[TF_Transform_Import_Error.txt](https://github.com/tensorflow/tensorflow/files/3302615/TF_Transform_Import_Error.txt)\r\n", "comments": ["Looks like a TF Transform issue. For better and faster support, please post it to [Transform repository](https://github.com/tensorflow/transform/issues). Thanks!", "Closing and reopened as https://github.com/tensorflow/transform/issues/123", "I believe this is not to do with the transform, but with the initialisation of tensorflow contrib. I just updated to 2.0.0b1, and when I run just\r\n\r\n```\r\nfrom tensorflow import contrib\r\nautograph = contrib.autograph\r\n``` \r\n\r\nfrom [here](https://www.tensorflow.org/guide/autograph) I receive the following error also:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/george/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 38, in <module>\r\n    from tensorflow.contrib import data\r\n  File \"/home/george/.local/lib/python3.6/site-packages/tensorflow/contrib/data/__init__.py\", line 103, in <module>\r\n    from tensorflow.contrib.data.python.ops.readers import CsvDataset\r\n  File \"/home/george/.local/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/readers.py\", line 48, in <module>\r\n    prefetch_buffer_size=optimization.AUTOTUNE,\r\nAttributeError: module 'tensorflow.python.data.experimental.ops.optimization' has no attribute 'AUTOTUNE'\r\n```\r\n\r\nLooking at the error log you provided, it appears to be the same problem"]}, {"number": 29918, "title": "22794", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide all the information asked by the template and post a new issue. Thanks!"]}, {"number": 29917, "title": "[TensorFlow 2.0] Taking a slice of a Keras Input layer gives a tensor with unknown shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Mac OS X 10.14.5**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1**\r\n- Python version: **3.6.8**\r\n- CUDA/cuDNN version: **don't have a GPU**\r\n- GPU model and memory: **don't have a GPU**\r\n\r\n**Describe the current behavior**\r\n\r\nIf I create an input layer with Keras and take a slice of it, the slice has unknown shape:\r\n\r\n```python\r\n>>> inputs = tf.keras.Input((10, 10))\r\n>>> inputs.shape\r\nTensorShape([None, 10, 10])\r\n>>> slice = inputs[:,:,:5]\r\n>>> slice.shape\r\nTensorShape(None)\r\n```\r\n\r\nThis is a problem when I want to use this slice in something else, e.g. here with a softmax:\r\n\r\n```python\r\n>>> tf.keras.activations.softmax(slice)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"[path_to_my_env]/lib/python3.6/site-packages/tensorflow/python/keras/activations.py\", line 66, in softmax\r\n    elif ndim > 2:\r\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI'd expect `slice` to have shape `TensorShape([None, 10, 5])`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.Input((10, 10))\r\nassert inputs.shape.as_list() == [None, 10, 10]  # This works\r\nassert inputs[:,:,:5].shape.as_list() == [None, 10, 5]  # Raises ValueError\r\n```", "comments": ["I was able to reproduce the above mentioned issue on Colab with Tensorflow 2.0.0.beta1. Thanks!", "I have the problem, any solution?", "@aaronlyt I found a temporary fix to get my code running.  You can call `set_shape` on a tensor to enforce a specific shape on it.  In my example it would look like this:\r\n```python\r\n>>> inputs = tf.keras.Input((10, 10))\r\n>>> slice = inputs[:,:,:5]\r\n>>> slice.shape\r\nTensorShape(None)\r\n>>> slice.set_shape(inputs.shape[:-1].concatenate(5))\r\n>>> slice.shape\r\nTensorShape([None, 10, 5])\r\n```", "Thanks, it works", "I believe the object returned from Input is simply a Tensor -- so this looks like a bug or missing feature in the slide shape function? @aselle is this known?", "The code snippet works in TF 1.14 .  However fails with TF 2.0 nightly (2.0.0-dev20190628)", "This is fixed with latest tf 2.0 nightly version '2.0.0-dev20190809'. Thanks!", "@omalleyt12 was the fixing commit before or after 2.0 branch cut? I'd like to make sure the release has the fix.", "@martinwicke oops sorry for not updating this, this (and other related loss-of-shape bugs) were fixed before the cut", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29917\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29917\">No</a>\n"]}, {"number": 29916, "title": "Second order gradient of tf.contrib.eager.function is broken", "body": "When using 2nd order gradients (i.e. Nested GradientTape) we cannot utilize @tf.defun and @tf.function.\r\n\r\n**System information**\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): 1.13.1\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_defun.py\", line 14, in <module>\r\n    print(meta_tape.gradient(a2, a))\r\n  File \"/home/felipe.such/uber/generative_learning/env/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 946, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n  File \"/home/felipe.such/uber/generative_learning/env/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\", line 72, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n  File \"/home/felipe.such/uber/generative_learning/env/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\", line 127, in _gradient_function\r\n    grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\r\n  File \"/home/felipe.such/uber/generative_learning/env/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 94, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: StatefulPartitionedCall\r\n```\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\n@tf.contrib.eager.function\r\ndef leaky_relu(x, alpha=0.1):\r\n  return tf.maximum(tf.minimum(0.0, alpha * x), x)\r\n\r\na = tf.Variable(())\r\nwith tf.GradientTape() as meta_tape:\r\n  with tf.GradientTape() as tape:\r\n    x = leaky_relu(a)\r\n  a2 = tape.gradient(x, a) + a\r\nprint(meta_tape.gradient(a2, a))\r\n```", "comments": ["Have tried on Colab with TF 13.1 and was able to reproduce the issue.", "Maybe related?\r\nhttps://github.com/tensorflow/tensorflow/issues/29942", "@fps7806 I could reproduce the issue with TF1.13.1 but I don't see any error with TF.1.14.0. I created a [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/bc4e6c61ebbe156c20fab10a9de3e51c/tf_29916.ipynb). Please check it. thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29916\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29916\">No</a>\n"]}, {"number": 29915, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The `--config=rocm` build was broken due to a combination of following PR + commit getting merged\r\n * PR #28488 - Adding ROCm support for scan ops\r\n   * (merged yesterday)\r\n * db3ecd34a4fd527912207e1374a905d8fd247622 - adds a new file `scan_ops_gpu_int.cu.cc`\r\n   * (merged 4 days ago)\r\n\r\nThe scan_ops_* file is a recent addition and has was not included in PR 28488 (which was filed a couple of weeks ago). It has kernels within it (which are enabled for CUDA only) and are referred to by the rest of the scan ops code, which is enabled for CUDA as well as ROCm (as a consequence of PR 28488 getting merged). This results in the `--config=rocm` build failure.\r\n\r\nThe \"fix\" here is to merely enable the code within the `scan_ops_gpu_int.cu.cc` file for the ROCm platform as well\r\n\r\n--------------------------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n\r\n", "comments": []}, {"number": 29914, "title": " No OpKernel was registered to support Op 'TensorArrayWriteV3'", "body": "I'm trying to perform the inference on a pre-trained model that I imported into the assets folder.\r\n\r\nWhen I make inference I get the following error:\r\n\r\n`java.lang.IllegalArgumentException: No OpKernel was registered to support Op TensorArrayWriteV3' used by {{node map_4/while/TensorArrayWrite/TensorArrayWriteV3}}with these attrs: [T=DT_UINT8, _class=[\"loc:@map_4/while/convert_image\"]]\r\n`\r\n", "comments": ["@veeeencedamico2 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29913, "title": "ModuleNotFoundError: No module named '_pywrap_tensorflow'", "body": "Hello all,\r\n\r\nI am trying to import tensorflow but it shows the following error. \r\n\r\n`ModuleNotFoundError: No module named '_pywrap_tensorflow'`\r\n\r\nI have already tried to add the MSVC140.DLL file from [ttps://www.dll-files.com/msvcp140.dll.html]( https://www.dll-files.com/msvcp140.dll.html)  it still shows the stated error. \r\n\r\nOS: Windows 10\r\nPython version: 3.6.4\r\nConda version: 4.6.14\r\nPIP version: 19.1.1\r\nGPU: AMD Radeon R540x\r\nCommand used to install tensorflow: `conda install tensorflow `  ; `pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl` which I got from this link [https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip)\r\n\r\nI don't use CUDA services and am using only CPU support.\r\n\r\n```\r\n(base) (picktolight) F:\\Internship\\Nokia_work>python\r\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"F:\\Internship\\Nokia_work\\picktolight\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\nAlso, I am getting the error using cmd. \r\n![image](https://user-images.githubusercontent.com/31880143/59679060-836fe000-91eb-11e9-9b24-d962049753f1.png)\r\n\r\n**Any help will be greatly appreciated.** \r\n **Thanks a lot!**\r\n", "comments": ["I'm not super familiar with conda, but I think it's supposed to be one or the other regarding pip. \r\n\r\nI think with conda, that's your package manager now (although you can still use pip). You can also use it to create an environment, and then just use pip (at that point virtualenv would make more sense though. Or pipenv). Using both doesn't make sense. \r\n\r\nFor windows, you could do a classic install of python and just do pip install tensorflow. \r\n\r\nIf you have an intel cpu, these might be useful as well:\r\n\r\nhttps://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide", "Why are you installing a MacOS pip on a windows machine?", "@rlewkowicz  I did try to use pip with this but got an error. \r\n\r\n```\r\n(picktolight) F:\\Internship\\Nokia_work>pip install tensorflow\r\nCollecting tensorflow\r\n  ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\nSo, decided to use conda instead but got this, \r\n\r\n```\r\n(picktolight) F:\\Internship\\Nokia_work>conda install tensorflow\r\nCollecting package metadata: done\r\nSolving environment: done\r\n\r\n# All requested packages already installed.\r\n\r\n(picktolight) F:\\Internship\\Nokia_work>python\r\nPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:04:45) [MSC v.1900 32 bit (Intel)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n>>> ^Z\r\n```\r\nI really am not able to understand this. ", "@mihaimaruseac  I first tried with this link, present on the main documentation.\r\n\r\n[https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl](https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl)\r\n\r\nbut then got the following error, \r\n\r\n```\r\n(picktolight) F:\\Internship\\Nokia_work>pip install https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl\r\nERROR: tensorflow-1.13.1-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\r\n```\r\nSo had no choice but to use the link, \r\n[https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl](https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.12.0-py3-none-any.whl)\r\n\r\nbecause this is the only thing that works, but then I get this error,\r\n`ModuleNotFoundError: No module named '_pywrap_tensorflow' `\r\n", "Are you by any chance on a 32bits system?\r\n\r\nAlso, the fact that Conda says TF is installed but Python doesn't see it is an indication that python path doesn't include conda dirs", "@mihaimaruseac  No, I am using a 64-bits OS.\r\nI think the path can be an issue, but I am using `virtualenv`  shouldn't that help? \r\nAll the required packages get stored in the `virtualenv` folder, so python should take it from there or that is what I think. \r\nMaybe, if you can elaborate more on the path issue, I can try with that.", "I don't think Conda install in the virtualenv path :-/", " @mihaimaruseac    Well, Anaconda installs its own version of python. I tried using `import tensorflow`.\r\nbut got the following error. \r\n\r\n```\r\nF:\\Internship\\Nokia_work>py\r\nPython 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\", line 16, in <module>\r\n    from . import multiarray\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 47, in <module>\r\n    import numpy as np\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\", line 142, in <module>\r\n    from . import add_newdocs\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\numpy\\add_newdocs.py\", line 13, in <module>\r\n    from numpy.lib import add_newdoc\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\numpy\\lib\\__init__.py\", line 8, in <module>\r\n    from .type_check import *\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\numpy\\lib\\type_check.py\", line 11, in <module>\r\n    import numpy.core.numeric as _nx\r\n  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\", line 26, in <module>\r\n    raise ImportError(msg)\r\nImportError:\r\nImporting the multiarray numpy extension module failed.  Most\r\nlikely you are trying to import a failed build of numpy.\r\nIf you're working with a numpy git repo, try `git clean -xdf` (removes all\r\nfiles not under version control).  Otherwise reinstall numpy.\r\n\r\nOriginal error was: DLL load failed: The specified module could not be found.\r\n\r\n>>> ^Z\r\n```\r\nI think conda did not install the package in the default python path (which has python 3.6.4) and installed the packages on its Python path (which has 3.7.1 version of python). ", "Just to verify did you try following steps from [TensorFlow website](https://www.tensorflow.org/install/pip). Let us know. Thanks!", "@achandraa  Yes, I followed the exact process as mentioned on the website. \r\nAs stated above `pip install tensorflow` gave me an error. So I tried using the wheel link provided on the website, but it showed unsupported wheel. \r\n\r\n[https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl](https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl) \r\nI tried to use the above link.\r\nThe command I gave was,\r\n`pip install https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.13.1-cp36-cp36m-win_amd64.whl`", "It really seems that your environment is in a terrible state. But let's try to debug. Can you please tell me the outputs of\r\n\r\n```bash\r\n$ pip --version\r\n$ python3.6 -m pip --version \r\n```\r\n\r\nMake sure you're running the above commands in a clear directory, without any virtualenv/conda environment active", "**@mihaimaruseac @rlewkowicz @achandraa  Thank you all! The problem got solved.** \r\n\r\nI solved my problem by completely migrating to Anaconda. I uninstalled all versions of python ( had 3 versions 3.6.4, 3.7.3, and 3.6.8 ). It seems 3.7.3  and 3.6.8 were installed my Anaconda. As stated `pip install tensorflow` was not working but `conda install tensorflow` was working.\r\n \r\nAs it was pointed out by **@mihaimaruseac** the conda command installed everything to its own Path (As conda used Python 3.7.3, and its Path was `C:\\Users\\Admin\\Anaconda3`) , while on my **Command Prompt** I was still running Python 3.6.4 (which had `C:\\Users\\Admin\\AppData\\Local\\Python35-36` as its path). \r\n\r\nThis created a conflict, as `conda install tensorflow` (I had added conda path to my cmd prompt so could access it from there ) installed everything in its own path which had **Python 3.7.3** as its default. As pointed out here [https://github.com/ContinuumIO/anaconda-issues/issues/1508]( https://github.com/ContinuumIO/anaconda-issues/issues/1508) **(while using Anaconda I got this Numpy  error, so decided to follow the steps listed out on this issue.)** \r\n\r\nFor convineince, I am mentioning the commands here:\r\n\r\n```\r\nset ROOT=%TEMP%\r\ncd %ROOT%\r\npowershell -command \"& { (New-Object Net.WebClient).DownloadFile('https://repo.continuum.io/miniconda/Miniconda3-4.3.21-Windows-x86.exe', 'mc3.exe') }\"\r\nstart /wait \"\" mc3.exe /InstallationType=JustMe /AddToPath=0 /RegisterPython=0 /NoRegistry=1 /S /D=%ROOT%\\mc3\r\n: Properly activate the env.\r\n%ROOT%\\mc3\\Scripts\\activate.bat\r\n: Update everything\r\nconda update -y --all\r\nconda install -y numpy\r\npython -c \"import numpy\"\r\n```\r\n**This solved my problem for a while but conda degraded its Python version to 3.6.8, and all my Python packages started to give error. It was like everything I ever wrote gave me an error.**  \r\nThis is when I lost my patience and decided to completely migrate from `pip` to `conda`. \r\nSo I decided to delete the current version of Anaconda Navigator and all versions of Python from my PC, and then redownloaded Anaconda Navigator from here:  [https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/). \r\n\r\nI have not downloaded any version of python after this and only use Anaconda Prompt to use Python that too in a conda environment. \r\n**Now, I make a conda environment for everything I do and usually use Python 3.5.0  (which In my opinion the most stable release of python till date)** , I hope I may never encounter this and everything goes on without Python creating any more problems in my life.\r\nNow, I only use anaconda distribution, hoping to never encounter these errors ever again. \r\n**But really thanks to you all!! I wouldn't have been able to solve this madness without you helping me.**\r\n\r\n**Thanks a lot for your help and Patience!!**\r\n**Cheers!**"]}, {"number": 29912, "title": "Change tf.py_func to tf.numpy_function", "body": "tf.py_func throws a warning and tf.numpy_function is the equivalent, always stateful, v2 implementation.\r\n\r\nThis should not break anything as `tf.numpy_function` is internally equivalent to `tf.py_func(stateless=True)`", "comments": []}, {"number": 29911, "title": "Input_signature of a tf.function decorator crashes when using multiple GPUs with MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7.6.1810\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): tensorflow-gpu 2.0.0-beta0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): / \r\n- CUDA/cuDNN version: 10.0.130 / 7.6.0\r\n- GPU model and memory: Tesla P100-SXM2-16GB\r\n\r\n**Describe the current behavior**\r\nTensorflow crashes when checking the input_signature of a tf.function decorator when using multiple GPUs in a MirroredStrategy. A ValueError is generated cause a PerReplica object cannot be converted to a Tensor (see the log below). Below you can find the minimum code needed to reproduce the error. The code runs just fine when I only utilize one GPU `strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])`. Furthermore, if the optional argument input_signature is discarded (only using `@tf.function()`) the error disappears too (again using multiple GPUs). Hence, the specific combination of input_signature and multiple GPUs causes the problem (which I need for performance reasons in my work).\r\n\r\n**Describe the expected behavior**\r\nThe code below won't generate any errors.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n        \r\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\r\n    \r\nwith strategy.scope():\r\n    dataset = tf.data.Dataset.from_tensor_slices(np.ones([100, 12]).astype(np.float32))\r\n    dataset = dataset.batch(4)\r\n    dataset = strategy.experimental_distribute_dataset(dataset)\r\n    \r\n    def compute(input_data):\r\n        return tf.reduce_sum(input_data, [1])\r\n    \r\n    @tf.function(input_signature = (tf.TensorSpec([None, 12], tf.float32),))\r\n    def distributed_run(input_data):\r\n        return strategy.experimental_run_v2(compute, args = (input_data,))\r\n\r\n    for x in dataset:\r\n        output = distributed_run(x)\r\n        print(output)\r\n```\r\n\r\n**Other info / logs**\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1216, in _convert_inputs_to_signature\r\n>     value, dtype_hint=spec.dtype)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1100, in convert_to_tensor\r\n>     return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1158, in convert_to_tensor_v2\r\n>     as_ref=False)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1237, in internal_convert_to_tensor\r\n>     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 305, in _constant_tensor_conversion_function\r\n>     return constant(v, dtype=dtype, name=name)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 246, in constant\r\n>     allow_broadcast=True)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 254, in _constant_impl\r\n>     t = convert_to_eager_tensor(value, ctx, dtype)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 115, in convert_to_eager_tensor\r\n>     return ops.EagerTensor(value, handle, device, dtype)\r\n> ValueError: Attempt to convert a value (PerReplica:{\r\n>   0 /job:localhost/replica:0/task:0/device:GPU:0: <tf.Tensor: id=107, shape=(2, 12), dtype=float32, numpy=\r\n> array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\r\n>        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>,\r\n>   1 /job:localhost/replica:0/task:0/device:GPU:1: <tf.Tensor: id=108, shape=(2, 12), dtype=float32, numpy=\r\n> array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\r\n>        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>\r\n> }) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"issue2.py\", line 19, in <module>\r\n>     output = distributed_run(x)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 432, in __call__\r\n>     *args, **kwds)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1169, in canonicalize_function_inputs\r\n>     self._flat_input_signature)\r\n>   File \"/data/gent/gvo000/gvo00003/vsc41939/GENIUS/miniconda3/envs/tensorflow2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1222, in _convert_inputs_to_signature\r\n>     (str(inputs), str(input_signature)))\r\n> ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors.Inputs ((PerReplica:{\r\n>   0 /job:localhost/replica:0/task:0/device:GPU:0: <tf.Tensor: id=107, shape=(2, 12), dtype=float32, numpy=\r\n> array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\r\n>        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>,\r\n>   1 /job:localhost/replica:0/task:0/device:GPU:1: <tf.Tensor: id=108, shape=(2, 12), dtype=float32, numpy=\r\n> array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\r\n>        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>\r\n> },)), input_signature((TensorSpec(shape=(None, 12), dtype=tf.float32, name=None),)).", "comments": ["Thank you for reporting, we are going to look into fixing this issue. \r\n\r\nIn the meantime, I am wondering how you can get around this issue to get the performance you're seeking. I am assuming that your input data has varying dimensions in various steps, and you don't want to tf.function to retrace everytime that happens. And you're trying to avoid that re-tracing by providing an appropriate input signature. \r\n\r\nThe 3 possible workarounds could be:\r\n- Try the `experimental_relax_shapes` argument to tf.function https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function. This may not give you the most optimal but it could work.\r\n- Instead of passing the actual inputs as argument to the tf.function, pass something that can be used to get the inputs. For instance, pass a callable that when called will return the data. This callable will never change so the function will not retrace. This is kind of hacky but something we have used sometimes to get good performance. You can also create the iterator on the dataset by `iter(dataset)` and pass that as argument to the function, and do `next()` inside the function to get the inputs. That will also get around this issue, I believe. \r\n- You could also consider passing the entire dataset into the tf.function and iterate inside it. This will arguably give you the best performance as it will convert the entire thing into a tf.while_loop. \r\n", "Thanks for the reply and the suggestions. I will experiment with them in the following days.", "I had the same problem and am curious if the suggestions (and which one, specifically) worked for you @mcoolsce ", "@dsgupta, I tried all the options and the second one seems to work fine: \r\n\r\n- The argument `experimental_relax_shapes` had no effect at all. \r\n\r\n- This is the option I am using now. I create my dataset and make an iterable out of it, which is the only argument going into the tf.function. Inside the function, I call `next(dataset)` to get access to my data. Performance-wise (tested on 1 GPU), it seems that is option is a little bit slower compared to my original code.\r\n\r\n- I think the third option is theoretically the fastest but it throws some Out of Memory errors a few iterations in the converted tf.while_loop. I just pass my entire dataset into the tf.function wrapper and used a python for loop to iterate over the data. This is very strange as I had no memory issues at all before (where I iterate over the data using a python for loop outside the tf.function). ", "@mcoolsce Thanks for the response! I faced the same sort of problem with the third approach, but instead of Out of Memory, I was getting Recursion Depth Exceeded if I tried using `for input in dataset:` I'll try the second one and see how it goes.", "The first one doesn't work.\r\n\r\nI tried the second option, but it turns out to be another problem \"BaseCollectiveExecutor::StartAbort Out of range: End of sequence\".\r\n\r\nThe third option is not suitable for me since I want to dynamically adjust the training strategy with the loss from the train step.\r\n\r\nAny other suggestions ? ", "@Slyne is the issue you encountered with second approach a bug? perhaps you can file a separate issue for that and we can figure out how you can make progress with that?\r\nthe only other workaround I can think of is if you want to actually make up the distributed input signature yourself. I can provide the code for that but it requires using private TF APIs. \r\n\r\n", "> @Slyne is the issue you encountered with second approach a bug? perhaps you can file a separate issue for that and we can figure out how you can make progress with that?\r\n> the only other workaround I can think of is if you want to actually make up the distributed input signature yourself. I can provide the code for that but it requires using private TF APIs.\r\n\r\nThe second workaround gives an error like this issue https://github.com/tensorflow/tensorflow/issues/22484, unless I stop iterating the dataset before the last step.\r\n", "@Slyne can you share the code for what you're doing when using the second workaround? likely you need to change your code to catch the out of range error (or i maybe able to suggest a slightly different API to get the next element)", "Any info about when will this bug be fixed?", "Any info about when this bug will be fixed. For dynamic batch size inputs, this input_signature is essential, otherwise it will lead to tracing, which is very expensive.", "This issue has now been [fixed](e1136256a791b02f793ff3f3e1f9ee7da4606807). You can use the `element_spec` property on the dataset or iterator to specify the `tf.TypeSpec`. For example,\r\n```\r\n# For the `experimental_distribute_dataset API`\r\ndataset = tf.data.Dataset(...)\r\ndist_dataset = strategy.experimental_distributed_dataset(dataset)\r\n# Use the `element_spec` of the distributed dataset\r\n@tf.function(input_signature=[dist_dataset.element_spec])\r\ndef train_step(...):\r\n  ....\r\n\r\n# Use the `element_spec` of the distributed iterator\r\niterator = iter(dist_dataset)\r\n@tf.function(input_signature=[iterator.element_spec])\r\ndef train_step(..)\r\n  ...\r\n\r\n# For the `experimental_distribute_datasets_from_function` API\r\n# Use the `element_spec` of the distributed iterator\r\ndataset = tf.data.Dataset(...)\r\ndist_dataset = strategy.experiemental_distribute_datasets_from_function(dataset)\r\niterator = iter(dist_dataset)\r\n@tf.function(input_signature=[iterator.element_spec])\r\ndef train_step(..)\r\n  ...\r\n```\r\nPlease reopen if you run into issues.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29911\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29911\">No</a>\n", "> This issue has now been [fixed](e1136256a791b02f793ff3f3e1f9ee7da4606807). You can use the `element_spec` property on the dataset or iterator to specify the `tf.TypeSpec`. For example,\r\n> \r\n> ```\r\n> # For the `experimental_distribute_dataset API`\r\n> dataset = tf.data.Dataset(...)\r\n> dist_dataset = strategy.experimental_distributed_dataset(dataset)\r\n> # Use the `element_spec` of the distributed dataset\r\n> @tf.function(input_signature=[dist_dataset.element_spec])\r\n> def train_step(...):\r\n>   ....\r\n> \r\n> # Use the `element_spec` of the distributed iterator\r\n> iterator = iter(dist_dataset)\r\n> @tf.function(input_signature=[iterator.element_spec])\r\n> def train_step(..)\r\n>   ...\r\n> \r\n> # For the `experimental_distribute_datasets_from_function` API\r\n> # Use the `element_spec` of the distributed iterator\r\n> dataset = tf.data.Dataset(...)\r\n> dist_dataset = strategy.experiemental_distribute_datasets_from_function(dataset)\r\n> iterator = iter(dist_dataset)\r\n> @tf.function(input_signature=[iterator.element_spec])\r\n> def train_step(..)\r\n>   ...\r\n> ```\r\n> \r\n> Please reopen if you run into issues.\r\n\r\nThis method only works for `tf-nightly-gpu` (Tensorflow version: 2.2.0) and **not** for TF 2.0", "@anj-s @alisaaalehi \r\n\r\nthe provided solution doesn't work for me (on `2.2.0.dev20200207`). \r\n\r\n* can you clarify which function would need to be decorated: the inner `train_step` or the `distributed_train_step`?\r\n* my guess is that in my case the input_signature doesn't match as I modified the input in the training step (`x = encoder(x)`). Is that correct? What would be a suggested better way to handle thins?\r\n\r\n```python\r\n        def train_step(x, y):\r\n            with tf.GradientTape() as tape:\r\n                logits = model(x, training=True)\r\n                # Loss value for this batch.\r\n                loss, metrics = loss_fn(logits, targets, encoder)\r\n                loss = loss * (1.0 / global_batch_size)\r\n                # Get gradients of weights wrt the loss.\r\n                gradients = tape.gradient(loss, mapper.trainable_weights)\r\n                # Update the weights of our model\r\n                optimizer.apply_gradients(zip(gradients, mapper.trainable_weights))\r\n                return loss, metrics\r\n\r\n        @tf.function(input_signature=[train_iter.element_spec])\r\n        def distributed_train_step(x, y):\r\n            per_example_losses, _ = mirrored_strategy.experimental_run_v2(\r\n                train_step, args=(x, y)\r\n            )\r\n            return mirrored_strategy.reduce(\r\n                tf.distribute.ReduceOp.SUM,\r\n                per_example_losses,\r\n                axis=None\r\n            )\r\n\r\n        def distributed_training(steps):\r\n            total_loss = 0.0\r\n            num_train_batches = 0.0\r\n            for _ in range(steps):\r\n                x, y = next(train_iter)\r\n                x = encoder(x)  #  would this be a problem?\r\n                total_loss += distributed_train_step(mix, targets)\r\n                ckpt.step.assign_add(1)\r\n                num_train_batches += 1\r\n\r\n            return total_loss, num_train_batches\r\n```\r\n\r\n__Update__: removing the `encoder`, thus making it part of the model does not yield into a crash and even works for `2.1.0` stable. That being said, I would add the following question:\r\n\r\n* Is it possible (and are there any caveats) to have/overwrite an input signature that does not match with dataset.element_spec so that it works in the case mentioned above? (Of course, using variable shape tensors as in the example by @mcoolsce)", "This is still not working even in TF-2.2.0 and 2.3 nightlly gpu versions. Any thoughts on this?\r\n@goldiegadde @anj-s ", "@s4sarath please provide code to repro your issue (either here or in a new bug).", "Hi @guptapriya . Thanks for the quick response.  I have tested this both on TF 2.2.0 GPU and TF 2.3 nightly build also . On colab.\r\n\r\n**a.) Using MirroredStrategy experimental distribute ( TF 2.2.0 )**\r\n\r\n```\r\nds = strategy.experimental_distribute_dataset(train_dataset)\r\ndef train_no_tracing(ds):\r\n  @tf.function(input_signature=[ds.element_spec])\r\n  def step_fn(inputs):\r\n    # train the model with inputs\r\n    return inputs\r\n  for index,batch in enumerate(ds):\r\n    replica_results = strategy.run(step_fn, args=(batch,))\r\n    print(index)\r\n    if index == 11:\r\n      break\r\n\r\ntrain_no_tracing(ds)\r\n```\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-45-da85827572ff> in <module>()\r\n----> 1 train_no_tracing(ds)\r\n\r\n12 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nTypeError: in user code:\r\n\r\n\r\n    TypeError: tf___call_for_each_replica() missing 1 required positional argument: 'kwargs'\r\n\r\n**b.) Same code in  TF nightly build '2.3.0-dev20200605'** \r\n\r\nNow error is somewhat similar not exact\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-6-679d965926a5> in <module>()\r\n     12       break\r\n     13 \r\n---> 14 train_no_tracing(ds)\r\n\r\n13 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_run.py:96 call_for_each_replica  *\r\n        return _call_for_each_replica(strategy, fn, args, kwargs)\r\n\r\n    TypeError: tf__step_fn() takes 1 positional argument but 2 were given\r\n\r\n\r\n**c.) Using experimental_distribute_from_dataset_function**\r\n\r\n```\r\ndef get_dataset_fn():\r\n  \"\"\"Gets a closure to create a dataset..\"\"\"\r\n\r\n  def _dataset_fn(input_context=None):\r\n    \"\"\"Returns tf.data.Dataset for distributed BERT pretraining.\"\"\"\r\n    batch_size = input_context.get_per_replica_batch_size(\r\n                    BATCH_SIZE)\r\n    print(\"batch_size\", batch_size, input_context)\r\n    print('---------------------------------------')\r\n    d = train_dataset\r\n    return d\r\n\r\n  return _dataset_fn\r\n\r\ninput_fn = get_dataset_fn()\r\nds = iter(strategy.experimental_distribute_datasets_from_function(input_fn))\r\n\r\n\r\n# Lets run a simple iteration\r\n\r\ndef train_with_tracing(ds):\r\n  @tf.function(input_signature=None)\r\n  def step_fn(inputs):\r\n    # train the model with inputs\r\n    return inputs\r\n  # epochs\r\n  for index in range(1000):\r\n    replica_results = strategy.run(step_fn, args=(next(ds),))\r\n    print(index)\r\n    if index == 11:\r\n      break\r\n\r\ntrain_no_tracing(ds)\r\n\r\n```\r\n\r\n    TypeError: tf___call_for_each_replica() missing 1 required positional argument: 'kwargs'\r\n\r\nHere is the gist for TF 2.2.0 \r\n\r\nhttps://colab.research.google.com/gist/s4sarath/899d93bd5381efd34dffe9f89c2688a6/dataset_gpu.ipynb\r\n\r\n\r\nHere is the gist for TF 2.3 nightly\r\n\r\nhttps://colab.research.google.com/gist/s4sarath/f42fece3e4f217ee3c0b9a5873223160/dataset_gpu_nightly.ipynb", "To give some idea about dataset, it is having same batch_size, but different inupt length in each batch. ( dynamic input , to avoid unnecessary padding ) . Thats what tracing is precisely using for I guess . ", "thanks @s4sarath. @anj-s could you take a look at the attached colabs? This looks like a bug to me - it could likely be due to how we handle `fn` passed to `run` that are annotated with tf.functions. \r\n\r\n@s4sarath one thing you could try right now is to put the call to `run` inside a tf.function and add the input signature to that. See this unit test here: \r\nhttps://github.com/tensorflow/tensorflow/blob/a00daa2f37954ed7d1fae09dfad81b3168b76715/tensorflow/python/distribute/input_lib_type_spec_test.py#L184"]}, {"number": 29910, "title": "Android performance: CPU affinity", "body": "**System information**\r\n- Mobile device: **Xiaomi A2 (8 core)**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **master** at [6cf83ea] (2019/06/17)\r\n\r\n**Describe the current behavior**\r\n\r\nThe `benchmark_model` that we run on the device performs very differently when we choose CPU affinity. E.g. average inference time (32 bit, CPU only) for our custom .tflite model is 172ms for taskset=ff, 143ms for taskset=f0, and 281ms taskset=0f. We used num_threads=8, i.e. much more threads that we have cores available. When we increase num_threads=12, inference time further decreases. The change of min times is even more impressive: 125ms for ff, 114ms for f0, and 211ms for 0f. And 105ms for f0 with 12 threads, i.e. 3 threads per CPU. Further increasing num_threads does not deliver visible improvements.\r\n\r\nNote that we have no control on the actual number of threads used by interpreter->Invoke(), and their CPU affinity. \r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect the _interpreter_ to choose the optimal threading model. The [document](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#reducing-variance-between-runs-on-android) reads:\r\n\r\n> Reducing variance between runs on Android.\r\n> When running benchmarks on these phones there can be significant variance between different runs of the benchmark.\r\n\r\nWhile benchmarks are nice, our real necessity is to run TFLite optimally in our app. But we cannot control thread CPU affinity or the _interpreter_?\r\n\r\nBut even if we could setup the _interpreter_ threading configuration beyond the generic _\"Set the number of threads available to the interpreter\"_,  across multiple devices, choosing the optimal taskset is beyond the capabilities of most development teams. This cannot be done by analyzing the cpuinfo: on our A2 development phone, all 8 cores are declared [almost](https://github.com/tensorflow/tensorflow/issues/29910#issuecomment-503589179) equal:\r\n\r\n```\r\nProcessor       : AArch64 Processor rev 4 (aarch64)\r\nprocessor       : 0\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x801\r\nCPU revision    : 4\r\n\r\nprocessor       : 1\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x801\r\nCPU revision    : 4\r\n\r\nprocessor       : 2\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x801\r\nCPU revision    : 4\r\n\r\nprocessor       : 3\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x801\r\nCPU revision    : 4\r\n\r\nprocessor       : 4\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x800\r\nCPU revision    : 2\r\n\r\nprocessor       : 5\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x800\r\nCPU revision    : 2\r\n\r\nprocessor       : 6\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x800\r\nCPU revision    : 2\r\n\r\nprocessor       : 7\r\nBogoMIPS        : 38.40\r\nFeatures        : fp asimd evtstrm aes pmull sha1 sha2 crc32\r\nCPU implementer : 0x51\r\nCPU architecture: 8\r\nCPU variant     : 0xa\r\nCPU part        : 0x800\r\nCPU revision    : 2\r\n\r\nHardware        : Qualcomm Technologies, Inc SDM660\r\n\r\n```\r\nMaybe I am missing something, but I see here no clue that the performance of **f0** will be so much different from **0f**.\r\n\r\nThe dynamic optimization should be performed once in a while, because the load on the device may change, either in the same process, or because of other processes/apps running along our app.\r\n\r\nIt could be useful to persist these findings, so that next time interpreter starts, it could have a reasonable starting point. I am not sure if this info is relevant for specific .tflite model, or for any model. In our experiments, the we only used similar FCNN networks, and their performance was effected by taskset just the same.", "comments": ["If you check `cpuinfo` carefully, you can find that processors 0-3 and processor 4-7 are different.\r\nFor 0-3,\r\n```\r\nCPU part        : 0x801\r\nCPU revision    : 4\r\n```\r\nFor 4-7,\r\n```\r\nCPU part        : 0x800\r\nCPU revision    : 2\r\n```\r\n\r\nModern cell phone SoCs tend to have 2 or more clusters. All processors use the same ISA, but different cluster may have different properties, e.g., one cluster for power-efficiency and the other for computation performance. This is what called [big.LITTLE](https://www.arm.com/why-arm/technologies/big-little) by ARM.", "Ok, so without running the benchmark on a specific device, we can know how to choose the performant CPU mask. But what is the recommended way to apply this when it happens in our Android app?", "Nope, not all devices are created equal. There can be more than 2 clusters and there are different kinds of big and LITTLE cores. There could be different scheduling and cpufreq policies. It's complicated. I guess there is no general recommended way to choose number of threads that can be applied to all phones.", "I guess I didn't ask the right question. Let me try again. Even if I know the optimal settings for this specific device, how can I regulate CPU affinity of inference threads?", "in Java? as far as I can remember, there is not such API. In C, the `sched_setaffinity(2)` system call should work.", "So, essentially you suggest to run TFLite in a dedicated process. This may be a very cool way to set it up, but IPC overhead may outweigh performance gains.\r\n\r\n**Update**:  as an afterthought, we can set cpu affinity for the whole application, no need to spawn TFLite into a separate process. More adventurous developers can even require a separate process for some Service or Provider in their manifest, which may improve overall performance even further if chosen optimally.", "One thing to note is that is that our legacy, multi-threaded floating point kernels are notoriously sensitive to unpinned execution, and generally work poorly with Android's thread scheduler.\r\n\r\nWe've been working on a [new gemm backend](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/BUILD#L193) which largely resolves this, and works *much* better with the native Android scheduler. If you build `benchmark_model` from head, and build against arm64 (`--config=android_arm64`), you should see this when running benchmarks both from the command-line as well as in your app (particularly when using multi-threading). We're preparing a blog post about this new backend.", "> One thing to note is that is that our legacy, multi-threaded floating point kernels are notoriously sensitive to unpinned execution, and generally work poorly with Android's thread scheduler.\r\n> \r\n> We've been working on a [new gemm backend](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/BUILD#L193) which largely resolves this, and works _much_ better with the native Android scheduler. If you build `benchmark_model` from head, and build against arm64 (`--config=android_arm64`), you should see this when running benchmarks both from the command-line as well as in your app (particularly when using multi-threading). We're preparing a blog post about this new backend.\r\n\r\n@jdduke I appreciate it if you share the link to the blog post. ", "Hey @arashb, the blog post itself has been in flux, though we've highlighted the work a bit in recent talks, e.g., at [TFWorld](https://youtu.be/0SpZy7iouFU?t=627).  We're planning a more comprehensive performance blog post soon.", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29910\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29910\">No</a>\n"]}, {"number": 29909, "title": "tf.meshgrid high variance in computational time", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: --\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.10\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): ----\r\n- GCC/Compiler version (if compiling from source): ----\r\n- CUDA/cuDNN version: CUDA 9.0, Cudnn, 7.1\r\n- GPU model and memory: GTX 1080 ti, 10GB\r\n\r\n**Describe the current behavior**\r\nRunning ``tf.meshgrid(tf.range(A), tf.range(B), tf.range(C), tf.range(D), indexing='ij')`` with the same amount of resulting elements can have high variations, for example sometimes 1 second, sometimes up to 8 seconds).\r\n\r\n**Describe the expected behavior**\r\nI expect this operation to be quicker, as (AFAIK) it is needed for any complex operations involving tf.scatter_nd and other element gathering ops.\r\n\r\n**Code to reproduce the issue**\r\nRun this code, ``a_s`` and ``b_s`` are set to always multiply to 1000. I see performance going from 0.9 seconds up to 45 seconds on this test, even for the same values.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\nA = tf.placeholder(dtype=tf.int32)\r\nB = tf.placeholder(dtype=tf.int32)\r\nC = 20\r\nD = 5000\r\n\r\nwith tf.device('/gpu:0'):\r\n  ii, bb, kk, _ = tf.meshgrid(tf.range(A), tf.range(B), tf.range(C), tf.range(D), indexing='ij')\r\n\r\nsess = tf.Session()\r\nwith sess.as_default():\r\n\r\n    times = []\r\n\r\n    print(\"Starting Testing\")\r\n\r\n    for d in range(100):\r\n      time_start = time.time()\r\n      a_s = np.random.randint(20, 60)\r\n      b_s = 1000 // a_s\r\n      _, _, _ = sess.run(fetches=[ii, bb, kk], feed_dict={A: a_s,\r\n                                                          B: b_s})\r\n      times.append(time.time() - time_start)\r\n      print(str(times[-1]) + \" \" + str(a_s) + \" \" + str(b_s))\r\n\r\n    print(\"--- Finished ---\")\r\n    print(\"Max time: \" + str(max(times)))\r\n    print(\"Min time: \" + str(min(times)))\r\n    print(\"Avg time: \" + str(sum(times)/len(times)))\r\n```\r\n\r\n**Other info / logs**\r\nHere's an example log extract from the program above (the first value is the time, the second and third the values of A and B respectively):\r\n```\r\nStarting Testing\r\n1.203284502029419 40 25\r\n1.083801031112671 51 19\r\n1.024698257446289 51 19\r\n1.0535898208618164 42 23\r\n1.0363869667053223 56 17\r\n1.1064929962158203 30 33\r\n1.0818214416503906 59 16\r\n1.1742267608642578 43 23\r\n1.172206163406372 31 32\r\n1.2406136989593506 32 31\r\n1.1510748863220215 38 26\r\n1.073162317276001 22 45\r\n1.0849990844726562 55 18\r\n1.0694386959075928 33 30\r\n1.0990991592407227 20 50\r\n1.2687609195709229 24 41\r\n1.1231935024261475 50 20\r\n2.591834783554077 49 20\r\n1.5073127746582031 35 28\r\n1.062272548675537 26 38\r\n2.6493754386901855 27 37\r\n1.137765645980835 30 33\r\n1.0350470542907715 59 16\r\n1.0511572360992432 25 40\r\n1.0805590152740479 55 18\r\n1.0525894165039062 42 23\r\n2.4179093837738037 53 18\r\n1.0785531997680664 42 23\r\n1.0189356803894043 45 22\r\n7.864070892333984 57 17\r\n```\r\n\r\n\r\nI'm using this in a large system based on the Transformer, where this is used together with tf.scatter_nd. I've attached the corresponding timeline, the meshgrid op is called ``output/rec/output_prob/meshgrid``\r\n[timeline.trace.tar.gz](https://github.com/tensorflow/tensorflow/files/3300969/timeline.trace.tar.gz)\r\n\r\n", "comments": ["@nikita68 Can you upgrade to recent version of TF 1.14. I have attached a [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/f732825a2c757d61bf41fb92b2733b73/tf29909_runtime.ipynb). It shows that it is consistently taking 0.43 sec after warming up (takes 0.6~0.8 sec). Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29909\">No</a>\n"]}, {"number": 29908, "title": "Cannot authenticate in CoLab with tensorfow 2.0.0-beta1", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CoLab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\ngoogle.colab.auth.authenticate_user() will fail with tensorflow 2.0.0-beta1\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n!pip install -q tensorflow==2.0.0-beta1\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n\r\n**Other info / logs**\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-e9280defa4ea> in <module>()\r\n     25 \r\n     26 from google.colab import auth\r\n---> 27 auth.authenticate_user()\r\n     28 \r\n     29 prepared_record_paths = tf.io.gfile.glob(training_data)\r\n\r\n/usr/local/lib/python3.6/dist-packages/google/colab/auth.py in authenticate_user(clear_output)\r\n    154       with tf.compat.v1.Session('grpc://{}'.format(colab_tpu_addr)) as sess:\r\n    155         with open(_get_adc_path()) as auth_info:\r\n--> 156           tf.contrib.cloud.configure_gcs(\r\n    157               sess, credentials=_json.load(auth_info))\r\n    158   if _check_adc():\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'", "comments": ["Was not able to reproduce, but am experiencing a time out:\r\n\r\n<img width=\"531\" alt=\"Screen Shot 2019-06-18 at 11 46 45 PM\" src=\"https://user-images.githubusercontent.com/3712347/59742835-6f47d500-9223-11e9-9312-dce284b6fe09.png\">\r\n", "Thanks for looking at this issue.\r\n\r\nI had opened the issue in the wrong repo as the issue is rather with colab itself. So I opened a new one there:\r\nhttps://github.com/googlecolab/colabtools/issues/602\r\n\r\nTo reproduce the issue you need to activate \"Hardware accelerator: TPU\" in \"Notebook Settings\". Otherwise it works indeed.\r\nI will close this ticket as the issue can be followed in colabtools issues directly."]}, {"number": 29907, "title": "\"Node 'Const' is not unique\" Using C++ api on windows to freeze graph", "body": "I use *FreezeSavedModel* fucntion in source code https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/cc/tools/freeze_saved_model.cc to freeze graph, and use tensorflow::WriteBinaryProto to export pb file. it report an error: \"Node 'Const' is not unique\". Then I check code in *FreezeSavedModel* fucntion, I find out in function *ConvertVariableToConstant*(https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/cc/tools/freeze_saved_model.cc#L149),   after running \r\n```\r\n const_node->set_op(\"Const\");\r\n```\r\nthe op and name and device of const_node all are set to \"Const\". And I check the address of member op_, name_, device_ of const_node, they are all the same.\r\n\r\nSimilarly, if you run`const_node->set_name(variable_node.name());` , the op and name and device of const_node all are set to name of this variable_node.\r\n\r\n**System information**\r\n- Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0\r\n- Bazel version (if compiling from source): bazel 0.15\r\n- CUDA/cuDNN version: 10/7\r\n- GPU: 1050ti\r\n\r\nI use vs2015 update3 to compile tensorflow and add some msvc symbols manually.\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "```\t\r\ntensorflow::SavedModelBundle saved_model_bundle;\r\ntensorflow::MetaGraphDef *meta_graph_def = &(&saved_model_bundle)->meta_graph_def;\r\n*meta_graph_def->mutable_graph_def() = graph_def;\r\nsaved_model_bundle.session->Create(graph_def);\r\nsaved_model_bundle.session->Run({ { \"save/Const\", t } }, {}, { \"save/restore_all\" }, nullptr);\r\ntensorflow::GraphDef frozen_graph_def;\r\nstd::unordered_set<string> inputs;\r\nstd::unordered_set<string> outputs;\r\nTF_CHECK_OK(FreezeSavedModel(saved_model_bundle, &frozen_graph_def,\r\n\t\t&inputs, &outputs));\r\n```\r\nthe definition of FreezeSavedModel is in https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/cc/tools/freeze_saved_model.cc ", "> \r\n> \r\n> ```\r\n> tensorflow::SavedModelBundle saved_model_bundle;\r\n> tensorflow::MetaGraphDef *meta_graph_def = &(&saved_model_bundle)->meta_graph_def;\r\n> *meta_graph_def->mutable_graph_def() = graph_def;\r\n> saved_model_bundle.session->Create(graph_def);\r\n> saved_model_bundle.session->Run({ { \"save/Const\", t } }, {}, { \"save/restore_all\" }, nullptr);\r\n> tensorflow::GraphDef frozen_graph_def;\r\n> std::unordered_set<string> inputs;\r\n> std::unordered_set<string> outputs;\r\n> TF_CHECK_OK(FreezeSavedModel(saved_model_bundle, &frozen_graph_def,\r\n> \t\t&inputs, &outputs));\r\n> ```\r\n> \r\n> the definition of FreezeSavedModel is in https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/cc/tools/freeze_saved_model.cc\r\n\r\nHey, try to edit your freeze_saved_model.cc.\r\nbefore:\r\nvoid ConvertVariableToConstant(const NodeDef& variable_node,\r\n                               const Tensor& variable_value,\r\n                               NodeDef* const_node) {\r\n  const_node->set_name(variable_node.name());\r\n  const_node->set_op(\"Const\");\r\n  (*const_node->mutable_attr())[\"dtype\"] = variable_node.attr().at(\"dtype\");\r\n  variable_value.AsProtoTensorContent(\r\n      (*const_node->mutable_attr())[\"value\"].mutable_tensor());\r\n}\r\nchange:\r\nvoid ConvertVariableToConstant(const NodeDef& variable_node,\r\n                               const Tensor& variable_value,\r\n                               NodeDef* const_node) {\r\n  *const_node = variable_node;\r\n  const_node->clear_attr();\r\n  const_node->set_name(variable_node.name());\r\n  const_node->set_op(\"Const\");\r\n  (*const_node->mutable_attr())[\"dtype\"] = variable_node.attr().at(\"dtype\");\r\n  variable_value.AsProtoTensorContent((*const_node->mutable_attr())[\"value\"].mutable_tensor());\r\n}\r\nIt is worked for me.", "@linyuhui We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. check https://github.com/tensorflow/tensorflow/issues/29907#issuecomment-596036686 Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29907\">No</a>\n", "@linyuhui I am facing similar issue on windows. Were you able to solve the issue?\r\nFor my work I have build Tensorflow from source, and I have added missing protobuf symbols in def_file_filter.py.tpl file, to resolve the protobuf related linking errors.  "]}]