[{"number": 22303, "title": "[xla] Improve validation of Broadcast shape", "body": "If one misreads the semantics of this instruction, it's easy to cause\r\nan out of bounds access into the dimensions here. Add an extra check\r\nto return a proper error to the user rather than crashing in that\r\ncase.\r\n\r\nRef #22130", "comments": ["This should be reviewed by someone on the XLA team. @eliben would you assign someone?", "Thanks for the contribution, Keno. Looks good to me with just one nit - @meheffernan can double check", "@Keno, can u address the last comment, and I will merge the change afterwards. Thanks.", "Yes, I will. Been a bit busy, but I'll push an update shortly.", "I had pushed an update to this PR to address the review. Doesn't look like @tensorflowbutler noticed though."]}, {"number": 22302, "title": "[xrt] Add a simple cc_binary server", "body": "Right now xrt isn't included in any of the standard ways to\r\nspin up a grpc server, making it tough to play with it without\r\nmaking any local modifications to the build system. This patch\r\nadds a small server binary that includes xrt and simply listens\r\non `localhost:8470`.", "comments": ["Michael, can u take look? Thanks.", "@mrry I was expecting to find something like a tensorflow_std_server BUILD rule but can't. Do we not currently supply any build rule for a standalone TF server? Should we add a general build rule, and specialize it for XRT, rather than just building this bespoke XRT server?", "@michaelisard This one maybe?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/e1a32c98210f8ebba42a0397259d948e1433c09e/tensorflow/core/distributed_runtime/rpc/BUILD#L305-L320\r\n\r\nThe most common way to start a server in OSS is by creating a `tf.train.Server` from Python. Currently it only configures a `MasterService` and `WorkerService`, but it wouldn't be too difficult to allow it to start other services as well/instead.", "> The most common way to start a server in OSS is by creating a `tf.train.Server` from Python\r\n\r\nMy primary reason for this is to avoid having to deal with the extra complexity and code of having to go through python when that is easily avoidable. This already loads slow enough in gdb as is ;).", "@mrry do you think it would be reasonable to put grpc_tensorflow_server.cc into a library that Keno could put into his build rule, so that he could avoid adding any C++ here (i.e., stop duplicating the code for main() here)? Or is it better just to duplicate it and leave things alone in the rpc directory?", "Actually I should also check: @keno if you deleted xrt_server.cc and linked against grpc_tensorflow_server.cc in your build rule instead, would your use case work?", "I think that should work, but I don't mind either way. One caveat, the command-line args to grpc_tensorflow_server.cc weren't designed for ease-of-use... just for testing. In particular, there's no simple flag for \"just run it locally and print out what port you chose\" or \"just run it locally on a particular port\" options. It might be desirable to add those :).", "OK, @Keno I would marginally prefer not to add xrt_server.cc because if it gets checked in then it will have to be supported forever, but I don't feel strongly enough to block you making progress if it's too inconvenient for you to switch to using grpc_tensorflow_server.cc. Let me know what you think!", "Sorry for the delay here. I'm happy to use `grpc_tensorflow_server.cc` (and maybe add convenience extras to those as necessary). However, I'm not sure my bazel-foo is sufficient to write those build rules correctly (to have a build option to build that .cc with xrt).", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 22301, "title": "Update broken link to intro on ADAGRAD", "body": "Small edit to documentation on the `tf.train.AdagradOptimizer` which had a broken link in the intro.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 22300, "title": "Can't import tensorflow", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: just tried to import it.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tried 1.10 down to 1.4, none worked, only cpu versions.\r\n- **Python version**: 3.7 (also tried with 3.6)\r\n- **CPU model and memory**: Intel core-i3 6100 - Skylake architecture [Tried on another system with the same OS and even more advanced cpu, still didn't work.]\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n### Describe the problem\r\nCan't import. I double checked everything. From github and stackoverflow issues to whether or not my cpu supports avx instructions (it does) and even enabled intel virutalization option from boot. **(Please consider these lines before tagging this as a duplicate.)**\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"FlowTest.py\", line 15, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["I am inclined to think this is an installation issue.\r\nCould you check `C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\` if it actually has `_pywrap_tensorflow_internal.dll\" ?", "I checked, it wasn't there.\r\n", "Then there is our problem. What command did you use to install?", "Trying `pip install tensorflow` didn't work, then I found [this](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip) which recommends using `pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl`, so I tried to get a little creative and changed the version to 1.10, 1.9 and so on, all resulting in the same problem.", "The URL you used is the macos URL, it definitely wont work.\r\nCould you share the full output of `pip install tensorflow` using pastebin?", "> Collecting tensorflow\r\n> Could not find a version that satisfies the requirement tensorflow (from versions: )\r\n> No matching distribution found for tensorflow\r\n\r\nP.S: [pastebin](https://pastebin.com/NLrrZEPu) if necessary. ", "Are you using 32 bit python by any chance?\r\nCould you share the output of `pip --version`", "No, I'm using 64 bit python. \r\n\r\n> Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32\r\n\r\nAnd as for pip,\r\n\r\n> pip 18.0 from c:\\users\\there\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pip (python 3.7)\r\n", "Ah, it is because this is python 3.7.\r\nThe same package we have for python 3.6 should work fine.\r\nCould you try \r\n```\r\npip install --upgrade https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl\r\n```", "I tried.\r\n> tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl is not a supported wheel on this platform.\r\n\r\nP.S: sorry for my late responses. :D", "I am so sorry, I linked the linux package.\r\nThis is the package you need:\r\nhttps://files.pythonhosted.org/packages/0e/2a/c3fe6035f0a8726e5b210680af3ccaf826f4a64ce7306e57017aba749447/tensorflow-1.10.0-cp36-cp36m-win_amd64.whl\r\n\r\nIf this also does not work could you:\r\n- Download the file\r\n- rename by replacing all 36 fields with 37\r\n- install the renamed whl file, by providing its path to the `pip install` command.\r\n\r\n\r\n", "I did, but it opened another entire can of worms. (I have importlib listed in my pip packages so I'm not entirely sure what's missing)\r\n\r\n`Traceback (most recent call last):\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"FlowTest.py\", line 15, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\there\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`", "Uhhh, guys?", "With python 3.7, the issue may be a duplicate of https://github.com/tensorflow/tensorflow/issues/20517", "I did the same with Python 3.6 and it imports successfully, but now the script doesn't run completely (I'm using the code used by google [here](https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=firststeps-colab&hl=en#scrollTo=9ivCDWnwE2Zx)\r\nSpecifically, this code:\r\n\r\n> import math\r\n> from IPython import display\r\n> from matplotlib import cm, gridspec, pyplot as plt\r\n> import numpy as np\r\n> import pandas as pd\r\n> import tensorflow as tf\r\n> from sklearn import metrics\r\n> from tensorflow.python.data import Dataset\r\n> \r\n> tf.logging.set_verbosity(tf.logging.ERROR)\r\n> pd.options.display.max_rows = 15\r\n> pd.options.display.float_format = '{:.1f}'.format\r\n> california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\r\n> california_housing_dataframe = california_housing_dataframe.reindex(\r\n>     np.random.permutation(california_housing_dataframe.index))\r\n> california_housing_dataframe[\"median_house_value\"] /= 1000.0\r\n> first_feature = california_housing_dataframe[[\"total_rooms\"]]\r\n> feature_columns = [tf.feature_column.numeric_column(\"total_rooms\")]\r\n> targets = california_housing_dataframe[\"median_house_value\"]\r\n> my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\r\n> my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\r\n> linear_regressor = tf.estimator.LinearRegressor(\r\n>     feature_columns=feature_columns,\r\n>     optimizer=my_optimizer\r\n> )\r\n> \r\n> def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\r\n>     features = {key:np.array(value) for key, value in dict(features).items()}\r\n>     ds = Dataset.from_tensor_slices((features, targets))\r\n>     ds = ds.batch(batch_size).repeat(num_epochs)\r\n>     if shuffle:\r\n>         ds = ds.shuffle(buffer_size=1000)\r\n>     \r\n>     features, labels = ds.make_one_shot_iterator().get_next()\r\n>     return features, labels\r\n> \r\n> _ = linear_regressor.train(\r\n>     input_fn = lambda:my_input_fn(first_feature, targets),\r\n>     steps=100\r\n> )\r\n> \r\n> prediction_input_fn = lambda: my_input_fn(first_feature, targets, num_epochs=1, shuffle=False)\r\n> predictions = linear_regressor.predict(input_fn=prediction_input_fn)\r\n> predictions = np.array([item['predictions'][0] for item in predictions])\r\n> mean_squared_error = metrics.mean_squared_error(predictions, targets)\r\n> root_mean_squared_error = math.sqrt(mean_squared_error)\r\n> \r\n\r\nand instead, gives me this error [Tried on both anaconda and outside of it, got the same result] \r\n`2018-09-26 19:52:12.039502: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2`\r\n\r\nI even tried to install it from source so I could config it with my current cpu instructions, but I ran into a problem when I ran this command from powershell. \r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\nGave\r\n`ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@gif_archive//': java.io.IOException: Error downloading [https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz, http://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz] to C:/users/there/_bazel_there/5v74hkvm/external/gif_archive/giflib-5.1.4.tar.gz: All mirrors are down: [GET returned 403 Forbidden]`\r\n\r\nI even tried installing using wheels made by other developers which were compiled to use AVX2, but they also ran into issues and didn't install.", "Hello?\r\nGuys?", "This message is not an error message, it just informs you that TF can run faster on your machine:\r\n```\r\n2018-09-26 19:52:12.039502: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n```\r\nSo if this is the only error you see, TF is running fine on your system.\r\n\r\nAbout the URLs, I manually verified that all of them are accessible and online. It is probably some issue with your internet connection, or a firewall if you are behind one.", "I experienced this issue when I installed tensorflow using conda. Then I uninstalled tensorflow and installed it using pip3 command. The issue has been resolved.", "help same issue\r\n`ImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     17         try:\r\n---> 18             fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n     19         except ImportError:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\imp.py in find_module(name, path)\r\n    296     else:\r\n--> 297         raise ImportError(_ERR_MSG.format(name), name=name)\r\n    298 \r\n\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             import _pywrap_tensorflow_internal\r\n     21             return _pywrap_tensorflow_internal\r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n`", "Hello there, \r\nI'm installing tensorflow on anaconda but I get the compiler error, please help me regarding this issue\r\n\r\nImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-847f01cd9c67> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 a= tf.constant(10)\r\n      3 b= tf.constant(10)\r\n      4 print(sess.run(a+b))\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\w\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\w\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\w\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\w\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\w\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795", "Looks like the issue was resolved?\r\n@theerfan could you confirm?", "Yeah, it was.\r\nMy apologies for forgetting to close the issue.", "Hi, could anyone help with the problem below? :( It'd be a great booster as im really just a newby. Similar or same problem may have shared with the community but im really new with this. \r\n\r\nImportError                               Traceback (most recent call last)\r\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     13         try:\r\n---> 14             return importlib.import_module(mname)\r\n     15         except ImportError:\r\n\r\nd:\\python\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nd:\\python\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nd:\\python\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nd:\\python\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nd:\\python\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nd:\\python\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nd:\\python\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nd:\\python\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     16             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 17     _pywrap_tensorflow_internal = swig_import_helper()\r\n     18     del swig_import_helper\r\n\r\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     15         except ImportError:\r\n---> 16             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     17     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nd:\\python\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-4a3efc4cbb3c> in <module>\r\n      4 import sys\r\n      5 import tarfile\r\n----> 6 import tensorflow as tf\r\n      7 import zipfile\r\n      8 \r\n\r\nd:\\python\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     20 \r\n     21 # pylint: disable=g-bad-import-order\r\n---> 22 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     23 \r\n     24 try:\r\n\r\nd:\\python\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"d:\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"d:\\python\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "Now I am using tensorflow as a backend with Keras.\n\nBest regards,\n\nKhalil Ur Rehman\nStudent at Jiangsu University, China\n\n________________________________\nFrom: azrnzhrl <notifications@github.com>\nSent: Sunday, October 27, 2019 5:43:14 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: khaliltalib277 <khaliluaf@hotmail.com>; Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Can't import tensorflow (#22300)\n\n\nHi, could anyone help with the problem below? :( It'd be a great booster as im really just a newby. Similar or same problem may have shared with the community but im really new with this.\n\nImportError Traceback (most recent call last)\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\n13 try:\n---> 14 return importlib.import_module(mname)\n15 except ImportError:\n\nd:\\python\\lib\\importlib_init_.py in import_module(name, package)\n125 level += 1\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\n127\n\nd:\\python\\lib\\importlib_bootstrap.py in _gcd_import(name, package, level)\n\nd:\\python\\lib\\importlib_bootstrap.py in find_and_load(name, import)\n\nd:\\python\\lib\\importlib_bootstrap.py in find_and_load_unlocked(name, import)\n\nd:\\python\\lib\\importlib_bootstrap.py in _load_unlocked(spec)\n\nd:\\python\\lib\\importlib_bootstrap.py in module_from_spec(spec)\n\nd:\\python\\lib\\importlib_bootstrap_external.py in create_module(self, spec)\n\nd:\\python\\lib\\importlib_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\n\nImportError: DLL load failed: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nModuleNotFoundError Traceback (most recent call last)\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in\n57\n---> 58 from tensorflow.python.pywrap_tensorflow_internal import *\n59 from tensorflow.python.pywrap_tensorflow_internal import version\n\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in\n16 return importlib.import_module('_pywrap_tensorflow_internal')\n---> 17 _pywrap_tensorflow_internal = swig_import_helper()\n18 del swig_import_helper\n\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\n15 except ImportError:\n---> 16 return importlib.import_module('_pywrap_tensorflow_internal')\n17 _pywrap_tensorflow_internal = swig_import_helper()\n\nd:\\python\\lib\\importlib_init_.py in import_module(name, package)\n125 level += 1\n--> 126 return _bootstrap._gcd_import(name[level:], package, level)\n127\n\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\nDuring handling of the above exception, another exception occurred:\n\nImportError Traceback (most recent call last)\nin\n4 import sys\n5 import tarfile\n----> 6 import tensorflow as tf\n7 import zipfile\n8\n\nd:\\python\\lib\\site-packages\\tensorflow_init_.py in\n20\n21 # pylint: disable=g-bad-import-order\n---> 22 from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import\n23\n24 try:\n\nd:\\python\\lib\\site-packages\\tensorflow\\python_init_.py in\n47 import numpy as np\n48\n---> 49 from tensorflow.python import pywrap_tensorflow\n50\n51 # Protocol buffers\n\nd:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in\n72 for some common reasons and solutions. Include the entire stack trace\n73 above this error message when asking for help.\"\"\" % traceback.format_exc()\n---> 74 raise ImportError(msg)\n75\n76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\n\nImportError: Traceback (most recent call last):\nFile \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\nreturn importlib.import_module(mname)\nFile \"d:\\python\\lib\\importlib_init_.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"\", line 994, in _gcd_import\nFile \"\", line 971, in _find_and_load\nFile \"\", line 955, in _find_and_load_unlocked\nFile \"\", line 658, in _load_unlocked\nFile \"\", line 571, in module_from_spec\nFile \"\", line 922, in create_module\nFile \"\", line 219, in _call_with_frames_removed\nImportError: DLL load failed: The specified module could not be found.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\nFile \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in\nfrom tensorflow.python.pywrap_tensorflow_internal import *\nFile \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in\n_pywrap_tensorflow_internal = swig_import_helper()\nFile \"d:\\python\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\nreturn importlib.import_module('pywrap_tensorflow_internal')\nFile \"d:\\python\\lib\\importlib_init.py\", line 126, in import_module\nreturn _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\n\nFailed to load the native TensorFlow runtime.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/22300?email_source=notifications&email_token=AGK4XPYUZ7EHXE7WM4UL3D3QQVPDFA5CNFSM4FVLZ222YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOECK2FSY#issuecomment-546677451>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AGK4XP2CU5EHP2DMPJT5BLLQQVPDFANCNFSM4FVLZ22Q>.\n", "Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Robuca\\Anaconda3\\envs\\finalthesis\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Robuca\\Anaconda3\\envs\\finalthesis\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 59, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\Robuca\\Anaconda3\\envs\\finalthesis\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n  File \"C:\\Users\\Robuca\\Anaconda3\\envs\\finalthesis\\lib\\site-packages\\tensorflow\\core\\framework\\node_def_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"C:\\Users\\Robuca\\Anaconda3\\envs\\finalthesis\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"C:\\Users\\Robuca\\Anaconda3\\envs\\finalthesis\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"C:\\Users\\Robuca\\Anaconda3\\envs\\finalthesis\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 91, in <module>\r\n    __module__ = 'tensorflow.core.framework.resource_handle_pb2'\r\nTypeError: expected bytes, Descriptor found\r\n\r\n\r\n\r\n\r\n\r\nplsss help me to fix this..\r\nIm using tensorflow cpu, I dont have graphics card.. ", "I had the same issue and for me the problem was in python and tensorflow version compatibility.\r\nTensorflow didn't work with python 3.8 , So when I downgraded it to Python 3.5, It worked like a charm.", "Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\ASLAM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\ASLAM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\ASLAM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\ASLAM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n\r\n\r\npls help anyone\r\n", "Locking as resolved. See the rest of the comment and if that doesn't help please open a new issue, filling in the issue template\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204"]}, {"number": 22299, "title": "Contributing to tf from windows environment", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:1.8\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0/7.1\r\n- **GPU model and memory**: Nvidia 1050 Ti, 4GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI would like to contribute to tensorflow, however, I have not found any information on how to do it\r\non windows in Visual Studio 2017? Just building the code(apparently cmake is depricated?) and running it in windows? I have installed TF with GPU support ,CUDA, cuDNN as explained here: https://www.tensorflow.org/install/install_windows\r\nBut this is only for running the library? What if I, for instance, want to port a gradient from python to C++, rebuild the code and then run it and see if that particular gradient works, on Windows? I would just like to contribute to tf in the easiest way possible, from Windows.\r\n", "comments": []}, {"number": 22297, "title": "shared_embedding_columns  with partitioner  is  ERROR", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.10.1\r\n- **Python version**:3.5.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:NO\r\n- **GPU model and memory**:NO\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen use shared_embedding_columns without partitioner  to save model ,then import_meta_graph from check_point is success. But use shared_embedding_columns with partitioner, then import_meta_graph from check_point is failed.   \r\nReport error: KeyError: \r\nThe name 'run_2/input_from_feature_columns/a1_shared_embedding/a1_a1_shared_embedding' refers to an Operation not in the graph\r\n\r\n### Source code / logs\r\n\r\ncode for model\r\nfirst, mkdir mypath/model\r\n````\r\nfrom tensorflow.contrib.layers.python.layers import feature_column as fc\r\nfrom tensorflow.python.ops import variable_scope\r\nfrom tensorflow.python.framework import sparse_tensor as sparse_tensor_lib\r\nfrom tensorflow.contrib.layers.python.layers import feature_column_ops\r\nfrom tensorflow.python.training import saver\r\nimport tensorflow as tf\r\n\r\ndef mySharedEmbeddingColumn():\r\n    a1 = fc.sparse_column_with_keys(\"a1\", [\"marlo\", \"omar\", \"stringer\"])\r\n    b = fc.shared_embedding_columns([a1, a1], dimension=4, combiner=\"mean\")\r\n    c = fc.embedding_column(a1, dimension=4, combiner=\"mean\")\r\n    input_tensor_c1 = sparse_tensor_lib.SparseTensor(\r\n        indices=[[0, 0], [1, 1], [2, 2]], values=[0, 1, 2], dense_shape=[3, 3])\r\n    dnn_partitioner = (\r\n        tf.min_max_variable_partitioner(\r\n        max_partitions=1))\r\n    with variable_scope.variable_scope(\"run_2\",partitioner=dnn_partitioner):\r\n      b2 = feature_column_ops.input_from_feature_columns({\r\n          b[1]: input_tensor_c1\r\n      }, [b[1]])\r\n    with variable_scope.variable_scope(\"run_3\", partitioner=dnn_partitioner):\r\n      c1 = feature_column_ops.input_from_feature_columns({\r\n          c: input_tensor_c1\r\n      }, [c])\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        variable_names = [v.name for v in tf.trainable_variables()]\r\n        values = sess.run(variable_names)\r\n        for k,v in zip(variable_names, values):\r\n            print(\"Variable: \", k)\r\n            print(\"Shape: \", v.shape)\r\n            print(v)\r\n        saver.save(sess,'model/model-ckt')\r\n\r\nmySharedEmbeddingColumn()\r\n````\r\ncode for restore\r\n````\r\nimport tensorflow as tf\r\n\r\ngraph = tf.Graph()\r\nsess = tf.Session(graph=graph)\r\nwith graph.as_default():\r\n    del tf.get_collection_ref(tf.GraphKeys.TRAIN_OP)[:]\r\n    check_point_path = 'model' \r\n    saver = tf.train.import_meta_graph('model/model-ckt.meta')\r\n    print(graph.get_operations())\r\n\r\n````\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "updated, please help me to solve this problerm ,thank u.", "Yeah this comes up if there's a PartitionedVariable in a collection without a read op. There was a fix in https://github.com/tensorflow/tensorflow/commit/b23df6e50255991b06cc8d6596b1075c3bd3f7e9. Looks like it'll be in 1.11."]}, {"number": 22296, "title": "Raise warning once only if trainable_weights and _collected_trainable_weights are inconsistent", "body": "Fix #22012\r\nIf ```trainable_weights``` and ```_collected_trainable_weights``` are inconsistent, raise warning only once, not every batch(```train_on_batch```) which would flood the screen. ```train_on_batch``` inside loop is not efficient as each batch would rebuild the ```train_function```, but it's the most simple way to train model like GAN in #22012 . This may not a perfect fix, but at least, don't flood my screen. The updated behavior would be the same as ```keras-team/keras```.", "comments": ["@fchollet It seems ```tf.logging``` only suppress warning if you explicitly call ```log_first_n```. Thanks.", "Nagging Assignee @facaiy: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22295, "title": "Allow grads to be scaled by tensors in tf.contrib.layers.optimize_loss()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**: TitanX Pascal\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\n`tf.contrib.layers.optimize_loss()` currently allows gradient multipliers by passing a `gradient_multipliers` dictionary of `{variable_name: multiplier}` where `multiplier` is a python float. It would be useful to allow `multiplier` to be tensor so that gradient scaling could be, for example, tied to global_step.\r\n\r\nIt seems like this could easily be implemented  by not pushing the multiplier to a constant in `_multiply_gradients` function at https://github.com/tensorflow/tensorflow/blob/e37baff90ed84faef679bbf1e785e6790fd2739d/tensorflow/contrib/layers/python/layers/optimizers.py#L436-L437\r\n\r\nIs there a reason why the dict shouldn't contain tensors that I'm overlooking?", "comments": ["Added a PR #22350 for tensor support.", "thank you!"]}, {"number": 22294, "title": "Golang: could not determine kind of name for C.TF_UINT64", "body": "I do have a issue with building my golang app. I'm getting this error while compiling my application. \r\n\r\ngithub.com/donutloop/imagedetection/vendor/github.com/tensorflow/tensorflow/tensorflow/go\r\nvendor/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:42:24: could not determine kind of name for C.TF_UINT32\r\nvendor/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:50:24: could not determine kind of name for C.TF_UINT64\r\n\r\ngo env\r\nGOARCH=\"amd64\"\r\nGOBIN=\"\"\r\nGOCACHE=\"/root/.cache/go-build\"\r\nGOEXE=\"\"\r\nGOFLAGS=\"\"\r\nGOHOSTARCH=\"amd64\"\r\nGOHOSTOS=\"linux\"\r\nGOOS=\"linux\"\r\nGOPATH=\"/go\"\r\nGOPROXY=\"\"\r\nGORACE=\"\"\r\nGOROOT=\"/usr/local/go\"\r\nGOTMPDIR=\"\"\r\nGOTOOLDIR=\"/usr/local/go/pkg/tool/linux_amd64\"\r\nGCCGO=\"gccgo\"\r\nCC=\"gcc\"\r\nCXX=\"g++\"\r\nCGO_ENABLED=\"1\"\r\nGOMOD=\"\"\r\nCGO_CFLAGS=\"-g -O2\"\r\nCGO_CPPFLAGS=\"\"\r\nCGO_CXXFLAGS=\"-g -O2\"\r\nCGO_FFLAGS=\"-g -O2\"\r\nCGO_LDFLAGS=\"-g -O2\"\r\nPKG_CONFIG=\"pkg-config\"\r\nGOGCCFLAGS=\"-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build290707876=/tmp/go-build -gno-record-gcc-switches\"\r\n\r\ngolang dep\r\n[[constraint]]\r\n  name = \"github.com/tensorflow/tensorflow\"\r\n  version = \"1.10.1\"\r\n\r\n", "comments": ["Fixed it", "@donutloop How did you fix it, I am having same issue", "i didn't use the right version of \"tensorflow/tensorflow/tensorflow/go\" for the tensorflow engine, that means binding and engine wasn't compatible with each other. Please check if you are using a correct binding version for your installed tensorflow c library ", "> \r\n\r\nprincesegzy01 did not ask you what was the problem, but hot to fix it.", "@zmajew version mismatch between client library and Tensorflow engine. Check that both version are compatible with each other."]}, {"number": 22293, "title": "'_TensorLike'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@cqfindingmyself  Please provide your system information and describe your problem in detail. It will help us to guide you better.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22292, "title": "Docker with python 3.6", "body": "Please update your [nvidia docker file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/nvidia.Dockerfile) to support python 3.6.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: tried to take relevant parts from docker file, but I get timeouts.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 (yes has weird python 3.5.3 dependency, but doesn't mean you can't make python3.6 docker image)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a\r\n- **TensorFlow installed from (source or binary)**: trying to use tensorflow-gpu>=1.10\r\n- **TensorFlow version (use command below)**: 1.10+\r\n- **Python version**: 3.6+ \r\n- **Bazel version (if compiling from source)**: N/a\r\n- **GCC/Compiler version (if compiling from source)**: N/a\r\n- **CUDA/cuDNN version**: nvidia/cuda:9.0-base-ubuntu16.04\r\n- **GPU model and memory**: Titan\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nFROM nvidia/cuda:9.0-base-ubuntu16.04\r\nFROM python:3.6-slim\r\nRUN apt-get update && apt-get install \\\r\n        cuda-command-line-tools-9-0 \\\r\n        libcudnn7=7.2.1.38-1+cuda9.0 \\\r\n        libnccl2=2.2.13-1+cuda9.0\r\n...\r\nRUN pip install tensorflow-gpu==1.10.0\r\n```\r\n\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nPython 3.6 introduces some non-backwards compatible features - most notably f-strings. Any code with an f-string anywhere will not work with any existing official nvidia or tensorflow docker image\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Second this. I was also wondering if there's a plan for when the docker image is going to move to ubuntu 18.04 as a base? I assume that would fix this problem at the same time.", "To clarify, I mean that taking the NVIDIA partial of the TF docker file:\r\n\r\n```\r\n# Pick up some TF dependencies                       \r\n\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\                       \r\n\r\n        build-essential \\                       \r\n\r\n        cuda-command-line-tools-9-0 \\                       \r\n\r\n        cuda-cublas-9-0 \\                       \r\n\r\n        cuda-cufft-9-0 \\                       \r\n\r\n        cuda-curand-9-0 \\                       \r\n\r\n        cuda-cusolver-9-0 \\                       \r\n\r\n        cuda-cusparse-9-0 \\                       \r\n\r\n        libcudnn7=7.2.1.38-1+cuda9.0 \\                       \r\n\r\n        libnccl2=2.2.13-1+cuda9.0 \\                       \r\n\r\n        libfreetype6-dev \\                       \r\n\r\n        libhdf5-serial-dev \\                       \r\n\r\n        libpng12-dev \\                       \r\n\r\n        libzmq3-dev \\                       \r\n\r\n        pkg-config \\                       \r\n\r\n        software-properties-common \\                       \r\n\r\n        unzip \\                       \r\n\r\n        && \\                       \r\n\r\n    apt-get clean && \\                       \r\n\r\n    rm -rf /var/lib/apt/lists/*                       \r\n\r\nRUN apt-get update && \\                       \r\n\r\n        apt-get install nvinfer-runtime-trt-repo-ubuntu1604-4.0.1-ga-cuda9.0 && \\                       \r\n\r\n        apt-get update && \\                       \r\n\r\n        apt-get install libnvinfer4=4.1.2-1+cuda9.0\r\n```\r\n\r\nresults in errors (unable to locate package), timeout or the like. So even if I wanted to make the python3.6 version myself, I can not :) \r\n\r\nTrying to use `FROM tensorflow/tensorflow` and `python3` will not work without a virtual environment... as non docker guru, I think many in the community would appreciate it\r\n\r\n@NegatioN that is likely to happen, but many people will still need 16.04LTS + py3.6", "+1 for a re-buildable, customisable Dockerfile with:\r\n\r\n- cuda\r\n- tf-deps\r\n- python 3.6.", "+1 Would appreciate tensorflow maintained docker images with python 3.6 and ubuntu 18.04 (if it has to be ubuntu, a smaller image would be nice).\r\n\r\nIsn't this achievable by upping the ubuntu version to 18.04 in [spec.yml](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/spec.yml)?", "Same issue here!", "@agrinh it is possible with ubuntu 18.04, but again, there will still be a fraction of the community requiring ubuntu 16.04.\r\n\r\neither way, I think we would all appreciate if @angersson  could give us an update", "btw until we wait `3.7.1` is already there ;) \u23f3 . Could we skip 3.6 and migrate to the last one?", "@angersson can we please get a \"working on it\" or an eta.... ", "See https://github.com/tensorflow/tensorflow/pull/23066 for our status on migrating to 18.04. Currently the answer seems to be \"We're not sure, maybe\".\r\n\r\nI am working on updating and making the Dockerfiles in tools/dockerfiles (that is, the new ones) into the official images, but I don't expect to be done before the end of November. Part of that work will be checking to see what versions of everything the files can use. We've had lots of issues before with new versions of Python and Ubuntu causing breakages, which is why we aren't eager to jump to new versions without being very cautious.\r\n\r\n", "Did anything come up with this ? @angersson ", "@maystroh See https://github.com/tensorflow/tensorflow/pull/24051. Also, I think the nightly CPU images are based on Ubuntu 18.04 now, so those should have a newer Python version.\r\n", "The GPU images are locked to Ubuntu 16 until TF migrates to CUDA 10.", "Thanks @angersson ", "`tensorflow/tensorflow:nightly-gpu-py3` has Cuda 10.0 but Python 3.5.2.\r\n\r\nHow to get an image with Cuda 10.0 and Python >= 3.6?", "Python3.6 offers many cool features, it would be great to have it by default instead of 3.5.", "Seriously, the image tag should explicitly specify Python version up to one decimal point. This has got me lots of headaches when there's an underlying Python version discrepancy between testing and deployment. ", "#28717 will upgrade our future Docker images to Ubuntu 18.04. TF 1.14's images should have 18.04 as a base once they're available. \r\n\r\n@jingw222 Good idea -- that may be something we can do later if it turns out to be simple enough. For now, I'd recommend deriving a custom image from ours if you require specific component versions. ", "This is *not* fixed: the new images indeed now use Ubuntu 18.04, but the installed Python is still Python 3.5.2 (checking `latest-py3`, via `docker run -it tensorflow/tensorflow:latest-py3 python3`).", "@minimaxir, that is correct. Starting from TF 1.14 our images will be based on Ubuntu 18.04. Since 1.14's release process has been delayed, the `latest` tags still point to the `1.13`-tagged images based on Ubuntu 16.04. The `nightly` tags have jumped into 18.04 with Python 3.6:\r\n\r\n```\r\n$ docker run -it --rm tensorflow/tensorflow:nightly-py3 python3 -V \r\nPython 3.6.7\r\n```", "I created an image that uses python 3.6 and has tensorflow 2.0a-gpu installed. \r\n\r\n`$ docker pull patientzero/tensorflow2.0-gpu-py3.6`\r\n\r\nThe repo is here: https://github.com/patientzero/tensorflow2.0-python3.6-Docker\r\n\r\nedit: changed to general image. now with 2.0beta support", "@patientzero Thanks! Please note also that our official `2.0.0b0` suite of images (e.g. `tensorflow/tensorflow:2.0.0b0-gpu-py3`), published this morning with the release of the 2.0 beta, also contain Python 3.6. "]}, {"number": 22291, "title": "vgg16  transfer learning will be error \"TypeError: provided list of inputs contains objects other than 'EagerTensor'\"", "body": "\r\n### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.11.0 (use tf-night-gpu)\r\nPython version:2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 9.0 /7.1\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\n### Describe the problem\r\nvgg16  used to `transfer learning ` and data `cocodataset 2014`  and it will be error `TypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor`  but use` InceptionV3` it is can work. \r\n\r\n### Source code / logs\r\nthis is my code.  the cocodataset need to download 3 hours,  \r\nif vgg16  change to InceptionV3, it is can work\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\n# We'll generate plots of attention in order to see which parts of an image\r\n# our model focuses on during captioning\r\nimport matplotlib.pyplot as plt\r\n#import inception_v4 \r\n# Scikit-learn includes many helpful utilities\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.utils import shuffle\r\nimport time\r\nimport re\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport json\r\nfrom glob import glob\r\nfrom PIL import Image\r\nimport pickle\r\n\r\nannotation_zip = tf.keras.utils.get_file('captions.zip', \r\n                                          cache_subdir=os.path.abspath('.'),\r\n                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\r\n                                          extract = True)\r\nannotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\r\n\r\nname_of_zip = 'train2014.zip'\r\nif not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\r\n    image_zip = tf.keras.utils.get_file(name_of_zip, \r\n                                      cache_subdir=os.path.abspath('.'),\r\n                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\r\n                                      extract = True)\r\n    PATH = os.path.dirname(image_zip)+'/train2014/'\r\nelse:\r\n    PATH = os.path.abspath('.')+'/train2014/'\r\n\r\n\r\n# read the json file# read  \r\nwith open(annotation_file, 'r') as f:\r\n    annotations = json.load(f)\r\n\r\n# storing the captions and the image name in vectors\r\nall_captions = []\r\nall_img_name_vector = []\r\n\r\nfor annot in annotations['annotations']:\r\n    caption = '<start> ' + annot['caption'] + ' <end>'\r\n    image_id = annot['image_id']\r\n    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\r\n    \r\n    all_img_name_vector.append(full_coco_image_path)\r\n    all_captions.append(caption)\r\n\r\ntrain_captions, img_name_vector = shuffle(all_captions,\r\n                                          all_img_name_vector,\r\n                                          random_state=1)\r\n\r\n# selecting the first 30000 captions from the shuffled set\r\nnum_examples = 30000\r\n\r\ndef load_image(image_path):\r\n    img = tf.read_file(image_path)\r\n    img = tf.image.decode_jpeg(img, channels=3)\r\n    img = tf.image.resize_images(img, (224, 224))\r\n    img = tf.keras.applications.vgg16.preprocess_input(x = img)\r\n    return img, image_path\r\n\r\nimage_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\r\nnew_input = image_model.input\r\nhidden_layer = image_model.layers[-1].output\r\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\r\n\r\nstartTime=time.time()\r\n# getting the unique images\r\nencode_train = sorted(set(img_name_vector))\r\n\r\n# feel free to change the batch_size according to your system configuration\r\nimage_dataset = tf.data.Dataset.from_tensor_slices(\r\n                                encode_train).map(load_image).batch(16)\r\n\r\nfor img, path in image_dataset:\r\n    batch_features_0 = image_features_extract_model(img)\r\n  \r\n    batch_features = tf.reshape(batch_features_0, (batch_features_0.shape[0], -1, batch_features_0.shape[3]))\r\n    Nan = np.any(np.isnan(batch_features))\r\n\r\n    for bf, p in zip(batch_features, path):\r\n\r\n        path_of_feature = p.numpy().decode(\"utf-8\")\r\n        np.save(path_of_feature, bf.numpy())\r\n\r\n### test image\r\nimage = './COCO_val2014_000000000042.jpg'\r\nprint(load_image(image))\r\n```\r\nthe error output\r\n```\r\n_FallbackException                        Traceback (most recent call last)\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   6211         _ctx._context_handle, _ctx._eager_context.device_name, \"Reshape\",\r\n-> 6212         name, _ctx._post_execution_callbacks, tensor, shape)\r\n   6213       return _result\r\n\r\n_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-90810626227a> in <module>()\r\n      1 image = './COCO_val2014_000000000042.jpg'\r\n----> 2 print(type(load_image(image)))\r\n      3 print(len(load_image(image)))\r\n      4 print(load_image(image))\r\n\r\n<ipython-input-4-34a6a48d497c> in load_image(image_path)\r\n      8     #img = tf.keras.applications.inception_v3.preprocess_input(img)\r\n      9     #img = inception_v4.preprocess_input(img)\r\n---> 10     img = tf.keras.applications.vgg16.preprocess_input(x = img)\r\n     11     return img, image_path\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in preprocess_input(x, data_format, mode)\r\n    197     return _preprocess_numpy_input(x, data_format=data_format, mode=mode)\r\n    198   else:\r\n--> 199     return _preprocess_symbolic_input(x, data_format=data_format, mode=mode)\r\n    200 \r\n    201 \r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/imagenet_utils.py in _preprocess_symbolic_input(x, data_format, mode)\r\n    160     x = K.bias_add(x, math_ops.cast(_IMAGENET_MEAN, K.dtype(x)), data_format)\r\n    161   else:\r\n--> 162     x = K.bias_add(x, _IMAGENET_MEAN, data_format)\r\n    163   if std is not None:\r\n    164     x /= std\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in bias_add(x, bias, data_format)\r\n   4480     elif data_format == 'channels_last':\r\n   4481       if len(bias_shape) == 1:\r\n-> 4482         x = x + reshape(bias, (1, 1, bias_shape[0]))\r\n   4483       else:\r\n   4484         x = x + reshape(bias, (1,) + bias_shape)\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py in reshape(x, shape)\r\n   2214       A tensor.\r\n   2215   \"\"\"\r\n-> 2216   return array_ops.reshape(x, shape)\r\n   2217 \r\n   2218 \r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   6214     except _core._FallbackException:\r\n   6215       return reshape_eager_fallback(\r\n-> 6216           tensor, shape, name=name, ctx=_ctx)\r\n   6217     except _core._NotOkStatusException as e:\r\n   6218       if name is not None:\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)\r\n   6233   _attrs = (\"T\", _attr_T, \"Tshape\", _attr_Tshape)\r\n   6234   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n-> 6235                              ctx=_ctx, name=name)\r\n   6236   _execute.record_gradient(\r\n   6237       \"Reshape\", _inputs_flat, _attrs, _result, name)\r\n\r\n/home/ailab/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     59                                                op_name, inputs, attrs,\r\n---> 60                                                num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nTypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor\r\n```\r\n", "comments": ["@akshaym can you look at this? It seems to be related to the convert_to_tensor codepath where the tf.stack thing is getting confused.", "Hi @Q82822, it seems from your stacktrace, the problem is simply calling load_image on any image file, but I'm unable to reproduce it at the moment (I downloaded a single file from the coco dataset locally). \r\n\r\nHere is the code I have: \r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ndef load_image(image_path):\r\n  img = tf.read_file(image_path)\r\n  img = tf.image.decode_jpeg(img, channels=3)\r\n  img = tf.image.resize_images(img, (224, 224))\r\n  img = tf.keras.applications.vgg16.preprocess_input(x = img)\r\n  return img, image_path\r\n\r\nload_image('~/Downloads/coco_example.png')\r\n```\r\n\r\nFrom the error it seems that there is a graph mode tensor involved here somewhere, but reading the code within the tf codebase, I can't find that happens anywhere. Do you have a more isolated reproduction that I can try?\r\n\r\nThanks!", "hi, if you just load image in vgg16 , it is work. but I need to \"transfer learning\" so I download about 30000 images. You can copy my all the code and run. \r\nthere have a weird thing !!! this problem will not happen in inception_v3. \r\n```\r\nimg = tf.keras.applications.vgg16.preprocess_input(img) \r\n==>img = tf.keras.applications.inception_v3.preprocess_input(img\r\n```\r\n```\r\n\r\nimg = tf.image.resize_images(img, (224, 224))\r\n==>img = tf.image.resize_images(img, (299, 299))\r\n```\r\n\r\n```\r\nimage_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet')\r\n==>image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\r\n```\r\nyou just change above 3 codes the problem was solved. but it very weird.  ", "This is a stale issue. I ran the above code (with little modifications as API paths changed little bit) without any issue. Please note that I used most recent TF versions `TF2.5` and `TF2.6`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/80a9e4e5009112420d42e166cdc68cde/untitled.ipynb) is a gist for reference.\r\n\r\nAs this was resolved in recent TF version, I am closing this issue. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 22290, "title": "can tf.FFT deal with images\uff1f", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on [StackOverflow ](https://stackoverflow.com/questions/tagged/tensorflow)since it is not a bug or feature request. There is also a larger community that reads questions there."]}, {"number": 22289, "title": "Fix bug of lacking axis when using array_op.concat in _unwrap_and_concat function", "body": "This is a small bug fix. Array_ops.concat should be always passed with two params. But it seems that the axis was missed in _unwrap_and_concat function.", "comments": ["@yuefengz @tensorflower-gardener  This is a small bug fix."]}, {"number": 22288, "title": "Fix bug of lacking axis when using array_op.concat in _unwrap_and_concat function", "body": "This is a small bug fix. Array_ops.concat should be always passed with two params. But it seems that the `axis` was missed in _unwrap_and_concat function.  ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->"]}, {"number": 22287, "title": "ConvertGraphDefToGraph in graph_construction.cc has a bug", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu1~16.04.10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: 1.10 from source, 1.8 from source\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: Python 2.7.15 |Anaconda, Inc.| (default, May  1 2018, 23:32:55)\r\n- **Bazel version (if compiling from source)**: Build label: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n/* Adding a Test to tensorflow/core/graph/graph_constructor_test.cc\r\nAdd includes at top */\r\n#include \"tensorflow/cc/saved_model/loader.h\"\r\n#include \"tensorflow/cc/tools/freeze_saved_model.h\"\r\n/*This test should download the faster_rcnn_resnet101_kitti, and fail when converting a frozen_graph_def to a graph*/\r\nTEST_F(GraphConstructorTest, FillOp_faster_rcnn) {\r\n FILE *file = fopen(\"/tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/frozen_inference_graph.pb\", \"r\");\r\n if (!file){\r\n    printf(\" Downloading faster_rcnn_resnet101_kitti_2018_01_28.tar.gz .....\");\r\n    system(\"wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2018_01_28.tar.gz\");\r\n    system(\"mkdir -p /tmp/resnet_example/1/\");\r\n    system(\"tar -xf faster_rcnn_resnet101_kitti_2018_01_28.tar.gz -C /tmp/resnet_example/1/\");\r\n    system(\"rm faster_rcnn_resnet101_kitti_2018_01_28.tar.gz\");\r\n    }\r\n  const string export_dir = \"/tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/saved_model\";\r\n  const std::unordered_set<std::string> tags = {\"serve\"};\r\n  SessionOptions opts;\r\n  RunOptions r_opts;\r\n  // Loading Saved Model\r\n  SavedModelBundle bundle;\r\n  std::unordered_set<std::string> inputs, outputs;\r\n  TF_CHECK_OK(LoadSavedModel(opts, r_opts, export_dir, tags, &bundle));\r\n  GraphDef gd2 = (bundle).meta_graph_def.graph_def();\r\n  Graph *g = new Graph(tensorflow::OpRegistry::Global());\r\n  TF_CHECK_OK(ConvertGraphDefToGraph(tensorflow::GraphConstructorOptions(), gd2, g));\r\n}\r\n```\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThere is a bug in ConvertGraphDefToGraph, I can currently run this model through the Python API but I am having problems getting the GraphDef, and then converting it back to a Graph in C++.\r\nThis seems to stem from the fact that its an old graphdef being loaded into a new version of tensorflow. The index type attribute\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n2018-09-15 00:25:13.325147: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/saved_model\r\n2018-09-15 00:25:13.440161: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2018-09-15 00:25:13.831448: I tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.\r\n2018-09-15 00:25:13.831492: I tensorflow/cc/saved_model/loader.cc:172] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /tmp/resnet_example/1/faster_rcnn_resnet101_kitti_2018_01_28/saved_model/variables/variables.index\r\n2018-09-15 00:25:13.831499: I tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key legacy_init_op on SavedModel bundle.\r\n2018-09-15 00:25:13.831525: I tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: success. Took 506360 microseconds.\r\n2018-09-15 00:25:14.044682: F tensorflow/core/graph/graph_constructor_test.cc:3248] Non-OK-status: ConvertGraphDefToGraph(tensorflow::GraphConstructorOptions(), gd2, g) status: Not found: No attr named 'index_type' in NodeDef:\r\n   [[{{node GridAnchorGenerator/Meshgrid/ExpandedShape/ones}} = Fill[T=DT_INT32, _output_shapes=[[?]]](GridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, GridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\r\n   [[{{node GridAnchorGenerator/Meshgrid/ExpandedShape/ones}} = Fill[T=DT_INT32, _output_shapes=[[?]]](GridAnchorGenerator/Meshgrid/ExpandedShape/Reshape, GridAnchorGenerator/Meshgrid/ExpandedShape/ones/Const)]]\r\n*** Received signal 6 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/_U_S_Stensorflow_Score_Cgraph_Ugraph_Uconstructor_Utest___Utensorflow/libtensorflow_framework.so(+0x6cb53d)[0x7fd9d02fc53d]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7fd9cee9d390]\r\n/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7fd9ce050428]\r\n/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7fd9ce05202a]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/_U_S_Stensorflow_Score_Cgraph_Ugraph_Uconstructor_Utest___Utensorflow/libtensorflow_framework.so(+0x6d2837)[0x7fd9d0303837]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/graph_graph_constructor_test.runfiles/org_tensorflow/tensorflow/core/graph_graph_constructor_test[0x54e091]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS_4TestEvEET0_PT_MS4_FS3_vEPKc+0x47)[0x7fd9d0ae95d7]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing4Test3RunEv+0xd2)[0x7fd9d0ae9822]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8TestInfo3RunEv+0x118)[0x7fd9d0ae9b28]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8TestCase3RunEv+0xb5)[0x7fd9d0ae9dc5]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal12UnitTestImpl11RunAllTestsEv+0x218)[0x7fd9d0aea068]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8internal35HandleExceptionsInMethodIfSupportedINS0_12UnitTestImplEbEET0_PT_MS4_FS3_vEPKc+0x47)[0x7fd9d0aea387]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libexternal_Scom_Ugoogle_Ugoogletest_Slibgtest.so(_ZN7testing8UnitTest3RunEv+0x92)[0x7fd9d0aea592]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/../../_solib_k8/libtensorflow_Score_Slibtest_Umain.so(main+0x9b)[0x7fd9d0b5db6b]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7fd9ce03b830]\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/0e110d1a1e25aaa081a6bc08469ac91f/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/graph_graph_constructor_test.runfiles/org_tensorflow/tensorflow/core/graph_graph_constructor_test[0x515239]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n  tensorflow::CurrentStackTrace[abi:cxx11]()\r\n\r\n\r\n  gsignal\r\n  abort\r\n\r\n\r\n  void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*)\r\n  testing::Test::Run()\r\n  testing::TestInfo::Run()\r\n  testing::TestCase::Run()\r\n  testing::internal::UnitTestImpl::RunAllTests()\r\n  bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*)\r\n  testing::UnitTest::Run()\r\n  main\r\n  __libc_start_main\r\n\r\n*** End stack trace ***\r\n", "comments": ["@sboshin could you check whether the TensorFlow version you used to train the model is the same as the one for serving the model? ", "@wt-huang I unfortunately didn't train this model its gotten from [Object Detection](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md), which is an official repo. From the little understanding its not able to find an attribute\r\n", "@sboshin you can retrain the model with the current TensorFlow version which should be the same as the one you will be using  for serving. ", "Closing this one, feel free to reopen if any issues surface.", "Sorry I haven't replied, the issue isn't stemming from the version differences, But its actually when you have an OP with a default attribute stripped. ConvertGraphDeftoGraph can't reinstate default attributes.", "I can write a more succinct test to show the issue. The description of the test would be. Create a node. Strip the default attributes, and Use ConvertGraphDeftoGraph function on the resulting graphdef. This will error out. If this is the correct response of the function to not be able to automatically assign default attributes then please leave this closed", "@sboshin Please go ahead and write a test to show this issue.", "Closing this issue for now, feel free to reopen if the problem persists."]}, {"number": 22286, "title": "[Intel MKL] Fixes for unit test failures", "body": "1) Changes in partitioned_function_ops.cc are for passing\r\n   Global OpRegistry as default_registry in PartitionedFunction op\r\n\r\n   This fix addresses failure in MKL layout pass when PartitionedFunction\r\n   op calls graph optimization passes. The problem was that the function\r\n   library definition that is used to create function graph and corresponding\r\n   subgraphs after partitioning did not use global OpRegistry as the\r\n   default OpRegistry used for look of operator names. Because of that,\r\n   standard operators such as \"Const\" were not available to graph passes.\r\n\r\n2) Changes in mkl_cpu_allocator.h are to address failure in\r\n   mkl_cpu_allocator_test which was expecting that max_bytes_limits is returned\r\n   via GetStats() in MKLCPUAllocator.", "comments": ["@tatianashp @penpornk This PR fixes the failures in the MKL CI, thanks.", "Thank you for the fixes! :)", "@nhasabni Thank you for fixing the test failure! \r\n\r\nPlease fix the formatting issues so that we could get this PR in.\r\n", "Thanks @andydavis1 @tatianashp @penpornk for review. Addressed review comments.", "@tatianashp I looked into the failing test -- //tensorflow/python/eager:function_test seems to fail in spite of my fix. Can you confirm?", "@tatianashp Sorry, my bad. The test is failing because of my fix. Will fix it.", "Sounds good. Looking forward to the fix. Let us know if you need feedback from TF Eager folks.", "@tatianashp I think I will need some input to fix the issue. I am not sure if the TensorFlow core API exists for what I am looking for. I think @mrry and @akshayka have worked on relevant piece of code. So if they can comment, that would be great.\r\n\r\n**The main issue that I was trying to address was**: some unit tests fail with MKL graph passes because these graph passes need the definition of standard operators (like Const) in order to rewrite graphs. The definitions of these standard operators are only available in `OpRegistry::Global`, and the code in `partitioned_function_ops.cc` was not passing `OpRegistry::Global()` to the graph passes. That is why my change was to pass `OpRegistry::Global()` while creating graph in `partitioned_function_ops.cc`.\r\n\r\nBut with my change, `python:eager_test` is failing because it cannot find a symbol `add_9n57JfTlx2g` (The precise error is ```graph.cc:379] Non-OK-status: ops_.LookUpOpDef(node->type_str    ing(), &op_def) status: Not found: Op type not registered add_9n57JfTlx2g```. \r\n\r\nI tried to debug `eager_test` failure. I see that, by default, `partitioned_function_ops.cc` passes `flib_def` and the `default_registry()` for that `flib_def` from the graph for the input function while creating a graph that is passed to the graph optimization passes. I feel that my change passes `OpRegistry::Global()` instead of `default_registry()` for input `flib_def` while creating the graph. And I think this is creating the problem in `.\r\n\r\nI think the solution should allow us to create a graph using both `default_registry` from input `flib_def` and `OpRegistry::Global()`. But I do not think such API exists.\r\n\r\nThanks.", "What I do not understand here is why something is in OpRegistry::Global() and not in default_registry, which should always be a superset of OpRegistry::Global.", "If you insist on creating a function library definition with the global registry then you should make sure to register all functions which you need in that new function library definition. Not sure if there's an API to get the list of registered functions, but it should not be hard to get one.", "@alextp Thanks for input. Yes, I also did not understand. I thought that default_registry should always include global registry. But that does not seem to be the case. Especially, in `nested_function_test` of `eager:function_test` top-level function's `flib_def`seems to be passed as `default_registry` to child function.", "The problem appeared after https://github.com/tensorflow/tensorflow/commit/eb71a1a3afbbe21407b2149d7adc4efa9e557b24 commit that added graph optimization passes to PartitionedCallOp.\r\n\r\n@akshayka - Could you help to figure out what to do here? \r\n", "@tatianashp Not sure if XLA failure is known? If not, I can debug.", "@nhasabni I don't think it has been reported yet. Please feel free to go for it. :)", "Sorry, I was wrong. This is known but hasn't been fixed yet. I'll see what we can do.", "@nhasabni  Most likely Windows Bazel and XLA failures are not due to your changes since the tests don't use --config=mkl.\r\n@qlzh727  Do you know if the failures above are specific to this PR?", "I ran the XLA test without my PR and without MKL. It fails in my setup also.", "Let me kick off a new test, from the dashboard, the XLA test is green at the moment, we will see if the new round of test passes or not.", "@qlzh727 can you merge this PR? the XLA test is failing in tests:exhaustive_f32_elementwise_op_test_cpu, seems to be unrelated to this PR. Other PRs are also failing this XLA test. Thanks.", "@qlzh727 The PR is stuck in \"ready to pull\". Can you take a look? Thanks and sorry to bother you.", "It is failing for the our internal test, and @tatianashp will probably take more actions for it. ", "@qlzh727 Thanks. @tatianashp let me know how we can help.\r\n\r\n", "In `partitioned_function_ops.cc`, could you please wrap `TF_CHECK_OK` around both your `AddFunctionLibrary` calls (and make sure the formatting is ok)?\r\nLine 105,\r\n```c++\r\nTF_CHECK_OK(\r\n            graph.get()->AddFunctionLibrary(global_flib.ToProto()));\r\n```\r\nLine 261,\r\n```c++\r\nTF_CHECK_OK(\r\n          subgraph.get()->AddFunctionLibrary(global_flib.ToProto()));\r\n```\r\nThank you!", "Thanks @penpornk. Pls take a look."]}, {"number": 22285, "title": "Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)", "body": "Note: I'm noob to tensorflow. I can provide more info if needed.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. I trained https://github.com/lightvector/GoNN on my own data (`data.h5`) without code modification\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: gentoo (kernel 4.17.14-gentoo)  uptodate\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no mobile device\r\n- **TensorFlow installed from (source or binary)**: source (from gentoo repository)\r\n- **TensorFlow version (use command below)**: 'unknown' 1.10.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0-r3 p1.4\r\n- **CUDA/cuDNN version**: cuda 9.2.88 / cudnn 7.1.4\r\n- **GPU model and memory**: GeForce GTX 1070 Ti 7.93GiB\r\n- **Exact command to reproduce**: `./train.py -traindir models -fast-factor 50 -gamesh5 data.h5`\r\n\r\n**Extra:** \r\n- ebuild from tensorflow I used: https://gitweb.gentoo.org/repo/gentoo.git/tree/sci-libs/tensorflow/tensorflow-1.10.0.ebuild\r\n- use flag : \r\n`sci-libs/tensorflow cuda -jemalloc -system-libs -mpi PYTHON_TARGETS: -* python3_6`\r\n\r\n### Describe the problem\r\n\r\nWhen I run tensorflow on GPU, it crashed.\r\n\r\nWhen I run on CPU, it works:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=\"\" ./train.py  -traindir models -fast-factor 50 -gamesh5 data.h5\r\n```\r\n\r\n### Source code / logs\r\n\r\nCode:\r\n\r\nhttps://github.com/lightvector/GoNN/tree/2d74b1300fef6700e75af7f2344d852798b713fb\r\n\r\nLogs:\r\n\r\n```\r\n ~/c/GoNN: ./train.py -traindir models -fast-factor 50 -gamesh5 data.h5                                                                                                                                      1058ms  ven. 14 sept. 2018 20:43:51 CEST\r\nBuilding model\r\nWARNING:tensorflow:From /home/tychota/code/GoNN/model.py:912: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n\r\nFuture major versions of TensorFlow will allow gradients to flow\r\ninto the labels input on backprop by default.\r\n\r\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\r\n\r\nAdjusting gradient for rconv1/w1:0 by Tensor(\"PadV2:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv1/w2:0 by Tensor(\"PadV2_1:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv2/w1a:0 by Tensor(\"PadV2_2:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv2/w1b:0 by Tensor(\"PadV2_3:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv2/w2:0 by Tensor(\"PadV2_4:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv3/w1a:0 by Tensor(\"PadV2_5:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv3/w1b:0 by Tensor(\"PadV2_6:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv3/w2:0 by Tensor(\"PadV2_7:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv4/w1a:0 by Tensor(\"PadV2_8:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv4/w1b:0 by Tensor(\"PadV2_9:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv4/w2:0 by Tensor(\"PadV2_10:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv5/w1a:0 by Tensor(\"PadV2_11:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv5/w1b:0 by Tensor(\"PadV2_12:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv5/w2:0 by Tensor(\"PadV2_13:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv7/w1a:0 by Tensor(\"PadV2_14:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv7/w1b:0 by Tensor(\"PadV2_15:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv7/w2:0 by Tensor(\"PadV2_16:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv8/w1a:0 by Tensor(\"PadV2_17:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv8/w1b:0 by Tensor(\"PadV2_18:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv8/w2:0 by Tensor(\"PadV2_19:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv9/w1a:0 by Tensor(\"PadV2_20:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv9/w1b:0 by Tensor(\"PadV2_21:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv9/w2:0 by Tensor(\"PadV2_22:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv10/w1a:0 by Tensor(\"PadV2_23:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv10/w1b:0 by Tensor(\"PadV2_24:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv10/w2:0 by Tensor(\"PadV2_25:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv11/w1a:0 by Tensor(\"PadV2_26:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv11/w1b:0 by Tensor(\"PadV2_27:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv11/w2:0 by Tensor(\"PadV2_28:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv13/w1a:0 by Tensor(\"PadV2_29:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv13/w1b:0 by Tensor(\"PadV2_30:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv13/w2:0 by Tensor(\"PadV2_31:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv14/w1a:0 by Tensor(\"PadV2_32:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv14/w1b:0 by Tensor(\"PadV2_33:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for rconv14/w2:0 by Tensor(\"PadV2_34:0\", shape=(3, 3, 1, 1), dtype=float32)\r\nAdjusting gradient for p1/norm/beta:0 by 0.25\r\nAdjusting gradient for p2/w:0 by 0.25\r\nModel variable conv1/wcenter:0, 4256 parameters\r\nModel variable conv1/w:0, 106400 parameters\r\nModel variable rconv1/norm1/beta:0, 224 parameters\r\nModel variable rconv1/w1:0, 451584 parameters\r\nModel variable rconv1/norm2/beta:0, 224 parameters\r\nModel variable rconv1/w2:0, 451584 parameters\r\nModel variable rconv2/norm1/beta:0, 224 parameters\r\nModel variable rconv2/w1a:0, 322560 parameters\r\nModel variable rconv2/w1b:0, 129024 parameters\r\nModel variable rconv2/norm2/beta:0, 224 parameters\r\nModel variable rconv2/w2:0, 451584 parameters\r\nModel variable rconv3/norm1/beta:0, 224 parameters\r\nModel variable rconv3/w1a:0, 322560 parameters\r\nModel variable rconv3/w1b:0, 129024 parameters\r\nModel variable rconv3/norm2/beta:0, 224 parameters\r\nModel variable rconv3/w2:0, 451584 parameters\r\nModel variable rconv4/norm1/beta:0, 224 parameters\r\nModel variable rconv4/w1a:0, 322560 parameters\r\nModel variable rconv4/w1b:0, 129024 parameters\r\nModel variable rconv4/norm2/beta:0, 224 parameters\r\nModel variable rconv4/w2:0, 451584 parameters\r\nModel variable rconv5/norm1/beta:0, 224 parameters\r\nModel variable rconv5/w1a:0, 322560 parameters\r\nModel variable rconv5/w1b:0, 129024 parameters\r\nModel variable rconv5/norm2/beta:0, 224 parameters\r\nModel variable rconv5/w2:0, 451584 parameters\r\nModel variable rconv7/norm1/beta:0, 224 parameters\r\nModel variable rconv7/w1a:0, 322560 parameters\r\nModel variable rconv7/w1b:0, 129024 parameters\r\nModel variable rconv7/norm1b/beta:0, 64 parameters\r\nModel variable rconv7/w1r:0, 20480 parameters\r\nModel variable rconv7/norm2/beta:0, 160 parameters\r\nModel variable rconv7/w2:0, 322560 parameters\r\nModel variable rconv8/norm1/beta:0, 224 parameters\r\nModel variable rconv8/w1a:0, 322560 parameters\r\nModel variable rconv8/w1b:0, 129024 parameters\r\nModel variable rconv8/norm2/beta:0, 224 parameters\r\nModel variable rconv8/w2:0, 451584 parameters\r\nModel variable rconv9/norm1/beta:0, 224 parameters\r\nModel variable rconv9/w1a:0, 322560 parameters\r\nModel variable rconv9/w1b:0, 129024 parameters\r\nModel variable rconv9/norm2/beta:0, 224 parameters\r\nModel variable rconv9/w2:0, 451584 parameters\r\nModel variable rconv10/norm1/beta:0, 224 parameters\r\nModel variable rconv10/w1a:0, 322560 parameters\r\nModel variable rconv10/w1b:0, 129024 parameters\r\nModel variable rconv10/norm2/beta:0, 224 parameters\r\nModel variable rconv10/w2:0, 451584 parameters\r\nModel variable rconv11/norm1/beta:0, 224 parameters\r\nModel variable rconv11/w1a:0, 322560 parameters\r\nModel variable rconv11/w1b:0, 129024 parameters\r\nModel variable rconv11/norm1b/beta:0, 64 parameters\r\nModel variable rconv11/w1r:0, 20480 parameters\r\nModel variable rconv11/norm2/beta:0, 160 parameters\r\nModel variable rconv11/w2:0, 322560 parameters\r\nModel variable rconv13/norm1/beta:0, 224 parameters\r\nModel variable rconv13/w1a:0, 322560 parameters\r\nModel variable rconv13/w1b:0, 129024 parameters\r\nModel variable rconv13/norm2/beta:0, 224 parameters\r\nModel variable rconv13/w2:0, 451584 parameters\r\nModel variable rconv14/norm1/beta:0, 224 parameters\r\nModel variable rconv14/w1a:0, 322560 parameters\r\nModel variable rconv14/w1b:0, 129024 parameters\r\nModel variable rconv14/norm2/beta:0, 224 parameters\r\nModel variable rconv14/w2:0, 451584 parameters\r\nModel variable trunk/norm/beta:0, 224 parameters\r\nModel variable p1/intermediate_conv/w:0, 96768 parameters\r\nModel variable g1/w:0, 64512 parameters\r\nModel variable g1/norm/beta:0, 32 parameters\r\nModel variable matmulg2w:0, 3072 parameters\r\nModel variable p1/norm/beta:0, 48 parameters\r\nModel variable p2/w:0, 48 parameters\r\nBuilt model, 10901664 total parameters\r\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv1/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv1/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv2/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv2/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv3/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv3/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv4/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv4/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv5/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv5/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv7/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg\r\nAdditional update op on train step: rconv7/norm1b/AssignMovingAvg_1\r\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv7/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv8/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv8/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv9/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv9/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv10/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv10/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv11/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg\r\nAdditional update op on train step: rconv11/norm1b/AssignMovingAvg_1\r\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv11/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv13/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv13/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg\r\nAdditional update op on train step: rconv14/norm1/AssignMovingAvg_1\r\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg\r\nAdditional update op on train step: rconv14/norm2/AssignMovingAvg_1\r\nAdditional update op on train step: trunk/norm/AssignMovingAvg\r\nAdditional update op on train step: trunk/norm/AssignMovingAvg_1\r\nAdditional update op on train step: g1/norm/AssignMovingAvg\r\nAdditional update op on train step: g1/norm/AssignMovingAvg_1\r\nAdditional update op on train step: p1/norm/AssignMovingAvg\r\nAdditional update op on train step: p1/norm/AssignMovingAvg_1\r\nOpening H5 file: data.h5\r\nAdjusting H5 cache settings to: [0, 521, 134217728, 0.75]\r\nTraining\r\n2018-09-14 20:44:04.209964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-14 20:44:04.210288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.34GiB\r\n2018-09-14 20:44:04.210299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-14 20:44:04.581780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-14 20:44:04.581800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-14 20:44:04.581804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-14 20:44:04.581914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7077 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nBegan session\r\nTraining on 13234987 rows, validating on 688542/688542 rows\r\nEpoch size = 20000\r\nh5_chunk_size = 6000\r\nBatch size = 200\r\nL2 coeff value = 3e-05\r\nuse_ranks = False\r\npredict_pass = False\r\n2018-09-14 20:44:05.383592: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Not found: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\r\n\t.  Registered:  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n\r\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\r\n\t.  Registered:  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n\r\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./train.py\", line 455, in <module>\r\n    vmetrics_evaled = merge_dicts(run_validation_in_batches(vmetrics), np.sum)\r\n  File \"./train.py\", line 325, in run_validation_in_batches\r\n    result = run(fetches, rows, symmetries=[False,False,False], training=False)\r\n  File \"./train.py\", line 309, in run\r\n    model.is_training: training\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'DataFormatDimMap' OpKernel for GPU devices compatible with node ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)\r\n\t.  Registered:  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]\r\n\r\n\t [[Node: ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer = DataFormatDimMap[T=DT_INT32, _kernel=\"host\", dst_format=\"NCHW\", src_format=\"NHWC\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](concat)]]\r\n\r\n```\r\n", "comments": ["/CC @zhangyaobit, any ideas what is happening here? The error message is complaining there is no GPU kernel for int32, but then in the list of supported combinations lists GPU with int32.", "@reedwm Yeah, the error message does seem to contradict with itself.\r\n\r\n@tychota Thanks for reporting the issue. Could you provide a minimal example to reproduce the error?", "@reedwm\r\n\r\n> /CC @zhangyaobit, any ideas what is happening here? The error message is complaining there is no GPU kernel for int32, but then in the list of supported combinations lists GPU with int32.\r\n\r\nThat was what bugged me also. When i tried to looks https://github.com/tensorflow/tensorflow issues for already existing bugs, the similar bugs were coherant (even if I have no idea what that actually means in detail).\r\n\r\n@zhangyaobit\r\n\r\n> @tychota Thanks for reporting the issue. Could you provide a minimal example to reproduce the error?\r\n\r\n(note that I have given a reproductible example, clearly not minimal in the first post : have you seen it ? Just to be sure that you have seen it and need something smaller because it can be really involving with my actual skill to find the root cause, see below.)\r\n\r\nI would love to find the root cause and build and Minimal Failing Example but I did not find anyway to do a link between `ReverseV2_1-1-DimMapNHWCToNCHW-LayoutOptimizer`  and any code from https://github.com/lightvector/GoNN/tree/2d74b1300fef6700e75af7f2344d852798b713fb\r\n\r\nThere is two \"reverse\" occurences:\r\n\r\nhttps://github.com/lightvector/GoNN/blob/0afa4e84b35967380bcfc3021fb748de8b8b7091/model.py#L274\r\n\r\nhttps://github.com/lightvector/GoNN/blob/0afa4e84b35967380bcfc3021fb748de8b8b7091/model.py#L289\r\n\r\nand no \"layout\" or \"Optimizer\" except a \"MomentumOptimizer\":\r\n\r\nhttps://github.com/lightvector/GoNN/blob/a3903ccd1693a94c4a506c3ec0ee8b94f288edb3/train.py#L105\r\n\r\nI'm really noob with tf (just started a few weeks ago); butI can try to build a minimal reproduction if you give me some pointer / pseudo code to help me going in the right direction.", "tychota@, To produce a smaller example, I think you can start with a trivial tensorflow program, next maybe try run the inference part only (forward pass) for the model to see if it hits the error; if so try reduce the model as much as possible and only keep the part that leads to the error.", "@tychota Did you get a chance to work on this?\r\n\r\n> tychota@, To produce a smaller example, I think you can start with a trivial tensorflow program, next maybe try run the inference part only (forward pass) for the model to see if it hits the error; if so try reduce the model as much as possible and only keep the part that leads to the error.\r\n\r\n", "Not yet.\r\n\r\nTrivial tensorflow works and I did not find the problematic line.\r\nI will try next week end.", "Ran into this myself after upgrading tensorflow versions. The below seems to cause the bug.\r\nEdit: Looks like tweaking the implementation of apply_symmetry to use tf.cond directly to switch on whether to apply tf.reverse or not rather than to build a rev_axes tensor works around the issue. I'm guessing there's some bug in the layout optimizer where tensorflow is not expecting this funny way of nesting operations?\r\n\r\n```\r\n#!/usr/bin/python3\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef apply_symmetry(tensor,symmetries,inverse):\r\n  ud = symmetries[0]\r\n  lr = symmetries[1]\r\n  transp = symmetries[2]\r\n\r\n  rev_axes = tf.concat([\r\n    tf.cond(ud, lambda: tf.constant([1]), lambda: tf.constant([],dtype='int32')),\r\n    tf.cond(lr, lambda: tf.constant([2]), lambda: tf.constant([],dtype='int32')),\r\n  ], axis=0)\r\n\r\n  if not inverse:\r\n    tensor = tf.reverse(tensor, rev_axes)\r\n\r\n  tensor = tf.cond(\r\n    transp,\r\n    lambda: tf.transpose(tensor, [0,2,1,3]),\r\n    lambda: tensor)\r\n\r\n  if inverse:\r\n    tensor = tf.reverse(tensor, rev_axes)\r\n\r\n  return tensor\r\n\r\ndef conv_weight_variable(diam1, diam2, in_channels, out_channels):\r\n  weights = tf.Variable(tf.zeros([diam1,diam2,in_channels,out_channels]))\r\n  return weights\r\ndef conv_layer(in_layer, diam, in_channels, out_channels):\r\n  weights = conv_weight_variable(diam, diam, in_channels, out_channels)\r\n  out_layer = tf.nn.conv2d(in_layer, weights, strides=[1,1,1,1], padding='SAME')\r\n  return out_layer\r\n\r\n\r\ninputs = tf.placeholder(tf.float32, [None, 8, 8, 4]) # NHWC\r\nsymmetries = tf.placeholder(tf.bool, [3])\r\nlayer = apply_symmetry(inputs,symmetries,inverse=False)\r\nlayer = conv_layer(layer, diam=3, in_channels=4, out_channels=1)\r\nlayer = apply_symmetry(layer,symmetries,inverse=True)\r\nlayer = tf.reshape(layer,[-1,64]) # Convert [N, 8, 8, 1] into [N,64]\r\n\r\nwith tf.Session() as session:\r\n  session.run(tf.global_variables_initializer())\r\n  fetches = layer\r\n  result = session.run(fetches, feed_dict={\r\n    inputs: np.zeros([1,8,8,4]),\r\n    symmetries: [False,False,False]\r\n  })\r\n  print(result)\r\n\r\n```\r\n\r\n", "Any updates on this? In particular, does the test case I posted reproduce for anyone else?\r\n(To clarify, this isn't very urgent, it only makes me a little nervous about if workarounded version is actually fixing the problem or if the graph is nominally working but still silently buggy due to whatever the layout optimizer is presumably doing underneath.  Aside from that, just wanted to make sure this was a sufficient test example for any debugging that might take place.)\r\n\r\nThanks!", "Nagging Assignee @ymodak: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue due to lack of repro example. Feel free to reopen this issue when new information is available. @lightvector please open a new issue by providing all the information asked by the template. This will help us to reduce the troubleshooting time. Thanks!", "Did u get the solution? If so please discuss here."]}, {"number": 22284, "title": "Update 1.11.0-rc0 version strings to 1.11.0-rc1", "body": "", "comments": []}, {"number": 22283, "title": "Fix performance issue when training with keras model in eager mode.", "body": "PiperOrigin-RevId: 212908218", "comments": []}, {"number": 22282, "title": "move SymGrad and RCall kernels to functional_ops", "body": "This moving of SymbolicGradient and RemoteCall is because\r\n  their OPs are defined in ops/functional_ops.h .\r\n\r\nBenefits:\r\nif a C++ developer only imports ops/function_ops.cc and kernels/function_ops.cc,\r\n  without 2 functional_ops.cc files,\r\nthen there won't be runtime errors reporing the OPs are not found.\r\n\r\nOther influence:\r\nThis commit removes the 2 kernels from //tensorflow/core/kernels:android_core_ops,\r\n  which is safe since the two OPs are not defined yet.", "comments": ["Thanks for reviewing @alextp!", "@gdh1995 gentle ping to resolve conflicts", "Sorry but recent days I have no enough time to complete this PR. So let me close it."]}, {"number": 22281, "title": "Update BAZEL_VERSION to 0.17.1", "body": "This fix tries to update BAZEL_VERSION from previous 0.15.0 to 0.17.1. The reason for the update was that PR #19461 depends on a Bazel issue https://github.com/bazelbuild/bazel/issues/5932 which has only been fixed in 0.17.1.\r\n\r\nThis fix updates the BAZEL_VERSION so that test builds could succeed for #19461.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yifeif @gunan The PR #19461 needs a fix in Bazel (https://github.com/bazelbuild/bazel/issues/5932) to pass the test. The Bazel fix is only available in 0.17.1.\r\n\r\nI am trying to update the Bazel to 0.17.1 for CI tests. I think some other changes might be needed. Please let me know if there are other places I need to modify.", "Hi @yongtang Yes, bazel upgrade needs more changes internally.\r\n\r\nHowever, for us to be able to use a feature only in 0.17.1, we have to bump our requirement of bazel version to 0.17.1\r\nI would like to wait one full bazel release cycle before I do that, as pushing dependencies to bleeding edge has caused problems before.\r\nDoes this sound reasonable?", "Thanks @gunan, lets wait until the next version of Bazel is released. Much appreciate the help \ud83d\udc4d !", "Let me close this PR. Will update the Bazel status when next version is released."]}, {"number": 22280, "title": "Broken link to \"capabilities and limitations\" in AutoGraph guide", "body": "https://www.tensorflow.org/guide/autograph has a link to `Autograph capabilities and limitations`.  The URL is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/autograph/LIMITATIONS.md, but this is a dead link.  The corrected link is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/LIMITATIONS.md.", "comments": ["Cc @alextp.", "Queue bot complaining that I didn't answer the list of questions...", "@lamberta can you update the page? I'm a little lost as to what change needs to be made now that we're changing the hosting of things", "Probably worth a quick grep to see if there are other bad links to contrib.", "Thanks. That file is here in the docs repo: https://github.com/tensorflow/docs/blob/master/site/en/guide/autograph.ipynb\r\n\r\nBut, yes, if it's out of contrib we should grep around and update.", "Fixed contrib links. What about `from tensorflow.contrib import autograph` ?", "For now the import is still through contrib; that should stay.\n\nOn Fri, Sep 14, 2018 at 9:42 AM Billy Lamberta <notifications@github.com>\nwrote:\n\n> Fixed contrib links. What about from tensorflow.contrib import autograph ?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22280#issuecomment-421414988>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxUGgbzvc8L7FxwXzS3oxaBF61O-Iks5ua9xmgaJpZM4Wphla>\n> .\n>\n\n\n-- \n - Alex\n", "This is broken again. I think the new link should be:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md"]}, {"number": 22279, "title": "tflite flatbuffer conversion to json by upgrade_schema.py doesn't transfer uint8 bias vals", "body": "the upgrade_schema conversion to json does not export the int32 bias values of the quantized tflite mobilenet v1 example, MobileNet_v1_1.0_224_quant.  For example, the first Conv2D has 32 int32 bias values, but these are not exported to a json output.  The bias values are visible  in the the tflite file using Netron.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/schema/upgrade_schema.py\r\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\nMobileNet_v1_1.0_224_quant\r\n\r\nI'm working in Ubuntu 18.04 with the anaconda installation of tensorflow v 1.9.0", "comments": ["hmm... I just tried the flatbuffer flatc convert using\r\nflatc -t schema.fbs -- mobilenet_v1_1.0_224_quant.tflite\r\nIt also creates a json file without the bias data.\r\nThis file has quantized uint8 data that does appear in the json data, but the int32 bias vals don't get into the json output.", "hmm... after looking at the schema, I'm wondering if perhaps the int32 bias data is just being split up and displayed as bytes in the json file.  I was just assuming that the byte data displayed was the uint8 quantized values.", "Please provide more information about this error:\r\n1. An attachment containing the model that you are working with (if this is possible). Please provide both the tflite and json file.\r\n2. Where you are getting your model file from (i.e. whether you are downloading the .tflite MobileNetV1 directly converting via tflite_convert/toco).\r\n3. Any commands you are running. If you are using our converter, please include that command.\r\n4. The exact error you are getting when running the model.\r\n5. Any snippets of code that are creating the error (if possible).", "I'm downloading the MobileNet_v1_1.0_224_quant gzip from the link I already provided, \r\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\nunzip it, and you'll find the tflite flatbuffer.\r\nThe schemas are in the Tensorflow layout, along with upgrade_schema.py\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/schema\r\n\r\nI installed flatbuffers from Anaconda\r\nconda install -c conda-forge flatbuffers \r\n\r\nThen run the flatc command I already provided:\r\nflatc -t schema.fbs -- mobilenet_v1_1.0_224_quant.tflite\r\n\r\nThat will create the json file.\r\nAs I said, though, I now suspect that the issue is that json is just displaying the bias data as bytes rather than as int32 values.  I haven't confirmed it yet, but it seems to me it would be too big of an error to have been missed if the bias data not there at all.  I just assumed it would be there as 32 bit signed data, in the same form as in the Netron viewer.\r\nYou can see the bias data by opening the tflite file in the Netron viewer and clicking on any of the bias tabs.\r\nhttps://github.com/lutzroeder/netron\r\n\r\n", "yeah, I see a buffer that starts with data: [ 170,227,255,255,153,52,0,0,\r\nwhich would correspond to the first two entries of the bias for the first conv2d in the graph, -7254, 13465, little endian.\r\nSo, I guess I can close this, assuming that someone intended the int32 data to be represented by bytes in the json file.", "Ok, I'm going to close this, since the bias data is in the json file as bytes.  You guys might want to consider changing it to save int32 data in the json file, though, since it seems reasonable that the array entries in the json should match the number of bias values."]}, {"number": 22278, "title": "TFLite Android ArrayIndex outofBound Exception length=40, index=-7", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Partially modified TFLite Android code\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI converted mobilenet_v2_ssd trained using my data to tflite following the instructions in the blog post:\r\nhttps://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n\r\nI did not quantize the model since I didn't train it using quantized training. To run on mobile I set the \r\nIS_QUANTIZED variable in Detector.Activity to false. When I run the app, i get the error \r\njava.lang.ArrayIndexOutOfBoundsException: length=40; index=-8\r\nat java.util.Vector.elementAt(Vector.java:326)\r\nat java.util.Vector.get(Vector.java:442)\r\nat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:214)\r\n\r\nI thought the issue is due to the length of the output vector. So I changed the NUM_DETECTIONS in TFLiteObjectDetectionAPIModel.java to 40, then  I get another error saying\r\nCannot copy between a TensorFlowLite tensor with shape [1, 10] and a Java object with shape [1, 40].\r\n\r\nIs the length of output vector fixed? Please help me with the issue.Thanks!", "comments": ["The android demo app was made for one specific case this [issue comment](https://github.com/tensorflow/tensorflow/issues/22106#issuecomment-428409506) the last part of the last comment may solve your issue, you don't have to switch branch or anything, just do boundary checks like he did.", "Closing due to inactivity and since this is answered above. ", "ERROR : W/System.err: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648\r\n\r\nEven I am facing the same issue could you brief on the solution?\r\n\r\n@suharshs", "I am having similar issues when using custom trained models converted by https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\n\r\nbut don't see the problem with default models automatically downloaded.", "I have it too.does not any one help us?\r\nit is force for me", "> I have it too.does not any one help us?\r\n> it is force for me\r\n\r\nI suggest you refer to #22106 https://github.com/tensorflow/tensorflow/issues/22106#issuecomment-428409506 for solution.\r\nIt would help change the input tensor.", "@patrick-ucr  Have you resolve this ?\r\nI'm also facing same issue while working with custom models.", "@DeepakSridhar @HaFred I am facing the same issue . If there is any legitimate solution found until now ,Please forward .\r\nAlso let me know if this issue is caused mostly due to tflite model instead of a bad written code "]}, {"number": 22277, "title": "How to compile the tensorflow-lite to get a smaller and faster library?", "body": "### System information\r\n- **Mobile device**: \r\n    * HUAWEI Mate 9\r\n    * Xiaomi Mi 6\r\n\r\n### Describe the problem\r\nDuring Arm AI Developer Global Summit, the Google engineer said that TensorFlow-Lite can be compiled into an about 400KB library. However, when I compile the latest TensorFlow-Lite, I can only get a  1.2MB dynamic link library. The following script is my compiled command:\r\n```bash\r\nbazel build -c opt --cxxopt='--std=c++11' --cxxopt='-O3' --cxxopt='-march=armv7-a' --cxxopt='-mfpu=neon' --cxxopt='-mfloat-abi=softfp' //tensorflow/contrib/lite/java:tensorflowlite --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\r\n```\r\nWhat's wrong with this compiled command?\r\n\r\nBesides,  in order to test the performance of tensorflow-lite, I also compiled its benchmarks according to [this official doc](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/benchmark) .The following script is my compiled command (offered by the official doc):\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm \\\r\n  --cxxopt='--std=c++11' \\\r\n  tensorflow/contrib/lite/tools/benchmark:benchmark_model\r\n```\r\nWhen running MobileNet_v1_1.0_224, its result is shown as follow:\r\n![image](https://user-images.githubusercontent.com/17102274/45552548-d9ca7f80-b863-11e8-8bd1-db14bf5c354f.png)\r\nI want to know how to get a faster result?\r\n", "comments": ["@wzzju : The command that you specified is building a java library, is that correct?\r\n\r\nFor benchmarks the command looks correct, the only thing I may change would be to use config=android_arm64 for 64 bit.\r\n\r\nWe have published a few performance numbers on our [website](https://www.tensorflow.org/mobile/tflite/performance): \r\n for some common models. Some of the numbers are also listed [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md). \r\n", "@shashishekhar, thanks. I want to compile the tf-lite to get a smaller size of jni dynamic link library\uff08.so file\uff09, not a java library. How can I achieve this goal\uff1f", "can U get a static library ? ie. libtensorflow-lite.a.  I can only get one works on pc, not on armeabi-v7", "@wzzju: You can use our experimental [C library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/experimental/c/BUILD#L56) and corresponding [shared library](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/experimental/c/BUILD#L20) but be warned a lot of the API is still experimental and subject to change.", "@wzzju : Please reopen if this doesn't handle your case."]}, {"number": 22276, "title": "How to incorporate custom functions into tf.data pipe-lining process for maximum efficiency", "body": "So tf.image for example has some elementary image processing methods already implemented which i'd assumed are optimized. The question is as I'm iterating through a large dataset of images what/how is the recommended way of implementing a more complex function on every image, in batches of course, (for example a a patch 2-D DCT) for it to go as best as possible with the whole tf.data framework.\r\n\r\nThanks in advance.  \r\n\r\np.s. of course I could use the \"Map\" method but i'm asking beyond that. like if I'm passing a pure numpy written function to pass to \"map\", it wouldn't help as much.", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 22275, "title": "Class_weight with tf.dataset as input to model.fit will throw an error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No Mobile device\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.10\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**: ---\r\n- **GCC/Compiler version (if compiling from source)**:---\r\n- **CUDA/cuDNN version**: cuda-8.0\r\n- **GPU model and memory**: (Titan X and GeForce GTX 1080 )\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am using` tf.keras `in order to be able to feed the train_data using` tf.dataset` API through model.fit directly. It works fine whenever you didn't pass `class_weight`, but if you pass dict of class_weights, it will throw the following error :\r\n\r\n`AxisError: axis 1 is out of bounds for array of dimension 1\r\n`\r\n\r\nWhen I debug the error , I found the error happened exactly at line 531 : \r\n```\r\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_utils.py(530)standardize_weights()\r\n    528       raise ValueError('`class_weight` not supported for '\r\n    529                        '3+ dimensional targets.')\r\n    530     if y.shape[1] > 1:\r\n-->  531       y_classes = np.argmax(y, axis=1)\r\n    532     elif y.shape[1] == 1:\r\n\r\n```\r\nThe dimension of y is `TensorShape([Dimension(None), Dimension(8)])\r\n`\r\nSo` y.shape[1] > 1` , but the problem y is a **tensor** now not an **numpy array,** that is why it throws the previous error.\r\n\r\nSo is there any solution to this situation? \r\n\r\n### **EDIT**: Adding sample code to reproduce the error.\r\n\r\n```\r\nimport os, sys, logging\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport itertools as itt\r\nlogging.basicConfig(level=logging.INFO)\r\n\r\n\r\ndef _int64_feature(value):\r\n  return tf.train.Feature(int64_list= tf.train.Int64List(value= [value]))\r\n\r\ndef _bytes_feature(value):\r\n  return tf.train.Feature(bytes_list= tf.train.BytesList(value= [value]))\r\n\r\ndef _float32_feature(value):\r\n  return tf.train.Feature(float_list= tf.train.FloatList(value= [value]))\r\n\r\ndef tf_records_creating(tfrecord_file):\r\n    logging.info('Creating random tfrecord files for 100 sample')\r\n\r\n    labels = np.random.uniform(0, num_classes, total_train).astype(np.int32)\r\n    data = np.random.uniform(0, 255, total_train*224*224*3).reshape(total_train, 224, 224, 3).astype(np.int32)\r\n\r\n    writer = tf.python_io.TFRecordWriter(tfrecord_file)\r\n\r\n    for idx, (image, label) in enumerate(itt.izip(data, labels)):\r\n        image = image.tostring()\r\n        example = tf.train.Example(features=tf.train.Features(feature={\r\n            'label': _int64_feature(int(label)),\r\n            'image': _bytes_feature(image),\r\n        }))\r\n        writer.write(example.SerializeToString())\r\n    writer.close()\r\n    return\r\n\r\ndef decode(serialized_example):\r\n  features = tf.parse_single_example(\r\n      serialized_example,\r\n      features={\r\n          'image': tf.FixedLenFeature([], tf.string),\r\n          'label': tf.FixedLenFeature([], tf.int64),\r\n      })\r\n\r\n  image = tf.decode_raw(features['image'], tf.float32)\r\n  image.set_shape([224*224*3])\r\n\r\n  image=tf.reshape(image, (224,224,3))\r\n\r\n  label = tf.cast(features['label'], tf.int32)\r\n  label_categorical = tf.one_hot(label,depth= num_classes, on_value=1,off_value=0,dtype=tf.int32,)\r\n  label_categorical = tf.reshape(label_categorical, [num_classes])\r\n  label_categorical.set_shape([num_classes])\r\n\r\n  return image, label_categorical\r\n\r\ndef data_preparing(tfrecord_file):\r\n\r\n    logging.info('Preparing the Training tf.dataset ')\r\n    training_files = [tfrecord_file]\r\n    dataset_train = tf.data.TFRecordDataset(training_files, num_parallel_reads=1)\r\n    dataset_train = dataset_train.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=4 * batch_size))\r\n    dataset_train = dataset_train.map(decode, num_parallel_calls=1)  \r\n    dataset_train = dataset_train.batch(batch_size)\r\n    dataset_train = dataset_train.prefetch(tf.contrib.data.AUTOTUNE)\r\n    return dataset_train\r\n\r\ndef train_model(tfrecord_file):\r\n\r\n    base_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet',\r\n                                                 input_shape=(224, 224, 3), pooling='avg')\r\n\r\n    for layer in base_model.layers:\r\n        layer.trainable = False\r\n\r\n    logging.info('Building Our Classifier')\r\n    x = base_model.output\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\r\n    x = tf.keras.layers.BatchNormalization()(x)\r\n    x = tf.keras.layers.Dropout(0.5)(x)\r\n    x = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\r\n\r\n    model = tf.keras.models.Model(inputs=base_model.input, outputs=x)\r\n    model.summary()\r\n    opt = tf.keras.optimizers.Adam(lr=0.001)\r\n    model.compile(loss='categorical_crossentropy',\r\n                  optimizer=opt, metrics=['accuracy'])\r\n    dataset_train = data_preparing(tfrecord_file=tfrecord_file)\r\n\r\n    weighted_array_train= np.array([0.01557266,0.00867447,0.04579864,0.08275284,0.18281397,\r\n       0.30659676, 0.04686068, 0.31092999])\r\n    class_weight_dict = dict(enumerate(weighted_array_train))\r\n\r\n    if using_class_weight == True:\r\n        model.fit(x=dataset_train, epochs=epochs, verbose=1,class_weight = class_weight_dict,\r\n              steps_per_epoch=int(np.ceil(total_train / batch_size)))\r\n    else:\r\n        model.fit(x=dataset_train, epochs=epochs, verbose=1,\r\n              steps_per_epoch=int(np.ceil(total_train / batch_size)))\r\n    return\r\n\r\nif __name__== '__main__':\r\n\r\n    total_train = 100.\r\n    num_classes= 8\r\n    batch_size = 10\r\n    epochs = 100\r\n\r\n    #TODO (1) :Set the path to tfrecord file that we will create it.\r\n    tfrecord_file = '~/train.tfrecords'\r\n    tf_records_creating(tfrecord_file)                # Implement this only one time\r\n\r\n    using_class_weight= False                         # if you set this to True, you will produce the error\r\n    train_model(tfrecord_file)\r\n```\r\n", "comments": ["@fchollet @mrry Would you mind taking a look?", "@was84san I am not able to reproduce the scenario, as in case a `tf.dataset` is used, `y` is not specified in `model.fit`.\r\n\r\nCan you provide a minimal example that trigger the error you encounter?", "@was84san Any chance you could provide a sample tfrecord file and a \"complete\" reproducible sample code (from first line of `import tensorflow as tf` to last line of `model.fit`)?\r\n\r\nMany parameters in your code snippet are missing.", "@yongtang I edited the main post, you can run the sample_code directly", "@was84san I tried but couldn't reproduce:\r\n```\r\nubuntu@ubuntu # python q.py \r\nRuntimeError: module compiled against API version 0xc but this version of numpy is 0xa\r\nRuntimeError: module compiled against API version 0xc but this version of numpy is 0xa\r\nINFO:root:Creating random tfrecord files for 100 sample\r\nq.py:24: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\r\n  labels = np.random.uniform(0, num_classes, total_train).astype(np.int32)\r\nq.py:25: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\r\n  data = np.random.uniform(0, 255, total_train*224*224*3).reshape(total_train, 224, 224, 3).astype(np.int32)\r\n2018-09-17 00:34:49.835337: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nINFO:root:Building Our Classifier\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 224, 224, 3)       0         \r\n_________________________________________________________________\r\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \r\n...\r\n...\r\ndense_1 (Dense)              (None, 8)                 4104      \r\n=================================================================\r\nTotal params: 14,983,496\r\nTrainable params: 267,784\r\nNon-trainable params: 14,715,712\r\n_________________________________________________________________\r\nINFO:root:Preparing the Training tf.dataset \r\nEpoch 1/100\r\n10/10 [==============================] - 26s 3s/step - loss: 2.0800 - acc: 0.1300 \r\n...\r\nEpoch 99/100\r\n10/10 [==============================] - 25s 3s/step - loss: 2.0549 - acc: 0.2000 \r\nEpoch 100/100\r\n10/10 [==============================] - 25s 3s/step - loss: 2.0548 - acc: 0.2000 \r\nubuntu@ubuntu:/v# \r\n\r\n```", "`using_class_weight= True  ` ;   will  produce the error\r\n`using_class_weight= False  ` ;  make the code run normally ", "@pavithrasv Could you please take a look at this issue? Thanks!", "There are two **solutions** for me if this issue is not fixed. **One** is to return back to use` fit_generator` instead and feed my data from my own `generator,` as numpy array, and this one is much  slower than `model.fit`. The **second** is to keep use model.fit but instead of passing class_weigth, I should build custom loss function where I do the weighted loss.\r\n\r\nIf anyone just justify the condition in line 531 to make it work for both `tensor` and `numpy` array, it will help a lot. ", "@pavithrasv,  is there any update? ", "anyone can reproduce the error?", "@was84san I could reproduce the error. I also could see where the issue is. However the fix might not be so straightforward so still think the best way to address it.", "Added a PR #23381 for the fix. Please take a look.", "It is the right fix. Thank you. When the new changes are going to be merged? Sorry, but I am not familiar about how this work , I mean when I am going to see the fix, like in the new version of tensor flow?", "@was84san The PR #23381 is pending review. If the PR is approved it will be merged and picked up in the new version of TensorFlow.", "@yongtang Is there any possibility that this issue being reviewed soon :( . I really need to use it before my deadline. "]}, {"number": 22274, "title": "CollectiveAllReduceStrategy fails with CPU-only workers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: tf-nightly\r\n- **TensorFlow version (use command below)**: ('v1.9.0-rc2-4081-g626bc997c2', '1.11.0-dev20180913')\r\n- **Python version**: Python 2.7.15\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: https://gist.github.com/df3df82f7ae8f47b6288fc42eb8c8b17\r\n\r\n### Describe the problem\r\nInvoke `tf.estimator.train_and_evaluate` with `CollectiveAllReduceStrategy` fails on CPU-only worker nodes, with the following message:\r\n\r\n```\r\nInternalError: ScopedAllocatorMgr not supported on device /job:worker/replica:0/task:0/device:CPU:0\r\n```\r\n\r\n### Source code / logs\r\n\r\n```python\r\nfrom tensorflow.contrib.distribute import CollectiveAllReduceStrategy\r\nfrom tensorflow.contrib.distribute import DistributeConfig\r\n\r\ndistribution = CollectiveAllReduceStrategy(num_gpus_per_worker=0)\r\n\r\nconfig = tf.estimator.RunConfig(\r\n    experimental_distribute=DistributeConfig(\r\n        train_distribute=distribution,\r\n        remote_cluster={\r\n            'worker': ['localhost:5000', 'localhost:5001'],\r\n        },\r\n    )\r\n)\r\n\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n\r\ntf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\r\n```\r\n```\r\nINFO:tensorflow:CollectiveAllReduceStrategy with local_devices = ['/device:CPU:0']\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpeIE_x6\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': {'worker': ['localhost:5000', 'localhost:5001']}, '_model_dir': '/var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpeIE_x6', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x123f09810>, eval_distribute=None, remote_cluster={'worker': ['localhost:5000', 'localhost:5001']}), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.contrib.distribute.python.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x123f09810>, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}\r\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'worker': ['localhost:5000', 'localhost:5001']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:5000', 'localhost:5001']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ['/job:worker/task:0']\r\nINFO:tensorflow:Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'worker': ['localhost:5000', 'localhost:5001']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ['/job:worker/task:1']\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Collective All-reduce invoked with batches size = 2, num_workers = 2\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Creating chief session creator with config: device_filters: \"/job:worker/task:0\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nisolate_session_state: true\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\nINFO:tensorflow:Collective All-reduce invoked with batches size = 2, num_workers = 2\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Creating chief session creator with config: device_filters: \"/job:worker/task:1\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nisolate_session_state: true\r\nexperimental {\r\n  collective_group_leader: \"/job:worker/replica:0/task:0\"\r\n}\r\n\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Initialize system\r\nINFO:tensorflow:Initialize system\r\nINFO:tensorflow:Saving checkpoints for 0 into /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpeIE_x6/model.ckpt.\r\n\r\nException in thread Thread-5:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 344, in _run_single_worker\r\n    worker_fn(strategy)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py\", line 232, in _worker_fn\r\n    hooks=list(train_spec.hooks))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1178, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1325, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1408, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1148, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1239, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1224, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1296, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1306, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInternalError: ScopedAllocatorMgr not supported on device /job:worker/replica:0/task:1/device:CPU:0\r\n\t [[{{node scoped_allocator_1}} = _ScopedAllocator[T=DT_FLOAT, expected_call_count=2, id=1, sa_name=\"scoped_allocator_1\", shape=[17], shapes=[[1,1], [1]], _device=\"/job:worker/replica:0/task:1/device:CPU:0\"]()]]\r\n\r\nException in thread Thread-4:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 344, in _run_single_worker\r\n    worker_fn(strategy)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.py\", line 232, in _worker_fn\r\n    hooks=list(train_spec.hooks))\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1178, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1325, in _train_model_distributed\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 1408, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 671, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1148, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1239, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1224, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1296, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1076, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1306, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInternalError: ScopedAllocatorMgr not supported on device /job:worker/replica:0/task:0/device:CPU:0\r\n\t [[{{node scoped_allocator_1}} = _ScopedAllocator[T=DT_FLOAT, expected_call_count=2, id=1, sa_name=\"scoped_allocator_1\", shape=[17], shapes=[[1,1], [1]], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\nPS: thanks @yuefengz for today's introduction of multi-node distribution strategy in TF Roadshow 2018@Beijing \ud83d\ude06", "comments": ["Ping @dubey @poxvoculi", "Looks like your CPU device is not overriding the \r\n\r\nvirtual ScopedAllocatorMgr* GetScopedAllocatorMgr() const { return nullptr; }\r\n\r\nmethod on DeviceBase.  It is overridden in common_runtime/threadpool_device.h, which is normally the only way of building a CPU device.   I'm not sure whether this is an issue with the MacOS build or something specific to your build.  Given some more time, I'll try to reproduce.", "@byronyi really appreciate you came to our roadshow event!\r\n\r\nFor now, if there is not a simple workaround, you can try MirroredStrategy as well. But we still recommend CollectiveAllReducStrategy for multi-node training and we will fix it as soon as possible. Thanks!", "It seems I used [SingleThreadedCpuDevice](../tree/master/tensorflow/core/common_runtime/single_threaded_cpu_device.h) that was added in 9b18bd70b5739d646b21b7d45de0e5c96b8cc2a1, later than when [ScopedAllocator](../tree/master/tensorflow/core/common_runtime/scoped_allocator.cc) was added in 282750fee5e2df502436ca9ef6a95283f8adab34.\r\n\r\n@skye Mind to fix this? Or I could help submit a patch if you're okay with it.\r\n\r\nEDIT: turns out it's the problem of [RenamedDevice](../tree/master/tensorflow/core/common_runtime/renamed_device.h).\r\n", "@byronyi if you'd like to submit a PR that'd be great!", "@poxvoculi @dubey PR submitted.", "@yuefengz I just tried MirroredStrategy but it seems to require NCCL even if it is a CPU-only build.\r\n\r\nIs this going to be fixed soon? If not, I will open a separate issue for this.\r\n\r\n```\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with STANDALONE_CLIENT mode\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, '_model_dir': '/var/folders/gn/sjntndrs1fs22kfr302697mr0000gn/T/tmpSuQoQy', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': DistributeConfig(train_distribute=<tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90>, eval_distribute=None, remote_cluster={'ps': ['localhost:5000'], 'worker': ['localhost:5001']}), '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x12316fc90>, '_master': '', '_distribute_coordinator_mode': 'standalone_client'}\r\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'standalone_client', cluster_spec = {'ps': ['localhost:5000'], 'worker': ['localhost:5001']}, task_type = None, task_id = None, environment = None, rpc_layer = 'grpc'\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\r\nWARNING:tensorflow:Could not find a standard reader in the input pipeline(one of TextLineDataset, TFRecordDataset, FixedLengthRecordDataset).So auto-sharding is not done. Please verify correctness of auto-sharding for your input.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 2 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Creating chief session creator with config: allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\nisolate_session_state: true\r\n\r\nINFO:tensorflow:Graph was finalized.\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-6-77bd9e8610db> in <module>()\r\n     17 train_spec = tf.estimator.TrainSpec(input_fn=input_fn)\r\n     18 eval_spec = tf.estimator.EvalSpec(input_fn=input_fn)\r\n---> 19 tf.estimator.train_and_evaluate(estimator=estimator, train_spec=train_spec, eval_spec=eval_spec)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/training.pyc in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    460     logging.info('Running `train_and_evaluate` with Distribute Coordinator.')\r\n    461     distribute_coordinator_training.train_and_evaluate(\r\n--> 462         estimator, train_spec, eval_spec, _TrainingExecutor)\r\n    463     return\r\n    464 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in train_and_evaluate(estimator, train_spec, eval_spec, executor_cls)\r\n    262       mode=run_config._distribute_coordinator_mode,\r\n    263       cluster_spec=cluster_spec,\r\n--> 264       session_config=run_config.session_config)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in run_distribute_coordinator(worker_fn, strategy, eval_fn, eval_strategy, mode, cluster_spec, task_type, task_id, session_config, rpc_layer)\r\n    747       else:\r\n    748         _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy,\r\n--> 749                              cluster_spec, session_config, rpc_layer)\r\n    750     else:\r\n    751       # If not a client job, run the standard server.\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_in_graph_client(worker_fn, strategy, eval_fn, eval_strategy, cluster_spec, session_config, rpc_layer)\r\n    471       None,\r\n    472       session_config,\r\n--> 473       rpc_layer=rpc_layer)\r\n    474   if eval_thread:\r\n    475     eval_thread.join()\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_coordinator.pyc in _run_single_worker(worker_fn, strategy, cluster_spec, task_type, task_id, session_config, rpc_layer, worker_barrier)\r\n    342       worker_barrier=worker_barrier)\r\n    343   with context:\r\n--> 344     worker_fn(strategy)\r\n    345 \r\n    346 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/distribute/estimator_training.pyc in _worker_fn(strategy)\r\n    230         input_fn=train_spec.input_fn,\r\n    231         max_steps=train_spec.max_steps,\r\n--> 232         hooks=list(train_spec.hooks))\r\n    233 \r\n    234   def _eval_fn(strategy):\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    353 \r\n    354       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 355       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    356       logging.info('Loss for final step: %s.', loss)\r\n    357       return self\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1176   def _train_model(self, input_fn, hooks, saving_listeners):\r\n   1177     if self._train_distribution:\r\n-> 1178       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1179     else:\r\n   1180       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n   1323         return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1324                                                hooks, global_step_tensor,\r\n-> 1325                                                saving_listeners)\r\n   1326 \r\n   1327   def _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks,\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n   1403         save_summaries_steps=self._config.save_summary_steps,\r\n   1404         config=self._session_config,\r\n-> 1405         log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n   1406       loss = None\r\n   1407       while not mon_sess.should_stop():\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in MonitoredTrainingSession(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\r\n    449         max_wait_secs=max_wait_secs,\r\n    450         save_checkpoint_steps=save_checkpoint_steps,\r\n--> 451         summary_dir=summary_dir)\r\n    452 \r\n    453   if not is_chief:\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_monitored_session_with_worker_context(worker_context, scaffold, checkpoint_dir, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\r\n    340       session_creator=session_creator,\r\n    341       hooks=all_hooks,\r\n--> 342       stop_grace_period_secs=stop_grace_period_secs)\r\n    343 \r\n    344 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, stop_grace_period_secs)\r\n    919     super(MonitoredSession, self).__init__(\r\n    920         session_creator, hooks, should_recover=True,\r\n--> 921         stop_grace_period_secs=stop_grace_period_secs)\r\n    922 \r\n    923 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, session_creator, hooks, should_recover, stop_grace_period_secs)\r\n    641         stop_grace_period_secs=stop_grace_period_secs)\r\n    642     if should_recover:\r\n--> 643       self._sess = _RecoverableSession(self._coordinated_creator)\r\n    644     else:\r\n    645       self._sess = self._coordinated_creator.create_session()\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __init__(self, sess_creator)\r\n   1105     \"\"\"\r\n   1106     self._sess_creator = sess_creator\r\n-> 1107     _WrappedSession.__init__(self, self._create_session())\r\n   1108 \r\n   1109   def _create_session(self):\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _create_session(self)\r\n   1110     while True:\r\n   1111       try:\r\n-> 1112         return self._sess_creator.create_session()\r\n   1113       except _PREEMPTION_ERRORS as e:\r\n   1114         logging.info('An error was raised while a session was being created. '\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\r\n    798       \"\"\"Creates a coordinated session.\"\"\"\r\n    799       # Keep the tf_sess for unit testing.\r\n--> 800       self.tf_sess = self._session_creator.create_session()\r\n    801       # We don't want coordinator to suppress any exception.\r\n    802       self.coord = coordinator.Coordinator(clean_stop_exception_types=[])\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in create_session(self)\r\n    564         init_op=self._scaffold.init_op,\r\n    565         init_feed_dict=self._scaffold.init_feed_dict,\r\n--> 566         init_fn=self._scaffold.init_fn)\r\n    567 \r\n    568 \r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.pyc in prepare_session(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\r\n    285                            \"init_fn or local_init_op was given\")\r\n    286       if init_op is not None:\r\n--> 287         sess.run(init_op, feed_dict=init_feed_dict)\r\n    288       if init_fn:\r\n    289         init_fn(sess)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    885     try:\r\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 887                          run_metadata_ptr)\r\n    888       if run_metadata:\r\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1109       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1110                              feed_dict_tensor, options, run_metadata)\r\n   1111     else:\r\n   1112       results = []\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1284     if handle is None:\r\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1286                            run_metadata)\r\n   1287     else:\r\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1304           pass\r\n   1305       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1306       raise type(e)(node_def, op, message)\r\n   1307 \r\n   1308   def _extend_graph(self):\r\n\r\nNotFoundError: Op type not registered 'NcclAllReduce' in binary running on Bairens-MacBook.local. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\n```", "@byronyi would be good to file a separate issue for MirroredStrategy, we should not default to nccl for CPU only builds. ", "I guess #21470 would be enough for now? I saw multiple issues opened for dist-strat, let's not overload @yuefengz at this moment :)", "@byronyi it uses nccl as default all-reduce operations. The simplest workaround it to install nccl, or specifiy `cross_tower_ops=tf.contrib.distribute.ReductionToOneDeviceCrossTowerOps` in `MirroredStrategy`. If you have multiple workers in your cluster, `MirroredStrategy` will switch to another `cross_tower_ops`. Would that work for you?\r\n\r\nWe probably can check whether `nccl` is installed and if not switch to `ReductionToOneDeviceCrossTowerOps` in the future."]}, {"number": 22273, "title": "failed to build tensorflow android", "body": "Using : \r\n---------\r\nOS: Ubuntu 16.04\r\nBazel : 0.8.1\r\ntensorflow : 1.5.0\r\nNDK : 12b -> sdk level 14\r\nsdk : 23\r\nbuild tools: 25.2.0\r\n\r\nThe full build log of error:\r\n**https://justpaste.it/621im**\r\ntried the follwing solutions but still getting the error:\r\n**[](**\r\nhttps://github.com/tensorflow/tensorflow/issues/8641)\r\n**[](**\r\nhttps://github.com/tensorflow/tensorflow/issues/6356)\r\n\r\nPlease help me out.\r\n\r\nBelow is the shot form of Error:\r\n\r\n<command-line>:0:0: note: this is the location of the previous definition\r\nERROR: /home/kv/Desktop/Work/AR/TensorFlow/Source/Tensorflow_1.3/tensorflow-master/tensorflow/contrib/android/BUILD:29:1: C++ compilation of rule '//tensorflow/contrib/android:android_tensorflow_inference_jni' failed (Exit 1): arm-linux-androideabi-gcc failed: error executing command \r\n  (cd /home/kv/.cache/bazel/_bazel_kv/9644bdd546e0ad6ab4fab8f0ed146de9/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/home/kv/anaconda3/bin:/home/kv/bin:/home/kv/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/kv/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/home/kv/anaconda3/lib/python3.6/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -MD -MF bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.d '-frandom-seed=bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/armeabi-v7a-py3-opt/genfiles -iquote external/protobuf_archive -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/nsync -iquote external/fft2d -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/fft2d -iquote external/gemmlowp -iquote bazel-out/armeabi-v7a-py3-opt/genfiles/external/gemmlowp -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/protobuf_archive/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-v7a-py3-opt/genfiles/external/nsync/public -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -Wno-narrowing -fomit-frame-pointer -O2 '--sysroot=external/androidndk/ndk/platforms/android-12/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/contrib/android/asset_manager_filesystem.cc -o bazel-out/armeabi-v7a-py3-opt/bin/tensorflow/contrib/android/_objs/android_tensorflow_inference_jni/tensorflow/contrib/android/asset_manager_filesystem.o)\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::{anonymous}::RandomAccessFileFromAsset::Read(tensorflow::uint64, size_t, tensorflow::StringPiece*, char*) const':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:97:69: error: 'AAsset_seek64' was not declared in this scope\r\n     off64_t new_offset = AAsset_seek64(asset.get(), offset, SEEK_SET);\r\n                                                                     ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:98:52: error: 'AAsset_getLength64' was not declared in this scope\r\n     off64_t length = AAsset_getLength64(asset.get());\r\n                                                    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::NewReadOnlyMemoryRegionFromFile(const string&, std::unique_ptr<tensorflow::ReadOnlyMemoryRegion>*)':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:158:68: error: 'AAsset_openFileDescriptor64' was not declared in this scope\r\n   int fd = AAsset_openFileDescriptor64(asset.get(), &start, &length);\r\n                                                                    ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:173:44: error: 'AAsset_getLength64' was not declared in this scope\r\n     length = AAsset_getLength64(asset.get());\r\n                                            ^\r\ntensorflow/contrib/android/asset_manager_filesystem.cc: In member function 'virtual tensorflow::Status tensorflow::AssetManagerFileSystem::GetFileSize(const string&, tensorflow::uint64*)':\r\ntensorflow/contrib/android/asset_manager_filesystem.cc:214:38: error: 'AAsset_getLength64' was not declared in this scope\r\n   *s = AAsset_getLength64(asset.get());\r\n                                      ^\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 358.381s, Critical Path: 76.20s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Try this steps. I avoid that problem this way.\r\n\r\n1. go to `<NDK_HOME>/platforms`\r\n2. delete directories that named `android-9` and `android-12`.\r\n3. Copy and paste `android-13`, then name them `android-9` and `android-12`.\r\n4. Try bazel build with `cpu=armeabi-v7a` again.\r\n\r\nIf you want to build with arm architecture, these functions - AAsset_seek64(), AAsset_getLength64(), AAsset_openFileDescriptor64() - are declared at `<NDK_HOME>/platforms/android-<version>/arch-arm/usr/include/android/asset_manager.h`, and complier will link libraries at `<NDK_HOME>/platforms/android-<version>/arch-arm/usr/lib`.\r\n\r\nBut, at `android-9` and `android-12`, **these declarations are not exist!** This means that these functions may not be implemented in this version. And I don't know the exact reason, but with the `cpu=armeabi-v7a` option, the cross-compiler seems to be referring to that path.\r\n\r\nI think that's why your problem is occur.", "We're trying to deprecate mainline TensorFlow Android builds in favor of TensorFlow Lite, so we're unlikely to get to this one. Closing."]}]